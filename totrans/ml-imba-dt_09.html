<html><head></head><body>
		<div id="_idContainer170">
			<h1 id="_idParaDest-181" class="chapter-number"><a id="_idTextAnchor256"/>9</h1>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor257"/>Hybrid Deep Learning Methods</h1>
			<p>In this chapter, we will talk about some of <a id="_idIndexMarker637"/>the hybrid deep learning techniques that combine the data-level (<a href="B17259_07.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Data-Level Deep Learning Methods</em>) and algorithm-level (<a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep Learning Techniques</em>) methods in some ways. This chapter contains some recent and more advanced techniques that can be challenging to implement, so it is recommended to have a good understanding of the <span class="No-Break">previous chapters.</span></p>
			<p>We will begin with an introduction to graph machine learning, clarifying how graph models exploit relationships within data to boost performance, especially for minority classes. Through a side-by-side comparison of a <strong class="bold">Graph Convolutional Network</strong> (<strong class="bold">GCN</strong>), XGBoost, and MLP models, using an imbalanced social network dataset, we will highlight the superior performance of <span class="No-Break">the GCN.</span></p>
			<p>We will continue to explore strategies to tackle class imbalance in deep learning, examining techniques that manipulate data distribution and prioritize challenging examples. We will also go over techniques called hard example mining and minority class incremental rectification, which focus on improving model performance through prioritization of difficult instances and iterative enhancement of minority class <span class="No-Break">representation, respectively.</span></p>
			<p>While a significant portion of our discussion will revolve around image datasets, notably the imbalanced MNIST, itâ€™s crucial to understand the broader applicability of these techniques. For instance, our deep dive into graph machine learning wonâ€™t rely on MNIST. Instead, weâ€™ll switch gears to a more realistic dataset from Facebook, offering a fresh perspective on handling imbalances in <span class="No-Break">real-world scenarios.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Graph machine learning for <span class="No-Break">imbalanced data</span></li>
				<li>Hard <span class="No-Break">example mining</span></li>
				<li>Minority class <span class="No-Break">incremental rectification</span></li>
			</ul>
			<p>By the end of this chapter,<a id="_idTextAnchor258"/> we will become familiar with some hybrid methods, enabling us to understand the core principles behind more <span class="No-Break">complex techniques.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor259"/>Technical requirements</h1>
			<p>Similar to prior chapters, we will continue to utilize common libraries such as <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">torch</strong>. For graph machine learning, we will use the <strong class="source-inline">torch_geometric</strong> library as well. The code and notebooks for this chapter are available on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter09</a>. You can open the GitHub notebook using Google Colab by clicking on the <strong class="bold">Open in Colab</strong> icon at the top of the chapterâ€™s notebook or by launching it from <a href="https://colab.research.google.com">https://colab.research.google.com</a>, using the GitHub URL of <span class="No-Break">the notebook.</span></p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor260"/>Using graph machine learning for imbalanced data</h1>
			<p>In this section, we <a id="_idIndexMarker638"/>will see when graphs can be useful tools in machine learning, when to use graph ML models in general, and how they can be helpful on certain kinds of imbalanced datasets. Weâ€™ll also be exploring how graph ML models can outperform classical models such as XGBoost on certain <span class="No-Break">imbalanced datasets.</span></p>
			<p>Graphs are incredibly versatile data structures that can represent complex relationships and structures, from social networks and web pages (think of links as edges) to molecules in chemistry (consider atoms as nodes and the bonds between them as edges) and various other domains. Graph models allow us to represent the relationships in data, which can be helpful to make predictions and gain insights, even for problems where the relationships are not <span class="No-Break">explicitly defined.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor261"/>Understanding graphs</h2>
			<p>Graphs <a id="_idIndexMarker639"/>are the<a id="_idIndexMarker640"/> foundation of graph ML, so itâ€™s important to understand them first. In the context of computer science, a graph is a collection of nodes (or vertices) and edges. Nodes represent entities, and edges represent relationships or interactions between these entities. For example, in a social network, each person can be a node, and the friendship between two people can be <span class="No-Break">an edge.</span></p>
			<p>Graphs can<a id="_idIndexMarker641"/> be either directed (edges have a direction) or undirected (edges do not have a direction). They can also be weighted (edges have a value) or unweighted (edges do not have <span class="No-Break">a value).</span></p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> shows a <a id="_idIndexMarker642"/>sample tabular dataset on the left and its corresponding graph representation on <span class="No-Break">the right:</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B17259_09_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 â€“ Tabular data (left) contrasted with its visual graph representation (right)</p>
			<p>The graph representation on the right emphasizes the relationships between various entities. In the tabular representation on the left, devices and their IP addresses are listed with connection details along with network bandwidth. The graphical representation on the right visually represents these connections, allowing easier comprehension of the network topology. Devices are nodes, connections are edges with bandwidth are the weights. Graphs provide a clearer view of interrelationships than tables, emphasizing network <span class="No-Break">design insights.</span></p>
			<p>In the following section, we will get an overview of how ML can be applied <span class="No-Break">to graphs.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor262"/>Graph machine learning</h2>
			<p><strong class="bold">Graph Machine Learning</strong> (<strong class="bold">GML</strong>) is a<a id="_idIndexMarker643"/> set of techniques that use the structure of a graph to extract features and make predictions. GML algorithms can leverage the rich information contained in the graph structure, such as the connections between nodes and the patterns of these connections, to improve model performance, especially on <span class="No-Break">imbalanced data.</span></p>
			<p>Two popular neural <a id="_idIndexMarker644"/>network GML algorithms are GCNs and <strong class="bold">Graph Attention Networks</strong> (<strong class="bold">GATs</strong>). Both <a id="_idIndexMarker645"/>algorithms use the graph structure to aggregate information from a nodeâ€™s neighborhood. However, they differ in how they weigh the importance of a nodeâ€™s neighbors. GCN gives equal weight to all neighbors, while GAT uses attention mechanisms to assign different weights to different neighbors. We will limit our discussion to GCNs only in <span class="No-Break">this chapter.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor263"/>Dealing with imbalanced data</h2>
			<p>In ML, when <a id="_idIndexMarker646"/>one class significantly outnumbers the others, the model may become biased toward the majority class, leading to poor performance on the minority class. This is problematic because, often, the minority class is the one of interest. For example, in fraud detection, the number of non-fraud cases significantly outnumbers the fraud cases, but itâ€™s the fraud cases that weâ€™re interested <span class="No-Break">in detecting.</span></p>
			<p>In the context of GML, the structure of the graph can provide additional information that can help mitigate the effects of data imbalance. For example, minority class nodes might be more closely connected to each other than to majority class nodes. GML algorithms can leverage this structure to improve the performance of the <span class="No-Break">minority class.</span></p>
			<h3>GCNs</h3>
			<p>We will briefly discuss the key ideas behind what GCNs are and how <span class="No-Break">they work.</span></p>
			<p>GCNs offer a <a id="_idIndexMarker647"/>unique method of processing structured data. Unlike standard neural networks that assume independent and identically distributed data, GCNs can operate over graph data, capturing dependencies and connections <span class="No-Break">between nodes.</span></p>
			<p>The essence of GCNs is message passing, which can be broken down <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Node messaging</strong>: Each<a id="_idIndexMarker648"/> node in the graph sends out and receives messages through its edges to and from <span class="No-Break">its neighbors</span></li>
				<li><strong class="bold">Aggregation</strong>: Nodes <a id="_idIndexMarker649"/>aggregate these messages to gain a broader understanding of these <span class="No-Break">local neighborhoods</span></li>
			</ul>
			<p>In GCNs, full feature <a id="_idIndexMarker650"/>vectors are passed around instead of just the labels of <span class="No-Break">the nodes.</span></p>
			<p>Think of a GCN layer as a transformation step. The primary operations can be viewed <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Aggregate neighbors</strong>: Nodes pull <a id="_idIndexMarker651"/>features from their neighbors, leading to <span class="No-Break">an aggregation</span></li>
				<li><strong class="bold">Neural network transformation</strong>: The<a id="_idIndexMarker652"/> aggregated feature set from the previous step then undergoes transformation via the neural<a id="_idTextAnchor264"/> <span class="No-Break">network layer</span></li>
			</ul>
			<p>Letâ€™s explore GCNs using the <a id="_idIndexMarker653"/>example of photos on Facebook. Users upload photos, and our objective is to categorize these images as either spam or non-spam. This categorization is based on the image content as well as the IP address or <span class="No-Break">user ID.</span></p>
			<p>Letâ€™s imagine we have a graph where each node is a Facebook photo, and two photos are connected if they were uploaded using either the same IP address or the <span class="No-Break">same account.</span></p>
			<p>Letâ€™s say we want to use the actual content of the photo (possibly a feature vector from a pre-trained CNN or some metadata) as a node attribute. Letâ€™s assume that we have a 5-dimensional vector representing each <span class="No-Break">photoâ€™s features.</span></p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B17259_09_02.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 â€“ An image with a 5-dimensional feature embedding</p>
			<h4>Step 1 â€“ graph creation</h4>
			<p>We will create a graph <a id="_idIndexMarker654"/>where every vertex symbolizes a Facebook image. We will establish a link between two images if they were uploaded via an identical IP address or <span class="No-Break">user ID.</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B17259_09_03.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 â€“ A diagram with images connected to each other if they share the IP or user ID</p>
			<h4>Step 2 â€“ image representation</h4>
			<p>Represent each <a id="_idIndexMarker655"/>image using a 5-dimensional vector. This can be accomplished by using image metadata, features derived from trained neural networks, or other techniques suitable for image data (<span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
			<h4>Step 3 â€“ a singular-layer GCN for image analysis</h4>
			<p>When a <a id="_idIndexMarker656"/>specific image is passed through a single-layer GCN, here is what <span class="No-Break">will happen:</span></p>
			<ol>
				<li>We aggregate all the neighboring imageâ€™s feature vectors. The neighbors are images with matching IP addresses or <span class="No-Break">user IDs.</span></li>
				<li>An average function is used to combine vectors. Letâ€™s call the combined vector the neighborhood <span class="No-Break">average vector.</span></li>
				<li>The neighborhood average vector is multiplied using a weight matrix (of, say, size 5x1, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></li>
				<li>Then, an activation function is applied to the outcome to get a single value, which <a id="_idIndexMarker657"/>suggests the likelihood of the image <span class="No-Break">being spam.</span></li>
			</ol>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B17259_09_04.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 â€“ How a single GCN layer works</p>
			<h4>Step 4 â€“ a dual-layer GCN</h4>
			<p>Multilayer GCNs, like<a id="_idIndexMarker658"/> traditional <a id="_idIndexMarker659"/>deep neural networks, can be stacked into <span class="No-Break">multiple layers:</span></p>
			<ul>
				<li>Raw node features feed into the <span class="No-Break">first layer</span></li>
				<li>The subsequent layerâ€™s input is the previous <span class="No-Break">layerâ€™s output</span></li>
			</ul>
			<p>With each added layer, GCNs grasp more extended neighborhood information. For instance, in a two-layer GCN, information can ripple two hops away in <span class="No-Break">the graph.</span></p>
			<p>With a foundational understanding of graph ML and GCNs in place, weâ€™re set to explore a case study. Weâ€™ll compare the performance of a graph model to other models, including a classical ML model, on an imbalanced graph dataset. Our aim is to determine whether graph models can outperform other models by leveraging the relationships between <span class="No-Break">graph structures.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor265"/>Case study â€“ the performance of XGBoost, MLP, and a GCN on an imbalanced dataset</h2>
			<p>We will use <a id="_idIndexMarker660"/>the Facebook Page-Page dataset from<a id="_idIndexMarker661"/> the <strong class="bold">PyTorch Geometric</strong> (<strong class="bold">PyG</strong>) library, designed to create and train deep learning models on graphs and other irregular structures. This dataset comprises a large collection of social networks from Facebook, where nodes represent official Facebook pages and edges signify reciprocal likes between them. Each node is labeled with one of four categories: Politicians, Governmental Organizations, Television Shows, or Companies. The task is to predict these categories based on the node characteristics, which are derived from descriptions provided by the <span class="No-Break">page owners.</span></p>
			<p>The dataset serves as a challenging benchmark for graph neural network models due to its size and complexity. It was collected via the Facebook Graph API in November 2017 and focuses on multi-class node classification within the four aforementioned categories. You can find more about the dataset <span class="No-Break">at </span><a href="https://snap.stanford.edu/data/facebook-large-page-page-network.html"><span class="No-Break">https://snap.stanford.edu/data/facebook-large-page-page-network.html</span></a><span class="No-Break">.</span></p>
			<p>We start by importing some of the common libraries and the <span class="No-Break">Facebook dataset:</span></p>
			<pre class="source-code">
import pandas as pd
from torch_geometric.datasets import FacebookPagePage
dataset = FacebookPagePage(root=".")
data = dataset[0]</pre>			<p>The data object here is of <span class="No-Break">type </span><span class="No-Break"><strong class="source-inline">torch_geometric.data</strong></span><span class="No-Break">.</span></p>
			<p>Here are some statistics about the <span class="No-Break">graph data:</span></p>
			<pre class="source-code">
Dataset: FacebookPagePage()
-----------------------
Number of graphs: 1
Number of features: 128
Number of classes: 4
Number of graphs: 1
Number of nodes: 22,470
Number of edges: 342,004
Average node degree: 15.22
Contains isolated nodes: False
Contains self-loops: True
Is undirected: True</pre>			<p>Letâ€™s <a id="_idIndexMarker662"/>print the features and label in a <span class="No-Break">tabular format:</span></p>
			<pre class="source-code">
dfx = pd.DataFrame(data.x.numpy())
dfx['label'] = pd.DataFrame(data.y)
dfx</pre>			<p>This prints the features and labels contained in the <span class="No-Break"><strong class="source-inline">dfx</strong></span><span class="No-Break"> DataFrame:</span></p>
			<table id="table001-8" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">2</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">â€¦</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">127</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">label</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.262576</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.276483</span></p>
						</td>
						<td class="No-Table-Style">
							<p>â€¦</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.223836</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.262576</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.276483</span></p>
						</td>
						<td class="No-Table-Style">
							<p>â€¦</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.128634</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.262576</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.265053</span></p>
						</td>
						<td class="No-Table-Style">
							<p>â€¦</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.223836</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
						<td class="No-Table-Style">
							<p>...</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">22468</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.262576</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.276483</span></p>
						</td>
						<td class="No-Table-Style">
							<p>â€¦</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.218148</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">22469</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.232275</span></p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.276483</span></p>
						</td>
						<td class="No-Table-Style">
							<p>â€¦</p>
						</td>
						<td class="No-Table-Style">
							<p>-<span class="No-Break">0.221275</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 â€“ A dataset with each row showing feature values; the last column shows the label for each data point</p>
			<p>The overall printed result is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
22470 rows Ã— 129 columns</pre>			<p>These <a id="_idIndexMarker663"/>127 features have been generated using the Doc2Vec technique from the page description text. These features act like an embedding vector for each <span class="No-Break">Facebook page.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em>, we visualize the dataset using Gephi, which is a graph <span class="No-Break">visualization software:</span></p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B17259_09_05.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 â€“ The Facebook Page-Page dataset Gephi visualization</p>
			<p>The graph contains both inter-category and intra-category connections, but the latter is more dominant, highlighting the mutual-like affinity within the same category. This leads to distinct clusters, offering a birdâ€™s-eye view of the strong intra-category affiliations<a id="_idIndexMarker664"/> on Facebook. If we analyze the original dataset for various classes, their proportion is not so imbalanced. So, letâ€™s add some imbalance by removing some nodes randomly (as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">):</span></p>
			<table id="table002-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Class</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Number of nodes in the </strong><span class="No-Break"><strong class="bold">original dataset</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Number of nodes after removing </strong><span class="No-Break"><strong class="bold">some nodes</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">3,327</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">3,327</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">6,495</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">1,410</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">6,880</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">460</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">5,768</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">256</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.2 â€“ The distribution of various classes in the dataset</p>
			<p>Here is what the distribution of data looks like after <span class="No-Break">adding imbalance:</span></p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B17259_09_06.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 â€“ The distribution of various classes after adding imbalance</p>
			<p>Letâ€™s split the <a id="_idIndexMarker665"/>data into training and test sets by specifying their ranges of indices. In the <strong class="source-inline">Data</strong> object, we can specify this range to denote training and test sets, <span class="No-Break">using masks:</span></p>
			<pre class="source-code">
# Create masks
data.train_mask = range(4368)
data.val_mask = range(4368, 4611)
data.test_mask = range(4611, 4853)</pre>			<h3>Training an XGBoost model</h3>
			<p>Letâ€™s set up a <a id="_idIndexMarker666"/>simple baseline using an XGBoost model on <span class="No-Break">this dataset.</span></p>
			<p>First, letâ€™s create our train/test dataset using the masks we <span class="No-Break">created earlier:</span></p>
			<pre class="source-code">
X_train, X_test, y_train, y_test = \
Â Â Â Â data1.x[data1.train_mask].cpu().numpy(), \
Â Â Â Â data1.x[data1.test_mask].cpu().numpy(), \
Â Â Â Â data1.y[data1.train_mask].cpu().numpy(), \
Â Â Â Â data1.y[data1.test_mask].cpu().numpy()</pre>			<p>Then, we train and evaluate on <span class="No-Break">the data:</span></p>
			<pre class="source-code">
xgb_clf = XGBClassifier(eval_metric='logloss')
xgb_clf.fit(X_train, y_train)
y_pred = xgb_clf.predict_proba(X_test)
test_acc = accuracy_score(y_test, np.argmax(y_pred,axis=1))
test_acc.round(3)</pre>			<p>This <a id="_idIndexMarker667"/>prints the following accuracy value on the <span class="No-Break">test set:</span></p>
			<pre class="source-code">
0.793</pre>			<p>Letâ€™s plot the <span class="No-Break">PR curve:</span></p>
			<pre class="source-code">
y1_test_one_hot = F.one_hot(data1.y[data1.test_mask], \
Â Â Â Â num_classes=4)
display_precision_recall_curve(y1_test_one_hot, y_pred)</pre>			<p>This prints the PR curve (<span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em>) and area for various classes, using the XGBoost model. The area for the most imbalanced class, 3, is the lowest, <span class="No-Break">as expected.</span></p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B17259_09_07.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 â€“ The PR curve using XGBoost</p>
			<h3>Training a MultiLayer Perceptron model</h3>
			<p>We can set up another baseline using the simplest of deep learning models, the <strong class="bold">MultiLayer Perceptron</strong> (<strong class="bold">MLP</strong>). <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.8</em> shows the PR curve for each class. Overall, the MLP did worse<a id="_idIndexMarker668"/> than XGBoost, but its performance on the most imbalanced class, 3, is better than that <span class="No-Break">of XGBoost.</span></p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B17259_09_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 â€“ The PR curve using the MLP model</p>
			<h3>Training a GCN model</h3>
			<p>Finally, we<a id="_idIndexMarker669"/> switch to using a graph convolutional network, which is a generalization of the convolution layers in CNNs. A GCN, as we discussed previously, uses the structure of the graph to update the features of each node, based on its neighborsâ€™ features. In other words, each node gets to learn from <span class="No-Break">its friends!</span></p>
			<p>The first step involves importing the required libraries. Here, we import <strong class="source-inline">PyTorch</strong>, the <strong class="source-inline">GCNConv</strong> module from PyG, and the <strong class="source-inline">functional</strong> module <span class="No-Break">from PyTorch:</span></p>
			<pre class="source-code">
import torch
from torch_geometric.nn import GCNConv
import torch.nn.functional as F</pre>			<p>The <strong class="source-inline">GraphConvolutionalNetwork</strong> class is a representation of our model. The class inherits from PyTorchâ€™s <strong class="source-inline">nn.Module</strong>. It contains an initializer, a forward function for forward propagation, a function to train the model, and a function to evaluate <span class="No-Break">the model:</span></p>
			<pre class="source-code">
class GraphConvolutionalNetwork(torch.nn.Module):
Â Â Â Â def __init__(self, input_dim, hidden_dim, output_dim):
Â Â Â Â Â Â Â Â super().__init__()
Â Â Â Â Â Â Â Â self.convolution_layer1 = GCNConv(input_dim, hidden_dim)
Â Â Â Â Â Â Â Â self.convolution_layer2 = GCNConv(hidden_dim, output_dim)</pre>			<p>In the <strong class="source-inline">__init__()</strong> function, we initialize the layers of the model. Our model contains two <strong class="bold">Graph Convolutional Network layers</strong> (<strong class="bold">GCNConv layers</strong>). The <a id="_idIndexMarker670"/>dimensions of the input, hidden, and output layers are required <span class="No-Break">as arguments.</span></p>
			<p>Then, we <a id="_idIndexMarker671"/>define a <strong class="source-inline">forward()</strong> function to perform forward propagation through the network. It takes the node features and edge index as input, applies the first GCN layer followed by a ReLU activation function, and then applies the second GCN layer. The function then applies a <strong class="source-inline">log_softmax</strong> activation function and returns <span class="No-Break">the result:</span></p>
			<pre class="source-code">
Â Â Â Â def forward(self, node_features, edge_index):
Â Â Â Â Â Â Â Â hidden_representation = self.convolution_layer1( \
Â Â Â Â Â Â Â Â Â Â Â Â node_features,edge_index)
Â Â Â Â Â Â Â Â hidden_representation = torch.relu(hidden_representation)
Â Â Â Â Â Â Â Â output_representation = self.convolution_layer2 \
Â Â Â Â Â Â Â Â Â Â Â Â (hidden_representation, edge_index)
Â Â Â Â Â Â Â Â return F.log_softmax(output_representation, dim=1)</pre>			<p>The <strong class="source-inline">train_model()</strong> function trains the model. It takes in the data and the number of epochs as input. It sets the model to training mode and initializes the <strong class="bold">negative log-likelihood loss</strong> (<strong class="bold">NLLLoss</strong>) as the<a id="_idIndexMarker672"/> loss function, and Adam as the optimizer. It then runs a loop for the specified number of epochs to train the model. Within each epoch, it computes the output <a id="_idIndexMarker673"/>of the model, calculates the loss and accuracy, performs backpropagation, and updates the model parameters. It also calculates and prints the training and validation losses and accuracies every <span class="No-Break">20 epochs:</span></p>
			<pre class="source-code">
Â Â Â Â def train_model(self, data, num_epochs):
Â Â Â Â Â Â Â Â loss_function = torch.nn.NLLLoss()
Â Â Â Â Â Â Â Â optimizer = torch.optim.Adam(self.parameters(),\
Â Â Â Â Â Â Â Â Â Â Â Â lr=0.01, weight_decay=5e-4)
Â Â Â Â Â Â Â Â self.train()
Â Â Â Â Â Â Â Â for epoch in range(num_epochs + 1):
Â Â Â Â Â Â Â Â Â Â Â Â optimizer.zero_grad()
Â Â Â Â Â Â Â Â Â Â Â Â network_output = self(data.x, data.edge_index)
Â Â Â Â Â Â Â Â Â Â Â Â true_train_labels = data.y[data.train_mask]
Â Â Â Â Â Â Â Â Â Â Â Â loss = loss_function(network_output[data.train_mask], \
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â true_train_labels)
Â Â Â Â Â Â Â Â Â Â Â Â accuracy = compute_accuracy(\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â network_output[data.train_mask].argmax(\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dim=1), true_train_labels)
Â Â Â Â Â Â Â Â Â Â Â Â loss.backward()
Â Â Â Â Â Â Â Â Â Â Â Â optimizer.step()
Â Â Â Â Â Â Â Â Â Â Â Â if(epoch % 20 == 0):
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â true_val_labels = data.y[data.val_mask]
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â val_loss = loss_function(\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â network_output[data.val_mask], true_val_labels)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â val_accuracy = compute_accuracy(\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â network_output[data.val_mask].argmax(\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â dim=1), true_val_labels)
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â print(f'Epoch: {epoch}\n'\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â f'Train Loss: {loss:.3f}, Accuracy:\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â {accuracy*100:.0f}%\n'\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â f'Validation Loss: {val_loss:.2f},\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Accuracy: {val_accuracy*100:.0f}%\n'\
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â '-------------------')</pre>			<p>The <strong class="source-inline">evaluate_model</strong> function is used to evaluate the model. It sets the model to evaluation <a id="_idIndexMarker674"/>mode and calculates the output of the model and the test accuracy. It returns the test accuracy and the output for the <span class="No-Break">test data.</span></p>
			<pre class="source-code">
Â Â Â Â @torch.no_grad()
Â Â Â Â def evaluate_model(self, data):
Â Â Â Â Â Â Â Â self.eval()
Â Â Â Â Â Â Â Â network_output = self(data.x, data.edge_index)
Â Â Â Â Â Â Â Â test_accuracy = compute_accuracy(\
Â Â Â Â Â Â Â Â Â Â Â Â network_output.argmax(dim=1)[data.test_mask],\
Â Â Â Â Â Â Â Â Â Â Â Â data.y[data.test_mask])
Â Â Â Â Â Â Â Â return test_accuracy,network_output[data.test_mask,:]</pre>			<p>We start the training process and then evaluate <span class="No-Break">the model:</span></p>
			<pre class="source-code">
gcn = GraphConvolutionalNetwork(dataset.num_features, 16,\
Â Â Â Â dataset.num_classes)
gcn.train_model(data1, num_epochs=100)
acc,_ = gcn.evaluate_model(data1)
print(f'\nGCN test accuracy: {acc*100:.2f}%\n')</pre>			<p>This produces<a id="_idIndexMarker675"/> the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
Epoch: 0
Train Loss: 1.414, Accuracy: 32%
Validation Loss: 1.38, Accuracy: 29%
-------------------
Epoch: 20
Train Loss: 0.432, Accuracy: 85%
Validation Loss: 0.48, Accuracy: 83%
-------------------
Epoch: 40
Train Loss: 0.304, Accuracy: 89%
Validation Loss: 0.43, Accuracy: 86%
-------------------
Epoch: 60
Train Loss: 0.247, Accuracy: 92%
Validation Loss: 0.43, Accuracy: 86%
-------------------
Epoch: 80
Train Loss: 0.211, Accuracy: 93%
Validation Loss: 0.43, Accuracy: 88%
-------------------
Epoch: 100
Train Loss: 0.184, Accuracy: 94%
Validation Loss: 0.44, Accuracy: 88%
-------------------
GCN test accuracy: 90.91%</pre>			<p>Letâ€™s print<a id="_idIndexMarker676"/> the <span class="No-Break">PR curves:</span></p>
			<pre class="source-code">
_, y1_score = gcn.evaluate_model(data1)
y1_test_one_hot = F.one_hot(data1.y[data1.test_mask], num_classes=4)
display_precision_recall_curve(y1_test_one_hot, y1_score)</pre>			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B17259_09_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 â€“ The PR curve using the GCN model</p>
			<p>In <em class="italic">Table 9.3</em>, we<a id="_idIndexMarker677"/> compare the overall accuracy values as well as class-wise accuracy values of <span class="No-Break">various models:</span></p>
			<table id="table003-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Accuracy %</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">MLP</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">XGBoost</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">GCN</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Overall</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76.5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">83.9</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">90.9</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Class 0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">84.9</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">95.2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">96.6</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Class 1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">72.9</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78.0</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">88.1</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Class 2</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">33.3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">57.1</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">71.4</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Class 3</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">68.8</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">37.5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">75.0</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.3 â€“ Class-wise accuracy values in % on the Facebook Page-Page network dataset</p>
			<p>Here are<a id="_idIndexMarker678"/> some insights from <span class="No-Break"><em class="italic">Table 9.3</em></span><span class="No-Break">:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Overall performance</strong></span><span class="No-Break">:</span><ul><li>GCN outshines both MLP and XGBoost with an overall accuracy of 90.9%. GCN is the best<a id="_idIndexMarker679"/> for this network data, excelling in <span class="No-Break">all classes.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Class-specific insights</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Class 0</strong>: GCN and XGBoost do well on <span class="No-Break">class 0.</span></li><li><strong class="bold">Classes 1â€“3</strong>: GCN leads, while MLP and XGBoost struggle, especially in classes 2 and 3. Note in particular that on class 3, which had the fewest number of examples in the training data, GCN performed significantly better <span class="No-Break">than others.</span></li></ul></li>
			</ul>
			<p>Here, we compared the performance of the traditional ML algorithm of XGBoost and a basic MLP deep learning model with GCN, a graph ML model on an imbalanced dataset. The results showed that graph ML algorithms can outperform traditional algorithms, demonstrating the potential of graph ML to deal with <span class="No-Break">imbalanced data.</span></p>
			<p>The superior performance of graph ML algorithms can<a id="_idIndexMarker680"/> be attributed to their ability to leverage the structure of the graph. By aggregating information from a nodeâ€™s neighborhood, graph ML algorithms can capture local and global patterns in data that traditional algorithms <span class="No-Break">might miss.</span></p>
			<p class="callout-heading">ğŸš€ Graph ML at Uber and Grab</p>
			<p class="callout">ğŸ¯<strong class="bold"> Problem </strong><span class="No-Break"><strong class="bold">being solved</strong></span><span class="No-Break">:</span></p>
			<p class="callout">Both Uber and Grab aimed to tackle the complex issue of fraud across their diverse service offerings, ranging from ride-hailing to food delivery and financial services. Uber focused on collusion fraud [1], where groups of users work together to commit fraud. For instance, users can collaborate to take fake trips using stolen credit cards and then request chargebacks from the bank to get refunds for those illegitimate purchases. Grab aimed for a general fraud detection framework that could adapt to new <span class="No-Break">patterns [2].</span></p>
			<p class="callout">âš–ï¸<strong class="bold"> Data </strong><span class="No-Break"><strong class="bold">imbalance issue</strong></span><span class="No-Break">:</span></p>
			<p class="callout">Fraudulent activities were rare but diverse, creating a class imbalance problem. Both companies faced the challenge of adapting to new <span class="No-Break">fraud patterns.</span></p>
			<p class="callout">ğŸ¨<strong class="bold"> Graph </strong><span class="No-Break"><strong class="bold">modeling strategy</strong></span><span class="No-Break">:</span></p>
			<p class="callout">â€¢  <strong class="bold">Graph models</strong>: Both companies employed <strong class="bold">Relational Graph Convolutional Networks</strong> (<strong class="bold">RGCNs</strong>) to capture complex relationships indicative of fraud. To determine whether an Uber user is fraudulent, Uber wanted to leverage not just the features of the target user but also the features of users connected to them within a defined <span class="No-Break">network distance.</span></p>
			<p class="callout">â€¢  <strong class="bold">Semi-supervised learning</strong>: Grabâ€™s RGCN model was trained on a graph with millions of nodes and edges, where only a small percentage had labels. Tree-based models rely heavily on quality labels and feature engineering, while graph-based models need minimal feature engineering and excel in detecting unknown fraud, using <span class="No-Break">graph structures.</span></p>
			<p class="callout">ğŸ“Š<strong class="bold"> </strong><span class="No-Break"><strong class="bold">Real-world impact</strong></span><span class="No-Break">:</span></p>
			<p class="callout">Graph-based models proved to be effective in detecting both known and unknown fraud risks. They require less feature engineering and are less dependent on labels, making them a sustainable foundation to combat various types of fraud risks. However, Grab does not use RGCN for real-time model prediction due to latency <span class="No-Break">concerns [2].</span></p>
			<p class="callout">ğŸ› <strong class="bold"> Challenges </strong><span class="No-Break"><strong class="bold">and tips</strong></span><span class="No-Break">:</span></p>
			<p class="callout">â€¢  <strong class="bold">Data pipeline and scalability</strong>: Large graph sizes necessitated distributed training and prediction. Future work was needed to enhance real-time capabilities <span class="No-Break">at Uber.</span></p>
			<p class="callout">â€¢  <strong class="bold">Batch real-time prediction</strong>: Fo<a id="_idTextAnchor266"/>r Grab, real-time graph updates were computationally intensive, making batch real-time predictions a <span class="No-Break">viable solution.</span></p>
			<p>In conclusion, graph ML <a id="_idIndexMarker681"/>offers a promising approach to deal with imbalanced data, when the data either inherently has a graph structure or we think we can exploit the interconnectedness in the data. By leveraging the rich information contained in the graph structure, graph ML algorithms can improve model performance and provide more accurate and reliable predictions. As more data becomes available and graphs become larger and more complex, its potential to deal with imbalanced data will only continue <span class="No-Break">to grow.</span></p>
			<p>In the following section, we will <a id="_idTextAnchor267"/>shift our focus to a different strategy called hard example mining, which operates on the principle of prioritizing the most challenging examples in <span class="No-Break">our dataset.</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor268"/>Hard example mining</h1>
			<p>Hard example mining<a id="_idIndexMarker682"/> is a technique in deep learning that forces the model to pay more attention to these difficult examples, and to prevent overfitting to the majority of the samples that are easy to predict. To do this, hard example mining identifies and selects the most challenging samples in the dataset and then backpropagates the loss incurred only by those challenging samples. Hard example mining is often used in computer vision tasks such as object detection. Hard examples can be of <span class="No-Break">two kinds:</span></p>
			<ul>
				<li><strong class="bold">Hard positive examples</strong> are the <a id="_idIndexMarker683"/>correctly labeled examples with low <span class="No-Break">prediction scores</span></li>
				<li><strong class="bold">Hard negative examples</strong> are<a id="_idIndexMarker684"/> incorrectly labeled examples with high prediction scores, which are obvious mistakes made by <span class="No-Break">the model</span></li>
			</ul>
			<p>The term â€œminingâ€ refers to the process of finding such examples that are â€œhard.â€ The idea of hard negative mining is not really new and is quite similar to the idea of <strong class="bold">boosting</strong>, on <a id="_idIndexMarker685"/>which the popular algorithms of boosted decision trees are based. The <a id="_idIndexMarker686"/>boosted decision trees essentially figure out the examples on which the model makes mistakes, and then a new model (called a weak learner) is trained on such â€œ<span class="No-Break">hardâ€ example<a id="_idTextAnchor269"/>s.</span></p>
			<p>When dealing with large datasets, processing all training data to identify difficult examples can be time-consuming. This motivates our exploration of the online version of hard <span class="No-Break">example mining.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor270"/>Online Hard Example Mining</h2>
			<p>In <strong class="bold">Online Hard Example Mining</strong> (<strong class="bold">OHEM</strong>) [3], the â€œhardâ€ examples<a id="_idIndexMarker687"/> are figured out for each batch of the training cycle, where we take the <em class="italic">k</em> examples, which have the lowest value of the loss. We then backpropagate the loss for only those <em class="italic">k</em> examples during <span class="No-Break">the training.</span></p>
			<p>This way, the network focuses on the most difficult samples that have more information than the easy samples, and the model improves faster with less <span class="No-Break">training data.</span></p>
			<p>The OHEM<a id="_idIndexMarker688"/> technique, introduced in the paper by Shrivastava et al. [3], has been quite popular. It is a technique primarily used in object detection to improve model performance by focusing on challenging cases. It aims to efficiently select a subset of â€œhardâ€ negative examples that are most informative to train the model. As an example, imagine weâ€™re developing a facial recognition model, and our dataset consists of images with faces (positive examples) and images without faces (negative examples). In practice, we often encounter a large number of negative examples compared to a smaller set of positive ones. To make our training more efficient, itâ€™s wise to select a subset of the most challenging negative examples that will be most informative for <span class="No-Break">our model.</span></p>
			<p>In our experiment, we found that online hard example mining did help the imbalanced MNIST dataset and improved our modelâ€™s performance on the most <span class="No-Break">imbalanced classes.</span></p>
			<p>Here is the core implementation of the <span class="No-Break">OHEM function:</span></p>
			<pre class="source-code">
class NLL_OHEM(torch.nn.NLLLoss):
Â Â Â Â def __init__(self):
Â Â Â Â Â Â Â Â super(NLL_OHEM, self).__init__()
Â Â Â Â def forward(self, cls_pred, cls_target, rate=0.95):
Â Â Â Â Â Â Â Â batch_size = cls_pred.size(0)
Â Â Â Â Â Â Â Â ohem_cls_loss = F.cross_entropy(cls_pred,\
Â Â Â Â Â Â Â Â Â Â Â Â cls_target, ignore_index=-1)
Â Â Â Â Â Â Â Â keep_num = int(batch_size*rate)
Â Â Â Â Â Â Â Â ohem_cls_loss = ohem_cls_loss.topk(keep_num)[0]
Â Â Â Â Â Â Â Â cls_loss = ohem_cls_loss.sum() / keep_num # mean
Â Â Â Â Â Â Â Â return cls_loss</pre>			<p>In the <strong class="source-inline">NLL_OHEM</strong> class, we first computed the regular cross-entropy loss, and then we figured out the <em class="italic">k</em> smallest loss values. These <em class="italic">k</em> values denote the hardest <em class="italic">k</em> examples that the <a id="_idIndexMarker689"/>model had trouble with. We then only propagate those <em class="italic">k</em> loss values during <span class="No-Break">the backpropagation.</span></p>
			<p>As <a id="_idIndexMarker690"/>we did earlier in <a href="B17259_08.xhtml#_idTextAnchor235"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Algorithm-Level Deep Learning Techniques</em>, we will continue using the long-tailed version of the MNIST dataset (<span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B17259_09_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 â€“ An imbalanced MNIST dataset showing the counts of each class</p>
			<p>In <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.11</em>, we show the performance of OHEM loss when compared with cross-entropy loss after <span class="No-Break">20 epochs.</span></p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/B17259_09_11.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 â€“ A performance comparison of online hard example mining when compared with cross-entropy loss</p>
			<p>Itâ€™s <a id="_idIndexMarker691"/>evident that the most significant <a id="_idIndexMarker692"/>improvements are observed for the classes with the highest level of imbalance. Though some research works [4] have tried to apply OHEM to general problems without much success, we think this is a good technique to be aware of <span class="No-Break">in general.</span></p>
			<p>In the following section, we will introduce our final topic of minority class <span class="No-Break">incremental rectification.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor271"/>Minority class incremental rectification</h1>
			<p>Minority class incremental rectification<a id="_idIndexMarker693"/> is a deep learning technique that boosts the representation of minority classes in imbalanced datasets using a <strong class="bold">Class Rectification Loss</strong> (<strong class="bold">CRL</strong>). This<a id="_idIndexMarker694"/> strategy dynamically adjusts to class imbalance, enhancing model performance by incorporating hard example mining and <span class="No-Break">other methods.</span></p>
			<p>This technique is based <a id="_idIndexMarker695"/>on the paper by Dong et al. [5][6]. Here are the main steps of <span class="No-Break">the technique:</span></p>
			<ol>
				<li><strong class="bold">Class identification in </strong><span class="No-Break"><strong class="bold">each batch</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Binary classification</strong>: We <a id="_idIndexMarker696"/>consider a class as a minority if it makes up less than 50% of the batch. The rest is the <span class="No-Break">majority class.</span></li><li><strong class="bold">Multi-class classification</strong>: We<a id="_idIndexMarker697"/> define all minority classes as those that collectively account for no more than 50% of the batch. The remaining classes are<a id="_idIndexMarker698"/> treated as <span class="No-Break">majority classes.</span></li></ul></li>
				<li><strong class="bold">Compute the class </strong><span class="No-Break"><strong class="bold">rectification loss</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Locate </strong><span class="No-Break"><strong class="bold">challenging samples</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Find hard positives</strong>: We <a id="_idIndexMarker699"/>identify samples from the minority <a id="_idIndexMarker700"/>class that our model incorrectly assesses with low <span class="No-Break">prediction scores.</span></li><li><strong class="bold">Find hard negatives</strong>: We locate samples from other (majority) classes that our model mistakenly assigns high prediction scores for the <span class="No-Break">minority class.</span></li></ul></li><li><span class="No-Break"><strong class="bold">Construct triplets</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Use minority samples as anchors</strong>: We use each sample from the minority <a id="_idIndexMarker701"/>class as an anchor for <span class="No-Break">triplet formation.</span></li><li><strong class="bold">Create triplets</strong>: We form triplets using an anchor sample, a hard positive, and a <span class="No-Break">hard negative.</span></li></ul></li><li><strong class="bold">Calculate distances within triplets</strong>: We define the distance (<span class="_-----MathTools-_Math_Variable">d</span>) between <a id="_idIndexMarker702"/>matched (anchor and hard positive) and unmatched (anchor and hard negative) pairs <span class="No-Break">as follows:</span></li></ul><p class="list-inset"><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">anchor</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">hard</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">positive</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">âˆ£</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Prediction</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">score</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">of</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">anchor</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">âˆ’</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Prediction</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">score</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">of</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">hard</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">positive</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator_Extended">âˆ£</span></span></p><p class="list-inset"><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">anchor</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">hard</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">negative</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Prediction</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">score</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">of</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">anchor</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">âˆ’</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Prediction</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">score</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">of</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">hard</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">negative</span></span></p><ul><li><strong class="bold">Impose margin ranking</strong>: We ensure<a id="_idIndexMarker703"/> the distance from the anchor to the hard negative is greater than the distance from the anchor to the hard positive, increased by <span class="No-Break">a margin.</span></li></ul></li>
				<li><strong class="bold">Formulate final </strong><span class="No-Break"><strong class="bold">loss function</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Class imbalance rectification</strong>: We <a id="_idIndexMarker704"/>modify the standard cross-entropy loss to <a id="_idIndexMarker705"/>address the class imbalance by introducing a <span class="No-Break"><strong class="bold">CRL</strong></span><span class="No-Break"> term.</span></li><li><strong class="bold">Custom loss calculation</strong>: We<a id="_idIndexMarker706"/> use the formed triplets to compute an average sum of the <span class="No-Break">defined distances.</span></li><li><span class="No-Break"><strong class="bold">Loss equation</strong></span><span class="No-Break">:</span></li></ul></li>
			</ol>
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">Â </span><span class="_-----MathTools-_Math_Variable">final</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Î±</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">Ã—</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">Â </span><span class="_-----MathTools-_Math_Variable">CRL</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">âˆ’</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Î±</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">Ã—</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">L</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">Â </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">CE</span></span></p>
			<p class="list-inset">Here, <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">Â </span><span class="_-----MathTools-_Math_Variable">CRL</span> is the CRL loss, <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">Â </span><span class="_-----MathTools-_Math_Variable">CE</span> is the cross-entropy loss, and <span class="_-----MathTools-_Math_Variable">Î±</span> is a hyperparameter <a id="_idIndexMarker707"/>dependent upon the amount <a id="_idIndexMarker708"/>of class imbalance in <span class="No-Break">the dataset.</span></p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B17259_09_12.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 â€“ A comic illustrating the usage of triplet loss in class rectification loss</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor272"/>Utilizing the hard sample mining technique in minority class incremental rectification</h2>
			<p>The <a id="_idIndexMarker709"/>minority<a id="_idIndexMarker710"/> class incremental rectification technique uses the hard negative technique but with <span class="No-Break">two customizations:</span></p>
			<ul>
				<li>It uses only minority classes for <span class="No-Break">hard mining</span></li>
				<li>It uses both hard positives and hard negatives for loss computation (triplet <span class="No-Break">margin loss)</span></li>
			</ul>
			<p>The key highlight of the minority class incremental rectification technique in handling highly imbalanced datasets is that it uses the triplet margin loss on the minority class of the batch that it operates upon. This makes sure that the model incrementally optimizes the triplet loss for the minor<a id="_idTextAnchor273"/>ity class in <span class="No-Break">every batch.</span></p>
			<p>Our results on imbalanced MNIST data by using <strong class="source-inline">ClassRectificationLoss</strong> were relatively mediocre compared to the baseline model that employed cross-entropy loss. This performance difference could be due to the techniqueâ€™s suitability for very large-scale training data, as opposed to a much smaller dataset such as MNIST, which we used here. Please find the complete notebook in the <span class="No-Break">GitHub repo.</span></p>
			<p>Itâ€™s worth <a id="_idIndexMarker711"/>noting that the original authors of the paper applied this method to the CelebA face attribute dataset, which is extensive and multi-labe<a id="_idTextAnchor274"/>l as well as multi-class. <em class="italic">Table 9.4</em> presents the results from the paper, where they used a five-layer CNN as a baseline and compared CRL with oversampling, undersampling, and <span class="No-Break">cost-sensitive techniques.</span></p>
			<table id="table004-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Attributes (</strong><span class="No-Break"><strong class="bold">imbalance ratio)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Baseline (</strong><span class="No-Break"><strong class="bold">five-layer CNN)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Over-sampling</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Under-sampling</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Cost-sensitive</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">CRL</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Bald (1:43)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">93</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">92</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">79</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">93</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">99</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Mustache (1:24)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">88</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">60</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">88</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">93</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Gray <span class="No-Break">hair (1:23)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">88</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">96</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Pale <span class="No-Break">skin (1:22)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">81</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">82</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">80</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">92</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Double <span class="No-Break">chin (1:20)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">83</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">84</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">80</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">84</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">89</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.4 â€“ A performance comparison of CRL on facial attribute recognition on the CelebA benchmark, using class-balanced accuracy (in %) (adapted from Dong et al. [6])</p>
			<p>As is <a id="_idIndexMarker712"/>evident from the table, the CRL technique consistently outperforms other methods across various facial attributes, even in high imbalance scenarios. Specifically, for the <strong class="bold">Bald</strong> attribute, with a 1:43 imbalance ratio, CRL achieved a remarkable 99% accuracy. Its effectiveness is also evident in attributes such as <strong class="bold">Mustache</strong> and <strong class="bold">Gray hair</strong>, where it surpassed the baseline by 5% and 6%, respectively. This demonstrates CRLâ€™s superior <a id="_idIndexMarker713"/>ability to address <span class="No-Break">class imbalances.</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B17259_09_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 â€“ A visual representation of CRL regularization in rectifying model biases from class-imbalanced training data</p>
			<p>Overall, the <strong class="source-inline">ClassRectificationLoss</strong> class provides a custom loss function that combines triplet loss and negative log-likelihood loss while also considering class imbalance in the dataset. This can be a useful tool to train models on imbalanced datasets where the minority class samples are of <span class="No-Break">particular interest.</span></p>
			<p>This <a id="_idIndexMarker714"/>chapter explored a few modern deep learning strategies to handle imbalanced data, including graph ML, hard example mining, and minority class incremental rectification. By blending data-level and algorithm-level techniques, and sometimes even transitioning a problem paradigm from tabular to graph-based data representation, we <a id="_idIndexMarker715"/>can effectively leverage challenging examples, improve the representation of <a id="_idIndexMarker716"/>less common classes, and advance our ability to manage <span class="No-Break">data imbalance.</span></p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor275"/>Summary</h1>
			<p>In this chapter, we were introduced to graph ML and saw how it can be useful for certain imbalanced datasets. We trained and compared the performance of the GCN model with baselines of XGBoost and MLP on the Facebook page-page dataset. For certain datasets (including tabular ones), where we are able to leverage the rich and interconnected structure of the graph data, the graph ML models can beat even XGBoost models. As we continue to encounter increasingly complex and interconnected data, the importance and relevance of graph ML models will only continue to grow. Understanding and utilizing these algorithms can be invaluable in <span class="No-Break">your arsenal.</span></p>
			<p>We then went over a hard mining technique, where the â€œhardâ€ examples with the lowest loss values are first identified. Then, the loss for only <em class="italic">k</em> such examples is backpropagated in order to force a model to focus on the minority class examples, which the model has the most trouble learning about. Finally, we deep-dived into another hybrid deep learning technique called minority class incremental rectification. This method employs triplet loss on examples that are mined using the online hard example mining technique. Because the minority class incremental rectification method combines hard sample mining from minority groups with a regularized objective function, known as CRL, it is considered a hybrid approach that combines both data-level and algorithm-level deep <span class="No-Break">learning techniques.</span></p>
			<p>We hope this chapter equipped you with the confidence to extract key insights from new techniques and understand their main ideas, taken directly from <span class="No-Break">research papers.</span></p>
			<p>In the following chapter, we will talk about model calibration, its importance, and some of the popular techniques to calibrate <span class="No-Break">ML model<a id="_idTextAnchor276"/>s.</span></p>
			<h1 id="_idParaDest-194">Questi<a id="_idTextAnchor277"/>ons</h1>
			<ol>
				<li>Apply triplet loss to the imbalanced MNIST dataset, and see whether the modelâ€™s performance is better than using the cross-entropy <span class="No-Break">loss function.</span></li>
				<li>Apply minority class incremental rectification technique to the imbalanced datasets â€“ CIFAR10-LT and CIFAR100-LT. For a reference implementation of this technique on the MNIST-LT dataset, you can refer to the accompanying <span class="No-Break">GitHub notebook.</span></li>
			</ol>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor278"/>References</h1>
			<ol>
				<li value="1"><em class="italic">Fraud Detection: Using Relational Graph Learning to Detect Collusion (</em><span class="No-Break"><em class="italic">2021)</em></span><span class="No-Break">: </span><a href="https://www.uber.com/blog/fraud-detection/"><span class="No-Break">https://www.uber.com/blog/fraud-detection/</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Graph for fraud detection (</em><span class="No-Break"><em class="italic">2022)</em></span><span class="No-Break">: </span><a href="https://engineering.grab.com/graph-for-fraud-detection"><span class="No-Break">https://engineering.grab.com/graph-for-fraud-detection</span></a><span class="No-Break">.</span></li>
				<li>A. Shrivastava, A. Gupta, and R. Girshick, <em class="italic">â€œTraining Region-Based Object Detectors with Online Hard Example Mining,â€</em> in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, Jun. 2016, pp. 761â€“769: <span class="No-Break">doi: 10.1109/CVPR.2016.89</span><span class="No-Break">.</span></li>
				<li>Marius Schmidt-Mengin, ThÃ©odore Soulier, Mariem Hamzaoui, Arya Yazdan-Panah, Benedetta Bod-ini, et al. <em class="italic">â€œOnline hard example mining vs. fixed oversampling strategy for segmentation of new multiple sclerosis lesions from longitudinal FLAIR MRIâ€</em>. Frontiers in Neuroscience, 2022, 16, pp.100405. <span class="No-Break">10.3389/fnins.2022.1004050. hal-03836922.</span></li>
				<li>Q. Dong, S. Gong, and X. Zhu, <em class="italic">â€œClass Rectification Hard Mining for Imbalanced Deep Learning,â€</em> in 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Oct. 2017, pp. 1869â€“1878. <span class="No-Break">doi: </span><span class="No-Break">10.1109/ICCV.2017.205</span><span class="No-Break">.</span></li>
				<li>Q. Dong, S. Gong, and X. Zhu, <em class="italic">â€œImbalanced Deep Learning by Minority Class Incremental Rectification.â€</em> arXiv, Apr. 28, 2018. Accessed: Jul. 26, 2022. [Online]. Available: <a href="http://arxiv.org/abs/1804.10851"><span class="No-Break">http://arxiv.org/abs/1804.10851</span></a><span class="No-Break">.</span></li>
			</ol>
		</div>
	</body></html>