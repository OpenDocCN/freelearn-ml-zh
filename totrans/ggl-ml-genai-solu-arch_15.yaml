- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML Governance and the Google Cloud Architecture Framework
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As technologists, we often, of course, find the technological aspects of **machine
    learning** (**ML**) to be the most fun and exciting parts, while legal and regulatory
    concepts don’t always inspire us quite as much. However, these concepts are required
    to build robust solutions at scale in production. They are what help us make the
    transition from hobbyist activities to designing reliable systems that can be
    vital to a company’s success or even affect millions of people’s lives.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, this chapter will cover the following subjects:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ML governance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of the Google Cloud Architecture Framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture Framework concepts about AI/ML workloads on Google Cloud
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go ahead and dive right into our first topic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: ML governance
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML governance refers to everything that’s required to manage ML models within
    an organization, throughout the entire model development life cycle. As models
    can play a significant role in critical decision-making processes, it’s important
    to ensure that they are transparent, reliable, fair, and secure, and we need to
    implement structured frameworks to achieve these goals. These frameworks include
    policies and best practices that ensure responsible and ethical use of data and
    ML technologies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: When discussing ML governance in this chapter, I will also include data governance
    in the scope of the discussion, because the use of data is so inherent in the
    ML life cycle. Let’s start there.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Data governance
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When it comes to managing data in the ML life cycle, there are a number of aspects
    that we need to consider, such as data quality, lineage, privacy, security, and
    retention. Let’s take a look at each of these in more detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Data security, privacy, and access control
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Of all the aspects of a data governance strategy, data security is arguably
    the most important. Data security incidents tend to make headline news, and those
    are not the kinds of news headlines you want to be responsible for!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental to data security is the concept of data access control, which, as
    the name suggests, focuses on who can access the data and how they can access
    the data. The worst-case scenario is that somebody from outside the company gains
    access to sensitive data and leaks it publicly or uses it for nefarious purposes
    such as ransom or sabotage.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to data security, my favorite term is **defense-in-depth** (**DiD**),
    which alludes to the fact that a thorough data security strategy consists of using
    many different tools to protect the data and other resources. In the following
    sub-sections, I will outline steps we can take to secure our data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Data categorization
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Before defining security policies for your data, it’s important to establish
    a categorization system in order to understand which elements of data need more
    focus in terms of protection. For instance, data that is published openly on your
    website, such as product descriptions and prices, is generally not considered
    to be top secret, whereas your customers’ credit card details are highly sensitive.
    You can categorize your data in terms of tiers, such as the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Tier 0**: Highly-sensitive, such as customer passwords and credit card details'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 1**: Sensitive, such as customer purchase or transaction history'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 2**: Somewhat sensitive, such as customer addresses and phone numbers'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tier 3**: Non-sensitive, such as publicly viewable information'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just examples; you would need to work with your organization’s information
    security experts to determine what categories would make the most sense for your
    organization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: After categorizing our data, let’s discuss how we can secure it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Network security
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can look at the DiD approach as layers in an onion. At the outermost layer,
    we begin with network security. If unintended users of the data do not have a
    network path to the data, then that’s a really solid foundation in your security
    strategy. Network security practices include setting up devices such as firewalls
    to control what kinds of traffic are allowed to enter a protected network. Google
    Cloud provides such devices, as well as other network security constructs, such
    as **Virtual Private Cloud** (**VPC**), which allows you to set up your own private
    networks and control how to access them, and **VPC Service Controls** (**VPC-SC**),
    which enables you to create a security perimeter around your resources to prevent
    data exfiltration.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Authentication and authorization
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The next layer in the onion is authentication and authorization to grant or
    deny permissions to access resources. Even if somebody gets access to a protected
    network, the next step is to determine which resources they are allowed to access
    and what actions they are allowed to perform on those resources. You need to ensure
    that the data can be accessed only by people and systems that are authorized to
    do so, and authorization should be based on business criticality. In other words,
    a person or system should only be able to access a piece of data if they need
    such access to perform a required business function. At all times, you should
    be able to easily determine who (or what) has access to which data, and why.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud **Identity and Access Management** (**IAM**) can be used to configure
    and enforce such permissions, or, for software components, additional mechanisms
    such as **Transport Layer Security** (**TLS**) authentication can also be used.
    A little later in this section, we will also cover data cataloging with Google
    Cloud Dataplex. Dataplex and Google Cloud BigLake can be used to make it easier
    for companies to manage and enforce permissions for accessing their data resources.
    Google Cloud BigQuery offers additional data security mechanisms such as row-level
    and column-level access control, meaning that not only can you grant or restrict
    access to tables within BigQuery, but you can more granularly grant or restrict
    access to specific rows and/or columns within those tables. This provides additional
    flexibility to protect resources from unintended access. For example, with column-level
    security, you could configure that only people in the finance department can view
    columns that contain customers’ credit card details, while other employees and
    systems cannot. With row-level security, you could configure that sales representatives
    can only view details for customers in their region and not in other regions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Data encryption
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the innermost layer are the data resources themselves. Best practices suggest
    that data should be encrypted as a further security measure. In that case, even
    if a malicious or unintended user gets access to the data, they would need the
    encryption keys to unencrypt the data. It goes without saying that encryption
    keys should be stored in a highly secure manner in a separate system, with all
    of the layers of security implemented to protect them. Again, Google Cloud provides
    tools to implement all of those layers of security mechanisms, including encryption
    and key management using Google Cloud Key Management.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Logging and auditing
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In addition to all of those mechanisms, a strong data security strategy should
    incorporate auditing and logging implementations to monitor data access and modifications
    and support audits and forensic investigations to detect or investigate policy
    violations and data breaches. Google Cloud Logging and Audit Logs can be used
    for those purposes. *Figure 13**.1* shows what kinds of logs can be tracked by
    Google Cloud Audit Logs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1: Audit log types in Google Cloud Audit Logs](img/B18143_13_1.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Audit log types in Google Cloud Audit Logs'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 13**.1*, there are three types of logs that we can
    capture using Google Cloud Audit Logs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While intrinsically related to data security, data privacy more specifically
    focuses on the lawful, ethical, and safe handling of **personal information**
    (**PI**). It is one of the most important aspects of data security because privacy
    violations can severely damage a company’s reputation and customer trust.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Not only that, but they can incur serious legal ramifications. There are numerous
    international standards and regulations that govern data privacy and protection,
    such as the **General Data Protection Regulation** (**GDPR**) in the **European
    Union** (**EU**), the **California Consumer Privacy Act** (**CCPA**) in the US,
    and many others globally. These regulations outline rules about data handling,
    such as collection, storage, processing, and sharing. Navigating these regulations
    and ensuring compliance in how your systems handle data can be quite challenging.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，他们还可能招致严重的法律后果。有许多国际标准和法规管理数据隐私和保护，例如欧盟的**通用数据保护条例**（**GDPR**），美国的**加州消费者隐私法案**（**CCPA**），以及全球许多其他法规。这些法规概述了数据处理规则，如收集、存储、处理和共享。遵守这些法规并确保系统处理数据的合规性可能相当具有挑战性。
- en: It can also be a challenge to keep track of sensitive data that may be dispersed
    throughout your datasets. Imagine a company that has petabytes of data that comes
    from many different sources, such as credit card readers in stores or customer-facing
    online forms. It’s easy to identify that data being transmitted from a credit
    card machine needs to be protected, but this may not be as obvious for other data
    interfaces. For example, perhaps customers are accidentally entering their credit
    card details into online form fields that are not intended for that purpose (for
    example, if a customer accidentally pastes their credit card details into a field
    that was not intended for entering credit card details). Such fields may not be
    considered sensitive and therefore do not get any special attention from a security
    perspective. The Google Cloud Sensitive Data Protection service, which now incorporates
    the Google Cloud **Data Loss Prevention** (**DLP**) tool, can help to identify
    and protect sensitive information in your datasets. If sensitive data is found,
    you can use the Sensitive Data Protection service to implement protection mechanisms
    such as de-identification, masking, tokenization, and redaction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪散布在数据集各个部分中的敏感数据也可能是一项挑战。想象一家拥有来自许多不同来源的PB级数据的公司，例如商店中的信用卡读取器或面向客户的在线表单。很容易识别从信用卡机传输的数据需要受到保护，但对于其他数据接口来说，这可能并不那么明显。例如，也许客户不小心将信用卡详细信息输入到非此目的的在线表单字段中（例如，如果客户不小心将信用卡详细信息粘贴到未打算输入信用卡详细信息的字段中）。这些字段可能不被视为敏感，因此从安全角度没有得到任何特殊关注。现在集成了谷歌云**数据丢失预防**（**DLP**）工具的谷歌云**敏感数据保护**服务可以帮助识别和保护数据集中的敏感信息。如果发现敏感数据，可以使用敏感数据保护服务实施保护机制，如去标识化、掩码、令牌化和编辑。
- en: 'In the context of data security, privacy, and access control, it’s important
    to highlight the concept of shared responsibilities and shared fate on Google
    Cloud. Rather than risking misstating legal terminology here, I instead recommend
    reading Google Cloud’s official policy on this topic, which can be found at the
    following URL:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据安全、隐私和访问控制方面，强调在谷歌云平台上的共同责任和共同命运的概念非常重要。为了避免在这里错误地表述法律术语，我建议阅读谷歌云关于此主题的官方政策，该政策可以在以下网址找到：
- en: '[https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate](https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate](https://cloud.google.com/architecture/framework/security/shared-responsibility-shared-fate)'
- en: Before we move on to discuss ML model governance, we will round out this section
    with a discussion of data quality, cataloging, and lineage.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论机器学习模型治理之前，我们将通过讨论数据质量、编目和血缘关系来结束本节。
- en: Data quality, cataloging, and lineage
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据质量、编目和血缘关系
- en: Considering that data can be such a valuable resource for a company, it’s important
    to establish practices that ensure the accuracy, consistency, discoverability,
    and (depending on the use case) timeliness of data. For example, considering that
    data is often used to make important business decisions, inaccurate or out-of-date
    data could have negative impacts on a company’s business. This also applies to
    business decisions that are automated by ML models. If we feed inaccurate data
    into an ML model, we will get inaccurate predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据对公司来说可能是一项宝贵的资源，因此建立确保数据准确性、一致性、可发现性以及（根据用例）及时性的实践至关重要。例如，考虑到数据通常用于做出重要的商业决策，不准确或过时的数据可能会对公司业务产生负面影响。这也适用于由机器学习模型自动化的商业决策。如果我们向机器学习模型输入不准确的数据，我们将得到不准确的预测。
- en: An effective data governance strategy starts with clear policies, responsibilities,
    and standards for data quality. We then need to establish frameworks to measure
    and monitor data quality factors. Best practices in this regard include regular
    data cleaning processes for correcting data errors, de-duplication, and filling
    in missing values, as well as automation mechanisms such as regular, automated
    data quality checks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有效的数据治理策略始于明确的数据政策、责任和标准，以确保数据质量。然后我们需要建立框架来衡量和监控数据质量因素。在此方面，最佳实践包括定期的数据清理过程，用于纠正数据错误、去重和填充缺失值，以及自动化机制，如定期的自动数据质量检查。
- en: Data discoverability is also an important factor. After all, even if you have
    created pristinely curated datasets, they are not very useful if nobody knows
    they exist. When companies have well-established data management practices, it’s
    easier for data producers and consumers to know exactly where their data is and
    the current status of that data, at any given time. In such companies, a robust
    data catalog forms the heart of the company’s data infrastructure. I’ve worked
    with many clients over the years in various consulting roles, and you’d be surprised
    how many companies operate without well-defined data management strategies. Given
    that data can be the life-blood of an organization, it’s surprising to learn that
    many companies are not fully aware of exactly what data they own. Various systems
    throughout the company are producing and gathering data all day every day, but
    if that data is not being cataloged in some way, it may simply sit in silos in
    remote, disparate parts of the company, unavailable and unknown to most of the
    rest of the organization. Bear in mind that data can be used for all kinds of
    interesting business use cases. If you don’t know what data you have access to,
    you may be missing out on significant business opportunities.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的可发现性也是一个重要因素。毕竟，即使你创建了精心策划的数据集，如果没有人知道它们的存在，它们也并不很有用。当公司建立了良好的数据管理实践时，数据生产者和消费者更容易确切地知道他们的数据在哪里以及数据的当前状态，在任何给定时间。在这样的公司中，一个强大的数据目录构成了公司数据基础设施的核心。多年来，我在各种咨询角色中与许多客户合作过，你会惊讶地发现有多少公司没有明确的数据管理策略。鉴于数据可能是组织的生命线，了解到许多公司并不完全清楚他们拥有哪些数据，这令人惊讶。公司内各种系统全天候都在产生和收集数据，但如果这些数据没有以某种方式编目，它们可能只是简单地坐在公司偏远、分散的部分的孤岛中，对大多数组织来说既不可用也不为人知。请记住，数据可以用于各种有趣的企业用例。如果你不知道你有什么数据可以访问，你可能会错失重大的商业机会。
- en: It’s also important to implement data lineage tracking to understand where data
    comes from and how it gets transformed as it moves through various processing
    systems within a company. If I find a piece of data somewhere in my company, I
    want to know how it got there and every step it took along the way. Which systems
    did it pass through? What did those systems do to the data? Are there intermediate
    datasets that were created by those other systems in various parts of the company?
    This is not only important from a business operations perspective but can be required
    for compliance purposes. For example, if you need to comply with data sovereignty
    regulations, you better know where your data is at all times. If a customer decides
    to exercise their right to be forgotten in markets that are subject to GDPR –
    or other relevant regulations – you will have a really hard time complying if
    you do not have a good handle on your data. Similarly, if a data breach occurs,
    data lineage can help identify what data was compromised and understand the potential
    impacts and recovery steps needed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Google Cloud provides numerous tools to help with each of the aforementioned
    activities, such as Dataproc and Dataflow for cleaning and processing data, and
    Dataplex for cataloging, data quality, and data lineage tracking.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss ML model governance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: ML model governance
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the aspects required to ensure that the models we
    build and deploy are reliable, scalable, and secure and that they continue to
    meet those requirements on an ongoing basis. There are a number of factors that
    we need to incorporate in order to achieve this goal, which we discuss in the
    following sub-sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Model documentation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting with every developer’s favorite topic: documentation! While documentation
    is not always the most fun part of a developer’s job, it is essential to building
    and maintaining production-grade systems. I’ve worked with clients and teams in
    various companies that have not always done a great job of developing accurate,
    high-quality documentation, and one thing that you can almost guarantee in the
    lack of such documentation is that it will make your job a lot more difficult
    when you need to maintain and improve your systems over time. Imagine that you
    join a new team and you are tasked with improving the performance of a particular
    application that uses ML to perform medical diagnoses, and you find that the original
    application and underlying model were developed years ago by people who have left
    the company and did not document how they implemented the system. This is not
    a good place to be in, and you would be surprised how commonly these kinds of
    scenarios exist in the industry. Perhaps most importantly in the context of this
    chapter, model documentation can be essential—and sometimes even legally required—for
    compliance purposes.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: So, what does high-quality model documentation look like? Generally, our documentation
    should keep detailed records of factors such as model design, data inputs, transformations,
    algorithms, and hyperparameters. Let’s take a look at each of these in more detail.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Model design
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Documentation regarding model design should clearly define the purpose of the
    model, such as the objectives the model intends to achieve and the context within
    which it needs to operate. This includes potential use cases and intended users
    or systems that will interact with the model. We also need to provide a detailed
    description of the model’s architecture, such as its layers, structures, and interdependencies
    among different components of the model. Additionally, we should include details
    regarding the model’s design rationale, such as the reasoning behind choosing
    this particular model architecture or design, including comparisons with other
    potential designs that were considered and an explanation of why they were not
    chosen.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Data inputs
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our model documentation should describe the data collection process we used,
    including sources, methods of collection, and the timeframe during which data
    was collected. We should list all the features used by the model, including their
    definitions, types (for example, categorical, and numerical), any assumptions
    made about the data, and explanations as to why each feature is relevant to the
    model’s predictions. Additionally, we need to document any known issues in terms
    of data quality, including missing values, outliers, or inconsistencies, and how
    these issues were handled or mitigated.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another best practice is to detail the steps taken to clean and preprocess
    the data, such as handling missing data, normalization, encoding techniques, and
    feature engineering, including explanations of any methods used for feature selection
    or reduction (such as **principal component analysis** (**PCA**), as depicted
    in *Figure 13**.2*), and the rationale for their use:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2: PCA](img/B18143_13_2.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: PCA'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our documentation, we should discuss why a particular ML algorithm was chosen,
    including a comparison with other algorithms that were considered and a rationale
    explaining the algorithm’s suitability for the problem at hand, citing relevant
    literature or empirical evidence, as appropriate. It’s also important to detail
    the configuration of the algorithm, including any customizations specific to the
    project.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We should include details on all hyperparameters that the model uses, providing
    a definition and range for each, as well as the process used for hyperparameter
    tuning, such as grid search, random search, or Bayesian optimization. Additionally,
    we should include the final values chosen for each hyperparameter and provide
    a rationale for why these particular values were chosen, supported by the tuning
    process’s outcomes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Additional factors
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our documentation should also explain how the model’s performance was evaluated,
    including the metrics used (**mean squared error** (**MSE**), **area under the
    ROC curve** (**ROC-AUC**)) and the results of these evaluations. We should document
    any known limitations of the model, document potential biases in the model’s predictions,
    and describe how these biases could impact different demographic groups or individuals.
    We also need to detail any regulatory standards or ethical guidelines relevant
    to the model and discuss how compliance has been ensured. Finally, we need to
    outline the plan for ongoing monitoring of the model’s performance and behavior
    in a production environment, including strategies for handling model drift, anomalies,
    or performance degradation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文档还应解释如何评估模型的表现，包括使用的指标（**均方误差**（**MSE**），**ROC曲线下面积**（**ROC-AUC**））以及这些评估的结果。我们应该记录模型已知的任何局限性，记录模型预测中的潜在偏差，并描述这些偏差如何影响不同的群体或个人。我们还需要详细说明与模型相关的任何监管标准或伦理指南，并讨论如何确保合规性。最后，我们需要概述在生产环境中持续监控模型表现和行为的计划，包括处理模型漂移、异常或性能下降的策略。
- en: Model versioning
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型版本控制
- en: 'Just as with code versioning in traditional software development, model versioning
    in ML projects is essential for ensuring that teams can trace back through the
    evolution of models, replicate results, roll back to previous versions when necessary,
    and maintain a record of all changes made throughout a model’s life cycle. This
    becomes especially important in large or collaborative environments where multiple
    iterations of models may be developed over time by many different people and teams.
    It’s also important for debugging, continuous improvement, and audit and compliance
    purposes. *Figure 13**.3* shows an example of model version metadata in the Vertex
    AI Model Registry:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在传统软件开发中的代码版本控制一样，在机器学习项目中进行模型版本控制对于确保团队能够追踪模型的演变过程、复制结果、在必要时回滚到先前版本，以及维护整个模型生命周期中所有变更的记录至关重要。这在大型或协作环境中尤为重要，因为在这样的环境中，随着时间的推移，可能由许多不同的人或团队开发出多个模型的多个迭代。对于调试、持续改进、审计和合规性目的来说，这也同样重要。*图13.3*展示了Vertex
    AI模型注册表中模型版本元数据的一个示例：
- en: '![Figure 13.3: Model version metadata in Vertex AI Model Registry](img/B18143_13_3.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图13.3：Vertex AI模型注册表中的模型版本元数据](img/B18143_13_3.jpg)'
- en: 'Figure 13.3: Model version metadata in Vertex AI Model Registry'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：Vertex AI模型注册表中的模型版本元数据
- en: When I speak of model versioning, I’m referring to more than just the model
    artifacts, such as the actual model files, weights, and architecture. We should
    implement version tracking for every relevant item that is used in the model development
    process. This includes any code associated with the model, be it for preprocessing,
    training, evaluation, or deployment, as well as datasets, whether they consist
    of raw data, preprocessed data, or feature-engineered data. Even hyperparameter
    values and performance evaluation metrics should be versioned so that we can easily
    understand which versions of these elements pertain to which versions of our models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当我提到模型版本控制时，我指的是不仅仅是模型工件，例如实际模型文件、权重和架构。我们应该为模型开发过程中使用的每个相关项目实施版本跟踪。这包括与模型相关的任何代码，无论是预处理、训练、评估还是部署，以及数据集，无论它们是原始数据、预处理数据还是特征工程数据。甚至超参数值和性能评估指标也应进行版本控制，这样我们就可以轻松理解哪些版本的这些元素与我们的模型版本相对应。
- en: A well-implemented model versioning tool, such as Vertex AI Model Registry,
    will also enable us to add custom metadata to more comprehensively track our various
    model versions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实施良好的模型版本控制工具，如Vertex AI模型注册表，也将使我们能够添加自定义元数据，以更全面地跟踪我们的各种模型版本。
- en: Model monitoring
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型监控
- en: As we discussed in previous chapters, we need to continuously monitor models
    after we have deployed them to production. The intention is to identify any drift,
    anomalies, or degradation in model performance, including important markers such
    as fairness and explainability metrics. Again, this is not only important for
    **business continuity** (**BC**) purposes but also for compliance reasons. We
    may have certified that our model is compliant with specific regulations before
    and during deployment, but it could drift and lapse out of compliance if not regularly
    monitored on an ongoing basis. If we have set up MLOps pipelines to automate the
    process of continuously improving our models over time, then this monitoring should
    extend to all aspects of our pipeline, such as ensuring that we perform data quality
    monitoring during the data preprocessing steps, in addition to new model validation
    and other important steps in the process.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: As with everything else regarding our model development and management implementations,
    we want our monitoring processes to be automated as much as possible. We should
    set thresholds beyond which some kind of corrective action is either automatically
    initiated or a human is notified if there is a problem. For example, if we see
    that performance or fairness metric values have changed by more than a specified
    amount, then corrective action is initiated, either by automatically training
    and evaluating a new model version on updated data or by paging on-call support
    engineers to intervene.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: As we also discussed in previous chapters, Vertex AI provides built-in tools
    for monitoring model performance both after deployment and throughout each relevant
    step in the model development process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Auditing and compliance
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many industries have strict regulations (for example, GDPR, HIPAA, and various
    financial regulations) that require certain standards for factors such as data
    privacy, bias, transparency, and many more. Non-compliance with these regulations
    can lead to legal penalties and loss of customer trust. If we have workloads that
    are subject to regulatory standards, then we will need to establish auditing processes
    to help ensure our workloads remain compliant on an ongoing basis.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: How we implement such processes will mainly depend on the types of regulations
    with which we need to comply. Such processes could consist of a regular human-review
    process, or, as always, it would be best if we could automate auditing processes
    as much as possible and notify a human only when an issue that appears not to
    be automatically resolvable occurs. In the case of human-review processes, this
    is where documentation is inherently important because good-quality documentation
    can greatly simplify the review process and can make it easier to determine corrective
    actions when issues are identified. Ideally, we would want to identify potential
    risks in model performance, security, or reliability before they escalate into
    larger issues, and establishing regular review processes can help to ensure this
    happens.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: For some types of regulations, well-established audit checklists and **standard
    operating procedures** (**SOPs**) can be used, making the auditing task a bit
    easier. However, bear in mind that the regulatory landscape for ML is still evolving,
    and organizations must stay abreast of changes to remain compliant.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the concept of explainability. Explainability
    is particularly important in the context of regulatory compliance. If you can’t
    easily or adequately explain how a given model or system works, then you will
    have a difficult time ensuring regulatory compliance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the important factors of ML governance in quite
    a bit of detail, let’s zoom back out and focus on the bigger picture again. In
    the coming sections, we will discuss how to operationalize ML governance, what
    ML governance looks like in different industries, and how to stay abreast of the
    evolving ML governance landscape.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Operationalization of ML governance
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I’ve alluded to in each of the previous sections, we usually would want to
    automate as much of our ML governance practices and processes as possible, and
    there are tools and platforms that can assist in achieving this goal, such as
    data catalogs, model management tools, and auditing tools, which I describe in
    the following sub-sections.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Data catalogs
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We briefly talked about data cataloging earlier in this chapter. Data catalogs
    are a kind of metadata management tool that helps companies find and manage large
    amounts of data spread across their organization, whether on-premises or in the
    cloud. You can think of a data catalog as a massive inventory of a company’s data
    assets, designed to let users discover, organize, and understand their data sources.
    We’ve already introduced Google Cloud Dataplex, which Google describes as an “*intelligent
    data fabric that enables organizations to centrally discover, manage, monitor,
    and govern their data across data lakes, data warehouses, and data marts, with
    consistent controls*.” *Figure 13**.4* shows an example of a catalog created by
    Google Cloud Dataplex:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4: Dataplex catalog](img/B18143_13_4.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Dataplex catalog'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: This concept of consistent controls is particularly relevant in the context
    of governance. With Dataplex, you can use metadata to describe all of your company’s
    data assets, and you can manage permissions in a uniform way across different
    Google Cloud data storage and processing tools. This is, of course, important
    from a governance perspective, and Dataplex also provides data quality and data
    lineage functionality, which are also important in the context of governance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Model management platforms
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These platforms assist in the model’s entire life cycle, including development,
    deployment, monitoring, and maintenance. They are essential for activities such
    as model versioning, experiment tracking, and model performance monitoring. By
    providing a structured environment for managing ML models, these platforms help
    ensure that models are reliable and reproducible and that they meet performance
    expectations. They also facilitate compliance by providing detailed model development
    and deployment process records. Of course, Vertex AI is Google Cloud’s native
    model management ecosystem, providing all of the aforementioned features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve focused on aspects of ML governance that are common to many industries.
    In the next section, let’s take a look at how ML governance applies to specific
    industries.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: ML governance in different industries and locations
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What can make regulatory compliance even more complex is that different countries,
    states, and industries can all have varying regulations. ML governance therefore
    varies significantly across different industries and geographic locations. In
    this section, we’ll discuss governance factors for specific regions and sectors,
    such as healthcare and finance, as well as regional considerations.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I currently live in the US, and when I hear the terms “regulatory compliance”
    and “healthcare” in the same sentence, the first thing that pops into my mind
    is the **Health Insurance Portability and Accountability Act** (**HIPAA**), which
    grants patients certain rights over their health information, including secure
    handling and confidentiality, and outlines significant penalties for breaches
    and non-compliance. If you work in healthcare in the US, you will almost certainly
    need to be aware of, and work within, the requirements of HIPAA. When designing
    and implementing ML systems in this industry, you must ensure that the data handling
    practices throughout the entire model development life cycle comply with those
    requirements. Other countries have their own regulatory requirements that you
    must learn and understand if you operate in those areas.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Finance
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the finance industry, fraud is perhaps one of the biggest concerns or threats,
    and the finance industry is heavily regulated to avoid the potential of fraud
    occurring. If your company operates in the US, for example, then the financial
    operations of your company will need to abide by regulations such as the **Sarbanes-Oxley
    Act** (**SOX**), which is mainly intended to prevent corporate fraud and improve
    the reliability and accuracy of corporate disclosures to protect investors. If
    your systems handle credit card data in any way, then you will likely need to
    comply with **Payment Card Industry Data Security Standard** (**PCI DSS**) regulations,
    which are a set of security standards relating to the secure handling of credit
    card information.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Region-specific governance considerations
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different regions have different regulatory requirements. For example, if you
    operate in the EU and the **European Economic Area** (**EEA**), then you will
    be subject to GDPR requirements, which protect the PI of individuals in those
    regions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The state of California has the CCPA, which regulates how businesses worldwide
    are allowed to handle the PI of California residents.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: There are also regulations that govern how data related to children must be
    handled, such as the **Children’s Online Privacy Protection Act** (**COPPA**),
    which is intended to protect the privacy of children under 13 years of age.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: In addition to region-specific and industry-specific regulations, we must also
    ensure that we remain compliant with obligations regarding fairness, transparency,
    explainability, accountability, and ethics. And, not only do we need to manage
    the complexity of regulations in different regions and industries, but that complexity
    is further extended by the fact that regulations may change over time. Let’s discuss
    this topic in more detail next.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Keeping up with the evolving landscape of ML governance
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I can confidently say that the ML industry is one of the most quickly evolving
    industries at the moment. Companies, governments, and research institutions all
    over the world continue to invest heavily in this industry. As a result, compliance
    regulations related to this industry continue to evolve rapidly. In order to consistently
    remain successful and, quite frankly, ensure that you don’t get into trouble,
    your company needs to stay current in this ever-evolving landscape. In this section,
    I outline important concepts and best practices in this space.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Ongoing education and training
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Your company’s employees need ongoing education to stay current with advances
    in regulatory requirements. In addition to technical capabilities, your employees
    must understand ethical considerations and risk management strategies related
    to ML deployment. Well-informed and well-trained individuals are less likely to
    make costly errors such as violating data privacy standards. Also, since this
    is not a static field, new types of bias and ethical dilemmas emerge almost daily.
    I highly recommend implementing regular training sessions, workshops, and educational
    resources for employees to learn about the latest trends, tools, ethical considerations,
    and best practices regarding ML governance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Regularly updating governance policies and practices
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Along with the technologies and ethical considerations, the legal landscape
    for data protection and privacy also continually evolves. Organizations must regularly
    update their governance policies to comply with new and changing laws and standards
    (GDPR, CCPA, or industry-specific regulations), and as new technologies and methodologies
    develop, governance policies must adapt to accommodate and manage them appropriately.
    What worked for a simple linear regression model might not be sufficient for a
    complex **deep learning** (**DL**) system. Unfortunately, the threat landscape
    also continues to develop as new technologies emerge, and what was secure yesterday
    might not be quite as secure tomorrow. As a result, we need to regularly review
    and update security policies and practices to protect sensitive data and ML systems
    against new vulnerabilities and attack strategies.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in consideration of Google’s **Site Reliability Engineering**
    (**SRE**) practices, while we should do everything we can to avoid a negative
    event occurring, if such an event does occur, we need to learn from that scenario.
    As such, we should conduct thorough post-mortems on any issues and use these insights
    to improve policies and prevent future occurrences.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Another set of concepts that are quite closely linked to SRE are encapsulated
    in the Google Cloud Architecture Framework, which I will discuss in the next section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the Google Cloud Architecture Framework
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Google’s own words, “*the Google Cloud Architecture Framework provides recommendations
    and describes best practices to help architects, developers, administrators, and
    other cloud practitioners design and operate a cloud topology that’s secure, efficient,
    resilient, high-performing, and cost-effective*.” In this section, we discuss
    some of the key concepts from the framework and how they can be applied to AI/ML
    workloads on Google Cloud, especially within the context of ML governance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the framework documentation can be found at the following URL:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/architecture/framework](https://cloud.google.com/architecture/framework)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with an overview of the fundamental concepts of the framework, which
    are referred to as **pillars**. One way to think of it is that, by ensuring all
    of these pillars are implemented, we can build a solid and enduring structure
    (system).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The categories of the framework are the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: System design
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational excellence
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security, privacy, and compliance
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reliability
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost optimization
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sub-sections, I’ll describe what each category represents,
    starting with system design.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Pillar 1 – System design
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Interestingly, the *System design* category in the Google Cloud Architecture
    Framework is more akin to the foundation of the overall framework than a pillar,
    because a well-designed system appropriately incorporates all of the pillars.
    The *System design* category encapsulates four core principles:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Document everything
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simplify your design and use fully managed services
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decouple your architecture
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a stateless architecture
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at each of these in more detail.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Principle 1 – Document everything
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We touched on this principle quite a bit in this chapter’s *ML model governance*
    section. In a broader system design context, we’re not just referring to documentation
    related to our model, but rather related to every aspect of how our systems are
    implemented, including aspects such as architecture diagrams and maintenance runbooks.
    The question I always ask in this context is: if a new member joined our team
    and were required to quickly learn everything they need to know to improve and
    maintain a system we’ve built, what are all of the details they would need to
    review? If any of those details are not adequately documented, then that’s a gap
    that we need to address by developing the required documentation. This also helps
    other teams that we need to collaborate with or that need to interact with our
    system in some way, and it makes everybody’s job easier if regulatory compliance
    officers need to audit our systems.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Principle 2 – Simplify your design and use fully managed services
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ever since I heard of the concept of Occam’s razor, I have been a big fan of
    it. There are a number of different ways in which it can be summarized, but a
    fairly common one is: “If there are two possible explanations for a particular
    phenomenon, use the simpler one.”This can be extended to say: don’t make things
    more complicated than needed. The opposite of this is the concept of a Rube Goldberg
    machine, which applies an extremely complex set of mechanisms to achieve a simple
    goal. While Rube Goldberg machines can be fun to watch, they are generally quite
    impractical, and they are not what you want to implement in the design of your
    large-scale, low-latency, highly sensitive production system.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Keeping your system design as simple as possible has many benefits, such as
    making it easier to troubleshoot, maintain, and secure your systems. Highly complex
    systems with many different components are difficult to troubleshoot when something
    goes wrong. Similarly, in terms of security, highly complex systems often have
    a larger **attack surface**. We will cover that concept in more detail later in
    this chapter.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Another way in which we can make our jobs easier is by offloading responsibilities
    to a **cloud service provider** (**CSP**). One of the primary benefits of cloud
    computing is that CSPs offer platforms and systems that have been designed to
    address common needs in the industry. When a company runs all of its workloads
    in its own data centers, it either needs to build solutions and platforms completely
    by itself or install and manage software created by other companies. Both of those
    options incur a lot of staff overhead and require specific training to manage.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of building and maintaining a platform to enable data
    scientists to develop and deploy ML models. In previous chapters, we outlined
    the various steps that are required in the model development life cycle. If we
    are running all of our workloads “on-premises” (that is, not in the cloud), then
    we need to either design, build, and maintain a platform that supports all of
    those steps or, as many companies do, we could try to hack something together,
    using a hodge-podge of random third-party software solutions that all require
    specific training and don’t necessarily work very well together. In the cloud,
    however, we can simply use the platform provided by the cloud provider and let
    them do all of the hard work for us so that our teams can focus on their core
    competencies and primary objectives rather than building and maintaining infrastructure.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Similar concepts exist across other types of workloads. For example, on-premises,
    we may build and maintain our own Kubernetes environments for our company’s containerized
    workloads. We would spend a lot of time maintaining those systems, but in the
    cloud, we could use managed services such as **Google Kubernetes Engine** (**GKE**)
    or Cloud Run. In this context, we talk about services that offload more of the
    infrastructure management tasks to the cloud provider as “going further up the
    stack.” In the case of GKE and Cloud Run, Cloud Run would be seen as “further
    up the stack” because it provides more of a fully managed experience than the
    basic form of GKE, although the more recent launch of GKE Autopilot also provides
    a very hands-off approach, in which more of the platform management tasks are
    implemented by Google Cloud.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Principle 3 – Decouple your architecture
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This ties in with *principle 2* to an extent. It refers to breaking your overall
    system design into smaller components. A classic example of this is to break a
    monolithic application into microservices. The reason is that each microservice
    is easier to manage than a very large and complex monolithic system architecture.
    Smaller components can be developed and scaled independently, which can improve
    the speed of development of new features in your systems.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Principle 4 – Use a stateless architecture
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a stateless architecture, each transaction is independent. When processing
    a request, the server does not remember any prior requests or transactions, and
    the client must send any necessary data for a transaction in each request. In
    a stateful architecture, on the other hand, the server maintains the state of
    the client’s session. The context of previous transactions is remembered, and
    future transactions can be affected by what happened in the past.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: I would add the words “whenever possible” to the title of this principle because
    sometimes your system will need to maintain state, but what you would want to
    do, as much as possible, is minimize the amount of state that needs to be maintained
    and offload the state management from your application to a separate mechanism
    such as a caching layer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Stateless architectures are generally easier to scale because they don’t require
    maintaining client states, allowing requests to be processed by any available
    server. Stateful architectures require more complex infrastructure to ensure that
    the client interacts with the same server or that the state is shared, which can
    be challenging in large-scale environments. Stateful systems also use more resources
    to manage and store session data, which can further affect scalability and system
    complexity.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Pillar 2 – Operational excellence
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Operational excellence* pillar is concerned with efficiently running, managing,
    and monitoring systems on Google Cloud. It includes concepts such as automation,
    observability, and scalability.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: This pillar talks about automating system deployments using **continuous integration
    and continuous deployment** (**CI/CD**), and managing your infrastructure using
    **infrastructure as code** (**IaC**). This is a very important concept because
    it provides all of the benefits of traditional software development, such as version
    tracking and incremental updates. If you manage your infrastructure updates using
    version tracking mechanisms, then you can maintain strong records for auditing
    purposes, and if issues are introduced by any updates, then you can more easily
    roll back to a previous version that was known to work well. This is often referred
    to as **GitOps**, and the opposite of this is referred to as **ClickOps**. In
    the case of ClickOps, infrastructure updates are made by people clicking around
    in a UI. If you have hundreds of people in your technology organization, and every
    day they are all making updates to your infrastructure by clicking around in a
    UI, then it can become difficult to coordinate and track these updates over time.
    Terraform is a popular tool for implementing IaC on Google Cloud.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The *Operational excellence* pillar also outlines best practices for incorporating
    testing throughout the software delivery life cycle. This includes unit tests,
    integration tests, system tests, and other types of tests such as performance
    and security testing. Rather than testing everything at the end, we should aim
    to include each type of test as relevant throughout each step in the development
    life cycle. For example, unit tests could be automated as part of our build process.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: When deploying software, the *Operational excellence* pillar recommends using
    approaches such as immutable infrastructure updates via blue/green deployments
    and A/B or canary tests. Google Cloud provides CI/CD tooling that can be used
    to implement these strategies. The recommendation is to use small but frequent
    updates to your systems, which are easier to manage and roll back than large,
    infrequent changes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: In terms of observability, this pillar provides recommendations on effectively
    setting up monitoring, alerting, and logging, including common metrics to keep
    an eye on, and defining thresholds beyond which some kind of alert or corrective
    action should be invoked. It also talks about the importance of setting up audit
    trails to keep track of changes to your systems. For cases in which something
    does go wrong, it provides guidelines on establishing support and escalation procedures,
    as well as review processes such as post-mortem assessments to learn from any
    failures.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it’s also important to ensure that your infrastructure is adequately
    scaled to handle your expected traffic volumes and that you implement plans to
    proactively scale accordingly for known peak events.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this pillar covers the importance of automating as many of your system
    management tasks as possible to minimize how much you need to rely on potentially
    error-prone manual processes to keep your systems running effectively.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Pillar 3 – Security, privacy, and compliance
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is perhaps the most relevant of the pillars in the context of ML governance,
    and we’ve already touched on these topics earlier in this chapter, but here, we
    will take a look at how these concepts are more formally structured within the
    Google Cloud Architecture Framework.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the concept of DiD that we discussed earlier, Google recommends
    strategies to implement **security by default**. This consists of best practices
    for ensuring that security is built in as the default configuration in your system
    architecture, including concepts such as the **principle of least privilege**
    (**PoLP**), in which users are given the minimum permissions required to perform
    their job functions, and nothing more. It also refers to locking down access at
    the network level. For example, if you know that, in normal operating circumstances,
    your system should only ever be accessed from one or two other systems, then you
    could set up network rules that block access from any sources other than those
    specific systems. Google Cloud also provides an offering called **Confidential
    Computing** for processing sensitive data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to take this opportunity to highlight that the pillars of the Google
    Cloud Architecture Framework are often interrelated. For example, we talked about
    the concept of GitOps in the context of the *Operational excellence* pillar. This
    concept of using IaC to manage how you deploy to your systems is a highly recommended
    way to establish security-by-default practices. For example, you can create Terraform
    modules that undergo stringent security assessment processes for setting up your
    infrastructure in a way that aligns with your corporate security policies and
    industry-wide best practices. Once those “secure-by-default” modules have been
    approved, anybody in your company could safely use them to set up the required
    infrastructure securely. This makes it much easier for your employees to abide
    by your security policies in terms of provisioning infrastructure. To make it
    easy for you to provision infrastructure resources that align with security best
    practices, Google Cloud provides the **security foundations blueprint**, which
    you can reference at the following URL:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/architecture/security-foundations](https://cloud.google.com/architecture/security-foundations)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: As an association between this pillar and the pillar of *Operational excellence*,
    the CI/CD pipelines that are used to deploy your resources should have security
    mechanisms built in. For example, you could use automation to check for security
    vulnerabilities when artifacts are created. Google Cloud also provides a mechanism
    called Binary Authorization to validate the contents of Docker containers that
    are built and deployed by your CI/CD pipelines to ensure that those images contain
    exactly what you expect them to contain and nothing more. It can also validate
    that a specific build system or pipeline created a specific container image. If
    a security check highlights any potential problems at any point in your CI/CD
    pipeline, the pipeline can automatically be halted to ensure that potential security
    threats are not introduced into your deployments. Similarly, you can use Google
    Cloud’s Artifact Analysis feature to scan automatically for potential vulnerabilities
    in containers stored in Artifact Registry and Container Registry. Even after deployment,
    you can continuously scan your web applications by using Google Cloud’s Web Security
    Scanner to identify vulnerabilities in applications deployed to Compute Engine,
    App Engine, and GKE.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This pillar also provides recommendations on proactively identifying and cataloging
    risks to your company and how to mitigate common risks. Google Cloud has also
    recently launched the Risk Protection Program, which includes tools such as Risk
    Manager, to help you manage risks.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Of course, **IAM** is an important component of this pillar. Google Cloud provides
    many tools that help manage this aspect, such as IAM and Cloud Audit Logs, which
    we discussed are essential for access management and auditing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: This pillar also calls out the importance of using cloud asset management tools
    such as Cloud Asset Inventory to track all of your company’s technology assets
    and monitor for deviations from your compliance policies.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: In addition to all of the topics we’ve covered in this section, the security
    pillar also covers topics such as network security, data security, privacy, and
    regulatory compliance, which we covered earlier in this chapter. It also provides
    details on how to use Google Cloud Assured Workloads to help you meet your compliance
    obligations, how to monitor for compliance, and how to address data sovereignty,
    data residency, software sovereignty, and operational sovereignty.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Pillar 4 – Reliability
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Reliability* pillar focuses on concepts such as **high availability** (**HA**),
    scalability, automated change management, and **disaster recovery** (**DR**).
    It covers topics that some of you may know from Google’s SRE practices, such as
    defining **service-level indicators** (**SLIs**), **service-level objectives**
    (**SLOs**), **service-level agreements** (**SLAs**), and error budgets. As was
    the case with the *Operational excellence* pillar, the *Reliability* pillar includes
    observability as a major component. It also reiterates some other concepts from
    the *Operational excellence* pillar, such as automating deployments and incremental
    updates using CI/CD pipelines, and the importance of setting up appropriate observability
    and alerting mechanisms, **incident management** (**IM**), and post-mortem practices.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: This pillar talks about ways of creating redundancy for higher availability
    in your system architectures, including using multiple Google Cloud zones and
    regions to mitigate any potential issues that may occur in a particular location.
    In addition to these kinds of proactive mitigation techniques, it also outlines
    practices for establishing DR strategies, such as synchronizing data to other
    regions and establishing playbooks for failing over to those regions if needed.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it goes into much detail regarding best practices for specific Google
    Cloud products. This pillar contains a wealth of knowledge and much more detail
    on many specific Google Cloud products than would be appropriate to include here.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Pillar 5 – Cost optimization
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can be guaranteed this is very important to almost every customer. In fact,
    cost optimization is often one of the main factors that entice companies to move
    to the cloud in the first place. When companies run their workloads in their own
    data centers, they often have to purchase and install enough infrastructure (and
    more) to cater for their highest peak events that may only happen once or twice
    per year. For the rest of the year, that infrastructure is highly under-utilized,
    which amounts to a lot of wasted money. In the cloud, however, companies can scale
    their infrastructure up and down based on what they actually need and therefore
    do not need to waste money on over-provisioned infrastructure. Also, as discussed
    earlier in this chapter, offloading infrastructure management to a cloud provider
    enables companies to invest their time in innovation and developing features that
    support their core business.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The first major focus of this pillar is on the concept of financial operations
    or **FinOps**, which is a cultural paradigm that includes a set of technical processes
    and business best practices to help organizations optimize and manage their cloud
    investments more effectively. In this context, it’s important to provide each
    technology team in the organization with visibility into their cloud spend and
    for each team to take accountability for that spend. To learn more about FinOps,
    I recommend reading the Google Cloud FinOps whitepaper, which can be found at
    the following URL:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cloud.google.com/resources/cloud-finops-whitepaper](https://cloud.google.com/resources/cloud-finops-whitepaper)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we generally cannot optimize or improve something without monitoring
    it. As such, the *Cost optimization* pillar provides recommendations regarding
    monitoring costs, analyzing trends, and forecasting future costs. If you forecast
    that you will spend a certain amount of money in the next year or the next 3 years,
    you can purchase **committed use discounts** (**CUDs**) to save money on those
    workloads. You can also use labels to categorize your expenses in billing reports,
    such as attributing resource expenses to specific workloads and environments.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The *Cost optimization* pillar also provides best practices on **optimizing**
    resource usage to reduce costs, such as ensuring that you provision your infrastructure
    based on your current and projected needs (including some buffer where appropriate),
    and do not over-provision. This is referred to as **right-sizing**, and Google
    Cloud even provides a right-sizing recommender that can highlight opportunities
    for improving your sizing by identifying resources that appear to be under-utilized
    (and therefore over-provisioned). You should also use auto-scaling, which, in
    addition to ensuring that you have enough resources to serve your required traffic
    volumes, can scale resources down when they’re not needed, thus saving money.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: When implementing cost optimization mechanisms, it’s important to set up budgets,
    alerts, and quotas to control your spending. For example, you can specify a certain
    spending budget and get alerted when you are close to reaching that budget. You
    can also use quotas to set hard limits on resource usage and can set API caps
    to limit API usage after a certain threshold is reached.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: As with the *Reliability* pillar, the *Cost optimization* pillar provides detailed
    best practices for many specific Google Cloud products, such as optimizing storage
    tiers in **Google Cloud Storage** (**GCS**) or optimizing partitions in BigQuery.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Pillar 6 – Performance optimization
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance optimization can be linked to cost optimization, so there is some
    overlap in terms of these concepts. For example, if your systems are performing
    optimally, then they may be less costly to run. A well-implemented auto-scaling
    strategy is a prime example of this. The *Performance optimization* pillar provides
    recommendations on how to define performance requirements, how to monitor and
    analyze performance, and, of course, how to optimize performance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: In terms of monitoring and analyzing performance, this refers back to the concept
    of observability, in which we need to implement and monitor performance metrics
    such as latency and resource utilization.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: As with the *Reliability* and *Cost optimization* pillars, the *Performance
    optimization* pillar also provides many in-depth recommendations for specific
    Google Cloud products, which is a level of detail beyond what would be appropriate
    to include here.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand what the Google Cloud Architecture Framework is and
    what it consists of, let’s look at how we can apply its concepts in the context
    of AI/ML on Google Cloud.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Across all of the categories in the Google Cloud Architecture Framework, the
    theme that is reiterated the most is automation. The idea is to automate everything
    as much as possible. Vetted, repeatable processes that can run automatically tend
    to make our jobs easier across all pillars.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Architecture Framework concepts about AI/ML workloads on Google Cloud
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will assess how the Google Cloud Architecture Framework
    can be used with regard to AI/ML workloads on Google Cloud. We will use the steps
    in the model development life cycle to frame our discussion. As a reminder, the
    steps in the model development life cycle are summarized at a high level in *Figure
    13**.5*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5: The ML model development life cycle](img/B18143_13_5.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: The ML model development life cycle'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with the data collection and preparation activities in the model
    development life cycle, which include gathering, ingesting, storing, and processing
    data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Spoiler alert!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that we have already been using many of these practices throughout
    this book. Here, we are calling them out explicitly so that you can understand
    how they apply to workloads in general.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and preparation
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data management is perhaps the most important of all the topics related to ML
    governance. As we know by now, the quality of your data directly impacts the quality
    of your models. Also, in terms of security, when malicious actors try to access
    your systems, they are usually after your data, because data is such a valuable
    resource, and data breaches can have catastrophic effects for your company. Let’s
    look at how we can apply recommendations in the Google Cloud Architecture Framework
    regarding data handling. In this case, we will not discuss system design as a
    separate pillar, because an effective system design encapsulates all of the other
    pillars.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Operational excellence in data collection and preparation
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that operational excellence focuses on concepts such as automation,
    observability, and availability. The following sub-sections explore these concepts
    in the context of data collection and preparation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Process automation and integration
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In earlier chapters of this book, I talked about the importance of building
    data pipelines to automate our data processing steps. Essentially, we want to
    establish repeatable processes and then put in place mechanisms to run those processes
    automatically, either based on a schedule (such as daily, weekly, or monthly)
    or in reaction to some event, such as new data becoming available. Hence, the
    concept of implementing data processing pipelines is an application of the automation
    recommendations outlined in the *Operational excellence* pillar of the Google
    Cloud Architecture Framework. Google Cloud provides many services that we can
    integrate together to set up data processing pipelines, such as Dataproc, Dataflow,
    GCS, Pub/Sub, and BigQuery.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Consistency and standardization
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the MLOps chapter, I shared experiences I’ve had with various organizations
    at different levels of maturity in terms of how they implemented their ML workload
    operations. When companies do not have well-established processes in place, their
    teams tend to use lots of random different tools that each have their own learning
    curves, and they maintain their artifacts in silos. These practices are not conducive
    to company-wide collaboration, and they hinder the scalability and efficiency
    of a company’s ML operations. I then talked about the importance of standardizing
    tools and processes throughout the company in order to overcome those limitations.
    This all relates to the operational excellence pillar of the Google Cloud Architecture
    Framework. Consistency in tools, libraries, and processes across teams and projects
    reduces complexity and learning curves, making it easier to manage and scale operations.
    Perhaps the most relevant example of this, in relation to ML operations, is to
    use Vertex AI for all of our model development and deployment needs, since it
    provides a standard set of tools for every step in the model development and management
    life cycle.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the regular system monitoring and logging requirements also apply to
    AI/ML workloads, and AI/ML workloads also have additional requirements regarding
    monitoring the quality of model prediction outputs on an ongoing basis to ensure
    that they do not drift over time. In earlier chapters, we discussed ML-specific
    metrics to monitor, such as MSE for linear regression use cases or AUC-ROC scores
    for classification use cases, as well as fairness metrics. Google Cloud Logging
    and Google Cloud Monitoring can be used for observability purposes at all points
    in the ML model development life cycle on Google Cloud, from evaluating training
    validation metrics to tracking the latency of responses from a model deployed
    on a Vertex AI prediction endpoint, and Vertex AI Model Monitoring can be used
    to watch for drift.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Security, privacy, and compliance in data collection and preparation
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By now, we should thoroughly understand that data security and privacy are paramount
    in almost every company. One sure way to quickly lose customers and damage your
    company’s reputation is to become a victim of a data breach. My opinion is that
    managing sensitive data securely is the most important thing you can possibly
    do; it takes priority over all other considerations in this chapter, this book,
    and your company’s business.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In our overview of this pillar earlier in this chapter, we talked about how
    data and systems should be protected through multiple layers of defenses (DiD),
    incorporating factors such as access management, encryption, and network security.
    The following sub-sections explore these concepts in the context of data collection
    and preparation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Data access control
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When collecting and preparing data in Google Cloud, we can use IAM to ensure
    that only authorized individuals and services can access the data, and we can
    control who can view, modify, or interact with the data based on roles and responsibilities.
    For example, a data scientist might have permission to read and analyze data but
    not delete it, or finance data may only be accessible to the finance department.
    This is made easier if we use Google Cloud Dataplex to build a data catalog because
    Dataplex allows us to centrally manage permissions for our data assets across
    multiple different GCS and processing services.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Data protection and encryption
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sensitive data should always be encrypted, both when it’s stored (at rest) and
    when it’s being transferred between services or locations (in transit). Google
    Cloud provides automatic encryption for data stored in its services, and we can
    use TLS to protect data in transit. For highly sensitive data, we can even encrypt
    it during processing by using Google Cloud Confidential Computing. Also, during
    the data preparation phase, sensitive data elements can be masked or tokenized
    to hide their actual values, therefore enhancing privacy while still allowing
    the data to be used for analysis.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Data classification and discovery
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use the Google Cloud Sensitive Data Protection service to discover, classify,
    and mask sensitive elements in datasets. When collecting data, this service can
    help automatically identify information such as **personally identifiable information**
    (**PII**) or financial data so that we can treat it with higher levels of protection.
    This is another area in which Google Cloud Dataplex can help because tracking
    all of our data assets in a data catalog makes classification and discovery easy.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Auditing and monitoring
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We introduced Cloud Audit Logs earlier. We can use Cloud Audit Logs to keep
    a detailed log of who accesses the data and what operations they perform, which
    is important for accountability and traceability. This is especially relevant
    in ML workloads, where understanding who introduced what data and when can be
    required for explainability and troubleshooting. And, guess what?! Google Cloud
    Dataplex integrates with Cloud Audit Logs to generate audit logs for actions that
    are performed in the data catalog.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Data retention and deletion
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When using Google Cloud, we can establish policies about how long data should
    be retained based on its nature and relevance. When using GCS, for example, a
    retention policy can be specified to prevent an object from being deleted within
    the timeframe specified by the retention policy. This can be important for regulatory
    purposes or to comply with legal holds. Conversely, Object Lifecycle Management
    can be used to automatically delete data after a certain period (as long as it
    does not conflict with a data retention policy). For sensitive data that you want
    to delete permanently, Google Cloud provides mechanisms to ensure that the deletion
    is performed securely and is irrecoverable.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Compliance frameworks and certifications
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Google Cloud provides tools and documentation to help businesses comply with
    standards such as GDPR and HIPAA (as well as many more), and it undergoes independent
    third-party audits to ensure its services comply with common regulatory standards.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Resilience against threats
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Services such as Cloud Security Command Center and Event Threat Detection allow
    for continuous data and environment monitoring for potential threats, offering
    insights and actionable recommendations. Regularly scanning and assessing the
    systems involved in data collection and preparation for vulnerabilities can help
    ensure that data isn’t exposed to potential breaches. You can also use VPC network
    security and VPC-SC to control access to your data storage and processing systems
    and to prevent data exfiltration.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: All of the items we discussed in this section are important to ensure that data
    is handled securely and that user privacy is protected. Ethical considerations
    also come into play to ensure that data is collected and used in ways that are
    fair, transparent, and don’t propagate biases, especially when it’ll be used to
    train ML models that might impact individuals’ lives.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Reliability in data collection and preparation
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that the *Reliability* pillar in the Google Cloud Architecture Framework
    focuses on ensuring that services and applications perform consistently and meet
    the expected SLOs, even in the case of unexpected disturbances or increased demands.
    The following sub-sections discuss how we can apply concepts from the *Reliability*
    pillar in the data collection and preparation phases of the ML model development
    life cycle.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Automated data ingestion and processing
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Relying on manual processes for data collection can be prone to errors, while
    automated data ingestion helps to ensure that data can be collected consistently.
    We can also automate data validation steps to ensure that incoming data adheres
    to the expected formats and value ranges, which can prevent corrupted or malformed
    data from propagating through our data processing and ML pipelines. For data transformation
    scripts and configurations, we should use version control to ensure that if changes
    introduce errors, we can easily revert to a previous, stable version.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure resilience
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many of Google Cloud’s data storage and processing services are either designed
    for HA by default or provide mechanisms to help you build resilience into your
    architecture, such as by using multiple machines across multiple zones and regions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: If we are designing systems ourselves, we should ensure that data storage and
    processing infrastructure have redundant components. In the event of a failure,
    backup systems can take over, ensuring uninterrupted data collection and preparation.
    We should also implement backup and restore mechanisms to regularly back up raw
    and processed data. We could store data across multiple zones or regions to safeguard
    against potential issues in any particular location. This not only protects against
    data loss but also allows for restoring to a previous state if data becomes corrupted
    or if there’s a need to revisit earlier data versions. We could also implement
    load balancing for data ingestion services for high-velocity data streams to help
    ensure an even distribution of data loads and prevent system overloads. We should
    also design our infrastructure to scale (up or out) based on demand to ensure
    reliable performance under varying loads, and we could implement queueing mechanisms
    to manage data spikes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Continuous monitoring and alerts
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As is the case with the *Operational excellence* pillar, observability is a
    key component of this pillar. We should regularly check the health of systems
    involved in data collection and preparation and implement alerting mechanisms
    that notify relevant teams when anomalies or failures are detected.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Cost optimization in data collection and preparation
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Managing costs is important during data collection and preparation due to potentially
    vast volumes of data, complex preprocessing tasks, and varying infrastructure
    needs. The following sub-sections discuss how we can apply concepts from the *Cost
    optimization* pillar in the data collection and preparation phases of the ML model
    development life cycle.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Efficient data storage
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Storage systems such as GCS and BigQuery provide various classes of storage
    that are priced differently. From a cost optimization perspective, it’s important
    that we use the appropriate storage class for our data. For example, frequently
    accessed data can be stored in Standard storage, while infrequently accessed data
    could be moved to Nearline or Coldline storage. To make this easier for us to
    manage, we can implement policies to automatically transition data to cheaper
    storage classes or delete it once it’s no longer needed. We could also reduce
    the amount of data stored (and, therefore, reduce our costs) by removing duplicates
    and compressing our data. In terms of feature storage, we should evaluate the
    necessity of every feature during the data preparation stage. Removing redundant
    or low-importance features can significantly reduce storage and computation costs.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Optimized data processing
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: I’m a huge fan of using serverless solutions wherever possible. Not only do
    we offload the headaches of managing infrastructure to the cloud provider, but
    with serverless solutions such as BigQuery and Dataflow, we generally only pay
    for what we use, and we don’t have to worry about over-provisioning (and therefore
    overpaying for) infrastructure. We can also opt for scalable services such as
    GKE or Cloud Dataflow that can handle spikes in data processing loads but scale
    down in low-demand periods, and for non-critical, fault-tolerant data processing
    tasks, we can use preemptible **virtual machines** (**VMs**), which are generally
    cheaper than regular instances.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to consider the location of our data, and we should generally
    aim to process data as near as possible to where it resides for a number of reasons,
    including cost and latency. For example, storing our data in the `us-central1`
    region while our processing infrastructure is located in the `us-east4` region
    would be sub-optimal from a latency perspective and would incur additional network
    egress costs as the data is transmitted across regions. This also applies in the
    cases of hybrid cloud infrastructures, in which some of your resources are in
    the cloud while others are located on your own premises. In such cases, consider
    the location at which your on-premises resources are connected to the cloud, as
    well as the data storage location and data processing location. We discussed the
    various methods of connecting your on-premises resources to Google Cloud (such
    as VPNs and “interconnects”) in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059),
    and you can further bolster the security of such hybrid configurations by using
    VPC-SC to establish a trusted perimeter within which your data is transmitted
    and processed.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Also, if we are using VMs and those VMs need to work together to process our
    data, we can use **Google Compute Engine** (**GCE**) placement policies (specifically,
    the “compact placement policy”) to specify that our VMs should be located close
    to each other, which can be particularly important for **high-performance computing**
    (**HPC**) workloads.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Finally, where real-time processing isn’t necessary, we can accumulate data
    and process it in batches, which is often more cost-effective than streaming.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Cost monitoring and analytics
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use tools such as Cost Explorer or custom dashboards in Cloud Monitoring
    to get insights into our spending patterns and set up billing alerts to notify
    us of unexpected spikes in costs so that we can intervene accordingly in a timely
    manner. Additionally, we should regularly analyze our billing reports to identify
    areas where costs can be trimmed, such as by looking for under-utilized resources
    or services.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Cost governance
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A good practice is to set budgets for projects or departments and implement
    quotas for specific services to prevent unintentional overspending. It’s also
    important to establish a resource organization and cost attribution strategy.
    We can use Google Cloud projects, folders, and labels to organize and attribute
    costs, which makes it easier to track and optimize expenses for specific tasks
    or teams, and we should promote a culture in which teams are aware of the costs
    associated with their data handling and processing activities, and encourage cost-saving
    practices.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Regular reviews
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We should regularly review the architecture of our data collection and preparation
    systems because newer and more cost-effective solutions might emerge over time.
    Similarly, we should periodically evaluate the relevance of the data we’re collecting
    because some data might become irrelevant over time, and the costs associated
    with its collection, storage, and processing could be eliminated.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization in data collection and preparation
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the overview sections earlier in this chapter, there are
    some links between performance optimization and cost optimization because a system
    that performs optimally will often use resources more efficiently. The following
    sub-sections discuss how we can apply concepts from the *Performance optimization*
    pillar in the data collection and preparation phases of the ML model development
    life cycle.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: High-performance data collection
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To optimize our data ingestion processes for real-time data ingestion, we can
    use services such as Cloud Pub/Sub or Cloud Dataflow, which can help to achieve
    minimal latency and efficient data streaming. We can also use parallel processing
    in our data collection strategies by using distributed systems to fetch data from
    multiple sources concurrently, thus making our data collection more efficient.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Efficient data storage
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It’s important to use appropriate data structures such as columnar formats (for
    example, Parquet) for analytics workloads, which can lead to faster querying.
    In the case of high-performance storage use cases, we can use storage solutions
    such as Cloud Bigtable for low-latency, high-throughput workloads, which can help
    to ensure quick data access during the preparation phase. How we index our datasets
    can also improve the speed of retrieval and querying, which is especially important
    for large datasets during the data exploration phase.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Accelerated data processing
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use platforms such as Cloud Dataflow and Cloud Dataproc, which provide
    managed Beam, Spark, and Hadoop clusters, to distribute data processing tasks
    across multiple nodes. For workloads such as feature engineering or data augmentation
    tasks in ML, using hardware accelerators such as GPU/TPU acceleration can drastically
    improve performance. Also, in platforms such as BigQuery, we can write optimized
    SQL queries to minimize computational overhead and improve processing speed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Network optimization
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we’re transferring large amounts of data from on-premises systems to Google
    Cloud, dedicated interconnects provide a high-speed, low-latency connection. For
    collecting data from global sources, **content delivery networks** (**CDNs**)
    ensure optimal data transfer speeds, and we can also use tools such as Traffic
    Director to manage and optimize network traffic, ensuring efficient data flow
    between services.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Resource allocation and auto-scaling
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed earlier, it’s important to ensure that services automatically
    scale resources based on demand. For example, Cloud Dataflow can auto-scale worker
    instances based on the data processing load. We should also tailor VM types and
    configurations (in terms of memory and CPU resources) to the specific needs of
    the data collection and preparation tasks.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss how the Google Cloud Architecture Framework applies to the
    model building and training steps in our model development life cycle.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Model building and training
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we did in the previous section regarding data collection and preparation,
    we will discuss the concepts of each pillar in the context of this phase in the
    model development life cycle.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Operational excellence in model building and training
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin with operational excellence, and how it applies to model building
    and training.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Standardized and automated workflows
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key components here are MLOps pipelines, version control, and CI/CD tooling.
    We can use Vertex AI Pipelines to create standardized, end-to-end ML pipelines
    that automate our model training and evaluation steps, including hyperparameter
    optimization. We can use Google Cloud’s source control tooling to manage our pipeline
    definition code, and Google Cloud’s CI/CD tooling, such as Cloud Build and Cloud
    Deploy, to build and deploy our pipeline definitions to Vertex AI Pipelines.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are two main types of monitoring and logging that we need to implement
    during the model training and building phase. First, we need to track model performance
    metrics during model training, such as loss, accuracy, and validation scores.
    We can do this by using Vertex AI and tools such as TensorBoard. The second is
    related to system resource monitoring, for which we can use Google Cloud Monitoring
    to keep an eye on the resource consumption of VMs, TPUs, or GPUs during model
    training, which can help to achieve optimal resource utilization and timely detection
    of any potential bottlenecks that might occur.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Managed infrastructure
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We should use managed infrastructure for our model training and building steps.
    By using managed infrastructure such as Vertex AI, we automatically use the recommendations
    outlined by the *Operational excellence* pillar in the Google Cloud Architecture
    Framework.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Security, privacy, and compliance in model building and training
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data security will still be a major focus in this section, considering that
    the training process involves data handling. The following sub-sections discuss
    how we can apply concepts from the *Security, privacy, and compliance* pillar
    in the model building and training phases of the ML model development life cycle.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Data security
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The mechanisms here are the same as we discussed in the *Data collection and
    preparation* section. We should ensure that data used for model building and training
    is encrypted both in transit and at rest. When using sensitive data for training,
    we can use data masking and tokenization to mask or tokenize specific fields to
    prevent exposure of PII or other sensitive data points. Additionally, we can use
    services such as VPC-SC to restrict the services and resources that can access
    our data, thus creating a secure perimeter around the data used for training.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Environment security
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can set up secure training environments to ensure that VMs and containers
    are securely configured, patched, and hardened, or use managed environments such
    as Vertex AI, which take care of a lot of these activities for us, and we can
    use VPC and firewall rules to secure the network traffic related to model training.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Compliance monitoring
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use tools such as Cloud Security Command Center to continuously monitor
    and ensure that the training environment adheres to compliance standards, and
    we should also regularly audit data sources for training to ensure compliance
    with data usage policies, especially if sourcing data from third parties.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Privacy
  id: totrans-276
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If working with sensitive datasets, we can use techniques such as differential
    privacy to introduce noise into the data, ensuring individual data points are
    not identifiable. We can also use data de-identification to remove PI so that
    it cannot be associated with specific individuals.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: In addition to all of the aforementioned, we can use IAM to control access to
    the training environment and artifacts.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Reliability in model building and training
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following sub-sections discuss how we can apply concepts from the *Reliability*
    pillar in the model-building and training phases of the ML model development life
    cycle.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Data reliability
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we’ve done in earlier chapters of this book, we can implement validation
    checks for incoming data to ensure consistency, quality, and completeness. We
    should also regularly back up training data to prevent data loss and use data
    versioning for reproducibility.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Training infrastructure reliability
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can provision redundant resources in regional or multi-regional deployments
    to ensure training can continue even if one data center faces issues. In terms
    of infrastructure scalability, Vertex AI can automatically scale resources based
    on the training workload. Of course, it’s important to use monitoring tools to
    keep an eye on resource utilization and health.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Model training resilience
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use checkpointing to save model states at regular intervals during training.
    In case of interruptions, training could resume from the latest checkpoint rather
    than starting from scratch. For transient failures in any stage of the model building
    process, we should implement retry policies to automatically attempt the task
    again before raising an error.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Dependency management
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vertex AI lets us use containerization to ensure consistent software and library
    versions across training runs, preventing “it works on my machine” issues. This
    also brings with it all of the other established benefits of containerization,
    such as standardization and scalability. Think about how we used containers during
    the practical exercises in our MLOps chapter. We packaged our custom data processing
    and training code into containers, and then we could use them seamlessly in later
    stages of the model development process, simply by pointing to the container locations
    during various steps being implemented on systems such as Vertex AI and Dataproc.
    This kind of packaging, which facilitates repeatable execution results, is essential
    for automating steps in an MLOps pipeline, as well as automatically scaling our
    training and inference workloads based on their varying resource requirements.
    Such automation is a core benefit of MLOps practices. Furthermore, by using discrete
    packages of code for each step in the MLOps life cycle in this way, we can scale
    each step independently, providing flexibility in accordance with the best practices
    outlined in the *Operational excellence* and *Reliability* pillars of the Google
    Cloud Architecture Framework.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: To further mitigate potential dependency issues, if we have dependencies on
    external systems such as data providers, we should ensure they have uptime guarantees
    and fallback mechanisms.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: DR
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'It’s important to regularly back up model architectures, configurations, trained
    weights, and other essential components to allow for quick recovery in case of
    data corruption or loss. We should establish clear protocols for restoring from
    backups, ensuring minimal downtime and a quick return to operational status in
    case of disruptions. The following point cannot be emphasized enough: we must
    periodically test our recovery procedures. Companies often focus only on backup
    mechanisms and not on testing the recovery processes. We want to ensure that our
    recovery processes are effective (that is, they actually work) and efficient (that
    is, they work as quickly as possible).'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Cost optimization in model building and training
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following sub-sections discuss how we can apply concepts from the *Cost
    optimization* pillar in the model building and training phases of the ML model
    development life cycle.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Resource efficiency
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To optimize costs during model building and training, we should ensure that
    VMs, GPUs, or TPUs used for training are appropriately sized for the workload.
    Initially, this may take some experimentation to find the best configuration of
    resources, but by the time we have standardized our training process into an MLOps
    pipeline, we should have a good idea of the required resources. Using Vertex AI
    and serverless services can help us optimize costs because those services can
    scale our resources based on demand. We can also utilize CUDs to save on computing
    costs.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: For training jobs that can handle interruptions, we can use preemptible VMs,
    which can offer substantial savings.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that simpler model architectures can be easier, quicker,
    and cheaper to train than complex model architectures. It’s also important to
    shut down all resources when not in use so that we don’t pay when they are idle.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Review and optimize regularly
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can use tools such as Google Cloud Cost Management to regularly review and
    analyze infrastructure costs, and identify opportunities for optimization. As
    always, we can use budgets, quotas, and billing alerts to help keep our costs
    under control, and we should periodically review our ML infrastructure, data storage,
    and associated processes to identify and eliminate inefficiencies.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization in model building and training
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following sub-sections discuss how we can apply concepts from the *Performance
    optimization* pillar in the model building and training phases of the ML model
    development life cycle.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Compute optimization
  id: totrans-302
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To optimize performance, we can use hardware acceleration and specialized hardware
    such as GPUs and TPUs, which can significantly accelerate the training process.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can distribute the training process across multiple nodes to run in parallel
    and reduce training time. Also, for hyperparameter tuning, we can use services
    such as Vertex AI Vizier to perform concurrent trials, significantly reducing
    the time required to find optimal model parameters.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Data I/O optimization
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We should use high-throughput data sources and systems for performant workloads
    so that the data coming into the training process isn’t a bottleneck.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in other sections of this chapter, it’s important to continuously
    track performance metrics such as processing speed, memory usage, and I/O throughput
    using tools such as Google Cloud Monitoring, and then adjusting resources or configurations
    as needed. We can also use profiling to analyze ML training code, identify performance
    bottlenecks, and then optimize the most time-consuming segments.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss how the Google Cloud Architecture Framework applies the
    model evaluation and deployment steps in our model development life cycle.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and deployment
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss the concepts of each pillar in the context
    of model evaluation and deployment in the model development life cycle. Note that,
    in some phases, the same concepts that we’ve already discussed in previous phases
    of the model development life cycle still apply. In the remaining sections of
    this chapter, I will briefly call out when the same concepts apply again.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Operational excellence in model evaluation and deployment
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s begin with operational excellence, and how it applies to model evaluation
    and deployment.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Automation, observability, and scalability
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this phase of the ML model development life cycle, the same concepts from
    the *Operational excellence* pillar, such as automated workflows, observability,
    and scalability, which we’ve already discussed in the context of the model building
    and training phase, apply again here. Basically, we can set up MLOps pipelines
    that automate our model evaluation and deployment steps using Vertex AI Pipelines,
    and we can use Google Cloud Monitoring and logging tools to track metrics related
    to our model evaluations and deployed model performance. We can also use load
    balancers and Vertex AI auto-scaling infrastructure to ensure that our models
    can handle varying levels of demand.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing and canary deployments
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When deploying new model versions, we can use A/B testing to gradually shift
    traffic and compare performance against previous versions. Of course, we want
    to ensure that the newer versions being deployed perform better than the previous
    versions and that they don’t negatively impact user experience. Using canary deployments,
    we can deploy new model versions to a small subset of users first, closely monitor
    performance, and then gradually expand to a broader user base. We should also
    use model versioning to allow for quick rollbacks if newer versions result in
    unexpected behaviors or errors.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Security, privacy, and compliance in model evaluation and deployment
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, the same concepts regarding data security and privacy apply here also,
    as well as access control, compliance regulations, and auditing. In addition to
    all of that, we can use network security controls and VPC-SC to protect the endpoints
    on which our models are hosted.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Reliability in model evaluation and deployment
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, the same concepts of infrastructure resilience, such as deploying
    resources in multiple zones or regions, also apply here, as well as health checks,
    load balancing, auto-scaling, DR, monitoring, alerting, and dependency management.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Cost optimization in model evaluation and deployment
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When discussing cost optimization in the context of model evaluation and deployment,
    some concepts from our previous phases apply again, such as right-sizing resources,
    shutting down idle resources, using CUDs, and setting budgets and alerts. It’s
    also important to note that smaller, simpler models require fewer resources and
    are therefore cheaper to run than larger, more complex models.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization in model evaluation and deployment
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You won’t be surprised to hear the terms *auto-scaling* and *load balancing*
    being used in the context of performance optimization for model evaluation and
    deployment, as well as optimizing compute and storage resources, and hardware
    acceleration.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: We can also use caching mechanisms to improve response times. For example, we
    can cache frequent prediction results so that repeated requests can be served
    without invoking the model again, and we can store frequently accessed data or
    intermediate model evaluation results in memory for quicker access.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: By now, you have become an expert in the Google Cloud Architecture Framework
    and how it specifically applies to the ML model development life cycle. Let’s
    take a moment to summarize everything we covered in this chapter.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed various aspects of ML model governance, including documentation,
    versioning, monitoring, auditing, compliance, operationalization, and continuous
    improvement. We then explored some industry-specific and region-specific regulations,
    such as HIPAA for healthcare, SOX for finance, GDPR (EU), and CCPA (California).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Next, we focused on the Google Cloud Architecture Framework and how to apply
    its pillars—*Operational excellence*, *Security, privacy and compliance*, *Reliability*,
    *Cost optimization*, and *Performance efficiency*—to the various stages of the
    ML life cycle. We dived deep into each pillar, detailing its relevance across
    different phases, from data collection and preparation to model evaluation and
    deployment. This included important concepts, such as cost-efficient model deployment,
    enhancing security throughout the model life cycle, and maintaining high reliability
    and performance standards. Overall, this chapter covered many factors related
    to deploying and managing ML workloads on Google Cloud, with best practices and
    optimizations in mind.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll take a look at using some other popular tools and
    frameworks in the industry—such as Spark MLlib and PyTorch—on Google Cloud.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
