<html><head></head><body>
<div id="_idContainer056">
<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-93"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.2.1">Conformal Prediction for Time Series and Forecasting</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will explore the exciting field of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">conformal prediction</span></strong><span class="koboSpan" id="kobo.5.1"> for time series and forecasting. </span><span class="koboSpan" id="kobo.5.2">Conformal prediction is a powerful tool for producing </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">prediction intervals</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.8.1">PIs</span></strong><span class="koboSpan" id="kobo.9.1">) for point forecasting models, and we will show you how to apply this technique to your data using open source libraries. </span><span class="koboSpan" id="kobo.9.2">This chapter will take you on a journey from understanding the fundamentals of </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">uncertainty quantification</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">UQ</span></strong><span class="koboSpan" id="kobo.13.1">) in time series to the intricate mechanisms behind conformal prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">in forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">With this chapter, you will have a solid understanding of the various approaches to producing PIs, and you will be able to build your PIs using </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">conformal prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.19.1">UQ for time series and </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">forecasting problems</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">The concept of PIs in </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">forecasting applications</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Various approaches to </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">producing PIs</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">Conformal prediction for time series </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">and forecasting</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.27.1">By the end of this chapter, you’ll be able to apply the concepts and open source tools that will be discussed to your industry applications, providing robust forecasting with well-defined uncertainty bounds. </span><span class="koboSpan" id="kobo.27.2">These lessons will enhance your forecasting abilities, giving your models an edge by allowing you to add confidence measures to </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">your predictions.</span></span></p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.29.1">UQ for time series and forecasting problems</span></h1>
<p><span class="koboSpan" id="kobo.30.1">UQ is not just a sophisticated</span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.31.1"> addition to time series forecasting; it is a fundamental aspect that provides invaluable insights into the nature of the predictions. </span><span class="koboSpan" id="kobo.31.2">Let’s look at why it’s important and a brief history of </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">its development.</span></span></p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.33.1">The importance of UQ</span></h2>
<p><span class="koboSpan" id="kobo.34.1">UQ is a critical </span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.35.1">component of time series forecasting. </span><span class="koboSpan" id="kobo.35.2">While a forecast model may provide accurate predictions on average, understanding the uncertainty around those predictions is equally essential. </span><span class="koboSpan" id="kobo.35.3">There are several key reasons why properly quantifying uncertainty is vital for practical time </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">series forecasting:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.37.1">Risk assessment</span></strong><span class="koboSpan" id="kobo.38.1">: In many domains, such as finance, healthcare, and environmental science, forecasting is closely linked with decision-making. </span><span class="koboSpan" id="kobo.38.2">Understanding the uncertainty in predictions aids in assessing potential risks, thus enabling </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">informed decisions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.40.1">Model confidence</span></strong><span class="koboSpan" id="kobo.41.1">: UQ provides an understanding of the confidence in each model’s predictions. </span><span class="koboSpan" id="kobo.41.2">This can lead to a more refined model selection and help identify areas where the model may </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">be underperforming.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.43.1">Optimization of resources</span></strong><span class="koboSpan" id="kobo.44.1">: By acknowledging the uncertainty, resources can be allocated more optimally. </span><span class="koboSpan" id="kobo.44.2">For example, understanding the uncertainty in demand forecasts in supply chain management may lead to better </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">inventory management.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Regulatory compliance</span></strong><span class="koboSpan" id="kobo.47.1">: In some industries, quantifying the uncertainty of forecasts might be mandated by regulatory bodies, emphasizing the importance of a systematic approach </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">to UQ</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.49.1">Having established the critical role of UQ, we now turn to </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">its evolution.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.51.1">The history of UQ</span></h2>
<p><span class="koboSpan" id="kobo.52.1">The need to provide</span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.53.1"> reliable measures of uncertainty alongside time series forecasts has long been recognized. </span><span class="koboSpan" id="kobo.53.2">Over the decades, advances in statistical and computational methods have enabled more sophisticated approaches to quantifying uncertainty. </span><span class="koboSpan" id="kobo.53.3">Some of the significant historical developments are </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Early statistical methods</span></strong><span class="koboSpan" id="kobo.56.1">: The roots of UQ in time series can be traced back to the early statistical models. </span><span class="koboSpan" id="kobo.56.2">Techniques such as PIs were applied to provide bounds </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">on predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.58.1">Bayesian approaches</span></strong><span class="koboSpan" id="kobo.59.1">: Bayesian methods brought a probabilistic perspective to UQ, allowing for more nuanced uncertainty descriptions. </span><span class="koboSpan" id="kobo.59.2">Bayesian forecasting models incorporate</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.60.1"> prior beliefs and likelihood functions to create posterior distributions, representing </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">uncertainty comprehensively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Bootstrapping and resampling</span></strong><span class="koboSpan" id="kobo.63.1">: Techniques such as bootstrapping enabled UQ without strong parametric assumptions, making it accessible for more </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">complex models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.65.1">The historical developments we’ve explored provided a critical foundation for UQ in time series analysis. </span><span class="koboSpan" id="kobo.65.2">Now, let’s dive deeper into some of those early statistical techniques and see how they enabled the first steps toward quantifying </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">forecast uncertainty.</span></span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.67.1">Early statistical methods – the roots of UQ in time series</span></h2>
<p><span class="koboSpan" id="kobo.68.1">UQ has always been a critical part of statistical analysis, and its role in time series forecasting is no different. </span><span class="koboSpan" id="kobo.68.2">The early days of statistical modeling laid the foundation for understanding uncertainty in predictions, and various techniques were developed to provide bounds on</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.69.1"> forecasts. </span><span class="koboSpan" id="kobo.69.2">Here, we will investigate some of these early statistical methods and see how they paved the way for the modern understanding of UQ in time </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">series analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">One of the seminal contributions to UQ in time series forecasting was the concept of </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">confidence intervals:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.73.1">T-distribution for small samples</span></strong><span class="koboSpan" id="kobo.74.1">: When dealing with small sample sizes, the t-distribution provided more accurate intervals, accounting for the increased uncertainty due to </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">limited data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.76.1">Interval estimation for autoregressive models</span></strong><span class="koboSpan" id="kobo.77.1">: Specific techniques were developed for time series models such as ARIMA, where the confidence intervals could be derived for the parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">and forecasts</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.79.1">Along with confidence intervals, prediction bounds were developed to encapsulate the uncertainty associated with future observations. </span><span class="koboSpan" id="kobo.79.2">These bounds considered the uncertainty in the model parameters and the random nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">future errors:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Prediction error variance</span></strong><span class="koboSpan" id="kobo.82.1">: By estimating the prediction error variance, bounds could be created around the </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">forecast values</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Forecast error decomposition</span></strong><span class="koboSpan" id="kobo.85.1">: Techniques were developed to decompose the forecast error into various components, providing insights into the sources </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">of uncertainty</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.87.1">While these early</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.88.1"> methods were highly influential, they often relied on strong assumptions about the underlying distributions and model structure. </span><span class="koboSpan" id="kobo.88.2">The parametric nature of these techniques made them less flexible in dealing with complex, non-linear time </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">series data:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Non-parametric methods</span></strong><span class="koboSpan" id="kobo.91.1">: Recognizing these limitations led to the development of non-parametric methods that didn’t rely on specific </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">distributional assumptions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Robust statistical techniques</span></strong><span class="koboSpan" id="kobo.94.1">: Efforts were also made to create more robust statistical methods that could handle outliers and non-constant variance, extending the scope of early </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">UQ methods</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.96.1">The early statistical methods for UQ in time series laid the groundwork for subsequent advancements in this field. </span><span class="koboSpan" id="kobo.96.2">The principles embedded in these techniques, such as confidence intervals and prediction bounds, continue to be central to modern UQ approaches. </span><span class="koboSpan" id="kobo.96.3">They represent a legacy that’s been built upon and refined, leading to various current methods for understanding uncertainty in time </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">series forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.98.1">Now, let’s dive deeper into some of those early statistical techniques and see how they enabled the first steps toward quantifying </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">forecast uncertainty.</span></span></p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.100.1">Modern machine learning approaches</span></h2>
<p><span class="koboSpan" id="kobo.101.1">The previous sections explored the</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.102.1"> early statistical foundations of UQ for time series predictions. </span><span class="koboSpan" id="kobo.102.2">These techniques, while pioneering, relied heavily on parametric assumptions and simple model structures. </span><span class="koboSpan" id="kobo.102.3">The rise of modern machine learning has enabled more flexible and robust approaches to quantifying uncertainty, overcoming some limitations of traditional methods. </span><span class="koboSpan" id="kobo.102.4">Let’s look at the key innovations in </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">this area:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.104.1">Modern machine learning approaches</span></strong><span class="koboSpan" id="kobo.105.1">: With the rise of machine learning, techniques such as dropout and ensemble methods have been developed to </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">quantify uncertainty.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.107.1">Conformal prediction</span></strong><span class="koboSpan" id="kobo.108.1">: Recently, conformal prediction has emerged as a robust framework for UQ. </span><span class="koboSpan" id="kobo.108.2">It provides a non-parametric approach, guaranteeing valid PIs under </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">mild assumptions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.110.1">UQ is integral to time series forecasting. </span><span class="koboSpan" id="kobo.110.2">It enriches the understanding of the predictions, facilitates better decision-making, and aligns with regulatory requirements. </span><span class="koboSpan" id="kobo.110.3">The evolution of UQ over time has led to diverse approaches, each adding value in </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">different contexts.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">The recent advent of conformal prediction, which will be explored later in this chapter, represents a significant advancement in this field, offering robust and universally applicable </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">uncertainty measures.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">In summary, the emergence of flexible machine learning techniques has enabled robust new approaches to UQ that overcome the limitations of early statistical methods. </span><span class="koboSpan" id="kobo.114.2">This evolution has provided a diverse toolkit for quantifying uncertainty in time </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">series forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">Next, we will explore the concepts behind PIs, a foundation for communicating </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">forecast uncertainty.</span></span></p>
<h1 id="_idParaDest-99"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.118.1">The concept of PIs in forecasting applications</span></h1>
<p><span class="koboSpan" id="kobo.119.1">PIs are vital tools in forecasting, providing a range of plausible values within which a future observation is likely to occur. </span><span class="koboSpan" id="kobo.119.2">Unlike point forecasts, which give a single best estimate, PIs communicate the</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.120.1"> uncertainty surrounding </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">that estimate.</span></span></p>
<p><span class="koboSpan" id="kobo.122.1">This section explores the fundamental concepts behind PIs and their significance in various </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">forecasting</span></span><span class="No-Break"><a id="_idIndexMarker355"/></span><span class="No-Break"><span class="koboSpan" id="kobo.124.1"> applications.</span></span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.125.1">Definition and construction</span></h2>
<p><span class="koboSpan" id="kobo.126.1">PIs are constructed around a point forecast to represent the range within which future observations are expected to lie with a given confidence level. </span><span class="koboSpan" id="kobo.126.2">For example, a 95% PI implies that 95 of 100 future </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.127.1">observations are expected to fall within the </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">defined range.</span></span></p>
<p><span class="koboSpan" id="kobo.129.1">PIs can take several</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.130.1"> forms, depending on the approach used to generate them. </span><span class="koboSpan" id="kobo.130.2">Two key distinguishing factors are </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Symmetric versus asymmetric intervals</span></strong><span class="koboSpan" id="kobo.133.1">: PIs can be symmetric, where the bounds are equidistant from the </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.134.1">point forecast, or asymmetric, reflecting differing uncertainty in </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">different directions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.136.1">Parametric versus non-parametric methods</span></strong><span class="koboSpan" id="kobo.137.1">: PIs can be created using parametric (for example, assuming </span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.138.1">normal distribution) or non-parametric methods, depending on the underlying data </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">distribution assumptions</span></span></li>
</ul>
<h2 id="_idParaDest-101"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.140.1">The importance of forecasting applications</span></h2>
<p><span class="koboSpan" id="kobo.141.1">PIs play an essential role in various forecasting</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.142.1"> domains, and </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">here’s why:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.144.1">Decision making</span></strong><span class="koboSpan" id="kobo.145.1">: PIs enable decision-makers to assess risks and opportunities – for instance, they allow investors to gauge the volatility of </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">an asset</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.147.1">Model evaluation</span></strong><span class="koboSpan" id="kobo.148.1">: Comparing actual observations with PIs can be a part of model diagnostic checks, helping evaluate a model’s adequacy in </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">capturing uncertainty</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Optimizing operations</span></strong><span class="koboSpan" id="kobo.151.1">: In supply chain management, PIs can aid in optimizing inventory by reflecting the uncertainty in </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">demand forecasts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.153.1">Communicating uncertainty</span></strong><span class="koboSpan" id="kobo.154.1">: PIs effectively communicate uncertainty to non-technical stakeholders, facilitating </span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.155.1">more nuanced discussions </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">and planning</span></span></li>
</ul>
<h2 id="_idParaDest-102"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.157.1">Challenges and considerations</span></h2>
<p><span class="koboSpan" id="kobo.158.1">While highly valuable, constructing accurate and reliable PIs is not </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">without challenges:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Assumption sensitivity</span></strong><span class="koboSpan" id="kobo.161.1">: PIs may be sensitive to the underlying assumptions about the data distribution, and incorrect </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.162.1">assumptions can lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">misleading intervals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.164.1">Coverage and width trade-off</span></strong><span class="koboSpan" id="kobo.165.1">: Achieving the correct coverage probability (95%) often competes with the desire for narrow intervals. </span><span class="koboSpan" id="kobo.165.2">Wider intervals may cover the desired percentage of observations but may need to be </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">more informative.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.167.1">Computational complexity</span></strong><span class="koboSpan" id="kobo.168.1">: Some methods for constructing PIs can be computationally intensive, particularly with large datasets or </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">complex models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.170.1">PIs are at the heart of UQ in forecasting applications, offering a more comprehensive view of prospects. </span><span class="koboSpan" id="kobo.170.2">They support strategic decision-making, enable model evaluations, and foster effective communication of uncertainty. </span><span class="koboSpan" id="kobo.170.3">Understanding the concept and practicalities of PIs is essential for anyone working with forecasting models, providing a means to navigate and leverage the inherent uncertainty in predicting </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">future outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">While PIs bring invaluable insights, constructing accurate and informative intervals is only sometimes straightforward, as we’ve seen. </span><span class="koboSpan" id="kobo.172.2">However, decades of research have produced diverse techniques to tackle </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">these challenges.</span></span></p>
<h1 id="_idParaDest-103"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.174.1">Various approaches to producing PIs</span></h1>
<p><span class="koboSpan" id="kobo.175.1">PIs are an essential</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.176.1"> tool in forecasting, allowing practitioners to understand the range within which future observations are likely to fall. </span><span class="koboSpan" id="kobo.176.2">Various approaches have been developed to produce these intervals, each with advantages, applications, and challenges. </span><span class="koboSpan" id="kobo.176.3">This section will explore the most prominent techniques for </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">creating PIs.</span></span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.178.1">Parametric approaches</span></h2>
<p><span class="koboSpan" id="kobo.179.1">Parametric approaches </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.180.1">make specific assumptions about the </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.181.1">distribution of forecast errors to derive PIs. </span><span class="koboSpan" id="kobo.181.2">Some standard techniques in this category are </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.183.1">Normal distribution assumptions</span></strong><span class="koboSpan" id="kobo.184.1">: By assuming that the forecast errors follow a normal distribution, we can compute symmetric PIs based on standard errors and critical values from the </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">normal distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.186.1">Time series models</span></strong><span class="koboSpan" id="kobo.187.1">: Models such as ARIMA and exponential smoothing can generate PIs by modeling the underlying stochastic process and using the estimated parameters to </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">produce intervals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.189.1">Generalized linear models (GLMs)</span></strong><span class="koboSpan" id="kobo.190.1">: GLMs extend linear models to non-normal distributions, allowing for more flexible PI construction. </span><span class="koboSpan" id="kobo.190.2">GLMs broaden linear regression to response variables</span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.191.1"> that follow distributions other than the normal distribution. </span><span class="koboSpan" id="kobo.191.2">GLMs allow us to model data with non-normal responses such as binary, count, or categorical outcomes. </span><span class="koboSpan" id="kobo.191.3">Like linear models, GLMs relate the mean response to</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.192.1"> explanatory</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.193.1"> variables through a link function and linear predictor. </span><span class="koboSpan" id="kobo.193.2">However, the response distribution can be non-normal, handled via an exponential family log-likelihood. </span><span class="koboSpan" id="kobo.193.3">Here are some common examples </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">of GLMs:</span></span><ul><li><span class="koboSpan" id="kobo.195.1">Logistic regression for binary classification (logit link, </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">binomial distribution)</span></span></li><li><span class="koboSpan" id="kobo.197.1">Poisson regression for count data (log link, </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">Poisson distribution)</span></span></li><li><span class="koboSpan" id="kobo.199.1">Multinomial regression for categorical responses (logit link, </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">multinomial distribution)</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.201.1">GLMs estimate coefficients </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.202.1">for each feature, just like ordinary linear regression. </span><span class="koboSpan" id="kobo.202.2">However, by expanding the response distribution and link function, they </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.203.1">can model non-normal processes needed for regression-style prediction with </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">non-continuous targets.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">Their flexibility makes GLMs helpful in constructing PIs for a broader range of problems compared to standard linear regression. </span><span class="koboSpan" id="kobo.205.2">The intervals incorporate the modeled </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">response distribution.</span></span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.207.1">Non-parametric approaches</span></h2>
<p><span class="koboSpan" id="kobo.208.1">Non-parametric methods</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.209.1"> aim to construct PIs without making strict assumptions about the distribution of forecast errors. </span><span class="koboSpan" id="kobo.209.2">Some fundamental techniques in this category are </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.211.1">Bootstrapping</span></strong><span class="koboSpan" id="kobo.212.1">: Bootstrapping involves resampling the observed data and estimating the distribution of </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.213.1">forecasts, from which PIs can </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">be derived</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.215.1">Quantile regression</span></strong><span class="koboSpan" id="kobo.216.1">: This method directly models the response variable’s quantiles, enabling the construction of PIs without specific </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">distributional assumptions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.218.1">Empirical percentiles</span></strong><span class="koboSpan" id="kobo.219.1">: Using historical and empirical percentiles, we can construct PIs without </span><a id="_idIndexMarker373"/><span class="No-Break"><span class="koboSpan" id="kobo.220.1">parametric assumptions</span></span></li>
</ul>
<h2 id="_idParaDest-106"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.221.1">Bayesian approaches</span></h2>
<p><span class="koboSpan" id="kobo.222.1">The Bayesian statistical</span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.223.1"> framework provides a probabilistic approach</span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.224.1"> to generating PIs by explicitly modeling different sources of uncertainty. </span><span class="koboSpan" id="kobo.224.2">Two critical techniques for Bayesian PI construction are </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.226.1">Bayesian forecasting models</span></strong><span class="koboSpan" id="kobo.227.1">: Bayesian models provide a probabilistic framework that captures uncertainty in</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.228.1"> parameters and predictions, allowing for the direct calculation of PIs from </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">posterior distributions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.230.1">Monte Carlo Markov Chain (MCMC) sampling</span></strong><span class="koboSpan" id="kobo.231.1">: MCMC sampling can be used to simulate the posterior </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.232.1">distribution of a Bayesian model, enabling the construction </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">of PIs</span></span></li>
</ul>
<h2 id="_idParaDest-107"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.234.1">Machine learning approaches</span></h2>
<p><span class="koboSpan" id="kobo.235.1">The flexibility of modern </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.236.1">machine learning models provides new opportunities for generating PIs in a data-driven manner. </span><span class="koboSpan" id="kobo.236.2">By leveraging techniques tailored for these highly complex and </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.237.1">nonlinear models, valid PIs can be obtained without strict distributional assumptions. </span><span class="koboSpan" id="kobo.237.2">Let’s look at some machine </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">learning approaches:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.239.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.240.1">: Techniques such as</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.241.1"> random forest and gradient boosting machines can create PIs by using the distribution of predictions from individual </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">ensemble members</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.243.1">Neural network quantile regression</span></strong><span class="koboSpan" id="kobo.244.1">: Neural </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.245.1">networks can be trained to predict specific quantiles, forming the basis </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">for PIs</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.247.1">Dropout as a Bayesian approximation</span></strong><span class="koboSpan" id="kobo.248.1">: In deep</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.249.1"> learning, dropout can approximate Bayesian</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.250.1"> inference, allowing</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.251.1"> for UQ and </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">PI construction</span></span></li>
</ul>
<h2 id="_idParaDest-108"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.253.1">Conformal prediction</span></h2>
<p><span class="koboSpan" id="kobo.254.1">As a non-parametric, distribution-free framework, conformal prediction can be integrated with various</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.255.1"> modeling approaches to produce PIs. </span><span class="koboSpan" id="kobo.255.2">Producing PIs for time series forecasting models using conformal prediction</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.256.1"> methods is the main subject of </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">this chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.258.1">Producing PIs is multifaceted, with various approaches tailored to different data types, models, and requirements. </span><span class="koboSpan" id="kobo.258.2">From traditional statistical methods to cutting-edge machine learning techniques and conformal prediction, the field of PI construction is rich and diverse. </span><span class="koboSpan" id="kobo.258.3">Understanding these approaches empowers practitioners to select the most appropriate method for their forecasting application, balancing accuracy, interpretability, computational efficiency, and other considerations. </span><span class="koboSpan" id="kobo.258.4">Whether operating within rigorous parametric assumptions or exploring flexible non-parametric techniques, these methods offer valuable insights into the uncertainty inherent </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">in forecasting.</span></span></p>
<p><span class="koboSpan" id="kobo.260.1">Conformal prediction, a robust framework for generating PIs for point forecasting models, has been widely utilized in time series and forecasting applications. </span><span class="koboSpan" id="kobo.260.2">Numerous studies have chronicled the evolution and popularity of various conformal prediction models for time </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">series forecasting.</span></span></p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.262.1">Conformal prediction for time series and forecasting</span></h1>
<p><span class="koboSpan" id="kobo.263.1">Creating reliable PIs for time series forecasting has been a longstanding, intricate challenge that remained unsolved for years until conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">prediction emerged.</span></span></p>
<p><span class="koboSpan" id="kobo.265.1">This problem was </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.266.1">underscored during the 2018 M4 Forecasting Competition, which necessitated participants to supply PIs and </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">point estimates.</span></span></p>
<p><span class="koboSpan" id="kobo.268.1">In the research paper titled </span><em class="italic"><span class="koboSpan" id="kobo.269.1">Combining Prediction Intervals in the M4 Competition</span></em><span class="koboSpan" id="kobo.270.1">, (https://www.sciencedirect.com/science/article/abs/pii/S0169207019301141), Yael Grushka-Cockayne from the Darden School of Business and Victor Richmond R. </span><span class="koboSpan" id="kobo.270.2">Jose from Harvard Business School </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.271.1">scrutinized 20 interval submissions. </span><span class="koboSpan" id="kobo.271.2">They assessed both the calibration and precision of the predictions and gauged their performances across different time horizons. </span><span class="koboSpan" id="kobo.271.3">Their analysis concluded that the submissions were ineffective in accurately </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">estimating uncertainty.</span></span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.273.1">Ensemble batch PIs (EnbPIs)</span></h2>
<p><em class="italic"><span class="koboSpan" id="kobo.274.1">Conformal Prediction Intervals for Dynamic Time-Series</span></em><span class="koboSpan" id="kobo.275.1"> (</span><a href="http://proceedings.mlr.press/v139/xu21h/xu21h.pdf"><span class="koboSpan" id="kobo.276.1">http://proceedings.mlr.press/v139/xu21h/xu21h.pdf</span></a><span class="koboSpan" id="kobo.277.1">), by researchers Chen Xu and Yao Xie from Georgia Tech University, was </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.278.1">the first paper to implement conformal prediction for time series forecasting </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.279.1">and was presented at the prestigious conference ICML </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">in 2021.</span></span></p>
<p><span class="koboSpan" id="kobo.281.1">EnbPI is currently one of the most popular implementations of conformal prediction for time series forecasting. </span><span class="koboSpan" id="kobo.281.2">It has been implemented in popular open source conformal prediction libraries such as MAPIE, Amazon Fortuna, </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">and PUNCC.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">The study introduces a technique for creating PIs not bound by any specific distribution for dynamic time series data. </span><span class="koboSpan" id="kobo.283.2">The EnbPI method encompasses a bootstrap ensemble estimator to formulate sequential PIs. </span><span class="koboSpan" id="kobo.283.3">Unlike classical conformal prediction methods that require data exchangeability, EnbPI does not require data exchangeability and has been custom-built for </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">time series.</span></span></p>
<p><span class="koboSpan" id="kobo.285.1">The data exchangeability assumption suggests that the sequence in which observations appear in the dataset doesn’t matter. </span><span class="koboSpan" id="kobo.285.2">However, this assumption does not apply to time series, where the sequence of data points is crucial. </span><span class="koboSpan" id="kobo.285.3">EnbPI doesn’t rely on data exchangeability, making it aptly suited for time </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">series analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">PIs generated by EnbPI attain a finite-sample, approximately valid marginal coverage for broad regression functions </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.288.1">and time series under the mild assumption of strongly mixing stochastic errors. </span><span class="koboSpan" id="kobo.288.2">Additionally, EnbPI is computationally efficient and avoids overfitting by not requiring data splitting or training</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.289.1"> multiple ensemble estimators. </span><span class="koboSpan" id="kobo.289.2">It is also scalable to producing arbitrarily many PIs sequentially and is well suited to a wide range of </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">regression functions.</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">Time series data is dynamic and often non-stationary, meaning the statistical properties can change over time. </span><span class="koboSpan" id="kobo.291.2">While various regression functions exist for predicting time series, such as those using boosted trees or neural network structures, these existing methods often need help constructing accurate PIs. </span><span class="koboSpan" id="kobo.291.3">Typically, they can only create reliable intervals by placing restrictive assumptions on the underlying distribution of the time series, which may only sometimes be appropriate </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">or feasible.</span></span></p>
<p><span class="koboSpan" id="kobo.293.1">Here’s a simplified version of the steps to</span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.294.1"> build an </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">EnbPI predictor:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.296.1">Select a bootstrap ensemble estimator</span></strong><span class="koboSpan" id="kobo.297.1">: Any bootstrap ensemble estimator can be used </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">with EnbPI.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.299.1">Train the ensemble estimators</span></strong><span class="koboSpan" id="kobo.300.1">: The base forecasting model is trained multiple times on different bootstrap samples drawn from the original training data to generate the ensemble. </span><span class="koboSpan" id="kobo.300.2">Each bootstrap sample is created by sampling with replacement from the training set. </span><span class="koboSpan" id="kobo.300.3">This results in an ensemble of models with slightly different </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">training data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.302.1">Compute residuals</span></strong><span class="koboSpan" id="kobo.303.1">: For each point in t = 1,…, T, calculate the residuals using ensemble estimators that did not use point </span><em class="italic"><span class="koboSpan" id="kobo.304.1">t</span></em><span class="koboSpan" id="kobo.305.1"> for training. </span><span class="koboSpan" id="kobo.305.2">The aim is to use out-of-sample errors as a nonconformity measure to indicate the variance of predictions. </span><span class="koboSpan" id="kobo.305.3">All such out-of-sample errors are compiled into a </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">single array.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.307.1">Generate predictions</span></strong><span class="koboSpan" id="kobo.308.1">: The ensemble estimator generates point predictions for the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">test data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.310.1">Construct PIs</span></strong><span class="koboSpan" id="kobo.311.1">: The PIs are constructed using the predictions from the ensemble estimator and a chosen significance level. </span><span class="koboSpan" id="kobo.311.2">Like many other conformal prediction methods, a</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.312.1"> quantile with a specified confidence level can be applied to the distribution of out-of-sample errors created in </span><em class="italic"><span class="koboSpan" id="kobo.313.1">step 3</span></em><span class="koboSpan" id="kobo.314.1">. </span><span class="koboSpan" id="kobo.314.2">This quantile value is then used to create PIs </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.315.1">by applying the quantile value to the aggregated </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.316.1">point prediction produced using a trained </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">ensemble estimator.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.318.1">To demonstrate EnbPI in action, we will use Amazon Fortuna (</span><a href="https://aws-fortuna.readthedocs.io/en/latest/index.html"><span class="koboSpan" id="kobo.319.1">https://aws-fortuna.readthedocs.io/en/latest/index.html</span></a><span class="koboSpan" id="kobo.320.1">) and follow its example, </span><em class="italic"><span class="koboSpan" id="kobo.321.1">Time series regression with EnbPI, a conformal prediction method</span></em><span class="koboSpan" id="kobo.322.1"> (</span><a href="https://aws-fortuna.readthedocs.io/en/latest/examples/enbpi_ts_regression.html"><span class="koboSpan" id="kobo.323.1">https://aws-fortuna.readthedocs.io/en/latest/examples/enbpi_ts_regression.html</span></a><span class="koboSpan" id="kobo.324.1">). </span><span class="koboSpan" id="kobo.324.2">You can find the Jupyter notebook, </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">Chapter_08_EnbPI_ipynb.ipynb</span></strong><span class="koboSpan" id="kobo.326.1">, in this book’s GitHub repository: </span><a href="https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_08_EnbPI.ipynb"><span class="koboSpan" id="kobo.327.1">https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_08_EnbPI.ipynb</span></a><span class="koboSpan" id="kobo.328.1">. </span><span class="koboSpan" id="kobo.328.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">get started:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.330.1">First, we will install Amazon Fortuna with </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">pip install</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.333.1">!pip install aws-fortuna</span></pre></li>
<li><span class="koboSpan" id="kobo.334.1">We will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">Bike sharing demand</span></strong><span class="koboSpan" id="kobo.336.1"> dataset, available </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">on scikit-learn:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.338.1">from sklearn.datasets import fetch_openml</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.339.1">bike_sharing = fetch_openml("Bike_Sharing_Demand", version=2, as_frame=True, parser="pandas")</span></pre></li>
<li><span class="koboSpan" id="kobo.340.1">Let’s inspect the </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">dataset header:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.342.1"><img alt="Figure 8.1 – Bike sharing demand dataset" src="image/B19925_08_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.343.1">Figure 8.1 – Bike sharing demand dataset</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.344.1">The dataset contains information about bike-sharing rentals, including additional information</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.345.1"> such as temperature, humidity, and wind speed. </span><span class="koboSpan" id="kobo.345.2">The problem requires forecasting bike sharing demand expressed in the count of </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">rented bikes.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.347.1">We can calculate the</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.348.1"> demand, grouped by weekday and hour, and illustrate the results with the </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">following plot:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.350.1"><img alt="Figure 8.2 – Average hourly bike demand during the week" src="image/B19925_08_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.351.1">Figure 8.2 – Average hourly bike demand during the week</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.352.1">EnbPI requires bootstrapping the data – that is, sampling with replacement random subsets of the time series and training a model for </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">each sample.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.354.1">We can test the </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">DataFrameBootstrapper</span></strong><span class="koboSpan" id="kobo.356.1"> class and look at an example of bootstrapped data samples. </span><span class="koboSpan" id="kobo.356.2">For example, the first bootstrapped sample looks </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">like this:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.358.1"><img alt="Figure 8.3 – Example of a bootstrapped sample" src="image/B19925_08_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.359.1">Figure 8.3 – Example of a bootstrapped sample</span></p>
<ol>
<li value="6"><span class="koboSpan" id="kobo.360.1">We can check for duplicates in </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.361.1">this bootstrapped sample – as bootstrapping is with replacement, as</span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.362.1"> expected, we can see that some objects have been duplicated </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">during bootstrapping:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.364.1"><img alt="Figure 8.4 – Duplicated objects in the bootstrapped sample" src="image/B19925_08_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.365.1">Figure 8.4 – Duplicated objects in the bootstrapped sample</span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.366.1">We can now train the model for each bootstrapped sample. </span><span class="koboSpan" id="kobo.366.2">To evaluate conformal PIs, we can calculate the coverage probability, which measures the percentage of test observations that fall within the generated intervals, and check what proportion of intervals contain the </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">point predictions.</span></span></li>
<li><span class="koboSpan" id="kobo.368.1">Ultimately, we evaluate the dimension of the conformal intervals, which, in this scenario where no online feedback is given, are assumed by EnbPI to be uniform for </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">all intervals.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.370.1">The percentage of intervals containing actual targets is </span><strong class="source-inline"><span class="koboSpan" id="kobo.371.1">0.95</span></strong><span class="koboSpan" id="kobo.372.1">, while the size of the conformal intervals </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">0.4446</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">.</span></span></p></li>
</ol>
<p><span class="koboSpan" id="kobo.376.1">Using EnbPI, we created PIs based on a user-defined coverage of 0.95. </span><span class="koboSpan" id="kobo.376.2">Contrary to most other UQ methods, which often fail to meet the user-specified confidence level, conformal prediction meets user requirements. </span><span class="koboSpan" id="kobo.376.3">It consistently generates PIs that align with the user-defined </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">confidence level.</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">Let’s plot </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">the predictions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.380.1"><img alt="Figure 8.5 – Predictions using EnbPI" src="image/B19925_08_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.381.1">Figure 8.5 – Predictions using EnbPI</span></p>
<p><span class="koboSpan" id="kobo.382.1">The EnbPI model is adept at avoiding </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.383.1">overfitting, ensuring computational efficiency, and is scalable for producing numerous PIs sequentially. </span><span class="koboSpan" id="kobo.383.2">In practice, the EnbPI model creates PIs in line with user-defined confidence levels, ensuring reliability in its predictions. </span><span class="koboSpan" id="kobo.383.3">We have provided practical</span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.384.1"> examples using Amazon Fortuna and the bike-sharing demand dataset from</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.385.1"> scikit-learn, demonstrating the model’s capability to accurately gauge </span><strong class="bold"><span class="koboSpan" id="kobo.386.1">prediction interval coverage probability</span></strong><span class="koboSpan" id="kobo.387.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.388.1">PICP</span></strong><span class="koboSpan" id="kobo.389.1">) and the size of the </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">conformal intervals.</span></span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.391.1">NeuralProphet</span></h2>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.393.1"> is a forecasting framework </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.394.1">built on PyTorch that merges the interpretability of traditional methods with the scalability of deep learning models. </span><span class="koboSpan" id="kobo.394.2">It’s trained using standard deep learning techniques and provides </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.395.1">accurate and interpretable results for various </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">forecasting applications.</span></span></p>
<p><span class="koboSpan" id="kobo.397.1">The framework introduces local context through auto-regression and covariate modules, which can be set up as either classical linear regression or neural networks. </span><span class="koboSpan" id="kobo.397.2">This allows </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.399.1"> to handle short-term forecasting while capturing complex nonlinear relationships between variables. </span><span class="koboSpan" id="kobo.399.2">The auto-regression module models the dependency of the target variable on its past values, while the covariate module addresses its dependence on other </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">known variables.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.402.1"> is designed to be user-friendly, offering reliable defaults and automatic hyperparameters for beginners while allowing experienced users to input domain knowledge through optional model customizations. </span><span class="koboSpan" id="kobo.402.2">As a successor to Facebook Prophet, it retains the foundational components but improves precision </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">and scalability.</span></span></p>
<p><span class="koboSpan" id="kobo.404.1">The essential model components of </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.406.1"> include the trend, seasonality, holidays, auto-regression, and covariate modules. </span><span class="koboSpan" id="kobo.406.2">These additive components can be scaled by the trend for a multiplicative effect. </span><span class="koboSpan" id="kobo.406.3">Each module has its inputs and modeling processes, but all modules </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.407.1">must produce </span><em class="italic"><span class="koboSpan" id="kobo.408.1">h</span></em><span class="koboSpan" id="kobo.409.1"> outputs, where </span><em class="italic"><span class="koboSpan" id="kobo.410.1">h</span></em><span class="koboSpan" id="kobo.411.1"> is the number of steps to be forecasted into the future </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">at once.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.414.1"> incorporates </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.415.1">conformal prediction techniques into its forecasting </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.416.1">workflow, specifically the </span><strong class="bold"><span class="koboSpan" id="kobo.417.1">inductive (split) conformal prediction</span></strong><span class="koboSpan" id="kobo.418.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.419.1">ICP</span></strong><span class="koboSpan" id="kobo.420.1">) approach. </span><span class="koboSpan" id="kobo.420.2">The key steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">as follows:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.422.1">Training set</span></strong><span class="koboSpan" id="kobo.423.1">: This is the initial dataset that’s used to train the forecasting model. </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.425.1"> uses this data to create an </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">initial PI.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.427.1">Calibration set</span></strong><span class="koboSpan" id="kobo.428.1">: This set refines the PIs established from the training set. </span><span class="koboSpan" id="kobo.428.2">After training the model, </span><strong class="source-inline"><span class="koboSpan" id="kobo.429.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.430.1"> evaluates the accuracy of its predictions by comparing the actual target variable values with the </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">predicted outputs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.432.1">Quantifying uncertainty</span></strong><span class="koboSpan" id="kobo.433.1">: The difference between actual and predicted values helps </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.435.1"> measure the uncertainty in its forecasts. </span><span class="koboSpan" id="kobo.435.2">This is a crucial step as understanding this variance is essential for generating </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">precise PIs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.437.1">Final PI formation</span></strong><span class="koboSpan" id="kobo.438.1">: After quantifying the uncertainty using the calibration set, </span><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.440.1"> formulates the final PI. </span><span class="koboSpan" id="kobo.440.2">This interval provides a range within which the actual future values are expected to lie with a predefined </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">confidence level.</span></span></li>
</ol>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.443.1"> integrates conformal prediction methods into its forecasting workflow, particularly employing the ICP strategy. </span><span class="koboSpan" id="kobo.443.2">This approach enables the creation of statistically robust uncertainty sets or intervals for model predictions, enhancing their reliability </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">and confidence.</span></span></p>
<p><span class="koboSpan" id="kobo.445.1">Let’s look at two methodologies that are utilized by </span><strong class="source-inline"><span class="koboSpan" id="kobo.446.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.447.1"> within the conformal prediction framework to </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">establish PIs:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.449.1">Quantile regression</span></strong><span class="koboSpan" id="kobo.450.1">: This technique enables the algorithm to learn only specific quantiles of output variables for </span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.451.1">each instance. </span><span class="koboSpan" id="kobo.451.2">By default, </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.453.1"> provides a singular output as a point estimate for each instance, calculated based on a</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.454.1"> 50th-percentile regression. </span><span class="koboSpan" id="kobo.454.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.456.1"> object requires at least one upper and lower quantile pair as parameters to create a PI. </span><span class="koboSpan" id="kobo.456.2">For instance, for a 90% probability of the actual value falling within the estimated interval, the confidence level is set at 0.9, defining two quantiles at 0.05 and 0.95, corresponding to the 5th and 95th percentiles of the </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">forecast distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.458.1">Conformal prediction</span></strong><span class="koboSpan" id="kobo.459.1">: This method introduces a calibration process to the existing model to ascertain uncertainties in</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.460.1"> point estimators and PIs. </span><span class="koboSpan" id="kobo.460.2">Post creation and splitting the data into training and calibration sets for a </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.462.1"> model, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">conformal_predict</span></strong><span class="koboSpan" id="kobo.464.1"> method can be utilized to produce a conformal forecast. </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.466.1"> uses two variants </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.467.1">of UQ – naïve conformal prediction and </span><strong class="bold"><span class="koboSpan" id="kobo.468.1">conformalized quantile regression</span></strong><span class="koboSpan" id="kobo.469.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.470.1">CQR</span></strong><span class="koboSpan" id="kobo.471.1">), which we looked at in </span><a href="B19925_07.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.472.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.473.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.474.1">To demonstrate how </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.476.1"> can create PIs using conformal prediction, we will follow the notebook at https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_08_NeuralProphet.ipynb, which is based on UQ in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.478.1"> tutorial (</span><a href="https://neuralprophet.com/how-to-guides/feature-guides/uncertainty_quantification.html"><span class="No-Break"><span class="koboSpan" id="kobo.479.1">https://neuralprophet.com/how-to-guides/feature guides/uncertainty_quantification.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.480.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.481.1">The dataset uses hospital electric load data from a San Francisco hospital electric load </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">dataset (</span></span><a href="https://github.com/ourownstory/neuralprophet-data"><span class="No-Break"><span class="koboSpan" id="kobo.483.1">https://github.com/ourownstory/neuralprophet-data</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.484.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.485.1">Let’s look at the header of </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.487.1"><img alt="Figure 8.6 – San Francisco hospital load dataset" src="image/B19925_08_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.488.1">Figure 8.6 – San Francisco hospital load dataset</span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.490.1"> requires the </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.491.1">data in the specific format with a time column named </span><em class="italic"><span class="koboSpan" id="kobo.492.1">ds</span></em><span class="koboSpan" id="kobo.493.1"> and time series values in a column </span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">called </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.495.1">y</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">Let’s create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.499.1"> object that specifies a data splitting ratio between 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">and 1:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.501.1">m = NeuralProphet()</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.502.1">train_df, test_df = m.split_df(df, freq="H", valid_p=1.0 / 16)</span></pre>
<p><span class="koboSpan" id="kobo.503.1">By default, </span><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.505.1">’s forecasting provides a singular output: a point estimate for each instance. </span><span class="koboSpan" id="kobo.505.2">This estimate is derived from a 50th percentile regression. </span><span class="koboSpan" id="kobo.505.3">A </span><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.507.1"> object requires</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.508.1"> at least one upper and lower quantile pair as its parameters to establish a PI. </span><span class="koboSpan" id="kobo.508.2">Yet, within a </span><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.510.1"> model, we can define multiple quantiles </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">as desired.</span></span></p>
<p><span class="koboSpan" id="kobo.512.1">For example, say we want to forecast a hospital’s electric load with 90% PIs. </span><span class="koboSpan" id="kobo.512.2">We want 90% of the actual values to fall within the </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">generated intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.514.1">We could train a quantile regression model to predict three quantiles – the 5th, 50th, and 95th percentiles. </span><span class="koboSpan" id="kobo.514.2">The 5th and 95th quantiles would provide the lower and upper bounds for 90% PIs. </span><span class="koboSpan" id="kobo.514.3">The 50th quantile would provide the median </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">point forecast:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.516.1">confidence_lv = 0.9</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.517.1">quantile_list = [round(((1 - confidence_lv) / 2), 2), round((confidence_lv + (1 - confidence_lv) / 2), 2)]</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.518.1">qr_model = NeuralProphet(quantiles=quantile_list)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.519.1">qr_model.set_plotting_backend("plotly-static")</span></pre>
<p><span class="koboSpan" id="kobo.520.1">Quantile regression is used in </span><strong class="source-inline"><span class="koboSpan" id="kobo.521.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.522.1"> to generate PIs. </span><span class="koboSpan" id="kobo.522.2">It trains the model using a specialized loss function called pinball loss, also </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.523.1">known as </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">quantile loss.</span></span></p>
<p><span class="koboSpan" id="kobo.525.1">Unlike simple error minimization, pinball loss weights errors asymmetrically based on the quantile. </span><span class="koboSpan" id="kobo.525.2">Under-prediction is penalized more heavily than over-prediction for an upper quantile such as 90%. </span><span class="koboSpan" id="kobo.525.3">The opposite is valid for a lower quantile, such </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">as 10%.</span></span></p>
<p><span class="koboSpan" id="kobo.527.1">This matches the inherent meaning of the quantile – 90% indicates we expect 90% of the actual values to lie below the prediction. </span><span class="koboSpan" id="kobo.527.2">So, errors where the actual value exceeds the forecast violate that </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">more significantly.</span></span></p>
<p><span class="koboSpan" id="kobo.529.1">By minimizing the asymmetric </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.530.1">pinball loss during training, the model learns quantile lines that reflect the appropriate probability of actual values </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.531.1">falling above or below based on the data. </span><span class="koboSpan" id="kobo.531.2">The upper and lower quantiles then form </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">the PI.</span></span></p>
<p><span class="koboSpan" id="kobo.533.1">We can now fit the model and create a DataFrame with the results, forecasting </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">30 periods:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.535.1">metrics = qr_model.fit(df, freq="H")</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.536.1">future = qr_model.make_future_dataframe(df, periods=30, n_historic_predictions=100)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.537.1">forecast = qr_model.predict(df=future)</span></pre>
<p><span class="koboSpan" id="kobo.538.1">We can visualize the PIs from quantile regression using a plot. </span><span class="koboSpan" id="kobo.538.2">The solid line shows the median forecast, while the shaded area depicts the interval between the lower and upper quantile lines. </span><span class="koboSpan" id="kobo.538.3">This represents the range expected to contain the actual values with the specified </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">confidence level:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.540.1"><img alt="Figure 8.7 – Forecasting hospital electric load using quantile regression" src="image/B19925_08_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.541.1">Figure 8.7 – Forecasting hospital electric load using quantile regression</span></p>
<p><span class="koboSpan" id="kobo.542.1">In summary, quantile regression allows </span><strong class="source-inline"><span class="koboSpan" id="kobo.543.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.544.1"> to generate PIs by training the model to forecast </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.545.1">quantiles that serve as interval bounds. </span><span class="koboSpan" id="kobo.545.2">The pinball loss function enables </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">quantile-based UQ.</span></span></p>
<p><span class="koboSpan" id="kobo.547.1">Quantile regression relies </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.548.1">on modeling assumptions and requires specifying the quantiles of interest. </span><span class="koboSpan" id="kobo.548.2">Next, we’ll explore how </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.550.1"> can produce distribution-free PIs using conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">prediction techniques.</span></span></p>
<h3><span class="koboSpan" id="kobo.552.1">Conformal prediction in NeuralProphet</span></h3>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.554.1"> employs the split conformal prediction method. </span><span class="koboSpan" id="kobo.554.2">This approach necessitates a holdout or calibration set. </span><span class="koboSpan" id="kobo.554.3">The </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.555.1">dataset must be divided into three sets to execute split conformal prediction: training, calibration, and testing. </span><span class="koboSpan" id="kobo.555.2">An initial PI is established using the model trained on the training dataset. </span><span class="koboSpan" id="kobo.555.3">Uncertainty is then gauged by </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.556.1">comparing the calibration set’s target variables with the predicted values. </span><span class="koboSpan" id="kobo.556.2">This quantified uncertainty</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.557.1"> is subsequently incorporated into both ends of the prediction value to form the final </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">conformal PI.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">Within </span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.561.1">, you can choose either the naïve (based on the absolute residual) or CQR for </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">conformal prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.563.1">Add a calibration set using the data </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">splitting function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.565.1">train_df, cal_df = m.split_df(train_df, freq="H", valid_p=1.0 / 11)</span></pre>
<p><span class="koboSpan" id="kobo.566.1">You can build any </span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.568.1"> model you deem fit as the base model. </span><span class="koboSpan" id="kobo.568.2">The calibration process in conformal prediction would be later added to the base model to quantify the uncertainty in our final estimation. </span><span class="koboSpan" id="kobo.568.3">We are interested in knowing how conformal prediction affects </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">different models.</span></span></p>
<p><span class="koboSpan" id="kobo.570.1">In our example, we will compare the </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.571.1">conformal prediction results between a simple quantile regression and a</span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.572.1"> complex four-layer autoregression model in </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">our illustration.</span></span></p>
<p><span class="koboSpan" id="kobo.574.1">We will specify 72 hours as lags and create a simple quantile regression model as base model 1. </span><span class="koboSpan" id="kobo.574.2">We will also create </span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.575.1">a four-layer autoregression model as base </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">model 2:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.577.1">n_lags = 3 * 24</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.578.1">cp_model1 = NeuralProphet(quantiles=quantile_list)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.579.1">cp_model1.set_plotting_backend("plotly-static")</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.580.1">cp_model2 = NeuralProphet(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.581.1">    yearly_seasonality=False,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.582.1">    weekly_seasonality=False,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.583.1">    daily_seasonality=False,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.584.1">    n_lags=n_lags,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.585.1">    ar_layers=[32, 32, 32, 32],</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.586.1">    learning_rate=0.003,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.587.1">    quantiles=quantile_list,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.588.1">)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.589.1">cp_model2.set_plotting_backend("plotly-static")</span></pre>
<p><span class="koboSpan" id="kobo.590.1">After configuring the model, we must fit the model with the train set. </span><span class="koboSpan" id="kobo.590.2">Suppose you have further split the training dataset into training and validation. </span><span class="koboSpan" id="kobo.590.3">In that case, you can either concatenate the two datasets in one dataset for training or assign the training and validation datasets as two </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">separate parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.592.1">Feed the training subset in the</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.593.1"> configured </span><strong class="source-inline"><span class="koboSpan" id="kobo.594.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.595.1"> models. </span><span class="koboSpan" id="kobo.595.2">Then, configure the hourly frequency </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.596.1">by assigning </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">H</span></strong><span class="koboSpan" id="kobo.598.1"> to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">freq</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.600.1"> parameter:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.601.1">set_random_seed(0)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.602.1">metrics1 = cp_model1.fit(train_df, freq="H")</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.603.1">set_random_seed(0)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.604.1">metrics2 = cp_model2.fit(train_df, freq="H")</span></pre>
<p><span class="koboSpan" id="kobo.605.1">Let’s use the fitted base model to forecast both the point prediction and the quantile regression PIs for the </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">testing </span></span><span class="No-Break"><a id="_idIndexMarker428"/></span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.608.1">forecast1 = cp_model1.predict(test_df)[n_lags:]</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.609.1">forecast2 = cp_model2.predict(test_df)[n_lags:]</span></pre>
<h3><span class="koboSpan" id="kobo.610.1">Option 1 – naïve conformal prediction</span></h3>
<p><span class="koboSpan" id="kobo.611.1">After training the base model, we</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.612.1"> can carry out the calibration process using the naïve module. </span><span class="koboSpan" id="kobo.612.2">The steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.614.1">Predict the output value of the instances within the </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">calibration set.</span></span></li>
<li><span class="koboSpan" id="kobo.616.1">Calculate the absolute residual by comparing the actual and predicted values for each observation in the </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">calibration set.</span></span></li>
<li><span class="koboSpan" id="kobo.618.1">Sort all residuals in </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">ascending order.</span></span></li>
<li><span class="koboSpan" id="kobo.620.1">Find the quantile of the distribution of the absolute residuals with the desired </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">confidence level.</span></span></li>
<li><span class="koboSpan" id="kobo.622.1">Use the quantile of the distribution of the absolute residuals to make the </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">final PIs.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.624.1">Returning to our example, we need to denote the parameter value for the calibration set and the significant level (alpha) for</span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.625.1"> conformal prediction on top of our </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">pre-trained models:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.627.1">Method = ""aïve"</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.628.1">alpha–= 1 - confidence_lv</span></pre>
<p><span class="koboSpan" id="kobo.629.1">We can now enable conformal prediction using </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">pre-trained models.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.631.1">naïve_forecast1 = cp_model1.conformal_predict(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.632.1">    test_df,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.633.1">    calibration_df=cal_df,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.634.1">    alpha=alpha,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.635.1">    method=method,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.636.1">    plotting_backend="plotly-static",</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.637.1">    show_all_PI=naïvee,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.638.1">)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.639.1">naive_forecast2 = cp_model2.conformal_predict(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.640.1">    test_df,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.641.1">    calibration_df=cal_df,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.642.1">    alpha=alpha,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.643.1">    method=method,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.644.1">    plotting_backend""plotly-static",</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.645.1">    show_all_PI=True,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.646.1">)</span></pre>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.647.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.648.1"> can plot the one-sided interval width versus the selected </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">confidence level:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.650.1"><img alt="Figure 8.8 – The one-side interval width versus the confidence level" src="image/B19925_08_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.651.1">Figure 8.8 – The one-side interval width versus the confidence level</span></p>
<p><span class="koboSpan" id="kobo.652.1">This plot demonstrates </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.653.1">how the width of the PI changes with different confidence </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">levels (1-alpha).</span></span></p>
<h3><span class="koboSpan" id="kobo.655.1">Option 2 – CQR</span></h3>
<p><span class="koboSpan" id="kobo.656.1">CQR operates in the following</span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.657.1"> manner within the </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">CQR module:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.659.1">Non-conformity scores are computed as the disparities between data points from the calibration dataset and their closest prediction quantile. </span><span class="koboSpan" id="kobo.659.2">These scores offer insight into the fit of the data to the existing quantile regression model. </span><span class="koboSpan" id="kobo.659.3">Data points within the quantile regression interval yield negative non-conformity scores, while those outside the interval produce </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">positive scores.</span></span></li>
<li><span class="koboSpan" id="kobo.661.1">The non-conformity scores are then organized </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">in order.</span></span></li>
<li><span class="koboSpan" id="kobo.663.1">The alpha value is determined so that a fraction of the scores greater than alpha matches the </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">error rate.</span></span></li>
<li><span class="koboSpan" id="kobo.665.1">An amount of alpha adjusts the regression </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">model’s quantiles.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.667.1">Based on alpha’s value, the CQR model can be interpreted in </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">two ways.</span></span></p>
<p><span class="koboSpan" id="kobo.669.1">When the one-sided PI width adjustment is positive, the CQR expands beyond the QR intervals. </span><span class="koboSpan" id="kobo.669.2">This suggests that the CQR perceives the QR interval as </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">overly confident.</span></span></p>
<p><span class="koboSpan" id="kobo.671.1">On the other hand, if the adjustment is negative, the CQR narrows the QR intervals, indicating that the QR interval might be </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">overly cautious.</span></span></p>
<p><span class="koboSpan" id="kobo.673.1">We can run the CQR option using the </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.675.1">method = "cqr"</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.676.1">cqr_forecast1 = cp_model1.conformal_predict(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.677.1">    test_df, calibration_df=cal_df, alpha=alpha, method=method, plotting_backend="plotly-static"</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.678.1">)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.679.1">cqr_forecast2 = cp_model2.conformal_predict(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.680.1">    test_df, calibration_df=cal_df, alpha=alpha, method=method, plotting_backend="plotly-static"</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.681.1">)</span></pre>
<p><span class="koboSpan" id="kobo.682.1">Again, we can plot the PIs to</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.683.1"> examine how this CQR method affects </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">the result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.685.1"><img alt="Figure 8.9 – Forecasting hospital electric load using quantile regression (the CQR option)" src="image/B19925_08_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.686.1">Figure 8.9 – Forecasting hospital electric load using quantile regression (the CQR option)</span></p>
<p><span class="koboSpan" id="kobo.687.1">We can see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.688.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.689.1"> has produced excellent PIs using conformal prediction with the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.690.1">cqr</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.691.1"> option.</span></span></p>
<p><span class="koboSpan" id="kobo.692.1">Now, let’s learn how to evaluate performance and obtain some insights by comparing various UQ methods we </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">have used.</span></span></p>
<p><span class="koboSpan" id="kobo.694.1">We are using the interval width and miscoverage rate as the </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">performance metrics:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.696.1">interval_width</span></strong><span class="koboSpan" id="kobo.697.1">: This is the average PI, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.698.1">q_hat</span></strong><span class="koboSpan" id="kobo.699.1">, multiplied by two because it is static or non-adaptive; this is also</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.700.1"> known as the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.701.1">efficiency metric</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">miscoverage_rate</span></strong><span class="koboSpan" id="kobo.703.1">: This is the actual miscoverage error rate on the OOS test set; this is also </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.704.1">known as the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.705.1">validity metric</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.706.1">Let’s evaluate the models we trained earlier. </span><span class="koboSpan" id="kobo.706.2">Based on the results in the notebook, we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">following conclusions:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.708.1">Quantile regression </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.709.1">does not provide the </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">required coverage</span></span></li>
<li><span class="koboSpan" id="kobo.711.1">The more complex the underlying model, the more accurate it is, hence the lower </span><em class="italic"><span class="koboSpan" id="kobo.712.1">interval width</span></em><span class="koboSpan" id="kobo.713.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">CP intervals</span></span></li>
<li><span class="koboSpan" id="kobo.715.1">For the default model, CQR outputs a narrower </span><em class="italic"><span class="koboSpan" id="kobo.716.1">PI width</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.717.1">than naïve</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.718.1">Conformal prediction with Nixtla</span></h3>
<p><span class="koboSpan" id="kobo.719.1">We will use the notebook at https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_08_NixtlaStatsforecast.ipynb to illustrate how to use conformal prediction to </span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.720.1">create PIs for popular statistic and</span><a id="_idIndexMarker438"/> <span class="No-Break"><span class="koboSpan" id="kobo.721.1">econometrics models.</span></span></p>
<p><span class="koboSpan" id="kobo.722.1">We will use the hourly dataset from the </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">M4 Competition:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.724.1">First, let’s install </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">Nixtla’s </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.726.1">statsforecast</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.728.1">!pip install git+https://github.com/Nixtla/statsforecast.git</span></pre></li>
<li><span class="koboSpan" id="kobo.729.1">Then, we must import the necessary modules, including specifically </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">Nixtla’s modules:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.731.1">from statsforecast.models import SeasonalExponentialSmoothing, ADIDA, and ARIMA</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.732.1">from statsforecast.utils import ConformalIntervals</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.733.1">import matplotlib.pyplot as plt</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.734.1">from statsforecast.models import (</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.735.1">    AutoETS,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.736.1">    HistoricAverage,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.737.1">    Naive,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.738.1">    RandomWalkWithDrift,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.739.1">    SeasonalNaive</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.740.1">)</span></pre></li>
<li><span class="koboSpan" id="kobo.741.1">Next, we must</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.742.1"> load the train </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.743.1">and </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">test datasets:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.745.1">train = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.746.1">test = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv'</span></pre></li>
<li><span class="koboSpan" id="kobo.747.1">Let’s look at the dataset’s structure. </span><span class="koboSpan" id="kobo.747.2">Similar to </span><strong class="source-inline"><span class="koboSpan" id="kobo.748.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.749.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">statsforecast</span></strong><span class="koboSpan" id="kobo.751.1"> requires columns to be named in a </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">specific way:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.753.1"><img alt="Figure 8.10 – Hourly dataset from the M4 Competition" src="image/B19925_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.754.1">Figure 8.10 – Hourly dataset from the M4 Competition</span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.755.1">We can now train</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.756.1"> the models; we will only use the first</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.757.1"> eight series of the dataset to reduce the total </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">computational time:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.759.1">n_series = 8</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.760.1">uids = train['unique_id'].unique()[:n_series]</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.761.1">train = train.query('unique_id in @uids')test = test.query('unique_id in @uids')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.762.1">Now, we can plot </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">the series:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.764.1">StatsForecast.plot(train, test, plot_random = False)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.765.1">We will get the </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">following output:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.767.1"><img alt="Figure 8.11 – Hourly series from the M4 Competition" src="image/B19925_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.768.1">Figure 8.11 – Hourly series from the M4 Competition</span></p>
<ol>
<li value="6"><span class="koboSpan" id="kobo.769.1">Let’s create a list of</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.770.1"> models and instantiation parameters. </span><span class="koboSpan" id="kobo.770.2">To use these models, we need to import them from </span><strong class="source-inline"><span class="koboSpan" id="kobo.771.1">statsforecast.models</span></strong><span class="koboSpan" id="kobo.772.1"> and then instantiate them. </span><span class="koboSpan" id="kobo.772.2">Given that we’re working with hourly data, we need</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.773.1"> to set </span><strong class="source-inline"><span class="koboSpan" id="kobo.774.1">seasonal_length=24</span></strong><span class="koboSpan" id="kobo.775.1"> in the models that require </span><span class="No-Break"><span class="koboSpan" id="kobo.776.1">this parameter:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.777.1">models = [</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.778.1">    AutoETS(season_length=24),</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.779.1">    HistoricAverage(),</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.780.1">    Naive(),</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.781.1">    RandomWalkWithDrift(),</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.782.1">    SeasonalNaive(season_length=24)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.783.1">]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.784.1">To instantiate a new </span><strong class="source-inline"><span class="koboSpan" id="kobo.785.1">StatsForecast</span></strong><span class="koboSpan" id="kobo.786.1"> object, we need the </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">following parameters:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.788.1">df</span></strong><span class="koboSpan" id="kobo.789.1">: The DataFrame that contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">training data.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.791.1">models</span></strong><span class="koboSpan" id="kobo.792.1">: The list of models defined in the </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">previous step.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.794.1">freq</span></strong><span class="koboSpan" id="kobo.795.1">: A string indicating the frequency of the data. </span><span class="koboSpan" id="kobo.795.2">See pandas’ </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">available frequencies.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.797.1">n_jobs</span></strong><span class="koboSpan" id="kobo.798.1">: An integer that indicates the number of jobs used in parallel processing. </span><span class="koboSpan" id="kobo.798.2">Use </span><strong class="source-inline"><span class="koboSpan" id="kobo.799.1">-1</span></strong><span class="koboSpan" id="kobo.800.1"> to select </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">all cores:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.802.1">sf = StatsForecast(</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.803.1">    df=train,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.804.1">    models=models,</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.805.1">    freq='H',</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.806.1">    n_jobs=-1</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.807.1">)</span></pre></li></ul></li>
<li><span class="koboSpan" id="kobo.808.1">Now, we’re ready</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.809.1"> to generate the point forecasts and the PIs. </span><span class="koboSpan" id="kobo.809.2">To do</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.810.1"> this, we’ll use the forecast method, which takes </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">two arguments:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">h</span></strong><span class="koboSpan" id="kobo.813.1">: An integer that represents the forecasting horizon. </span><span class="koboSpan" id="kobo.813.2">In this case, we’ll forecast the next </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">48 hours.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.815.1">level</span></strong><span class="koboSpan" id="kobo.816.1">: A list of floats with the confidence levels of the PIs. </span><span class="koboSpan" id="kobo.816.2">For example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.817.1">level=[95]</span></strong><span class="koboSpan" id="kobo.818.1"> means that the range of values should include the actual future value with a probability </span><span class="No-Break"><span class="koboSpan" id="kobo.819.1">of 95%:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.820.1">levels = [80, 90, 95, 99] # confidence levels of the prediction intervals</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.821.1">forecasts = sf.forecast(h=48, level=levels)</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.822.1">forecasts = forecasts.reset_index()</span></pre><pre class="source-code"><span class="koboSpan" id="kobo.823.1">forecasts.head()</span></pre></li></ul></li>
<li><span class="koboSpan" id="kobo.824.1">Now, we can plot </span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">the PIs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.826.1">sf.plot(train, test, plot_random = False, models=['SeasonalNaive'], level=levels)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.827.1">Here’s the plot </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">for this:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.829.1"><img alt="Figure 8.12 – Forecasting hourly series with a seasonal naïve benchmark" src="image/B19925_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.830.1">Figure 8.12 – Forecasting hourly series with a seasonal naïve benchmark</span></p>
<p><span class="koboSpan" id="kobo.831.1">Multi-quantile losses and</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.832.1"> statistical models can provide PIs. </span><span class="koboSpan" id="kobo.832.2">Still, the problem is that these are uncalibrated, meaning that the actual frequency of observations falling</span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.833.1"> within the interval does not align with its confidence level. </span><span class="koboSpan" id="kobo.833.2">For example, a calibrated 95% PI should contain the actual value in repeated sampling 95% of the time. </span><span class="koboSpan" id="kobo.833.3">On the other hand, an uncalibrated 95% PI might contain the true value only 80% of the time or 99% of the time. </span><span class="koboSpan" id="kobo.833.4">In the first case, the interval is too narrow and underestimates the uncertainty, while in the second case, it is too broad and overestimates </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">the uncertainty.</span></span></p>
<p><span class="koboSpan" id="kobo.835.1">Statistical methods also often assume normality. </span><span class="koboSpan" id="kobo.835.2">Here, we calibrated PIs produced by statistical models using conformal prediction. </span><span class="koboSpan" id="kobo.835.3">Conformal PIs use cross-validation on a point forecaster model to generate the intervals. </span><span class="koboSpan" id="kobo.835.4">No prior probabilities are needed, and the output is well calibrated. </span><span class="koboSpan" id="kobo.835.5">No additional training is required, and the model is treated as a black box. </span><span class="koboSpan" id="kobo.835.6">The approach is compatible with any model. </span><strong class="source-inline"><span class="koboSpan" id="kobo.836.1">Statsforecast</span></strong><span class="koboSpan" id="kobo.837.1"> now supports conformal prediction on all </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">available models.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.839.1">StatsForecast</span></strong><span class="koboSpan" id="kobo.840.1"> can train multiple models on different time series efficiently. </span><span class="koboSpan" id="kobo.840.2">These models can generate probabilistic forecasts, producing point forecasts and PIs. </span><span class="koboSpan" id="kobo.840.3">For this example, we’ll use </span><strong class="source-inline"><span class="koboSpan" id="kobo.841.1">SimpleExponentialSmoothing</span></strong><span class="koboSpan" id="kobo.842.1"> and ADIDA (a model for intermittent demand), which do not provide a PI natively. </span><span class="koboSpan" id="kobo.842.2">Thus, using conformal prediction to generate the PI makes sense. </span><span class="koboSpan" id="kobo.842.3">We’ll also show how to use it with ARIMA to provide PIs that don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">assume </span></span><span class="No-Break"><a id="_idIndexMarker449"/></span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">normality.</span></span></p>
<p><span class="koboSpan" id="kobo.845.1">To use these models, we</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.846.1"> first need to import them from </span><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">statsforecast.models</span></strong><span class="koboSpan" id="kobo.848.1">, after which we need to instantiate them, </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.850.1">intervals = ConformalIntervals(h=24, n_windows=2)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.851.1">models = [</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.852.1">   SeasonalExponentialSmoothing(season_length=24,alpha=0.1, prediction_intervals=intervals),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.853.1">    ADIDA(prediction_intervals=intervals),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.854.1">    ARIMA(order=(24,0,12), season_length=24, prediction_intervals=intervals),</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.855.1">]</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.856.1">sf = StatsForecast(</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.857.1">    df=train,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.858.1">    models=models,</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.859.1">    freq='H',</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.860.1">)</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.861.1">levels = [80, 90] # confidence levels of the prediction intervals</span></pre>
<pre class="source-code"><span class="koboSpan" id="kobo.862.1">forecasts = sf.forecast(h=24, level=levels)</span></pre>
<p><span class="koboSpan" id="kobo.863.1">Let’s plot the forecasts that are produced using conformal prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">for ARIMA:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<span class="koboSpan" id="kobo.865.1"><img alt="Figure 8.13 – PIs produced for ARIMA using conformal prediction" src="image/B19925_08_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.866.1">Figure 8.13 – PIs produced for ARIMA using conformal prediction</span></p>
<p><span class="koboSpan" id="kobo.867.1">This section explored implementations of conformal prediction for time series forecasting in several popular open </span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">source libraries.</span></span></p>
<p><span class="koboSpan" id="kobo.869.1">Amazon Fortuna provides conformal </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.870.1">prediction capabilities through its EnbPI module. </span><span class="koboSpan" id="kobo.870.2">This allows us to generate non-parametric PIs by wrapping any ensemble model with bootstrap resampling. </span><span class="koboSpan" id="kobo.870.3">We</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.871.1"> saw how EnbPI leverages an ensemble to approximate the forecast distribution </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">without assumptions.</span></span></p>
<p><span class="koboSpan" id="kobo.873.1">Nixtla, an open source library for time series modeling, includes conformal prediction functions for forecasting tasks. </span><span class="koboSpan" id="kobo.873.2">We examined how its CP module can take any underlying model and add conformal PIs. </span><span class="koboSpan" id="kobo.873.3">Nixtla also supports online conformal prediction for </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">adaptive intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.875.1">Finally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.876.1">NeuralProphet</span></strong><span class="koboSpan" id="kobo.877.1"> natively integrates conformal prediction and quantile regression to quantify uncertainty. </span><span class="koboSpan" id="kobo.877.2">We examined its ICP approach, which uses a calibration set to refine initial intervals. </span><span class="koboSpan" id="kobo.877.3">This generates valid prediction regions without relying on </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">distributional assumptions.</span></span></p>
<p><span class="koboSpan" id="kobo.879.1">By incorporating conformal prediction, these libraries make robust and accessible UQ available to time series forecasters in Python. </span><span class="koboSpan" id="kobo.879.2">The diversity of implementations demonstrates the flexibility of conformal prediction as a model-agnostic framework that can be applied to any </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">forecasting method.</span></span></p>
<h1 id="_idParaDest-112"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.881.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.882.1">This chapter taught you how to apply conformal prediction to time series forecasting. </span><span class="koboSpan" id="kobo.882.2">Conformal prediction is a powerful technique for crafting PIs for point </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">forecasting models.</span></span></p>
<p><span class="koboSpan" id="kobo.884.1">This chapter also offered insights into how to harness this method using open </span><span class="No-Break"><span class="koboSpan" id="kobo.885.1">source platforms.</span></span></p>
<p><span class="koboSpan" id="kobo.886.1">We began by exploring UQ in a time series, delving into the significance of PIs, and showcasing various strategies to </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">generate them.</span></span></p>
<p><span class="koboSpan" id="kobo.888.1">The concept of conformal prediction and its application in forecasting scenarios was central to this chapter. </span><span class="koboSpan" id="kobo.888.2">At this point, you are equipped with the knowledge to apply these methodologies in real-world settings, empowering your forecasting models with precise uncertainty bounds. </span><span class="koboSpan" id="kobo.888.3">Adding confidence measures to predictions ensures that the forecasts are accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">and reliable.</span></span></p>
<p><span class="koboSpan" id="kobo.890.1">With a solid understanding of conformal prediction for time series, we will now focus on another critical application area – </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">computer vision.</span></span></p>
</div>
</body></html>