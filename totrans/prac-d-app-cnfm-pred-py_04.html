<html><head></head><body>
<div id="_idContainer017">
<h1 class="chapter-number" id="_idParaDest-42"><a id="_idTextAnchor040"/><span class="koboSpan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-43"><a id="_idTextAnchor041"/><span class="koboSpan" id="kobo.2.1">Validity and Efficiency of Conformal Prediction</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will dive deeper into the concepts of validity and efficiency in the context of probabilistic prediction models, building upon the foundations laid in the </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">previous chapters.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Validity and efficiency are crucial aspects that ensure the practicality and robustness of prediction models across a wide range of industry applications. </span><span class="koboSpan" id="kobo.5.2">Understanding these concepts and their implications will enable you to develop unbiased and high-performing models that can reliably support decision-making and risk </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">assessment processes.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we will explore the definitions, metrics, and examples of valid and efficient models and discuss the automatic validity guarantees provided by </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">conformal prediction</span></strong><span class="koboSpan" id="kobo.9.1">, a cutting-edge approach to uncertainty quantification. </span><span class="koboSpan" id="kobo.9.2">By the end of this chapter, you will be equipped with the knowledge necessary to assess and improve the validity and efficiency of your predictive models, paving the way for more reliable and effective applications in your </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">respective fields.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">The validity of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">probabilistic predictors</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">The efficiency of </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">probabilistic predictors</span></span></li>
</ul>
<h1 id="_idParaDest-44"><a id="_idTextAnchor042"/><span class="koboSpan" id="kobo.17.1">The validity of probabilistic predictors</span></h1>
<p><span class="koboSpan" id="kobo.18.1">We start by </span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.19.1">summarizing the reasons why unbiased point prediction models are important across various domains </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">and applications:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.21.1">Accuracy and reliability</span></strong><span class="koboSpan" id="kobo.22.1">: An unbiased model ensures that the predictions it generates are accurate and reliable on average, meaning that the model is neither systematically overestimating nor underestimating the true values. </span><span class="koboSpan" id="kobo.22.2">This accuracy is crucial for making well-informed decisions, minimizing risks, and improving the overall </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.23.1">performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">a system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.25.1">Trust and credibility</span></strong><span class="koboSpan" id="kobo.26.1">: Unbiased prediction models help build trust and credibility among stakeholders, as they provide a reliable basis for decision-making. </span><span class="koboSpan" id="kobo.26.2">Users can have more confidence in the outputs generated by an unbiased model, knowing that it is not skewed or favoring any </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">specific outcome.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.28.1">Fairness and equity</span></strong><span class="koboSpan" id="kobo.29.1">: In some applications, such as finance, healthcare, and human resources, unbiased models are essential to ensure fairness and equity among different groups or individuals. </span><span class="koboSpan" id="kobo.29.2">Biased models can inadvertently reinforce existing inequalities or create new ones, leading to unfair treatment or allocation </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">of resources.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.31.1">Generalizability</span></strong><span class="koboSpan" id="kobo.32.1">: Unbiased models are more likely to generalize well to new, unseen data because they accurately represent the underlying relationships in the data. </span><span class="koboSpan" id="kobo.32.2">In contrast, biased models may perform poorly when applied to new data or different conditions, leading to unexpected errors or </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">suboptimal outcomes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.34.1">Regulatory compliance</span></strong><span class="koboSpan" id="kobo.35.1">: In certain industries, unbiased models are a requirement for regulatory compliance. </span><span class="koboSpan" id="kobo.35.2">For example, in finance, healthcare, and insurance, models must be free of bias to meet regulatory standards and ensure that customers are treated fairly and risks are </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">managed effectively.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.37.1">In the context of conformal prediction, validity refers to the ability of a prediction model to provide accurate, reliable, and unbiased estimates of the uncertainty associated with its predictions. </span><span class="koboSpan" id="kobo.37.2">More specifically, a valid conformal predictor generates prediction intervals that contain the true values of the target variable with a predefined coverage probability, ensuring that the model’s uncertainty quantification is reliable and </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">well calibrated.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">The significance of validity in conformal prediction can be understood from </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">various perspectives:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.41.1">Confidence in </span></strong><strong class="bold"><a id="_idIndexMarker120"/></strong><strong class="bold"><span class="koboSpan" id="kobo.42.1">predictions</span></strong><span class="koboSpan" id="kobo.43.1">: Valid conformal predictors allow users to have confidence in the prediction intervals they generate, as they know that these intervals truly reflect the uncertainty in the predictions. </span><span class="koboSpan" id="kobo.43.2">For instance, if a conformal predictor produces a 95% prediction interval for a certain data point, users can trust that</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.44.1"> there is a 95% probability that the true value lies within that interval. </span><span class="koboSpan" id="kobo.44.2">This confidence is crucial for decision-making and risk management in </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">various applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Robustness to model misspecification</span></strong><span class="koboSpan" id="kobo.47.1">: One of the key strengths of conformal prediction is its ability to provide valid uncertainty estimates even when the underlying prediction model is misspecified or imperfect. </span><span class="koboSpan" id="kobo.47.2">This robustness to model misspecification is particularly valuable in real-world settings, where the true data-generating process is often unknown or complex      and the available models may only provide approximations of the </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">underlying relationships.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Non-parametric nature</span></strong><span class="koboSpan" id="kobo.50.1">: Conformal prediction is a non-parametric method, meaning that it does not rely on any specific assumptions about the data distribution or prediction errors. </span><span class="koboSpan" id="kobo.50.2">This non-parametric property further contributes to the validity of conformal predictors, as they can adapt to different data structures and provide accurate uncertainty estimates without requiring explicit knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">underlying distributions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.52.1">Applicability across domains</span></strong><span class="koboSpan" id="kobo.53.1">: The validity of conformal prediction is a universal property that holds across various domains and applications. </span><span class="koboSpan" id="kobo.53.2">This universality allows practitioners to leverage conformal prediction in diverse fields, such as finance, healthcare, energy, and transportation, knowing that the uncertainty estimates provided by conformal predictors will be valid and reliable regardless of the </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">specific context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Automatic validity guarantees</span></strong><span class="koboSpan" id="kobo.56.1">: A key advantage of conformal prediction over traditional methods is its ability to provide automatic validity guarantees, meaning that the uncertainty estimates it produces are guaranteed to be valid under mild assumptions, such as the exchangeability of the data. </span><span class="koboSpan" id="kobo.56.2">This automatic validity ensures that conformal predictors maintain their reliability even as new data points are added or as the underlying relationships evolve </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">over time.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.58.1">In conformal prediction, validity is </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.59.1">mathematically defined in terms of the coverage probability of the prediction intervals or regions generated by the conformal predictor. </span><span class="koboSpan" id="kobo.59.2">A conformal predictor is valid if, for any desired confidence level </span><em class="italic"><span class="koboSpan" id="kobo.60.1">(1−α)</span></em><span class="koboSpan" id="kobo.61.1">, the proportion of true target values contained within their corresponding prediction intervals is at least </span><em class="italic"><span class="koboSpan" id="kobo.62.1">(1−α)</span></em><span class="koboSpan" id="kobo.63.1">, on average, across </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">multiple instances.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">Mathematically, let’s denote the target variable as </span><em class="italic"><span class="koboSpan" id="kobo.66.1">Y</span></em><span class="koboSpan" id="kobo.67.1"> and the prediction interval (in regression) or set (in classification) as </span><em class="italic"><span class="koboSpan" id="kobo.68.1">I(x, α)</span></em><span class="koboSpan" id="kobo.69.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.70.1">x</span></em><span class="koboSpan" id="kobo.71.1"> represents the features of the test data point and </span><em class="italic"><span class="koboSpan" id="kobo.72.1">α</span></em><span class="koboSpan" id="kobo.73.1"> is the significance level </span><em class="italic"><span class="koboSpan" id="kobo.74.1">(α </span></em><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.75.1">∈</span></span><em class="italic"><span class="koboSpan" id="kobo.76.1"> [0, 1])</span></em><span class="koboSpan" id="kobo.77.1">. </span><span class="koboSpan" id="kobo.77.2">The conformal predictor is valid if the following </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">condition holds:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.79.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.80.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.81.1">Y</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended"><span class="koboSpan" id="kobo.82.1">∈</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.83.1">I</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.84.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.85.1">x</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.86.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.87.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.88.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.89.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.90.1">≥</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.91.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.92.1">–</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.93.1">α</span></span></p>
<p><span class="koboSpan" id="kobo.94.1">This condition states that the probability that the true target value </span><em class="italic"><span class="koboSpan" id="kobo.95.1">Y</span></em><span class="koboSpan" id="kobo.96.1"> is within the prediction interval </span><em class="italic"><span class="koboSpan" id="kobo.97.1">I(x, α)</span></em><span class="koboSpan" id="kobo.98.1"> is at least </span><em class="italic"><span class="koboSpan" id="kobo.99.1">(1−α)</span></em><span class="koboSpan" id="kobo.100.1"> for any given input data </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">point </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.102.1">x</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.104.1">Validity in conformal prediction is closely related to the calibration concept in probabilistic prediction. </span><span class="koboSpan" id="kobo.104.2">A calibrated predictor generates prediction intervals with the correct coverage probability, ensuring that the uncertainty estimates it provides are well aligned with the true underlying uncertainty in </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">It is important to note that validity in conformal prediction is guaranteed under the assumption of exchangeability, which requires that the observed data points are exchangeable with future, unseen data points. </span><span class="koboSpan" id="kobo.106.2">This assumption holds for </span><strong class="bold"><span class="koboSpan" id="kobo.107.1">independent and identically distributed</span></strong><span class="koboSpan" id="kobo.108.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.109.1">IID</span></strong><span class="koboSpan" id="kobo.110.1">) data. </span><span class="koboSpan" id="kobo.110.2">In addition, successful modifications of</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.111.1"> conformal prediction have been developed to address non-exchangeable data, including many successful models for </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">time series.</span></span></p>
<h2 id="_idParaDest-45"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.113.1">Classifier calibration</span></h2>
<p><span class="koboSpan" id="kobo.114.1">Classifier calibration ensures that the predicted probabilities of an event match the true probabilities or</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.115.1"> frequencies of that event occurring. </span><span class="koboSpan" id="kobo.115.2">For example, in</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.116.1"> weather forecasting, calibration ensures that the forecasted probability of rain aligns with the actual occurrence of rain over a series </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">of predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.118.1">The concept of classifier calibration has been applied to weather forecasting since the 1950s, pioneered by Glen Brier. </span><span class="koboSpan" id="kobo.118.2">In the case of rain forecasting, a forecaster might declare an 80% chance of rain. </span><span class="koboSpan" id="kobo.118.3">If, on average, rain occurs 60% of the time following such statements, we consider the forecast </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">well calibrated.</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">Let’s consider a weather forecaster who makes a statement that there is an </span><em class="italic"><span class="koboSpan" id="kobo.121.1">x</span></em><span class="koboSpan" id="kobo.122.1">% chance of rain for a particular day. </span><span class="koboSpan" id="kobo.122.2">To assess the calibration of the forecaster’s predictions, we would collect a series of similar predictions along with their corresponding outcomes (whether it rained or not). </span><span class="koboSpan" id="kobo.122.3">For example, suppose we gather 100 instances in which the forecaster predicted a 60% chance of rain. </span><span class="koboSpan" id="kobo.122.4">If the forecaster’s predictions are well calibrated, it should rain on approximately 60 of those 100 days, resulting in an observed frequency of rain that matches the </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">predicted probability.</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">But what would this mean in practice? </span><span class="koboSpan" id="kobo.124.2">Let’s consider an example to understand this </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">concept better.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">Suppose, over time, a weather forecaster made 10 predictions of an </span><em class="italic"><span class="koboSpan" id="kobo.127.1">x</span></em><span class="koboSpan" id="kobo.128.1">% chance of rain. </span><span class="koboSpan" id="kobo.128.2">In the following table, we show these forecasts and the </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">actual outcome:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.130.1">Day</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.131.1">Forecasted probability </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.132.1">of rain</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.133.1">Actual </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.134.1">outcome (rain)</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.135.1">1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.136.1">80%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.137.1">Yes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.138.1">2</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.139.1">60%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.140.1">No</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.141.1">3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.142.1">90%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.143.1">Yes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.144.1">4</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.145.1">30%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.146.1">No</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.147.1">5</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.148.1">70%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.149.1">Yes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.150.1">6</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.151.1">50%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.152.1">No</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.153.1">7</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.154.1">80%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.155.1">Yes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.156.1">8</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.157.1">20%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.158.1">No</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.159.1">9</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.160.1">40%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.161.1">Yes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.162.1">10</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.163.1">60%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.164.1">Yes</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.165.1">Table 4.1 – Forecasted probability of rain versus actual outcome</span></p>
<p><span class="koboSpan" id="kobo.166.1">To determine whether this </span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.167.1">forecast is well calibrated, we need to compare the forecasted rain probabilities with the actual outcomes for each probability level. </span><span class="koboSpan" id="kobo.167.2">We can group the forecasts by their probability levels and calculate the observed frequency of rain for </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">each group.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.169.1">Chance </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.170.1">of rain</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.171.1">20%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.172.1">30%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.173.1">40%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.174.1">50%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.175.1">60%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.176.1">70%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.177.1">80%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.178.1">90%</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.179.1">Predicted</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.180.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.181.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.182.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.183.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.184.1">2 days</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.185.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.186.1">2 days</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.187.1">1 day</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.188.1">Rained</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.189.1">0 days</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.190.1">0 days</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.191.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.192.1">0 days</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.193.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.194.1">1 day</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.195.1">2 days</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.196.1">1 day</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.197.1">Observed </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.198.1">frequency</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.199.1">0%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.200.1">0%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.201.1">100%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.202.1">0%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.203.1">50%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.204.1">100%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.205.1">100%</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.206.1">100%</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.207.1">Table 4.2 – Forecasted probability of rain versus observed frequency</span></p>
<p><span class="koboSpan" id="kobo.208.1">Based on the observed frequencies, we can see that the forecast is not well calibrated. </span><span class="koboSpan" id="kobo.208.2">The observed frequencies do not align with the forecasted probabilities for most probability levels. </span><span class="koboSpan" id="kobo.208.3">For example, on the two days with a 60% chance of rain, it only rained once (50% of the predicted frequency), and on the day with a 50% chance of rain, it didn’t rain at all (0% the </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">predicted frequency).</span></span></p>
<p><span class="koboSpan" id="kobo.210.1">How can we aggregate these results into certain metrics? </span><span class="koboSpan" id="kobo.210.2">We could use the </span><strong class="bold"><span class="koboSpan" id="kobo.211.1">Brier score</span></strong><span class="koboSpan" id="kobo.212.1">, which we encountered in previous chapters. </span><span class="koboSpan" id="kobo.212.2">The Brier score is a commonly used calibration metric for binary </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">classification problems.</span></span></p>
<p><span class="koboSpan" id="kobo.214.1">Recall that the Brier score is calculated as the mean squared difference between the predicted probabilities and the true binary outcomes (0 </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">or 1):</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.216.1">B</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.217.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.218.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.219.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.220.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.221.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.222.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.223.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.224.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.225.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.226.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.227.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.228.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.229.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.230.1">N</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.231.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.232.1">∑</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.233.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.235.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.236.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.237.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.238.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.239.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.240.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.241.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.242.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.243.1">n</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.244.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.245.1">i</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.246.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.247.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.248.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.249.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.250.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.251.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.252.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.253.1">e</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.254.1">_</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.255.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.256.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.257.1">^</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.258.1">2</span></span></p>
<p><span class="koboSpan" id="kobo.259.1">where </span><em class="italic"><span class="koboSpan" id="kobo.260.1">N</span></em><span class="koboSpan" id="kobo.261.1"> is the number of predictions, </span><em class="italic"><span class="koboSpan" id="kobo.262.1">prediction_i</span></em><span class="koboSpan" id="kobo.263.1"> is the predicted probability for the </span><em class="italic"><span class="koboSpan" id="kobo.264.1">i</span></em><span class="koboSpan" id="kobo.265.1">-th instance, and </span><em class="italic"><span class="koboSpan" id="kobo.266.1">outcome_i</span></em><span class="koboSpan" id="kobo.267.1"> is the true binary outcome for the </span><em class="italic"><span class="koboSpan" id="kobo.268.1">i</span></em><span class="koboSpan" id="kobo.269.1">-th instance (1 for rain, 0 for </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">no rain).</span></span></p>
<p><span class="koboSpan" id="kobo.271.1">Now we can compute</span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.272.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">Brier score:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.274.1">B</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.275.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.276.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.277.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.278.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.279.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.280.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.281.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.283.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.284.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.285.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.286.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.287.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.288.1">10</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.289.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.290.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.291.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.292.1">0.04</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.293.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.294.1">0.36</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.295.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.296.1">0.01</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.297.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.298.1">0.09</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.299.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.300.1">0.09</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.301.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.302.1">0.25</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.303.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.304.1">0.04</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.305.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.306.1">0.04</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.307.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.308.1">0.36</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.309.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.310.1">0.16</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.311.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.312.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.313.1">0.144</span></span></span></p>
<p><span class="koboSpan" id="kobo.314.1">A lower Brier score indicates better model performance and, consequently, better calibration. </span><span class="koboSpan" id="kobo.314.2">However, with a reference or comparison to other models, it is easier to determine whether a Brier score of 0.144 is good or not. </span><span class="koboSpan" id="kobo.314.3">Additionally, it is important to remember that this assessment is based on a limited sample size of only 10 days, which may not provide an accurate representation of the forecast’s calibration over a </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">longer period.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">We can also create a calibration diagram, also known as a reliability diagram, to assess the calibration of probabilistic prediction models. </span><span class="koboSpan" id="kobo.316.2">The diagram plots the predicted probabilities (grouped into bins) on the </span><em class="italic"><span class="koboSpan" id="kobo.317.1">x</span></em><span class="koboSpan" id="kobo.318.1"> axis against the observed frequencies of the event on the </span><em class="italic"><span class="koboSpan" id="kobo.319.1">y</span></em><span class="koboSpan" id="kobo.320.1"> axis. </span><span class="koboSpan" id="kobo.320.2">A well-calibrated model would have points along the diagonal (45-degree angle), indicating that the predicted probabilities match the </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">observed frequencies.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">As we can see from </span><em class="italic"><span class="koboSpan" id="kobo.323.1">Table 4.2</span></em><span class="koboSpan" id="kobo.324.1">, the forecast is not well calibrated. </span><span class="koboSpan" id="kobo.324.2">The observed frequencies do not match the forecasted probabilities for most </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">probability levels.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">There are two types of miscalibration: </span><em class="italic"><span class="koboSpan" id="kobo.327.1">underconfidence</span></em><span class="koboSpan" id="kobo.328.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.329.1">overconfidence</span></em><span class="koboSpan" id="kobo.330.1">. </span><span class="koboSpan" id="kobo.330.2">When a classifier exhibits underconfidence, it underestimates its ability to distinguish between classes, performing better in practice than its predictions suggest. </span><span class="koboSpan" id="kobo.330.3">In contrast, an overconfident classifier overestimates its capacity to separate classes, performing worse than its predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">probabilities imply.</span></span></p>
<p><span class="koboSpan" id="kobo.332.1">Another metric that can be used to evaluate calibration is log loss, also known as logarithmic loss or cross-entropy, which</span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.333.1"> is a metric used to evaluate the performance of classification models that produce probability estimates for each class. </span><span class="koboSpan" id="kobo.333.2">It measures the divergence between the true and predicted probability distributions, penalizing incorrect and </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">uncertain predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">The concept of log loss is based on the idea that a classifier should not only predict the correct class but also be confident in its prediction. </span><span class="koboSpan" id="kobo.335.2">Log loss quantifies the uncertainty in the predicted</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.336.1"> probabilities by comparing them with the </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">actual outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.338.1">For binary classification, log loss is defined </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.340.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.341.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.342.1">g</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.343.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.344.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.345.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.346.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.347.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.348.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.349.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.350.1">y</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.351.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.352.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.353.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.354.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.355.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.356.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.357.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.358.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.359.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.360.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.361.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.362.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.363.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.364.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.365.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.366.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.367.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.368.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.369.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.370.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.371.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.372.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.373.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.374.1">In this scenario, </span><em class="italic"><span class="koboSpan" id="kobo.375.1">y</span></em><span class="koboSpan" id="kobo.376.1"> stands for the true class label, which can be either 0 or 1. </span><span class="koboSpan" id="kobo.376.2">The symbol </span><em class="italic"><span class="koboSpan" id="kobo.377.1">p</span></em><span class="koboSpan" id="kobo.378.1"> signifies the predicted probability for the positive class (class 1). </span><span class="koboSpan" id="kobo.378.2">The term </span><em class="italic"><span class="koboSpan" id="kobo.379.1">log</span></em><span class="koboSpan" id="kobo.380.1"> indicates the natural logarithm. </span><span class="koboSpan" id="kobo.380.2">The log loss is calculated for each instance and then averaged across all instances to obtain the final log </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">loss value.</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">In a calibration context, log loss can be used to assess how well the predicted probabilities match the true outcomes. </span><span class="koboSpan" id="kobo.382.2">A well-calibrated model will have a lower log loss, as the predicted probabilities will be closer to the actual class labels. </span><span class="koboSpan" id="kobo.382.3">Conversely, a poorly calibrated model will have a higher log loss, indicating a discrepancy between the predicted probabilities and the </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">true outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.384.1">It is important to note that log loss alone might not be sufficient to assess calibration, as it also depends on the classification accuracy. </span><span class="koboSpan" id="kobo.384.2">However, when used in conjunction with other metrics, such as calibration diagrams, log loss can provide valuable insights into the calibration of a classification model. </span><span class="koboSpan" id="kobo.384.3">In practice, log loss is often used alongside the Brier score to evaluate a model’s calibration. </span><span class="koboSpan" id="kobo.384.4">When both metrics agree on the relative calibration of two models, this provides stronger evidence than relying on a single calibration metric, such as log loss or Brier loss, alone. </span><span class="koboSpan" id="kobo.384.5">A more comprehensive assessment of a model’s calibration can be achieved by considering </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">both metrics.</span></span></p>
<p><span class="koboSpan" id="kobo.386.1">Recall the rain prediction example </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">from earlier.</span></span></p>
<p><span class="koboSpan" id="kobo.388.1">To calculate the log loss for this example, we will use </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">this formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.390.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.391.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.392.1">g</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.393.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.394.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.395.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.396.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.397.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.398.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.399.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.400.1">y</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.401.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.402.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.403.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.404.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.405.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.406.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.407.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.408.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.409.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.410.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.411.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.412.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.413.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.414.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.415.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.416.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.417.1">g</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.418.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.419.1">1</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.420.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.421.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.422.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.423.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.424.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.425.1">y</span></em><span class="koboSpan" id="kobo.426.1"> is the true</span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.427.1"> class label (0 or 1) and </span><em class="italic"><span class="koboSpan" id="kobo.428.1">p</span></em><span class="koboSpan" id="kobo.429.1"> is the predicted probability of the positive </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">class (rain).</span></span></p>
<p><span class="koboSpan" id="kobo.431.1">Let’s compute the log loss for </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">each day:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.433.1">Day 1: (1 * log(0.8) + (1 - 1) * log(1 - 0.8)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">0.223</span></span></li>
<li><span class="koboSpan" id="kobo.435.1">Day 2: (0 * log(0.8) + (1 - 0) * log(1 - 0.8)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">1.609</span></span></li>
<li><span class="koboSpan" id="kobo.437.1">Day 3: (1 * log(0.6) + (1 - 1) * log(1 - 0.6)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">0.511</span></span></li>
<li><span class="koboSpan" id="kobo.439.1">Day 4: (1 * log(0.7) + (1 - 1) * log(1 - 0.7)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">0.357</span></span></li>
<li><span class="koboSpan" id="kobo.441.1">Day 5: (1 * log(0.9) + (1 - 1) * log(1 - 0.9)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">0.105</span></span></li>
<li><span class="koboSpan" id="kobo.443.1">Day 6: (0 * log(0.7) + (1 - 0) * log(1 - 0.7)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">1.204</span></span></li>
<li><span class="koboSpan" id="kobo.445.1">Day 7: (1 * log(0.6) + (1 - 1) * log(1 - 0.6)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">0.511</span></span></li>
<li><span class="koboSpan" id="kobo.447.1">Day 8: (1 * log(0.5) + (1 - 1) * log(1 - 0.5)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">0.693</span></span></li>
<li><span class="koboSpan" id="kobo.449.1">Day 9: (0 * log(0.6) + (1 - 0) * log(1 - 0.6)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">0.916</span></span></li>
<li><span class="koboSpan" id="kobo.451.1">Day 10: (0 * log(0.4) + (1 - 0) * log(1 - 0.4)) = </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">0.511</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.453.1">Now, we can compute the average log loss across all </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">10 days:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.455.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.456.1">v</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.457.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.458.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.459.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.460.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.461.1">e</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.462.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.463.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.464.1">g</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.465.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.466.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.467.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.468.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.469.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.470.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.471.1">0.223</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.472.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.473.1">1.609</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.474.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.475.1">0.511</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.476.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.477.1">0.357</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.478.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.479.1">0.105</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.480.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.481.1">1.204</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.482.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.483.1">0.511</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.484.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.485.1">0.693</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.486.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.487.1">0.916</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.488.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.489.1">0.511</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.490.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.491.1">/</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.492.1">10</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.493.1">≈</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.494.1">0.664</span></span></span></p>
<p><span class="koboSpan" id="kobo.495.1">The average log loss for this rain prediction example is </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">approximately 0.664.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">A question naturally arises: of statistical, machine, and deep learning, which are well calibrated and which </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">are not?</span></span></p>
<p><span class="koboSpan" id="kobo.499.1">As a general guideline, it is important to remember that most machine learning models are miscalibrated to some extent, with varying degrees of severity. </span><span class="koboSpan" id="kobo.499.2">However, logistic regression has its own limitations and may only be suitable for some applications due to its relatively simpler </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">modeling capacity.</span></span></p>
<p><span class="koboSpan" id="kobo.501.1">A classical study of calibration is the paper </span><em class="italic"><span class="koboSpan" id="kobo.502.1">Predicting Good Probabilities With Supervised Learning (</span></em><em class="italic"><span class="koboSpan" id="kobo.503.1">2005): </span></em><a href="https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf"><span class="koboSpan" id="kobo.504.1">https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf</span></a><span class="koboSpan" id="kobo.505.1"> which examined the calibration properties of various supervised classification algorithms. </span><span class="koboSpan" id="kobo.505.2">The paper found that maximum margins, such as support vector machines and boosted trees, produced miscalibrated class scores and tended to push</span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.506.1"> predictions close to 0 and 1, while other methods, such as naïve Bayes, pushed predictions in the </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">other directions.</span></span></p>
<p><span class="koboSpan" id="kobo.508.1">While it was initially thought that simple neural networks produced calibrated predictions, this conclusion has since been reevaluated. </span><span class="koboSpan" id="kobo.508.2">In a more recent paper titled </span><em class="italic"><span class="koboSpan" id="kobo.509.1">Are Traditional Neural Networks Well-Calibrated?</span></em><span class="koboSpan" id="kobo.510.1"> (</span><a href="https://ieeexplore.ieee.org/document/8851962"><span class="koboSpan" id="kobo.511.1">https://ieeexplore.ieee.org/document/8851962</span></a><span class="koboSpan" id="kobo.512.1">), the authors showed that individual multilayer perceptrons, as well as ensembles of multilayer perceptrons, frequently display </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">poor calibration.</span></span></p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.514.1">The efficiency of probabilistic predictors</span></h1>
<p><span class="koboSpan" id="kobo.515.1">Efficiency is a</span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.516.1"> performance metric used to evaluate probabilistic predictors. </span><span class="koboSpan" id="kobo.516.2">It measures how precise or informative the prediction intervals or regions are. </span><span class="koboSpan" id="kobo.516.3">In other words, efficiency indicates how tight or narrow the predicted probability distributions are. </span><span class="koboSpan" id="kobo.516.4">Smaller intervals or regions are considered more efficient, as they convey more certainty about the </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">predicted outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.518.1">While validity focuses on ensuring that the error rate is controlled, efficiency assesses the usefulness and precision of the predictions. </span><span class="koboSpan" id="kobo.518.2">An efficient predictor provides more specific information about the possible outcomes, whereas a less efficient predictor generates wider intervals or regions, resulting in less </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">precise information.</span></span></p>
<p><span class="koboSpan" id="kobo.520.1">There is an inherent trade-off between validity and efficiency. </span><span class="koboSpan" id="kobo.520.2">A conformal predictor can always achieve perfect validity by outputting very wide prediction sets that encompass all possible outcomes. </span><span class="koboSpan" id="kobo.520.3">However, this lacks efficiency, as the predictions are too conservative </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">and imprecise.</span></span></p>
<p><span class="koboSpan" id="kobo.522.1">On the other hand, a model can output very narrow, precise predictions but may fail on the validity criteria by making erroneous predictions more than the allowed threshold. </span><span class="koboSpan" id="kobo.522.2">This results from overconfidence and unreliable </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">probability estimates.</span></span></p>
<p><span class="koboSpan" id="kobo.524.1">Ideally, a conformal predictor finds an optimal balance; the predictions are as tight as possible while still meeting the validity guarantee. </span><span class="koboSpan" id="kobo.524.2">This ensures accuracy and precision without being overly conservative or exceeding the error </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">rate threshold.</span></span></p>
<p><span class="koboSpan" id="kobo.526.1">In conformal prediction, efficiency is typically measured by evaluating the size of the prediction intervals or regions generated by the conformal predictor. </span><span class="koboSpan" id="kobo.526.2">Smaller intervals or regions are considered more efficient, as they provide more precise information about the possible outcomes. </span><span class="koboSpan" id="kobo.526.3">Here are a few common ways to measure efficiency in </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">conformal prediction:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.528.1">Prediction</span></strong><strong class="bold"><a id="_idIndexMarker133"/></strong><strong class="bold"><span class="koboSpan" id="kobo.529.1"> interval length</span></strong><span class="koboSpan" id="kobo.530.1">: For regression problems, the average length of the prediction intervals can be calculated by finding the difference between the upper and lower bounds of each interval and then averaging these differences across all instances. </span><span class="koboSpan" id="kobo.530.2">Smaller average lengths indicate </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">higher efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.532.1">Prediction set size</span></strong><span class="koboSpan" id="kobo.533.1">: For classification problems, the size of the prediction sets can be evaluated. </span><span class="koboSpan" id="kobo.533.2">A smaller prediction set contains fewer class labels and is considered more efficient. </span><span class="koboSpan" id="kobo.533.3">One way to measure this is by computing the average size of the prediction sets across all instances. </span><span class="koboSpan" id="kobo.533.4">A lower average set size indicates </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">better efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.535.1">Coverage probability</span></strong><span class="koboSpan" id="kobo.536.1">: Coverage probability measures the proportion of true outcomes that fall within the prediction intervals or regions. </span><span class="koboSpan" id="kobo.536.2">While it is mainly used to evaluate the validity of conformal predictors, it can also provide insights into efficiency. </span><span class="koboSpan" id="kobo.536.3">A predictor with tight intervals or regions will have a higher coverage probability, indicating </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">better efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.538.1">P-value histograms</span></strong><span class="koboSpan" id="kobo.539.1">: In conformal prediction, p-values are calculated for each instance and class label. </span><span class="koboSpan" id="kobo.539.2">Examining the distribution of p-values can provide insights into efficiency. </span><span class="koboSpan" id="kobo.539.3">A uniform distribution of p-values suggests that the predictor is valid but not necessarily efficient, while a more concentrated distribution (e.g., with p-values close to 0 or 1) implies </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">greater efficiency.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.541.1">We have already seen in the previous chapters how conformal prediction guarantees the automatic validity of prediction sets by constructing prediction intervals (for regression) or prediction sets (for classification) that come with a guaranteed error rate, which is determined by a user-defined confidence level. </span><span class="koboSpan" id="kobo.541.2">The key idea behind conformal prediction is to use past data and the observed behavior of a given machine learning model to estimate the uncertainty in </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">its predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.543.1">Let’s recap the stages in </span><strong class="bold"><span class="koboSpan" id="kobo.544.1">inductive conformal prediction,</span></strong><span class="koboSpan" id="kobo.545.1"> which consists of two main phases: the calibration phase and the prediction phase. </span><span class="koboSpan" id="kobo.545.2">Here’s an outline of how </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">it works:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.547.1">Calibration phase</span></strong><span class="koboSpan" id="kobo.548.1">: In this</span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.549.1"> phase, a machine learning model is trained on a dataset, and a nonconformity measure is calculated for each instance in the dataset. </span><span class="koboSpan" id="kobo.549.2">The nonconformity measure quantifies the strangeness or atypicality of an instance with respect to the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.551.1">Prediction phase</span></strong><span class="koboSpan" id="kobo.552.1">: When a </span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.553.1">new instance needs to be predicted, the nonconformity measure, for instance, is calculated using the same nonconformity measure function used in the calibration phase. </span><span class="koboSpan" id="kobo.553.2">The instance’s nonconformity score is then compared to the nonconformity scores of the calibration instances. </span><span class="koboSpan" id="kobo.553.3">A p-value is computed for each possible outcome, reflecting the proportion of calibration instances with nonconformity scores higher than or equal to that of the new instance. </span><span class="koboSpan" id="kobo.553.4">The p-values can be interpreted as a measure of how likely it is for the instance to belong to each class (for classification) or to fall within a certain range (</span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">for regression).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.555.1">Prediction intervals or sets</span></strong><span class="koboSpan" id="kobo.556.1">: Based on the computed p-values and the user-defined confidence level, prediction</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.557.1"> intervals (for regression) or prediction sets (for classification) are constructed. </span><span class="koboSpan" id="kobo.557.2">These intervals or sets are guaranteed to contain the true outcome with a probability equal to the chosen confidence level. </span><span class="koboSpan" id="kobo.557.3">For instance, if the confidence level is set to 0.95, the true outcome will fall within the prediction interval or set 95% of </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">the time.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.559.1">By ensuring that the prediction intervals or sets contain the true outcomes with the desired probability, conformal prediction provides automatic validity for predictions. </span><span class="koboSpan" id="kobo.559.2">It is worth noting that while conformal prediction guarantees validity, it does not necessarily guarantee efficiency, which depends on the precision of the prediction intervals </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">or sets.</span></span></p>
<p><span class="koboSpan" id="kobo.561.1">In </span><a href="B19925_04.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.562.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.563.1">, we will look at and learn about different types of conformal prediction with </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">practical examples.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.565.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.566.1">In this chapter, we have deep-dived into the concepts of validity and efficiency in the context of probabilistic prediction models, building upon the foundations laid in the previous chapters. </span><span class="koboSpan" id="kobo.566.2">We have looked at the definitions of validity and efficiency and learned about various metrics that can be used to evaluate and compare different models in terms of validity </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">and efficiency.</span></span></p>
<p><span class="koboSpan" id="kobo.568.1">In the next chapter, we will learn about different families of conformal predictors and explore various approaches to </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">quantifying uncertainty.</span></span></p>
</div>
</body></html>