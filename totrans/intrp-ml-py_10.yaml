- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Selection and Engineering for Interpretability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first three chapters, we discussed how complexity hinders **Machine
    Learning** (**ML**) interpretability. There’s a trade-off because you may need
    some complexity to maximize predictive performance, yet not to the extent that
    you cannot rely on the model to satisfy the tenets of interpretability: fairness,
    accountability, and transparency. This chapter is the first of four focused on
    how to tune for interpretability. One of the easiest ways to improve interpretability
    is through feature selection. It has many benefits, such as faster training and
    making the model easier to interpret. But if these two reasons don’t convince
    you, perhaps another one will.'
  prefs: []
  type: TYPE_NORMAL
- en: A common misunderstanding is that complex models can self-select features and
    perform well nonetheless, so why even bother to select features? Yes, many model
    classes have mechanisms that can take care of useless features, but they aren’t
    perfect. And the potential for overfitting increases with each one that remains.
    Overfitted models aren’t reliable, even if they are more accurate. So, while employing
    model mechanisms such as regularization is still highly recommended to avoid overfitting,
    feature selection is still useful.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will comprehend how irrelevant features adversely weigh
    on the outcome of a model and thus, the importance of feature selection for model
    interpretability. Then, we will review filter-based feature selection methods
    such as **Spearman’s correlation** and learn about embedded methods such as **LASSO
    and ridge regression**. Then, we will discover wrapper methods such as **sequential
    feature selection**, and hybrid ones such as **Recursive Feature Elimination**
    (**RFE**). Lastly, even though feature engineering is typically conducted before
    selection, there’s value in exploring feature engineering for many reasons after
    the dust has settled and features have been selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main topics we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the effect of irrelevant features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing filter-based feature selection methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring embedded feature selection methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering wrapper, hybrid, and advanced feature selection methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `scipy`, `mlxtend`,
    `sklearn-genetic-opt`, `xgboost`, `sklearn`, `matplotlib`, and `seaborn` libraries.
    Instructions on how to install all these libraries are in the *Preface*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GitHub code for this chapter is located here: [https://packt.link/1qP4P](https://packt.link/1qP4P).'
  prefs: []
  type: TYPE_NORMAL
- en: The mission
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It has been estimated that there are over 10 million non-profits worldwide,
    and while a large portion of them have public funding, most of them depend mostly
    on private donors, both corporate and individual, to continue operations. As such,
    fundraising is mission-critical and carried out throughout the year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Year over year, donation revenue has grown, but there are several problems
    non-profits face: donor interests evolve, so a charity popular one year might
    be forgotten the next; competition is fierce between non-profits, and demographics
    are shifting. In the United States, the average donor only gives two charitable
    gifts per year and is over 64 years old. Identifying potential donors is challenging,
    and campaigns to reach them can be expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: A National Veterans Organization non-profit arm has a large mailing list of
    about 190,000 past donors and would like to send a special mailer to ask for donations.
    However, even with a special bulk discount rate, it costs them $0.68 per address.
    This adds up to over $130,000\. They only have a marketing budget of $35,000\.
    Given that they have made this a high priority, they are willing to extend the
    budget but only if the **Return On Investment** (**ROI**) is high enough to justify
    the additional cost.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize the use of their limited budget, instead of mass mailing, they’d
    like to try direct mailing, which aims to identify potential donors using what
    is already known, such as past donations, geographic location, and demographic
    data. They will reach other donors via email instead, which is much cheaper, costing
    no more than $1,000 per month for their entire list. They hope this hybrid marketing
    plan will yield better results. They also recognize that high-value donors respond
    better to personalized paper mailers, while smaller donors respond better to email
    anyway.
  prefs: []
  type: TYPE_NORMAL
- en: No more than six percent of the mailing list donates to any given campaign.
    Using ML to predict human behavior is by no means an easy task, especially when
    the data categories are imbalanced. Nevertheless, success is not measured by the
    highest predictive accuracy but by profit lift. In other words, the direct mailing
    model evaluated on the test dataset should produce more profit than if they mass-mailed
    the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: They have sought your assistance to use ML to produce a model that identifies
    the most probable donors, but also in a way that *guarantees* an ROI.
  prefs: []
  type: TYPE_NORMAL
- en: You received the dataset from the non-profit, which is approximately evenly
    split into training and test data. If you send the mailer to absolutely everybody
    in the test dataset, you make a profit of $11,173, but if you manage to identify
    only those who will donate, the maximum yield of $73,136 will be attained. Your
    goal is to achieve a high-profit lift and reasonable ROI. When the campaign runs,
    it will identify the most probably donors for the entire mailing list, and the
    non-profit hopes to spend not much more than $35,000 in total. However, the dataset
    has 435 columns, and some simple statistical tests and modeling exercises show
    that the data is too noisy to identify the potential donors’ reliability because
    of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You’ve decided to first fit a base model with all the features and assess it
    at different levels of complexity to understand the relationship between the increased
    number of features and the propensity for the predictive model to overfit to the
    training data. Then, you will employ a series of feature selection methods ranging
    from simple filter-based methods to the most advanced ones to determine which
    one achieves the profitability and reliability goals sought by the client. Lastly,
    once a list of final features has been selected, you can try feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Given the cost-sensitive nature of the problem, thresholds are important to
    optimize the profit lift. We will get into the role of thresholds later on, but
    one significant effect is that even though this is a classification problem, it
    is best to use regression models, and then use predictions to classify so that
    there’s only one threshold to tune. That is, for classification models, you would
    need a threshold for the label, say those that donated over $1, and then another
    one for probabilities predicted. On the other hand, regression predicts the donation,
    and the threshold can be optimized based on that.
  prefs: []
  type: TYPE_NORMAL
- en: The preparations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found at [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Loading the libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run this example, we need to install the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mldatasets` to load the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas`, `numpy`, and `scipy` to manipulate it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlxtend`, `sklearn-genetic-opt`, `xgboost`, and `sklearn` (scikit-learn) to
    fit the models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` and `seaborn` to create and visualize the interpretations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To load the libraries, use the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will load and prepare the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We load the data like this into two DataFrames (`X_train` and `X_test`) with
    the features and two `numpy` arrays with corresponding labels (`y_train` and `y_test`).
    Please note that these DataFrames have already been previously prepared for us
    to remove sparse or unnecessary features, treat missing values, and encode categorical
    features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'All features are numeric with no missing values and categorical features have
    already been one-hot encoded for us. Between both train and test mailing lists,
    there should be over 191,500 records and 435 features. You can check this is the
    case like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can verify that the test labels have the right number of donors (`test_donors`),
    donations (`test_donations`), and hypothetical profit ranges (`test_min_profit`
    and `test_max_profit`) using the variable cost of $0.68 (`var_cost`). We can print
    these, and then do the same for the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, if the non-profit mass-mailed to everyone on the test mailing list,
    they’d make about $11,000 profit but would have to go grossly over budget to achieve
    this. The non-profit recognizes that making the max profit by identifying and
    targeting only donors is nearly an impossible feat. Therefore, they would be content
    with producing a model that reliably can yield more than the min profit but with
    a smaller cost, preferably under budget.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the effect of irrelevant features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Feature selection** is also known as **variable** or **attribute selection**.
    It is the method by which you can automatically or manually select a subset of
    specific features useful to the construction of ML models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s not necessarily true that more features lead to better models. Irrelevant
    features can impact the learning process, leading to overfitting. Therefore, we
    need some strategies to remove any features that might adversely affect learning.
    Some of the advantages of selecting a smaller subset of features include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*It’s easier to understand simpler models*: For instance, feature importance
    for a model that uses 15 variables is much easier to grasp than one that uses
    150 variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Shorter training time*: Reducing the number of variables decreases the cost
    of computing, speeds up model training, and perhaps most notably, simpler models
    have quicker inference times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Improved generalization by reducing overfitting*: Sometimes, with little prediction
    value, many of the variables are just noise. The ML model, however, learns from
    this noise and triggers overfitting to the training data while minimizing generalization
    simultaneously. We may significantly enhance the generalization of ML models by
    removing these irrelevant or noisy features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Variable redundancy*: It is common for datasets to have collinear features,
    which could mean some are redundant. In cases like these, as long as no significant
    information is lost, we can retain only one of the correlated features and delete
    the others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will fit some models to demonstrate the effect of too many features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a base model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a base model for our mailing list dataset to see how this plays
    out. But first, let’s set our random seed for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use XGBoost’s **Random Forest** (**RF**) regressor (`XGBRFRegressor`)
    throughout this chapter. It’s just like scikit-learn’s but faster because it uses
    second-order approximations of the objective function. It also has more options,
    such as setting the learning rate and monotonic constraints, examined in *Chapter
    12*, *Monotonic Constraints and Model Tuning for Interpretability*. We initialize
    `XGBRFRegressor` with a conservative initial `max_depth` value of `4` and always
    use `200` estimators for consistency. Then, we fit it with our training data.
    We will use `timeit` to measure how long it takes, which we save in a variable
    (`baseline_time`) for later reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a base model, let’s evaluate it.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let’s create a dictionary (`reg_mdls`) to house all the models we will
    fit in this chapter to test which feature subsets produce the best models. Here,
    we can evaluate the RF model with all the features and a `max_depth` value of
    `4` (`rf_4_all`) using `evaluate_reg_mdl`. It will make a summary and a scatter
    plot with a regression line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the metrics and plot shown in *Figure 10.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18406_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Base model predictive performance'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a plot like the one in *Figure 10.1*, usually, a diagonal line is expected,
    so one glance at this plot would tell you that the model is not predictive. Also,
    the RMSEs may not seem bad but in the context of such a lopsided problem, they
    are dismal. Consider this: only 5% of the list makes a donation, and only 20%
    of those are over $20, so an average error of $4.3 – $4.6 is enormous.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, is this model useless? The answer lies in what thresholds we use to classify
    with it. Let’s start by defining an array of thresholds (`threshs`), ranging from
    $0.40 to $25\. We start spacing these out by a cent until it reaches $1, then
    by 10 cents until it reaches $3, and after that, space by $1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s a function in `mldatasets` that can compute profit at every threshold
    (`profits_by_thresh`). All it needs is the actual (`y_test`) and predicted labels,
    followed by the thresholds (`threshs`), the variable cost (`var_costs`), and the
    `min_profit` required. It produces a `pandas` DataFrame with the revenue, costs,
    profit, and ROI for every threshold, as long as the profit is above `min_profit`.
    Remember, we had set this minimum at the beginning of the chapter as $11,173 because
    it makes no sense to target donors under this amount. After we generate these
    profit DataFrames for the test and train datasets, we can place the maximum and
    minimum amounts in the model’s dictionary for later use. And then, we employ `compare_df_plots`
    to plot the costs, profits, and ROI ratio for testing and training for every threshold
    where it exceeded the profit minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Comparison between profit, costs, and ROI for the test and train
    datasets for the base model across thresholds'
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference in RMSEs for the train and test sets didn’t lie. The model did
    not overfit. The main reason for this is that we used relatively shallow trees
    by setting our `max_depth` value at `4`. We can easily see this effect of using
    shallow trees by computing how many features had a `feature_importances_` value
    of over 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code outputs `160`. In other words, only 160 were used out of
    435—there are only so many features that can be accommodated in such a shallow
    tree! Naturally, this leads to lowering overfitting, but at the same time, the
    choice of features with measures of impurity over a random selection of features
    is not necessarily the most optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Training the base model at different max depths
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So, what happens if we make the trees deeper? Let’s repeat all the steps we
    did for the shallow one but for max depths between 5 and 12:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s plot the details in the profits DataFrames for the “deepest” model
    (with a max depth of 12) as we did before with `compare_df_plots`, producing *Figure
    10.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Comparison between profit, costs, and ROI for the test and train
    datasets for a “deep” base model across thresholds'
  prefs: []
  type: TYPE_NORMAL
- en: See how different **Test** and **Train** are this time in *Figure 10.3*. **Test**
    reaches a max of about $15,000 and **Train** exceeds $20,000\. **Train**’s costs
    dramatically fall, making the ROI orders of magnitude much higher than **Test**.
    Also, the ranges of thresholds are much different. Why is this a problem, you
    ask? If we had to guess what threshold to use to pick who to target in the next
    mailer, the optimal for **Train** is higher than for **Test**—meaning that by
    using an overfit model, we could miss the mark and underperform on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s convert our model dictionary (`reg_mdls`) into a DataFrame and
    extract some details from it. Then, we can sort it by depth, format it, color-code
    it, and output it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Table  Description automatically generated](img/B18406_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Comparing metrics for all base RF models with different depths'
  prefs: []
  type: TYPE_NORMAL
- en: We could be tempted to use `rf_11_all` with the highest profitability, but it
    would be risky to use it! A common misunderstanding is that black-box models can
    effectively cut through any number of irrelevant features. While they are often
    be able to find something of value and make the most out of it, too many features
    could hinder their reliability by overfitting to noise in the training dataset.
    Fortunately, there is a sweet spot where you can reach high profitability with
    minimal overfitting, but to get there, we have to reduce the number of features
    first!
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing filter-based feature selection methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Filter-based methods** independently select features from a dataset without
    employing any ML. These methods depend only on the variables’ characteristics
    and are relatively effective, computationally inexpensive, and quick to perform.
    Therefore, being the low-hanging fruit of feature selection methods, they are
    usually the first step in any feature selection pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Filter-based methods can be categorized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Univariate**: Individually and independently of the feature space, they evaluate
    and rate a single feature at a time. One problem that can occur with univariate
    methods is that they may filter out too much since they don’t take into consideration
    the relationship between features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multivariate**: These take into account the entire feature space and how
    features interact with each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, for the removal of obsolete, redundant, constant, duplicated, and uncorrelated
    features, filter methods are very strong. However, by not accounting for complex,
    non-linear, non-monotonic correlations and interactions that only ML models can
    find, they aren’t effective whenever these relationships are prominent in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will review three categories of filter-based methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will explain them further in their own sections.
  prefs: []
  type: TYPE_NORMAL
- en: Basic filter-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We employ **basic filter methods** in the data preparation stage, specifically,
    the data cleaning stage, before any modeling. The reason for this is there’s a
    low risk of making feature selection decisions that would adversely impact models.
    They involve common-sense operations such as removing features that carry no information
    or duplicate it.
  prefs: []
  type: TYPE_NORMAL
- en: Constant features with a variance threshold
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Constant features** don’t change in the training dataset and, therefore,
    carry no information, and the model can’t learn from them. We can use a univariate
    method called `VarianceThreshold`, which removes low-variance features. We will
    use a threshold of zero because we want to filter out only features with **zero
    variance**—in other words, constant features. It only works with numeric features,
    so we must first identify which features are numeric and which are categorical.
    Once we fit the method on the numeric columns, `get_support()` returns the list
    of features that aren’t constant, and we can use set algebra to return only the
    constant features (`num_const_cols`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In most cases, removing constant features isn’t good enough. A redundant feature
    might be almost constant or **quasi-constant**.
  prefs: []
  type: TYPE_NORMAL
- en: Quasi-constant features with value_counts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Quasi-constant features** are almost entirely the same value. Unlike constant
    filtering, using a variance threshold won’t work because high variance and quasi-constantness
    aren’t mutually exclusive. Instead, we will iterate all features and get `value_counts()`,
    which returns the number of rows for each value. Then, divide these counts by
    the total number of rows to get a percentage and sort by the highest. If the top
    value is higher than the predetermined threshold (`thresh`), we append it to a
    list of quasi-constant columns (`quasi_const_cols`). Please note that choosing
    this threshold must be done with a lot of care and understanding of the problem.
    For instance, in this case, we know that it’s lopsided because only 5% donate,
    most of whom donate a low amount, so even a tiny percentage of a feature might
    make an impact, which is why our threshold is so high at 99.9%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should have printed five features, which include the three
    that were previously obtained. Next, we will deal with another form of irrelevant
    features: duplicates!'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicating features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usually, when we discuss duplicates with data, we immediately think of duplicate
    rows, but **duplicate columns** are also problematic. We can find them just as
    you would find duplicate rows with the `pandas duplicated()` function, except
    we would transpose the DataFrame first, inversing columns and rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet outputs a list with the two duplicated columns.
  prefs: []
  type: TYPE_NORMAL
- en: Removing unnecessary features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike other feature selection methods, which you should test with models,
    you can apply basic filter-based feature selection methods right away by removing
    the features you deemed useless. But just in case, it’s good practice to make
    a copy of the original data. Please note that we don’t include constant columns
    (`all_constant_cols`) in the columns we are to drop (`drop_cols`) because the
    quasi-constant ones already include them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will explore multivariate filter-based methods on the remaining features.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation filter-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Correlation filter-based methods** quantify the strength of the relationship
    between two features. It is useful for feature selection because we might want
    to filter out extremely correlated features or those that aren’t correlated with
    others at all. Either way, it is a multivariate feature selection method—bivariate,
    to be precise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, we ought to choose a correlation method:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pearson’s correlation coefficient**: Measures the linear correlation between
    two features. It outputs a coefficient between -1 (negative) and 1 (positive),
    with 0 meaning no linear correlation. Like linear regression, it assumes linearity,
    normality, and homoscedasticity—that is, the error term around the linear regression
    line is similarly sized across all values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spearman’s rank correlation coefficient**: Measures the strength of monotonicity
    of two features regardless of whether they are linearly related or not. Monotonicity
    is the degree to which as one feature increases, the other one consistently increases
    or decreases. It is measured between -1 and 1, with 0 meaning no monotonic correlation.
    It makes no distribution assumptions and can work with both continuous and discrete
    features. However, its weakness is with non-monotonic relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kendall’s tau correlation coefficient**: Measures the ordinal association
    between features—that is, it computes the similarity between lists of ordered
    numbers. It also ranges between -1 and 1, but they mean low and high, respectively.
    It’s useful with discrete features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset is a mix of continuous and discrete, and we cannot make any linear
    assumptions about it, so `spearman` is the right choice. All three can be used
    with the `pandas` `corr` function though:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code should output the shape of the correlation matrix, which
    is `(428, 428)`. This dimension makes sense because there are 428 features left,
    and each feature has a relationship with 428 features, including itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now look for features to remove in the correlation matrix (`corrs`).
    Note that to do so, we must establish thresholds. For instance, we can say that
    an extremely correlated feature has an absolute value coefficient of over 0.99
    and less than 0.15 for an uncorrelated feature. With these thresholds in mind,
    we can find features that are correlated to only one feature and extremely correlated
    to more than one feature. Why one feature? Because the diagonals in a correlation
    matrix are always 1 because a feature is always perfectly correlated with itself.
    The `lambda` functions in the following code make sure we are accounting for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the two lists as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The first list contains features that are extremely correlated with ones other
    than themselves. While this is useful to know, you shouldn’t remove features from
    this list without understanding what features they are correlated with and how,
    as well as with the target. Then, only if redundancy is found, make sure you only
    remove one of them. The second list is of features uncorrelated to any others
    than themself, which, in this case, is suspicious given the sheer number of features.
    That being said, we also should inspect them one by one, especially to measure
    them against the target to see whether they are redundant. However, we will take
    a chance and make a feature subset (`corr_cols`) excluding the uncorrelated ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should output `419`. Let’s now fit the RF model with only
    these features. Given that there are still over 400 features, we will use a `max_depth`
    value of `11`. Except for that and a different model name (`mdlname`), it’s the
    same code as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Before we compare the results for the preceding model, let’s learn about ranking
    filter methods.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking filter-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Ranking filter-based methods** are based on statistical univariate ranking
    tests, which assess the strength of the dependency between a feature and the target.
    These are some of the most popular methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ANOVA F-test: Analysis of Variance** (**ANOVA**) F-test measures the linear
    dependency between features and the target. As the name suggests, it does this
    by decomposing the variance. It makes similar assumptions to linear regression,
    such as normality, independence, and homoscedasticity. In scikit-learn, you can
    use `f_regression` and `f_classification` for regression and classification, respectively,
    to rank features by the F-score yielded by the F-test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chi-square test of independence**: This test measures the association between
    non-negative categorical variables and binary targets, so it’s only suitable for
    classification problems. In scikit-learn, you can use `chi2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutual information** (**MI**): Unlike the two previous methods, this one
    is derived from information theory rather than classical statistical hypothesis
    testing. It’s a different name but a concept we have already discussed in this
    book as the **Kullback-Leibler** (**KL**) **divergence** because it’s the KL for
    feature *X* and target *Y*. The Python implementation in scikit-learn uses a numerically
    stable and symmetric offshoot of KL called **Jensen-Shannon** (**JS**) divergence
    instead and leverages k-nearest neighbors to compute distances. Features can be
    ranked by MI with `mutual_info_regression` and `mutual_info_classif` for regression
    and classification, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of the three options mentioned, the one that is most appropriate for this dataset
    is MI because we cannot assume linearity among our features, and most of them
    aren’t categorical either. We can try classification with a threshold of $0.68,
    which at least covers the cost of sending the mailer. To that end, we must first
    create a binary classification target (`y_train_class`) with that threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can use `SelectKBest` to get the top 160 features according to **MI
    Classification** (**MIC**). We then employ `get_support()` to obtain a Boolean
    vector (or mask), which tells us which features are in the top 160, and we subset
    the list of features with this mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should confirm that there are 160 features in the `mic_cols`
    list. Incidentally, this is an arbitrary number. Ideally, we could test different
    thresholds for the classification target and *k*s for the MI, looking for the
    model that achieved the highest profit lift while underfitting the least. Next,
    we can fit the RF model as we’ve done before with the MIC features. This time,
    we will use a max depth of `5` because there are significantly fewer features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s plot the profits for **Test** and **Train** as we did in *Figure
    10.3*, but for the MIC model. It will produce what’s shown in *Figure 10.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Comparison between profit, costs, and ROI for the test and train
    datasets for a model with MIC features across thresholds'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 10.5*, you can tell that there is quite a bit of difference between
    **Test** and **Train**, yet similarities indicate minimal overfitting. For instance,
    the highest profitability can be found between 0.66 and 0.75 for **Train**, and
    while **Test** is mostly between 0.66 and 0.7, it only gradually decreases afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Although we have visually examined the MIC model, it’s nice to have some reassurance
    by looking at raw metrics. Next, we will compare all the models we have trained
    so far using consistent metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing filter-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have been saving metrics into a dictionary (`reg_mdls`), which we easily
    convert to a DataFrame and output as we have done before, but this time we sort
    by `max_profit_test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![Application, table  Description automatically generated with medium confidence](img/B18406_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Comparing metrics for all base models and filter-based feature-selected
    models'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 10.6*, we can tell that the correlation filter model (`rf_11_f-corr`)
    performs worse than the model with more features and an equal amount of `max_depth`
    `(rf_11_all)`, which suggests that we must have removed an important feature.
    As cautioned in that section, the problem with blindly setting thresholds and
    removing anything above it is that you can inadvertently remove something useful.
    Not all extremely correlated and uncorrelated features are useless, so further
    inspection is required. Next, we will explore some embedded methods that, when
    combined with cross-validation, require less oversight.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring embedded feature selection methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Embedded methods** exist within models themselves by naturally selecting
    features during training. You can leverage the intrinsic properties of any model
    that has them to capture the features selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tree-based models**: For instance, we have used the following code many times
    to count the number of features used by the RF models, which is evidence of feature
    selection naturally occurring in the learning process:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: XGBoost’s RF uses gain by default, which is the average decrease in error in
    all splits where it used the feature to compute feature importance. We can increase
    the threshold above 0 to select even fewer features according to their relative
    contribution. However, by constraining the trees’ depth, we forced the model to
    choose even fewer features already.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Regularized models with coefficients**: We will study this further in *Chapter
    12*, *Monotonic Constraints and Model Tuning for Interpretability*, but many model
    classes can incorporate penalty-based regularization, such as L1, L2, and elastic
    net. However, not all of them have intrinsic parameters such as coefficients that
    can be extracted to determine which features were penalized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section will only cover regularized models given that we are using a tree-based
    model already. It’s best to leverage different model classes to get different
    perspectives on what features matter the most.
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered some of these models in *Chapter 3*, *Interpretation Challenges*,
    but these are a few model classes that incorporate penalty-based regularization
    and output feature-specific coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Absolute Shrinkage and Selection Operator** (**LASSO**): Because it
    uses L1 penalty in the loss function, LASSO can set coefficients to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Least-Angle Regression** (**LARS**): Similar to LASSO but is vector-based
    and is more suitable for high-dimensional data. It is also fairer toward equally
    correlated features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ridge regression**: Uses L2 penalty in the loss function and because of this,
    can only shrink coefficients of irrelevance close to 0 but not to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elastic net regression**: Uses a mix of both L1 and L2 norms as penalties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic regression**: Contingent on the solver, it can handle L1, L2, or
    elastic net penalties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also several variations of the preceding models, such as **LASSO
    LARS**, which is a LASSO fit using the LARS algorithm, or even **LASSO LARS IC**,
    which is the same but uses AIC or BIC criteria for the model section:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Akaike’s Information Criteria** (**AIC**): A relative goodness of fit measure
    founded in information theory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian Information Criteria** (**BIC**): Has a similar formula to AIC but
    has a different penalty term'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OK, now let’s use `SelectFromModel` to extract top features from a LASSO model.
    We will use `LassoCV` because it can automatically cross-validate to find the
    optimal penalty strength. Once you fit it, we can get the feature mask with `get_support()`.
    We can then print the number of features and list of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s try the same but with `LassoLarsCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'LASSO shrunk coefficients for all but seven features to 0, and LASSO LARS did
    the same but for eight. However, notice how there’s no overlap between both lists!
    OK, so let’s try incorporating AIC model selection into LASSO LARS with `LassoLarsIC`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding snippet generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s the same algorithm but with a different method for selecting the value
    of the regularization parameter. Note how this less-conservative approach expands
    the number of features to 111\. Now, so far, all of the methods we have used have
    the L1 norm. Let’s try one with L2—more specifically, L2-penalized logistic regression.
    We do exactly what we did before, but this time, we fit with the binary classification
    targets (`y_train_class`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a few feature subsets to test, we can place their names into
    a list (`fsnames`) and the feature subset lists into another list (`fscols`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then iterate across all list names and fit and evaluate our `XGBRFRegressor`
    model as we have done before, but increasing `max_depth` at every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s see how our embedded feature-selected models fare in comparison
    to the filtered ones. We will rerun the code we ran to output what was shown in
    *Figure 10.6*. This time, we will get what is shown in *Figure 10.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated with medium confidence](img/B18406_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Comparing metrics for all base models and filter-based and embedded
    feature-selected models'
  prefs: []
  type: TYPE_NORMAL
- en: According to *Figure 10.7*, three of the four embedded methods we tried produced
    models with the lowest test RMSE (`rf_5_e-llarsic`, `rf_e-lasso`, and `rf_4_e-llars`).
    They also all trained much faster than the others and are more profitable than
    any other model of equal complexity. One of them (`rf_5_e-llarsic`) is even highly
    profitable. Compare this with `rf_9_all` with similar test profitability to see
    how performance diverges from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering wrapper, hybrid, and advanced feature selection methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The feature selection methods studied so far are computationally inexpensive
    because they require no model fitting or fitting simpler white-box models. In
    this section, we will learn about other, more exhaustive methods with many possible
    tuning options. The categories of methods included here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Wrapper**: Exhaustively searches for the best subset of features by fitting
    an ML model using a search strategy that measures improvement on a metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid**: A method that combines embedded and filter methods with wrapper
    methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced**: A method that doesn’t fall into any of the previously discussed
    categories. Examples include dimensionality reduction, model-agnostic feature
    importance, and **Genetic Algorithms** (**GAs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And now, let’s get started with wrapper methods!
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The concept behind **wrapper methods** is reasonably simple: evaluate different
    subsets of features on the ML model and choose the one that achieves the best
    score in a predetermined objective function. What varies here is the search strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential Forward Selection** (**SFS**): This approach begins without a
    feature and adds one, one at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential Forward Floating Selection** (**SFFS**): The same as the previous
    except for every feature it adds, it can remove one, as long as the objective
    function increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential Backward Selection** (**SBS**): This process begins with all features
    present and eliminates one feature at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequential Floating Backward Selection** (**SFBS**): The same as the previous
    except for every feature it removes, it can add one, as long as the objective
    function increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exhaustive Feature Selection** (**EFS**): This approach seeks all possible
    combinations of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BiDirectional Search** (**BDS**): This last one simultaneously allows both
    forward and backward function selection to get one unique solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methods are greedy algorithms because they solve the problem piece by
    piece, choosing pieces based on their immediate benefit. Even though they may
    arrive at a global maximum, they take an approach more suited for finding local
    maxima. Depending on the number of features, they might be too computationally
    expensive to be practical, especially EFS, which grows exponentially. Another
    important distinction is the difference between forward methods’ accuracy increases
    as features are added and backward ones, monitor accuracy decreases as features
    are removed. To allow for shorter search times, we will do two things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start our search with the features collectively selected by other methods to
    have a smaller feature space to choose from. To that end, we combine feature lists
    from several methods into a single `top_cols` list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sample our datasets so that ML models speed up. We can use `np.random.choice`
    to do a random selection of row indexes without replacement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Out of the wrapper methods presented, we will only perform SFS, given how time-consuming
    they are. Still, with an even smaller dataset, you can try the other options,
    which the `mlextend` library also supports.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential forward selection (SFS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first argument of a wrapper method is an unfitted estimator (a model).
    In `SequentialFeatureSelector`, we are placing a `LinearDiscriminantAnalysis`
    model. Other arguments include the direction (`forward=true`), whether it’s floating
    (`floating=False`), which means it might undo the previous exclusion or inclusion
    of a feature, the number of features we wish to select (`k_features=27`), the
    number of cross-validations (`cv=3`), and the loss function to use (`scoring=f1`).
    Some recommended optional arguments to enter are the verbosity (`verbose=2`) and
    the number of jobs to run in parallel (`n_jobs=-1`). Since it could take a while,
    we’ll definitely want it to output something and use as many processors as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Once we fit the SFS, it will return the index of features that have been selected
    with `k_feature_idx_`, and we can use those to subset the columns and obtain the
    list of feature names.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting with 435 features, there are over 10^(42) combinations of 27 feature
    subsets alone! So, you can see how EFS would be impractical in such a large feature
    space. Therefore, except for EFS on the entire dataset, wrapper methods will invariably
    take some shortcuts to select the features. Whether you are going forward, backward,
    or both, as long as you are not assessing every single combination of features,
    you could easily miss out on the best one.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can leverage the more rigorous, exhaustive search approach of wrapper
    methods with filter and embedded methods’ efficiency. The result of this is **hybrid
    methods**. For instance, you could employ filter or embedded methods to derive
    only the top 10 features and perform EFS or SBS on only those.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive Feature Elimination (RFE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another, more common approach is something such as SBS, but instead of removing
    features based on improving a metric alone, using the model’s intrinsic parameters
    to rank the features and only removing the least ranked. The name of this approach
    is **Recursive Feature Elimination** (**RFE**), and it is a hybrid between embedded
    and wrapper methods. We can only use models with `feature_importances_` or coefficients
    (`coef_`) because this is how the method knows what features to remove. Model
    classes in scikit-learn with these attributes are classified under `linear_model`,
    `tree`, and `ensemble`. Also, scikit-learn-compatible versions of XGBoost, LightGBM,
    and CatBoost also have `feature_importances_`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the cross-validated version of RFE because it’s more reliable.
    `RFECV` takes the estimator first (`LinearDiscriminantAnalysis`). We can then
    define `step`, which sets how many features it should remove in every iteration,
    the number of cross-validations (`cv`), and the metric used for evaluation (`scoring`).
    Lastly, it is recommended to set the verbosity (`verbose=2`) and leverage as many
    processors as possible (`n_jobs=-1`). To speed it up, we will use a sample again
    for the training and start with the 267 for `top_cols`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will try different methods that don’t relate to the main three feature
    selection categories: filter, embedded, and wrapper.'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many methods can be categorized under advanced feature selection methods, including
    the following subcategories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-agnostic feature importance**: Any feature importance method covered
    in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*, can be used to
    obtain the top features of a model for feature selection purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Genetic algorithms**: This is a wrapper method in the sense that it “wraps”
    a model assessing predictive performance across many feature subsets. However,
    unlike the wrapper methods we examined, it doesn’t make the most locally optimal
    choice. It’s more optimized to work with large feature spaces. It’s called genetic
    because it’s inspired by biology—natural selection, specifically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality reduction**: Some dimensionality reduction methods, such as
    **Principal Component Analysis** (**PCA**), can return explained variance on a
    feature basis. For others, such as factor analysis, it can be derived from other
    outputs. Explained variance can be used to rank features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoders**: We won’t delve into this one, but deep learning can be leveraged
    for feature selection with autoencoders. This method has many variants you can
    find in Google Scholar and is not widely adopted in industry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will briefly cover the first two in this section so you can understand how
    they can be implemented. Let’s dive right in!
  prefs: []
  type: TYPE_NORMAL
- en: Model-agnostic feature importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A popular model-agnostic feature importance method that we have used throughout
    this book is SHAP, and it has many properties that make it more reliable than
    other methods. In the following code, we can take our best model and extract `shap_values`
    for it using `TreeExplainer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Then, the average for the absolute value of the SHAP values across the first
    dimension is what provides us with a ranking for each feature. We put this value
    in a DataFrame and sort it as we did for PCA. Lastly, also take the top 120 and
    place them in a list (`shap_cols`).
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GAs are a stochastic global optimization technique inspired by natural selection,
    which wrap a model much like wrapper methods do. However, they don’t follow a
    sequence on a step-by-step basis. GAs don’t have iterations but generations, which
    include populations of chromosomes. Each chromosome is a binary representation
    of your feature space, where 1 means to select a feature and 0 to not. Each generation
    is produced with the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection**: Like with natural selection, this is partially random (exploration)
    and partially based on what has already worked (exploitation). What has worked
    is its fitness. Fitness is assessed with a “scorer” much like wrapper methods.
    Poor fitness chromosomes are removed, whereas good ones get to reproduce through
    “crossover.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crossover**: Randomly, some good bits (or features) of each parent go to
    a child.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mutation**: Even when a chromosome has proved effective, given a low mutation
    rate, it will occasionally mutate or flip one of its bits, in other words, features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python implementation we will use has many options. We won’t explain all
    of them here, but they are documented well in the code should you be interested.
    The first attribute is the estimator. We can also define the cross-validation
    iterations (`cv=3`) and `scoring` to determine whether chromosomes are fit. There
    are some important probabilistic properties, such as the probability for a mutated
    bit (`mutation_probability`) and that bits will get exchanged (`crossover_probability`).
    Generation-wise, `n_gen_no_change` provides a means for early stopping if generations
    haven’t improved, and `generations` by default is 40, but we will use 5\. We can
    fit `GeneticSelectionCV` as you would any model. It can take a while, so it is
    best to define the verbosity and allow it to use all the processing capacity.
    Once finished, we can use the Boolean mask (`support_`) to subset the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: OK, now that we have covered a wide variety of wrapper, hybrid, and advanced
    feature selection methods in this section, let’s evaluate all of them at once
    and compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating all feature-selected models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we have done with embedded methods, we can place feature subset names (`fsnames`),
    lists (`fscols`), and corresponding `depths` in lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use the two functions we created to first iterate across all feature
    subsets, training and evaluating a model with them. Then the second function outputs
    the results of the evaluation in a DataFrame with previously trained models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Comparing metrics for all feature-selected models'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.8* shows how feature-selected models are more profitable than ones
    that include all the features compared at the same depths. Also, the embedded
    LASSO LARS with AIC (`e-llarsic`) method and the MIC (`f-mic`) filter method outperform
    all wrapper, hybrid, and advanced methods with the same depths. Still, we also
    impeded these methods by using a sample of the training dataset, which was necessary
    to speed up the process. Maybe they would have outperformed the top ones otherwise.
    However, the three feature selection methods that follow are pretty competitive:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RFE with LDA: Hybrid method (`h-rfe-lda`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logistic regression with L2 regularization: Embedded method (`e-logl2`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GAs with RF: Advanced method (`a-ga-rf`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It would make sense to spend many days running many variations of the methods
    reviewed in this book. For instance, perhaps RFE with L1 regularized logistic
    regression or GA with support vector machines with additional mutation yields
    the best model. There are so many different possibilities! Nevertheless, if you
    were forced to make a recommendation based on *Figure 10.8*, by profit alone,
    the 111-feature `e-llarsic` is the best option, but it also has higher minimum
    costs and lower maximum ROI than any of the top models. There’s a trade-off. And
    even though it has among the highest test RMSEs, the 160-feature model (`f-mic`)
    has a similar spread between max profit train and test and beat it in max ROI
    and min costs. Therefore, these are the two reasonable options. But before making
    a final determination, profitability would have to be compared side by side across
    different thresholds to assess when each model can make the most reliable predictions
    and at what costs and ROIs.
  prefs: []
  type: TYPE_NORMAL
- en: Considering feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume that the non-profit has chosen to use the model whose features
    were selected with LASSO LARS with AIC (`e-llarsic`) but would like to evaluate
    whether you can improve it further. Now that you have removed over 300 features
    that might have only marginally improved predictive performance but mostly added
    noise, you are left with more relevant features. However, you also know that 8
    features selected by `e-llars` produced the same amount of RMSE as the 111 features.
    This means that while there’s something in those extra features that improves
    profitability, it does not improve the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a feature selection standpoint, many things can be done to approach this
    problem. For instance, examine the overlap and difference of features between
    `e-llarsic` and `e-llars`, and do feature selection variations strictly on those
    features to see whether the RMSE dips on any combination while keeping or improving
    on current profitability. However, there’s also another possibility, which is
    feature engineering. There are a few important reasons you would want to perform
    feature engineering at this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Make model interpretation easier to understand**: For instance, sometimes
    features have a scale that is not intuitive, or the scale is intuitive, but the
    distribution makes it hard to understand. As long as transformations to these
    features don’t worsen model performance, there’s value in transforming the features
    to understand the outputs of interpretation methods better. As you train models
    on more engineered features, you realize what works and why it does. This will
    help you understand the model and, more importantly, the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Place guardrails on individual features**: Sometimes, features have an uneven
    distribution, and models tend to overfit in sparser areas of the feature’s histogram
    or where influential outliers exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clean up counterintuitive interactions**: Some interactions that models find
    make no sense and only exist because the features correlate, but not for the right
    reasons. They could be confounding variables or perhaps even redundant ones (such
    as the one we found in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*).
    You could decide to engineer an interaction feature or remove a redundant one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In reference to the last two reasons, we will examine feature engineering strategies
    in more detail in *Chapter 12*, *Monotonic Constraints and Model Tuning for Interpretability*.
    This section will focus on the first reason, particularly because it’s a good
    place to start since it will allow you to understand the data better until you
    know it well enough to make more transformational changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we are left with 111 features but have no idea how they relate to the target
    or each other. The first thing we ought to do is run a feature importance method.
    We can use SHAP’s `TreeExplainer` on the `e-llarsic` model. An advantage of `TreeExplainer`
    is that it can compute SHAP interaction values, `shap_interaction_values`. Instead
    of outputting an array of `(N, 111)` dimensions where *N* is the number of observations
    as `shap_values` does, it will output `(N, 111, 111)`. We can produce a `summary_plot`
    graph with it that ranks both individual features and interactions. The only difference
    for interaction values is you use `plot_type="compact_dot"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface, application, table  Description automatically generated](img/B18406_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: SHAP interaction summary plot'
  prefs: []
  type: TYPE_NORMAL
- en: We can read *Figure 10.9* as we would any summary plot except it includes bivariate
    interactions twice—first with one feature and then with another. For instance,
    `MDMAUD_A* - CLUSTER` is the interaction SHAP values for that interaction from
    `MDMAUD_A`'s perspective, so the feature values correspond to that feature alone,
    but the SHAP values are for the interaction. One thing that we can agree on here
    is that the plot is hard to read given the scale of the importance values and
    complexity of comparing bivariate interactions in no order. We will address this
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this book, chapters with tabular data have started with a data dictionary.
    This one was an exception, given that there were 435 features to begin with. Now,
    it makes sense to at the very least understand what the top features are. The
    complete data dictionary can be found at [https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt](https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt),
    but some of the features have already been changed because of categorical encoding,
    so we will explain them in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MAXRAMNT`: Continuous, the dollar amount of the largest gift to date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HVP2`: Discrete, percentage of homes with a value of >= $150,000 in the neighborhoods
    of donors (values between 0 and 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LASTGIFT`: Continuous, the dollar amount of the most recent gift'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RAMNTALL`: Continuous, the dollar amount of lifetime gifts to date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AVGGIFT`: Continuous, the average dollar amount of gifts to date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MDMAUD_A`: Ordinal, the donation amount code for donors who have given a $100
    + gift at any time in their giving history (values between 0 and 3, and -1 for
    those who have never exceeded $100). The amount code is the third byte of an **RFA**
    (**recency/frequency/amount**) major customer matrix code, which is the amount
    given. The categories are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0: Less than $100 (low dollar)'
  prefs: []
  type: TYPE_NORMAL
- en: '1: $100 – 499 (core)'
  prefs: []
  type: TYPE_NORMAL
- en: '2: $500 – 999 (major)'
  prefs: []
  type: TYPE_NORMAL
- en: '3: $1,000 + (top)'
  prefs: []
  type: TYPE_NORMAL
- en: '`NGIFTALL`: Discrete, number of lifetime gifts to date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AMT_14`: Ordinal, donation amount code of the RFA for the 14th previous promotion
    (2 years prior), which corresponds to the last dollar amount given back then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0: $0.01 – 1.99'
  prefs: []
  type: TYPE_NORMAL
- en: '1: $2.00 – 2.99'
  prefs: []
  type: TYPE_NORMAL
- en: '2: $3.00 – 4.99'
  prefs: []
  type: TYPE_NORMAL
- en: '3: $5.00 – 9.99'
  prefs: []
  type: TYPE_NORMAL
- en: '4: $10.00 – 14.99'
  prefs: []
  type: TYPE_NORMAL
- en: '5: $15.00 – 24.99'
  prefs: []
  type: TYPE_NORMAL
- en: '6: $25.00 and above'
  prefs: []
  type: TYPE_NORMAL
- en: '`DOMAIN_SOCIALCLS`: Nominal, **Socio-Economic Status** (**SES**) of the neighborhood,
    which combines with `DOMAIN_URBANICITY` (0: Urban, 1: City, 2: Suburban, 3: Town,
    and 4: Rural), meaning the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: Highest SES'
  prefs: []
  type: TYPE_NORMAL
- en: '2: Average SES, except above average for urban communities'
  prefs: []
  type: TYPE_NORMAL
- en: '3: Lowest SES, except below average for urban communities'
  prefs: []
  type: TYPE_NORMAL
- en: '4: Lowest SES for urban communities only'
  prefs: []
  type: TYPE_NORMAL
- en: '`CLUSTER`: Nominal, code indicating which cluster group the donor falls in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MINRAMNT`: Continuous, dollar amount of the smallest gift to date'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LSC2`: Discrete, percentage of Spanish-speaking families in the donor’s neighborhood
    (values between 0 and 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IC15`: Discrete, percentage of families with an income of < $15,000 in the
    donor’s neighborhood (values between 0 and 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following insights can be distilled from the preceding dictionary and *Figure
    10.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gift amounts prevail**: Seven of the top features pertain to gift amounts,
    whether it’s a total, min, max, average, or last. If you include the count of
    gifts (`NGIFTALL`), there are eight features involving donation history, making
    complete sense. So, why is this relevant? Because they are likely highly correlated
    and understanding how could hold the keys to improving the model. Perhaps other
    features can be created that distill these relationships much better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High values of continuous gift amount features have high SHAP values**: Plot
    a box plot of any of those features like this, `plt.boxplot(X_test.MAXRAMNT)`,
    and you’ll see how right-skewed these features are. Perhaps a transformation such
    as breaking them into bins—called “discretization”—or using a different scale,
    such as logarithmic (try `plt.boxplot(np.log(X_test.MAXRAMNT))`), can help interpret
    these features but also help find the pockets where the likelihood of donation
    dramatically increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relationship with the 14th previous promotion**: What happened two years
    before they made that promotion connect to the one denoted in the dataset labels?
    Were the promotional materials similar? Is there a seasonality factor occurring
    at the same time every couple of years? Maybe you can engineer a feature that
    better identifies this phenomenon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistent classifications**: `DOMAIN_SOCIALCLS` has different categories
    depending on the `DOMAIN_URBANICITY` value. We can make this consistent by using
    all five categories in the scale (Highest, Above Average, Average, Below Average,
    and Lowest) even if this means non-urban donors would be using only three. The
    advantage to doing this would be easier interpretation, and it’s highly unlikely
    it would adversely impact the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SHAP interaction summary plot can be useful for identifying feature and
    interaction rankings and some commonalities between them, but in this case (see
    *Figure 10.9*), it was hard to read. But to dig deeper into interactions, you
    first need to quantify their impact. To this end, let’s create a heatmap with
    only the top interactions as measured by their mean absolute SHAP value (`shap_rf_interact_avgs`).
    We should then set all the diagonal values to 0 (`shap_rf_interact_avgs_nodiag`)
    because these aren’t interactions but feature SHAP values, and it’s easier to
    observe the interactions without them. We can place this matrix in a DataFrame,
    but it’s a DataFrame of 111 columns and 111 rows, so to filter it by those features
    with the most interactions, we sum them and rank them with `scipy`''s `rankdata`.
    Then, we use the ranking to identify the 12 most interactive features (`most_interact_cols`)
    and subset the DataFrame by them. Finally, we plot the DataFrame as a heatmap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![Chart  Description automatically generated](img/B18406_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: SHAP interactions heatmap'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way in which we can understand feature interactions one by one is with
    SHAP’s `dependence_plot`. For instance, we can take our top feature, `MAXRAMNT`,
    and plot it with color-coded interactions with features such as `RAMNTALL`, `LSC4`,
    `HVP2`, and `AVGGIFT`. But first, we will need to compute `shap_values`. There
    are a couple of problems though that need to be addressed, which we mentioned
    earlier. They have to do with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The prevalence of outliers**: We can cut them out of the plot by limiting
    the *x*- and *y*-axes using percentiles for the feature and SHAP values, respectively,
    with `plt.xlim` and `plt.ylim`. This essentially zooms in on cases that lie between
    the 1st and 99th percentiles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lopsided distribution of dollar amount features**: It is common in any feature
    involving money for it to be right-skewed. There are many ways to simplify it,
    such as using percentiles to bin the feature, but a quick way to make it easier
    to appreciate is by using a logarithmic scale. In `matplotlib`, you can do this
    with `plt.xscale(''log'')` without any need to transform the feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code accounts for the two issues. You can try commenting out
    `xlim`, `ylim`, or `xscale` to see the big difference they individually make in
    understanding `dependence_plot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates what is shown in *Figure 10.11*. It shows how
    there’s a tipping point somewhere between 10 and 100 for `MAXRAMNT` where the
    mean impact on the model output starts to creep out, and these correlate with
    a higher `AVGGIFT` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with medium confidence](img/B18406_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: SHAP interaction plot between MAXRAMNT and AVGGIFT'
  prefs: []
  type: TYPE_NORMAL
- en: A lesson you could take from *Figure 10.11* is that a cluster is formed by certain
    values of these features and possibly a few others that increase the likelihood
    of a donation. From a feature engineering standpoint, we could take unsupervised
    methods to create special cluster features solely based on the few features you
    have identified as related. Or we could take a more manual route, comparing different
    plots to understand how to best identify clusters. We could derive binary features
    from this process or even a ratio between features that more clearly depict interactions
    or cluster belonging.
  prefs: []
  type: TYPE_NORMAL
- en: The idea here is not to reinvent the wheel trying to do what the model already
    does so well but to, first and foremost, aim for a more straightforward model
    interpretation. Hopefully, that will even have a positive impact on predictive
    performance by tidying up the features, because if you understand them better,
    maybe the model does too! It’s like smoothing a grainy image; it might confuse
    you less and the model too (see *Chapter 13*, *Adversarial Robustness*, for more
    on that)! But understanding the data better through the model has other positive
    side effects.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the lessons don’t stop with feature engineering or modeling but can
    be directly applied to promotions. What if tipping points identified could be
    used to encourage donations? Perhaps get a free mug if you donate over $*X*? Or
    set up a recurring donation of $*X* and be on the exclusive list of “silver” patrons?
  prefs: []
  type: TYPE_NORMAL
- en: We will end this topic on that curious note, but hopefully, this inspires you
    to appreciate how we can apply lessons from model interpretation to feature selection,
    engineering, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: Mission accomplished
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To approach this mission, you have reduced overfitting using primarily the toolset
    of feature selection. The non-profit is pleased with a profit lift of roughly
    30%, costing a total of $35,601, which is $30,000 less than it would cost to send
    everyone in the test dataset the mailer. However, they still want assurance that
    they can safely employ this model without worries that they’ll experience losses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ve examined how overfitting can cause the profitability
    curves not to align. Misalignment is critical because it could mean that choosing
    a threshold based on training data would not be reliable on out-of-sample data.
    So, you use `compare_df_plots` to compare profitability between the test and train
    sets as you’ve done before, but this time, for the chosen model (`rf_5_e-llarsic`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates what is shown in *Figure 10.12*. You can show
    this to the non-profit to prove that there’s a sweet spot at $0.68 that is the
    second highest profit attainable in **Test**. It is also within reach of their
    budget and achieves an ROI of 41%. More importantly, these numbers are not far
    from what they are for **Train**. Another thing that is great to see is that the
    profit curve slowly slides down for both **Train** and **Test** instead of dramatically
    falling off a cliff. The non-profit can be assured that the operation would still
    be profitable if they choose to increase the threshold. After all, they want to
    target donors from the entire mailing list, and for that to be financially feasible,
    they have to be more exclusive. Say they are using a threshold of $0.77 on the
    entire mailing list, the campaign would cost about $46,000 but return over $24,000
    in profit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Comparison between profit, costs, and ROI for the test and train
    datasets for the model with LASSO LARS via AIC features across different thresholds'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have accomplished this mission!
  prefs: []
  type: TYPE_NORMAL
- en: But there’s one crucial detail that we’d be careless if we didn’t bring up.
  prefs: []
  type: TYPE_NORMAL
- en: Although we trained this model with the next campaign in mind, the model will
    likely be used in future direct marketing campaigns without retraining. This model
    reusing presents a problem. There’s a concept called **data drift**, also known
    as **feature drift**, which is that, over time, what the model learned about the
    features concerning the target variable no longer holds true. Another concept,
    **concept drift**, is about how the definition of the target feature changes over
    time. For instance, what constitutes a profitable donor can change. Both drifts
    can happen simultaneously, and with problems involving human behavior, this is
    to be expected. Behavior is shaped by cultures, habits, attitudes, technologies,
    and fashions, which are always evolving. You can caution the non-profit that you
    can only assure them that the model will be reliable for the next campaign, but
    they can’t afford to hire you for model retraining every single time!
  prefs: []
  type: TYPE_NORMAL
- en: You can propose to the client creating a script that monitors drift directly
    on their mailing list database. If it finds significant changes in the features
    used by the model, it will alert both them and you. You could, at this point,
    trigger automatic retraining of the model. However, if the drift is due to data
    corruption, you won’t have an opportunity to address the problem. And even if
    automatic retraining is done, it can’t be deployed if performance metrics don’t
    meet predetermined standards. Either way, you should keep a close eye on predictive
    performance to be able to guarantee reliability. Reliability is an essential theme
    in model interpretability because it relates heavily to accountability. We won’t
    cover drift detection in this book, but future chapters discuss data augmentation
    (*Chapter 11*, *Bias Mitigation and Causal Inference Methods*) and adversarial
    robustness (*Chapter 13*, *Adversarial Robustness*), which pertain to reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about how irrelevant features impact model
    outcomes and how feature selection provides a toolset to solve this problem. We
    then explored many different methods in this toolset, from the most basic filter
    methods to the most advanced ones. Lastly, we broached the subject of feature
    engineering for interpretability. Feature engineering can make for a more interpretable
    model that will perform better. We will cover this topic in more detail in *Chapter
    12*, *Monotonic Constraints and Model Tuning for Interpretability*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss methods for bias mitigation and causal
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ling, C., and Li, C., 1998, *Data Mining for Direct Marketing: Problems and
    Solutions*. In Proceedings of the Fourth International Conference on Knowledge
    Discovery and Data Mining (KDD’98). AAAI Press, 73–79: [https://dl.acm.org/doi/10.5555/3000292.3000304](https://dl.acm.org/doi/10.5555/3000292.3000304)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UCI Machine Learning Repository, 1998, KDD Cup 1998 Data Data Set: [https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data](https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ross, B.C., 2014*, Mutual Information between Discrete and Continuous Data
    Sets*. PLoS ONE, 9: [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geurts, P., Ernst, D., and Wehenkel, L., 2006, *Extremely randomized trees*.
    Machine Learning, 63(1), 3-42: [https://link.springer.com/article/10.1007/s10994-006-6226-1](https://link.springer.com/article/10.1007/s10994-006-6226-1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abid, A., Balin, M.F., and Zou, J., 2019, *Concrete Autoencoders for Differentiable
    Feature Selection and Reconstruction*. ICML: [https://arxiv.org/abs/1901.09346](https://arxiv.org/abs/1901.09346)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan, F., Fu, X., Zhang, Y., and Bourgeois, A.G., 2008, *A genetic algorithm-based
    method for feature subset selection*. Soft Computing, 12, 111-120: [https://link.springer.com/article/10.1007/s00500-007-0193-8](https://link.springer.com/article/10.1007/s00500-007-0193-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calzolari, M., 2020, October 12, manuel-calzolari/sklearn-genetic: sklearn-genetic
    0.3.0 (Version 0.3.0). Zenodo: [http://doi.org/10.5281/zenodo.4081754](http://doi.org/10.5281/zenodo.4081754)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/inml](Chapter_10.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code107161072033138125.png)'
  prefs: []
  type: TYPE_IMG
