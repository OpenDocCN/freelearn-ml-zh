- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Feature Selection and Engineering for Interpretability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性特征选择与工程
- en: 'In the first three chapters, we discussed how complexity hinders **Machine
    Learning** (**ML**) interpretability. There’s a trade-off because you may need
    some complexity to maximize predictive performance, yet not to the extent that
    you cannot rely on the model to satisfy the tenets of interpretability: fairness,
    accountability, and transparency. This chapter is the first of four focused on
    how to tune for interpretability. One of the easiest ways to improve interpretability
    is through feature selection. It has many benefits, such as faster training and
    making the model easier to interpret. But if these two reasons don’t convince
    you, perhaps another one will.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前三章中，我们讨论了复杂性如何阻碍**机器学习**（**ML**）的可解释性。这里有一个权衡，因为你可能需要一些复杂性来最大化预测性能，但又不能达到无法依赖模型来满足可解释性原则：公平性、责任和透明度的程度。本章是四个专注于如何调整以实现可解释性的章节中的第一个。提高可解释性最简单的方法之一是通过特征选择。它有许多好处，例如加快训练速度并使模型更容易解释。但如果这两个原因不能说服你，也许另一个原因会。
- en: A common misunderstanding is that complex models can self-select features and
    perform well nonetheless, so why even bother to select features? Yes, many model
    classes have mechanisms that can take care of useless features, but they aren’t
    perfect. And the potential for overfitting increases with each one that remains.
    Overfitted models aren’t reliable, even if they are more accurate. So, while employing
    model mechanisms such as regularization is still highly recommended to avoid overfitting,
    feature selection is still useful.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的误解是，复杂的模型可以自行选择特征并仍然表现良好，那么为什么还要费心选择特征呢？是的，许多模型类别都有机制可以处理无用的特征，但它们并不完美。而且，随着每个剩余的机制的加入，过拟合的可能性也会增加。过拟合的模型是不可靠的，即使它们更准确。因此，虽然仍然强烈建议使用模型机制，如正则化，以避免过拟合，但特征选择仍然是有用的。
- en: In this chapter, we will comprehend how irrelevant features adversely weigh
    on the outcome of a model and thus, the importance of feature selection for model
    interpretability. Then, we will review filter-based feature selection methods
    such as **Spearman’s correlation** and learn about embedded methods such as **LASSO
    and ridge regression**. Then, we will discover wrapper methods such as **sequential
    feature selection**, and hybrid ones such as **Recursive Feature Elimination**
    (**RFE**). Lastly, even though feature engineering is typically conducted before
    selection, there’s value in exploring feature engineering for many reasons after
    the dust has settled and features have been selected.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将理解无关特征如何对模型的输出产生不利影响，从而了解特征选择对模型可解释性的重要性。然后，我们将回顾基于过滤器的特征选择方法，如**斯皮尔曼相关系数**，并了解嵌入式方法，如**LASSO和岭回归**。然后，我们将发现包装方法，如**顺序特征选择**，以及混合方法，如**递归特征消除**（**RFE**）。最后，尽管特征工程通常在选择之前进行，但在特征选择完成后，探索特征工程仍有其价值。
- en: 'These are the main topics we are going to cover in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将在本章中讨论的主要主题：
- en: Understanding the effect of irrelevant features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解无关特征的影响
- en: Reviewing filter-based feature selection methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾基于过滤器的特征选择方法
- en: Exploring embedded feature selection methods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索嵌入式特征选择方法
- en: Discovering wrapper, hybrid, and advanced feature selection methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现包装、混合和高级特征选择方法
- en: Considering feature engineering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑特征工程
- en: Let’s begin!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `scipy`, `mlxtend`,
    `sklearn-genetic-opt`, `xgboost`, `sklearn`, `matplotlib`, and `seaborn` libraries.
    Instructions on how to install all these libraries are in the *Preface*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`scipy`、`mlxtend`、`sklearn-genetic-opt`、`xgboost`、`sklearn`、`matplotlib`和`seaborn`库。有关如何安装所有这些库的说明见**前言**。
- en: 'The GitHub code for this chapter is located here: [https://packt.link/1qP4P](https://packt.link/1qP4P).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的GitHub代码位于此处：[https://packt.link/1qP4P](https://packt.link/1qP4P)。
- en: The mission
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: It has been estimated that there are over 10 million non-profits worldwide,
    and while a large portion of them have public funding, most of them depend mostly
    on private donors, both corporate and individual, to continue operations. As such,
    fundraising is mission-critical and carried out throughout the year.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 据估计，全球有超过1000万个非营利组织，尽管其中很大一部分有公共资金，但大多数组织主要依赖私人捐赠者，包括企业和个人，以继续运营。因此，筹款是至关重要的任务，并且全年都在进行。
- en: 'Year over year, donation revenue has grown, but there are several problems
    non-profits face: donor interests evolve, so a charity popular one year might
    be forgotten the next; competition is fierce between non-profits, and demographics
    are shifting. In the United States, the average donor only gives two charitable
    gifts per year and is over 64 years old. Identifying potential donors is challenging,
    and campaigns to reach them can be expensive.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 年复一年，捐款收入有所增长，但非营利组织面临一些问题：捐赠者的兴趣在变化，因此一年受欢迎的慈善机构可能在下一年被遗忘；非营利组织之间的竞争激烈，人口结构也在变化。在美国，平均捐赠者每年只捐赠两次慈善礼物，且年龄超过64岁。识别潜在捐赠者具有挑战性，而且吸引他们的活动可能成本高昂。
- en: A National Veterans Organization non-profit arm has a large mailing list of
    about 190,000 past donors and would like to send a special mailer to ask for donations.
    However, even with a special bulk discount rate, it costs them $0.68 per address.
    This adds up to over $130,000\. They only have a marketing budget of $35,000\.
    Given that they have made this a high priority, they are willing to extend the
    budget but only if the **Return On Investment** (**ROI**) is high enough to justify
    the additional cost.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个国家级退伍军人组织非营利分支拥有大约190,000名往届捐赠者的庞大邮件列表，并希望发送一份特别邮件请求捐款。然而，即使有特殊的批量折扣率，每地址的成本也高达0.68美元。这总计超过130,000美元。他们的市场预算只有35,000美元。鉴于他们已将此事列为高优先级，他们愿意扩展预算，但前提是**投资回报率**（**ROI**）足够高，以证明额外成本是合理的。
- en: To minimize the use of their limited budget, instead of mass mailing, they’d
    like to try direct mailing, which aims to identify potential donors using what
    is already known, such as past donations, geographic location, and demographic
    data. They will reach other donors via email instead, which is much cheaper, costing
    no more than $1,000 per month for their entire list. They hope this hybrid marketing
    plan will yield better results. They also recognize that high-value donors respond
    better to personalized paper mailers, while smaller donors respond better to email
    anyway.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大限度地减少使用他们有限的预算，他们希望尝试直接邮寄，目的是利用已知的信息来识别潜在捐赠者，例如过去的捐赠、地理位置和人口统计数据。他们将通过电子邮件联系其他捐赠者，这要便宜得多，整个列表的月成本不超过1,000美元。他们希望这种混合营销计划能产生更好的结果。他们还认识到，高价值捐赠者对个性化的纸质邮件响应更好，而较小的捐赠者无论如何对电子邮件的响应更好。
- en: No more than six percent of the mailing list donates to any given campaign.
    Using ML to predict human behavior is by no means an easy task, especially when
    the data categories are imbalanced. Nevertheless, success is not measured by the
    highest predictive accuracy but by profit lift. In other words, the direct mailing
    model evaluated on the test dataset should produce more profit than if they mass-mailed
    the entire dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最多只有6%的邮件列表捐赠者会对任何特定的活动进行捐赠。使用机器学习预测人类行为绝非易事，尤其是在数据类别不平衡的情况下。尽管如此，成功不是以最高的预测准确性来衡量的，而是以利润提升来衡量。换句话说，在测试数据集上评估的直接邮寄模型应该产生比如果他们向整个数据集进行群发邮件更多的利润。
- en: They have sought your assistance to use ML to produce a model that identifies
    the most probable donors, but also in a way that *guarantees* an ROI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 他们寻求您的帮助，使用机器学习（ML）来生成一个模型，以识别最可能的捐赠者，但同时也保证一个高的ROI。
- en: You received the dataset from the non-profit, which is approximately evenly
    split into training and test data. If you send the mailer to absolutely everybody
    in the test dataset, you make a profit of $11,173, but if you manage to identify
    only those who will donate, the maximum yield of $73,136 will be attained. Your
    goal is to achieve a high-profit lift and reasonable ROI. When the campaign runs,
    it will identify the most probably donors for the entire mailing list, and the
    non-profit hopes to spend not much more than $35,000 in total. However, the dataset
    has 435 columns, and some simple statistical tests and modeling exercises show
    that the data is too noisy to identify the potential donors’ reliability because
    of overfitting.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您收到了非营利组织的数据集，该数据集大约平均分为训练数据和测试数据。如果您向测试数据集中的所有人发送邮件，您将获得11,173美元的利润，但如果您能够仅识别那些会捐赠的人，最大收益将达到73,136美元。您的目标是实现高利润提升和合理的ROI。当活动进行时，它将识别整个邮件列表中最可能的捐赠者，非营利组织希望总支出不超过35,000美元。然而，数据集有435个列，一些简单的统计测试和建模练习表明，由于过度拟合，数据过于嘈杂，无法识别潜在捐赠者的可靠性。
- en: The approach
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: You’ve decided to first fit a base model with all the features and assess it
    at different levels of complexity to understand the relationship between the increased
    number of features and the propensity for the predictive model to overfit to the
    training data. Then, you will employ a series of feature selection methods ranging
    from simple filter-based methods to the most advanced ones to determine which
    one achieves the profitability and reliability goals sought by the client. Lastly,
    once a list of final features has been selected, you can try feature engineering.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定首先使用所有特征拟合一个基础模型，并在不同的复杂度级别上评估它，以了解特征数量增加与预测模型过度拟合训练数据之间的关联。然后，你将采用一系列从简单的基于过滤的方法到最先进的方法的特征选择方法，以确定哪种方法实现了客户寻求的盈利性和可靠性目标。最后，一旦选定了最终特征列表，你就可以尝试特征工程。
- en: Given the cost-sensitive nature of the problem, thresholds are important to
    optimize the profit lift. We will get into the role of thresholds later on, but
    one significant effect is that even though this is a classification problem, it
    is best to use regression models, and then use predictions to classify so that
    there’s only one threshold to tune. That is, for classification models, you would
    need a threshold for the label, say those that donated over $1, and then another
    one for probabilities predicted. On the other hand, regression predicts the donation,
    and the threshold can be optimized based on that.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于问题的成本敏感性，阈值对于优化利润提升至关重要。我们将在稍后讨论阈值的作用，但一个显著的影响是，尽管这是一个分类问题，最好使用回归模型，然后使用预测来分类，这样只有一个阈值需要调整。也就是说，对于分类模型，你需要一个用于标签的阈值，比如那些捐赠超过1美元的，然后还需要另一个用于预测概率的阈值。另一方面，回归预测捐赠金额，阈值可以根据这个进行优化。
- en: The preparations
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The code for this example can be found at [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的代码可以在[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/10/Mailer.ipynb)找到。
- en: Loading the libraries
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, we need to install the following libraries:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，我们需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mldatasets`加载数据集
- en: '`pandas`, `numpy`, and `scipy` to manipulate it'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`、`numpy`和`scipy`来操作它
- en: '`mlxtend`, `sklearn-genetic-opt`, `xgboost`, and `sklearn` (scikit-learn) to
    fit the models'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mlxtend`、`sklearn-genetic-opt`、`xgboost`和`sklearn`（scikit-learn）来拟合模型
- en: '`matplotlib` and `seaborn` to create and visualize the interpretations'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`和`seaborn`创建和可视化解释
- en: 'To load the libraries, use the following code block:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载库，请使用以下代码块：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will load and prepare the dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载并准备数据集。
- en: Understanding and preparing the data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We load the data like this into two DataFrames (`X_train` and `X_test`) with
    the features and two `numpy` arrays with corresponding labels (`y_train` and `y_test`).
    Please note that these DataFrames have already been previously prepared for us
    to remove sparse or unnecessary features, treat missing values, and encode categorical
    features:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据这样加载到两个DataFrame（`X_train`和`X_test`）中，其中包含特征，以及两个相应的`numpy`数组标签（`y_train`和`y_test`）。请注意，这些DataFrame已经为我们预先准备，以删除稀疏或不必要的特征，处理缺失值，并对分类特征进行编码：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'All features are numeric with no missing values and categorical features have
    already been one-hot encoded for us. Between both train and test mailing lists,
    there should be over 191,500 records and 435 features. You can check this is the
    case like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征都是数值型，没有缺失值，并且分类特征已经为我们进行了一热编码。在训练和测试邮件列表之间，应有超过191,500条记录和435个特征。你可以这样检查：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code should output the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码应该输出以下内容：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we can verify that the test labels have the right number of donors (`test_donors`),
    donations (`test_donations`), and hypothetical profit ranges (`test_min_profit`
    and `test_max_profit`) using the variable cost of $0.68 (`var_cost`). We can print
    these, and then do the same for the training dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用变量成本0.68（`var_cost`）验证测试标签是否有正确的捐赠者数量（`test_donors`）、捐赠金额（`test_donations`）和假设的利润范围（`test_min_profit`和`test_max_profit`）。我们可以打印这些信息，然后对训练数据集做同样的操作：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code should output the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码应该输出以下内容：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Indeed, if the non-profit mass-mailed to everyone on the test mailing list,
    they’d make about $11,000 profit but would have to go grossly over budget to achieve
    this. The non-profit recognizes that making the max profit by identifying and
    targeting only donors is nearly an impossible feat. Therefore, they would be content
    with producing a model that reliably can yield more than the min profit but with
    a smaller cost, preferably under budget.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果非营利组织向测试邮件列表上的每个人大量邮寄，他们可能会获得大约11,000美元的利润，但为了实现这一目标，他们必须严重超支。非营利组织认识到，通过仅识别和针对捐赠者来获得最大利润几乎是一项不可能完成的任务。因此，他们宁愿生产一个能够可靠地产生超过最低利润但成本更低的模型，最好是低于预算。
- en: Understanding the effect of irrelevant features
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解无关特征的影响
- en: '**Feature selection** is also known as **variable** or **attribute selection**.
    It is the method by which you can automatically or manually select a subset of
    specific features useful to the construction of ML models.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征选择**也称为**变量**或**属性选择**。这是你可以自动或手动选择一组对构建机器学习模型有用的特定特征的方法。'
- en: 'It’s not necessarily true that more features lead to better models. Irrelevant
    features can impact the learning process, leading to overfitting. Therefore, we
    need some strategies to remove any features that might adversely affect learning.
    Some of the advantages of selecting a smaller subset of features include the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并非更多的特征就一定能导致更好的模型。无关特征可能会影响学习过程，导致过拟合。因此，我们需要一些策略来移除可能对学习产生不利影响的任何特征。选择较小特征子集的一些优点包括以下内容：
- en: '*It’s easier to understand simpler models*: For instance, feature importance
    for a model that uses 15 variables is much easier to grasp than one that uses
    150 variables.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解简单的模型更容易*：例如，对于使用15个变量的模型，其特征重要性比使用150个变量的模型更容易理解。'
- en: '*Shorter training time*: Reducing the number of variables decreases the cost
    of computing, speeds up model training, and perhaps most notably, simpler models
    have quicker inference times.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩短训练时间*：减少变量的数量可以降低计算成本，加快模型训练速度，而且最值得注意的是，简单的模型具有更快的推理时间。'
- en: '*Improved generalization by reducing overfitting*: Sometimes, with little prediction
    value, many of the variables are just noise. The ML model, however, learns from
    this noise and triggers overfitting to the training data while minimizing generalization
    simultaneously. We may significantly enhance the generalization of ML models by
    removing these irrelevant or noisy features.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过减少过拟合来提高泛化能力*：有时，预测价值很小，许多变量只是噪声。然而，机器学习模型却会从这些噪声中学习，并在最小化泛化的同时触发对训练数据的过拟合。通过移除这些无关或噪声特征，我们可以显著提高机器学习模型的泛化能力。'
- en: '*Variable redundancy*: It is common for datasets to have collinear features,
    which could mean some are redundant. In cases like these, as long as no significant
    information is lost, we can retain only one of the correlated features and delete
    the others.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变量冗余*：数据集中存在共线性特征是很常见的，这可能意味着某些特征是冗余的。在这些情况下，只要没有丢失显著信息，我们只需保留一个相关的特征，删除其他特征即可。'
- en: Now, we will fit some models to demonstrate the effect of too many features.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将拟合一些模型来展示过多特征的影响。
- en: Creating a base model
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基础模型
- en: 'Let’s create a base model for our mailing list dataset to see how this plays
    out. But first, let’s set our random seed for reproducibility:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的邮件列表数据集创建一个基础模型，看看这将如何展开。但首先，让我们设置随机种子以确保可重复性：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will use XGBoost’s **Random Forest** (**RF**) regressor (`XGBRFRegressor`)
    throughout this chapter. It’s just like scikit-learn’s but faster because it uses
    second-order approximations of the objective function. It also has more options,
    such as setting the learning rate and monotonic constraints, examined in *Chapter
    12*, *Monotonic Constraints and Model Tuning for Interpretability*. We initialize
    `XGBRFRegressor` with a conservative initial `max_depth` value of `4` and always
    use `200` estimators for consistency. Then, we fit it with our training data.
    We will use `timeit` to measure how long it takes, which we save in a variable
    (`baseline_time`) for later reference:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用XGBoost的**随机森林**（**RF**）回归器（`XGBRFRegressor`）。它就像scikit-learn一样，但更快，因为它使用了目标函数的二阶近似。它还有更多选项，例如设置学习率和单调约束，这些在*第12章*，*单调约束和模型调优以提高可解释性*中进行了考察。我们以保守的初始`max_depth`值`4`初始化`XGBRFRegressor`，并始终使用`200`估计量以确保一致性。然后，我们使用我们的训练数据对其进行拟合。我们将使用`timeit`来测量它需要多长时间，并将其保存在变量（`baseline_time`）中供以后参考：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have a base model, let’s evaluate it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个基础模型了，让我们来评估它。
- en: Evaluating the model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Next, let’s create a dictionary (`reg_mdls`) to house all the models we will
    fit in this chapter to test which feature subsets produce the best models. Here,
    we can evaluate the RF model with all the features and a `max_depth` value of
    `4` (`rf_4_all`) using `evaluate_reg_mdl`. It will make a summary and a scatter
    plot with a regression line:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个字典(`reg_mdls`)来存放我们将在本章中拟合的所有模型，以测试哪些特征子集会产生最好的模型。在这里，我们可以使用`evaluate_reg_mdl`来评估具有所有特征和`max_depth`值为`4`的随机森林模型(`rf_4_all`)。它将生成一个总结和一个带有回归线的散点图：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code produces the metrics and plot shown in *Figure 10.1*:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码生成了*图10.1*中显示的指标和图表：
- en: '![Chart  Description automatically generated](img/B18406_10_01.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_10_01.png)'
- en: 'Figure 10.1: Base model predictive performance'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：基础模型的预测性能
- en: 'For a plot like the one in *Figure 10.1*, usually, a diagonal line is expected,
    so one glance at this plot would tell you that the model is not predictive. Also,
    the RMSEs may not seem bad but in the context of such a lopsided problem, they
    are dismal. Consider this: only 5% of the list makes a donation, and only 20%
    of those are over $20, so an average error of $4.3 – $4.6 is enormous.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像*图10.1*这样的图表，通常期望看到一条对角线，所以一眼看去就能判断出这个模型不具备预测性。此外，均方根误差（RMSEs）可能看起来并不糟糕，但在这种不平衡的问题背景下，它们却是令人沮丧的。考虑一下这个情况：只有5%的人捐赠，而其中只有20%的人捐赠额超过20美元，所以平均误差4.3美元至4.6美元是巨大的。
- en: 'So, is this model useless? The answer lies in what thresholds we use to classify
    with it. Let’s start by defining an array of thresholds (`threshs`), ranging from
    $0.40 to $25\. We start spacing these out by a cent until it reaches $1, then
    by 10 cents until it reaches $3, and after that, space by $1:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这个模型有没有用呢？答案在于我们用它来分类所使用的阈值。让我们首先定义一个从$0.40到$25的阈值数组(`threshs`)，我们首先以每0.01美元的间隔来设置这些阈值，直到达到$1，然后以每0.1美元的间隔设置，直到达到$3，之后以每1美元的间隔设置：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There’s a function in `mldatasets` that can compute profit at every threshold
    (`profits_by_thresh`). All it needs is the actual (`y_test`) and predicted labels,
    followed by the thresholds (`threshs`), the variable cost (`var_costs`), and the
    `min_profit` required. It produces a `pandas` DataFrame with the revenue, costs,
    profit, and ROI for every threshold, as long as the profit is above `min_profit`.
    Remember, we had set this minimum at the beginning of the chapter as $11,173 because
    it makes no sense to target donors under this amount. After we generate these
    profit DataFrames for the test and train datasets, we can place the maximum and
    minimum amounts in the model’s dictionary for later use. And then, we employ `compare_df_plots`
    to plot the costs, profits, and ROI ratio for testing and training for every threshold
    where it exceeded the profit minimum:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`mldatasets`中有一个函数可以计算每个阈值下的利润(`profits_by_thresh`)。它只需要实际的(`y_test`)和预测标签，然后是阈值(`threshs`)、可变成本(`var_costs`)和所需的`min_profit`。只要利润高于`min_profit`，它就会生成一个包含每个阈值的收入、成本、利润和投资回报率的`pandas`
    DataFrame。记住，我们在本章开始时将这个最低值设置为$11,173，因为针对低于这个金额的捐赠者是没有意义的。在为测试和训练数据集生成这些利润DataFrame之后，我们可以将这些最大和最小金额放入模型的字典中，以供以后使用。然后，我们使用`compare_df_plots`来绘制每个阈值的测试和训练的成本、利润和投资回报率比率，只要它超过了利润最低值：'
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_02.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图描述自动生成](img/B18406_10_02.png)'
- en: 'Figure 10.2: Comparison between profit, costs, and ROI for the test and train
    datasets for the base model across thresholds'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：测试和训练数据集在阈值下基础模型的利润、成本和投资回报率比较
- en: 'The difference in RMSEs for the train and test sets didn’t lie. The model did
    not overfit. The main reason for this is that we used relatively shallow trees
    by setting our `max_depth` value at `4`. We can easily see this effect of using
    shallow trees by computing how many features had a `feature_importances_` value
    of over 0:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集的RMSEs差异是真实的。模型没有过拟合。主要原因是我们通过将`max_depth`值设置为`4`使用了相对较浅的树。我们可以通过计算有多少特征的`feature_importances_`值超过0来轻易地看到使用浅树的效果：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The preceding code outputs `160`. In other words, only 160 were used out of
    435—there are only so many features that can be accommodated in such a shallow
    tree! Naturally, this leads to lowering overfitting, but at the same time, the
    choice of features with measures of impurity over a random selection of features
    is not necessarily the most optimal.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码输出`160`。换句话说，只有160个在435个中使用了——在这样的浅树中只能容纳这么多的特征！自然地，这会导致降低过度拟合，但与此同时，在具有杂质度量的特征与随机选择特征之间的选择并不一定是最佳选择。
- en: Training the base model at different max depths
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在不同的最大深度下训练基础模型
- en: 'So, what happens if we make the trees deeper? Let’s repeat all the steps we
    did for the shallow one but for max depths between 5 and 12:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果我们使树更深会发生什么？让我们重复之前为浅层模型所做的所有步骤，但这次的最大深度在5到12之间：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let’s plot the details in the profits DataFrames for the “deepest” model
    (with a max depth of 12) as we did before with `compare_df_plots`, producing *Figure
    10.3*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们像之前使用 `compare_df_plots` 一样，绘制“最深”模型（最大深度为12）的利润DataFrame的细节，生成*图10.3*：
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_03.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图描述自动生成](img/B18406_10_03.png)'
- en: 'Figure 10.3: Comparison between profit, costs, and ROI for the test and train
    datasets for a “deep” base model across thresholds'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.3*：比较测试和训练数据集对于“深”基础模型在阈值下的利润、成本和ROI'
- en: See how different **Test** and **Train** are this time in *Figure 10.3*. **Test**
    reaches a max of about $15,000 and **Train** exceeds $20,000\. **Train**’s costs
    dramatically fall, making the ROI orders of magnitude much higher than **Test**.
    Also, the ranges of thresholds are much different. Why is this a problem, you
    ask? If we had to guess what threshold to use to pick who to target in the next
    mailer, the optimal for **Train** is higher than for **Test**—meaning that by
    using an overfit model, we could miss the mark and underperform on unseen data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这次*图10.3*中不同的**测试**和**训练**。**测试**达到约15,000的最大值，而**训练**超过20,000。**训练**的成本大幅下降，使得投资回报率比**测试**高几个数量级。此外，阈值范围也大不相同。你可能会问，这为什么会成为问题？如果我们必须猜测使用什么阈值来选择在下一封邮件中要针对的目标，**训练**的最佳阈值高于**测试**——这意味着使用过度拟合的模型，我们可能会错过目标，并在未见过的数据上表现不佳。
- en: 'Next, let’s convert our model dictionary (`reg_mdls`) into a DataFrame and
    extract some details from it. Then, we can sort it by depth, format it, color-code
    it, and output it:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将我们的模型字典（`reg_mdls`）转换为DataFrame，并从中提取一些细节。然后，我们可以按深度排序它，格式化它，用颜色编码它，并输出它：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Table  Description automatically generated](img/B18406_10_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_10_04.png)'
- en: 'Figure 10.4: Comparing metrics for all base RF models with different depths'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.4*：比较所有基础RF模型在不同深度下的指标'
- en: We could be tempted to use `rf_11_all` with the highest profitability, but it
    would be risky to use it! A common misunderstanding is that black-box models can
    effectively cut through any number of irrelevant features. While they are often
    be able to find something of value and make the most out of it, too many features
    could hinder their reliability by overfitting to noise in the training dataset.
    Fortunately, there is a sweet spot where you can reach high profitability with
    minimal overfitting, but to get there, we have to reduce the number of features
    first!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会倾向于使用具有最高盈利能力的 `rf_11_all`，但使用它是有风险的！一个常见的误解是，黑盒模型可以有效地消除任何数量的无关特征。虽然它们通常能够找到有价值的东西并充分利用它，但过多的特征可能会因为过度拟合训练数据集中的噪声而降低它们的可靠性。幸运的是，存在一个甜蜜点，你可以以最小的过度拟合达到高盈利能力，但为了达到这一点，我们首先必须减少特征的数量！
- en: Reviewing filter-based feature selection methods
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查基于过滤的特征选择方法
- en: '**Filter-based methods** independently select features from a dataset without
    employing any ML. These methods depend only on the variables’ characteristics
    and are relatively effective, computationally inexpensive, and quick to perform.
    Therefore, being the low-hanging fruit of feature selection methods, they are
    usually the first step in any feature selection pipeline.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于过滤的方法**独立地从数据集中选择特征，而不使用任何机器学习。这些方法仅依赖于变量的特征，并且相对有效、计算成本低、执行速度快。因此，作为特征选择方法的低垂之果，它们通常是任何特征选择流程的第一步。'
- en: 'Filter-based methods can be categorized as:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过滤的方法可以分为：
- en: '**Univariate**: Individually and independently of the feature space, they evaluate
    and rate a single feature at a time. One problem that can occur with univariate
    methods is that they may filter out too much since they don’t take into consideration
    the relationship between features.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单变量**：它们独立于特征空间，一次评估和评级一个特征。单变量方法可能存在的问题是，由于它们没有考虑特征之间的关系，可能会过滤掉太多信息。'
- en: '**Multivariate**: These take into account the entire feature space and how
    features interact with each other.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多元性**：这些方法考虑整个特征空间以及特征之间的相互作用。'
- en: Overall, for the removal of obsolete, redundant, constant, duplicated, and uncorrelated
    features, filter methods are very strong. However, by not accounting for complex,
    non-linear, non-monotonic correlations and interactions that only ML models can
    find, they aren’t effective whenever these relationships are prominent in the
    data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，对于移除过时、冗余、常数、重复和不相关的特征，过滤方法非常有效。然而，由于它们没有考虑到只有机器学习模型才能发现的复杂、非线性、非单调的相关性和相互作用，当这些关系在数据中突出时，它们并不有效。
- en: 'We will review three categories of filter-based methods:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将回顾三种基于过滤的方法：
- en: Basic
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础
- en: Correlation
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性
- en: Ranking
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排序
- en: We will explain them further in their own sections.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在各自的章节中进一步解释它们。
- en: Basic filter-based methods
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础过滤方法
- en: We employ **basic filter methods** in the data preparation stage, specifically,
    the data cleaning stage, before any modeling. The reason for this is there’s a
    low risk of making feature selection decisions that would adversely impact models.
    They involve common-sense operations such as removing features that carry no information
    or duplicate it.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备阶段，我们采用**基本过滤方法**，特别是在任何建模之前的数据清洗阶段。这样做的原因是，做出可能对模型产生不利影响的特征选择决策的风险很低。这些方法涉及常识性操作，例如移除不携带信息或重复信息的特征。
- en: Constant features with a variance threshold
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有方差阈值的常数特征
- en: '**Constant features** don’t change in the training dataset and, therefore,
    carry no information, and the model can’t learn from them. We can use a univariate
    method called `VarianceThreshold`, which removes low-variance features. We will
    use a threshold of zero because we want to filter out only features with **zero
    variance**—in other words, constant features. It only works with numeric features,
    so we must first identify which features are numeric and which are categorical.
    Once we fit the method on the numeric columns, `get_support()` returns the list
    of features that aren’t constant, and we can use set algebra to return only the
    constant features (`num_const_cols`):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**常数特征**在训练数据集中不发生变化，因此不携带任何信息，模型无法从中学习。我们可以使用一个名为`VarianceThreshold`的单变量方法，它移除低方差特征。我们将使用零作为阈值，因为我们只想过滤掉具有**零方差**的特征——换句话说，就是常数特征。它仅适用于数值特征，因此我们必须首先确定哪些特征是数值的，哪些是分类的。一旦我们将方法拟合到数值列上，`get_support()`返回的不是常数特征的列表，我们可以使用集合代数来返回仅包含常数特征的集合（`num_const_cols`）：'
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In most cases, removing constant features isn’t good enough. A redundant feature
    might be almost constant or **quasi-constant**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，仅移除常数特征是不够的。一个冗余特征可能几乎是常数或**准常数**。
- en: Quasi-constant features with value_counts
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有value_counts的准常数特征
- en: '**Quasi-constant features** are almost entirely the same value. Unlike constant
    filtering, using a variance threshold won’t work because high variance and quasi-constantness
    aren’t mutually exclusive. Instead, we will iterate all features and get `value_counts()`,
    which returns the number of rows for each value. Then, divide these counts by
    the total number of rows to get a percentage and sort by the highest. If the top
    value is higher than the predetermined threshold (`thresh`), we append it to a
    list of quasi-constant columns (`quasi_const_cols`). Please note that choosing
    this threshold must be done with a lot of care and understanding of the problem.
    For instance, in this case, we know that it’s lopsided because only 5% donate,
    most of whom donate a low amount, so even a tiny percentage of a feature might
    make an impact, which is why our threshold is so high at 99.9%:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**准常数特征**几乎都是相同的值。与常数过滤不同，使用方差阈值不会起作用，因为高方差和准常数性不是互斥的。相反，我们将迭代所有特征并获取 `value_counts()`，它返回每个值的行数。然后，将这些计数除以总行数以获得百分比，并按最高百分比排序。如果最高值高于预先设定的阈值（`thresh`），则将其追加到准常数列列表（`quasi_const_cols`）中。请注意，选择此阈值必须非常谨慎，并且需要对问题有深入的理解。例如，在这种情况下，我们知道这是不平衡的，因为只有
    5% 的人捐赠，其中大多数人捐赠的金额很低，所以即使是特征的一小部分也可能产生影响，这就是为什么我们的阈值如此之高，达到 99.9%：'
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code should have printed five features, which include the three
    that were previously obtained. Next, we will deal with another form of irrelevant
    features: duplicates!'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该已经打印出五个特征，其中包括之前获得的三个。接下来，我们将处理另一种形式的不相关特征：重复项！
- en: Duplicating features
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重复特征
- en: 'Usually, when we discuss duplicates with data, we immediately think of duplicate
    rows, but **duplicate columns** are also problematic. We can find them just as
    you would find duplicate rows with the `pandas duplicated()` function, except
    we would transpose the DataFrame first, inversing columns and rows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们讨论数据中的重复项时，我们首先想到的是重复的行，但**重复的列**也是问题所在。我们可以像查找重复行一样找到它们，使用 `pandas duplicated()`
    函数，但首先需要将 DataFrame 转置，反转列和行：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding snippet outputs a list with the two duplicated columns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段输出了一个包含两个重复列的列表。
- en: Removing unnecessary features
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除不必要的特征
- en: 'Unlike other feature selection methods, which you should test with models,
    you can apply basic filter-based feature selection methods right away by removing
    the features you deemed useless. But just in case, it’s good practice to make
    a copy of the original data. Please note that we don’t include constant columns
    (`all_constant_cols`) in the columns we are to drop (`drop_cols`) because the
    quasi-constant ones already include them:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他特征选择方法不同，您应该用模型测试这些方法，您可以直接通过移除您认为无用的特征来应用基于基本过滤的特征选择方法。但以防万一，制作原始数据的副本是一个好习惯。请注意，我们不将常数列（`all_constant_cols`）包括在我们打算删除的列（`drop_cols`）中，因为准常数列已经包含它们：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, we will explore multivariate filter-based methods on the remaining features.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索剩余特征上的多变量过滤方法。
- en: Correlation filter-based methods
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于相关性的过滤方法
- en: '**Correlation filter-based methods** quantify the strength of the relationship
    between two features. It is useful for feature selection because we might want
    to filter out extremely correlated features or those that aren’t correlated with
    others at all. Either way, it is a multivariate feature selection method—bivariate,
    to be precise.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于相关性的过滤方法**量化两个特征之间关系的强度。这对于特征选择很有用，因为我们可能想要过滤掉高度相关的特征或那些与其他特征完全不相关的特征。无论如何，它是一种多变量特征选择方法——更确切地说，是双变量特征选择方法。'
- en: 'But first, we ought to choose a correlation method:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们应该选择一个相关性方法：
- en: '**Pearson’s correlation coefficient**: Measures the linear correlation between
    two features. It outputs a coefficient between -1 (negative) and 1 (positive),
    with 0 meaning no linear correlation. Like linear regression, it assumes linearity,
    normality, and homoscedasticity—that is, the error term around the linear regression
    line is similarly sized across all values.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**皮尔逊相关系数**：衡量两个特征之间的线性相关性。它输出一个介于 -1（负相关）和 1（正相关）之间的系数，0 表示没有线性相关性。与线性回归类似，它假设线性、正态性和同方差性——也就是说，线性回归线周围的误差项在所有值中大小相似。'
- en: '**Spearman’s rank correlation coefficient**: Measures the strength of monotonicity
    of two features regardless of whether they are linearly related or not. Monotonicity
    is the degree to which as one feature increases, the other one consistently increases
    or decreases. It is measured between -1 and 1, with 0 meaning no monotonic correlation.
    It makes no distribution assumptions and can work with both continuous and discrete
    features. However, its weakness is with non-monotonic relationships.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**斯皮尔曼秩相关系数**：衡量两个特征单调性的强度，无论它们是否线性相关。单调性是指一个特征增加时，另一个特征持续增加或减少的程度。它在-1和1之间衡量，0表示没有单调相关性。它不做分布假设，可以与连续和离散特征一起使用。然而，它的弱点在于非单调关系。'
- en: '**Kendall’s tau correlation coefficient**: Measures the ordinal association
    between features—that is, it computes the similarity between lists of ordered
    numbers. It also ranges between -1 and 1, but they mean low and high, respectively.
    It’s useful with discrete features.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肯德尔tau相关系数**：衡量特征之间的序数关联——也就是说，它计算有序数字列表之间的相似性。它也介于-1和1之间，但分别代表低和高。对于离散特征来说，它很有用。'
- en: 'The dataset is a mix of continuous and discrete, and we cannot make any linear
    assumptions about it, so `spearman` is the right choice. All three can be used
    with the `pandas` `corr` function though:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是连续和离散的混合，我们不能对其做出任何线性假设，因此`spearman`是正确的选择。尽管如此，所有三个都可以与`pandas`的`corr`函数一起使用：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The preceding code should output the shape of the correlation matrix, which
    is `(428, 428)`. This dimension makes sense because there are 428 features left,
    and each feature has a relationship with 428 features, including itself.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该输出相关矩阵的形状，即`(428, 428)`。这个维度是有意义的，因为还剩下428个特征，每个特征都与428个特征有关，包括它自己。
- en: 'We can now look for features to remove in the correlation matrix (`corrs`).
    Note that to do so, we must establish thresholds. For instance, we can say that
    an extremely correlated feature has an absolute value coefficient of over 0.99
    and less than 0.15 for an uncorrelated feature. With these thresholds in mind,
    we can find features that are correlated to only one feature and extremely correlated
    to more than one feature. Why one feature? Because the diagonals in a correlation
    matrix are always 1 because a feature is always perfectly correlated with itself.
    The `lambda` functions in the following code make sure we are accounting for this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在相关矩阵（`corrs`）中寻找要删除的特征。请注意，为了做到这一点，我们必须建立阈值。例如，我们可以说一个高度相关的特征具有超过0.99的绝对值系数，而对于一个不相关的特征则小于0.15。有了这些阈值，我们可以找到只与一个特征相关并且与多个特征高度相关的特征。为什么是一个特征？因为在相关矩阵的对角线总是1，因为一个特征总是与自己完美相关。以下代码中的`lambda`函数确保我们考虑到这一点：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code outputs the two lists as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码以如下方式输出两个列表：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first list contains features that are extremely correlated with ones other
    than themselves. While this is useful to know, you shouldn’t remove features from
    this list without understanding what features they are correlated with and how,
    as well as with the target. Then, only if redundancy is found, make sure you only
    remove one of them. The second list is of features uncorrelated to any others
    than themself, which, in this case, is suspicious given the sheer number of features.
    That being said, we also should inspect them one by one, especially to measure
    them against the target to see whether they are redundant. However, we will take
    a chance and make a feature subset (`corr_cols`) excluding the uncorrelated ones:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个列表包含与除自身以外的其他特征高度相关的特征。虽然了解这一点很有用，但你不应在没有理解它们与哪些特征以及如何相关，以及与目标相关的情况下从该列表中删除特征。然后，只有在发现冗余的情况下，确保只删除其中一个。第二个列表包含与除自身以外的任何其他特征都不相关的特征，鉴于特征的数量众多，这在当前情况下是可疑的。话虽如此，我们也应该逐个检查它们，特别是要衡量它们与目标的相关性，看看它们是否冗余。然而，我们将冒险排除不相关的特征，创建一个特征子集（`corr_cols`）：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code should output `419`. Let’s now fit the RF model with only
    these features. Given that there are still over 400 features, we will use a `max_depth`
    value of `11`. Except for that and a different model name (`mdlname`), it’s the
    same code as before:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该输出`419`。现在让我们只使用这些特征来拟合RF模型。鉴于仍有超过400个特征，我们将使用`max_depth`值为`11`。除了这一点和一个不同的模型名称（`mdlname`）之外，代码与之前相同：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Before we compare the results for the preceding model, let’s learn about ranking
    filter methods.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在比较前面模型的输出结果之前，让我们了解一下排名滤波方法。
- en: Ranking filter-based methods
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于排序过滤的方法
- en: '**Ranking filter-based methods** are based on statistical univariate ranking
    tests, which assess the strength of the dependency between a feature and the target.
    These are some of the most popular methods:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于排序过滤的方法**基于统计单变量排序测试，这些测试评估特征与目标之间的依赖强度。这些是一些最受欢迎的方法：'
- en: '**ANOVA F-test: Analysis of Variance** (**ANOVA**) F-test measures the linear
    dependency between features and the target. As the name suggests, it does this
    by decomposing the variance. It makes similar assumptions to linear regression,
    such as normality, independence, and homoscedasticity. In scikit-learn, you can
    use `f_regression` and `f_classification` for regression and classification, respectively,
    to rank features by the F-score yielded by the F-test.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差分析 F 检验**（**ANOVA**）F 检验衡量特征与目标之间的线性依赖性。正如其名所示，它是通过分解方差来做到这一点的。它做出了与线性回归类似的假设，例如正态性、独立性和同方差性。在
    scikit-learn 中，您可以使用 `f_regression` 和 `f_classification` 分别对回归和分类进行排序，以 F 检验产生的
    F 分数来排序特征。'
- en: '**Chi-square test of independence**: This test measures the association between
    non-negative categorical variables and binary targets, so it’s only suitable for
    classification problems. In scikit-learn, you can use `chi2`.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卡方检验独立性**：这个测试衡量非负分类变量与二元目标之间的关联性，因此它只适用于分类问题。在 scikit-learn 中，您可以使用 `chi2`。'
- en: '**Mutual information** (**MI**): Unlike the two previous methods, this one
    is derived from information theory rather than classical statistical hypothesis
    testing. It’s a different name but a concept we have already discussed in this
    book as the **Kullback-Leibler** (**KL**) **divergence** because it’s the KL for
    feature *X* and target *Y*. The Python implementation in scikit-learn uses a numerically
    stable and symmetric offshoot of KL called **Jensen-Shannon** (**JS**) divergence
    instead and leverages k-nearest neighbors to compute distances. Features can be
    ranked by MI with `mutual_info_regression` and `mutual_info_classif` for regression
    and classification, respectively.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互信息**（**MI**）：与前面两种方法不同，这种方法是从信息理论而不是经典统计假设检验中推导出来的。虽然名称不同，但这个概念我们在本书中已经讨论过，称为**库尔巴克-莱布勒**（**KL**）**散度**，因为它是对特征
    *X* 和目标 *Y* 的 KL。scikit-learn 中的 Python 实现使用了一个数值稳定的对称 KL 衍生品，称为**Jensen-Shannon**（**JS**）散度，并利用
    k-最近邻来计算距离。可以使用 `mutual_info_regression` 和 `mutual_info_classif` 分别对回归和分类进行特征排序。'
- en: 'Of the three options mentioned, the one that is most appropriate for this dataset
    is MI because we cannot assume linearity among our features, and most of them
    aren’t categorical either. We can try classification with a threshold of $0.68,
    which at least covers the cost of sending the mailer. To that end, we must first
    create a binary classification target (`y_train_class`) with that threshold:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在提到的三种选项中，最适合这个数据集的是 MI，因为我们不能假设特征之间存在线性关系，而且其中大部分也不是分类数据。我们可以尝试使用阈值为 $0.68
    的分类，这至少可以覆盖发送邮件的成本。为此，我们必须首先使用该阈值创建一个二元分类目标（`y_train_class`）：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we can use `SelectKBest` to get the top 160 features according to **MI
    Classification** (**MIC**). We then employ `get_support()` to obtain a Boolean
    vector (or mask), which tells us which features are in the top 160, and we subset
    the list of features with this mask:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `SelectKBest` 根据互信息分类（**MIC**）获取前 160 个特征。然后我们使用 `get_support()`
    获取一个布尔向量（或掩码），它告诉我们哪些特征在前 160 个中，并使用这个掩码对特征列表进行子集化：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code should confirm that there are 160 features in the `mic_cols`
    list. Incidentally, this is an arbitrary number. Ideally, we could test different
    thresholds for the classification target and *k*s for the MI, looking for the
    model that achieved the highest profit lift while underfitting the least. Next,
    we can fit the RF model as we’ve done before with the MIC features. This time,
    we will use a max depth of `5` because there are significantly fewer features:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码应该确认 `mic_cols` 列表中确实有 160 个特征。顺便说一下，这是一个任意数字。理想情况下，我们可以测试分类目标的不同阈值和 MI
    的 *k* 值，寻找在最小过拟合的同时实现最高利润提升的模型。接下来，我们将使用与之前相同的 MIC 特征拟合 RF 模型。这次，我们将使用最大深度为 `5`，因为特征数量显著减少：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, let’s plot the profits for **Test** and **Train** as we did in *Figure
    10.3*, but for the MIC model. It will produce what’s shown in *Figure 10.5*:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们像在 *图 10.3* 中所做的那样绘制 **测试** 和 **训练** 的利润，但这次是针对 MIC 模型。它将产生 *图 10.5* 中所示的内容：
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_05.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_10_05.png)'
- en: 'Figure 10.5: Comparison between profit, costs, and ROI for the test and train
    datasets for a model with MIC features across thresholds'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：具有MIC特征的模型在阈值之间的利润、成本和ROI测试和训练数据集的比较
- en: In *Figure 10.5*, you can tell that there is quite a bit of difference between
    **Test** and **Train**, yet similarities indicate minimal overfitting. For instance,
    the highest profitability can be found between 0.66 and 0.75 for **Train**, and
    while **Test** is mostly between 0.66 and 0.7, it only gradually decreases afterward.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图10.5*中，你可以看出**测试**和**训练**之间存在相当大的差异，但相似之处表明过拟合最小。例如，**训练**的最高盈利性可以在0.66和0.75之间找到，而**测试**主要在0.66和0.7之间，之后逐渐下降。
- en: Although we have visually examined the MIC model, it’s nice to have some reassurance
    by looking at raw metrics. Next, we will compare all the models we have trained
    so far using consistent metrics.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经视觉检查了MIC模型，但查看原始指标也是一种令人放心的方式。接下来，我们将使用一致的指标比较我们迄今为止训练的所有模型。
- en: Comparing filter-based methods
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较基于滤波的方法
- en: 'We have been saving metrics into a dictionary (`reg_mdls`), which we easily
    convert to a DataFrame and output as we have done before, but this time we sort
    by `max_profit_test`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将指标保存到一个字典（`reg_mdls`）中，我们很容易将其转换为DataFrame，并像之前那样输出，但这次我们按`max_profit_test`排序：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Application, table  Description automatically generated with medium confidence](img/B18406_10_06.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![应用，表格，自动生成中等置信度的描述](img/B18406_10_06.png)'
- en: 'Figure 10.6: Comparing metrics for all base models and filter-based feature-selected
    models'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：比较所有基础模型和基于滤波的特征选择模型的指标
- en: In *Figure 10.6*, we can tell that the correlation filter model (`rf_11_f-corr`)
    performs worse than the model with more features and an equal amount of `max_depth`
    `(rf_11_all)`, which suggests that we must have removed an important feature.
    As cautioned in that section, the problem with blindly setting thresholds and
    removing anything above it is that you can inadvertently remove something useful.
    Not all extremely correlated and uncorrelated features are useless, so further
    inspection is required. Next, we will explore some embedded methods that, when
    combined with cross-validation, require less oversight.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图10.6*中，我们可以看出，与具有更多特征和相同`max_depth`量的模型（`rf_11_all`）相比，相关滤波模型（`rf_11_f-corr`）的表现更差，这表明我们可能移除了一个重要的特征。正如该部分所警告的，盲目设置阈值并移除其上所有内容的问题在于你可能会无意中移除有用的东西。并非所有高度相关和无关的特征都是无用的，因此需要进一步检查。接下来，我们将探索一些嵌入方法，当与交叉验证结合使用时，需要更少的监督。
- en: Exploring embedded feature selection methods
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索嵌入特征选择方法
- en: '**Embedded methods** exist within models themselves by naturally selecting
    features during training. You can leverage the intrinsic properties of any model
    that has them to capture the features selected:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入方法**存在于模型本身中，通过训练过程中自然选择特征。你可以利用具有这些特性的任何模型的内在属性来捕获所选特征：'
- en: '**Tree-based models**: For instance, we have used the following code many times
    to count the number of features used by the RF models, which is evidence of feature
    selection naturally occurring in the learning process:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于树的模型**：例如，我们多次使用以下代码来计算RF模型使用的特征数量，这是学习过程中自然发生特征选择的证据：'
- en: '[PRE28]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: XGBoost’s RF uses gain by default, which is the average decrease in error in
    all splits where it used the feature to compute feature importance. We can increase
    the threshold above 0 to select even fewer features according to their relative
    contribution. However, by constraining the trees’ depth, we forced the model to
    choose even fewer features already.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: XGBoost的RF默认使用增益，这是在所有使用该特征进行特征重要性计算的分割中平均错误减少。我们可以将阈值提高到0以上，根据它们的相对贡献选择更少的特征。然而，通过限制树的深度，我们迫使模型选择更少的特征。
- en: '**Regularized models with coefficients**: We will study this further in *Chapter
    12*, *Monotonic Constraints and Model Tuning for Interpretability*, but many model
    classes can incorporate penalty-based regularization, such as L1, L2, and elastic
    net. However, not all of them have intrinsic parameters such as coefficients that
    can be extracted to determine which features were penalized.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有系数的正则化模型**：我们将在*第12章*，*单调约束和模型调优以提高可解释性*中进一步研究这个问题，但许多模型类可以采用基于惩罚的正则化，如L1、L2和弹性网络。然而，并非所有这些模型都具有可以提取以确定哪些特征被惩罚的内在参数，如系数。'
- en: This section will only cover regularized models given that we are using a tree-based
    model already. It’s best to leverage different model classes to get different
    perspectives on what features matter the most.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将仅涵盖正则化模型，因为我们已经使用了一个基于树的模型。最好利用不同的模型类别来获得对哪些特征最重要的不同视角。
- en: 'We covered some of these models in *Chapter 3*, *Interpretation Challenges*,
    but these are a few model classes that incorporate penalty-based regularization
    and output feature-specific coefficients:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第3章*，*解释挑战*中介绍了一些这些模型，但这些都是一些结合基于惩罚的正则化和输出特征特定系数的模型类别：
- en: '**Least Absolute Shrinkage and Selection Operator** (**LASSO**): Because it
    uses L1 penalty in the loss function, LASSO can set coefficients to 0.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小绝对收缩和选择算子**（**LASSO**）：因为它在损失函数中使用L1惩罚，所以LASSO可以将系数设置为0。'
- en: '**Least-Angle Regression** (**LARS**): Similar to LASSO but is vector-based
    and is more suitable for high-dimensional data. It is also fairer toward equally
    correlated features.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小角度回归**（**LARS**）：类似于LASSO，但基于向量，更适合高维数据。它也对等相关的特征更加公平。'
- en: '**Ridge regression**: Uses L2 penalty in the loss function and because of this,
    can only shrink coefficients of irrelevance close to 0 but not to 0.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**岭回归**：在损失函数中使用L2惩罚，因此只能将不相关的系数缩小到接近0，但不能缩小到0。'
- en: '**Elastic net regression**: Uses a mix of both L1 and L2 norms as penalties.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性网络回归**：使用L1和L2范数的混合作为惩罚。'
- en: '**Logistic regression**: Contingent on the solver, it can handle L1, L2, or
    elastic net penalties.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**：根据求解器，它可以处理L1、L2或弹性网络惩罚。'
- en: 'There are also several variations of the preceding models, such as **LASSO
    LARS**, which is a LASSO fit using the LARS algorithm, or even **LASSO LARS IC**,
    which is the same but uses AIC or BIC criteria for the model section:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的模型也有一些变体，例如**LASSO LARS**，它使用LARS算法进行LASSO拟合，或者甚至是**LASSO LARS IC**，它与前者相同，但在模型部分使用AIC或BIC准则：
- en: '**Akaike’s Information Criteria** (**AIC**): A relative goodness of fit measure
    founded in information theory'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**赤池信息准则**（**AIC**）：基于信息理论的一种相对拟合优度度量'
- en: '**Bayesian Information Criteria** (**BIC**): Has a similar formula to AIC but
    has a different penalty term'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯信息准则**（**BIC**）：与AIC具有相似的公式，但具有不同的惩罚项'
- en: 'OK, now let’s use `SelectFromModel` to extract top features from a LASSO model.
    We will use `LassoCV` because it can automatically cross-validate to find the
    optimal penalty strength. Once you fit it, we can get the feature mask with `get_support()`.
    We can then print the number of features and list of features:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们使用`SelectFromModel`从LASSO模型中提取顶级特征。我们将使用`LassoCV`，因为它可以自动进行交叉验证以找到最优的惩罚强度。一旦拟合，我们就可以使用`get_support()`获取特征掩码。然后我们可以打印特征数量和特征列表：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code outputs the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码输出以下内容：
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let’s try the same but with `LassoLarsCV`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用`LassoLarsCV`进行相同的操作：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding snippet produces the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成以下输出：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'LASSO shrunk coefficients for all but seven features to 0, and LASSO LARS did
    the same but for eight. However, notice how there’s no overlap between both lists!
    OK, so let’s try incorporating AIC model selection into LASSO LARS with `LassoLarsIC`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: LASSO将除七个特征外的所有系数缩小到0，而LASSO LARS也将八个系数缩小到0。然而，请注意这两个列表之间没有重叠！好的，那么让我们尝试将AIC模型选择与LASSO
    LARS结合使用`LassoLarsIC`：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding snippet generates the following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成以下输出：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It’s the same algorithm but with a different method for selecting the value
    of the regularization parameter. Note how this less-conservative approach expands
    the number of features to 111\. Now, so far, all of the methods we have used have
    the L1 norm. Let’s try one with L2—more specifically, L2-penalized logistic regression.
    We do exactly what we did before, but this time, we fit with the binary classification
    targets (`y_train_class`):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种相同的算法，但采用了不同的方法来选择正则化参数的值。注意这种不那么保守的方法将特征数量扩展到111个。到目前为止，我们使用的方法都具有L1范数。让我们尝试一个使用L2的——更具体地说，是L2惩罚逻辑回归。我们做的是之前所做的，但这次，我们使用二元分类目标（`y_train_class`）进行拟合：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code produces the following output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成以下输出：
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that we have a few feature subsets to test, we can place their names into
    a list (`fsnames`) and the feature subset lists into another list (`fscols`):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有几个特征子集要测试，我们可以将它们的名称放入一个列表（`fsnames`）中，将特征子集列表放入另一个列表（`fscols`）中：
- en: '[PRE37]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can then iterate across all list names and fit and evaluate our `XGBRFRegressor`
    model as we have done before, but increasing `max_depth` at every iteration:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以遍历所有列表名称，并在每次迭代中增加`max_depth`，就像我们之前做的那样来拟合和评估我们的`XGBRFRegressor`模型：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, let’s see how our embedded feature-selected models fare in comparison
    to the filtered ones. We will rerun the code we ran to output what was shown in
    *Figure 10.6*. This time, we will get what is shown in *Figure 10.7*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的嵌入式特征选择模型与过滤模型相比的表现如何。我们将重新运行之前运行的代码，输出*图10.6*中显示的内容。这次，我们将得到*图10.7*中显示的内容：
- en: '![Table  Description automatically generated with medium confidence](img/B18406_10_07.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![表描述自动生成，置信度中等](img/B18406_10_07.png)'
- en: 'Figure 10.7: Comparing metrics for all base models and filter-based and embedded
    feature-selected models'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：比较所有基础模型和基于过滤和嵌入式特征选择模型的指标
- en: According to *Figure 10.7*, three of the four embedded methods we tried produced
    models with the lowest test RMSE (`rf_5_e-llarsic`, `rf_e-lasso`, and `rf_4_e-llars`).
    They also all trained much faster than the others and are more profitable than
    any other model of equal complexity. One of them (`rf_5_e-llarsic`) is even highly
    profitable. Compare this with `rf_9_all` with similar test profitability to see
    how performance diverges from the training data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图10.7，我们尝试的四种嵌入式方法中有三种产生了具有最低测试RMSE（`rf_5_e-llarsic`、`rf_e-lasso`和`rf_4_e-llars`）的模型。它们也都比其他模型训练得快得多，并且比任何同等复杂性的模型都更有利可图。其中之一（`rf_5_e-llarsic`）甚至非常有利可图。与具有相似测试盈利能力的`rf_9_all`进行比较，看看性能如何从训练数据中偏离。
- en: Discovering wrapper, hybrid, and advanced feature selection methods
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现包装、混合和高级特征选择方法
- en: 'The feature selection methods studied so far are computationally inexpensive
    because they require no model fitting or fitting simpler white-box models. In
    this section, we will learn about other, more exhaustive methods with many possible
    tuning options. The categories of methods included here are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止研究的特征选择方法在计算上成本较低，因为它们不需要模型拟合或拟合更简单的白盒模型。在本节中，我们将了解其他更全面的方法，这些方法具有许多可能的调整选项。这里包括的方法类别如下：
- en: '**Wrapper**: Exhaustively searches for the best subset of features by fitting
    an ML model using a search strategy that measures improvement on a metric.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包装**：通过使用测量指标改进的搜索策略来拟合机器学习模型，彻底搜索最佳特征子集。'
- en: '**Hybrid**: A method that combines embedded and filter methods with wrapper
    methods.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合**：一种结合嵌入式和过滤方法以及包装方法的方法。'
- en: '**Advanced**: A method that doesn’t fall into any of the previously discussed
    categories. Examples include dimensionality reduction, model-agnostic feature
    importance, and **Genetic Algorithms** (**GAs**).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级**：一种不属于之前讨论的任何类别的的方法。例如包括降维、模型无关特征重要性和**遗传算法**（**GAs**）。'
- en: And now, let’s get started with wrapper methods!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始包装方法吧！
- en: Wrapper methods
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 包装方法
- en: 'The concept behind **wrapper methods** is reasonably simple: evaluate different
    subsets of features on the ML model and choose the one that achieves the best
    score in a predetermined objective function. What varies here is the search strategy:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法背后的概念相当简单：评估特征的不同子集在机器学习模型上的表现，并选择在预定的目标函数上实现最佳得分的那个。这里变化的是搜索策略：
- en: '**Sequential Forward Selection** (**SFS**): This approach begins without a
    feature and adds one, one at a time.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序正向选择**（**SFS**）：这种方法开始时没有特征，然后每次添加一个。'
- en: '**Sequential Forward Floating Selection** (**SFFS**): The same as the previous
    except for every feature it adds, it can remove one, as long as the objective
    function increases.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序正向浮点选择**（**SFFS**）：与之前相同，除了每次添加一个特征时，它可以移除一个特征，只要目标函数增加。'
- en: '**Sequential Backward Selection** (**SBS**): This process begins with all features
    present and eliminates one feature at a time.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序向后选择**（**SBS**）：这个过程从所有特征都存在开始，每次消除一个特征。'
- en: '**Sequential Floating Backward Selection** (**SFBS**): The same as the previous
    except for every feature it removes, it can add one, as long as the objective
    function increases.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序浮点向后选择**（**SFBS**）：与之前相同，除了每次移除一个特征时，它还可以添加一个特征，只要目标函数增加。'
- en: '**Exhaustive Feature Selection** (**EFS**): This approach seeks all possible
    combinations of features.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**穷举特征选择**（**EFS**）：这种方法寻求所有可能的特征组合。'
- en: '**BiDirectional Search** (**BDS**): This last one simultaneously allows both
    forward and backward function selection to get one unique solution.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向搜索**（**BDS**）：这个方法同时允许向前和向后进行函数选择，以获得一个独特的解决方案。'
- en: 'These methods are greedy algorithms because they solve the problem piece by
    piece, choosing pieces based on their immediate benefit. Even though they may
    arrive at a global maximum, they take an approach more suited for finding local
    maxima. Depending on the number of features, they might be too computationally
    expensive to be practical, especially EFS, which grows exponentially. Another
    important distinction is the difference between forward methods’ accuracy increases
    as features are added and backward ones, monitor accuracy decreases as features
    are removed. To allow for shorter search times, we will do two things:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法是贪婪算法，因为它们逐个解决问题，根据它们的即时利益选择部分。尽管它们可能达到全局最大值，但它们采取的方法更适合寻找局部最大值。根据特征的数量，它们可能过于计算密集，以至于不实用，特别是EFS，它呈指数增长。另一个重要的区别是，向前方法随着特征的添加而提高准确性，而向后方法则随着特征的移除而监控准确性下降。为了缩短搜索时间，我们将做两件事：
- en: 'Start our search with the features collectively selected by other methods to
    have a smaller feature space to choose from. To that end, we combine feature lists
    from several methods into a single `top_cols` list:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从其他方法共同选出的特征开始搜索，以拥有更小的特征空间进行选择。为此，我们将来自几种方法的特征列表合并成一个单一的`top_cols`列表：
- en: '[PRE39]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Sample our datasets so that ML models speed up. We can use `np.random.choice`
    to do a random selection of row indexes without replacement:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本我们的数据集，以便机器学习模型加速。我们可以使用`np.random.choice`进行随机选择行索引，而不进行替换：
- en: '[PRE40]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Out of the wrapper methods presented, we will only perform SFS, given how time-consuming
    they are. Still, with an even smaller dataset, you can try the other options,
    which the `mlextend` library also supports.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在所提出的包装方法中，我们只执行SFS，因为它们非常耗时。然而，对于更小的数据集，你可以尝试其他选项，这些选项`mlextend`库也支持。
- en: Sequential forward selection (SFS)
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顺序前向选择（SFS）
- en: 'The first argument of a wrapper method is an unfitted estimator (a model).
    In `SequentialFeatureSelector`, we are placing a `LinearDiscriminantAnalysis`
    model. Other arguments include the direction (`forward=true`), whether it’s floating
    (`floating=False`), which means it might undo the previous exclusion or inclusion
    of a feature, the number of features we wish to select (`k_features=27`), the
    number of cross-validations (`cv=3`), and the loss function to use (`scoring=f1`).
    Some recommended optional arguments to enter are the verbosity (`verbose=2`) and
    the number of jobs to run in parallel (`n_jobs=-1`). Since it could take a while,
    we’ll definitely want it to output something and use as many processors as possible:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法的第一参数是一个未拟合的估计器（一个模型）。在`SequentialFeatureSelector`中，我们放置了一个`LinearDiscriminantAnalysis`模型。其他参数包括方向（`forward=true`），是否浮动（`floating=False`），这意味着它可能会撤销之前对特征的排除或包含，我们希望选择的特征数量（`k_features=27`），交叉验证的数量（`cv=3`），以及要使用的损失函数（`scoring=f1`）。一些推荐的可选参数包括详细程度（`verbose=2`）和并行运行的工作数量（`n_jobs=-1`）。由于它可能需要一段时间，我们肯定希望它输出一些内容，并尽可能多地使用处理器：
- en: '[PRE41]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Once we fit the SFS, it will return the index of features that have been selected
    with `k_feature_idx_`, and we can use those to subset the columns and obtain the
    list of feature names.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合了SFS，它将返回使用`k_feature_idx_`选定的特征的索引，我们可以使用这些索引来子集列并获取特征名称列表。
- en: Hybrid methods
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合方法
- en: Starting with 435 features, there are over 10^(42) combinations of 27 feature
    subsets alone! So, you can see how EFS would be impractical in such a large feature
    space. Therefore, except for EFS on the entire dataset, wrapper methods will invariably
    take some shortcuts to select the features. Whether you are going forward, backward,
    or both, as long as you are not assessing every single combination of features,
    you could easily miss out on the best one.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 从435个特征开始，仅27个特征子集的组合就有超过10^(42)种！所以，你可以看到在如此大的特征空间中EFS是如何不切实际的。因此，除了在整个数据集上使用EFS之外，包装方法不可避免地会采取一些捷径来选择特征。无论你是向前、向后还是两者都进行，只要你不评估每个特征的组合，你就很容易错过最佳选择。
- en: However, we can leverage the more rigorous, exhaustive search approach of wrapper
    methods with filter and embedded methods’ efficiency. The result of this is **hybrid
    methods**. For instance, you could employ filter or embedded methods to derive
    only the top 10 features and perform EFS or SBS on only those.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以利用包装方法的更严格、更全面的搜索方法，同时结合筛选和嵌入方法的效率。这种方法的结果是**混合方法**。例如，你可以使用筛选或嵌入方法仅提取前10个特征，并在这些特征上仅执行EFS或SBS。
- en: Recursive Feature Elimination (RFE)
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归特征消除（RFE）
- en: Another, more common approach is something such as SBS, but instead of removing
    features based on improving a metric alone, using the model’s intrinsic parameters
    to rank the features and only removing the least ranked. The name of this approach
    is **Recursive Feature Elimination** (**RFE**), and it is a hybrid between embedded
    and wrapper methods. We can only use models with `feature_importances_` or coefficients
    (`coef_`) because this is how the method knows what features to remove. Model
    classes in scikit-learn with these attributes are classified under `linear_model`,
    `tree`, and `ensemble`. Also, scikit-learn-compatible versions of XGBoost, LightGBM,
    and CatBoost also have `feature_importances_`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更常见的方法是SBS，但它不是仅基于改进一个指标来删除特征，而是使用模型的内在参数来对特征进行排序，并仅删除排名最低的特征。这种方法被称为**递归特征消除**（**RFE**），它是嵌入和包装方法之间的混合。我们只能使用具有`feature_importances_`或系数（`coef_`）的模型，因为这是该方法知道要删除哪些特征的方式。具有这些属性的scikit-learn模型类别被归类为`linear_model`、`tree`和`ensemble`。此外，XGBoost、LightGBM和CatBoost的scikit-learn兼容版本也具有`feature_importances_`。
- en: 'We will use the cross-validated version of RFE because it’s more reliable.
    `RFECV` takes the estimator first (`LinearDiscriminantAnalysis`). We can then
    define `step`, which sets how many features it should remove in every iteration,
    the number of cross-validations (`cv`), and the metric used for evaluation (`scoring`).
    Lastly, it is recommended to set the verbosity (`verbose=2`) and leverage as many
    processors as possible (`n_jobs=-1`). To speed it up, we will use a sample again
    for the training and start with the 267 for `top_cols`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用交叉验证版本的递归特征消除（RFE），因为它更可靠。`RFECV`首先采用估计器（`LinearDiscriminantAnalysis`）。然后我们可以定义`step`，它设置每次迭代应删除多少特征，交叉验证的次数（`cv`），以及用于评估的指标（`scoring`）。最后，建议设置详细程度（`verbose=2`）并尽可能利用更多处理器（`n_jobs=-1`）。为了加快速度，我们将再次使用样本进行训练，并从`top_cols`的267开始：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we will try different methods that don’t relate to the main three feature
    selection categories: filter, embedded, and wrapper.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试与主要三个特征选择类别（筛选、嵌入和包装）无关的不同方法。
- en: Advanced methods
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级方法
- en: 'Many methods can be categorized under advanced feature selection methods, including
    the following subcategories:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法可以归类为高级特征选择方法，包括以下子类别：
- en: '**Model-agnostic feature importance**: Any feature importance method covered
    in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*, can be used to
    obtain the top features of a model for feature selection purposes.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型无关特征重要性**：任何在*第4章*、*全局模型无关解释方法*中提到的特征重要性方法都可以用于获取模型的特征选择中的顶级特征。'
- en: '**Genetic algorithms**: This is a wrapper method in the sense that it “wraps”
    a model assessing predictive performance across many feature subsets. However,
    unlike the wrapper methods we examined, it doesn’t make the most locally optimal
    choice. It’s more optimized to work with large feature spaces. It’s called genetic
    because it’s inspired by biology—natural selection, specifically.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗传算法**：这是一种包装方法，因为它“包装”了一个评估多个特征子集预测性能的模型。然而，与我们所检查的包装方法不同，它并不总是做出最局部最优的选择。它更适合与大型特征空间一起工作。它被称为遗传算法，因为它受到了生物学的启发——自然选择，特别是。'
- en: '**Dimensionality reduction**: Some dimensionality reduction methods, such as
    **Principal Component Analysis** (**PCA**), can return explained variance on a
    feature basis. For others, such as factor analysis, it can be derived from other
    outputs. Explained variance can be used to rank features.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：一些降维方法，如**主成分分析**（**PCA**），可以在特征基础上返回解释方差。对于其他方法，如因子分析，它可以从其他输出中推导出来。解释方差可以用于对特征进行排序。'
- en: '**Autoencoders**: We won’t delve into this one, but deep learning can be leveraged
    for feature selection with autoencoders. This method has many variants you can
    find in Google Scholar and is not widely adopted in industry.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动编码器**：我们不会深入探讨这一点，但深度学习可以利用自动编码器进行特征选择。这种方法在Google Scholar上有许多变体，但在工业界并不广泛采用。'
- en: We will briefly cover the first two in this section so you can understand how
    they can be implemented. Let’s dive right in!
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍前两种方法，以便您了解它们如何实现。让我们直接进入正题！
- en: Model-agnostic feature importance
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型无关特征重要性
- en: 'A popular model-agnostic feature importance method that we have used throughout
    this book is SHAP, and it has many properties that make it more reliable than
    other methods. In the following code, we can take our best model and extract `shap_values`
    for it using `TreeExplainer`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书的整个过程中，我们使用的一个流行的模型无关特征重要性方法是SHAP，它有许多属性使其比其他方法更可靠。在下面的代码中，我们可以使用`TreeExplainer`提取我们最佳模型的`shap_values`：
- en: '[PRE43]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Then, the average for the absolute value of the SHAP values across the first
    dimension is what provides us with a ranking for each feature. We put this value
    in a DataFrame and sort it as we did for PCA. Lastly, also take the top 120 and
    place them in a list (`shap_cols`).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，SHAP值绝对值的平均值在第一维上为我们提供了每个特征的排名。我们将这个值放入一个DataFrame中，并按我们为PCA所做的方式对其进行排序。最后，也将前120个放入一个列表（`shap_cols`）。
- en: Genetic algorithms
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遗传算法
- en: 'GAs are a stochastic global optimization technique inspired by natural selection,
    which wrap a model much like wrapper methods do. However, they don’t follow a
    sequence on a step-by-step basis. GAs don’t have iterations but generations, which
    include populations of chromosomes. Each chromosome is a binary representation
    of your feature space, where 1 means to select a feature and 0 to not. Each generation
    is produced with the following operations:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 算法遗传学（GAs）是一种受自然选择启发的随机全局优化技术，它像包装方法一样包装一个模型。然而，它们不是基于一步一步的序列。GAs没有迭代，但有代，包括染色体的种群。每个染色体是特征空间的二进制表示，其中1表示选择一个特征，0表示不选择。每一代都是通过以下操作产生的：
- en: '**Selection**: Like with natural selection, this is partially random (exploration)
    and partially based on what has already worked (exploitation). What has worked
    is its fitness. Fitness is assessed with a “scorer” much like wrapper methods.
    Poor fitness chromosomes are removed, whereas good ones get to reproduce through
    “crossover.”'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择**：就像自然选择一样，这部分是随机的（探索）和部分是基于已经有效的东西（利用）。有效的是其适应性。适应性是通过一个“scorer”来评估的，就像包装方法一样。适应性差的染色体被移除，而好的染色体则通过“交叉”繁殖。'
- en: '**Crossover**: Randomly, some good bits (or features) of each parent go to
    a child.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉**：随机地，一些好的位（或特征）从每个父代传递给子代。'
- en: '**Mutation**: Even when a chromosome has proved effective, given a low mutation
    rate, it will occasionally mutate or flip one of its bits, in other words, features.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变异**：即使染色体已经证明有效，给定一个低的突变率，它偶尔也会突变或翻转其位之一，换句话说，特征。'
- en: 'The Python implementation we will use has many options. We won’t explain all
    of them here, but they are documented well in the code should you be interested.
    The first attribute is the estimator. We can also define the cross-validation
    iterations (`cv=3`) and `scoring` to determine whether chromosomes are fit. There
    are some important probabilistic properties, such as the probability for a mutated
    bit (`mutation_probability`) and that bits will get exchanged (`crossover_probability`).
    Generation-wise, `n_gen_no_change` provides a means for early stopping if generations
    haven’t improved, and `generations` by default is 40, but we will use 5\. We can
    fit `GeneticSelectionCV` as you would any model. It can take a while, so it is
    best to define the verbosity and allow it to use all the processing capacity.
    Once finished, we can use the Boolean mask (`support_`) to subset the features:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的Python实现有许多选项。在这里我们不会解释所有这些选项，但如果您感兴趣，它们在代码中都有很好的文档说明。第一个属性是估计器。我们还可以定义交叉验证迭代次数（`cv=3`）和`scoring`来决定染色体是否适合。有一些重要的概率属性，例如突变位（`mutation_probability`）的概率和位交换（`crossover_probability`）的概率。在每一代中，`n_gen_no_change`提供了一种在代数没有改进时提前停止的手段，默认的`generations`是40，但我们将使用5。我们可以像任何模型一样拟合`GeneticSelectionCV`。这可能需要一些时间，因此最好定义详细程度并允许它使用所有处理能力。一旦完成，我们可以使用布尔掩码（`support_`）来子集特征：
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: OK, now that we have covered a wide variety of wrapper, hybrid, and advanced
    feature selection methods in this section, let’s evaluate all of them at once
    and compare the results.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经在本节中介绍了各种包装、混合和高级特征选择方法，让我们一次性评估它们并比较结果。
- en: Evaluating all feature-selected models
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估所有特征选择模型
- en: 'As we have done with embedded methods, we can place feature subset names (`fsnames`),
    lists (`fscols`), and corresponding `depths` in lists:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们对待嵌入方法一样，我们可以将特征子集名称 (`fsnames`)、列表 (`fscols`) 和相应的 `depths` 放入列表中：
- en: '[PRE45]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we can use the two functions we created to first iterate across all feature
    subsets, training and evaluating a model with them. Then the second function outputs
    the results of the evaluation in a DataFrame with previously trained models:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用我们创建的两个函数，首先遍历所有特征子集，用它们训练和评估一个模型。然后第二个函数输出评估结果，以DataFrame的形式包含先前训练的模型：
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_10_08.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_10_08.png)'
- en: 'Figure 10.8: Comparing metrics for all feature-selected models'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：比较所有特征选择模型的指标
- en: '*Figure 10.8* shows how feature-selected models are more profitable than ones
    that include all the features compared at the same depths. Also, the embedded
    LASSO LARS with AIC (`e-llarsic`) method and the MIC (`f-mic`) filter method outperform
    all wrapper, hybrid, and advanced methods with the same depths. Still, we also
    impeded these methods by using a sample of the training dataset, which was necessary
    to speed up the process. Maybe they would have outperformed the top ones otherwise.
    However, the three feature selection methods that follow are pretty competitive:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.8* 展示了与包含所有特征相比，特征选择模型在相同深度下的盈利能力更强。此外，嵌入的LASSO LARS与AIC (`e-llarsic`)
    方法和MIC (`f-mic`) 过滤方法在相同深度下优于所有包装、混合和高级方法。尽管如此，我们还是通过使用训练数据集的一个样本来阻碍了这些方法，这是加快过程所必需的。也许在其他情况下，它们会优于最顶尖的模型。然而，接下来的三种特征选择方法竞争力相当强：'
- en: 'RFE with LDA: Hybrid method (`h-rfe-lda`)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LDA的RFE：混合方法 (`h-rfe-lda`)
- en: 'Logistic regression with L2 regularization: Embedded method (`e-logl2`)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有L2正则化的逻辑回归：嵌入方法 (`e-logl2`)
- en: 'GAs with RF: Advanced method (`a-ga-rf`)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于RF的GAs：高级方法 (`a-ga-rf`)
- en: It would make sense to spend many days running many variations of the methods
    reviewed in this book. For instance, perhaps RFE with L1 regularized logistic
    regression or GA with support vector machines with additional mutation yields
    the best model. There are so many different possibilities! Nevertheless, if you
    were forced to make a recommendation based on *Figure 10.8*, by profit alone,
    the 111-feature `e-llarsic` is the best option, but it also has higher minimum
    costs and lower maximum ROI than any of the top models. There’s a trade-off. And
    even though it has among the highest test RMSEs, the 160-feature model (`f-mic`)
    has a similar spread between max profit train and test and beat it in max ROI
    and min costs. Therefore, these are the two reasonable options. But before making
    a final determination, profitability would have to be compared side by side across
    different thresholds to assess when each model can make the most reliable predictions
    and at what costs and ROIs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中回顾的方法有很多变体，花费很多天去运行这些变体是有意义的。例如，也许RFE与L1正则化的逻辑回归或GA与支持向量机以及额外的突变会产生最佳模型。有如此多的不同可能性！然而，如果你被迫仅基于
    *图10.8* 中的利润来做出推荐，那么111特征的 `e-llarsic` 是最佳选择，但它也有比任何顶级模型更高的最低成本和更低的最高回报率。这是一个权衡。尽管它的测试RMSE值最高，但160特征的模型
    (`f-mic`) 在最大利润训练和测试之间的差异相似，并且在最大回报率和最低成本方面超过了它。因此，这两个选项是合理的。但在做出最终决定之前，必须将不同阈值下的盈利能力进行比较，以评估每个模型在什么成本和回报率下可以做出最可靠的预测。
- en: Considering feature engineering
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑特征工程
- en: Let’s assume that the non-profit has chosen to use the model whose features
    were selected with LASSO LARS with AIC (`e-llarsic`) but would like to evaluate
    whether you can improve it further. Now that you have removed over 300 features
    that might have only marginally improved predictive performance but mostly added
    noise, you are left with more relevant features. However, you also know that 8
    features selected by `e-llars` produced the same amount of RMSE as the 111 features.
    This means that while there’s something in those extra features that improves
    profitability, it does not improve the RMSE.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 假设非营利组织选择了使用具有LASSO LARS与AIC (`e-llarsic`) 选择特征的模型，但想评估你是否可以进一步改进它。现在你已经移除了可能只略微提高预测性能但主要增加噪声的300多个特征，你剩下的是更相关的特征。然而，你也知道，`e-llars`
    选出的8个特征产生了与111个特征相同的RMSE。这意味着虽然那些额外特征中有些东西可以提高盈利能力，但它并没有提高RMSE。
- en: 'From a feature selection standpoint, many things can be done to approach this
    problem. For instance, examine the overlap and difference of features between
    `e-llarsic` and `e-llars`, and do feature selection variations strictly on those
    features to see whether the RMSE dips on any combination while keeping or improving
    on current profitability. However, there’s also another possibility, which is
    feature engineering. There are a few important reasons you would want to perform
    feature engineering at this stage:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从特征选择的角度来看，可以采取许多方法来解决这个问题。例如，检查`e-llarsic`和`e-llars`之间特征的交集和差异，并在这些特征上严格进行特征选择，以查看RMSE是否在任何组合中下降，同时保持或提高当前的盈利能力。然而，还有一种可能性，那就是特征工程。在这个阶段进行特征工程有几个重要的原因：
- en: '**Make model interpretation easier to understand**: For instance, sometimes
    features have a scale that is not intuitive, or the scale is intuitive, but the
    distribution makes it hard to understand. As long as transformations to these
    features don’t worsen model performance, there’s value in transforming the features
    to understand the outputs of interpretation methods better. As you train models
    on more engineered features, you realize what works and why it does. This will
    help you understand the model and, more importantly, the data.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使模型解释更容易理解**：例如，有时特征有一个不直观的尺度，或者尺度是直观的，但分布使得理解变得困难。只要对这些特征的转换不会降低模型性能，转换特征以更好地理解解释方法的输出是有价值的。随着你在更多工程化特征上训练模型，你会意识到什么有效以及为什么有效。这将帮助你理解模型，更重要的是，理解数据。'
- en: '**Place guardrails on individual features**: Sometimes, features have an uneven
    distribution, and models tend to overfit in sparser areas of the feature’s histogram
    or where influential outliers exist.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对单个特征设置护栏**：有时，特征分布不均匀，模型倾向于在特征直方图的稀疏区域或存在重要异常值的地方过拟合。'
- en: '**Clean up counterintuitive interactions**: Some interactions that models find
    make no sense and only exist because the features correlate, but not for the right
    reasons. They could be confounding variables or perhaps even redundant ones (such
    as the one we found in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*).
    You could decide to engineer an interaction feature or remove a redundant one.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理反直觉的交互**：一些模型发现的不合逻辑的交互，仅因为特征相关，但并非出于正确的原因而存在。它们可能是混淆变量，甚至可能是冗余的（例如我们在*第4章*，*全局模型无关解释方法*中找到的）。你可以决定设计一个交互特征或删除一个冗余的特征。'
- en: In reference to the last two reasons, we will examine feature engineering strategies
    in more detail in *Chapter 12*, *Monotonic Constraints and Model Tuning for Interpretability*.
    This section will focus on the first reason, particularly because it’s a good
    place to start since it will allow you to understand the data better until you
    know it well enough to make more transformational changes.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最后两个原因，我们将在*第12章*，*单调约束和模型调优以实现可解释性*中更详细地研究特征工程策略。本节将重点介绍第一个原因，尤其是因为它是一个很好的起点，因为它将允许你更好地理解数据，直到你足够了解它，可以做出更转型的改变。
- en: 'So, we are left with 111 features but have no idea how they relate to the target
    or each other. The first thing we ought to do is run a feature importance method.
    We can use SHAP’s `TreeExplainer` on the `e-llarsic` model. An advantage of `TreeExplainer`
    is that it can compute SHAP interaction values, `shap_interaction_values`. Instead
    of outputting an array of `(N, 111)` dimensions where *N* is the number of observations
    as `shap_values` does, it will output `(N, 111, 111)`. We can produce a `summary_plot`
    graph with it that ranks both individual features and interactions. The only difference
    for interaction values is you use `plot_type="compact_dot"`:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们剩下111个特征，但不知道它们如何与目标或彼此相关。我们首先应该做的是运行一个特征重要性方法。我们可以在`e-llarsic`模型上使用SHAP的`TreeExplainer`。`TreeExplainer`的一个优点是它可以计算SHAP交互值，`shap_interaction_values`。与`shap_values`输出一个`(N,
    111)`维度的数组不同，其中*N*是观察数量，它将输出`(N, 111, 111)`。我们可以用它生成一个`summary_plot`图，该图对单个特征和交互进行排名。交互值唯一的区别是您使用`plot_type="compact_dot"`：
- en: '[PRE47]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Graphical user interface, application, table  Description automatically generated](img/B18406_10_09.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，表格  自动生成的描述](img/B18406_10_09.png)'
- en: 'Figure 10.9: SHAP interaction summary plot'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：SHAP交互总结图
- en: We can read *Figure 10.9* as we would any summary plot except it includes bivariate
    interactions twice—first with one feature and then with another. For instance,
    `MDMAUD_A* - CLUSTER` is the interaction SHAP values for that interaction from
    `MDMAUD_A`'s perspective, so the feature values correspond to that feature alone,
    but the SHAP values are for the interaction. One thing that we can agree on here
    is that the plot is hard to read given the scale of the importance values and
    complexity of comparing bivariate interactions in no order. We will address this
    later.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像阅读任何总结图一样阅读*图10.9*，除了它包含了两次双变量交互——首先是一个特征，然后是另一个。例如，`MDMAUD_A* - CLUSTER`是从`MDMAUD_A`的角度来看该交互的交互SHAP值，因此特征值对应于该特征本身，但SHAP值是针对交互的。我们在这里可以达成一致的是，考虑到重要性值的规模和比较无序的双变量交互的复杂性，这个图很难阅读。我们将在稍后解决这个问题。
- en: 'Throughout this book, chapters with tabular data have started with a data dictionary.
    This one was an exception, given that there were 435 features to begin with. Now,
    it makes sense to at the very least understand what the top features are. The
    complete data dictionary can be found at [https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt](https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt),
    but some of the features have already been changed because of categorical encoding,
    so we will explain them in more detail here:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，带有表格数据的章节通常以数据字典开始。这个例外是因为一开始有435个特征。现在，至少了解哪些是顶级特征是有意义的。完整的数据字典可以在[https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt](https://kdd.ics.uci.edu/databases/kddcup98/epsilon_mirror/cup98dic.txt)找到，但由于分类编码，一些特征已经发生了变化，因此我们将在这里更详细地解释它们：
- en: '`MAXRAMNT`: Continuous, the dollar amount of the largest gift to date'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MAXRAMNT`: 连续型，迄今为止最大赠礼的美元金额'
- en: '`HVP2`: Discrete, percentage of homes with a value of >= $150,000 in the neighborhoods
    of donors (values between 0 and 100)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HVP2`: 离散型，捐赠者社区中价值>= $150,000的房屋比例（值在0到100之间）'
- en: '`LASTGIFT`: Continuous, the dollar amount of the most recent gift'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LASTGIFT`: 连续型，最近一次赠礼的美元金额'
- en: '`RAMNTALL`: Continuous, the dollar amount of lifetime gifts to date'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RAMNTALL`: 连续型，迄今为止终身赠礼的美元金额'
- en: '`AVGGIFT`: Continuous, the average dollar amount of gifts to date'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AVGGIFT`: 连续型，迄今为止赠礼的平均美元金额'
- en: '`MDMAUD_A`: Ordinal, the donation amount code for donors who have given a $100
    + gift at any time in their giving history (values between 0 and 3, and -1 for
    those who have never exceeded $100). The amount code is the third byte of an **RFA**
    (**recency/frequency/amount**) major customer matrix code, which is the amount
    given. The categories are as follows:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MDMAUD_A`: 序数型，对于在其捐赠历史中任何时间点都捐赠了$100+赠礼的捐赠者的捐赠金额代码（值在0到3之间，对于从未超过$100的捐赠者为-1）。金额代码是**RFA**（**最近/频率/金额**）主要客户矩阵代码的第三个字节，即捐赠的金额。类别如下：'
- en: '0: Less than $100 (low dollar)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 0：少于$100（低金额）
- en: '1: $100 – 499 (core)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 1：$100 – 499（核心）
- en: '2: $500 – 999 (major)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '2: $500 – 999 (major)'
- en: '3: $1,000 + (top)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '3: $1,000 + (top)'
- en: '`NGIFTALL`: Discrete, number of lifetime gifts to date'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NGIFTALL`: 离散型，迄今为止终身赠礼的数量'
- en: '`AMT_14`: Ordinal, donation amount code of the RFA for the 14th previous promotion
    (2 years prior), which corresponds to the last dollar amount given back then:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AMT_14`: 序数型，14次之前推广的RFA捐赠金额代码，这对应于当时最后一次捐赠的金额：'
- en: '0: $0.01 – 1.99'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '0: $0.01 – 1.99'
- en: '1: $2.00 – 2.99'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '1: $2.00 – 2.99'
- en: '2: $3.00 – 4.99'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '2: $3.00 – 4.99'
- en: '3: $5.00 – 9.99'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '3: $5.00 – 9.99'
- en: '4: $10.00 – 14.99'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '4: $10.00 – 14.99'
- en: '5: $15.00 – 24.99'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '5: $15.00 – 24.99'
- en: '6: $25.00 and above'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '6: $25.00及以上'
- en: '`DOMAIN_SOCIALCLS`: Nominal, **Socio-Economic Status** (**SES**) of the neighborhood,
    which combines with `DOMAIN_URBANICITY` (0: Urban, 1: City, 2: Suburban, 3: Town,
    and 4: Rural), meaning the following:'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DOMAIN_SOCIALCLS`: 名义型，**社会经济地位**（**SES**）的社区，它与`DOMAIN_URBANICITY`（0：城市，1：城市，2：郊区，3：镇，4：农村）结合，意味着以下：'
- en: '1: Highest SES'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 最高社会经济地位'
- en: '2: Average SES, except above average for urban communities'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '2: 平均社会经济地位，但城市社区的平均水平以上'
- en: '3: Lowest SES, except below average for urban communities'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '3: 最低社会经济地位，但城市社区的平均水平以下'
- en: '4: Lowest SES for urban communities only'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '4: 仅城市社区最低社会经济地位'
- en: '`CLUSTER`: Nominal, code indicating which cluster group the donor falls in'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CLUSTER`: 名义型，表示捐赠者所属的集群组的代码'
- en: '`MINRAMNT`: Continuous, dollar amount of the smallest gift to date'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MINRAMNT`: 连续型，迄今为止最小赠礼的美元金额'
- en: '`LSC2`: Discrete, percentage of Spanish-speaking families in the donor’s neighborhood
    (values between 0 and 100)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LSC2`: 离散型，捐赠者社区中西班牙语家庭的比例（值在0到100之间）'
- en: '`IC15`: Discrete, percentage of families with an income of < $15,000 in the
    donor’s neighborhood (values between 0 and 100)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IC15`：离散值，捐赠者所在地区家庭收入低于$15,000的家庭百分比（值在0到100之间）'
- en: 'The following insights can be distilled from the preceding dictionary and *Figure
    10.9*:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从前面的字典和*图10.9*中提炼出以下见解：
- en: '**Gift amounts prevail**: Seven of the top features pertain to gift amounts,
    whether it’s a total, min, max, average, or last. If you include the count of
    gifts (`NGIFTALL`), there are eight features involving donation history, making
    complete sense. So, why is this relevant? Because they are likely highly correlated
    and understanding how could hold the keys to improving the model. Perhaps other
    features can be created that distill these relationships much better.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**赠款金额优先**：其中七个顶级功能与赠款金额相关，无论是总额、最小值、最大值、平均值还是最后值。如果你包括赠款总数（`NGIFTALL`），则有八个特征涉及捐赠历史，这完全合理。那么，这有什么相关性呢？因为这些特征很可能高度相关，理解它们可能是提高模型的关键。也许可以创建其他特征，更好地提炼这些关系。'
- en: '**High values of continuous gift amount features have high SHAP values**: Plot
    a box plot of any of those features like this, `plt.boxplot(X_test.MAXRAMNT)`,
    and you’ll see how right-skewed these features are. Perhaps a transformation such
    as breaking them into bins—called “discretization”—or using a different scale,
    such as logarithmic (try `plt.boxplot(np.log(X_test.MAXRAMNT))`), can help interpret
    these features but also help find the pockets where the likelihood of donation
    dramatically increases.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续赠款金额特征的值较高具有高SHAP值**：像这样绘制任何这些特征的箱线图，例如`plt.boxplot(X_test.MAXRAMNT)`，你会看到这些特征是如何右偏斜的。也许通过将它们分成区间——称为“离散化”——或使用不同的尺度，如对数尺度（尝试`plt.boxplot(np.log(X_test.MAXRAMNT))`），可以帮助解释这些特征，同时也有助于找到捐赠可能性显著增加的区域。'
- en: '**Relationship with the 14th previous promotion**: What happened two years
    before they made that promotion connect to the one denoted in the dataset labels?
    Were the promotional materials similar? Is there a seasonality factor occurring
    at the same time every couple of years? Maybe you can engineer a feature that
    better identifies this phenomenon.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与第十四次促销的关系**：他们在两年前进行的促销与数据集标签中标记的促销之间发生了什么？促销材料是否相似？是否每两年发生一次季节性因素？也许你可以设计一个特征来更好地识别这种现象。'
- en: '**Inconsistent classifications**: `DOMAIN_SOCIALCLS` has different categories
    depending on the `DOMAIN_URBANICITY` value. We can make this consistent by using
    all five categories in the scale (Highest, Above Average, Average, Below Average,
    and Lowest) even if this means non-urban donors would be using only three. The
    advantage to doing this would be easier interpretation, and it’s highly unlikely
    it would adversely impact the model’s performance.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类不一致**：`DOMAIN_SOCIALCLS`根据`DOMAIN_URBANITY`值的不同而具有不同的类别。我们可以通过使用量表中的所有五个类别（最高、高于平均水平、平均水平、低于平均水平、最低）来使这一分类一致，即使这意味着非城市捐赠者只会使用三个类别。这样做的好处是更容易解释，而且不太可能对模型的性能产生不利影响。'
- en: 'The SHAP interaction summary plot can be useful for identifying feature and
    interaction rankings and some commonalities between them, but in this case (see
    *Figure 10.9*), it was hard to read. But to dig deeper into interactions, you
    first need to quantify their impact. To this end, let’s create a heatmap with
    only the top interactions as measured by their mean absolute SHAP value (`shap_rf_interact_avgs`).
    We should then set all the diagonal values to 0 (`shap_rf_interact_avgs_nodiag`)
    because these aren’t interactions but feature SHAP values, and it’s easier to
    observe the interactions without them. We can place this matrix in a DataFrame,
    but it’s a DataFrame of 111 columns and 111 rows, so to filter it by those features
    with the most interactions, we sum them and rank them with `scipy`''s `rankdata`.
    Then, we use the ranking to identify the 12 most interactive features (`most_interact_cols`)
    and subset the DataFrame by them. Finally, we plot the DataFrame as a heatmap:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP交互摘要图可以用来识别特征和交互排名以及它们之间的某些共同点，但在这种情况下（见*图10.9*），阅读起来很困难。但要深入挖掘交互，你首先需要量化它们的影响。为此，让我们创建一个热图，只包含按其平均绝对SHAP值（`shap_rf_interact_avgs`）测量的顶级交互。然后，我们应该将所有对角线值设置为0（`shap_rf_interact_avgs_nodiag`），因为这些不是交互，而是特征SHAP值，没有它们更容易观察交互。我们可以将这个矩阵放入DataFrame中，但它是一个有111列和111行的DataFrame，所以为了过滤出具有最多交互的特征，我们使用`scipy`的`rankdata`对它们求和并排名。然后，我们使用排名来识别12个最具交互性的特征（`most_interact_cols`），并按这些特征子集DataFrame。最后，我们将DataFrame绘制成热图：
- en: '[PRE48]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Chart  Description automatically generated](img/B18406_10_10.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图表  描述自动生成](img/B18406_10_10.png)'
- en: 'Figure 10.10: SHAP interactions heatmap'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：SHAP交互热图
- en: 'One way in which we can understand feature interactions one by one is with
    SHAP’s `dependence_plot`. For instance, we can take our top feature, `MAXRAMNT`,
    and plot it with color-coded interactions with features such as `RAMNTALL`, `LSC4`,
    `HVP2`, and `AVGGIFT`. But first, we will need to compute `shap_values`. There
    are a couple of problems though that need to be addressed, which we mentioned
    earlier. They have to do with the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过SHAP的`dependence_plot`逐个理解特征交互。例如，我们可以选择我们的顶级特征`MAXRAMNT`，并将其与`RAMNTALL`、`LSC4`、`HVP2`和`AVGGIFT`等特征进行颜色编码的交互绘图。但首先，我们需要计算`shap_values`。然而，还有一些问题需要解决，我们之前已经提到了。这些问题与以下内容有关：
- en: '**The prevalence of outliers**: We can cut them out of the plot by limiting
    the *x*- and *y*-axes using percentiles for the feature and SHAP values, respectively,
    with `plt.xlim` and `plt.ylim`. This essentially zooms in on cases that lie between
    the 1st and 99th percentiles.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值的普遍性**：我们可以通过使用特征和SHAP值的百分位数来限制*x*轴和*y*轴，分别用`plt.xlim`和`plt.ylim`来将这些异常值从图中剔除。这本质上是在1st和99th百分位数之间的案例上进行放大。'
- en: '**Lopsided distribution of dollar amount features**: It is common in any feature
    involving money for it to be right-skewed. There are many ways to simplify it,
    such as using percentiles to bin the feature, but a quick way to make it easier
    to appreciate is by using a logarithmic scale. In `matplotlib`, you can do this
    with `plt.xscale(''log'')` without any need to transform the feature.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金额特征的偏斜分布**：在涉及金钱的任何特征中，它通常是右偏斜的。有许多方法可以简化它，例如使用百分位数对特征进行分箱，但一个快速的方法是使用对数刻度。在`matplotlib`中，您可以通过`plt.xscale(''log'')`来实现这一点，而无需转换特征。'
- en: 'The following code accounts for the two issues. You can try commenting out
    `xlim`, `ylim`, or `xscale` to see the big difference they individually make in
    understanding `dependence_plot`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码考虑了两个问题。您可以尝试取消注释`xlim`、`ylim`或`xscale`，以查看它们各自在理解`dependence_plot`时产生的巨大差异：
- en: '[PRE49]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding code generates what is shown in *Figure 10.11*. It shows how
    there’s a tipping point somewhere between 10 and 100 for `MAXRAMNT` where the
    mean impact on the model output starts to creep out, and these correlate with
    a higher `AVGGIFT` value:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成了*图10.11*中所示的内容。它显示了`MAXRAMNT`在10到100之间有一个转折点，模型输出的平均影响开始逐渐增加，这些与更高的`AVGGIFT`值相关：
- en: '![Chart  Description automatically generated with medium confidence](img/B18406_10_11.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图表  描述自动生成，中等置信度](img/B18406_10_11.png)'
- en: 'Figure 10.11: SHAP interaction plot between MAXRAMNT and AVGGIFT'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：MAXRAMNT和AVGGIFT之间的SHAP交互图
- en: A lesson you could take from *Figure 10.11* is that a cluster is formed by certain
    values of these features and possibly a few others that increase the likelihood
    of a donation. From a feature engineering standpoint, we could take unsupervised
    methods to create special cluster features solely based on the few features you
    have identified as related. Or we could take a more manual route, comparing different
    plots to understand how to best identify clusters. We could derive binary features
    from this process or even a ratio between features that more clearly depict interactions
    or cluster belonging.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图10.11*中可以得到的教训是，这些特征的一定值以及可能的一些其他值可以增加捐赠的可能性，从而形成一个簇。从特征工程的角度来看，我们可以采用无监督方法，仅基于您已识别为相关的少数特征来创建特殊的簇特征。或者，我们可以采取更手动的方法，通过比较不同的图表来了解如何最好地识别簇。我们可以从这个过程中推导出二元特征，甚至可以推导出特征之间的比率，这些比率可以更清楚地描述交互或簇归属。
- en: The idea here is not to reinvent the wheel trying to do what the model already
    does so well but to, first and foremost, aim for a more straightforward model
    interpretation. Hopefully, that will even have a positive impact on predictive
    performance by tidying up the features, because if you understand them better,
    maybe the model does too! It’s like smoothing a grainy image; it might confuse
    you less and the model too (see *Chapter 13*, *Adversarial Robustness*, for more
    on that)! But understanding the data better through the model has other positive
    side effects.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法不是试图重新发明轮子，去做模型已经做得很好的事情，而是首先追求一个更直观的模型解释。希望这甚至可以通过整理特征对预测性能产生积极影响，因为如果您更好地理解它们，也许模型也会！这就像平滑一个颗粒感强的图像；它可能会让您和模型都少一些困惑（有关更多信息，请参阅*第13章*，*对抗鲁棒性*）！但通过模型更好地理解数据还有其他积极的影响。
- en: In fact, the lessons don’t stop with feature engineering or modeling but can
    be directly applied to promotions. What if tipping points identified could be
    used to encourage donations? Perhaps get a free mug if you donate over $*X*? Or
    set up a recurring donation of $*X* and be on the exclusive list of “silver” patrons?
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，课程不仅仅是关于特征工程或建模，还可以直接应用于促销活动。如果能够识别出转折点，能否用来鼓励捐款呢？或许如果你捐款超过*X*美元，就可以获得一个免费的杯子？或者设置一个每月捐款*X*美元的定期捐款，并成为“银牌”赞助者的专属名单之一？
- en: We will end this topic on that curious note, but hopefully, this inspires you
    to appreciate how we can apply lessons from model interpretation to feature selection,
    engineering, and much more.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以这个好奇的笔记结束这个话题，但希望这能激发你去欣赏我们如何将模型解释的教训应用到特征选择、工程以及更多方面。
- en: Mission accomplished
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: To approach this mission, you have reduced overfitting using primarily the toolset
    of feature selection. The non-profit is pleased with a profit lift of roughly
    30%, costing a total of $35,601, which is $30,000 less than it would cost to send
    everyone in the test dataset the mailer. However, they still want assurance that
    they can safely employ this model without worries that they’ll experience losses.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，你主要使用特征选择工具集来减少过拟合。非营利组织对大约30%的利润提升感到满意，总成本为35,601美元，比向测试数据集中的每个人发送邮件的成本低30,000美元。然而，他们仍然希望确保他们可以安全地使用这个模型，而不用担心会亏损。
- en: 'In this chapter, we’ve examined how overfitting can cause the profitability
    curves not to align. Misalignment is critical because it could mean that choosing
    a threshold based on training data would not be reliable on out-of-sample data.
    So, you use `compare_df_plots` to compare profitability between the test and train
    sets as you’ve done before, but this time, for the chosen model (`rf_5_e-llarsic`):'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了过拟合如何导致盈利曲线不一致。不一致性是关键的，因为它可能意味着基于训练数据选择的阈值在样本外数据上不可靠。因此，你使用`compare_df_plots`来比较测试集和训练集之间的盈利，就像你之前做的那样，但这次是为了选定的模型（`rf_5_e-llarsic`）：
- en: '[PRE50]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding code generates what is shown in *Figure 10.12*. You can show
    this to the non-profit to prove that there’s a sweet spot at $0.68 that is the
    second highest profit attainable in **Test**. It is also within reach of their
    budget and achieves an ROI of 41%. More importantly, these numbers are not far
    from what they are for **Train**. Another thing that is great to see is that the
    profit curve slowly slides down for both **Train** and **Test** instead of dramatically
    falling off a cliff. The non-profit can be assured that the operation would still
    be profitable if they choose to increase the threshold. After all, they want to
    target donors from the entire mailing list, and for that to be financially feasible,
    they have to be more exclusive. Say they are using a threshold of $0.77 on the
    entire mailing list, the campaign would cost about $46,000 but return over $24,000
    in profit:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图10.12*中所示的内容。你可以向非营利组织展示，以证明在**测试**中，0.68美元是一个甜点，是可获得的第二高利润。它也在他们的预算范围内，实现了41%的投资回报率。更重要的是，这些数字与**训练**数据非常接近。另一个令人高兴的是，**训练**和**测试**的利润曲线缓慢下降，而不是突然跌落悬崖。非营利组织可以确信，如果他们选择提高阈值，运营仍然会盈利。毕竟，他们希望针对整个邮件列表的捐赠者，为了使这从财务上可行，他们必须更加专属。比如说，他们在整个邮件列表上使用0.77美元的阈值，活动成本约为46,000美元，但利润超过24,000美元：
- en: '![Chart, line chart  Description automatically generated](img/B18406_10_12.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_10_12.png)'
- en: 'Figure 10.12: Comparison between profit, costs, and ROI for the test and train
    datasets for the model with LASSO LARS via AIC features across different thresholds'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：通过AIC特征在不同阈值下，模型使用LASSO LARS的测试集和训练集的盈利、成本和投资回报率比较
- en: Congratulations! You have accomplished this mission!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了这个任务！
- en: But there’s one crucial detail that we’d be careless if we didn’t bring up.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个关键细节，如果我们不提出来，我们可能会疏忽。
- en: Although we trained this model with the next campaign in mind, the model will
    likely be used in future direct marketing campaigns without retraining. This model
    reusing presents a problem. There’s a concept called **data drift**, also known
    as **feature drift**, which is that, over time, what the model learned about the
    features concerning the target variable no longer holds true. Another concept,
    **concept drift**, is about how the definition of the target feature changes over
    time. For instance, what constitutes a profitable donor can change. Both drifts
    can happen simultaneously, and with problems involving human behavior, this is
    to be expected. Behavior is shaped by cultures, habits, attitudes, technologies,
    and fashions, which are always evolving. You can caution the non-profit that you
    can only assure them that the model will be reliable for the next campaign, but
    they can’t afford to hire you for model retraining every single time!
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们考虑到下一场活动来训练这个模型，但这个模型很可能会在未来直接营销活动中使用，而无需重新训练。这种模型的重用带来一个问题。有一个概念叫做**数据漂移**，也称为**特征漂移**，即随着时间的推移，模型关于目标变量特征的所学内容不再成立。另一个概念，**概念漂移**，是关于目标特征定义随时间变化的情况。例如，构成有利捐赠者的条件可能会改变。这两种漂移可能同时发生，并且涉及人类行为的问题，这是可以预料的。行为受到文化、习惯、态度、技术和时尚的影响，这些总是在不断发展。您可以警告非营利组织，您只能保证模型在下一场活动中是可靠的，但他们无法承担每次都雇佣您进行模型重新训练的费用！
- en: You can propose to the client creating a script that monitors drift directly
    on their mailing list database. If it finds significant changes in the features
    used by the model, it will alert both them and you. You could, at this point,
    trigger automatic retraining of the model. However, if the drift is due to data
    corruption, you won’t have an opportunity to address the problem. And even if
    automatic retraining is done, it can’t be deployed if performance metrics don’t
    meet predetermined standards. Either way, you should keep a close eye on predictive
    performance to be able to guarantee reliability. Reliability is an essential theme
    in model interpretability because it relates heavily to accountability. We won’t
    cover drift detection in this book, but future chapters discuss data augmentation
    (*Chapter 11*, *Bias Mitigation and Causal Inference Methods*) and adversarial
    robustness (*Chapter 13*, *Adversarial Robustness*), which pertain to reliability.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以向客户提议创建一个脚本，直接监控他们的邮件列表数据库中的漂移情况。如果它检测到模型使用的特征有显著变化，它将向他们和您发出警报。在这种情况下，您可以触发模型的自动重新训练。然而，如果漂移是由于数据损坏造成的，您将没有机会解决这个问题。即使进行了自动重新训练，如果性能指标没有达到预定的标准，也无法部署。无论如何，您都应该密切关注预测性能，以确保可靠性。可靠性是模型可解释性的一个基本主题，因为它与问责制密切相关。本书不会涵盖漂移检测，但未来的章节将讨论数据增强（第11章，*偏差缓解和因果推断方法*）和对抗鲁棒性（第13章，*对抗鲁棒性*），这些都关乎可靠性。
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned about how irrelevant features impact model
    outcomes and how feature selection provides a toolset to solve this problem. We
    then explored many different methods in this toolset, from the most basic filter
    methods to the most advanced ones. Lastly, we broached the subject of feature
    engineering for interpretability. Feature engineering can make for a more interpretable
    model that will perform better. We will cover this topic in more detail in *Chapter
    12*, *Monotonic Constraints and Model Tuning for Interpretability*.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了无关特征如何影响模型结果，以及特征选择如何提供一套工具来解决此问题。然后，我们探讨了这套工具中的许多不同方法，从最基本过滤器方法到最先进的方法。最后，我们讨论了特征工程的可解释性问题。特征工程可以使模型更具可解释性，从而表现更好。我们将在第12章，*单调约束和模型调优以实现可解释性*中更详细地介绍这个主题。
- en: In the next chapter, we will discuss methods for bias mitigation and causal
    inference.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论偏差缓解和因果推断的方法。
- en: Dataset sources
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集来源
- en: 'Ling, C., and Li, C., 1998, *Data Mining for Direct Marketing: Problems and
    Solutions*. In Proceedings of the Fourth International Conference on Knowledge
    Discovery and Data Mining (KDD’98). AAAI Press, 73–79: [https://dl.acm.org/doi/10.5555/3000292.3000304](https://dl.acm.org/doi/10.5555/3000292.3000304)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling, C. 和 Li, C.，1998年，《直接营销的数据挖掘：问题和解决方案》。在第四届国际知识发现和数据挖掘会议（KDD’98）论文集中。AAAI出版社，第73-79页：[https://dl.acm.org/doi/10.5555/3000292.3000304](https://dl.acm.org/doi/10.5555/3000292.3000304)
- en: 'UCI Machine Learning Repository, 1998, KDD Cup 1998 Data Data Set: [https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data](https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UCI 机器学习仓库，1998，KDD Cup 1998 数据集：[https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data](https://archive.ics.uci.edu/ml/datasets/KDD+Cup+1998+Data)
- en: Further reading
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Ross, B.C., 2014*, Mutual Information between Discrete and Continuous Data
    Sets*. PLoS ONE, 9: [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross, B.C.，2014，*离散和连续数据集之间的互信息*。PLoS ONE，9：[https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0087357)
- en: 'Geurts, P., Ernst, D., and Wehenkel, L., 2006, *Extremely randomized trees*.
    Machine Learning, 63(1), 3-42: [https://link.springer.com/article/10.1007/s10994-006-6226-1](https://link.springer.com/article/10.1007/s10994-006-6226-1)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geurts, P., Ernst, D., 和 Wehenkel, L.，2006，*极端随机树*。Machine Learning，63(1)，3-42：[https://link.springer.com/article/10.1007/s10994-006-6226-1](https://link.springer.com/article/10.1007/s10994-006-6226-1)
- en: 'Abid, A., Balin, M.F., and Zou, J., 2019, *Concrete Autoencoders for Differentiable
    Feature Selection and Reconstruction*. ICML: [https://arxiv.org/abs/1901.09346](https://arxiv.org/abs/1901.09346)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abid, A.，Balin, M.F.，和 Zou, J.，2019，*用于可微分特征选择和重建的混凝土自编码器*。ICML：[https://arxiv.org/abs/1901.09346](https://arxiv.org/abs/1901.09346)
- en: 'Tan, F., Fu, X., Zhang, Y., and Bourgeois, A.G., 2008, *A genetic algorithm-based
    method for feature subset selection*. Soft Computing, 12, 111-120: [https://link.springer.com/article/10.1007/s00500-007-0193-8](https://link.springer.com/article/10.1007/s00500-007-0193-8)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan, F., Fu, X., Zhang, Y., 和 Bourgeois, A.G.，2008，*基于遗传算法的特征子集选择方法*。Soft Computing，12，111-120：[https://link.springer.com/article/10.1007/s00500-007-0193-8](https://link.springer.com/article/10.1007/s00500-007-0193-8)
- en: 'Calzolari, M., 2020, October 12, manuel-calzolari/sklearn-genetic: sklearn-genetic
    0.3.0 (Version 0.3.0). Zenodo: [http://doi.org/10.5281/zenodo.4081754](http://doi.org/10.5281/zenodo.4081754)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calzolari, M.，2020，10月12日，manuel-calzolari/sklearn-genetic：sklearn-genetic 0.3.0（版本
    0.3.0）。Zenodo：[http://doi.org/10.5281/zenodo.4081754](http://doi.org/10.5281/zenodo.4081754)
- en: Learn more on Discord
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的 Discord 社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_10.xhtml)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_10.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code107161072033138125.png)'
