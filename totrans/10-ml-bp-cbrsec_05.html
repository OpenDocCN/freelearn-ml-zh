<html><head></head><body>
		<div id="_idContainer057">
			<h1 id="_idParaDest-64" class="chapter-number"><a id="_idTextAnchor016"/>4</h1>
			<h1 id="_idParaDest-65">Detecting Fake Reviews</h1>
			<p>Reviews are an important element in online marketplaces as they convey the customer experience and their opinions on products. Customers heavily depend upon reviews to determine the quality of a product, the truth about various claims in the description, and the experiences of other fellow customers. However, in recent times, the number of fake reviews has increased. Fake reviews are misleading and fraudulent and cause harm to consumers. They are prevalent not only on shopping sites but also on any site where there is a notion of reputation through reviews, such as Google Maps, Yelp, Tripadvisor, and even the Google <span class="No-Break">Play Store.</span></p>
			<p>Fraudulent reviews harm the integrity of the platform and allow scammers to profit, while genuine users (sellers and customers) are harmed. As data scientists in the security space, understanding reputation manipulation and how it presents itself, as well as techniques for detecting it, is essential. This chapter focuses on examining reputation manipulation through <span class="No-Break">fake reviews.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Reviews <span class="No-Break">and integrity</span></li>
				<li><span class="No-Break">Statistical analysis</span></li>
				<li>Modeling fake reviews <span class="No-Break">with regression</span></li>
			</ul>
			<p>By the end of this chapter, you will have a clear understanding of reputation manipulation through fake reviews and how they can be detected. You will also learn about statistical tests and how to apply them for analysis and how the reviews data can be modeled <span class="No-Break">using regression.</span></p>
			<h1 id="_idParaDest-66">Technical requirements</h1>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%204"><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%204</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-67">Reviews and integrity</h1>
			<p>Let us first look at the importance of online reviews and why fake <span class="No-Break">reviews exist.</span></p>
			<h2 id="_idParaDest-68">Why fake reviews exist</h2>
			<p>E-commerce websites always <a id="_idIndexMarker272"/>have reviews for products. Reviews play an important role in the online world. Reviews allow consumers to post their experiences and facilitate peer-to-peer reputation building. Reviews are important on online platforms for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li>Online reviews <a id="_idIndexMarker273"/>provide valuable information to potential customers about the quality and performance of a product or service. Customers can read about other people’s experiences with a product or service before deciding whether to buy it <span class="No-Break">or not.</span></li>
				<li>Reviews from other customers help build trust between the seller and the buyer. Positive reviews can reassure potential customers that a product or service is worth buying, while negative reviews can warn them about <span class="No-Break">potential problems.</span></li>
				<li>Online reviews can provide businesses with valuable feedback about their products and services. This feedback can help businesses improve their offerings and <span class="No-Break">customer service.</span></li>
				<li>Online reviews can also help businesses improve their search engine rankings. Search engines such as Google take into account the number and quality of reviews when ranking websites in <span class="No-Break">search results.</span></li>
			</ul>
			<p>It is therefore natural that better reviews imply better sales and more profit for the seller. The seller has an incentive to have as many great reviews for their product as possible, and this has led to the problem of <span class="No-Break">fake reviews.</span></p>
			<p>Fake reviews are reviews that are deliberately written to mislead or deceive readers. They can be positive <a id="_idIndexMarker274"/>or negative and are usually written by individuals or companies who have a vested interest in manipulating the reputation of a product or service. Here are some common types of <span class="No-Break">fake reviews:</span></p>
			<ul>
				<li><strong class="bold">Paid reviews</strong>: Some <a id="_idIndexMarker275"/>individuals or companies pay people to write positive reviews about their products or services, even if they haven’t <span class="No-Break">used them</span></li>
				<li><strong class="bold">Fake negative reviews</strong>: Competitors or individuals with a grudge may write fake negative reviews <a id="_idIndexMarker276"/>to harm the reputation of a business <span class="No-Break">or product</span></li>
				<li><strong class="bold">Review swaps</strong>: Some <a id="_idIndexMarker277"/>individuals or companies offer to exchange positive reviews with other businesses or individuals to boost their <span class="No-Break">own ratings</span></li>
				<li><strong class="bold">Review bots</strong>: Some businesses <a id="_idIndexMarker278"/>use automated software programs to generate large numbers of fake <span class="No-Break">reviews quickly</span></li>
			</ul>
			<p>Fake reviews are <a id="_idIndexMarker279"/>problematic because they can mislead potential customers into making poor purchasing decisions. They can also harm the reputation of businesses that rely on genuine customer feedback to improve their products and services. Many online platforms have policies in place to detect and remove fake reviews to protect the integrity of their review systems. For example, Amazon explicitly bans reviewers from receiving any compensation or free products in exchange <span class="No-Break">for reviews.</span></p>
			<h2 id="_idParaDest-69">Evolution of fake reviews</h2>
			<p>The problem of fake <a id="_idIndexMarker280"/>reviews and reputation manipulation is not a new one – it has existed for decades. However, the nature of fake reviews has changed significantly, from bot-generated reviews to crowdsourced reviews and then to <span class="No-Break">incentivized reviews.</span></p>
			<h3>Bot-generated reviews</h3>
			<p>These were the <a id="_idIndexMarker281"/>very first form of fake reviews that <a id="_idIndexMarker282"/>could be onboarded by sellers at scale. Bot-generated reviews are reviews that are created using automated software programs, also known as review bots. These bots are designed to generate and post a large number of reviews quickly, without any human intervention. These reviews have several <span class="No-Break">tell-tale signs:</span></p>
			<ul>
				<li>They are usually generic and lack specific details about the product or service <span class="No-Break">being reviewed</span></li>
				<li>They may use similar language and sentence structures and often have a high number of <span class="No-Break">positive ratings</span></li>
				<li>They also exhibit similarities in terms of the IP addresses, subnets, and networks that they <span class="No-Break">come from</span></li>
			</ul>
			<p>Many online platforms have implemented measures to detect and remove bot-generated reviews to <a id="_idIndexMarker283"/>maintain the integrity of their review <a id="_idIndexMarker284"/>systems. Some of these measures include using machine learning algorithms to identify patterns in the reviews, monitoring for suspicious IP addresses and activity, and requiring reviewers to verify <span class="No-Break">their identities.</span></p>
			<h3>Crowdsourced reviews</h3>
			<p>Knowing that bot-generated reviews were easy to detect by flagging IP addresses and other symptoms <a id="_idIndexMarker285"/>of automated activity, malicious <a id="_idIndexMarker286"/>sellers turned to crowdsourced reviews. With this kind of fake reviews, crowd workers are hired simply to write hundreds of reviews for products. These individuals are often part of online marketplaces or platforms that offer payment in exchange for writing positive reviews about products or services. Crowd workers may not have actually used the product or service being reviewed and may simply be provided with basic information to write a review. Sourced on freelancing and crowd-working websites such as Amazon MTurk or Fiverr, these crowd workers work on a commission basis and earn a fee for every review they post. Notably, these reviews have certain peculiar characteristics as well. As many reviews are written by the same user, they show very high inter-review similarity. They also show high burstiness (that is, a sudden spike in the number of reviews, corresponding to the time when the crowd worker was hired). These signals can be used as features to detect this type <span class="No-Break">of reviews.</span></p>
			<h3>Incentivized reviews</h3>
			<p>These are the <a id="_idIndexMarker287"/>latest trend in online fake reviews. Incentivized reviews are <a id="_idIndexMarker288"/>reviews written by customers who have received some form of compensation or reward in exchange for their reviews. This compensation can come in various forms, such as free products, discounts, gift cards, refunds, or other incentives. Incentivized reviews are often used by companies as a marketing strategy to increase positive reviews and ratings of their products or services. By providing incentives to customers, companies hope to encourage them to write positive reviews, which can then be used to attract more customers and <span class="No-Break">boost sales.</span></p>
			<p>However, incentivized reviews can be controversial because they can create a bias toward positive reviews and may not accurately reflect the true opinions of customers. This is because customers who receive incentives may feel obligated to write a positive review, even if they <a id="_idIndexMarker289"/>did not have a positive experience with <a id="_idIndexMarker290"/>the product or service. As a result, many review platforms and websites have strict policies against incentivized reviews and may remove them from their platforms to ensure the integrity of their <span class="No-Break">review system.</span></p>
			<p>Compared to bot-generated or crowdsourced reviews, incentivized reviews are less easy to detect. These are written by human users and hence do not show the symptoms of automated activity as bots do. As these are real users and not crowd workers, these reviews also show <span class="No-Break">less similarity.</span></p>
			<h1 id="_idParaDest-70">Statistical analysis</h1>
			<p>In this section, we <a id="_idIndexMarker291"/>will try to understand some review data and check whether there are any differences between genuine and fake reviews. We will use the Amazon fake reviews dataset that Amazon has published on Kaggle. It is a set of around 20,000 reviews with associated labels (real or fake) as labeled by domain experts <span class="No-Break">at Amazon.</span></p>
			<h2 id="_idParaDest-71">Exploratory data analysis</h2>
			<p>We will first load <a id="_idIndexMarker292"/>up the data and take a first pass over it to understand <a id="_idIndexMarker293"/>the features and <span class="No-Break">their distribution.</span></p>
			<p>We begin by importing the <span class="No-Break">necessary libraries:</span></p>
			<pre class="source-code">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</pre>
			<p>We will then read the <strong class="source-inline">reviews</strong> data. Although it is a text file, it is structured and therefore can be read with the <strong class="source-inline">read_csv</strong> function <span class="No-Break">in Pandas:</span></p>
			<pre class="source-code">
reviews_df = pd.read_csv("amazon_reviews.txt", sep="\t")
reviews_df.head()</pre>
			<p>This is what the output should <span class="No-Break">look like:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B19327_04_01.jpg" alt="Figure 4.1 – A glimpse of the reviews dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – A glimpse of the reviews dataset</p>
			<p>Notice the <strong class="bold">LABEL</strong> column in the data. Rather than a simple label, it has labels of <strong class="bold">__label1__</strong> and <strong class="bold">__label2__</strong>. Looking at the documentation for this dataset, we can see that <strong class="bold">__label1__</strong> corresponds to real reviews, and <strong class="bold">__label2__</strong> to <span class="No-Break">fake ones.</span></p>
			<p>For easier understanding, we <a id="_idIndexMarker294"/>will transform these labels. We <a id="_idIndexMarker295"/>want <strong class="source-inline">0</strong> to correspond to a real review, and <strong class="source-inline">1</strong> to a fake one. The following code snippet does this <span class="No-Break">for us:</span></p>
			<pre class="source-code">
def label_to_int(label):
    if label == "__label2__":
        # Real Review
        return 0
    else:
        # Fake Review
        return 1
reviews_df["FRAUD_LABEL"] = reviews_df["LABEL"].apply(label_to_int)
reviews_df.head()</pre>
			<p>The output of this code is as follows. You can see that a new column, <strong class="bold">FRAUD_LABEL</strong>, with <strong class="bold">0</strong> and <strong class="bold">1</strong> values has <span class="No-Break">been created.</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B19327_04_02.jpg" alt="Figure 4.2 – Dataset with clear labels added"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Dataset with clear labels added</p>
			<p>First, we want to look at the distribution of real and fake reviews. This will tell us whether the dataset is balanced. If it is an imbalanced dataset, we may have problems in building a classifier. If it is highly imbalanced (such as only 1% of the reviews being fake), we may want to move from classification to an anomaly <span class="No-Break">detection approach.</span></p>
			<p>Note that there is a product category feature in the dataset. We want to examine how reviews are <a id="_idIndexMarker296"/>distributed across categories. We do this because different <a id="_idIndexMarker297"/>kinds of products may have different kinds of reviews. The nature of fake reviews for home apparel might be different from the ones for electronics products. We want to look at the distribution to anticipate any bias or <span class="No-Break">generalization concerns.</span></p>
			<p>To do so, we will group the reviews by category, and count the number of reviews per category. The result in the form of a bar graph will show us <span class="No-Break">the distribution:</span></p>
			<pre class="source-code">
axes = reviews_df.groupby("FRAUD_LABEL").PRODUCT_CATEGORY\
       .value_counts()\
       .unstack(<strong class="bold">0</strong>)\
       .plot.barh()
axes.set_xlabel("# Reviews")
axes.set_ylabel("Product Category")</pre>
			<p>The output should be as follows. We can see that there are ~30 categories of products, and each one has 350 real and 350 fake reviews. This dataset is therefore well balanced and we do not need to worry about bias coming from the review <span class="No-Break">category feature.</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B19327_04_03.jpg" alt="Figure 4.3 – Review distribution across categories"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Review distribution across categories</p>
			<p>Next, we want to check how the real and fake reviews are distributed across ratings. This is an important factor to consider, and failing to do so can result in a biased model. For example, if the <a id="_idIndexMarker298"/>dataset is set up so that the fake reviews are all 4-star <a id="_idIndexMarker299"/>and 5-star and all the real reviews are 1-star and 2-star, our model will learn to detect sentiment instead of review authenticity. The sentiment will introduce bias into our model and this beats the actual purpose of building the <span class="No-Break">model itself.</span></p>
			<p>We will group the reviews by rating and calculate the number of real and fake reviews for every rating. Note that this is feasible because there are only five classes of ratings and they <span class="No-Break">are discrete:</span></p>
			<pre class="source-code">
axes = reviews_df.groupby("FRAUD_LABEL").RATING\
      .value_counts()\
      .unstack(0)\
      .plot.bar()
axes.set_xlabel("# Reviews")
axes.set_ylabel("Rating")</pre>
			<p>The output is shown as follows. We can see that there is some disparity in the number of reviews by rating class (there are only 2,000 1-star reviews whereas there are around 12,000 5-star reviews). However, within each class, the number of real and fake reviews is roughly equal. Therefore, the data is well distributed on the rating front <span class="No-Break">as well.</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B19327_04_04.jpg" alt="Figure 4.4 – Review distribution across ratings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Review distribution across ratings</p>
			<p>Finally, we will review the distribution of real and fake reviews by whether they are verified or not. A review being verified means that the e-commerce platform guarantees that the <a id="_idIndexMarker300"/>product was actually purchased by the reviewer, typically <a id="_idIndexMarker301"/>on the platform itself. We can observe the distribution with the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
axes = reviews_df.groupby("FRAUD_LABEL").VERIFIED_PURCHASE\
      .value_counts()\
      .unstack(0)\
      .plot.bar()
axes.set_xlabel("Purchase Verified")
axes.set_ylabel("Rating")</pre>
			<p>The output is shown in the following <span class="No-Break">bar plot:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B19327_04_05.jpg" alt="Figure 4.5 – Review distribution across the verified label"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Review distribution across the verified label</p>
			<p>Looking at it, we observe two <span class="No-Break">interesting occurrences:</span></p>
			<ul>
				<li>In the verified reviews, the percentage of genuine (real) reviews is higher than the fake ones. There are almost 9,000 real reviews, but only around 3,000 fake ones. This means that only 25% of verified reviews <span class="No-Break">are fake.</span></li>
				<li>In the non-verified reviews, we see that the trend is reversed. There are almost 7,000 fake reviews and approximately 1,800 real reviews. Therefore, almost 80% of non-verified reviews <span class="No-Break">are fake.</span></li>
			</ul>
			<p>This trend is <a id="_idIndexMarker302"/>natural. It is simpler for a bot or review service to trivially <a id="_idIndexMarker303"/>generate non-verified reviews. However, generating a verified review involves actually buying the product, which incurs <span class="No-Break">additional expenses.</span></p>
			<h2 id="_idParaDest-72">Feature extraction</h2>
			<p>In this section, we <a id="_idIndexMarker304"/>will extract some features from the <strong class="source-inline">reviews</strong> data. The goal is to characterize fake reviews by certain signals or trends they may exhibit. Here, we build these features using our intuition, as well as leveraging prior research in <span class="No-Break">the field.</span></p>
			<p>First, we will write a function to extract the length of the review. This function first checks whether the review text is empty. If so, it returns <strong class="source-inline">0</strong>. If not, it returns the number of words in <span class="No-Break">the review:</span></p>
			<pre class="source-code">
def review_length(text):
    if text is None:
        return 0
    else:
        words = text.split(" ")
        return len(words)</pre>
			<p>Next, we will write a function to compute the average word length in the review. To do so, we will split the <a id="_idIndexMarker305"/>review into individual words, add up their lengths, and divide by the total number of words to compute the mean. Of course, we must first check that the review is <span class="No-Break">not empty:</span></p>
			<pre class="source-code">
def average_word_length(text):
    if text is None or text == "":
        return 0
    else:
        words = text.split(" ")
        total_lengths = 0
        for word in words:
            total_lengths = total_lengths + len(word)
        avg_len = total_lengths/len(words)
        return avg_len</pre>
			<p>Another feature we will derive is the number of words spelled incorrectly in the review. We will use the <strong class="source-inline">enchant</strong> Python library for this. The <strong class="source-inline">enchant</strong> Python library is a module that provides an interface to the <strong class="source-inline">enchant</strong> spellchecking system. <strong class="source-inline">enchant</strong> is a C/C++ library that provides a unified interface for several different spellchecking engines, including Aspell, Hunspell, and MySpell. <strong class="source-inline">enchant</strong> can be used from within your Python code to perform spellchecking on text. <strong class="source-inline">enchant</strong> provides a number of useful features for spellchecking, including suggestions for misspelled words, the ability to add and remove words from a user dictionary, and the ability to work with <span class="No-Break">multiple languages.</span></p>
			<p>To use <strong class="source-inline">enchant</strong> in your Python code, you first need to install the <strong class="source-inline">enchant</strong> library on your system. This can be done using the Python <strong class="source-inline">pip</strong> utility. Once installed, you can import the <strong class="source-inline">enchant</strong> module into your Python code and use the provided functions to perform <span class="No-Break">spell checking.</span></p>
			<p>To derive our feature, we <a id="_idIndexMarker306"/>will split the review into words, and count the number of words that are <span class="No-Break">incorrectly spelled:</span></p>
			<pre class="source-code">
import enchant
def count_misspellings(text):
    english_dict = enchant.Dict("en_US")
    if text is None or text == "":
        return 0
    else:
        misspelling = 0
        words = text.split(" ")
        for word in words:
            if word != "" and not english_dict.check(word.lower()):
                misspelling = misspelling + 1
        return misspelling</pre>
			<p>Finally, now that we have defined the functions to extract features, it is time to put them to use. We will efficiently compute the features for each review in our DataFrame using the <strong class="source-inline">apply()</strong> function <span class="No-Break">in Pandas.</span></p>
			<p>The <strong class="source-inline">apply()</strong> function in the <a id="_idIndexMarker307"/>pandas library is used to apply a given function to every element of a pandas DataFrame, Series, or column. It can be used to transform, filter, or aggregate the data in the DataFrame, and is a powerful tool for <span class="No-Break">data manipulation.</span></p>
			<p>The <strong class="source-inline">apply()</strong> function takes a function as an argument, which is applied to every element in the DataFrame. The function <a id="_idIndexMarker308"/>can be defined by the user or can be a built-in Python function. In this case, we will use the custom functions we defined earlier. The <strong class="source-inline">apply()</strong> function also <a id="_idIndexMarker309"/>has some optional arguments that can be used to customize its behavior, such as its axis (to specify whether to apply the function row-wise or column-wise) and arguments (to pass additional arguments to <span class="No-Break">the function).</span></p>
			<p>Here is how we will <span class="No-Break">use it:</span></p>
			<pre class="source-code">
reviews_df["Review_Text_Length"] = reviews_df["REVIEW_TEXT"].apply(review_length)
reviews_df["Avg_Word_Len"] = reviews_df["REVIEW_TEXT"].apply(average_word_length)
reviews_df["Num_Misspelling"] = reviews_df["REVIEW_TEXT"].apply(count_misspellings)</pre>
			<p>This completes our feature-engineering code. You can, of course, add more features if you wish <span class="No-Break">to experiment!</span></p>
			<h2 id="_idParaDest-73">Statistical tests</h2>
			<p>Now that we have our <a id="_idIndexMarker310"/>features, we want to check whether they differ between real and fake reviews. The process of testing for differences is known as <span class="No-Break">hypothesis testing.</span></p>
			<h3>Hypothesis testing</h3>
			<p>Hypothesis testing is a statistical method used to determine whether a claim or hypothesis about a <a id="_idIndexMarker311"/>population parameter is supported by the evidence <a id="_idIndexMarker312"/>provided by a sample of data. In other words, it is a way to test the validity of a hypothesis or claim about a population based on a sample <span class="No-Break">of data.</span></p>
			<p>The hypothesis being tested is typically called <a id="_idIndexMarker313"/>the null hypothesis (H0), and the alternative hypothesis (H1) is the hypothesis that is considered as an alternative to the null hypothesis. The null hypothesis usually represents the status quo or the default assumption, and <a id="_idIndexMarker314"/>the alternative hypothesis represents the hypothesis that the researcher is trying <span class="No-Break">to support.</span></p>
			<p>From the previous section, we have data belonging to two different groups: real reviews and fake reviews. Here is how the process of hypothesis testing will work <span class="No-Break">for us:</span></p>
			<ol>
				<li><strong class="bold">Formulating the null and alternative hypotheses</strong>: The null hypothesis represents the status quo or the default assumption, and the alternative hypothesis represents the hypothesis that the researcher is trying to support. In this case, the null hypothesis would be that there is no difference between the features of the group of fake reviews and the group of real reviews. The alternative hypothesis would be that there is a difference between the features of the <span class="No-Break">two groups.</span></li>
				<li><strong class="bold">Collecting and analyzing data</strong>: A sample of data is collected, and descriptive statistics <a id="_idIndexMarker315"/>and inferential statistics are used <a id="_idIndexMarker316"/>to analyze the data. A sample of reviews is collected, including both fake and real reviews. The feature of interest (such as the length or average word length) is computed for each review in the sample, and descriptive statistics are calculated for each group of reviews, including the mean and standard deviation of <span class="No-Break">the feature.</span></li>
				<li><strong class="bold">Choosing the level of significance</strong>: The level of significance represents the probability that we will reject the null hypothesis even when it is true. The lower the significance level, the lower the chances of falsely rejecting the null hypothesis, which means higher confidence in our estimate. The most commonly used level of significance is 0.05, which means that there is a 5% chance of rejecting the null hypothesis when it is <span class="No-Break">actually true.</span></li>
				<li><strong class="bold">Calculating the test statistic</strong>: A test statistic is calculated based on the sample data, which measures how far the sample statistic is from the null hypothesis. The test statistic depends on the type of test being performed. For example, if we are comparing the means of the two groups, we can use a t-test to calculate the t-value, which measures the difference between the means of the fake reviews and the real reviews relative to the variability of <span class="No-Break">the data.</span></li>
				<li><strong class="bold">Determining the p-value</strong>: The p-value is the probability of obtaining a test statistic as extreme or more extreme than the one observed, assuming that the null hypothesis is true. In this case, the null hypothesis is that there is no difference between the features of the group of fake reviews and the group of real reviews. If the p-value is less than 0.05, we reject the null hypothesis and conclude that there is a significant difference between the features of the two groups. If the p-value is greater than or equal to 0.05, we fail to reject the null hypothesis and conclude that there is not enough evidence to conclude that there is a significant difference. Note that we can simply reject or not reject the null hypothesis - we can never conclude that we accept the <span class="No-Break">alternate hypothesis.</span></li>
				<li><strong class="bold">Making a decision</strong>: Based on the p-value and the level of significance, a decision is made on whether to reject the null hypothesis or not. If the p-value is less than 0.05, we <a id="_idIndexMarker317"/>reject the null hypothesis and <a id="_idIndexMarker318"/>conclude that there is a significant difference between the features of the group of fake reviews and the group of real reviews. If the p-value is greater than or equal to 0.05, we fail to reject the null hypothesis and conclude that there is not enough evidence to conclude that there is a <span class="No-Break">significant difference.</span></li>
				<li><strong class="bold">Drawing conclusions</strong>: The final step is to draw conclusions based on the results of the hypothesis test and to determine whether the evidence supports the alternative hypothesis or not. In this case, if we reject the null hypothesis, we conclude that there is a significant difference between the features of the group of fake reviews and the group of real reviews. If we fail to reject the null hypothesis, we conclude that there is not enough evidence to conclude that there is a significant difference between the features of the <span class="No-Break">two groups.</span></li>
			</ol>
			<p>Overall, hypothesis testing is a powerful statistical method used to test claims about populations using sample data. By following these given steps, we can make informed decisions based on data and draw valid conclusions about whether there is a significant difference between the features of the group of fake reviews and the group of <span class="No-Break">real reviews.</span></p>
			<p>Note that here we have used real and fake reviews as an example. In theory, this experiment can be repeated for any two groups and any features. In the chapters to come, we will see several examples of varied datasets (containing images, text, video, and malware). We will also collect different kinds of features (image feature vectors, linguistic features, and API call sequences). While we will not conduct hypothesis testing every time, you are <a id="_idIndexMarker319"/>highly encouraged to do so in order to strengthen <a id="_idIndexMarker320"/>your understanding of <span class="No-Break">the concepts.</span></p>
			<p>Now that we have defined clearly the steps involved in hypothesis testing, let us look at the most crucial part of it: the actual tests <span class="No-Break">being conducted.</span></p>
			<h3>T-tests</h3>
			<p>T-tests are a statistical <a id="_idIndexMarker321"/>method used to compare the means of two groups <a id="_idIndexMarker322"/>and determine whether there is a statistically significant difference between them. The t-test is a hypothesis test that is based on the t-distribution, which is similar to the normal distribution but with a <span class="No-Break">heavier tail.</span></p>
			<p>There are two main types <span class="No-Break">of t-tests:</span></p>
			<ul>
				<li><strong class="bold">Independent samples t-test</strong>: This test is used when we want to compare the means of two <a id="_idIndexMarker323"/>independent groups, such as a treatment <a id="_idIndexMarker324"/>group and a control group, or a group of men and a group of women. The null hypothesis is that there is no difference in the means of the <span class="No-Break">two groups.</span></li>
				<li><strong class="bold">Paired samples t-test</strong>: This test is used when we want to compare the means of two related <a id="_idIndexMarker325"/>groups, such as before-and-after measurements for <a id="_idIndexMarker326"/>the same individuals, or measurements taken from two matched groups. The null hypothesis is that there is no difference between the means of the two <span class="No-Break">related groups.</span></li>
			</ul>
			<p>The t-test calculates a t-value, which is a measure of how different the means of the two groups are, relative to the <a id="_idIndexMarker327"/>variability within each group. The t-value is calculated by dividing the difference between the means of the two groups by the standard error of the difference. The standard error of the difference takes into account both the sample sizes and the variances of the <span class="No-Break">two groups.</span></p>
			<p>T-tests can be one-sided <span class="No-Break">or two-sided:</span></p>
			<ul>
				<li><strong class="bold">One-sided t-test</strong>: A one-sided t-test, also <a id="_idIndexMarker328"/>known as a <a id="_idIndexMarker329"/>directional t-test, is used when we have a specific hypothesis about the direction of the difference between the means of the two groups. For example, we might hypothesize that the mean of one group is greater than the mean of the <a id="_idIndexMarker330"/>other group. In this case, we would use a one-sided t-test to test <a id="_idIndexMarker331"/>this hypothesis. The null hypothesis for a one-sided t-test is that there is no difference between the means of the two groups in the <span class="No-Break">hypothesized direction.</span></li>
				<li><strong class="bold">Two-sided t-test</strong>: A two-sided t-test, also known as a non-directional t-test, is used when <a id="_idIndexMarker332"/>we do not have a specific hypothesis about the <a id="_idIndexMarker333"/>direction of the difference between the means of the two groups. For example, we might simply want to test whether the means of the two groups are different. In this case, we would use a two-sided t-test. The null hypothesis for a two-sided t-test is that there is no difference between the means of the <span class="No-Break">two groups.</span></li>
			</ul>
			<p>Once the t-value is calculated, we can determine the p-value, which is the probability of obtaining a t-value as <a id="_idIndexMarker334"/>extreme or more extreme than the one observed, assuming <a id="_idIndexMarker335"/>that the null hypothesis is true. If the p-value is less than the level of significance (usually 0.05), we reject the null hypothesis and conclude that there is a statistically significant difference between the means of the <span class="No-Break">two groups.</span></p>
			<p>T-tests are commonly used in many fields, such as psychology, biology, economics, and engineering, to compare the means of two groups and make statistical inferences. Note that in t-tests, we always decide on whether the null hypothesis is to be rejected based on the p-value. If the p-value is higher than our chosen threshold, we never say that we <em class="italic">accept</em> the alternate hypothesis; we simply say that we <em class="italic">failed to reject</em> the <span class="No-Break">null hypothesis.</span></p>
			<h4>Conducting t-tests</h4>
			<p>We will now see how <a id="_idIndexMarker336"/>the t-test is actually conducted in Python and what information it gives us. Let us use the t-test to compare the distribution of the <em class="italic">review text length</em> feature in the fake and real reviews group. Therefore, our hypotheses are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The null hypothesis (H0) is that <a id="_idIndexMarker337"/>the mean review length is the same for real and <span class="No-Break">fake reviews</span></li>
				<li>The alternate hypothesis (H1) is that the <a id="_idIndexMarker338"/>mean review length between the two groups <span class="No-Break">is different</span></li>
			</ul>
			<p>Note that this is an unpaired t-test as samples in both groups are independent. This is also a two-tailed t-test as our hypothesis is not directional. Let us first plot the distribution of the <a id="_idIndexMarker339"/>review length for the two groups. We will obtain an array representing the review lengths within the two groups separately, and then plot their histograms on the <span class="No-Break">same grid:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
# Separate real and fake reviews
fake_reviews = reviews_df[reviews_df['FRAUD_LABEL'] == 1]['Review_Text_Length'].values
real_reviews = reviews_df[reviews_df['FRAUD_LABEL'] == 0]['Review_Text_Length'].values
# Plot the two histograms
bins = np.linspace(0, 500, 500)
plt.hist(fake_reviews, bins, alpha=0.5, label='Fake')
plt.hist(real_reviews, bins, alpha=0.5, label='Real')
# Label the plot
plt.xlabel("Review Length")
plt.ylabel("# Reviews")
plt.legend()
# Display the plot
plt.show()</pre>
			<p>Here is <span class="No-Break">the output:</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B19327_04_06.jpg" alt="Figure 4.6 – Review length distribution across real and fake reviews"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Review length distribution across real and fake reviews</p>
			<p>So we can see that there are some distributional differences between the two groups. The real reviews have a sharp peak at the lower values of the feature while the fake reviews have several intermediate peaks. Now we will do a t-test to see whether the difference is statistically significant. We will use the <strong class="source-inline">scipy</strong> module in Python to <span class="No-Break">do this.</span></p>
			<p><strong class="source-inline">scipy</strong> is a powerful and widely used scientific computing library for the Python programming <a id="_idIndexMarker340"/>language. It provides a broad range of functionality for scientific <a id="_idIndexMarker341"/>and technical computing, including optimization, integration, interpolation, linear algebra, signal and image processing, and more. It is built on top of the <strong class="source-inline">NumPy</strong> library, which provides support for large, multi-dimensional arrays and matrices, and extends its capabilities to higher-level <span class="No-Break">mathematical functions.</span></p>
			<p>One of the key strengths of <strong class="source-inline">scipy</strong> is its sub-modules, which provide specialized functionality for different areas of <span class="No-Break">scientific computing:</span></p>
			<ul>
				<li>The <strong class="source-inline">optimization</strong> sub-module <a id="_idIndexMarker342"/>provides functions for finding the minimum or maximum of a <a id="_idIndexMarker343"/>function, root-finding, curve-fitting, <span class="No-Break">and more</span></li>
				<li>The <strong class="source-inline">integration</strong> sub-module <a id="_idIndexMarker344"/>provides functions for numerical integration and solving <span class="No-Break">differential equations</span></li>
				<li>The <strong class="source-inline">interpolate</strong> sub-module <a id="_idIndexMarker345"/>provides functions for interpolating and <span class="No-Break">smoothing data</span></li>
				<li>The <strong class="source-inline">linalg</strong> sub-module provides <a id="_idIndexMarker346"/>functions for linear algebra, including matrix decompositions, solving linear systems of equations, <span class="No-Break">and more</span></li>
				<li>The <strong class="source-inline">signal</strong> sub-module provides <a id="_idIndexMarker347"/>functions for signal processing, including filtering, Fourier transforms, <span class="No-Break">and more</span></li>
				<li>The <strong class="source-inline">sparse</strong> sub-module <a id="_idIndexMarker348"/>provides sparse matrix implementations <a id="_idIndexMarker349"/>and <span class="No-Break">related operations</span></li>
			</ul>
			<p>The following code will conduct the t-test and print out some statistics <span class="No-Break">from it:</span></p>
			<pre class="source-code">
from scipy.stats import ttest_ind
# Conduct t-test
t_stat, p_value = ttest_ind(fake_reviews, real_reviews)
# Print group means
print("Mean in Fake Reviews: ", np.mean(fake_reviews))
print("Mean in Real Reviews: ", np.mean(real_reviews))
# Print t-test statistics
print("T-statistic value: ", t_stat)
print("P-Value: ", p_value)</pre>
			<p>After running this, you will see the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
Mean in Fake Reviews:  59.62095238095238
Mean in Real Reviews:  80.63780952380952
T-statistic value:  -17.56360707600074
P-Value:  1.4478425823590511e-68</pre>
			<p>This tells us a couple <span class="No-Break">of things:</span></p>
			<ul>
				<li>The mean review length in the fake reviews is 59.62 while in the real reviews, it is 80.63. Therefore, the average fake review is nearly 21 words shorter than the average <span class="No-Break">real review.</span></li>
				<li>Our p-value comes out to be 1.447e-68, which is of the order of 10<span class="superscript">-68</span> and is therefore very small. As this value is much smaller than 0.05, we can conclude that the difference we saw previously was <span class="No-Break">statistically significant.</span></li>
				<li>As p &lt; 0.05, this implies that the null hypothesis can be rejected. Therefore, we can conclude that the mean review length differs between real and fake reviews and this difference is <span class="No-Break">statistically significant.</span></li>
			</ul>
			<p>We can repeat <a id="_idIndexMarker350"/>this exercise for any feature of our choice. For example, here is how we do it for the number of words misspelled. The code is more or less the same, but note the change in the bins. The bins depend on the range of the feature under consideration. To plot the data, we use the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
# Separate real and fake reviews
fake_reviews = reviews_df[reviews_df['FRAUD_LABEL'] == 1]['Num_Misspelling'].values
real_reviews = reviews_df[reviews_df['FRAUD_LABEL'] == 0]['Num_Misspelling'].values
# Plot the two histograms
bins = np.linspace(0, 50, 50)
plt.hist(fake_reviews, bins, alpha=0.5, label='Fake')
plt.hist(real_reviews, bins, alpha=0.5, label='Real')
# Label the plot
plt.xlabel("Review Length")
plt.ylabel("# Misspelt Words")
plt.legend()
# Display the plot
plt.show()</pre>
			<p>This shows us the distribution <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B19327_04_07.jpg" alt="Figure 4.7 – Distribution of misspelled words across real and fake reviews"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Distribution of misspelled words across real and fake reviews</p>
			<p>The distribution <a id="_idIndexMarker351"/>looks pretty similar. Let’s do <span class="No-Break">the t-test:</span></p>
			<pre class="source-code">
from scipy.stats import ttest_ind
# Conduct t-test
t_stat, p_value = ttest_ind(fake_reviews, real_reviews)
# Print group means
print("Mean in Fake Reviews: ", np.mean(fake_reviews))
print("Mean in Real Reviews: ", np.mean(real_reviews))
# Print t-test statistics
print("T-statistic value: ", t_stat)
print("P-Value: ", p_value)</pre>
			<p>Here’s what <span class="No-Break">we see:</span></p>
			<pre class="source-code">
Mean in Fake Reviews:  4.716952380952381
Mean in Real Reviews:  7.844952380952381
T-statistic value:  -18.8858003626682
P-Value:  6.730184744054038e-79</pre>
			<p>So this shows us that <a id="_idIndexMarker352"/>there are statistically significant differences between these two features <span class="No-Break">as well.</span></p>
			<p>Hypothesis testing is a useful tool that helps us examine signals that differentiate between groups, and the statistical significance of those signals. It helps data scientists and statisticians make claims about the data and give them <span class="No-Break">mathematical backing.</span></p>
			<h3>A note on ANOVA tests</h3>
			<p>In the previous <a id="_idIndexMarker353"/>section, we learned how t-tests can help compare <a id="_idIndexMarker354"/>the means of two dependent or independent groups. This is helpful when you have a clean binary class problem. However, there often may be multiple classes. For example, there could be real reviews, bot reviews, incentivized reviews, and crowdsourced reviews all labeled separately. In such cases, we compare the means using an <strong class="bold">Analysis of Variance (</strong><span class="No-Break"><strong class="bold">ANOVA)</strong></span><span class="No-Break"> test.</span></p>
			<p><strong class="bold">ANOVA</strong> is a statistical method used to test the hypothesis that there is <a id="_idIndexMarker355"/>no significant difference between the means of two or more groups. ANOVA compares the variation within groups to the variation between groups to determine whether there is a significant difference in means. There are several types of ANOVA, but the most common is one-way ANOVA. One-way ANOVA is used to compare the means of two or more independent groups. For example, you might use one-way ANOVA to compare the average test scores of students in three <span class="No-Break">different classes.</span></p>
			<p>The basic idea behind ANOVA is to partition the total variation in the data into two sources: variation due to differences between groups and variation due to differences within groups. The ratio of these two sources of variation is then used to determine whether the means of the groups are significantly different. The results of ANOVA are typically reported <a id="_idIndexMarker356"/>as an F-statistic, which is a measure of the ratio of the between-group <a id="_idIndexMarker357"/>variation to the within-group variation. If the F-statistic is large enough (that is, if the between-group variation is much larger than the within-group variation), then the null hypothesis of no difference between the groups is rejected, and it can be concluded that there is a significant difference between the means of <span class="No-Break">the groups.</span></p>
			<p>As we only have two classes here, we will not be implementing ANOVA or seeing it in practice. However, the <strong class="source-inline">scipy</strong> library contains implementations for ANOVA just like it does for t-tests, and I encourage you to go <span class="No-Break">through it.</span></p>
			<p>Now that we have seen how to examine feature differences, let us turn toward modeling our data with the most fundamental machine learning algorithm – <span class="No-Break">linear regression.</span></p>
			<h1 id="_idParaDest-74">Modeling fake reviews with regression</h1>
			<p>In this section, we will <a id="_idIndexMarker358"/>use the features we <a id="_idIndexMarker359"/>examined to attempt to model our data with <span class="No-Break">linear regression.</span></p>
			<h2 id="_idParaDest-75">Ordinary Least Squares regression</h2>
			<p><strong class="bold">Ordinary Least Squares</strong> (<strong class="bold">OLS</strong>) linear regression is a statistical method used to model the <a id="_idIndexMarker360"/>relationship between a dependent variable and one or more independent variables. The goal of OLS is to find the linear function that best fits the data by minimizing the sum of squared errors between the observed values and the predicted values of the <span class="No-Break">dependent variable.</span></p>
			<p>The linear function is typically <span class="No-Break">expressed as:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">Y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">+</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">ε</span></span></p>
			<p>where <em class="italic">Y</em> is the dependent variable, X<span class="subscript">1</span>, X<span class="subscript">2</span>, ..., X<span class="subscript">n</span> are the independent variables, β<span class="subscript">0</span>, β<span class="subscript">1</span>, β<span class="subscript">2</span>, ..., β<span class="subscript">n</span> are the coefficients (or parameters) that measure the effect of each independent variable on the dependent variable, and ε is the error term (or residual) that captures the part of the dependent variable that is not explained by the <span class="No-Break">independent variables.</span></p>
			<p>The OLS method <a id="_idIndexMarker361"/>estimates the coefficients by finding the values that minimize the sum of <span class="No-Break">squared errors:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">Σ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">ŷ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Symbol">²</span></span></p>
			<p>where y<span class="subscript">i</span> is the observed value of the dependent variable, ŷ<span class="subscript">i</span> is the predicted value of the dependent variable based on the linear function, and the sum is taken over all the observations in <span class="No-Break">the sample.</span></p>
			<p>Once the values of the coefficients have been estimated, they can be plugged back into the equation to compute the predicted output value based on the defined equation. In this section, we will explore how linear regression can be used to detect whether a given review is fake <span class="No-Break">or not.</span></p>
			<h2 id="_idParaDest-76">OLS assumptions</h2>
			<p>OLS regression makes several <a id="_idIndexMarker362"/>assumptions about the data, and violating these assumptions can lead to biased or inefficient estimates of the regression coefficients. Here are the main assumptions of <span class="No-Break">OLS regression:</span></p>
			<ul>
				<li><strong class="bold">Linearity</strong>: The <a id="_idIndexMarker363"/>relationship between the dependent variable and the independent variables is linear. This means that the effect of each independent variable on the dependent variable is constant across the range of values of the independent variable. In the context of fake review detection, this assumption would mean that the relationship between the textual features (such as sentiment scores or word frequencies) and the likelihood of a review being fake <span class="No-Break">is linear.</span></li>
				<li><strong class="bold">Independence</strong>: The <a id="_idIndexMarker364"/>observations are independent of each other, meaning that the value of one observation does not depend on the value of another observation. For example, in fake review <a id="_idIndexMarker365"/>detection, this assumption would mean that the reviews are not systematically related to each other, such as being written by the same person or being about the <span class="No-Break">same product.</span></li>
				<li><strong class="bold">Homoscedasticity</strong>: The variance of the errors (residuals) is constant across all values of the <a id="_idIndexMarker366"/>independent variables. This means that the spread of the residuals does not change as the values of the independent variables change. In the context of fake review detection, this assumption would mean that the variability in the likelihood of a review being fake is the same for all levels of the <span class="No-Break">textual features.</span></li>
				<li><strong class="bold">Normality</strong>: The errors are normally distributed with a mean of zero. This means that the distribution <a id="_idIndexMarker367"/>of the residuals is symmetrical around 0 and follows a bell-shaped curve. In the context of fake review detection, this assumption would mean that the errors in the predicted probabilities of a review being fake are <span class="No-Break">normally distributed.</span></li>
				<li><strong class="bold">No multicollinearity</strong>: There is no perfect correlation between any pair of independent <a id="_idIndexMarker368"/>variables. This means that the independent variables are not redundant or highly correlated with each other. In the context of fake review detection, this assumption would mean that the textual features are not too similar to each other, as this could lead to multicollinearity and make it difficult to identify which features are driving the probability of a review <span class="No-Break">being fake.</span></li>
			</ul>
			<p>To ensure that these assumptions are met in practice, it is important to carefully preprocess the data <a id="_idIndexMarker369"/>and to check for violations of these assumptions during and after the modeling process. For example, in the context of fake review detection, we might check for independence by removing reviews written by the same person or about the same product, and we might check for normality by plotting the residuals and checking for a <span class="No-Break">normal distribution.</span></p>
			<h2 id="_idParaDest-77">Interpreting OLS regression</h2>
			<p>In this section, we will look at the <a id="_idIndexMarker370"/>metrics we use to evaluate the OLS regression model, namely the <strong class="bold">R-squared</strong> (<strong class="bold">R²</strong>), F-statistic, and <span class="No-Break">regression coefficients.</span></p>
			<h3>R<span class="superscript">2</span></h3>
			<p>In OLS linear regression, R² is a statistical measure that represents the proportion of variance in the dependent <a id="_idIndexMarker371"/>variable that can be explained by the independent <a id="_idIndexMarker372"/>variables in the model. It provides a measure of how well the regression line fits the data. R²<span class="superscript"> </span> takes values between 0 and 1, with 0 indicating that none of the variation in the dependent variable is explained by the independent variables, and 1 indicating that all of the variation in the dependent variable is explained by the <span class="No-Break">independent variables.</span></p>
			<p>Adjusted R² is a modified version of the R² value in OLS linear regression that takes into account the number of independent variables in the model. It is calculated using the <span class="No-Break">following formula:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">__________</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span></p>
			<p>Here, <em class="italic">n</em> is the number of observations in the sample and <em class="italic">k</em> is the number of independent variables in the model. The adjusted R² value provides a more conservative estimate of the goodness of fit of the model than the R² value. Unlike the R² value, which increases as the number of independent variables in the model increases, the adjusted R² value penalizes models that have additional independent variables that do not significantly improve the fit of <span class="No-Break">the model.</span></p>
			<h3>The F-statistic</h3>
			<p>The F-statistic is a statistical test used in OLS regression to determine whether the overall regression <a id="_idIndexMarker373"/>model is statistically significant or not. It is calculated by <a id="_idIndexMarker374"/>comparing the variance explained by the regression model <a id="_idIndexMarker375"/>to the variance not explained by the <a id="_idIndexMarker376"/>model. In particular, the F-statistic measures the ratio of the <strong class="bold">mean square of the regression</strong> (<strong class="bold">MSR</strong>) to the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). The MSR represents the variation in the dependent variable that is explained by the independent variables in the model, while the MSE represents the unexplained variation in the dependent variable that is not accounted for by <span class="No-Break">the model.</span></p>
			<p>A high F-statistic with a low associated p-value suggests that the regression model as a whole is statistically significant and that at least one of the independent variables in the model is related to the dependent variable. In contrast, a low F-statistic with a high associated p-value suggests that the model is not statistically significant and that the independent variables do not significantly explain the variation in the <span class="No-Break">dependent variable.</span></p>
			<h3>Regression coefficients</h3>
			<p>Regression coefficients represent the change in the dependent variable for a one-unit change in the <a id="_idIndexMarker377"/>independent variable, holding all other <a id="_idIndexMarker378"/>independent variables constant. More specifically, in linear regression, the coefficients indicate the slope of the regression line, which represents the relationship between the independent variable and the dependent variable. A positive coefficient means that as the independent variable increases, the dependent variable also increases. A negative coefficient means that as the independent variable increases, the dependent variable decreases. The size of the coefficient indicates the magnitude of the effect of the independent variable on the dependent variable. Larger coefficients indicate a stronger relationship between the independent variable and the dependent variable, while smaller coefficients indicate a <span class="No-Break">weaker relationship.</span></p>
			<p>It is important to note that the interpretation of coefficients may be affected by the scaling of the variables. For example, if one independent variable is measured in dollars and another independent variable is measured in percentages, the coefficients cannot be directly compared, as they are not on the same scale. Additionally, coefficients may also be affected by collinearity among the independent variables, which can make it difficult to distinguish <a id="_idIndexMarker379"/>the unique effect of each independent variable <a id="_idIndexMarker380"/>on the dependent variable. Therefore, when interpreting regression coefficients, it is important to consider the scale of the variables, the context of the study, and potential <span class="No-Break">confounding factors.</span></p>
			<h2 id="_idParaDest-78">Implementing OLS regression</h2>
			<p>We will attempt to <a id="_idIndexMarker381"/>model the fake reviews problem with an OLS regression model using the <strong class="source-inline">statsmodels</strong> package, which is a comprehensive Python package that provides a wide range of statistical tools and models for data analysis. It is designed to be used in conjunction with other scientific computing libraries such as <strong class="source-inline">NumPy</strong>, <strong class="source-inline">sciPy</strong>, and <strong class="source-inline">pandas</strong>. One of the key features of <strong class="source-inline">statsmodels</strong> is its ability to estimate statistical models for different types of data. For example, it provides a range of models for linear regression, generalized linear models, time-series analysis, and multilevel models. These models can be used for a variety of tasks such as prediction, classification, <span class="No-Break">and inference.</span></p>
			<p>In addition to its model estimation capabilities, <strong class="source-inline">statsmodels</strong> also includes a range of statistical tests and diagnostics. These tools can be used to assess the quality of a model and determine whether its assumptions are being violated. Some of the statistical tests provided by <strong class="source-inline">statsmodels</strong> include hypothesis tests, goodness-of-fit tests, and tests for stationarity and cointegration. <strong class="source-inline">statsmodels</strong> also includes a range of visualization tools for exploring data and model results. These tools can help users gain insights into their data and communicate their findings effectively. Some of the visualization tools provided by <strong class="source-inline">statsmodels</strong> include scatter plots, line plots, histograms, and <span class="No-Break">QQ plots.</span></p>
			<p>In order to implement OLS regression, you specify a formula that indicates the dependent variable and the independent variables we want to model it on. Here is how it can be done in the <span class="No-Break"><strong class="source-inline">statsmodels</strong></span><span class="No-Break"> library:</span></p>
			<pre class="source-code">
import statsmodels.formula.api as smf
model = smf.ols(formula = 'FRAUD_LABEL ~ Review_Text_Length + Num_Misspelling + Avg_Word_Len',
                data = reviews_df).fit()
print(model.summary())</pre>
			<p>The output is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B19327_04_08.jpg" alt="Figure 4.8 – OLS regression results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – OLS regression results</p>
			<p>Let us interpret <span class="No-Break">these results:</span></p>
			<ul>
				<li>The R-squared <a id="_idIndexMarker382"/>value of 0.017 indicates that only 1.7% of the variance in the dependent variable (<strong class="bold">FRAUD_LABEL</strong>) is explained by the independent variables (<strong class="bold">Review_Text_Length</strong>, <strong class="bold">Num_Misspelling</strong>, and <strong class="bold">Avg_Word_Len</strong>). This means that the model is not very effective in predicting fraud based on the provided independent variables. In general, a higher R-squared value <span class="No-Break">is better.</span></li>
				<li>The coefficients for the independent variables give an indication of how each variable affects the dependent variable. The coefficient for <strong class="bold">Review_Text_Length</strong> is negative, but not statistically significant (P&gt;|t| = 0.081). This suggests that there may be a weak negative relationship between the length of the review text and the likelihood of fraud, but this relationship is not strong enough to be <span class="No-Break">statistically significant.</span></li>
				<li>The coefficient for <strong class="bold">Num_Misspelling</strong> is negative and statistically significant (P&gt;|t| = 0.000). This suggests that there is a strong negative relationship between the number of misspellings in a review and the likelihood of fraud. Specifically, for each additional misspelling, the likelihood of fraud decreases <span class="No-Break">by 0.0044.</span></li>
				<li>The coefficient for <strong class="bold">Avg_Word_Len</strong> is positive and statistically significant (P&gt;|t| = 0.027). This suggests that there is a weak positive relationship between the average word length in a review and the likelihood of fraud. Specifically, for each additional unit of average word length, the likelihood of fraud increases <span class="No-Break">by 0.0023.</span></li>
				<li>The Omnibus test shows that the residuals are not normally distributed (Prob(Omnibus) &lt; 0.05), which suggests that the normality assumption of the model may not hold. The Durbin-Watson value of 0.033 indicates that there may be autocorrelation in the residuals, while the Jarque-Bera test suggests that there may be some small deviation from normality in the residuals. These results should be investigated further to determine whether they are problematic for <span class="No-Break">the analysis.</span></li>
			</ul>
			<p>Overall, the regression model suggests that the number of misspellings in a review is the most important <a id="_idIndexMarker383"/>predictor of fraud, while the length of the review text and the average word length are weaker predictors. However, the R² value is low, indicating that the model is not a strong predictor of fraud <span class="No-Break">in general.</span></p>
			<p>In our previous example, we used only continuous features that we had derived. However, it is possible to use categorical features as well. The formula that we pass to the OLS regression function can be amended <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
import statsmodels.formula.api as smf
model = smf.ols(formula = """FRAUD_LABEL ~ Review_Text_Length + Num_Misspelling + Avg_Word_Len + C(RATING) + C(VERIFIED_PURCHASE)""", data = reviews_df).fit()
print(model.summary())</pre>
			<p>And you see the <span class="No-Break">following result:</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B19327_04_09.jpg" alt="Figure 4.9 – OLS regression with categorical features"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – OLS regression with categorical features</p>
			<p>From this output, we can see that the R² is now 0.359, which indicates that the independent variables <a id="_idIndexMarker384"/>explain about 36% of the variation in the dependent variable. This is an improvement over our previous model, where the independent variables could explain less than 2% of the variance in the <span class="No-Break">dependent variables.</span></p>
			<p>While the interpretation for the other variables (review text length, average word length, and number of misspellings) remains the same, we have also added categorical variables to our analysis. In this OLS regression model, the coefficients for <strong class="bold">RATING</strong> and <strong class="bold">VERIFIED_PURCHASE</strong> correspond to the estimated difference in the mean value of the dependent variable, <strong class="bold">FRAUD_LABEL</strong>, for the corresponding category and the <span class="No-Break">reference category.</span></p>
			<p>For <strong class="bold">RATING</strong>, the reference category is assumed to be the lowest rating (1). The estimated coefficients for <strong class="bold">RATING</strong> suggest that, compared to the lowest rating, the mean value of <strong class="bold">FRAUD_LABEL</strong> is <span class="No-Break">as follows:</span></p>
			<ul>
				<li>0.0086 units lower for a rating of 2 (not significant, p &gt; <span class="No-Break">0.05)</span></li>
				<li>0.0316 units lower for a rating of 3 (significant, p &lt; <span class="No-Break">0.05)</span></li>
				<li>0.0086 units lower for a rating of 4 (not significant, p &gt; <span class="No-Break">0.05)</span></li>
				<li>0.0520 units higher for a rating of 5 (significant, p &lt; <span class="No-Break">0.05)</span></li>
			</ul>
			<p>For <strong class="bold">VERIFIED_PURCHASE</strong>, the reference category is assumed to be a non-verified purchase (N). The estimated coefficient for <strong class="bold">VERIFIED_PURCHASE</strong> indicates that the mean value of <strong class="bold">FRAUD_LABEL</strong> is 0.5951 units lower for verified purchases (significant, p &lt; 0.05) compared <a id="_idIndexMarker385"/>to non-verified purchases. This suggests that verified purchases are associated with a lower probability <span class="No-Break">of fraud.</span></p>
			<p>It’s worth noting that the coefficients for categorical variables in this model are estimated relative to the reference category. Therefore, if you were to change the reference category for <strong class="bold">RATING</strong> or <strong class="bold">VERIFIED_PURCHASE</strong>, the estimated coefficients would change, but the overall model fit and significance levels would remain <span class="No-Break">the same.</span></p>
			<h1 id="_idParaDest-79">Summary</h1>
			<p>In this chapter, we examined the problem of fake reviews on e-commerce platforms through the lens of statistical and machine learning models. We began by understanding the review ecosystem and the nature of fake reviews, including their evolution over time. We then explored a dataset of fake reviews and conducted statistical tests to determine whether they show characteristics significantly different from genuine reviews. Finally, we modeled the review integrity using OLS regression and examined how various factors affect the likelihood that a review <span class="No-Break">is fake.</span></p>
			<p>This chapter introduced you to the foundations of data science, including exploratory data analysis, statistics, and the beginnings of <span class="No-Break">machine learning.</span></p>
			<p>In the next chapter, we will discuss techniques for detecting deepfakes, which plague the internet and social <span class="No-Break">media today.</span></p>
		</div>
	</body></html>