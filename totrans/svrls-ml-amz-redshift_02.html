<html><head></head><body>
		<div id="_idContainer065">
			<h1 id="_idParaDest-27" class="chapter-number"><a id="_idTextAnchor027"/>2</h1>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor028"/>Data Loading and Analytics on Redshift Serverless</h1>
			<p>In the previous chapter, we<a id="_idIndexMarker053"/> introduced you to <strong class="bold">Amazon Redshift Serverless</strong> and demonstrated how to create a serverless endpoint from the Amazon Redshift console. We also explained how to connect and query your data warehouse using <strong class="bold">Amazon Redshift query editor v</strong> In this chapter, we will dive deeper into the<a id="_idIndexMarker054"/> different ways you can load data into your Amazon Redshift Serverless <span class="No-Break">data warehouse.</span></p>
			<p>We will cover three main topics in this chapter to help you load your data efficiently into Redshift Serverless. First, we will demonstrate how to load data using Amazon Redshift query editor v where you will learn how to load data from your Amazon S3 bucket and local data file onto your computer using <span class="No-Break">the GUI.</span></p>
			<p>Next, we will explore the <strong class="source-inline">COPY</strong> command<a id="_idIndexMarker055"/> in detail, and you will learn how to load a file by writing a <strong class="source-inline">COPY</strong> command to load the data. We will cover everything you need to know to use this command effectively and load your data smoothly into <span class="No-Break">Redshift Serverless.</span></p>
			<p>Finally, we will cover the built-in native API interface to access and load data into your Redshift Serverless endpoint using Jupyter Notebook. We will guide you through the process of setting up and using<a id="_idIndexMarker056"/> the <strong class="bold">Redshift </strong><span class="No-Break"><strong class="bold">Data API</strong></span><span class="No-Break">.</span></p>
			<p>The topics are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Data loading using Amazon Redshift query <span class="No-Break">editor v</span></li>
				<li>Data loading from Amazon S3 using the <span class="No-Break">COPY command</span></li>
				<li>Data loading using the Redshift <span class="No-Break">Data API</span></li>
			</ul>
			<p>The goal of this chapter is to equip you with the knowledge and skills to load data into Amazon Redshift Serverless using different mechanisms. By the end of this chapter, you will be able to load data quickly and efficiently into Redshift Serverless using the methods covered in this chapter, which will enable you to perform analytics on your data and extract <span class="No-Break">valuable insights.</span></p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor029"/>Technical requirements</h1>
			<p>This chapter requires a web browser and access to <span class="No-Break">the following:</span></p>
			<ul>
				<li>An <span class="No-Break">AWS account</span></li>
				<li><span class="No-Break">Amazon Redshift</span></li>
				<li>Amazon Redshift Query <span class="No-Break">Editor v2</span></li>
				<li>Amazon SageMaker for <span class="No-Break">Jupyter Notebook</span></li>
			</ul>
			<p>The code snippets in this chapter are available in this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/CodeFiles/chapter2</span></a><span class="No-Break">.</span></p>
			<p>The data files used in this chapter can be found in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor030"/>Data loading using Amazon Redshift Query Editor v2</h1>
			<p>Query Editor v2 <a id="_idIndexMarker057"/>supports different database actions, including <strong class="bold">data definition language</strong> (<strong class="bold">DDL</strong>), to <a id="_idIndexMarker058"/>create schema and tables and load data from data files with just a click of a button. Let’s take a look at how you can carry out these tasks to enable easy analytics on your data warehouse. Log in to your AWS console, navigate to your Amazon Redshift Serverless endpoint, and select <strong class="bold">Query data</strong>. This will open <strong class="bold">Redshift query editor v2</strong> in a new tab. Using the steps we followed in <a href="B19071_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, log in to your database and perform the tasks<a id="_idIndexMarker059"/> outlined in the <span class="No-Break">following subsections.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor031"/>Creating tables</h2>
			<p>Query editor v2 provides a wizard<a id="_idIndexMarker060"/> to execute the DDL commands shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>. Let’s create a new schema named <span class="No-Break"><strong class="source-inline">chapter2</strong></span><span class="No-Break"> first:</span></p>
			<ol>
				<li>Click on <strong class="bold">Create</strong> and select <strong class="bold">Schema</strong>, as <span class="No-Break">shown here.</span></li>
			</ol>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B19071_02_001.jpg" alt="Figure 2.1 – The creation wizard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The creation wizard</p>
			<p>Ensure that your <strong class="bold">Cluster or workgroup</strong> and <strong class="bold">Database</strong> parameters are correctly populated. If not, then select the correct values from the dropdowns. Give a suitable name for your schema; we will name <span class="No-Break">it </span><span class="No-Break"><strong class="source-inline">chapter2</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">Then, click on <strong class="bold">Create schema</strong>, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B19071_02_002.jpg" alt="Figure 2.2 – Create schema"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Create schema</p>
			<p>Once you have your <a id="_idIndexMarker061"/>schema created, navigate to the <strong class="bold">Create</strong> drop-down button and click on <strong class="bold">Table</strong>. This will open up the <strong class="bold">Create table</strong> wizard. Select the appropriate values for your workgroup and database, and enter <strong class="source-inline">chapter2</strong> in the <strong class="bold">Schema</strong> field. Give your table the name <strong class="source-inline">customer</strong>. With Query Editor v2, you can either enter the column names and their data type manually, or you can use the data file to automatically infer the column names and their <span class="No-Break">data type.</span></p>
			<p>Let’s create a table with a data file. We will use <strong class="source-inline">customer.csv</strong>, which is available in this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2</a>. You can download this file locally to create the table using <span class="No-Break">the wizard.</span></p>
			<p>The file contains a subset of the data from the <strong class="source-inline">TPC-H</strong> dataset, available in this book's GitHub <span class="No-Break">repository</span><span class="No-Break">: </span><a href="https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH"><span class="No-Break">https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH</span></a><span class="No-Break">.</span></p>
			<p>On the <strong class="bold">Create table</strong> wizard, click on <strong class="bold">Load from CSV</strong> under the <strong class="bold">Columns</strong> tab, and provide a path to the CSV file. Once the file is selected, the schema will be inferred and automatically populated from the file, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em>. Optionally, you can modify the schema in the <strong class="bold">Column name</strong>, <strong class="bold">Data type</strong>, and <strong class="bold">Encoding</strong> fields, and under <strong class="bold">Column options</strong>, you can select different options such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Choose a<a id="_idIndexMarker062"/> default value for <span class="No-Break">the column.</span></li>
				<li>Optionally, you can turn on <strong class="bold">Automatically increment</strong> if you want the column values to increment. If you enable this option, only then can you specify a value for <strong class="bold">Auto increment seed</strong> and <strong class="bold">Auto </strong><span class="No-Break"><strong class="bold">increment step</strong></span><span class="No-Break">.</span></li>
				<li>Enter a size value for <span class="No-Break">the column.</span></li>
				<li>You also have the option to define constraints such as <strong class="bold">Not NULL</strong>, <strong class="bold">Primary key</strong>, and <span class="No-Break"><strong class="bold">Unique key</strong></span><span class="No-Break">.</span></li>
			</ul>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B19071_02_003.jpg" alt="Figure 2.3 – Create table"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Create table</p>
			<p>Additionally, as<a id="_idIndexMarker063"/> shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.4</em>, under the <strong class="bold">Table details</strong> tab, you can optionally set the table properties, such as <strong class="bold">Distribution key</strong>, <strong class="bold">Distribution style</strong>, <strong class="bold">Sort key</strong>, and <strong class="bold">Sort type</strong>. When these options are not set, Redshift will pick default settings for you, which are <strong class="bold">Auto Distribution Key</strong> and <strong class="bold">Auto </strong><span class="No-Break"><strong class="bold">Sort Key</strong></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B19071_02_004.jpg" alt="Figure 2.4 – Table details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Table details</p>
			<p>Amazon Redshift<a id="_idIndexMarker064"/> distributes data in a table according to the table’s distribution style (<strong class="source-inline">DISTSTYLE</strong>). The data rows are distributed within each compute node according to the number of slices. When you run a query against the table, all the slices of the compute node process the rows that are assigned in parallel. As a best practice (<a href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-best-dist-key.html</a>), select a table’s <strong class="source-inline">DISTSTYLE</strong> parameter to ensure even distribution<a id="_idIndexMarker065"/> of the data or use <span class="No-Break">automatic distribution.</span></p>
			<p>Amazon Redshift orders data within each slice using the table’s sort key. Amazon Redshift also enables you to define a table with compound sort keys, interleaved sort keys, or no <a id="_idIndexMarker066"/>sort keys. As a best practice (<a href="https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html">https://docs.aws.amazon.com/redshift/latest/dg/c_best-practices-sort-key.html</a>), define the sort keys and style according to your data access pattern. Having a proper sort key defined on a table can hugely improve your <span class="No-Break">query performance.</span></p>
			<p>Lastly, under <strong class="bold">Other options</strong> you can select <span class="No-Break">the following:</span></p>
			<ul>
				<li>Whether to include your table in automated and <span class="No-Break">manual snapshots</span></li>
				<li>Whether to create a session-based temporary table instead of a permanent <span class="No-Break">database table</span></li>
			</ul>
			<p>Once you have entered all the details, you can view the DDL of your table by clicking <strong class="bold">Open query in editor</strong>. You can use this later or even share it with <span class="No-Break">other users.</span></p>
			<p>Now, let’s create our table by clicking on the <strong class="bold">Create table</strong> button (<span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></p>
			<p>As you can see, it is easy for any data scientist, analyst, or user to use this wizard to create database objects (such as tables) without having to write DDL and enter each column's data type and <span class="No-Break">its length.</span></p>
			<p>Let’s now work <a id="_idIndexMarker067"/>on loading data in the customer table. Query Editor v2 enables you to load data from Amazon S3 or the local file on your computer. Please note that, at the time of writing, the option to load a local file currently supports only CSV files with a maximum size of <span class="No-Break">5 MB.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor032"/>Loading data from Amazon S3</h2>
			<p>Query editor v2 enables <a id="_idIndexMarker068"/>you to load data from Amazon S3 buckets into an existing <span class="No-Break">Redshift table.</span></p>
			<p>The <strong class="bold">Load data</strong> wizard populates data from Amazon S3 by generating the <strong class="source-inline">COPY</strong> command, which really makes it easier for a data analyst or data scientist, as they don’t have to remember the intricacies of the <strong class="source-inline">COPY</strong> command. You can load data from various file formats supported by the <strong class="source-inline">COPY</strong> command, such as CSV, JSON, Parquet, Avro, and Orc. Refer to this <a id="_idIndexMarker069"/>link for all the supported data <span class="No-Break">formats: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-format.html#copy-format</span></a><span class="No-Break">.</span></p>
			<p>Let’s look at loading data using the <strong class="bold">Load data</strong> wizard. We will load the data into our customer table from our data file (<strong class="source-inline">customer.csv</strong>), which is stored in the following Amazon S3 <span class="No-Break">location: </span><span class="No-Break">s3://packt-serverless-ml-redshift/chapter02/customer.csv</span><span class="No-Break">.</span></p>
			<p>Note that if you want to use your own Amazon S3 bucket to load the data, then download the data file from the GitHub location mentioned in the <em class="italic">Technical </em><span class="No-Break"><em class="italic">requirements</em></span><span class="No-Break"> section.</span></p>
			<p>To download a data file from GitHub, navigate to your repository, select the file, right-click the <strong class="bold">View raw</strong> button at the top of the file, select <strong class="bold">Save Link As…</strong> (as shown in the following screenshot), choose the location on your computer where you want to save the file, and <span class="No-Break">select </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B19071_02_005.jpg" alt="Figure 2.5 – Saving the data file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Saving the data file</p>
			<p>On Query Editor v2, click on <strong class="bold">Load data</strong>, which opens up the data <span class="No-Break">load wizard.</span></p>
			<p>Under <strong class="bold">Data source</strong>, select the <strong class="bold">Load from S3</strong> radio button. You can browse the S3 bucket in your account to select the data file or a folder that you want to load, or you can select a manifest file. For this exercise, paste the aforementioned S3 <span class="No-Break">file location.</span></p>
			<p>If the data file is in <a id="_idIndexMarker070"/>a different region than your Amazon Redshift Serverless, you can select the source region from the S3 file location dropdown. The wizard provides different options if you want to load a Parquet file. Then, select an option from <strong class="bold">File format</strong>, or under <strong class="bold">File</strong> options, you can select <strong class="bold">Delimiter</strong> if your data is delimited by a different character. If your file is compressed, then you can select the appropriate compression from the dropdown, such as <strong class="bold">gzip</strong>, <strong class="bold">lzop</strong>, <strong class="bold">zstd</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="bold">bzip2</strong></span><span class="No-Break">.</span></p>
			<p>Under <strong class="bold">Advanced settings</strong>, note that there are two options, <strong class="bold">Data conversion parameters</strong> and <span class="No-Break"><strong class="bold">Load operations</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li>Under the <strong class="bold">Data conversion parameters</strong> option, you can handle explicit data conversion settings – for example, a time format (<strong class="source-inline">TIMEFORMAT</strong>) as <strong class="source-inline">‘MM.DD.YYYY HH:MI:SS'</strong>. Refer to this documentation link for a full list of <span class="No-Break">parameters: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-conversion.html#copy-timeformat</span></a><span class="No-Break">.</span></li>
				<li>Under <strong class="bold">Load operations</strong>, you can manage the behavior of the load operation – for example, the number of rows for compression analysis (<strong class="source-inline">COMPROWS</strong>) as <strong class="source-inline">1,000,000</strong>. Refer to this documentation for a full list of <span class="No-Break">options: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/copy-parameters-data-load.html</span></a><span class="No-Break">.</span></li>
			</ul>
			<p>As our file<a id="_idIndexMarker071"/> contains the header row, please ensure that under <strong class="bold">Advanced settings</strong> | <strong class="bold">Data conversion parameters</strong> | <strong class="bold">Frequently used parameters</strong>, the <strong class="bold">Ignore header rows (as 1)</strong> option <span class="No-Break">is checked.</span></p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.6</em>, select the <strong class="bold">Target table</strong> parameters and <strong class="bold">IAM role</strong> to load <span class="No-Break">the data:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B19071_02_006.jpg" alt="Figure 2.6 – Load data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Load data</p>
			<p>Once you click on <strong class="bold">Load data</strong>, Query Editor v2 will generate the <strong class="source-inline">COPY</strong> command in the editor and start loading by running the <span class="No-Break"><strong class="source-inline">COPY</strong></span><span class="No-Break"> statement.</span></p>
			<p>Now that we have loaded our data, let’s quickly verify the load and check the data by querying the table, as <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B19071_02_007.jpg" alt="Figure 2.7 – Querying the data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Querying the data</p>
			<p>Query Editor<a id="_idIndexMarker072"/> v2 enables you to save your queries in the editor for later use. You can do so by clicking on the <strong class="bold">Save</strong> button and providing a name for the saved query. For example, if you want to reuse the preceding load data query (the <strong class="source-inline">COPY</strong> command) in the future and, let’s say, the target table is the same but the data location on Amazon S3 is different, then you can easily modify this query and load the data quickly. Alternatively, you can even parameterize the query to pass, for example, an S3 location as <strong class="source-inline">${s3_location}</strong>, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B19071_02_008.jpg" alt="Figure 2.8 – Saving the query"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Saving the query</p>
			<p class="callout-heading">Sharing queries</p>
			<p class="callout">With Query Editor v2, you can share your saved queries with your team. This way, many users can collaborate and share the same query. Internally, Query Editor manages the query versions, so you can track the changes as well. To learn more about this, refer to this AWS <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-team.html#query-editor-v2-query-share</span></a><span class="No-Break">.</span></p>
			<p>Now that we <a id="_idIndexMarker073"/>have covered how Query Editor v2 enables users to easily create database objects and load data using the UI interface with a click of a few buttons, let us dive into Amazon Redshift’s <strong class="source-inline">COPY</strong> command to load the data into your <span class="No-Break">data warehouse.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>Loading data from a local drive</h2>
			<p>Query Editor v2 <a id="_idIndexMarker074"/>enables users to load data from a local file on their computer and perform analysis on it quickly. Often, database users such as data analysts or data scientists have data files on their local computer that they want to load quickly into a Redshift table, without moving the file into a remote location such as <span class="No-Break">Amazon S3.</span></p>
			<p>In order to load the data from a local file, Query Editor v2 requires a staging Amazon S3 bucket in your account. If it is not configured, then you will see an error similar to the one seen in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B19071_02_009.jpg" alt="Figure 2.9 – An error message"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – An error message</p>
			<p>To avoid the preceding error, users must do the <span class="No-Break">following configuration:</span></p>
			<ol>
				<li>The <a id="_idIndexMarker075"/>account users must be configured with the proper permissions, as follows. Attach <a id="_idIndexMarker076"/>the following policy to your Redshift Serverless IAM role. Replace the resource names <span class="No-Break">as highlighted:</span><pre class="source-code">
{</pre><pre class="source-code">
    "Version": "2012-10-17",</pre><pre class="source-code">
    "Statement": [</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Action": [</pre><pre class="source-code">
                "s3:ListBucket",</pre><pre class="source-code">
                "s3:GetBucketLocation"</pre><pre class="source-code">
            ],</pre><pre class="source-code">
            "Resource": [</pre><pre class="source-code">
                "arn:aws:s3:::<strong class="bold">&lt;staging-bucket-name&gt;"</strong></pre><pre class="source-code">
            ]</pre><pre class="source-code">
        },</pre><pre class="source-code">
        {</pre><pre class="source-code">
            "Effect": "Allow",</pre><pre class="source-code">
            "Action": [</pre><pre class="source-code">
                "s3:PutObject",</pre><pre class="source-code">
                "s3:GetObject",</pre><pre class="source-code">
                "s3:DeleteObject"</pre><pre class="source-code">
            ],</pre><pre class="source-code">
            "Resource": [</pre><pre class="source-code">
                "arn:aws:s3<strong class="bold">:::&lt;staging-bucket-name&gt;[/&lt;optional-prefix&gt;]</strong>/${aws:userid}/*"</pre><pre class="source-code">
            ]</pre><pre class="source-code">
        }</pre><pre class="source-code">
    ]</pre><pre class="source-code">
}</pre></li>
				<li>Your <a id="_idIndexMarker077"/>administrator must configure the common Amazon S3 bucket in the <strong class="bold">Account settings</strong> window, as <span class="No-Break">shown here:</span><ol><li>Click on the settings icon (<img src="image/B19071_02_icon_1.png" alt=""/>) and select <strong class="bold">Account settings</strong>, as shown in the <span class="No-Break">following screenshot:</span></li></ol></li>
			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B19071_02_010.jpg" alt="Figure 2.10 – Account settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Account settings</p>
			<ol>
				<li value="2">In the <strong class="bold">Account settings</strong> window, under <strong class="bold">General settings</strong> | <strong class="bold">S3 bucket</strong> | <strong class="bold">S3 URI</strong>, enter the URI of the S3 bucket that will be used for staging during the local file load, and then click on <strong class="bold">Save</strong>. Ensure that your IAM role has permission to read and write on the <span class="No-Break">S3 bucket:</span></li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B19071_02_011.jpg" alt="Figure 2.11 – Enter the URI of the S3 bucket under General settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Enter the URI of the S3 bucket under General settings</p>
			<p>Refer to this documentation for <span class="No-Break">complete information:</span></p>
			<p><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-loading.html#query-editor-v2-loading-data-local</span></a></p>
			<h3>Creating a table and loading data from a local CSV file</h3>
			<p>Let’s create<a id="_idIndexMarker078"/> a new table. Navigate to Query Editor v2 and create a supplier table using the following <span class="No-Break">DDL command:</span></p>
			<pre class="source-code">
CREATE TABLE chapter2.supplier (
    s_suppkey integer NOT NULL ENCODE raw distkey,
    s_name character(25) NOT NULL ENCODE lzo,
    s_address character varying(40) NOT NULL ENCODE lzo,
    s_nationkey integer NOT NULL ENCODE az64,
    s_phone character(15) NOT NULL ENCODE lzo,
    s_acctbal numeric(12, 2) NOT NULL ENCODE az64,
    s_comment character varying(101) NOT NULL ENCODE lzo,
    PRIMARY KEY (s_suppkey)
) DISTSTYLE KEY
SORTKEY (s_suppkey);</pre>
			<p>We will load the data into our supplier table from our data file (<strong class="source-inline">supplier.csv</strong>), which is stored in the following GitHub <span class="No-Break">location: </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/DataFiles/chapter2/supplier.csv</span></a><span class="No-Break">.</span></p>
			<p>To download the file on your local computer, right-click on <strong class="bold">Raw</strong> and click on <strong class="bold">Save </strong><span class="No-Break"><strong class="bold">Link as</strong></span><span class="No-Break">.</span></p>
			<p>In order to load<a id="_idIndexMarker079"/> data into the supplier table from Query Editor v2, click on <strong class="bold">Load data</strong>, which opens up the data load wizard. Under <strong class="bold">Data source</strong>, select the <strong class="bold">Load from local file</strong> radio button. Click on <strong class="bold">Browse</strong> and select the <strong class="source-inline">supplier.csv</strong> file from your local drive. Under the <strong class="bold">Target table</strong> options, set <strong class="bold">Schema</strong> as <strong class="bold">chapter2</strong> and <strong class="bold">Table</strong> as <strong class="bold">supplier</strong>. Click on <strong class="bold">Load data</strong> to start <span class="No-Break">the load:</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B19071_02_012.jpg" alt="Figure 2.12 – The Load data wizard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – The Load data wizard</p>
			<p>Once the data is <a id="_idIndexMarker080"/>loaded successfully, you would see a message like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B19071_02_013.jpg" alt="Figure 2.13 – The message after successfully loading the data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – The message after successfully loading the data</p>
			<p>Verify the data load by running the following <span class="No-Break">SQL query:</span></p>
			<pre class="source-code">
select * from chapter2.supplier;</pre>
			<p>You should be able to see 100 rows loaded from <span class="No-Break">the file:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B19071_02_014.jpg" alt="Figure 2.14 – Data load verification"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Data load verification</p>
			<p>We have <a id="_idIndexMarker081"/>now successfully loaded our data from the Query Editor v2 <strong class="bold">Load data</strong> wizard, using files from an Amazon S3 bucket and your local computer. Let’s look into Amazon Redshift’s <strong class="source-inline">COPY</strong> command in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor034"/>Data loading from Amazon S3 using the COPY command</h1>
			<p>Data warehouses<a id="_idIndexMarker082"/> are typically designed to ingest and store huge volumes of data, and one of the key aspects of any analytical process is to ingest such huge volumes in the most efficient way. Loading such huge data can take a long time as well as consume a lot of compute resources. As pointed out earlier, there are several ways to load data in your Redshift Serverless data warehouse, and one of the fastest and most scalable methods is the <span class="No-Break"><strong class="source-inline">COPY</strong></span><span class="No-Break"> command.</span></p>
			<p>The <strong class="source-inline">COPY</strong> command<a id="_idIndexMarker083"/> loads your data in parallel from<a id="_idIndexMarker084"/> files, taking advantage of Redshift’s <strong class="bold">massively parallel processing</strong> (<strong class="bold">MPP</strong>) architecture. It can load data from Amazon S3, Amazon EMR, Amazon DynamoDB, or text files on remote hosts (SSH). It is the most efficient way to load a table in your Redshift data warehouse. With proper IAM policies, you can securely control who can access and load data in <span class="No-Break">your database.</span></p>
			<p>In the earlier section, we saw how Query Editor v2 generates the <strong class="source-inline">COPY</strong> command to load data from the wizard. In this section, we will dive deep and talk about how you can write the <strong class="source-inline">COPY</strong> command and load data from Amazon S3, and what some of the best <span class="No-Break">practices are.</span></p>
			<p>Let’s take a look at the <strong class="source-inline">COPY</strong> command to load data into your Redshift <span class="No-Break">data warehouse:</span></p>
			<pre class="source-code">
COPY table-name
[ column-list ]
FROM data_source
authorization
[ [ FORMAT ] [ AS ] data_format ]
[ parameter [ argument ] [, ... ] ]</pre>
			<p>The <strong class="source-inline">COPY</strong> command<a id="_idIndexMarker085"/> requires <span class="No-Break">three parameters:</span></p>
			<ul>
				<li><strong class="source-inline">table-name</strong>: The target table name existing in the database (persistent <span class="No-Break">or temporary)</span></li>
				<li><strong class="source-inline">data_source</strong>: The data source location (such as the <span class="No-Break">S3 bucket)</span></li>
				<li><strong class="source-inline">authorization</strong>: The authentication method (for example, the <span class="No-Break">IAM role)</span></li>
			</ul>
			<p>By default, the <strong class="source-inline">COPY</strong> command <a id="_idIndexMarker086"/>source data format is expected to be in character-delimited UTF-8 text files, with a pipe character (<strong class="source-inline">|</strong>) as the default delimiter. If your source data is in another format, you can pass it as a parameter to specify the data format. Amazon Redshift supports different data formats, such as fixed-width text files, character-delimited files, CSV, JSON, Parquet, <span class="No-Break">and Avro.</span></p>
			<p>Additionally, the <strong class="source-inline">COPY</strong> command provides optional parameters to handle data conversion such as the data format, <strong class="source-inline">null</strong>, and encoding. To get the latest details, refer to this AWS <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>Loading data from a Parquet file</h2>
			<p>In the earlier section, we worked on loading a <a id="_idIndexMarker087"/>CSV file into the customer table in our database. For this exercise, let’s try to load a columnar data format file such as<a id="_idIndexMarker088"/> Parquet. We will be using a subset of <strong class="source-inline">TPC-H</strong> data, which may be found <span class="No-Break">here: </span><a href="https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB"><span class="No-Break">https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH/3TB</span></a><span class="No-Break">.</span></p>
			<p>The TPC is an organization focused on developing data benchmark standards. You may read more about TPC <span class="No-Break">here: </span><a href="https://www.tpc.org/default5.asp"><span class="No-Break">https://www.tpc.org/default5.asp</span></a><span class="No-Break">.</span></p>
			<p>The modified data (<strong class="source-inline">lineitem.parquet</strong>) is available on <span class="No-Break">GitHub: </span><a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2"><span class="No-Break">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2</span></a><span class="No-Break">.</span></p>
			<p>The data needed for the <strong class="source-inline">COPY</strong> command is available <span class="No-Break">here: </span><span class="No-Break"><strong class="source-inline">s3://packt-serverless-ml-redshift/chapter02/lineitem.parquet</strong></span><span class="No-Break">.</span></p>
			<p>This file contains approximately 6 million rows and is around 200 MB <span class="No-Break">in size:</span></p>
			<ol>
				<li>Let’s first start by creating a table named <strong class="source-inline">lineitem</strong> in the <span class="No-Break"><strong class="source-inline">chapter2</strong></span><span class="No-Break"> schema:</span><pre class="source-code">
-- Create lineitem table</pre><pre class="source-code">
CREATE TABLE chapter2.lineitem</pre><pre class="source-code">
(l_orderkey     bigint,</pre><pre class="source-code">
l_partkey       bigint,</pre><pre class="source-code">
l_suppkey       integer,</pre><pre class="source-code">
l_linenumber    integer,</pre><pre class="source-code">
l_quantity      numeric(12,2),</pre><pre class="source-code">
l_extendedprice numeric(12,2),</pre><pre class="source-code">
l_discount      numeric(12,2),</pre><pre class="source-code">
l_tax           numeric(12,2),</pre><pre class="source-code">
l_returnflag    character(1),</pre><pre class="source-code">
l_linestatus    character(1),</pre><pre class="source-code">
l_shipdate      date,</pre><pre class="source-code">
l_commitdate    date,</pre><pre class="source-code">
l_receiptdate   date,</pre><pre class="source-code">
l_shipinstruct  character(25),</pre><pre class="source-code">
l_shipmode      character(10),</pre><pre class="source-code">
l_comment       varchar(44))</pre><pre class="source-code">
distkey(l_orderkey) compound sortkey(l_orderkey,l_shipdate);</pre></li>
				<li>Now, let’s <a id="_idIndexMarker089"/>load the data using the <strong class="source-inline">COPY</strong> command <a id="_idIndexMarker090"/>from the <span class="No-Break"><strong class="source-inline">lineitem.parquet</strong></span><span class="No-Break"> file:</span><pre class="source-code">
COPY chapter2.lineitem</pre><pre class="source-code">
FROM 's3://packt-serverless-ml-redshift/chapter02/lineitem.parquet'</pre><pre class="source-code">
IAM_ROLE default</pre><pre class="source-code">
FORMAT AS PARQUET;</pre></li>
			</ol>
			<p>Now that we have loaded our data, let’s quickly verify the load and check the data by querying the table, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B19071_02_015.jpg" alt="Figure 2.15 – The query table"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – The query table</p>
			<p>In this section, we discussed how the <strong class="source-inline">COPY</strong> command helps load your data in different formats, such as CSV, Parquet, and JSON, from Amazon S3 buckets. Let’s see how you can automate the <strong class="source-inline">COPY</strong> command to load the data as soon as it is available in an Amazon S3 bucket. The next section on automating a <strong class="source-inline">COPY</strong> job is currently in public preview at the time <span class="No-Break">of writing.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Automating file ingestion with a COPY job</h2>
			<p>In your <a id="_idIndexMarker091"/>data warehouse, data is continuously ingested from Amazon <a id="_idIndexMarker092"/>S3. Previously, you wrote custom code externally or locally to achieve this continuous ingestion of data with scheduling tools. With Amazon Redshift’s auto-copy feature, users can easily automate data ingestion from Amazon S3 to Amazon Redshift. To achieve this, you will write a simple SQL command to create a <strong class="source-inline">COPY</strong> job (<a href="https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html">https://docs.aws.amazon.com/redshift/latest/dg/r_COPY-JOB.html</a>), and the <strong class="source-inline">COPY</strong> command will trigger automatically as and when it detects new files in the source Amazon S3 path. This will ensure that users have the latest data for processing available shortly after it lands in the S3 path, without having to build an external <span class="No-Break">custom framework.</span></p>
			<p>To get started, you can set up a <strong class="source-inline">COPY</strong> job, as shown here, or modify the existing <strong class="source-inline">COPY</strong> command by adding the <strong class="source-inline">JOB </strong><span class="No-Break"><strong class="source-inline">CREATE</strong></span><span class="No-Break"> parameter:</span></p>
			<pre class="source-code">
COPY &lt;table-name&gt;
FROM 's3://&lt;s3-object-path&gt;'
[COPY PARAMETERS...]
JOB CREATE &lt;job-name&gt; [AUTO ON | OFF];</pre>
			<p>Let’s break <span class="No-Break">this down:</span></p>
			<ul>
				<li><strong class="source-inline">job-name</strong> is the name of <span class="No-Break">the job</span></li>
				<li><strong class="source-inline">AUTO ON | OFF</strong> indicates whether the data from Amazon S3 has loaded automatically into an Amazon <span class="No-Break">Redshift table</span></li>
			</ul>
			<p>As you<a id="_idIndexMarker093"/> can see, the <strong class="source-inline">COPY</strong> job is an extension of <a id="_idIndexMarker094"/>the <strong class="source-inline">COPY</strong> command, and auto-ingestion of <strong class="source-inline">COPY</strong> jobs is enabled <span class="No-Break">by default.</span></p>
			<p>If you want to run a <strong class="source-inline">COPY</strong> job, you can do so by running the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
COPY JOB RUN job-name</pre>
			<p>For the latest details, refer to this AWS <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html"><span class="No-Break">https://docs.aws.amazon.com/redshift/latest/dg/loading-data-copy-job.html</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Best practices for the COPY command</h2>
			<p>The <a id="_idIndexMarker095"/>following best practices<a id="_idIndexMarker096"/> will help you get the most out of the <span class="No-Break"><strong class="source-inline">COPY</strong></span><span class="No-Break"> command:</span></p>
			<ul>
				<li>Make the most of parallel processing by splitting data into multiple compressed files or by defining distribution keys on your target tables, as we did in <span class="No-Break">our example.</span></li>
				<li>Use a single <strong class="source-inline">COPY</strong> command to load data from multiple files. If you use multiple concurrent <strong class="source-inline">COPY</strong> commands to load the same target table from multiple files, then the load is done serially, which is much slower than a single <span class="No-Break"><strong class="source-inline">COPY</strong></span><span class="No-Break"> command.</span></li>
				<li>If your data file contains an uneven or mismatched number of fields, then provide the list of columns as <span class="No-Break">comma-separated values.</span></li>
				<li>When you want to load a single target table from multiple data files and your data files have a similar structure but different naming conventions, or are in different folders in an Amazon S3 bucket, then use a manifest file. You can supply the full path of the files to be loaded in a JSON-formatted text file. The following is the syntax to use a <span class="No-Break">manifest file:</span><pre class="source-code">
copy &lt;table_name&gt; from 's3://&lt;bucket_name&gt;/&lt;manifest_file&gt;'</pre><pre class="source-code">
authorization</pre><pre class="source-code">
manifest;</pre></li>
				<li>For a <strong class="source-inline">COPY</strong><a id="_idIndexMarker097"/> job, use unique filenames for each file that you <a id="_idIndexMarker098"/>want to load. If a file is already processed and any changes are done after that, then the <strong class="source-inline">COPY</strong> job will not process the file, so remember to rename the <span class="No-Break">updated file.</span></li>
			</ul>
			<p>So far, we have seen two approaches to data loading in your Amazon Redshift data warehouse – using the Query Editor v2 wizard and writing an individual <strong class="source-inline">COPY</strong> command to trigger ad hoc data loading. Let us now look into how you can use an AWS SDK to load data using the Redshift <span class="No-Break">Data API.</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor038"/>Data loading using the Redshift Data API</h1>
			<p>The Amazon Redshift Data API<a id="_idIndexMarker099"/> is a built-in native API interface to access your Amazon Redshift database without configuring any <strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>) or <strong class="bold">Open Database Connectivity</strong> (<strong class="bold">ODBC</strong>) drivers. You can ingest or query data with a simple API endpoint without managing a persistent connection. The Data API provides a secure way to access your database by using either IAM temporary credentials or AWS Secrets Manager. It provides a secure HTTP endpoint to run SQL statements asynchronously, meaning you can retrieve your results later. By default, your query results are stored for 24 hours. The Redshift Data API integrates seamlessly with different AWS SDKs, such as Python, Go, Java, Node.js, PHP, Ruby, and C++. You can also integrate the API with AWS Glue for an ETL data pipeline or use it with AWS Lambda to invoke different <span class="No-Break">SQL statements.</span></p>
			<p>There are many use cases where you can utilize the Redshift Data API, such as ETL orchestration with AWS Step Functions, web service-based applications, event-driven applications, and accessing your Amazon Redshift database using Jupyter notebooks. If you want to just run an individual SQL statement, then you can use the <strong class="bold">AWS Command-Line Interface</strong> (<strong class="bold">AWS CLI</strong>) or<a id="_idIndexMarker100"/> any programming language. The following is an example of executing a single SQL statement in Amazon Redshift Serverless from the <span class="No-Break">AWS CLI:</span></p>
			<pre class="source-code">
aws redshift-data execute-statement
--WorkgroupName redshift-workgroup-name
--database dev
--sql 'select * from redshift_table';</pre>
			<p>Note that, for <a id="_idIndexMarker101"/>Redshift Serverless, you only need to provide the workgroup name and database name. Temporary user credentials are pulled from IAM authorization. For Redshift Serverless, add the following permission in the IAM policy attached to your cluster IAM role to access the Redshift <span class="No-Break">Data API:</span></p>
			<pre class="source-code">
redshift-serverless:GetCredentials</pre>
			<p>In order to showcase how you can ingest data using the Redshift Data API, we will carry out the following steps using Jupyter Notebook. Let’s create a notebook instance in our <span class="No-Break">AWS account.</span></p>
			<p>On the console home page, search for <strong class="source-inline">Amazon SageMaker</strong>. Click on the hamburger icon (<img src="image/B19071_02_icon_2.png" alt=""/>) in the top-left corner, then <strong class="bold">Notebook</strong>, and then <strong class="bold">Notebook instances</strong>. Click on <strong class="bold">Create notebook instance</strong> and provide the necessary input. Once the notebook instance is in service, click on <span class="No-Break"><strong class="bold">Open Jupyter</strong></span><span class="No-Break">.</span></p>
			<p>The following screenshot shows a created <span class="No-Break">notebook instance:</span></p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B19071_02_016.jpg" alt="Figure 2.16 – Creating a notebook instance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Creating a notebook instance</p>
			<p>The Jupyter notebook for this exercise is available at this GitHub location: <a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/blob/main/Chapter2.ipynb</a>. Download this notebook to your local machine and save it in <span class="No-Break">a folder.</span></p>
			<p>The data (<strong class="source-inline">orders.parquet</strong>) for this exercise is available on GitHub at <a href="https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2">https://github.com/PacktPublishing/Serverless-Machine-Learning-with-Amazon-Redshift/tree/main/DataFiles/chapter2</a>, as well as this Amazon S3 <span class="No-Break">location: </span><span class="No-Break"><strong class="source-inline">s3://packt-serverless-ml-redshift/chapter2/orders.parquet</strong></span><span class="No-Break">.</span></p>
			<p>We will use a subset of the <strong class="source-inline">orders</strong> data, which is referenced from the <strong class="source-inline">TPC-H</strong> dataset available <span class="No-Break">here: </span><a href="https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH"><span class="No-Break">https://github.com/awslabs/amazon-redshift-utils/tree/master/src/CloudDataWarehouseBenchmark/Cloud-DWB-Derived-from-TPCH</span></a><span class="No-Break">.</span></p>
			<p>Let’s first open the<a id="_idIndexMarker102"/> downloaded notebook (<strong class="source-inline">Chapter2.ipynb</strong>) by following <span class="No-Break">these steps:</span></p>
			<ol>
				<li>On the Jupyter Notebook landing page, click on <strong class="bold">Upload</strong> and open the previously <span class="No-Break">downloaded notebook.</span></li>
				<li>Select the kernel (<strong class="source-inline">conda_python3</strong>) once the notebook <span class="No-Break">is uploaded.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Redshift Serverless requires your <strong class="source-inline">boto3</strong> version to be greater than <span class="No-Break">version </span><span class="No-Break">1.24.32</span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Let’s check our <strong class="source-inline">boto3</strong> library version, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.17</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B19071_02_017.jpg" alt="Figure 2.17 – Checking the boto3 version"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Checking the boto3 version</p>
			<p>If you want to install a specific version greater than 1.24.32, then check the <span class="No-Break">following example:</span></p>
			<pre class="source-code">
pip install boto3==1.26.35</pre>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>Creating table</h2>
			<p>As you can see <a id="_idIndexMarker103"/>in the <strong class="source-inline">chapter2.ipynb</strong> notebook, we have provided step-by-step instructions to connect to your Redshift Serverless endpoint and perform the <span class="No-Break">necessary operations:</span></p>
			<ol>
				<li>Let’s start by setting up the parameters and importing the necessary libraries for this exercise. We will set the following <span class="No-Break">two parameters:</span><ul><li><strong class="source-inline">REDSHIFT_WORKGROUP</strong>: The name of the Redshift <span class="No-Break">Serverless workgroup</span></li><li><strong class="source-inline">S3_DATA_FILE</strong>: The source data file for <span class="No-Break">the load:</span><pre class="source-code">
import boto3</pre><pre class="source-code">
import time</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
session = boto3.session.Session()</pre><pre class="source-code">
region = session.region_name</pre><pre class="source-code">
REDSHIFT_WORKGROUP = '&lt;workgroup name&gt;'</pre><pre class="source-code">
S3_DATA_FILE='s3://packt-serverless-ml-redshift/chapter2/orders.parquet'</pre></li></ul></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Remember to set the parameters as per your settings in the <span class="No-Break">Jupyter notebook.</span></p>
			<ol>
				<li value="2">In order to <a id="_idIndexMarker104"/>create the table, let’s first prepare our DDL and assign it to a <span class="No-Break"><strong class="source-inline">table_ddl</strong></span><span class="No-Break"> variable:</span><pre class="source-code">
table_ddl = """</pre><pre class="source-code">
DROP TABLE IF EXISTS chapter2.orders CASCADE;</pre><pre class="source-code">
CREATE TABLE chapter2.orders</pre><pre class="source-code">
(o_orderkey     bigint NOT NULL,</pre><pre class="source-code">
o_custkey       bigint NOT NULL encode az64,</pre><pre class="source-code">
o_orderstatus   character(1) NOT NULL encode lzo,</pre><pre class="source-code">
o_totalprice    numeric(12,2) NOT NULL encode az64,</pre><pre class="source-code">
o_orderdate     date NOT NULL,</pre><pre class="source-code">
o_orderpriority character(15) NOT NULL encode lzo,</pre><pre class="source-code">
o_clerk         character(15) NOT NULL encode lzo,</pre><pre class="source-code">
o_shippriority  integer NOT NULL encode az64,</pre><pre class="source-code">
o_comment       character varying(79) NOT NULL encode lzo</pre><pre class="source-code">
)</pre><pre class="source-code">
distkey(o_orderkey) compound sortkey(o_orderkey,o_orderdate);"""</pre></li>
				<li>Using the <strong class="source-inline">boto3</strong> library, we will connect to the Redshift <span class="No-Break">Serverless workgroup:</span><pre class="source-code">
client = boto3.client("redshift-data")</pre></li>
			</ol>
			<p>There are different methods that are available to execute different operations on your Redshift Serverless endpoint. Check out the entire list in this <span class="No-Break">documentation: </span><a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html"><span class="No-Break">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift-data.html</span></a><span class="No-Break">.</span></p>
			<ol>
				<li value="4">We will use the <strong class="source-inline">execute_statement</strong> method to run an SQL statement, which can <a id="_idIndexMarker105"/>be in the <strong class="bold">data manipulation language</strong> (<strong class="bold">DML</strong>) or<a id="_idIndexMarker106"/> DDL. This method runs a single SQL statement. To run multiple statements, you can use <strong class="source-inline">BatchExecuteStatement</strong>. To get a complete list of different methods and how to use them, please refer to this AWS <span class="No-Break">documentation: </span><a href="https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html"><span class="No-Break">https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html</span></a><span class="No-Break">:</span><pre class="source-code">
client = boto3.client("redshift-data")</pre><pre class="source-code">
res = client.execute_statement(Database='dev', Sql=table_ddl,                                   WorkgroupName=REDSHIFT_WORKGROUP)</pre></li>
			</ol>
			<p>As you can see from the preceding code block, we will first set the client as <strong class="source-inline">redshift-data</strong> and then call <strong class="source-inline">execute_statement</strong> to connect the Serverless endpoint, using the <strong class="source-inline">Database</strong> name and <strong class="source-inline">WorkgroupName</strong>. The method uses temporary credentials to connect to your <span class="No-Break">Serverless workgroup.</span></p>
			<p>We will also pass <strong class="source-inline">table_ddl</strong> as a parameter to create the table. We will create the <strong class="source-inline">Orders </strong>table in our <span class="No-Break"><strong class="source-inline">chapter2</strong></span><span class="No-Break"> schema.</span></p>
			<ol>
				<li value="5">The Redshift Data API sends back a response element once the action is successful, in a JSON format as a dictionary object. One of the response elements is a SQL statement identifier. This value is universally unique and generated by the Amazon Redshift Data API. As you can see in the following code, we have captured the response element, <strong class="source-inline">Id</strong>, from the output <span class="No-Break">object, </span><span class="No-Break"><strong class="source-inline">res</strong></span><span class="No-Break">:</span><pre class="source-code">
query_id = res["Id"]</pre><pre class="source-code">
print(query_id)</pre></li>
				<li>In order <a id="_idIndexMarker107"/>to make sure that your query is completed, you can use the <strong class="source-inline">describe_statement</strong> method and pass your <strong class="source-inline">id</strong> statement as a parameter. This method sends out the response, which contains information that includes when the query started, when it finished, the query status, the number of rows returned, and the <span class="No-Break">SQL statement.</span></li>
			</ol>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B19071_02_018.jpg" alt="Figure 2.18 – Checking the query status"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.18 – Checking the query status</p>
			<p>As you can see in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.18</em>, we have captured the status of the statement that we ran, and it sends out the status as <strong class="source-inline">FINISHED</strong>. This means that we have created our table in the database, and you can verify this by writing a simple <strong class="source-inline">SELECT</strong> statement against <span class="No-Break">the table.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Loading data using the Redshift Data API</h2>
			<p>Now, let’s<a id="_idIndexMarker108"/> move forward to load data into this newly created table. You can use the S3 location for the source data, as mentioned previously. If you use a different S3 location, then remember to replace the path in the <span class="No-Break">parameter (</span><span class="No-Break"><strong class="source-inline">S3_DATA_FILE</strong></span><span class="No-Break">):</span></p>
			<ol>
				<li>Let’s write a <strong class="source-inline">COPY</strong> command, as shown in the following code block. We will create the <strong class="source-inline">COPY</strong> command in the <strong class="source-inline">load_data</strong> variable, using the S3 path as <span class="No-Break">a parameter:</span><pre class="source-code">
load_data = f"""COPY chapter2.orders</pre><pre class="source-code">
FROM '{S3_DATA_FILE}'</pre><pre class="source-code">
IAM_ROLE default</pre><pre class="source-code">
FORMAT AS PARQUET;"""</pre></li>
				<li>Next, we will use the <strong class="source-inline">execute_statement</strong> method to run this <strong class="source-inline">COPY</strong> command and capture the <span class="No-Break"><strong class="source-inline">id</strong></span><span class="No-Break"> statement:</span><pre class="source-code">
res = client.execute_statement(Database='dev', Sql=load_data,</pre><pre class="source-code">
                              WorkgroupName=REDSHIFT WORKGROUP)</pre><pre class="source-code">
query_id = res["Id"]</pre><pre class="source-code">
print(query_id)</pre></li>
			</ol>
			<p>Be sure to check whether the status of the query <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">FINISHED</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Once the statement status is defined as <strong class="source-inline">FINISHED</strong>, we will verify our data load by running a count query, as <span class="No-Break">shown here:</span><pre class="source-code">
cnt = client.execute_statement(Database='dev', Sql='Select count(1) from chapter2.orders ;', WorkgroupName=REDSHIFT_WORKGROUP)</pre><pre class="source-code">
query_id = cnt["Id"]</pre></li>
			</ol>
			<p>We will now print <span class="No-Break">the results:</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B19071_02_019.jpg" alt="Figure 2.19 – Count query results"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.19 – Count query results</p>
			<p>As you <a id="_idIndexMarker109"/>can see in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.19</em>, we have successfully loaded 1.5 <span class="No-Break">million rows.</span></p>
			<p>In the notebook, we have provided a combined code block to show how you can convert all these steps into a function, calling it as and when you require it to load data into a <span class="No-Break">new table.</span></p>
			<p>We also have a GitHub repository (<a href="https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/">https://github.com/aws-samples/getting-started-with-amazon-redshift-data-api/</a>), which showcases how to get started with the Amazon Redshift Data API in different languages, such as Go, Java, JavaScript, Python, and TypeScript. You can go through the step-by-step process explained in the repository to build your custom application in all these languages, using the Redshift <span class="No-Break">Data API.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor041"/>Summary</h1>
			<p>In this chapter, we showcased how you can load data into your Amazon Redshift Serverless database using three different tools and methods, by using the query editor v GUI interface, the Redshift <strong class="source-inline">COPY</strong> command to load the data, and the Redshift Data API using Python in a Jupyter notebook. All three methods are efficient and easy to use for your different <span class="No-Break">use cases.</span></p>
			<p>We also talked about some of the best practices for the <strong class="source-inline">COPY</strong> command to make efficient use <span class="No-Break">of it.</span></p>
			<p>In the next chapter, we will start with our first topic concerning Amazon Redshift machine learning, and you will see how you can leverage it in your Amazon Redshift Serverless <span class="No-Break">data warehouse.</span></p>
		</div>
	</body></html>