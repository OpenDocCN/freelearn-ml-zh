- en: Machine Learning in Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉中的机器学习
- en: In the previous chapters, we learned about a number of algorithms for object
    detection and tracking. We learned how to use color-based algorithms, such as
    Mean Shift and CAM Shift, in conjunction with histograms and back-projection images
    to locate an object in an image with incredible speed. We also learned about template
    matching and how it can be used to find objects with a known template of pixels
    in an image. All of these algorithms rely in one way or another on image properties,
    such as brightness or color, that are easily affected by a change in lighting
    of the environment. Based on these facts, we moved on to learn about algorithms
    that are based on knowledge about significant areas in an image, called **keypoints**
    or **features**. We learned about many edge- and keypoint-detection algorithms
    and how to extract descriptors for those keypoints. We also learned about descriptor
    matchers and how to detect an object in an image using good matches of descriptors
    extracted from an image of the object of interest and the scene where we're looking
    for that object.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了关于目标检测和跟踪的许多算法。我们学习了如何结合直方图和反向投影图像使用基于颜色的算法，如均值漂移和CAM漂移，以极快的速度在图像中定位目标。我们还学习了模板匹配及其如何用于在图像中找到具有已知像素模板的对象。所有这些算法都以某种方式依赖于图像属性，如亮度或颜色，这些属性很容易受到环境光照变化的影响。基于这些事实，我们继续学习基于图像中显著区域知识的算法，称为**关键点**或**特征**。我们学习了关于边缘和关键点检测算法以及如何提取这些关键点的描述符。我们还学习了描述符匹配器以及如何使用从感兴趣对象图像和搜索该对象的场景中提取的描述符的良好匹配来检测图像中的对象。
- en: In this chapter, we're going to take one big step forward and learn about algorithms
    that can be used to extract a model from a large number of images of an object,
    and later use that model to detect an object in an image or simply classify an
    image. Such algorithms are the meeting point of machine learning algorithms and
    computer vision algorithms. Anyone familiar with artificial intelligence and machine
    learning algorithms in general will have an easy time proceeding with this chapter,
    even if they are not fluent in the exact algorithms and examples presented in
    this chapter. However, those who are totally new to such concepts will probably
    need to grab another book, preferably about machine learning, to familiarize themselves
    with algorithms, such as **support vector machines** (**SVM**), **artificial neural
    networks** (**ANN**), cascade classification, and deep learning, which we'll be
    learning about in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将迈出一大步，学习可以用于从大量对象图像中提取模型并随后使用该模型在图像中检测对象或简单地分类图像的算法。这些算法是机器学习算法和计算机视觉算法的交汇点。任何熟悉人工智能和一般机器学习算法的人都将很容易继续本章的学习，即使他们不熟悉本章中介绍的精确算法和示例。然而，对于那些对这类概念完全陌生的人来说，可能需要再找一本书，最好是关于机器学习的，以便熟悉我们将在本章学习的算法，例如**支持向量机**（**SVM**）、**人工神经网络**（**ANN**）、级联分类和深度学习。
- en: 'In this chapter, we''ll look at the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下内容：
- en: How to train and use SVM for classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练和使用SVM进行分类
- en: Using HOG and SVM for image classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用HOG和SVM进行图像分类
- en: How to train and use ANN for prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练和使用ANN进行预测
- en: How to train and use Haar or LBP cascade classifiers for real-time object detection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练和使用Haar或LBP级联分类器进行实时目标检测
- en: How to use pre-trained models from third-party deep learning frameworks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用第三方深度学习框架中的预训练模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: An IDE to develop C++ or Python applications
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于开发C++或Python应用程序的IDE
- en: The OpenCV library
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV库
- en: Refer to [Chapter 2](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4),
    *Getting Started with OpenCV* for more information about how to set up a personal
    computer and make it ready for developing computer vision applications using the
    OpenCV library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何设置个人计算机并使其准备好使用OpenCV库开发计算机视觉应用程序的更多信息，请参阅[第2章](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4)，*OpenCV入门*。
- en: You can use this URL to download the source codes and examples for this chapter: [https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此URL下载本章的源代码和示例：[https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08)。
- en: Support vector machines
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: To put it as simply as possible, SVMs are used for creating a model from a labeled
    set of training samples that can be used to predict the label of new samples.
    For instance, assume we have a set of sample data belonging to two different groups.
    Each sample in our training dataset is a vector of floating-point numbers that
    can correspond to anything, such as a simple point in 2D or 3D space, and each
    sample is labeled with a number, such as 1, 2, or 3\. Having such data, we can
    train an SVM model that can be used to predict the label of new 2D or 3D points.
    Let's think about another problem. Imagine we have the data of temperatures for
    365 days in cities from all the continents in the world, and each vector of the
    365 temperature values is labeled with 1 for Asia, 2 for Europe, 3 for Africa,
    and so on. We can use this data to train an SVM model that can be used to predict
    the continent of new vectors of temperature values (for 365 days) and associate
    them with a label. Even though these examples might not be useful in practice,
    they describe the concept of SVMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，支持向量机（SVMs）用于从标记的训练样本集中创建一个模型，该模型可以用来预测新样本的标签。例如，假设我们有一组属于两个不同组的样本数据。我们训练数据集中的每个样本都是一个浮点数向量，它可以对应于任何东西，例如二维或三维空间中的一个简单点，并且每个样本都标记为一个数字，如1、2或3。有了这样的数据，我们可以训练一个SVM模型，用来预测新的二维或三维点的标签。让我们再考虑另一个问题。想象一下，我们有了来自世界所有大陆城市的365天的温度数据，365天的温度值向量被标记为1代表亚洲，2代表欧洲，3代表非洲等等。我们可以使用这些数据来训练一个SVM模型，用来预测新的温度值向量（365天）所属的大陆，并将它们与标签关联起来。尽管这些例子在实践上可能没有用，但它们描述了SVM的概念。
- en: 'We can use the `SVM` class in OpenCV to train and use SVM models. Let''s go
    through the usage of the `SVM` class in detail with a complete example:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用OpenCV中的 `SVM` 类来训练和使用SVM模型。让我们通过一个完整的示例详细说明 `SVM` 类的使用方法：
- en: 'Since machine learning algorithms in OpenCV are included under the `ml` namespace,
    we need to make sure we include those namespaces in our code, so that the classes
    within them are easily accessible, using the following code:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于OpenCV中的机器学习算法包含在 `ml` 命名空间下，我们需要确保在我们的代码中包含这些命名空间，以便其中的类可以轻松访问，以下是如何做到这一点的代码：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create the training dataset. As we mentioned before, the training dataset is
    a set of vectors (samples) of floating-point numbers, and each vector is labeled
    with the class ID or category of that vector. Let''s start with samples first:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练数据集。正如我们之前提到的，训练数据集是一组浮点数向量（样本）的集合，每个向量都被标记为该向量的类别ID或类别。让我们从样本开始：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, each sample in our dataset of eight samples contains two floating-point
    values that can be demonstrated using a point on an image with an *x* and *y*
    value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们八个样本的数据集中的每个样本包含两个浮点值，这些值可以用图像上的一个点来表示，该点具有 *x* 和 *y* 值。
- en: 'We also need to create the label (or response) data, which obviously must be
    the same length as the samples. Here it is:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要创建标签（或响应）数据，显然它必须与样本长度相同。以下是它：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, our samples are labeled with the `1` and `2` values, so we're
    expecting our model to be able to differentiate new samples between the given
    two groups of samples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们的样本被标记为 `1` 和 `2` 的值，因此我们期望我们的模型能够区分给定两组样本中的新样本。
- en: 'OpenCV uses the `TrainData` class to simplify the preparation and usage of
    the training dataset. Here''s how it''s used:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenCV 使用 `TrainData` 类来简化训练数据集的准备和使用。以下是它的使用方法：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`layout` in the preceding code is set to `ROW_SAMPLE` because each row in our
    dataset contains one sample. If the layout of the dataset was vertical, in other
    words, if each sample in the dataset was a column in the `samples` matrix, we''d
    need to set `layout` to `COL_SAMPLE`.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`layout` 被设置为 `ROW_SAMPLE`，因为我们的数据集中的每一行包含一个样本。如果数据集的布局是垂直的，换句话说，如果数据集中的每个样本是
    `samples` 矩阵中的一列，我们需要将 `layout` 设置为 `COL_SAMPLE`。
- en: 'Create the actual `SVM` class instance. This class in OpenCV implements various
    types of SVM classification algorithms, and they can be used by setting the correct
    parameters. In this example, we''re going to use the most basic (and common) set
    of parameters for the `SVM` class, but to be able to use all possible features
    of this algorithm, make sure to go through the OpenCV `SVM` class documentation
    pages. Here''s an example that shows how we can use SVM to perform a linear *n*-class
    classification:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建实际的`SVM`类实例。这个类在OpenCV中实现了各种类型的SVM分类算法，并且可以通过设置正确的参数来使用。在这个例子中，我们将使用`SVM`类最基本（也是最常见）的参数集，但要能够使用此算法的所有可能功能，请确保查阅OpenCV
    `SVM`类文档页面。以下是一个示例，展示了我们如何使用SVM执行线性*n*-类分类：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Train the SVM model using the `train` (or `trainAuto`) method, as seen here:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train`（或`trainAuto`）方法训练SVM模型，如下所示：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Based on the amount of data in our training samples dataset, the training process
    might take some time. In our case, it should be fast enough though, since we just
    used a handful of samples to train the model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们训练样本数据集中的数据量，训练过程可能需要一些时间。在我们的例子中，应该足够快，因为我们只是使用了一小部分样本来训练模型。
- en: 'We''re going to use the SVM model to actually predict the label of new samples.
    Remember that each sample in our training set was a 2D point in an image. We''re
    going to find the label of each 2D point in an image with a width and height of
    `300` pixels, and then color each pixel as green or blue, based on whether its
    predicted label is `1` or `2`. Here''s how:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用SVM模型来实际预测新样本的标签。记住，我们训练集中的每个样本都是一个图像中的2D点。我们将找到图像中宽度为`300`像素、高度为`300`像素的每个2D点的标签，然后根据其预测标签是`1`还是`2`，将每个像素着色为绿色或蓝色。以下是方法：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Go ahead and display the result of predictions, but, to be able to perfectly
    visualize the classification result of the SVM algorithm, it''s better to draw
    the training samples we used to create the SVM model. Let''s do it using the following
    code:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示预测结果，但要能够完美地可视化SVM算法的分类结果，最好绘制我们用来创建SVM模型的训练样本。让我们使用以下代码来完成它：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The two types of samples (`1` and `2`) are drawn as black and white circles
    over the resultant image. The following diagram depicts the result of the complete
    SVM classification we just performed:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 两种类型的样本（`1`和`2`）在结果图像上被绘制为黑色和白色的圆圈。以下图表展示了我们刚刚执行的完整SVM分类的结果：
- en: '![](img/00100.gif)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00100.gif)'
- en: This demonstration is quite simple and, in reality, SVM can be used for much
    more complex classification problems, however, it literally shows the most essential
    aspect of SVM, which is the separation of various groups of data that are labeled
    the same. As you can see in the preceding image, the line separating the blue
    region from the green region is the best single line that can most efficiently
    separate the black dots and white dots on the image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示非常简单，实际上，SVM可以用于更复杂的分类问题，然而，它实际上展示了SVM最基本的一个方面，即分离被标记为相同的数据组。正如您在前面的图像中可以看到，将蓝色区域与绿色区域分开的线是能够最有效地分离图像上黑色点和白色点的最佳单一线。
- en: 'You can experiment with this phenomenon by updating the labels, or, in other
    words, the responses in the preceding example, as seen here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过更新标签，或者换句话说，更新前一个示例中的响应来实验这种现象，如下所示：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Trying to visualize the results now will produce something similar to the following,
    which again depicts the most efficient line for separating the two groups of dots:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试可视化结果会产生类似以下内容，它再次描绘了分离两组点的最有效线：
- en: '![](img/00101.gif)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00101.gif)'
- en: 'You can very easily add more classes to your data, or, in other words, have
    more labels for your training sample set. Here''s an example:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以非常容易地向数据添加更多类别，或者换句话说，为您的训练样本集添加更多标签。以下是一个示例：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can try visualizing the results again by adding a yellow color, for instance,
    for the third class region, and a gray dot for training samples that belong to
    that class. Here''s the result of the same SVM example when used with three classes
    instead of two:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加黄色，例如，为第三类区域添加颜色，为属于该类的训练样本添加灰色点来再次尝试可视化结果。以下是使用三个类别而不是两个类别时相同SVM示例的结果：
- en: '![](img/00102.gif)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00102.gif)'
- en: If you recall the example of 365 days from before, it is quite obvious that
    we can also add more dimensionality to the SVM model and not just classes, but
    it wouldn't be visually possible to display the results with a simple image such
    as the one in the preceding example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回想起之前的365天示例，很明显我们也可以向SVM模型添加更多的维度，而不仅仅是类别，但使用像前面示例那样的简单图像来可视化结果将是不可能的。
- en: Before continuing with the usage of the SVM algorithm for actual object detection
    and image classification, it's worth noting that, just like any other machine
    learning algorithm, having more samples in your dataset will result in a much
    better classification and higher accuracy, but it will also take more time to
    train the model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续使用SVM算法进行实际目标检测和图像分类之前，值得注意的是，就像任何其他机器学习算法一样，数据集中样本数量的增加将导致分类效果更好、准确率更高，但也会使模型训练所需的时间更长。
- en: Classifying images using SVM and HOG
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM和HOG进行图像分类
- en: '**Histogram of Oriented Gradients** (**HOG**) is an algorithm that can be used
    to describe an image using a vector of floating-point descriptors that correspond
    to the oriented gradient values extracted from that image. The HOG algorithm is
    very popular and certainly worth reading about in detail to understand how it
    is implemented in OpenCV, but, for the purposes of this book and especially this
    section, we''ll just mention that the number of the floating-point descriptors
    will always be the same when they are extracted from images that have exactly
    the same size with the same HOG parameters. To better understand this, recall
    that descriptors extracted from an image using the feature detection algorithms
    we learned about in the previous chapter can have different numbers of elements
    in them. The HOG algorithm, though, will always produce a vector of the same length
    if the parameters are unchanged across a set of images of the same size.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**方向梯度直方图**（**HOG**）是一种算法，可以用来描述图像，使用从该图像中提取的对应于方向梯度值的浮点描述符向量。HOG算法非常流行，并且详细阅读它以了解其在OpenCV中的实现方法是非常有价值的，但出于本书和特别是本节的目的，我们只需提到，当从具有相同大小和相同HOG参数的图像中提取时，浮点描述符的数量始终相同。为了更好地理解这一点，请回忆一下，使用我们在上一章中学习的特征检测算法从图像中提取的描述符可能具有不同数量的元素。然而，HOG算法在参数不变的情况下，对于同一大小的图像集，总是会生成相同长度的向量。'
- en: 'This makes the HOG algorithm ideal for being used in conjunction with SVM,
    to train a model that can be used to classify images. Let''s see how it''s done
    with an example. Imagine we have a set of images that contain images of a traffic
    sign in one folder, and anything but that specific traffic sign in another folder.
    The following pictures depicts the images in our samples dataset, separated by
    a black line in between:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得HOG算法非常适合与SVM结合使用，以训练一个可以用于图像分类的模型。让我们通过一个例子来看看它是如何实现的。想象一下，我们有一组图像，其中包含一个文件夹中的交通标志图像，另一个文件夹中则包含除该特定交通标志之外的所有图像。以下图片展示了我们的样本数据集中的图像，它们之间用一条黑色线分隔：
- en: '![](img/00103.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00103.jpeg)'
- en: 'Using images similar to the preceding samples, we''re going to train the SVM
    model to detect whether an image is the traffic sign we''re looking for or not.
    Let''s start:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与前面样本相似的图像，我们将训练SVM模型以检测图像是否是我们正在寻找的交通标志。让我们开始吧：
- en: 'Create an `HOGDescriptor` object. `HOGDescriptor`, or the HOG algorithm, is
    a special type of descriptor algorithm that relies on a given window size, block
    size, and various other parameters; for the sake of simplicity, we''ll avoid all
    but the window size. The HOG algorithm''s window size in our example is `128`
    by `128` pixels, which is set as seen here:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`HOGDescriptor`对象。`HOGDescriptor`，或称HOG算法，是一种特殊的描述符算法，它依赖于给定的窗口大小、块大小以及各种其他参数；为了简化，我们将避免除窗口大小之外的所有参数。在我们的例子中，HOG算法的窗口大小是`128`像素乘以`128`像素，如下所示：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Sample images should have the same size as the window size, otherwise we need
    to use the `resize` function to make sure they are resized to the HOG window size
    later on. This guarantees the same descriptor size every time the HOG algorithm
    is used.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 样本图像应该与窗口大小相同，否则我们需要使用`resize`函数确保它们在后续操作中调整到HOG窗口大小。这保证了每次使用HOG算法时描述符大小的一致性。
- en: 'As we just mentioned, the vector length of the descriptor extracted using `HOGDescriptor`
    will be constant if the image size is constant, and, assuming that image has the
    same size as `winSize,` you can get the descriptor length using the following
    code:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们刚才提到的，如果图像大小是恒定的，那么使用 `HOGDescriptor` 提取的描述符的向量长度将是恒定的，并且假设图像大小与 `winSize`
    相同，你可以使用以下代码来获取描述符长度：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We'll use `descriptorSize` later on when we read the sample images.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在读取样本图像时使用 `descriptorSize`。
- en: 'Assuming the images of the traffic sign are inside a folder called `pos` (for
    positive) and the rest inside a folder called `neg` (for negative), we can use
    the `glob` function to get the list of image files in those folders, as seen here:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设交通标志的图像存储在一个名为 `pos`（表示正面）的文件夹中，其余的图像存储在一个名为 `neg`（表示负面）的文件夹中，我们可以使用 `glob`
    函数来获取这些文件夹中图像文件的列表，如下所示：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create buffers to store the HOG descriptors for negative and positive sample
    images (from `pos` and `neg` folders). We also need an additional buffer for the
    labels (or responses), as seen in the following example:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建缓冲区以存储来自 `pos` 和 `neg` 文件夹的正负样本图像的 HOG 描述符。我们还需要一个额外的缓冲区来存储标签（或响应），如下所示：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We need to use the `HOGDescriptor` class to extract the HOG descriptors from
    positive images and store them in `samples`, as seen here:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用 `HOGDescriptor` 类从正图像中提取 HOG 描述符并将它们存储在 `samples` 中，如下所示：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It needs to be noted that we have added `+1` for the labels (responses) of the
    positive samples. We'll need to use a different number, such as `-1`, when we
    label the negative samples.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们为正样本的标签（响应）添加了 `+1`。当我们对负样本进行标记时，我们需要使用不同的数字，例如 `-1`。
- en: 'After the positive samples, we add the negative samples and their responses
    to the designated buffers:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在正样本之后，我们将负样本及其响应添加到指定的缓冲区中：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Similar to the example from the previous section, we need to form a `TrainData`
    object using `samples` and `responses` to be used with the `train` function. Here''s
    how it''s done:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与上一节中的示例类似，我们需要使用 `samples` 和 `responses` 来形成一个 `TrainData` 对象，以便与 `train` 函数一起使用。以下是实现方式：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we need to train the SVM model as seen in the following example code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要按照以下示例代码训练 SVM 模型：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After the training is completed, the SVM model is ready to be used for classifying
    images with the same size as the HOG window size (in this case, `128` by `128`
    pixels) using the `predict` method of the  `SVM` class. Here is how:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，SVM 模型就准备好使用与 HOG 窗口大小相同的图像（在这种情况下，`128` x `128` 像素）进行分类了，使用 `SVM` 类的
    `predict` 方法。以下是操作方法：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we simply read an image and resize it to the HOG window
    size. Then we use the `compute` method of the `HOGDescriptor` class, just like
    when we were training the model. Except, this time, we use the `predict` method
    to find the label of this new image. If the `result` equals `+1`, which was the
    label we assigned for traffic sign images when we trained the SVM model, then
    we know that the image is the image of a traffic sign, otherwise it's not.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们简单地读取一个图像并将其调整到 HOG 窗口大小。然后我们使用 `HOGDescriptor` 类的 `compute` 方法，就像我们在训练模型时做的那样。但是，这次我们使用
    `predict` 方法来找到新图像的标签。如果 `result` 等于 `+1`，这是我们训练 SVM 模型时为交通标志图像分配的标签，那么我们知道该图像是交通标志的图像，否则不是。
- en: The accuracy of the result completely depends on the quantity and quality of
    the data you have used to train your SVM model. This, in fact, is the case for
    each and every machine learning algorithm. The more you train your model, the
    more accurate it becomes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的准确性完全取决于你用于训练 SVM 模型的数据的数量和质量。实际上，每个机器学习算法都是如此。你训练模型越多，它就越准确。
- en: This method of classification assumes that the input image is of the same characteristics
    as the trained images. Meaning, if the image contains a traffic sign, it is cropped
    similarly to the images we used to train the model. For instance, if you use an
    image that contains the traffic sign image we're looking for, but also contain
    much more, then the result will probably be incorrect.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分类方法假设输入图像与训练图像具有相同的特征。这意味着，如果图像包含交通标志，它将被裁剪得与用于训练模型的图像相似。例如，如果你使用包含我们正在寻找的交通标志图像的图像，但包含得更多，那么结果可能是不正确的。
- en: 'As the amount of data in your training set increases, it will take more time
    to train your model. So, it''s important to avoid retraining your model every
    time you want to use it. The `SVM` class allows you to save and load SVM models
    using the `save` and `load` methods. Here is how you can save a trained SVM model
    for later use and to avoid retraining it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练集中数据量的增加，训练模型将需要更多时间。因此，每次你想使用模型时避免重新训练模型是很重要的。`SVM`类允许你使用`save`和`load`方法保存和加载SVM模型。以下是如何保存训练好的SVM模型以供以后使用并避免重新训练的方法：
- en: '[PRE19]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The file will be saved using the provided filename and extension (XML or any
    other  file type supported by OpenCV). Later, using the static `load` function,
    you can create an SVM object that contains the exact parameters and trained model.
    Here''s an example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 文件将使用提供的文件名和扩展名（XML或OpenCV支持的任何其他文件类型）保存。稍后，使用静态`load`函数，你可以创建一个包含确切参数和训练模型的SVM对象。以下是一个示例：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Try using the `SVM` class along with `HOGDescriptor` to train models that can
    detect and classify more types using images of various objects stored in different
    folders.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用`SVM`类和`HOGDescriptor`来训练模型，这些模型可以使用存储在不同文件夹中的各种对象的图像检测和分类更多类型。
- en: Training models with artificial neural networks
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用人工神经网络训练模型
- en: ANN can be used to train a model using a set of sample input and output vectors.
    ANN is a highly popular machine learning algorithm and the basis of many modern
    artificial intelligence algorithms that are used to train models for classification
    and correlation. Especially in computer vision, the ANN algorithm can be used
    along with a wide range of feature-description algorithms to learn about images
    of objects, or even faces of different people, and then used to detect them in
    images.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ANN可以使用一组样本输入和输出向量来训练模型。ANN是一种高度流行的机器学习算法，是许多现代人工智能算法的基础，这些算法用于训练用于分类和关联的模型。特别是在计算机视觉中，ANN算法可以与广泛的特征描述算法一起使用，以了解物体的图像，甚至不同人的面部，然后用于在图像中检测它们。
- en: You can use the `ANN_MLP` class (which stands for **artificial neural networks—****multi-layer** **perceptron**)
    in OpenCV to implement ANN in your applications. The usage of this class is quite
    similar to that of the `SVM` class, so we're going to give a simple example to
    learn the differences and how it's used in practice, and we'll leave the rest
    for you to discover by yourself.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用OpenCV中的`ANN_MLP`类（代表**人工神经网络——多层感知器**）在你的应用程序中实现ANN。这个类的使用方法与`SVM`类非常相似，所以我们将给出一个简单的示例来学习差异以及它在实际中的应用，其余的我们将留给你自己探索。
- en: 'Creating the training samples dataset is exactly the same for all machine learning
    algorithms in OpenCV, or, to be precise, for all subclasses of the `StatsModel`
    class. The `ANN_MLP` class is no exception to this, so, just like with the `SVM`
    class, first we need to create a `TrainData` object that contains all the sample
    and response data that we need to use when training our ANN model, as seen here:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中，创建训练样本数据集对所有机器学习算法都是一样的，或者更准确地说，对所有`StatsModel`类的子类都是这样。`ANN_MLP`类也不例外，因此，就像`SVM`类一样，我们首先需要创建一个`TrainData`对象，该对象包含我们在训练我们的ANN模型时需要使用的所有样本和响应数据，如下所示：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`samples` and `responses`, in the preceding code, are both `Mat` objects that
    contain a number of rows that equals the number of all the training data we have
    in our dataset. As for the number of columns in them, let''s recall that the ANN
    algorithm can be used to learn the relationship between vectors of input and output
    data. This means that the number of columns in the training input data, or `samples`,
    can be different from the number of columns in the training output data, or `responses`.
    We''ll refer to the number of columns in `samples` as the number of features,
    and to the number of columns in `responses` as the number of classes. Simply put,
    we''re going to learn the relationship of features to classes using a training
    dataset.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`samples`和`responses`都是`Mat`对象，它们包含的行数等于我们数据集中所有训练数据的数量。至于它们的列数，让我们回忆一下，ANN算法可以用来学习输入和输出数据向量之间的关系。这意味着训练输入数据（或`samples`）中的列数可以不同于训练输出数据（或`responses`）中的列数。我们将`samples`中的列数称为特征数，将`responses`中的列数称为类别数。简单来说，我们将使用训练数据集来学习特征与类别之间的关系。
- en: 'After taking care of the training dataset, we need to create an `ANN_MLP` object
    using the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完训练数据集之后，我们需要使用以下代码创建一个`ANN_MLP`对象：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We have skipped all the customizations and used the default set of parameters.
    In the case that you need to use a fully customized `ANN_MLP` object, you need
    to set the activation function, termination criteria, and various other parameters
    in the `ANN_MLP` class. To learn more about this, make sure to refer to the OpenCV
    documentation and online resources about artificial neural networks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们跳过了所有自定义设置，使用了默认参数集。如果您需要使用完全自定义的`ANN_MLP`对象，您需要在`ANN_MLP`类中设置激活函数、终止标准以及各种其他参数。要了解更多信息，请确保参考OpenCV文档和关于人工神经网络的网络资源。
- en: 'Setting the correct layer sizes in the ANN algorithm requires experience and
    depends on the use case, but it can also be set using a few trial-and-error sessions.
    Here''s how you can set the number and size of each layer in the ANN algorithm,
    and the `ANN_MLP` class to be specific:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络（ANN）算法中设置正确的层大小需要经验和依赖具体的使用场景，但也可以通过几次试错来设置。以下是您如何设置ANN算法中每一层的数量和大小，特别是`ANN_MLP`类：
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, the number of rows in the `layers` object refers to the
    number of layers we want to have in our ANN. The first element in the `layers`
    object should contain the number of features in our dataset, and the last element
    in the `layers` object should contain the number of classes. Recall that the number
    of features equals the column count of `samples`, and the number of classes equals
    the column count of `responses`. The rest of the elements in the `layers` object
    contain the sizes of hidden layers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`layers`对象中的行数表示我们希望在ANN中拥有的层数。`layers`对象中的第一个元素应包含数据集中的特征数量，而`layers`对象中的最后一个元素应包含类的数量。回想一下，特征的数量等于`samples`的列数，类的数量等于`responses`的列数。`layers`对象中的其余元素包含隐藏层的尺寸。
- en: 'Training the ANN model is done by using the `train` method, as seen in the
    following example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`train`方法来训练ANN模型，如下面的示例所示：
- en: '[PRE24]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After the training is completed, we can use the `save` and `load` methods in
    exactly the same way as we saw before, to save the model for later use, or reload
    it from a saved file.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以像之前看到的那样使用`save`和`load`方法，以保存模型供以后使用，或从保存的文件中重新加载它。
- en: 'Using the model with the `ANN_MLP` class is also quite similar to the `SVM`
    class. Here''s an example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ANN_MLP`类与`SVM`类类似。以下是一个示例：
- en: '[PRE25]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Choosing the right machine learning algorithm for each problem requires experience
    and knowledge about where the project is going to be used. SVM is quite simple,
    and suitable when we need to work with the classification of data and in the segmentation
    of groups of similar data, whereas ANN can be easily used to approximate a function
    between sets of input and output vectors (regression). Make sure to try out different
    machine learning problems to better understand where and when to use a specific
    algorithm.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个问题选择合适的机器学习算法需要经验和知识，了解项目将如何被使用。支持向量机（SVM）相当简单，适用于我们需要对数据进行分类以及在相似数据组的分割中，而人工神经网络（ANN）可以很容易地用来近似输入和输出向量集之间的函数（回归）。确保尝试不同的机器学习问题，以更好地理解何时以及在哪里使用特定的算法。
- en: The cascading classification algorithm
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 级联分类算法
- en: Cascading classification is another machine learning algorithm that can be used
    to train a model from many (hundreds, or even thousands) positive and negative
    image samples. As we explained earlier, a positive image refers to the image in
    an object of interest (such as a face, a car, or a traffic signal) that we want
    our model to learn and later classify or detect. On the other hand, a negative
    image corresponds to any arbitrary image that does not contain our object of interest.
    The model trained using this algorithm is referred to as a cascade classifier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 级联分类是另一种机器学习算法，可以用来从许多（数百甚至数千）正负图像样本中训练模型。正如我们之前解释的，正图像指的是我们感兴趣的对象（如人脸、汽车或交通信号）中的图像，我们希望我们的模型学习并随后进行分类或检测。另一方面，负图像对应于任何不包含我们感兴趣对象的任意图像。使用此算法训练的模型被称为级联分类器。
- en: The most important aspect of a cascade classifier, as can be guessed from its
    name, is its cascading nature of learning and detecting an object using the extracted
    features. The most widely used features in cascade classifiers, and consequently
    cascade classifier types, are Haar and **local binary pattern** (**LBP**). In
    this section, we're going to learn how to use existing OpenCV Haar and LBP cascade
    classifiers to detect faces, eyes, and more in real-time, and then learn how to
    train our own cascade classifiers to detect any other objects.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 级联分类器最重要的方面，正如其名称所暗示的，是其学习检测对象的级联性质，使用提取的特征。在级联分类器中最广泛使用的特征，以及相应的级联分类器类型，是Haar和**局部二进制模式**（**LBP**）。在本节中，我们将学习如何使用现有的OpenCV
    Haar和LBP级联分类器在实时中检测面部、眼睛等，然后学习如何训练我们自己的级联分类器以检测任何其他对象。
- en: Object detection using cascade classifiers
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用级联分类器进行目标检测
- en: 'To be able to use previously trained cascade classifiers in OpenCV, you can
    use the `CascadeClassifier` class and the simple methods it provides for loading
    a classifier from file or performing scale-invariant detection in images. OpenCV
    contains a number of trained classifiers to detect faces, eyes, and so on in real-time.
    If we browse to the OpenCV installation (or build) folder, it usually contains
    a folder called `etc`, which contains the following subfolders:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要在OpenCV中使用先前训练的级联分类器，你可以使用`CascadeClassifier`类及其提供用于从文件加载分类器或执行图像中的尺度不变检测的简单方法。OpenCV包含许多用于实时检测面部、眼睛等对象的预训练分类器。如果我们浏览到OpenCV的安装（或构建）文件夹，它通常包含一个名为`etc`的文件夹，其中包含以下子文件夹：
- en: '`haarcascades`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`haarcascades`'
- en: '`lbpcascades`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lbpcascades`'
- en: '`haarcascades` contains pre-trained Haar cascade classifiers. `lbpcascades`,
    on the other hand, contains pre-trained LBP cascade classifiers. Haar cascade
    classifiers are usually slower than LBP cascade classifiers, but they also provide
    much better accuracy in most cases. To learn about the details of Haar and LBP
    cascade classifies, make sure to refer to the OpenCV documentation as well as
    online resources about Haar wavelets, Haar-like features, and local binary patterns.
    As we''ll learn in the next section, LBP cascade classifiers are also a lot faster
    to train than Haar classifiers; with enough training data samples, you can reach
    a similar accuracy for both of the classifier types.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`haarcascades`包含预训练的Haar级联分类器。另一方面，`lbpcascades`包含预训练的LBP级联分类器。与LBP级联分类器相比，Haar级联分类器通常速度较慢，但在大多数情况下也提供了更好的准确性。要了解Haar和LBP级联分类器的详细信息，请务必参考OpenCV文档以及关于Haar小波、Haar-like特征和局部二进制模式的相关在线资源。正如我们将在下一节中学习的，LBP级联分类器的训练速度也比Haar分类器快得多；只要有足够的训练数据样本，你就可以达到这两种分类器类型相似的准确性。'
- en: 'Under each one of the classifier folders we just mentioned, you can find a
    number of pre-trained cascade classifiers. You can load these classifiers and
    prepare them for object detection in real-time using the `load` method of the `CascadeClassifier`
    class, as seen in the following example:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚才提到的每个分类器文件夹下，你可以找到许多预训练的级联分类器。你可以使用`CascadeClassifier`类的`load`方法加载这些分类器，并准备它们进行实时目标检测，如下面的示例所示：
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After a cascade classifier is successfully loaded, you can use the `detectMultiScale`
    method to detect objects in an image and return a vector containing the bounding
    rectangles of the detected objects, as seen in the following example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功加载级联分类器之后，你可以使用`detectMultiScale`方法在图像中检测对象，并返回一个包含检测到的对象边界框的向量，如下面的示例所示：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`color` and `thickness` are previously defined to affect the rectangle drawn
    for each detected object, as seen here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`color`和`thickness`之前已定义，用于影响为每个检测到的对象绘制的矩形，如下所示：'
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Try loading the `haarcascade_frontalface_default.xml` classifier in the `haarcascades`
    folder, which comes preinstalled with OpenCV, to test the preceding example. Trying
    to run the preceding code with an image that contains a face would result in something
    similar to this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试加载`haarcascade_frontalface_default.xml`分类器，它位于`haarcascades`文件夹中，这是OpenCV预安装的，以测试前面的示例。尝试使用包含面部图像的图像运行前面的代码，结果将类似于以下内容：
- en: '![](img/00104.gif)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00104.gif)'
- en: 'The accuracy of the cascade classifier, as with any other machine learning
    model, depends completely on the quality and quantity of the training samples
    dataset. As it was mentioned before, cascade classifiers are widely popular, especially
    for real-time object detection. To be able to view the performance of cascade
    classifiers on any computer, you can use the following code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 级联分类器的准确度，就像任何其他机器学习模型一样，完全取决于训练样本数据集的质量和数量。正如之前提到的，级联分类器在实时对象检测中非常受欢迎。为了能够在任何计算机上查看级联分类器的性能，你可以使用以下代码：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The last line in the preceding code is used to convert the unit of the time
    measurement from seconds to milliseconds. You can use the following code to print
    out the result over the output image, in the lower-left corner for example:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的最后一行用于将时间测量的单位从秒转换为毫秒。你可以使用以下代码在输出图像上打印结果，例如在左下角：
- en: '[PRE30]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will produce an output image that contains text similar to what is seen
    in the following example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个包含类似以下示例中的文本的输出图像：
- en: '![](img/00105.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00105.jpeg)'
- en: Try different pre-trained cascade classifiers shipped with OpenCV and check
    their performance against each other. One very obvious observation will be the
    significantly faster detection speed of LBP cascade classifiers.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用OpenCV附带的不同预训练级联分类器，并检查它们之间的性能。一个非常明显的观察结果是LBP级联分类器的检测速度显著更快。
- en: 'In the previous examples, we used only the default set of parameters needed
    for the `detectMultiScale` method of the `CascadeClassifier` class, however, to
    modify its behavior and, in some cases, to significantly improve its performance,
    you''ll need to adjust a few more parameters, as seen in the following example:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们只使用了`CascadeClassifier`类中`detectMultiScale`方法所需的默认参数集，然而，为了修改其行为，以及在某些情况下显著提高其性能，你将需要调整更多一些参数，如下面的示例所示：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `scaleFactor` parameter is used to specify the scaling of the image after
    each detection. This means resizing the image and performing the detection internally.
    This is in fact how multi-scale detection algorithms work. An image is searched
    for in an object, its size is reduced by the given `scaleFactor`, and the search
    is performed again. Size reduction is performed repeatedly until the image size
    is smaller than the classifier size. The results from all detections in all scales
    are then returned. The `scaleFactor` parameter must always contain a value greater
    than 1.0 (not equal to and not lower than). For higher sensitivity in multi-scale
    detection, you can set a value such as 1.01 or 1.05, which will lead to much longer
    detection times, and vice versa. The `minNeighbors` parameter refers to the grouping
    of detections that are near or similar to each other to retain a detected `object`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`scaleFactor`参数用于指定每次检测后图像的缩放。这意味着内部对图像进行缩放并执行检测。这实际上就是多尺度检测算法的工作方式。在对象中搜索图像，其大小通过给定的`scaleFactor`减小，然后再次进行搜索。重复进行尺寸减小，直到图像大小小于分类器大小。然后返回所有尺度中所有检测的结果。`scaleFactor`参数必须始终包含一个大于1.0的值（不等于且不低于）。为了在多尺度检测中获得更高的灵敏度，你可以设置一个值，如1.01或1.05，这将导致检测时间更长，反之亦然。`minNeighbors`参数指的是将彼此靠近或相似的检测分组以保留检测到的`对象`。'
- en: The `flags` parameter is simply ignored in recent versions of OpenCV. As for
    the `minSize` and `maxSize` parameters, they are used to specify the minimum and
    maximum possible sizes of an object in an image. This can significantly increase
    the accuracy and speed of the `detectMultiScale` function, since detected objects
    that do not fall into the given size range are simply ignored and rescaling is
    done only until `minSize` is reached.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV的较新版本中，`flags`参数被简单地忽略。至于`minSize`和`maxSize`参数，它们用于指定图像中对象可能的最小和最大尺寸。这可以显著提高`detectMultiScale`函数的准确性和速度，因为不在给定尺寸范围内的检测到的对象将被简单地忽略，并且仅重新缩放直到达到`minSize`。
- en: '`detectMultiScale` has two other variations that we skipped for the sake of
    simplifying the examples, but you should check them out for yourself to learn
    more about cascade classifiers and multi-scale detection in general. Make sure
    to also search online for pre-trained classifiers by fellow computer vision developers
    and try using them in your applications.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`detectMultiScale`还有两个其他变体，我们为了简化示例而跳过了，但你应该亲自检查它们，以了解更多关于级联分类器和多尺度检测的信息。确保还要在网上搜索其他计算机视觉开发者提供的预训练分类器，并尝试将它们用于你的应用程序中。'
- en: Training cascade classifiers
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练级联分类器
- en: As we mentioned previously, you can also create your own cascade classifiers
    to detect any other object if you have enough positive and negative sample images.
    Training a classifier using OpenCV involves taking a number of steps and using
    a number of special OpenCV applications, which we'll go through in this section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，如果你有足够的正负样本图像，你也可以创建自己的级联分类器来检测任何其他对象。使用OpenCV训练分类器涉及多个步骤和多个特殊的OpenCV应用程序，我们将在本节中介绍这些内容。
- en: Creating samples
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建样本
- en: First things first, you need a tool called `opencv_createsamples` to prepare
    the positive image sample set. The negative image samples, on the other hand,
    are extracted automatically during the training from a provided folder containing
    arbitrary images that do NOT include the object of interest. The `opencv_createsamples`
    application can be found inside the `bin` folder of the OpenCV installation. It
    can be used to create the positive samples dataset, either by using a single image
    of the object of interest and applying distortions and transformations to it,
    or by using previously cropped or annotated images of the object of interest.
    Let's learn about the former case first.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要一个名为`opencv_createsamples`的工具来准备正图像样本集。另一方面，负图像样本在训练过程中自动从包含任意图像的提供的文件夹中提取，这些图像不包含感兴趣的对象。`opencv_createsamples`应用程序可以在OpenCV安装的`bin`文件夹中找到。它可以用来创建正样本数据集，要么使用感兴趣对象的单个图像并对其应用扭曲和变换，要么使用之前裁剪或注释的感兴趣对象的图像。让我们首先了解前一种情况。
- en: 'Imagine you have the following image of a traffic sign (or any other object,
    for that matter) and you want to create a positive sample dataset using it:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个交通标志（或任何其他对象）的以下图像，并且你想使用它创建一个正样本数据集：
- en: '![](img/00106.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](img/00106.jpeg)'
- en: 'You should also have a folder containing the source of the negative samples.
    As we mentioned previously, you need to have a folder containing arbitrary images
    that do not contain the object of interest. Let''s assume we have some images
    similar to the following that we''ll be using to create negative samples from:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该有一个包含负样本源的文件夹。如我们之前提到的，你需要一个包含任意图像的文件夹，这些图像不包含感兴趣的对象。让我们假设我们有一些类似于以下图像，我们将使用这些图像来创建负样本：
- en: '![](img/00107.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](img/00107.jpeg)'
- en: Note that the size and aspect ratio of negative images, or to use the correct
    terminology, the background images, is not at all important. However, they must
    be at least as big as the minimum-detectable object (classifier size) and they
    must never contain images of the object of interest.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，负图像的大小和宽高比，或者使用正确的术语，背景图像的大小，并不重要。然而，它们必须至少与最小可检测对象（分类器大小）一样大，并且它们绝不能包含感兴趣对象的图像。
- en: 'To train a proper cascade classifier, sometimes you need hundreds or even thousands
    of sample images that are distorted in different ways, which is not easy to create.
    In fact, gathering training data is one of the most time-consuming steps in creating
    a cascade classifier. The `opencv_createsamples` application can help with this
    problem by taking in the previous image of the object we''re creating a classifier
    for and producing a positive samples dataset by applying distortions and using
    the background images. Here''s an example of how it''s used:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个合适的级联分类器，有时你需要数百甚至数千个以不同方式扭曲的样本图像，这并不容易创建。实际上，收集训练数据是创建级联分类器中最耗时的步骤之一。`opencv_createsamples`应用程序可以通过对创建分类器的对象的前一个图像应用扭曲和使用背景图像来生成正样本数据集，从而帮助解决这个问题。以下是如何使用它的一个示例：
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is a description of the parameters used in the preceding command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面命令中使用的参数描述：
- en: '`vec` is used to specify the positive samples file that will be created. In
    this case, it is the `samples.vec` file.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vec`用于指定要创建的正样本文件。在这种情况下，它是`samples.vec`文件。'
- en: '`img` is used to specify the input image that will be used to generate the
    samples. In our case, it''s `sign.png`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`img`用于指定用于生成样本的输入图像。在我们的例子中，它是`sign.png`。'
- en: '`bg` is used to specify the background''s description file. A background''s
    description file is a simple text file that contains the paths to all background
    images (each line in the background''s description file contains the path to one
    background image). We have created a file named `bg.txt` and provided it to the `bg`
    parameter.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bg` 用于指定背景的描述文件。背景的描述文件是一个简单的文本文件，其中包含所有背景图像的路径（背景描述文件中的每一行包含一个背景图像的路径）。我们创建了一个名为
    `bg.txt` 的文件，并将其提供给 `bg` 参数。'
- en: The `num` parameter determines the number of positive samples you want to generate
    using the given input image and backgrounds; 250, in our case. You can, but of
    course, use a higher or lower number, depending on the accuracy and duration of
    training that you require.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num` 参数确定您想使用给定的输入图像和背景生成的正样本数量；在我们的例子中是 250。当然，您可以使用更高的或更低的数字，这取决于您所需的准确性和训练时间。'
- en: '`bgcolor` can be used to define the background color in terms of its grayscale
    intensity. As you can see in our input image (the traffic sign image), the background
    color is black, thus the value of this parameter is zero.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bgcolor` 可以用来用灰度强度定义背景颜色。正如您可以在我们的输入图像（交通标志图像）中看到的那样，背景颜色是黑色，因此此参数的值为零。'
- en: The `bgthresh` parameter specifies the threshold of the accepted `bgcolor` parameter.
    This is especially useful in the case of compression artifacts that are common
    to some image formats and might cause slightly different pixel values for the
    same color. We have used 10 for the value of this parameter to allow a slight
    level of tolerance for background pixels.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bgthresh` 参数指定了接受的 `bgcolor` 参数的阈值。这在处理某些图像格式中常见的压缩伪影的情况下特别有用，可能会造成相同颜色略有不同的像素值。我们为这个参数使用了
    10 的值，以允许对背景像素的一定程度的容忍度。'
- en: '`maxidev` can be used to set the maximum intensity deviation of the foreground
    pixel values while generating the samples. A value of 50 means the intensity of
    the foreground pixels can vary between their original values +/- 50.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxidev` 可以用来设置在生成样本时前景像素值的最大强度偏差。值为 50 表示前景像素的强度可以在其原始值 +/- 50 之间变化。'
- en: '`maxxangle`, `maxyangle`, and `maxzangle` correspond to the maximum possible
    rotation allowed in the *x*, *y*, and *z* directions when creating new samples.
    These values are in radians, for which we have provided 0.7, 0.7, and 0.5.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxxangle`, `maxyangle`, 和 `maxzangle` 分别对应在创建新样本时在 *x*、*y* 和 *z* 方向上允许的最大旋转角度。这些值以弧度为单位，我们提供了
    0.7、0.7 和 0.5。'
- en: The `w` and `h` parameters define the width and height of the samples. We have
    used 32 for both of them since the object we're looking to train a classifier
    for fits in a square shape. These same values will be used later on, when training
    the classifier. Also note that this will be the minimum detectable size in your
    trained classifier later on.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w` 和 `h` 参数定义了样本的宽度和高度。我们为它们都使用了 32，因为我们想要训练分类器的对象适合正方形形状。这些相同的值将在稍后训练分类器时使用。此外，请注意，这将是您训练的分类器中可以检测到的最小尺寸。'
- en: Besides the parameters in the preceding list, the `opencv_createsamples` application
    also accepts a `show` parameter that can be used to display the created samples,
    an `inv` parameter that can be used to invert the colors of samples, and a `randinv` parameter
    that can be used to set or unset the random inversion of pixels in samples.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面列表中的参数外，`opencv_createsamples` 应用程序还接受一个 `show` 参数，可以用来显示创建的样本，一个 `inv`
    参数可以用来反转样本的颜色，以及一个 `randinv` 参数可以用来设置或取消样本中像素的随机反转。
- en: 'Running the preceding command will produce the given number of samples by performing
    rotations and intensity changes to the foreground pixels. Here are some of the
    resultant samples:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的命令将通过旋转和强度变化对前景像素进行操作，从而生成指定数量的样本。以下是一些生成的样本：
- en: '![](img/00108.gif)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.gif)'
- en: Now that we have a positive samples vector file, produced by `opencv_createsamples`,
    and a folder that contains the background images along with a background's description
    file (`bg.txt` from the previous example), we can start the training of our cascade
    classifier, but before that, let's also learn about the second method of creating
    our positive samples vector, which is by extracting them from various annotated
    images that contain our object of interest.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了由 `opencv_createsamples` 生成的正样本向量文件，以及包含背景图像和背景描述文件（前一个示例中的 `bg.txt`）的文件夹，我们可以开始训练我们的级联分类器了。但在那之前，让我们也了解一下创建正样本向量的第二种方法，即从包含我们感兴趣对象的各个标注图像中提取它们。
- en: This second method involves using another official OpenCV tool which is used
    for annotating positive samples in images. This tool is called `opencv_annotation`
    and it can be used to conveniently mark the areas in a number of images that contain
    our positive samples, or in other words, the objects, we're going to train a cascade
    classifier for them. The `opencv_annotation` tool produces an annotation text
    file (after a manual annotation of the objects) that can be used with the `opencv_createsamples`
    tool to produce a positive samples vector suitable for use with the OpenCV cascade
    training tool that we'll learn about in the next section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法涉及使用另一个官方 OpenCV 工具，该工具用于在图像中注释正样本。这个工具被称为 `opencv_annotation`，它可以方便地标记包含我们的正样本（换句话说，即我们打算为它们训练级联分类器的对象）的多个图像中的区域。`opencv_annotation`
    工具在手动注释对象后生成一个注释文本文件，可以用 `opencv_createsamples` 工具生成适合与 OpenCV 级联训练工具一起使用的正样本向量，我们将在下一节中学习该工具。
- en: 'Let''s assume we have a folder containing images similar to the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含类似以下图片的文件夹：
- en: '![](img/00109.jpeg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00109.jpeg)'
- en: 'All of these images are located in a single folder and all of them contain
    one or more samples of the traffic sign (the object of interest) that we''re looking
    for. We can use the following command to start the `opencv_annotation` tool and
    manually annotate the samples:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些图像都位于一个文件夹中，并且它们都包含我们正在寻找的交通标志（感兴趣的对象）的一个或多个样本。我们可以使用以下命令启动 `opencv_annotation`
    工具并手动注释样本：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the preceding command, `imgpath` must be replaced with the path (preferably
    the absolute path and with forward slashes) to the folder containing the images.
    `anno.txt`, or any other file name provided instead, will be filled with the annotation
    results, which can be used with `opencv_createsamples` to create a positive samples
    vector. Executing the preceding command will start the `opencv_annotation` tool
    and output the following text, which describes how to use the tool and its shortcut
    keys:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，`imgpath` 必须替换为包含图片的文件夹路径（最好是绝对路径，并使用正斜杠）。`anno.txt` 或任何其他提供的文件名将被填充注释结果，这些结果可以用
    `opencv_createsamples` 生成正样本向量。执行前面的命令将启动 `opencv_annotation` 工具并输出以下文本，描述如何使用该工具及其快捷键：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Immediately after the preceding output, a window similar to the following will
    be displayed:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出之后，将显示一个类似于以下窗口：
- en: '![](img/00110.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00110.jpeg)'
- en: You can highlight an object using your mouse's left button, which will cause
    a red rectangle to be drawn. Pressing the *C* key will finalize the annotation
    and it will become red. Continue this process for the rest of the samples (if
    any) in the same image and press *N* to go to the next image. After all images
    are annotated, you can exit the application by pressing the *Esc* key.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用鼠标左键突出显示一个对象，这将导致绘制一个红色矩形。按下 *C* 键将完成注释，它将变成红色。继续对同一图像中的其余样本（如果有）进行此过程，然后按
    *N* 键转到下一图像。在所有图像都注释完毕后，你可以通过按 *Esc* 键退出应用程序。
- en: In addition to the `-images` and `-annotations` parameters, the `opencv_annotation`
    tool also includes an optional parameter, called `-maxWindowHeight`, that can
    be used to resize images that are bigger than a given size. The resize factor
    in this case can be specified with another optional parameter called `-resizeFactor`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `-images` 和 `-annotations` 参数之外，`opencv_annotation` 工具还包括一个可选参数，称为 `-maxWindowHeight`，可以用来调整大于指定尺寸的图片大小。在这种情况下，调整因子可以通过另一个名为
    `-resizeFactor` 的可选参数来指定。
- en: 'The annotation file created by the `opencv_annotation` tool will look like
    the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由 `opencv_annotation` 工具创建的注释文件将看起来像以下这样：
- en: '[PRE35]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Each line in the annotations file contains the path to an image, followed by
    the number of objects of interest in that image followed by the *x*, *y*, width,
    and height values of the bounding rectangles of those objects. You can use the
    following command to produce a samples vector using this annotation text file:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注释文件中的每一行都包含一个图像的路径，后面跟着该图像中感兴趣对象的数量，然后是这些对象的边界框的 *x*、*y*、宽度和高度值。你可以使用以下命令使用这个注释文本文件生成样本向量：
- en: '[PRE36]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that this time we used the `opencv_createsamples` tool with the `-info`
    parameter, which wasn't present when we used this tool to generate samples from
    an image and arbitrary backgrounds. We are now ready to train a cascade classifier
    that is capable of detecting the traffic sign we created the samples for.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这次我们使用了带有 `-info` 参数的 `opencv_createsamples` 工具，而当我们使用这个工具从图像和任意背景中生成样本时，这个参数是不存在的。我们现在已经准备好训练一个能够检测我们创建的样本的交通标志的分类器。
- en: Creating the classifier
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建分类器
- en: 'The last tool we''re going to learn about is called `opencv_traincascade`,
    which, as you can guess, is used to train cascade classifiers. If you have enough
    samples and background images, and if you have already taken care of the samples
    vector as it was described in the preceding section, then the only thing you need
    to do is to run the `opencv_traincascade` tool and wait for the training to be
    completed. Let''s see an example training command and then go through the parameters
    in detail:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要学习的最后一个工具叫做 `opencv_traincascade`，正如你可以猜到的，它用于训练级联分类器。如果你有足够的样本和背景图像，并且如果你已经按照前述章节描述的那样处理了样本向量，那么你唯一需要做的就是运行
    `opencv_traincascade` 工具并等待训练完成。让我们看看一个示例训练命令，然后详细说明参数：
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is the simplest way of starting the training process, and only uses the
    mandatory parameters. All parameters used in this command are self-explanatory,
    except the `-data` parameter, which must be an existing folder that will be used
    to create the files required during the training process and the final trained
    classifier (called `cascade.xml`) will be created in this folder.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的开始训练过程的方式，并且只使用必须的参数。在这个命令中使用的所有参数都是自解释的，除了 `-data` 参数，它必须是一个现有的文件夹，该文件夹将用于在训练过程中创建所需的文件，并且最终训练好的分类器（称为
    `cascade.xml`）将在这个文件夹中创建。
- en: '`numPos` cannot contain a number higher than the number of positive samples
    in your `samples.vec` file, however, `numNeg` can contain basically any number
    since the training process, will simply try to create random negative samples
    by extracting portions of the provided background images.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`numPos` 不能包含高于你的 `samples.vec` 文件中正样本数量的数字，然而，`numNeg` 可以包含基本上任何数字，因为训练过程将简单地尝试通过提取提供的背景图像的部分来创建随机负样本。'
- en: 'The `opencv_traincascade` tool will create a number of XML files in the folder
    set as the `-data` parameter, which must not be modified until the training process
    is completed. Here is a short description for each one of them:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`opencv_traincascade` 工具将在设置为 `-data` 参数的文件夹中创建多个 XML 文件，这个文件夹在训练过程完成之前不得修改。以下是每个文件的简要描述：'
- en: The `params.xml` file will contain the parameters used for training the classifier.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params.xml` 文件将包含用于训练分类器的参数。'
- en: '`stage#.xml` files are checkpoints that are created after each training stage
    is completed. They then can be used to resume the training later on if the training
    process was terminated for an unexpected reason.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stage#.xml` 文件是在每个训练阶段完成后创建的检查点。如果训练过程因意外原因而终止，可以使用它们稍后继续训练。'
- en: The `cascade.xml` file is the trained classifier and the last file that will
    be created by the training tool. You can copy this file, rename it to something
    convenient (such as `trsign_classifier.xml` or something like that), and use it
    with the `CascadeClassifier` class, as we learned in the previous sections, to
    perform multi-scale object detection.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cascade.xml` 文件是训练好的分类器，并且是训练工具最后创建的文件。你可以复制这个文件，将其重命名为方便的名字（例如 `trsign_classifier.xml`
    或类似的名字），然后使用我们之前章节中学到的 `CascadeClassifier` 类，来执行多尺度目标检测。'
- en: '`opencv_traincascade` is an extremely customizable and flexible tool, and you
    can easily modify its many optional parameters to make sure the trained classifier
    fits your needs. Here is a description of some of its most used parameters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`opencv_traincascade` 是一个极其可定制和灵活的工具，你可以轻松修改其许多可选参数，以确保训练好的分类器符合你的需求。以下是其中一些最常用参数的描述：'
- en: '`numStages` can be used to set the number of stages used to train the cascade
    classifier. By default, `numStages` equals 20, but you can decrease this value
    to shorten the training time while sacrificing the accuracy or vice versa.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `numStages` 来设置用于训练级联分类器的阶段数。默认情况下，`numStages` 等于 20，但你可以减小这个值以缩短训练时间，同时牺牲准确性，或者相反。
- en: The `precalcValBufSize` and `precalcIdxBufSize` parameters can be used to increase
    or decrease the amount of memory used for various calculations during the training
    of the cascade classifier. You can modify these parameters to make sure the training
    process is performed with more efficiency.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precalcValBufSize` 和 `precalcIdxBufSize` 参数可以用来增加或减少在级联分类器训练过程中用于各种计算的记忆量。你可以修改这些参数以确保训练过程以更高的效率进行。'
- en: '`featureType` is one of the most important parameters of the training tool,
    and it can be used to set the type of the trained classifier to `HAAR` (default
    if ignored) or `LBP`. As mentioned before, LBP classifiers are trained much faster
    than Haar classifiers and their detection is also significantly faster, but they
    lack the accuracy of the Haar cascade classifiers. With a proper amount of training
    samples, you might be able to train an LBP classifier that can compete with a
    Haar classifier in terms of accuracy.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`featureType` 是训练工具最重要的参数之一，它可以用来设置训练分类器的类型为 `HAAR`（如果忽略则为默认值）或 `LBP`。如前所述，LBP
    分类器比 Haar 分类器训练得更快，它们的检测速度也显著更快，但它们缺乏 Haar 级联分类器的准确性。有了适当数量的训练样本，你可能能够训练出一个在准确性方面可以与
    Haar 分类器相媲美的 LBP 分类器。'
- en: For a complete list of parameters and their descriptions, make sure to refer
    to the OpenCV online documentation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取参数及其描述的完整列表，请确保查阅 OpenCV 在线文档。
- en: Using deep learning models
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习模型
- en: In recent years, there has been a huge improvement in the field of deep learning,
    or, to be precise, **deep neural networks** (**DNN**), and more and more libraries
    and frameworks are being introduced that use deep learning algorithms and models,
    especially for computer vision purposes such as object detection in real-time.
    You can use the most recent versions of the OpenCV library to read pre-trained
    models for the most popular DNN frameworks, such as Caffe, Torch, and TensorFlow,
    and use them for object detection and prediction tasks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习领域取得了巨大的进步，或者更准确地说，是 **深度神经网络**（**DNN**），越来越多的库和框架被引入，它们使用深度学习算法和模型，特别是用于计算机视觉目的，如实时目标检测。你可以使用
    OpenCV 库的最新版本来读取最流行的 DNN 框架（如 Caffe、Torch 和 TensorFlow）的预训练模型，并将它们用于目标检测和预测任务。
- en: 'DNN-related algorithms and classes in OpenCV are all located under the `dnn`
    namespace, so, to be able to use them, you need to make sure to include the following
    in your code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 中的 DNN 相关算法和类都位于 `dnn` 命名空间下，因此，为了能够使用它们，你需要在你的代码中确保包含以下内容：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We''re going to walk through the loading and use of a pre-trained model from
    the TensorFlow library in OpenCV for real-time object detection. This example
    demonstrates the basics of how to use deep neural networ models trained by a third-party
    library (TensorFlow in this case). So, let''s start:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步介绍在 OpenCV 中加载和使用 TensorFlow 库的预训练模型进行实时目标检测。这个例子演示了如何使用由第三方库（本例中为 TensorFlow）训练的深度神经网络模型的基礎。所以，让我们开始吧：
- en: Download a pre-trained TensorFlow model that can be used for object detection.
    For our example, make sure you download the latest version of `ssd_mobilenet_v1_coco`
    from the search, for official TensorFlow models online instead.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一个可用于目标检测的预训练 TensorFlow 模型。对于我们的示例，请确保从搜索中下载 `ssd_mobilenet_v1_coco` 的最新版本，从官方
    TensorFlow 模型在线搜索结果中下载。
- en: Note that this link can possibly change in the future (maybe not soon, but it's
    worth mentioning), so, in case this happens, you need to simply search online
    for the `TensorFlow` model zoo, which, in `TensorFlow` terms, is a zoo containing
    the pre-trained object detection models.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个链接未来可能会发生变化（可能不会很快，但提一下是值得的），所以，如果发生这种情况，你需要简单地在网上搜索 `TensorFlow` 模型动物园，在
    `TensorFlow` 的术语中，这是一个包含预训练目标检测模型的动物园。
- en: 'After downloading the `ssd_mobilenet_v1_coco` model package file, you need
    to extract it to a folder of your choice. You''ll end up with the `frozen_inference_graph.pb`
    file in the folder where you extracted the model package, along with a few more
    files. You need to extract a text graph file from this model file before it can
    be used for real-time object detection in OpenCV. This extraction can be performed
    by using a script called `tf_text_graph_ssd.py`, which is a Python script that
    is included in the OpenCV installation by default and can be found in the following
    path:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下载`ssd_mobilenet_v1_coco`模型包文件后，您需要将其解压到您选择的文件夹中。您将得到一个名为`frozen_inference_graph.pb`的文件，以及一些其他文件。在OpenCV中进行实时对象检测之前，您需要从该模型文件中提取一个文本图文件。此提取可以通过使用名为`tf_text_graph_ssd.py`的脚本完成，这是一个默认包含在OpenCV安装中的Python脚本，可以在以下路径找到：
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can execute this script using the following command:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令执行此脚本：
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that the correct execution of this script totally depends on whether you
    have a correct TensorFlow installation on your computer or not.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，此脚本的正确执行完全取决于您是否在计算机上安装了正确的TensorFlow。
- en: 'You should have the `frozen_inference_graph.pb` and `frozen_inference_graph.pbtxt`
    files, so we can start using them in OpenCV to detect objects. For this reason,
    we need to create a DNN `Network` object and read the model files into it, as
    seen in the following example:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该有`frozen_inference_graph.pb`和`frozen_inference_graph.pbtxt`文件，这样我们就可以在OpenCV中使用它们来检测对象。因此，我们需要创建一个DNN
    `Network`对象并将模型文件读入其中，如下例所示：
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'After making sure the model is correctly loaded, you can use the following
    code to perform a real-time object detection in a frame read from the camera,
    an image, or a video file:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在确保模型正确加载后，您可以使用以下代码在从摄像头读取的帧、图像或视频文件上执行实时对象检测：
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: It's worth noting that the values passed to the `blobFromImage` function completely
    depend on the model, and you should use the exact same values if you're using
    the same model from this example. The `blobFromImage` function will create a BLOB
    that is suitable for use with the deep neural network's prediction function, or,
    to be precise, the `forward` function.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，传递给`blobFromImage`函数的值完全取决于模型，如果您使用的是本例中的相同模型，则应使用完全相同的值。`blobFromImage`函数将创建一个BLOB，适用于与深度神经网络预测函数一起使用，或者更准确地说，是与`forward`函数一起使用。
- en: 'After the detection is complete, you can use the following code to extract
    the detected objects and their bounding rectangles, all into a single `Mat` object:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检测完成后，您可以使用以下代码提取检测到的对象及其边界矩形，所有这些都放入一个单独的`Mat`对象中：
- en: '[PRE43]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `detections` object can be looped through to extract the individual detections
    that have an acceptable detection confidence level and draw the results on the
    input image. Here''s an example:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以遍历`detections`对象以提取具有可接受检测置信水平的单个检测，并在输入图像上绘制结果。以下是一个示例：
- en: '[PRE44]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The confidence level, which is the third element in each row of the `detections`
    object, can be adjusted to get more accurate results, but `0.5` should be a reasonable
    value for most cases, or at least for a start.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 置信度，即`detections`对象每行的第三个元素，可以调整以获得更准确的结果，但`0.5`对于大多数情况或至少作为开始来说应该是一个合理的值。
- en: 'After a detection passes the confidence criteria, we can extract the detected
    object ID and bounding rectangle and draw it on the input image, as seen here:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检测通过置信度标准后，我们可以提取检测到的对象ID和边界矩形，并在输入图像上绘制，如下所示：
- en: '[PRE45]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In the preceding example, `objectClass` refers to the ID of the detected object,
    which is the second element in each row of the detection''s object. The third,
    fourth, fifth, and sixth elements, on the other hand, correspond to the left,
    top, right, and bottom values of the bounding rectangle of each detected object.
    The rest of the code is simply drawing the results, which leaves the `labels`
    object. `labels` is a `vector` of `string` values that can be used to retrieve
    the human-readable text of each object ID. These labels, similar to the rest of
    the parameters we used in this example, are model-dependent. For instance, in
    our example case, the labels can be found here:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，`objectClass`指的是检测到的对象的ID，它是检测对象每行的第二个元素。另一方面，第三、第四、第五和第六个元素对应于每个检测对象边界框的左、上、右和下值。其余的代码只是绘制结果，这留下了`labels`对象。`labels`是一个`string`值的`vector`，可以用来检索每个对象ID的可读文本。这些标签，类似于我们在本例中使用的其余参数，是模型相关的。例如，在我们的示例案例中，标签可以在以下位置找到：
- en: '[https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)'
- en: 'We have converted this into the following labels vector used in the preceding
    example:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将其转换为以下标签向量，用于前面的示例：
- en: '[PRE46]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following image demonstrates the result of the object detection using a
    pre-trained TensorFlow model in OpenCV:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了在 OpenCV 中使用预训练的 TensorFlow 模型进行目标检测的结果：
- en: '![](img/00111.jpeg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00111.jpeg)'
- en: Using deep learning has proven to be highly efficient, especially when we need
    to train and detect multiple objects in real-time. Make sure to refer to the `TensorFlow`
    and OpenCV documentation for more about how to use pre-trained models, or how
    to train and retrain DNN models for an object that doesn't have an already trained
    model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习已被证明非常高效，尤其是在我们需要实时训练和检测多个对象时。确保参考 `TensorFlow` 和 OpenCV 文档以获取有关如何使用预训练模型或如何为没有已训练模型的物体训练和重新训练
    DNN 模型的更多信息。
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started the final chapter of this book by learning about SVM models and how
    to train them to classify groups of similar data. We learned how SVM can be used
    in conjunction with the HOG descriptor to learn about one or more specific objects
    and then detect and classify them in new images. After learning about SVM models,
    we moved on to using ANN models, which offer much more power in cases where we
    have multiple columns in both the input and output of the training samples. This
    chapter also included a complete guide on how to train and use Haar and LBP cascade
    classifiers. We are now familiar with the usage of official OpenCV tools that
    can be used to prepare a training dataset from scratch and then train a cascade
    classifier using that dataset. Finally, we ended this chapter and this book by
    learning about the usage of pre-trained deep learning object detection models
    in OpenCV.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习 SVM 模型及其如何训练以对相似数据组进行分类来开始这本书的最后一章。我们学习了 SVM 如何与 HOG 描述符结合使用，以了解一个或多个特定对象，然后在新的图像中检测和分类它们。在了解
    SVM 模型之后，我们转向使用 ANN 模型，在输入和输出训练样本的多个列的情况下，这些模型提供了更多的功能。本章还包括了如何训练和使用 Haar 和 LBP
    级联分类器的完整指南。我们现在熟悉了使用官方 OpenCV 工具从头开始准备训练数据集，然后使用该数据集训练级联分类器的方法。最后，我们通过学习在 OpenCV
    中使用预训练的深度学习目标检测模型来结束这一章和这本书。
- en: Questions
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the difference between the `train` and `trainAuto` methods in the `SVM`
    class?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `SVM` 类中，`train` 和 `trainAuto` 方法之间的区别是什么？
- en: Demonstrate the difference between the linear and histogram intersection.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示线性与直方图交集之间的区别。
- en: How do you calculate the HOG descriptor size for a HOG window size of 128 x
    96 pixels (the rest of the HOG parameters are untouched)?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何计算 HOG 窗口大小为 128 x 96 像素的 HOG 描述符大小（其他 HOG 参数保持不变）？
- en: How do you update an existing trained `ANN_MLP`, instead of training from scratch?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何更新现有的已训练 `ANN_MLP`，而不是从头开始训练？
- en: What is the required command (by using `opencv_createsamples`) to create a positive
    samples vector from a single image of a company logo? Assume we want to have 1,000
    samples with a width of 24 and a height of 32, and by using default parameters
    for rotations and inversions.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `opencv_createsamples` 创建来自单个公司标志图像的正样本向量所需的命令是什么？假设我们想要有 1,000 个样本，宽度为 24，高度为
    32，并且使用默认的旋转和反转参数。
- en: What is the required command to train an LBP cascade classifier for the company
    logo from the previous question?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练用于之前问题中的公司标志的 LBP 级联分类器所需的命令是什么？
- en: What is the default number of stages for training a cascade classifier in `opencv_traincascade`?
    How can we change it? What is the downside of increasing and decreasing the number
    of stages far beyond its default value?
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `opencv_traincascade` 中训练级联分类器的默认阶段数是多少？我们如何更改它？增加和减少阶段数远超过其默认值有什么缺点？
