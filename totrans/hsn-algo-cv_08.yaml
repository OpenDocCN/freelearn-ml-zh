- en: Machine Learning in Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about a number of algorithms for object
    detection and tracking. We learned how to use color-based algorithms, such as
    Mean Shift and CAM Shift, in conjunction with histograms and back-projection images
    to locate an object in an image with incredible speed. We also learned about template
    matching and how it can be used to find objects with a known template of pixels
    in an image. All of these algorithms rely in one way or another on image properties,
    such as brightness or color, that are easily affected by a change in lighting
    of the environment. Based on these facts, we moved on to learn about algorithms
    that are based on knowledge about significant areas in an image, called **keypoints**
    or **features**. We learned about many edge- and keypoint-detection algorithms
    and how to extract descriptors for those keypoints. We also learned about descriptor
    matchers and how to detect an object in an image using good matches of descriptors
    extracted from an image of the object of interest and the scene where we're looking
    for that object.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're going to take one big step forward and learn about algorithms
    that can be used to extract a model from a large number of images of an object,
    and later use that model to detect an object in an image or simply classify an
    image. Such algorithms are the meeting point of machine learning algorithms and
    computer vision algorithms. Anyone familiar with artificial intelligence and machine
    learning algorithms in general will have an easy time proceeding with this chapter,
    even if they are not fluent in the exact algorithms and examples presented in
    this chapter. However, those who are totally new to such concepts will probably
    need to grab another book, preferably about machine learning, to familiarize themselves
    with algorithms, such as **support vector machines** (**SVM**), **artificial neural
    networks** (**ANN**), cascade classification, and deep learning, which we'll be
    learning about in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to train and use SVM for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using HOG and SVM for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train and use ANN for prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to train and use Haar or LBP cascade classifiers for real-time object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use pre-trained models from third-party deep learning frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An IDE to develop C++ or Python applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 2](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4),
    *Getting Started with OpenCV* for more information about how to set up a personal
    computer and make it ready for developing computer vision applications using the
    OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: You can use this URL to download the source codes and examples for this chapter: [https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To put it as simply as possible, SVMs are used for creating a model from a labeled
    set of training samples that can be used to predict the label of new samples.
    For instance, assume we have a set of sample data belonging to two different groups.
    Each sample in our training dataset is a vector of floating-point numbers that
    can correspond to anything, such as a simple point in 2D or 3D space, and each
    sample is labeled with a number, such as 1, 2, or 3\. Having such data, we can
    train an SVM model that can be used to predict the label of new 2D or 3D points.
    Let's think about another problem. Imagine we have the data of temperatures for
    365 days in cities from all the continents in the world, and each vector of the
    365 temperature values is labeled with 1 for Asia, 2 for Europe, 3 for Africa,
    and so on. We can use this data to train an SVM model that can be used to predict
    the continent of new vectors of temperature values (for 365 days) and associate
    them with a label. Even though these examples might not be useful in practice,
    they describe the concept of SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `SVM` class in OpenCV to train and use SVM models. Let''s go
    through the usage of the `SVM` class in detail with a complete example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since machine learning algorithms in OpenCV are included under the `ml` namespace,
    we need to make sure we include those namespaces in our code, so that the classes
    within them are easily accessible, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the training dataset. As we mentioned before, the training dataset is
    a set of vectors (samples) of floating-point numbers, and each vector is labeled
    with the class ID or category of that vector. Let''s start with samples first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, each sample in our dataset of eight samples contains two floating-point
    values that can be demonstrated using a point on an image with an *x* and *y*
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to create the label (or response) data, which obviously must be
    the same length as the samples. Here it is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, our samples are labeled with the `1` and `2` values, so we're
    expecting our model to be able to differentiate new samples between the given
    two groups of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV uses the `TrainData` class to simplify the preparation and usage of
    the training dataset. Here''s how it''s used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`layout` in the preceding code is set to `ROW_SAMPLE` because each row in our
    dataset contains one sample. If the layout of the dataset was vertical, in other
    words, if each sample in the dataset was a column in the `samples` matrix, we''d
    need to set `layout` to `COL_SAMPLE`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the actual `SVM` class instance. This class in OpenCV implements various
    types of SVM classification algorithms, and they can be used by setting the correct
    parameters. In this example, we''re going to use the most basic (and common) set
    of parameters for the `SVM` class, but to be able to use all possible features
    of this algorithm, make sure to go through the OpenCV `SVM` class documentation
    pages. Here''s an example that shows how we can use SVM to perform a linear *n*-class
    classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the SVM model using the `train` (or `trainAuto`) method, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Based on the amount of data in our training samples dataset, the training process
    might take some time. In our case, it should be fast enough though, since we just
    used a handful of samples to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to use the SVM model to actually predict the label of new samples.
    Remember that each sample in our training set was a 2D point in an image. We''re
    going to find the label of each 2D point in an image with a width and height of
    `300` pixels, and then color each pixel as green or blue, based on whether its
    predicted label is `1` or `2`. Here''s how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and display the result of predictions, but, to be able to perfectly
    visualize the classification result of the SVM algorithm, it''s better to draw
    the training samples we used to create the SVM model. Let''s do it using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The two types of samples (`1` and `2`) are drawn as black and white circles
    over the resultant image. The following diagram depicts the result of the complete
    SVM classification we just performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00100.gif)'
  prefs: []
  type: TYPE_IMG
- en: This demonstration is quite simple and, in reality, SVM can be used for much
    more complex classification problems, however, it literally shows the most essential
    aspect of SVM, which is the separation of various groups of data that are labeled
    the same. As you can see in the preceding image, the line separating the blue
    region from the green region is the best single line that can most efficiently
    separate the black dots and white dots on the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can experiment with this phenomenon by updating the labels, or, in other
    words, the responses in the preceding example, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Trying to visualize the results now will produce something similar to the following,
    which again depicts the most efficient line for separating the two groups of dots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00101.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'You can very easily add more classes to your data, or, in other words, have
    more labels for your training sample set. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can try visualizing the results again by adding a yellow color, for instance,
    for the third class region, and a gray dot for training samples that belong to
    that class. Here''s the result of the same SVM example when used with three classes
    instead of two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00102.gif)'
  prefs: []
  type: TYPE_IMG
- en: If you recall the example of 365 days from before, it is quite obvious that
    we can also add more dimensionality to the SVM model and not just classes, but
    it wouldn't be visually possible to display the results with a simple image such
    as the one in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: Before continuing with the usage of the SVM algorithm for actual object detection
    and image classification, it's worth noting that, just like any other machine
    learning algorithm, having more samples in your dataset will result in a much
    better classification and higher accuracy, but it will also take more time to
    train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images using SVM and HOG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Histogram of Oriented Gradients** (**HOG**) is an algorithm that can be used
    to describe an image using a vector of floating-point descriptors that correspond
    to the oriented gradient values extracted from that image. The HOG algorithm is
    very popular and certainly worth reading about in detail to understand how it
    is implemented in OpenCV, but, for the purposes of this book and especially this
    section, we''ll just mention that the number of the floating-point descriptors
    will always be the same when they are extracted from images that have exactly
    the same size with the same HOG parameters. To better understand this, recall
    that descriptors extracted from an image using the feature detection algorithms
    we learned about in the previous chapter can have different numbers of elements
    in them. The HOG algorithm, though, will always produce a vector of the same length
    if the parameters are unchanged across a set of images of the same size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes the HOG algorithm ideal for being used in conjunction with SVM,
    to train a model that can be used to classify images. Let''s see how it''s done
    with an example. Imagine we have a set of images that contain images of a traffic
    sign in one folder, and anything but that specific traffic sign in another folder.
    The following pictures depicts the images in our samples dataset, separated by
    a black line in between:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00103.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using images similar to the preceding samples, we''re going to train the SVM
    model to detect whether an image is the traffic sign we''re looking for or not.
    Let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `HOGDescriptor` object. `HOGDescriptor`, or the HOG algorithm, is
    a special type of descriptor algorithm that relies on a given window size, block
    size, and various other parameters; for the sake of simplicity, we''ll avoid all
    but the window size. The HOG algorithm''s window size in our example is `128`
    by `128` pixels, which is set as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Sample images should have the same size as the window size, otherwise we need
    to use the `resize` function to make sure they are resized to the HOG window size
    later on. This guarantees the same descriptor size every time the HOG algorithm
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we just mentioned, the vector length of the descriptor extracted using `HOGDescriptor`
    will be constant if the image size is constant, and, assuming that image has the
    same size as `winSize,` you can get the descriptor length using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We'll use `descriptorSize` later on when we read the sample images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming the images of the traffic sign are inside a folder called `pos` (for
    positive) and the rest inside a folder called `neg` (for negative), we can use
    the `glob` function to get the list of image files in those folders, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create buffers to store the HOG descriptors for negative and positive sample
    images (from `pos` and `neg` folders). We also need an additional buffer for the
    labels (or responses), as seen in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to use the `HOGDescriptor` class to extract the HOG descriptors from
    positive images and store them in `samples`, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It needs to be noted that we have added `+1` for the labels (responses) of the
    positive samples. We'll need to use a different number, such as `-1`, when we
    label the negative samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the positive samples, we add the negative samples and their responses
    to the designated buffers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the example from the previous section, we need to form a `TrainData`
    object using `samples` and `responses` to be used with the `train` function. Here''s
    how it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to train the SVM model as seen in the following example code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training is completed, the SVM model is ready to be used for classifying
    images with the same size as the HOG window size (in this case, `128` by `128`
    pixels) using the `predict` method of the  `SVM` class. Here is how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we simply read an image and resize it to the HOG window
    size. Then we use the `compute` method of the `HOGDescriptor` class, just like
    when we were training the model. Except, this time, we use the `predict` method
    to find the label of this new image. If the `result` equals `+1`, which was the
    label we assigned for traffic sign images when we trained the SVM model, then
    we know that the image is the image of a traffic sign, otherwise it's not.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the result completely depends on the quantity and quality of
    the data you have used to train your SVM model. This, in fact, is the case for
    each and every machine learning algorithm. The more you train your model, the
    more accurate it becomes.
  prefs: []
  type: TYPE_NORMAL
- en: This method of classification assumes that the input image is of the same characteristics
    as the trained images. Meaning, if the image contains a traffic sign, it is cropped
    similarly to the images we used to train the model. For instance, if you use an
    image that contains the traffic sign image we're looking for, but also contain
    much more, then the result will probably be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the amount of data in your training set increases, it will take more time
    to train your model. So, it''s important to avoid retraining your model every
    time you want to use it. The `SVM` class allows you to save and load SVM models
    using the `save` and `load` methods. Here is how you can save a trained SVM model
    for later use and to avoid retraining it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The file will be saved using the provided filename and extension (XML or any
    other  file type supported by OpenCV). Later, using the static `load` function,
    you can create an SVM object that contains the exact parameters and trained model.
    Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Try using the `SVM` class along with `HOGDescriptor` to train models that can
    detect and classify more types using images of various objects stored in different
    folders.
  prefs: []
  type: TYPE_NORMAL
- en: Training models with artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANN can be used to train a model using a set of sample input and output vectors.
    ANN is a highly popular machine learning algorithm and the basis of many modern
    artificial intelligence algorithms that are used to train models for classification
    and correlation. Especially in computer vision, the ANN algorithm can be used
    along with a wide range of feature-description algorithms to learn about images
    of objects, or even faces of different people, and then used to detect them in
    images.
  prefs: []
  type: TYPE_NORMAL
- en: You can use the `ANN_MLP` class (which stands for **artificial neural networks—****multi-layer** **perceptron**)
    in OpenCV to implement ANN in your applications. The usage of this class is quite
    similar to that of the `SVM` class, so we're going to give a simple example to
    learn the differences and how it's used in practice, and we'll leave the rest
    for you to discover by yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating the training samples dataset is exactly the same for all machine learning
    algorithms in OpenCV, or, to be precise, for all subclasses of the `StatsModel`
    class. The `ANN_MLP` class is no exception to this, so, just like with the `SVM`
    class, first we need to create a `TrainData` object that contains all the sample
    and response data that we need to use when training our ANN model, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`samples` and `responses`, in the preceding code, are both `Mat` objects that
    contain a number of rows that equals the number of all the training data we have
    in our dataset. As for the number of columns in them, let''s recall that the ANN
    algorithm can be used to learn the relationship between vectors of input and output
    data. This means that the number of columns in the training input data, or `samples`,
    can be different from the number of columns in the training output data, or `responses`.
    We''ll refer to the number of columns in `samples` as the number of features,
    and to the number of columns in `responses` as the number of classes. Simply put,
    we''re going to learn the relationship of features to classes using a training
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After taking care of the training dataset, we need to create an `ANN_MLP` object
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We have skipped all the customizations and used the default set of parameters.
    In the case that you need to use a fully customized `ANN_MLP` object, you need
    to set the activation function, termination criteria, and various other parameters
    in the `ANN_MLP` class. To learn more about this, make sure to refer to the OpenCV
    documentation and online resources about artificial neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the correct layer sizes in the ANN algorithm requires experience and
    depends on the use case, but it can also be set using a few trial-and-error sessions.
    Here''s how you can set the number and size of each layer in the ANN algorithm,
    and the `ANN_MLP` class to be specific:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the number of rows in the `layers` object refers to the
    number of layers we want to have in our ANN. The first element in the `layers`
    object should contain the number of features in our dataset, and the last element
    in the `layers` object should contain the number of classes. Recall that the number
    of features equals the column count of `samples`, and the number of classes equals
    the column count of `responses`. The rest of the elements in the `layers` object
    contain the sizes of hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training the ANN model is done by using the `train` method, as seen in the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: After the training is completed, we can use the `save` and `load` methods in
    exactly the same way as we saw before, to save the model for later use, or reload
    it from a saved file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the model with the `ANN_MLP` class is also quite similar to the `SVM`
    class. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Choosing the right machine learning algorithm for each problem requires experience
    and knowledge about where the project is going to be used. SVM is quite simple,
    and suitable when we need to work with the classification of data and in the segmentation
    of groups of similar data, whereas ANN can be easily used to approximate a function
    between sets of input and output vectors (regression). Make sure to try out different
    machine learning problems to better understand where and when to use a specific
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The cascading classification algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cascading classification is another machine learning algorithm that can be used
    to train a model from many (hundreds, or even thousands) positive and negative
    image samples. As we explained earlier, a positive image refers to the image in
    an object of interest (such as a face, a car, or a traffic signal) that we want
    our model to learn and later classify or detect. On the other hand, a negative
    image corresponds to any arbitrary image that does not contain our object of interest.
    The model trained using this algorithm is referred to as a cascade classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The most important aspect of a cascade classifier, as can be guessed from its
    name, is its cascading nature of learning and detecting an object using the extracted
    features. The most widely used features in cascade classifiers, and consequently
    cascade classifier types, are Haar and **local binary pattern** (**LBP**). In
    this section, we're going to learn how to use existing OpenCV Haar and LBP cascade
    classifiers to detect faces, eyes, and more in real-time, and then learn how to
    train our own cascade classifiers to detect any other objects.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection using cascade classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To be able to use previously trained cascade classifiers in OpenCV, you can
    use the `CascadeClassifier` class and the simple methods it provides for loading
    a classifier from file or performing scale-invariant detection in images. OpenCV
    contains a number of trained classifiers to detect faces, eyes, and so on in real-time.
    If we browse to the OpenCV installation (or build) folder, it usually contains
    a folder called `etc`, which contains the following subfolders:'
  prefs: []
  type: TYPE_NORMAL
- en: '`haarcascades`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lbpcascades`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`haarcascades` contains pre-trained Haar cascade classifiers. `lbpcascades`,
    on the other hand, contains pre-trained LBP cascade classifiers. Haar cascade
    classifiers are usually slower than LBP cascade classifiers, but they also provide
    much better accuracy in most cases. To learn about the details of Haar and LBP
    cascade classifies, make sure to refer to the OpenCV documentation as well as
    online resources about Haar wavelets, Haar-like features, and local binary patterns.
    As we''ll learn in the next section, LBP cascade classifiers are also a lot faster
    to train than Haar classifiers; with enough training data samples, you can reach
    a similar accuracy for both of the classifier types.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under each one of the classifier folders we just mentioned, you can find a
    number of pre-trained cascade classifiers. You can load these classifiers and
    prepare them for object detection in real-time using the `load` method of the `CascadeClassifier`
    class, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After a cascade classifier is successfully loaded, you can use the `detectMultiScale`
    method to detect objects in an image and return a vector containing the bounding
    rectangles of the detected objects, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`color` and `thickness` are previously defined to affect the rectangle drawn
    for each detected object, as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Try loading the `haarcascade_frontalface_default.xml` classifier in the `haarcascades`
    folder, which comes preinstalled with OpenCV, to test the preceding example. Trying
    to run the preceding code with an image that contains a face would result in something
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00104.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'The accuracy of the cascade classifier, as with any other machine learning
    model, depends completely on the quality and quantity of the training samples
    dataset. As it was mentioned before, cascade classifiers are widely popular, especially
    for real-time object detection. To be able to view the performance of cascade
    classifiers on any computer, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line in the preceding code is used to convert the unit of the time
    measurement from seconds to milliseconds. You can use the following code to print
    out the result over the output image, in the lower-left corner for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce an output image that contains text similar to what is seen
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00105.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Try different pre-trained cascade classifiers shipped with OpenCV and check
    their performance against each other. One very obvious observation will be the
    significantly faster detection speed of LBP cascade classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous examples, we used only the default set of parameters needed
    for the `detectMultiScale` method of the `CascadeClassifier` class, however, to
    modify its behavior and, in some cases, to significantly improve its performance,
    you''ll need to adjust a few more parameters, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `scaleFactor` parameter is used to specify the scaling of the image after
    each detection. This means resizing the image and performing the detection internally.
    This is in fact how multi-scale detection algorithms work. An image is searched
    for in an object, its size is reduced by the given `scaleFactor`, and the search
    is performed again. Size reduction is performed repeatedly until the image size
    is smaller than the classifier size. The results from all detections in all scales
    are then returned. The `scaleFactor` parameter must always contain a value greater
    than 1.0 (not equal to and not lower than). For higher sensitivity in multi-scale
    detection, you can set a value such as 1.01 or 1.05, which will lead to much longer
    detection times, and vice versa. The `minNeighbors` parameter refers to the grouping
    of detections that are near or similar to each other to retain a detected `object`.
  prefs: []
  type: TYPE_NORMAL
- en: The `flags` parameter is simply ignored in recent versions of OpenCV. As for
    the `minSize` and `maxSize` parameters, they are used to specify the minimum and
    maximum possible sizes of an object in an image. This can significantly increase
    the accuracy and speed of the `detectMultiScale` function, since detected objects
    that do not fall into the given size range are simply ignored and rescaling is
    done only until `minSize` is reached.
  prefs: []
  type: TYPE_NORMAL
- en: '`detectMultiScale` has two other variations that we skipped for the sake of
    simplifying the examples, but you should check them out for yourself to learn
    more about cascade classifiers and multi-scale detection in general. Make sure
    to also search online for pre-trained classifiers by fellow computer vision developers
    and try using them in your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Training cascade classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned previously, you can also create your own cascade classifiers
    to detect any other object if you have enough positive and negative sample images.
    Training a classifier using OpenCV involves taking a number of steps and using
    a number of special OpenCV applications, which we'll go through in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first, you need a tool called `opencv_createsamples` to prepare
    the positive image sample set. The negative image samples, on the other hand,
    are extracted automatically during the training from a provided folder containing
    arbitrary images that do NOT include the object of interest. The `opencv_createsamples`
    application can be found inside the `bin` folder of the OpenCV installation. It
    can be used to create the positive samples dataset, either by using a single image
    of the object of interest and applying distortions and transformations to it,
    or by using previously cropped or annotated images of the object of interest.
    Let's learn about the former case first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine you have the following image of a traffic sign (or any other object,
    for that matter) and you want to create a positive sample dataset using it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00106.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You should also have a folder containing the source of the negative samples.
    As we mentioned previously, you need to have a folder containing arbitrary images
    that do not contain the object of interest. Let''s assume we have some images
    similar to the following that we''ll be using to create negative samples from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the size and aspect ratio of negative images, or to use the correct
    terminology, the background images, is not at all important. However, they must
    be at least as big as the minimum-detectable object (classifier size) and they
    must never contain images of the object of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a proper cascade classifier, sometimes you need hundreds or even thousands
    of sample images that are distorted in different ways, which is not easy to create.
    In fact, gathering training data is one of the most time-consuming steps in creating
    a cascade classifier. The `opencv_createsamples` application can help with this
    problem by taking in the previous image of the object we''re creating a classifier
    for and producing a positive samples dataset by applying distortions and using
    the background images. Here''s an example of how it''s used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a description of the parameters used in the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vec` is used to specify the positive samples file that will be created. In
    this case, it is the `samples.vec` file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`img` is used to specify the input image that will be used to generate the
    samples. In our case, it''s `sign.png`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bg` is used to specify the background''s description file. A background''s
    description file is a simple text file that contains the paths to all background
    images (each line in the background''s description file contains the path to one
    background image). We have created a file named `bg.txt` and provided it to the `bg`
    parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `num` parameter determines the number of positive samples you want to generate
    using the given input image and backgrounds; 250, in our case. You can, but of
    course, use a higher or lower number, depending on the accuracy and duration of
    training that you require.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bgcolor` can be used to define the background color in terms of its grayscale
    intensity. As you can see in our input image (the traffic sign image), the background
    color is black, thus the value of this parameter is zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `bgthresh` parameter specifies the threshold of the accepted `bgcolor` parameter.
    This is especially useful in the case of compression artifacts that are common
    to some image formats and might cause slightly different pixel values for the
    same color. We have used 10 for the value of this parameter to allow a slight
    level of tolerance for background pixels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxidev` can be used to set the maximum intensity deviation of the foreground
    pixel values while generating the samples. A value of 50 means the intensity of
    the foreground pixels can vary between their original values +/- 50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxxangle`, `maxyangle`, and `maxzangle` correspond to the maximum possible
    rotation allowed in the *x*, *y*, and *z* directions when creating new samples.
    These values are in radians, for which we have provided 0.7, 0.7, and 0.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `w` and `h` parameters define the width and height of the samples. We have
    used 32 for both of them since the object we're looking to train a classifier
    for fits in a square shape. These same values will be used later on, when training
    the classifier. Also note that this will be the minimum detectable size in your
    trained classifier later on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides the parameters in the preceding list, the `opencv_createsamples` application
    also accepts a `show` parameter that can be used to display the created samples,
    an `inv` parameter that can be used to invert the colors of samples, and a `randinv` parameter
    that can be used to set or unset the random inversion of pixels in samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding command will produce the given number of samples by performing
    rotations and intensity changes to the foreground pixels. Here are some of the
    resultant samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00108.gif)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have a positive samples vector file, produced by `opencv_createsamples`,
    and a folder that contains the background images along with a background's description
    file (`bg.txt` from the previous example), we can start the training of our cascade
    classifier, but before that, let's also learn about the second method of creating
    our positive samples vector, which is by extracting them from various annotated
    images that contain our object of interest.
  prefs: []
  type: TYPE_NORMAL
- en: This second method involves using another official OpenCV tool which is used
    for annotating positive samples in images. This tool is called `opencv_annotation`
    and it can be used to conveniently mark the areas in a number of images that contain
    our positive samples, or in other words, the objects, we're going to train a cascade
    classifier for them. The `opencv_annotation` tool produces an annotation text
    file (after a manual annotation of the objects) that can be used with the `opencv_createsamples`
    tool to produce a positive samples vector suitable for use with the OpenCV cascade
    training tool that we'll learn about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume we have a folder containing images similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'All of these images are located in a single folder and all of them contain
    one or more samples of the traffic sign (the object of interest) that we''re looking
    for. We can use the following command to start the `opencv_annotation` tool and
    manually annotate the samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command, `imgpath` must be replaced with the path (preferably
    the absolute path and with forward slashes) to the folder containing the images.
    `anno.txt`, or any other file name provided instead, will be filled with the annotation
    results, which can be used with `opencv_createsamples` to create a positive samples
    vector. Executing the preceding command will start the `opencv_annotation` tool
    and output the following text, which describes how to use the tool and its shortcut
    keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Immediately after the preceding output, a window similar to the following will
    be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can highlight an object using your mouse's left button, which will cause
    a red rectangle to be drawn. Pressing the *C* key will finalize the annotation
    and it will become red. Continue this process for the rest of the samples (if
    any) in the same image and press *N* to go to the next image. After all images
    are annotated, you can exit the application by pressing the *Esc* key.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the `-images` and `-annotations` parameters, the `opencv_annotation`
    tool also includes an optional parameter, called `-maxWindowHeight`, that can
    be used to resize images that are bigger than a given size. The resize factor
    in this case can be specified with another optional parameter called `-resizeFactor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The annotation file created by the `opencv_annotation` tool will look like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Each line in the annotations file contains the path to an image, followed by
    the number of objects of interest in that image followed by the *x*, *y*, width,
    and height values of the bounding rectangles of those objects. You can use the
    following command to produce a samples vector using this annotation text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note that this time we used the `opencv_createsamples` tool with the `-info`
    parameter, which wasn't present when we used this tool to generate samples from
    an image and arbitrary backgrounds. We are now ready to train a cascade classifier
    that is capable of detecting the traffic sign we created the samples for.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last tool we''re going to learn about is called `opencv_traincascade`,
    which, as you can guess, is used to train cascade classifiers. If you have enough
    samples and background images, and if you have already taken care of the samples
    vector as it was described in the preceding section, then the only thing you need
    to do is to run the `opencv_traincascade` tool and wait for the training to be
    completed. Let''s see an example training command and then go through the parameters
    in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is the simplest way of starting the training process, and only uses the
    mandatory parameters. All parameters used in this command are self-explanatory,
    except the `-data` parameter, which must be an existing folder that will be used
    to create the files required during the training process and the final trained
    classifier (called `cascade.xml`) will be created in this folder.
  prefs: []
  type: TYPE_NORMAL
- en: '`numPos` cannot contain a number higher than the number of positive samples
    in your `samples.vec` file, however, `numNeg` can contain basically any number
    since the training process, will simply try to create random negative samples
    by extracting portions of the provided background images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `opencv_traincascade` tool will create a number of XML files in the folder
    set as the `-data` parameter, which must not be modified until the training process
    is completed. Here is a short description for each one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: The `params.xml` file will contain the parameters used for training the classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stage#.xml` files are checkpoints that are created after each training stage
    is completed. They then can be used to resume the training later on if the training
    process was terminated for an unexpected reason.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `cascade.xml` file is the trained classifier and the last file that will
    be created by the training tool. You can copy this file, rename it to something
    convenient (such as `trsign_classifier.xml` or something like that), and use it
    with the `CascadeClassifier` class, as we learned in the previous sections, to
    perform multi-scale object detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`opencv_traincascade` is an extremely customizable and flexible tool, and you
    can easily modify its many optional parameters to make sure the trained classifier
    fits your needs. Here is a description of some of its most used parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`numStages` can be used to set the number of stages used to train the cascade
    classifier. By default, `numStages` equals 20, but you can decrease this value
    to shorten the training time while sacrificing the accuracy or vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `precalcValBufSize` and `precalcIdxBufSize` parameters can be used to increase
    or decrease the amount of memory used for various calculations during the training
    of the cascade classifier. You can modify these parameters to make sure the training
    process is performed with more efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`featureType` is one of the most important parameters of the training tool,
    and it can be used to set the type of the trained classifier to `HAAR` (default
    if ignored) or `LBP`. As mentioned before, LBP classifiers are trained much faster
    than Haar classifiers and their detection is also significantly faster, but they
    lack the accuracy of the Haar cascade classifiers. With a proper amount of training
    samples, you might be able to train an LBP classifier that can compete with a
    Haar classifier in terms of accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a complete list of parameters and their descriptions, make sure to refer
    to the OpenCV online documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Using deep learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, there has been a huge improvement in the field of deep learning,
    or, to be precise, **deep neural networks** (**DNN**), and more and more libraries
    and frameworks are being introduced that use deep learning algorithms and models,
    especially for computer vision purposes such as object detection in real-time.
    You can use the most recent versions of the OpenCV library to read pre-trained
    models for the most popular DNN frameworks, such as Caffe, Torch, and TensorFlow,
    and use them for object detection and prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'DNN-related algorithms and classes in OpenCV are all located under the `dnn`
    namespace, so, to be able to use them, you need to make sure to include the following
    in your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We''re going to walk through the loading and use of a pre-trained model from
    the TensorFlow library in OpenCV for real-time object detection. This example
    demonstrates the basics of how to use deep neural networ models trained by a third-party
    library (TensorFlow in this case). So, let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: Download a pre-trained TensorFlow model that can be used for object detection.
    For our example, make sure you download the latest version of `ssd_mobilenet_v1_coco`
    from the search, for official TensorFlow models online instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that this link can possibly change in the future (maybe not soon, but it's
    worth mentioning), so, in case this happens, you need to simply search online
    for the `TensorFlow` model zoo, which, in `TensorFlow` terms, is a zoo containing
    the pre-trained object detection models.
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading the `ssd_mobilenet_v1_coco` model package file, you need
    to extract it to a folder of your choice. You''ll end up with the `frozen_inference_graph.pb`
    file in the folder where you extracted the model package, along with a few more
    files. You need to extract a text graph file from this model file before it can
    be used for real-time object detection in OpenCV. This extraction can be performed
    by using a script called `tf_text_graph_ssd.py`, which is a Python script that
    is included in the OpenCV installation by default and can be found in the following
    path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You can execute this script using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note that the correct execution of this script totally depends on whether you
    have a correct TensorFlow installation on your computer or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should have the `frozen_inference_graph.pb` and `frozen_inference_graph.pbtxt`
    files, so we can start using them in OpenCV to detect objects. For this reason,
    we need to create a DNN `Network` object and read the model files into it, as
    seen in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'After making sure the model is correctly loaded, you can use the following
    code to perform a real-time object detection in a frame read from the camera,
    an image, or a video file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: It's worth noting that the values passed to the `blobFromImage` function completely
    depend on the model, and you should use the exact same values if you're using
    the same model from this example. The `blobFromImage` function will create a BLOB
    that is suitable for use with the deep neural network's prediction function, or,
    to be precise, the `forward` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the detection is complete, you can use the following code to extract
    the detected objects and their bounding rectangles, all into a single `Mat` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `detections` object can be looped through to extract the individual detections
    that have an acceptable detection confidence level and draw the results on the
    input image. Here''s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The confidence level, which is the third element in each row of the `detections`
    object, can be adjusted to get more accurate results, but `0.5` should be a reasonable
    value for most cases, or at least for a start.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a detection passes the confidence criteria, we can extract the detected
    object ID and bounding rectangle and draw it on the input image, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, `objectClass` refers to the ID of the detected object,
    which is the second element in each row of the detection''s object. The third,
    fourth, fifth, and sixth elements, on the other hand, correspond to the left,
    top, right, and bottom values of the bounding rectangle of each detected object.
    The rest of the code is simply drawing the results, which leaves the `labels`
    object. `labels` is a `vector` of `string` values that can be used to retrieve
    the human-readable text of each object ID. These labels, similar to the rest of
    the parameters we used in this example, are model-dependent. For instance, in
    our example case, the labels can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have converted this into the following labels vector used in the preceding
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image demonstrates the result of the object detection using a
    pre-trained TensorFlow model in OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using deep learning has proven to be highly efficient, especially when we need
    to train and detect multiple objects in real-time. Make sure to refer to the `TensorFlow`
    and OpenCV documentation for more about how to use pre-trained models, or how
    to train and retrain DNN models for an object that doesn't have an already trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the final chapter of this book by learning about SVM models and how
    to train them to classify groups of similar data. We learned how SVM can be used
    in conjunction with the HOG descriptor to learn about one or more specific objects
    and then detect and classify them in new images. After learning about SVM models,
    we moved on to using ANN models, which offer much more power in cases where we
    have multiple columns in both the input and output of the training samples. This
    chapter also included a complete guide on how to train and use Haar and LBP cascade
    classifiers. We are now familiar with the usage of official OpenCV tools that
    can be used to prepare a training dataset from scratch and then train a cascade
    classifier using that dataset. Finally, we ended this chapter and this book by
    learning about the usage of pre-trained deep learning object detection models
    in OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the difference between the `train` and `trainAuto` methods in the `SVM`
    class?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Demonstrate the difference between the linear and histogram intersection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you calculate the HOG descriptor size for a HOG window size of 128 x
    96 pixels (the rest of the HOG parameters are untouched)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you update an existing trained `ANN_MLP`, instead of training from scratch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the required command (by using `opencv_createsamples`) to create a positive
    samples vector from a single image of a company logo? Assume we want to have 1,000
    samples with a width of 24 and a height of 32, and by using default parameters
    for rotations and inversions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the required command to train an LBP cascade classifier for the company
    logo from the previous question?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the default number of stages for training a cascade classifier in `opencv_traincascade`?
    How can we change it? What is the downside of increasing and decreasing the number
    of stages far beyond its default value?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
