<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-26"><a id="_idTextAnchor029"/>2</h1>
<h1 id="_idParaDest-27"><a id="_idTextAnchor030"/>Distributions of Data</h1>
<p><a id="_idTextAnchor031"/>In this chapter, we will cover the essential aspects of data and distributions. We will start by covering the types of data and distributions of data. Having covered the essential measurements of distributions, we will describe the normal distribution and its important properties, including the central limit theorem. Finally, we will cover resampling methods such as permutations and transformation methods such as log transformations. This chapter covers the foundational knowledge necessary to begin statistical modeling.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Understanding data types</li>
<li>Measuring and describing distributions</li>
<li>The normal distribution and the central limit theorem</li>
<li>Bootstrapping</li>
<li>Permutations</li>
<li>Transformations</li>
</ul>
<h1 id="_idParaDest-28"><a id="_idTextAnchor032"/>Technical requirements</h1>
<p>This chapter will make use of Python 3.8.</p>
<p>The code for this chapter can be found here – <a href="https://github.com/PacktPublishing/Building-Statistical-Models-in-Python">https://github.com/PacktPublishing/Building-Statistical-Models-in-Python</a> – in the <code>ch2</code> folder.</p>
<p>Please set up a virtual environment or Anaconda environment with the following packages installed:</p>
<ul>
<li><code>numpy==1.23.0</code></li>
<li><code>scipy==1.8.1</code></li>
<li><code>matplotlib==3.5.2</code></li>
<li><code>pandas==1.4.2</code></li>
<li><code>statsmodels==0.13.2</code></li>
</ul>
<h1 id="_idParaDest-29"><a id="_idTextAnchor033"/>Understanding data types</h1>
<p>Before <a id="_idIndexMarker076"/>discussing data distributions, it would be useful to understand the types of data. Understanding data types is critical because the type of data determines what kind of analysis can be used since the type of data determines what operations can be used with the data (this will become clearer through the examples in this chapter). There are four distinct types of data:</p>
<ul>
<li>Nominal data</li>
<li>Ordinal data</li>
<li>Interval data</li>
<li>Ratio data</li>
</ul>
<p>These types of data can also be grouped into two sets. The first two types of data (nominal and ordinal) are <strong class="bold">qualitative data</strong>, generally <a id="_idIndexMarker077"/>non-numeric categories. The last two types of<a id="_idIndexMarker078"/> data (interval and ratio) are <strong class="bold">quantitative data</strong>, generally numeric values.</p>
<p>Let’s start with nominal data.</p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor034"/>Nominal data</h2>
<p>Nominal data is <a id="_idIndexMarker079"/>data labeled with distinct groupings. As an example, take machines in a sign factory. It is common for factories to source machines <a id="_idIndexMarker080"/>from different suppliers, which would also have different model numbers. For example, the example factory may have 3 of <strong class="bold">Model A</strong> and 5 of <strong class="bold">Model B</strong> (see <em class="italic">Figure 2</em><em class="italic">.1</em>). The machines would make up a set of nominal data where <strong class="bold">Model A</strong> and <strong class="bold">Model B</strong> are the distinct group labels. With nominal data, there is only one operation that can be performed: equality. Each member of a group is equal while members from different groups are unequal. In our factory example, a <strong class="bold">Model A</strong> machine would be equal to another <strong class="bold">Model A</strong> machine while a <strong class="bold">Model B</strong> machine would be unequal to a <strong class="bold">Model </strong><strong class="bold">A</strong> machine.</p>
<div><div><img alt="Figure 2.1 – Two groups of machines in a factory" height="331" src="img/B18945_02_001.jpg" width="596"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Two groups of machines in a factory</p>
<p>As we can see, with this type of data, we can only group items together under labels. With the next type of data, we will introduce a new feature: order.</p>
<h2 id="_idParaDest-31"><a id="_idTextAnchor035"/>Ordinal data</h2>
<p>The<a id="_idIndexMarker081"/> next type <a id="_idIndexMarker082"/>of data is like nominal data but exhibits an order. The data can be labeled into distinct groups and the groups can be ordered. We call this type of data ordinal data. Continuing with the factory example, let’s suppose that there is a <strong class="bold">Model C</strong> machine, and <strong class="bold">Model C</strong> is supplied by the same vendor as <strong class="bold">Model B</strong>. However, <strong class="bold">Model C</strong> is the high-performance version, which generates higher output. In this case, <strong class="bold">Model B</strong> and <strong class="bold">Model C</strong> are ordinal data because <strong class="bold">Model B</strong> is a lower-output machine, and <strong class="bold">Model C</strong> is a higher-output machine, which creates a natural order. For instance, we can put the model labels in ascending order of performance: <strong class="bold">Model B</strong>, <strong class="bold">Model C</strong>. University education levels are another example of ordinal data with the levels BS, MS, and PhD. As mentioned, the new operation for this type of data is ordering, meaning the data can be sorted. Thus, ordinal data supports order and equality. While this type of data can be ordered in ascending or descending order, we cannot add or subtract the data, meaning <strong class="bold">Model B</strong> + <strong class="bold">Model C</strong> is not a meaningful statement. The next type of data we will discuss will support addition and subtraction.</p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor036"/>Interval data</h2>
<p>The<a id="_idIndexMarker083"/> next type of data, interval data, is used to describe data that <a id="_idIndexMarker084"/>exists on an interval scale but does not have a clear definition of zero. This means the difference between two data points is meaningful. Take the Celsius temperature scale, for example. The data points are numeric, and the data points are evenly spaced at an interval (for example, 20 and 40 are both 10 degrees away from 30). In this example of the temperature scale, the definition of 0 is arbitrary. For Celsius, 0 happens to be set at water’s freezing point, but this is an arbitrary choice made by the designers of the scale. So, the interval data type supports equality, ordering, and addition/subtraction.</p>
<h2 id="_idParaDest-33"><a id="_idTextAnchor037"/>Ratio data</h2>
<p>The <a id="_idIndexMarker085"/>final data type<a id="_idIndexMarker086"/> is ratio data. Like interval data, ratio data is ordered numeric data, but unlike interval data, ratio data has an absolute 0. Absolute 0 means that if the value of a ratio-type variable is zero, none of that variable exists or is present. For example, consider wait times for rides at an amusement park. If no<a id="_idIndexMarker087"/> one is in line<a id="_idIndexMarker088"/> for the ride, the wait time is 0; new guests can ride the amusement ride immediately. There is no meaningful negative measurement for wait times. A wait time of 0 is the absolute minimum value. Ratio data also supports meaningful multiplication/division, making ratio data the type of data with the most supported operations.</p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor038"/>Visualizing data types</h2>
<p>Data visualization<a id="_idIndexMarker089"/> is a critical step for understanding distributions<a id="_idIndexMarker090"/> and identifying properties of data. In this chapter (and throughout this book), we will utilize <code>matplotlib</code> for visualizing data. While other Python libraries can be used for visualizing data, <code>matplotlib</code> is the de facto standard plotting library for Python. In this section, we will begin using <code>matplotlib</code> to visualize the four types of data discussed previously.</p>
<h3>Plotting qualitative data types</h3>
<p>Since <a id="_idIndexMarker091"/>the first two types of data are <a id="_idIndexMarker092"/>categorical, we will use a bar chart to visualize these distributions of data. Example bar charts are shown in <em class="italic">Figure 2</em><em class="italic">.2</em>.</p>
<div><div><img alt="Figure 2.2 – Nominal data in a bar chart (left) and ordinal data in a bar chart (right)" height="480" src="img/B18945_02_002.jpg" width="1039"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Nominal data in a bar chart (left) and ordinal data in a bar chart (right)</p>
<p>The left <a id="_idIndexMarker093"/>bar chart in <em class="italic">Figure 2</em><em class="italic">.2</em> shows the <a id="_idIndexMarker094"/>distribution of the <strong class="bold">Model A</strong> machines and <strong class="bold">Model B</strong> machines given in the factory example. The right bar chart shows an example distribution of the education levels of a team of engineers. Note that in the education level bar chart, the <em class="italic">x</em>-axis labels are ordered from the lowest level of education to the highest level of education.</p>
<p>The code used to generate <em class="italic">Figure 2</em><em class="italic">.2</em> is shown next.</p>
<p>The code has three main parts.</p>
<ul>
<li><strong class="bold">The </strong><strong class="bold">library imports</strong>:</li>
</ul>
<p>In this case, we are only importing <code>pyplot</code> from <code>matplotlib</code>, which is canonically imported as <code>plt</code>.</p>
<ul>
<li><strong class="bold">The code for </strong><strong class="bold">data creation</strong>:</li>
</ul>
<p>After the <code>import</code> statement, there are a few statements to create the data we will plot. The data for the first plot is stored in two Python lists: <code>label</code> and <code>counts</code>, which contain the machine labels and the number of machines, respectively. It’s worth noting that each of these two lists contains the same number of elements (two elements). The education data is stored similarly. While in this example, we are using simple example data, in later chapters, we will have additional steps for retrieving and formatting data.</p>
<ul>
<li><strong class="bold">The code for plotting </strong><strong class="bold">the data</strong>:</li>
</ul>
<p>The final <a id="_idIndexMarker095"/>step is plotting the data. Since<a id="_idIndexMarker096"/> we are plotting two sets of data in this example, we use the <code>subplots</code> method, which will create a grid of plots. The first two arguments to <code>subplots</code> are the number of rows and the number of columns for the grid of figures. In our case, the number of rows is <code>1</code> and the number of columns is <code>2</code>. The <code>subplots</code> method returns two objects; the figure, <code>fig</code>, and the axes, <code>ax</code>. The first returned object, <code>fig</code>, has high-level controls over the figure, such as saving the figure, showing the figure in a new window, and many others. The second object, <code>ax</code>, will either be an individual axis object or an array of axis objects. In our case, <code>ax</code> is an array of axes objects – since our grid has two plots, indexing into <code>ax</code> gives us the axes object. We use the <code>bar</code> method of an axes object to create a bar chart. The <code>bar</code> method has two required arguments. The first required argument is the list of labels. The second argument is the bar heights that correspond to each label, which is why the two lists must have the same length. The other three methods, <code>set_title</code>, <code>set_ylabel</code>, and <code>set_xlabel</code>, set the values for the corresponding plot attributes: <code>title</code>, <code>ylabel</code>, and <code>x-label</code>.</p>
<p>Finally, the figure is created using <code>fig.show()</code>:</p>
<pre class="source-code">
import matplotlib.pyplot as plt
label = ['model A', 'model B']
counts = [3, 5]
edu_label = ['BS', 'MS', 'PhD']
edu_counts = [10, 5, 2]
fig, ax = plt.subplots(1, 2, figsize=(12, 5))
ax[0].bar(label, counts)
ax[0].set_title('Counts of Machine Models')
ax[0].set_ylabel('Count')
ax[0].set_xlabel('Machine Model')
ax[1].bar(edu_label, edu_counts)
ax[1].set_title('Counts of Education Level')
ax[1].set_ylabel('Count')
ax[1].set_xlabel('Education Level')
fig.show()</pre>
<p>Now let’s look <a id="_idIndexMarker097"/>at how to plot data from<a id="_idIndexMarker098"/> the other two data types.</p>
<h3>Plotting quantitative data types</h3>
<p>Since<a id="_idIndexMarker099"/> the last two data types are numeric, we <a id="_idIndexMarker100"/>will use a histogram to visualize the distributions. Two example histograms are shown in <em class="italic">Figure 2</em><em class="italic">.3</em>.</p>
<div><div><img alt="Figure 2.3 – Nominal data in a bar chart (left) and ordinal data in a bar chart (right)" height="479" src="img/B18945_02_003.jpg" width="1054"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Nominal data in a bar chart (left) and ordinal data in a bar chart (right)</p>
<p>The<a id="_idIndexMarker101"/> left histogram is synthetic wait time data (ratio data) that might represent wait times at an amusement park. The right<a id="_idIndexMarker102"/> histogram is temperature data (interval data) for the Dallas-Fort Worth area during April and May of 2022 (pulled from <a href="https://www.iweathernet.com/texas-dfw-weather-records">https://www.iweathernet.com/texas-dfw-weather-records</a>).</p>
<p>The code used to generate <em class="italic">Figure 2</em><em class="italic">.3</em> is shown next. Again, the code has three main parts, the library imports, the code for data creation, and the code for plotting the data.</p>
<p>Like in the previous example, <code>matplotlib</code> is imported as <code>plt</code>. In this example, we also import a function from <code>scipy</code>; however, this function is only used for generating sample data to work with and we will not discuss it at length here. For our purposes, just think of <code>skewnorm</code> as producing an array of numbers. This code block is very similar to the previous code block.</p>
<p>The main difference is the method used for plotting the data, <code>hist</code>, which creates a histogram. The <code>hist</code> method has one required argument, which is the sequence of numbers to plot in the histogram. The second argument used in this example is <code>bins</code>, which effectively controls the granularity of the histogram – granularity increases with more bins. The bin count of a histogram can be adjusted for the desired visual effect and is generally set experimentally for the data plotted:</p>
<pre class="source-code">
from scipy.stats import skewnorm
import matplotlib.pyplot as plt
a = 4
x = skewnorm.rvs(a, size=3000) + 0.5
x = x[x &gt; 0]
dfw_highs = [
    85, 87, 75, 88, 80, 86, 90, 94, 93, 92, 90, 92, 94,
    93, 97, 90, 95, 96, 96, 95, 92, 70, 79, 73, 88, 92,
    94, 93, 95, 76, 78, 86, 81, 95, 77, 71, 69, 88, 86,
    89, 84, 82, 77, 84, 81, 79, 75, 75, 91, 86, 86, 84,
    82, 68, 75, 78, 82, 83, 85]
fig, ax = plt.subplots(1,2, figsize=(12, 5))
ax[0].hist(x, bins=30)
ax[0].set_xlabel('Wait Time (hr)')
ax[0].set_ylabel('Frequency')
ax[0].set_title('Wait Times');
ax[1].hist(dfw_highs, bins=7)
ax[1].set_title('High Temperatures for DFW (4/2022-5/2022)')
ax[1].set_ylabel('Frequency')
ax[1].set_xlabel('Temperature (F)')
fig.show()</pre>
<p>In this <a id="_idIndexMarker103"/>section, we had a glimpse into <a id="_idIndexMarker104"/>how varied data and distributions can appear. Since distributions of data appear in many shapes and sizes in the wild, it is useful to have methods for describing distributions. In the next section, we will discuss the measurements <a id="_idIndexMarker105"/>available for distributions, how those measurements are performed, and the types of data that can be measured.</p>
<h1 id="_idParaDest-35"><a id="_idTextAnchor039"/>Measuring and describing distributions</h1>
<p>The <a id="_idIndexMarker106"/>distributions of data found in the wild come in many shapes and sizes. This section<a id="_idIndexMarker107"/> will discuss how distributions are measured and which measurements apply to the four types of data. These measurements will provide methods to compare and contrast different distributions. The measurements discussed in this section can be broken into the following categories:</p>
<ul>
<li>Central tendency</li>
<li>Variability</li>
<li>Shape</li>
</ul>
<p>These measurements are <a id="_idIndexMarker108"/>called <strong class="bold">descriptive statistics</strong>. The <a id="_idIndexMarker109"/>descriptive statistics discussed in this section are commonly used in statistical summaries of data.</p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor040"/>Measuring central tendency</h2>
<p>There<a id="_idIndexMarker110"/> are <a id="_idIndexMarker111"/>three types of measurement of central tendency:</p>
<ul>
<li>Mode</li>
<li>Median</li>
<li>Mean</li>
</ul>
<p>Let’s discuss each one of them.</p>
<h3>Mode</h3>
<p>The<a id="_idIndexMarker112"/> first measurement of central tendency we will discuss is the mode. The mode of a dataset is simply the most commonly occurring instance. Using the machines in the factory as an example (see <em class="italic">Figure 2</em><em class="italic">.1</em>), the mode of the dataset would be model B. In the example, there are 3 of model A and 5 of model B, therefore, making model B the most common – the mode.</p>
<p>A dataset can be one of the following:</p>
<ul>
<li>Unimodal – having one mode</li>
<li>Multimodal – having more than one mode</li>
</ul>
<p>In the preceding example, the data is unimodal.</p>
<p>Using the factory example again, let’s imagine that there are 3 of <strong class="bold">Model A</strong>, 5 of <strong class="bold">Model B</strong>, and 5 of <strong class="bold">Model D</strong> (a new model). Then, the dataset will have two modes: <strong class="bold">Model B</strong> and <strong class="bold">Model D</strong>, as shown in <em class="italic">Figure 2</em><em class="italic">.4</em>.</p>
<div><div><img alt="Figure 2.4 – Multimodel distribution of machines in a factory" height="405" src="img/B18945_02_004.jpg" width="847"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Multimodel distribution of machines in a factory</p>
<p>Therefore, this dataset is multimodal.</p>
<p class="callout-heading">Mode and Data Types</p>
<p class="callout">These examples of modes have used nominal data, but all four types of data support the mode because all four data types support the equality operation.</p>
<p>While the<a id="_idIndexMarker113"/> mode refers to the most common instance, in multimodal cases of continuous data, the term mode is often used in a less strict sense. For example, the distribution in <em class="italic">Figure 2</em><em class="italic">.5</em> would commonly be referred to as multimodal even though the peaks of the distribution are not the same magnitude. However, with nominal and ordinal data, it is more common to use the stricter definition of <em class="italic">most common</em> when referring to the modality of a distribution.</p>
<div><div><img alt="Figure 2.5 – A multimodal distribution of data" height="463" src="img/image_00_005.jpg" width="709"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – A multimodal distribution of data</p>
<p>Now, we will look at how to calculate the mode with code using <code>scipy</code>. The <code>scipy</code> library contains functions for calculating descriptive statistics in the <code>stats</code> module. In this example, we import <code>mode</code> from <code>scipy.stats</code> and calculate the mode of the following numbers, <code>1, 2, 3, 4, 4, 4, </code><code>5, 5</code>:</p>
<pre class="source-code">
from scipy.stats import mode
m = mode([1,2,3,4,4,4,5,5])
print(
    f"The mode is {m.mode[0]} with a count of"
    f" {m.count[0]} instances"
)
# The mode is 4 with a count of 3 instances</pre>
<p>The <code>mode</code> function<a id="_idIndexMarker114"/> returns a <code>mode</code> object containing <code>mode</code> and <code>count</code> members. Unsurprisingly, the <code>mode</code> and <code>count</code> members contain the modes of the dataset and the number of times the modes appear, respectively. Note that <code>mode</code> and <code>count</code> members are indexable (like lists) because a dataset can contain multiple modes.</p>
<h3>Median</h3>
<p>The next <a id="_idIndexMarker115"/>measure of the center is the median. The median<a id="_idIndexMarker116"/> is the middle value occurring when the values are arranged in an order.</p>
<p class="callout-heading">Median and Data Types</p>
<p class="callout">This measure can be performed on ordinal data, interval data, and ratio data, but not on nominal data.</p>
<p>We will discuss two cases here.</p>
<h4>Finding the median when the number of instances is odd</h4>
<p>Finding <a id="_idIndexMarker117"/>the median of some numeric data is shown in <em class="italic">Figure 2</em><em class="italic">.6</em>. The data is sorted, then the median is identified.</p>
<div><div><img alt="Figure 2.6 – Identifying the median with an odd number of instances" height="260" src="img/B18945_02_006.jpg" width="574"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Identifying the median with an odd number of instances</p>
<p>In the <a id="_idIndexMarker118"/>preceding example, the instances are odd in number (7 instances), which have a center value. However, if the number of instances had been even, it would not have been possible to just take the middle number after sorting the values.</p>
<h4>Finding the median when the number of instances is even</h4>
<p>When <a id="_idIndexMarker119"/>there are an even number of instances, the average of the two middle-most values is taken. Unlike the mode, there is no concept of multiple medians for the same series of data. An example with an even number of instances (8 instances) is shown in <em class="italic">Figure 2</em><em class="italic">.7</em>.</p>
<div><div><img alt="Figure 2.7 – Identifying the median with an even number of instances" height="398" src="img/B18945_02_007.jpg" width="682"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Identifying the median with an even number of instances</p>
<p>Now, let’s see how to calculate the median of a dataset with <code>numpy</code>. Like <code>scipy</code>, <code>numpy</code> contains functions for calculating descriptive statistics. We will calculate the median for the eight numbers listed in the preceding example:</p>
<pre class="source-code">
import numpy as np
values = [85, 99, 70, 71, 86, 88, 94, 105]
median = np.median(values)
print(f"The median value is {median:.2f}")
# The median value is 87.00</pre>
<p>The<a id="_idIndexMarker120"/> result of the median calculation is 87, as expected. Note that the <code>median</code> function returns a single value, in contrast to the <code>mode</code> function in the previous code example.</p>
<h3>Mean</h3>
<p>The <a id="_idIndexMarker121"/>next center measure is the mean, which is commonly referred to as the average. The mean is defined by the following equation:</p>
<p> _ x  =  ∑ i=0 n  x i _ N </p>
<p>Let me explain the equation in words. To calculate the mean, we must add all the values together, then divide the sum by the number of values. Please refer to the following example. The 7 numbers are first added together, which brings the total sum to 593. This sum is then divided by the number of instances, resulting in a value of 84.7.</p>
<div><div><img alt="Figure 2.8 – Finding the mean" height="265" src="img/B18945_02_008.jpg" width="672"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Finding the mean</p>
<p>Note that<a id="_idIndexMarker122"/> the mean and the median of these values (84.7 and 86, respectively) are not the same value. In general, the mean and median will not be the same value, but there are special cases where the mean and median will converge.</p>
<p class="callout-heading">Mean and Data Types</p>
<p class="callout">As for the supported data types, the mean is valid for interval and ratio data since the values are added together.</p>
<p>Now, we will look at how to calculate the mean with <code>numpy</code>. The following code example shows the calculation of the mean for the values in the previous example:</p>
<pre class="source-code">
import numpy as np
values = [85, 99, 70, 71, 86, 88, 94]
mean = np.mean(values)
print(f"The mean value is {mean:.1f}")
# The mean value is 84.7</pre>
<p>Like the <code>median</code> function, the <code>mean</code> function returns a single number.</p>
<p>Before concluding this section on center measures, it is worth discussing the use of the mean and median in various situations. As mentioned previously, the median and mean will, in general, be different values. This is an effect driven by the shape of the distribution.</p>
<p class="callout-heading">Shape impacts on Mean and Median</p>
<p class="callout">If the distribution is symmetric, the mean and median will tend to converge. However, if the distribution is not symmetric, the mean and median will diverge.</p>
<p>The<a id="_idIndexMarker123"/> degree to which the measures diverge is driven by how asymmetric the distribution is. Four example distributions are given in <em class="italic">Figure 2</em><em class="italic">.6</em> to show this effect. Distributions 1 and 2 show the mean pulled toward a higher value than the median. The mean is pulled toward values with a larger absolute value. This is an important <a id="_idIndexMarker124"/>effect of the mean to be aware of when a dataset contains (or may contain) <strong class="bold">outlier values</strong> (often called outliers or influential points), which will tend to pull the mean in their direction. Unlike the mean, the median is not affected by outliers if outliers account for a smaller percentage of the data. Outliers will be discussed further in the <em class="italic">Measuring </em><em class="italic">variability</em> section.</p>
<div><div><img alt="Figure 2.9 – Two asymmetric distributions and two symmetric distributions" height="699" src="img/B18945_02_009.jpg" width="729"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Two asymmetric distributions and two symmetric distributions</p>
<p>The next <a id="_idIndexMarker125"/>category of measurements for distributions is measures of variability.</p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor041"/>Measuring variability</h2>
<p>By variability, we <a id="_idIndexMarker126"/>essentially mean how wide a <a id="_idIndexMarker127"/>distribution is. The measurements in this category are as follows:</p>
<ul>
<li>Range</li>
<li>Quartile ranges</li>
<li>Tukey fences</li>
<li>Variance</li>
</ul>
<p>Let’s discuss each of them.</p>
<h3>Range</h3>
<p>The range <a id="_idIndexMarker128"/>is simply the difference between the maximum value and the minimum value in the distribution. Like the mean, the range will be affected by outliers since it depends on the max and min values. However, there is another variability method that, like the median, is robust to the presence of outliers.</p>
<p>Let’s take a look at calculating a range with code with <code>numpy</code>:</p>
<pre class="source-code">
import numpy as np
values = [85, 99, 70, 71, 86, 88, 94, 105]
max_value = np.max(values)
min_value = np.min(values)
range_ = max_value - min_value
print(f"The data have a range of {range_}"
      f" with max of {max_value}"
      f" and min of {min_value}")
# The data have a range of 35 with max of 105 and min of 70</pre>
<p>While <code>numpy</code> does<a id="_idIndexMarker129"/> not have a range function, the range can be calculated using the <code>min</code> and <code>max</code> functions provided by <code>numpy</code>.</p>
<h3>Quartile ranges</h3>
<p>The <a id="_idIndexMarker130"/>next measures of variability are determined by sorting the data and then dividing the data into four equal sections. The boundaries of the four sections are the quartiles, which are called the following:</p>
<ul>
<li>The lower quartile (Q1)</li>
<li>The middle quartile (Q2)</li>
<li>The upper quartile (Q3)</li>
</ul>
<p>An example of quartiles is shown as follows. Like the median, quartiles are robust to outliers so long as the outliers are a small percentage of the dataset. Note that the middle quartile is, in fact, the median. An adjusted range measurement that is less sensitive to outliers than the normal range discussed in the <em class="italic">Range</em> section is the middle quartile, the <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>). The <a id="_idIndexMarker131"/>IQR is the difference between the upper and lower quartiles (Q3 - Q1). While this range is less sensitive to outliers, it only contains 50% of the data. Thus, making the interquartile range likely to be <em class="italic">less representative of the total variation</em> of the data.</p>
<div><div><img alt="Figure 2.10 – Q1, Q2, and Q3" height="352" src="img/B18945_02_010.jpg" width="606"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Q1, Q2, and Q3</p>
<p>We <a id="_idIndexMarker132"/>can calculate the quartiles and IQR range using <code>numpy</code> and <code>scipy</code>. In the following code example, we use the <code>quantiles</code> function to calculate the quartiles. We will not discuss <code>quantiles</code> here, other than to mention that <code>quantiles</code> are a generalization where the data can be split into any number of equal parts. Since we are splitting the data into four equal parts for quartiles, the <code>quantiles</code> values used for the calculation are 0.25, 0.5, and 0.75. Quartiles Q1 and Q3 could then be used to calculate the IQR. However, we could also use the <code>iqr</code> function from <code>scipy</code> to make the calculation:</p>
<pre class="source-code">
import numpy as np
from scipy import stats
values = [85, 99, 70, 71, 86, 88, 94]
quartiles = np.quantile(values, [0.25, 0.5, 0.75],
    method="closest_observation")
print(f"The quartiles are Q1: {quartiles[0]},
    Q2: {quartiles[1]}, Q3: {quartiles[2]}")
iqr = stats.iqr(values,interpolation='closest_observation')
print(f"The interquartile range is {iqr}")
# The quartiles are Q1: 71, Q2: 85, Q3: 88
# The interquartile range is 17</pre>
<p>Note the use of the <code>method</code> and <code>interpolation</code> keyword arguments in the <code>quantiles</code> function and the <code>iqr</code> function, respectively. Several options can be used for these keyword arguments, which will lead to different results.</p>
<p>Quartiles are often visualized with a boxplot. The following <em class="italic">Figure 2</em><em class="italic">.11</em> shows the main parts of a boxplot. A boxplot is made up of two main parts:</p>
<ul>
<li>The box</li>
<li>The whiskers</li>
</ul>
<p>The <a id="_idIndexMarker133"/>box part represents 50% of the data that is contained by the IQR. The whiskers are drawn starting from the edge of the boxes to a length of k * IQR, where k is commonly chosen to be 1.5. Any values beyond the whiskers are considered outliers.</p>
<div><div><img alt="Figure 2.11 – Parts of a box and whisker plot" height="478" src="img/B18945_02_011.jpg" width="845"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Parts of a box and whisker plot</p>
<p><em class="italic">Figure 2</em><em class="italic">.12</em> shows how histograms and boxplots visualize the variability of a symmetric and asymmetric distribution. Notice how the boxplot of the asymmetric data is compressed on the left and expanded on the right, while the other boxplot is clearly symmetric. While a boxplot is useful for visualizing the symmetry of the data and the presence of outliers, the modality of the distribution would not be evident.</p>
<div><div><img alt="Figure 2.12 – Comparison of boxplots and histograms for asymmetric and symmetric distributions" height="693" src="img/B18945_02_012.jpg" width="726"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Comparison of boxplots and histograms for asymmetric and symmetric distributions</p>
<p>When exploring, it is <a id="_idIndexMarker134"/>common to use multiple visualizations since each type of visualization has its own advantages and disadvantages. It is common to use multiple visualizations since each type of visualization has its own advantages and disadvantages.</p>
<h3>Tukey fences</h3>
<p>In the last<a id="_idIndexMarker135"/> few sections on measurements, the concept of outliers has appeared a few times. Outliers are values that are atypical compared to the main distribution, or anomalous values. While there are methods for classifying data points as outliers, there is no generally robust method for classifying data points as outliers. Defining outliers typically should be informed by the use case of the data analysis, as there will be different factors to consider based on the application domain. However, it is worth mentioning the common technique shown in the boxplot example <a id="_idIndexMarker136"/>called Tukey fences. The lower and upper Tukey fences are based on the IQR and defined as follows:</p>
<ul>
<li>Lower fence : Q1 − k(IQR)</li>
<li>Upper fence : Q3 + k(IQR)</li>
</ul>
<p>As mentioned earlier, k is often chosen to be 1.5 as a default value, but there may be a more appropriate value for a given application domain.</p>
<p>Now let’s take a look at how to calculate Tukey fences with <code>numpy</code> and <code>scipy</code>. This code example will build upon the previous example since there is no function to calculate the fences directly. We will again calculate the quartiles and the IQR with <code>numpy</code> and <code>scipy</code>. Then, we apply these operations to the values listed in the preceding equations:</p>
<pre class="source-code">
import numpy as np
from scipy import stats
values = stats.norm.rvs(10, size=3000)
q1, q3 = np.quantile(values, [.25, .75],
    method='closest_observation')
iqr = stats.iqr(values,interpolation='closest_observation')
lower_fence = q1 - iqr * 1.5
upper_fence = q3 + iqr * 1.5
# may vary due to randomness in data generation
print(f"The lower fence is {lower_fence:.2f} and the upper
    fence is {upper_fence:.2f}")
# The lower fence is 7.36 and the upper fence is 12.67</pre>
<p>In this<a id="_idIndexMarker137"/> case, we used both <code>numpy</code> and <code>scipy</code>; however, the <code>scipy</code> calculation could be replaced with <code>Q3-Q1</code> as mentioned previously.</p>
<h3>Variance</h3>
<p>The last measure<a id="_idIndexMarker138"/> of variability that will be covered in this section is variance. Variance is a measure of dispersion that can be understood as how <em class="italic">spread out</em> the numbers are from the average value. The formula for variance, denoted S 2, is as follows:</p>
<p>S 2 =  ∑ (x i −  _ x ) 2 _ N − 1 </p>
<p>In this equation, the term (x i −  _ x ) is considered the deviation from the mean, which leads to another measure that is closely related to variance – the standard deviation, which is the square root of variance. The formula for standard deviation, denoted σ, is given here:</p>
<p>σ = √ _ S 2  = √ ___________  ∑ (x i −  _ x ) 2 _ N − 1  </p>
<p>In general, a wider distribution will have a larger variance and a larger standard deviation, but these values are not as easy to interpret as a range or IQR. These concepts will be covered more in detail in the next section, in the context of the normal distribution, which will provide clearer intuition for what these values measure.</p>
<p>Again, these values will be calculated with code using <code>numpy</code>. The functions for variance and standard deviation are <code>var</code> and <code>std</code>, respectively:</p>
<pre class="source-code">
import numpy as np
values = [85, 99, 70, 71, 86, 88, 94]
variance = np.var(values)
standard_dev = np.std(values)
print(f"The variance is {variance:.2f} and the standard
    deviation is {standard_dev:.2f}")
# The variance is 101.06 and the standard deviation is 10.05</pre>
<h2 id="_idParaDest-38"><a id="_idTextAnchor042"/>Measuring shape</h2>
<p>The <a id="_idIndexMarker139"/>next type of measure has to do with the shapes <a id="_idIndexMarker140"/>of distributions. They are as follows:</p>
<ul>
<li>Skewness</li>
<li>Kurtosis</li>
</ul>
<p>Let’s discuss each of them.</p>
<h3>Skewness</h3>
<p>The first<a id="_idIndexMarker141"/> measurement is skewness. Put simply, skewness is measurement asymmetry [<em class="italic">1</em>]. An example<a id="_idIndexMarker142"/> of skewed distributions is shown in <em class="italic">Figure 2</em><em class="italic">.13</em>.</p>
<p>There are two types of skewed distributions:</p>
<ul>
<li>Left-skewed</li>
<li>Right-skewed</li>
</ul>
<p>A distribution is skewed in the direction of the dominant tail, meaning that a distribution with a dominant tail to the right is right-skewed and a distribution with a dominant tail to the left is left-skewed (as shown in <em class="italic">Figure 2</em><em class="italic">.13</em>).</p>
<div><div><img alt="Figure 2.13 – Distributions demonstrating skewness" height="1021" src="img/B18945_02_013.jpg" width="636"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Distributions demonstrating skewness</p>
<p>The <a id="_idIndexMarker143"/>formula <a id="_idIndexMarker144"/>for skewness will not be shown here since it can be calculated trivially with modern software packages. The output of the skewness calculation can be used to determine the skewness and the direction of the skew. If the skewness value is 0 or near 0, the distribution does not exhibit strong skewness. If the skewness is positive, the distribution is right-skewed, and if the skewness value is negative, the <a id="_idIndexMarker145"/>distribution is left-skewed. The larger the absolute value of the skewness value, the more the distribution exhibits skewness. An example of how to calculate<a id="_idIndexMarker146"/> skewness with <code>scipy</code> is shown in the following code example:</p>
<pre class="source-code">
from scipy.stats import skewnorm, norm
from scipy.stats import skew as skew_calc
# generate data
skew_left = -skewnorm.rvs(10, size=3000) + 4
skew_right = skewnorm.rvs(10, size=3000) + 3
symmetric = norm.rvs(10, size=3000)
# calculate skewness
skew_left_value = skew_calc(skew_left)
skew_right_value = skew_calc(skew_right)
symmetric_value = skew_calc(symmetric)
# Output may vary some due to randomness of generated data
print(f"The skewness value of this left skewed
    distribution is {skew_left_value:.3f}")
print(f"The skewness value of this right skewed
    distribution is {skew_right_value:.3f}")
print(f"The skewness value of this symmetric distribution
    is {symmetric_value:.3f}")</pre>
<p>The other shape measurement covered in this section is kurtosis.</p>
<h3>Kurtosis</h3>
<p>Kurtosis is a <a id="_idIndexMarker147"/>measurement of how heavy or light the tail of a distribution is relative to the normal distribution [<em class="italic">2</em>]. While the normal distribution <a id="_idIndexMarker148"/>has not been covered in depth yet, the idea of kurtosis can still be discussed. A light-tailed distribution means that more of the data is near or around the mode of the distribution. In contrast, a heavy-tailed distribution means that more of the data is at the edges of the distribution than near the mode. A light-tailed distribution, a normal distribution, and a heavy-tailed distribution are shown in <em class="italic">Figure 2</em><em class="italic">.14</em>.</p>
<div><div><img alt="Figure 2.14 – Distributions demonstrating tailedness with reference to a normal distribution" height="1012" src="img/image_00_014.jpg" width="624"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Distributions demonstrating tailedness with reference to a normal distribution</p>
<p>The<a id="_idIndexMarker149"/> formula<a id="_idIndexMarker150"/> for kurtosis will not be shown here since it can be calculated trivially with modern software packages. If the kurtosis value is 0 or near 0, the distribution does not exhibit kurtosis. If the kurtosis value is negative, the distribution exhibits light-tailedness, and if the kurtosis value is positive, the <a id="_idIndexMarker151"/>distribution exhibits heavy-tailedness. An example of how to calculate kurtosis with <code>scipy</code> is shown in the<a id="_idIndexMarker152"/> following code example:</p>
<pre class="source-code">
from scipy.stats import norm
from scipy.stats import gennorm
from scipy.stats import kurtosis
# generate data
light_tailed = gennorm.rvs(5, size=3000)
symmetric = norm.rvs(10, size=3000)
heavy_tailed = gennorm.rvs(1, size=3000)
# calculate skewness
light_tailed_value = kurtosis(light_tailed)
heavy_tailed_value = kurtosis(heavy_tailed)
symmetric_value = kurtosis(symmetric)
# Output may vary some due to randomness of generated data
print(f"The kurtosis value of this light-tailed
    distribution is {light_tailed_value:.3f}")
print(f"The kurtosis value of this heavy_tailed
    distribution is {heavy_tailed_value:.3f}")
print(f"The kurtosis value of this normal
    distribution is {symmetric_value:.3f}")</pre>
<p>In this section, we walked through the common descriptive statistics that are used for measuring and describing distributions of data. These measurements provide a common language for describing and comparing distributions. The concepts discussed in this chapter are<a id="_idIndexMarker153"/> fundamental to many of the concepts discussed in future chapters. In the next section, we<a id="_idIndexMarker154"/> will discuss the normal distribution and describe the normal distribution using these measurements.</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor043"/>The normal distribution and central limit theorem</h1>
<p>When discussing the <a id="_idIndexMarker155"/>normal distribution, we refer to the bell-shaped, <strong class="bold">standard normal distribution</strong>, which is <a id="_idIndexMarker156"/>formally synonymous with <a id="_idIndexMarker157"/>the <strong class="bold">Gaussian distribution</strong>, named after Carl Friedrich Gauss, an 18th- and 19th-century mathematician and physicist who – among other things – contributed to the concepts of approximation, and, in 1795, invented the method of least squares and the normal distribution, which is commonly used in statistical modeling techniques, such as least squares regression [<em class="italic">3</em>]. The standard normal distribution, also referred to<a id="_idIndexMarker158"/> as a <strong class="bold">parametric</strong> distribution, is characterized by a symmetrical distribution with a probability of data point dispersion consistent around the mean – that is, the data appears near the mean more frequently than data farther away. Since the location data dispersed within this distribution follows the laws of probability, we can call this a <strong class="bold">standard normal probability distribution</strong>. As an<a id="_idIndexMarker159"/> aside, a distribution in statistics that is not a probability distribution is generated through non-probability sampling based on non-random selection, whereas a probability distribution is based on random sampling. Both probability-based and non-probability-based distributions can have a standard normal distribution. The standard normal distribution exhibits neither skew nor kurtosis. It has equal variance throughout and frequently occurs in nature. The <strong class="bold">Empirical Rule</strong> is<a id="_idIndexMarker160"/> used to describe this distribution as having three pertinent standard deviations centered around the mean, <strong class="bold">μ</strong>. There are two distinct assumptions about this distribution:</p>
<ul>
<li>The first, second, and third standard deviations contain 68%, 95%, and 99.7% of the measurements dispersed, respectively</li>
<li>The mean, median, and mode are all equal to each other</li>
</ul>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 2.15 – The standard normal distribution" height="524" src="img/B18945_02_015.jpg" width="978"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – The standard normal distribution</p>
<p>Two <a id="_idIndexMarker161"/>common forms<a id="_idIndexMarker162"/> of a normal distribution are as follows:</p>
<ul>
<li>The probability density distribution</li>
<li>The cumulative density distribution</li>
</ul>
<p>As mentioned before, the <a id="_idIndexMarker163"/>probability density distribution is <a id="_idIndexMarker164"/>based on random sampling, whereas the <a id="_idIndexMarker165"/>cumulative density distribution is<a id="_idIndexMarker166"/> based on accumulated data, which is not necessarily random.</p>
<p>The two-tailed probability density function <a id="_idIndexMarker167"/>of the standard normal distribution is this:</p>
<p>f(x) =  e −(x−μ) 2 _ 2 σ 2  _ σ √ _ 2π  </p>
<p>The <a id="_idIndexMarker168"/>left-tailed cumulative function of the standard normal distribution is this:</p>
<p>f(x) =  ∫ −∞ x e −x 2 _  2 _ √ _ 2π  </p>
<p>With respect to statistical modeling, the normal distribution represents balance and symmetry. This is important when building statistical models as many models assume normal distribution and are not robust to many deviations from that assumption, as they are built around a mean. Consequently, if variables in such a model are not normally <a id="_idIndexMarker169"/>distributed, the model’s errors will be increased and inconsistent, thus diminishing the model’s stability. When considering multiple variables in a statistical model, their interaction is more easily approximated when both are normally distributed.</p>
<p>In the following <em class="italic">Figure 2</em><em class="italic">.16</em>, in the left plot, variables X and Y interact with each other and create a centralized dispersion around a mean. In this case, modeling Y using X with a mean line or linear distance can be done reasonably well. However, if the two variables’ distributions were skewed, as in the plot on the right, this would result in non-constant variance between the two, resulting in an unequal distribution of errors and unreliable output.</p>
<div><div><img alt="Figure 2.16 – Bivariate normal (left) and skewed (right) distributions" height="812" src="img/B18945_02_16.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Bivariate normal (left) and skewed (right) distributions</p>
<p>In the <a id="_idIndexMarker170"/>case of linear classification and regression models, this will mean some results are better than others while some will likely be very bad. This can be difficult to assess at times using basic model metrics and requires deeper model analysis to prevent trusting what could end up being misleading results. Furthermore, deployment into a production environment would be very risky. More on this will be discussed in <a href="B18945_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a>.</p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor044"/>The Central Limit Theorem</h2>
<p>When<a id="_idIndexMarker171"/> sampling data, it is common to encounter the issue of non-normal data. This may be for multiple reasons, such as the population not having a normal distribution or the sample being misrepresentative of the population. The Central Limit Theorem, which is important in statistical inference, postulates that if random samples of <em class="italic">n</em> observations are taken from a population that has a specific mean, μ, and <strong class="bold">standard deviation</strong>, <strong class="bold">σ</strong>, the<a id="_idIndexMarker172"/> sampling distribution constructed from the means of the randomly selected sub-sample distributions will approximate a <a id="_idIndexMarker173"/>normal distribution having roughly the same mean, μ, and standard deviation, calculated as √ _ ∑ (x i − μ) 2 _ N  , as the population. The next section will use bootstrapping to demonstrate the Central Limit Theorem in action. A later section discussing transformations will provide techniques for reshaping data distributions that do not conform to normal distributions so that tools requiring normal distributions can still be effectively applied.</p>
<h1 id="_idParaDest-41"><a id="_idTextAnchor045"/>Bootstrapping</h1>
<p>Bootstrapping <a id="_idIndexMarker174"/>is a method of resampling that uses random sampling – typically with replacement – to generate statistical estimates about a population by resampling from subsets of the sampled distribution, such as the following:</p>
<ul>
<li>Confidence intervals</li>
<li>Standard error</li>
<li>Correlation coefficients (Pearson’s correlation)</li>
</ul>
<p>The idea is that repeatedly sampling different random subsets of a sample distribution and taking the average each time, given enough repeats, will begin to approximate the true population using each subsample’s average. This follows directly the concept of the Central Limit Theorem, which to be restated, asserts that sampling means begins to approximate normal sampling distributions, centered around the original distribution’s mean, as sample sizes and counts increase. Bootstrapping is useful when a limited quantity of samples exists in a distribution relative to the amount needed for a specific test, but inference is needed.</p>
<p>As discussed in <a href="B18945_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>,<em class="italic"> Sampling and Generalization</em>, constraints such as time and expense are common reasons for obtaining samples rather than populations. Because the underlying concept of bootstrapping is to make assumptions about the population using samples, it is not beneficial to apply this technique to populations as the true statistical parameters – such as the percentiles and variance – of a population are known. Regarding sample preparation, the balance of attributes in the sample should represent the true approximation of the population. Otherwise, the results will likely be misleading. For example, if the population of species <a id="_idIndexMarker175"/>within a zoo is a split of 40% reptiles and 60% mammals and we want to bootstrap their longevity to identify the confidence intervals for their lifespans, it would be necessary to ensure the dataset to which bootstrapping was applied contained a split of 40% reptiles and 60% mammals; a split of 15% reptiles and 85% mammals, for example, would lead to misleading results. In other words, the sample stratification should be balanced in proportion to the population.</p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor046"/>Confidence intervals</h2>
<p>As<a id="_idIndexMarker176"/> mentioned before, one useful application of bootstrapping is to create confidence intervals around sparsely defined or limited datasets – that is to say, datasets with a wide range of values without many samples. Consider an example of bootstrapping to perform a hypothesis test using a 95% confidence interval using the <code>"Duncan"</code> dataset in <code>statsmodels</code>, which contains incomes by profession, type, education, and prestige. While this is the full dataset, consider this dataset a sample since the sampling method is not mentioned and it is not likely to consider all incomes for all workers of every profession and type. To obtain the dataset, we first load the <code>matplotlib</code>, <code>statsmodels</code>, <code>pandas</code>, and <code>numpy</code> libraries. We then download the dataset and store it as a <code>pandas</code> DataFrame in the <code>df_duncan</code> variable. Following this, we recode the “<code>prof"</code>, <code>"wc"</code>, and <code>"bc"</code> types as <code>"professional"</code>, <code>"white-collar"</code>, and <code>"blue collar"</code>, respectively. Finally, we create two separate <code>pandas</code> DataFrames; one for professional job types and another for blue-collar job types, as these are the two subsets we will analyze using bootstrapping:</p>
<pre class="source-code">
import matplotlib.pyplot as plt, statsmodels.api as sm, pandas as pd, numpy as np, scipy.stats
df_duncan = sm.datasets.get_rdataset("Duncan",
    "carData").data
df_duncan.loc[df_duncan['type'] == 'prof',
    'type'] = 'professional'
df_duncan.loc[df_duncan['type'] == 'wc',
    'type'] = 'white-collar'
df_duncan.loc[df_duncan['type'] == 'bc',
    'type'] = 'blue-collar'
df_professional = df_duncan.loc[(
    df_duncan['type'] == 'professional')]
df_blue_collar = df_duncan.loc[(
    df_duncan['type'] == 'blue-collar')]</pre>
<div><div><img alt="Figure 2.17 – Table displaying the first five rows of the statsmodels Duncan data" height="575" src="img/B18945_02_017.jpg" width="1344"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Table displaying the first five rows of the statsmodels Duncan data</p>
<p>We then<a id="_idIndexMarker177"/> build a set of plotting functions, as seen next. In <code>plot_distributions()</code>, we denote <code>p=5</code>, meaning the p-value will be significant at a significance level of 0.05 (1.00 - 0.05 = 0.95, hence, 95% confidence). We then divide this value by 2 since this will be a two-sided test, meaning we want to know the full interval rather than just one bound (discussed in <a href="B18945_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a> as a representative test statistic). Regarding the plots, we visualize the data using histograms (the <code>hist()</code> function) in <code>matplotlib</code> and then plot the 95% sampling confidence intervals using the <code>axvline()</code> functions, which we build using the <code>numpy</code> function <code>percentile()</code>.</p>
<p class="callout-heading">Percentile in Bootstrapping</p>
<p class="callout">When applied to the original data, the percentile is only that, but when applied to the bootstrapped sampling distribution, it is the confidence interval.</p>
<p>To state the<a id="_idIndexMarker178"/> confidence interval simply, a 95% confidence interval means that for every 100 sample means taken, 95 of them will fall within this interval. In the <code>numpy</code> <code>percentile()</code> function, we use <code>p=5</code> to support that 1-p is the confidence level, where <em class="italic">p</em> is the level of significance (think <em class="italic">p-value</em>, where any value at or lower than <em class="italic">p</em> is significant). Since the test is two-tailed, we divide <em class="italic">p</em> by 2 and split 2.5 in the left tail and 2.5 in the right since we have a symmetrical, standard normal distribution. The <code>subplot(2,1,...)</code> code creates two rows and one column. Axis 0 of the figure is used for professional incomes and axis 1 is used for blue-collar incomes:</p>
<pre class="source-code">
def plot_distributions(n_replicas, professional_sample, blue_collar_sample, professional_label, blue_collar_label, p=5):
    fig, ax = plt.subplots(2, 1, figsize=(10,8))
    ax[0].hist(professional_sample, alpha=.3, bins=20)
    ax[0].axvline(professional_sample.mean(),
        color='black', linewidth=5)
# sampling distribution mean
    ax[0].axvline(np.percentile(professional_sample, p/2.),
        color='red', linewidth=3, alpha=0.99)
# 95% CI Lower limit (if bootstrapping)
    ax[0].axvline(np.percentile(professional_sample,
        100-p/2.), color='red', linewidth=3, alpha=0.99)
# 95% CI Upper Limit  (if bootstrapping)
    ax[0].title.set_text(str(professional_label) +
        "\nn = {} Resamples".format(n_replicas))
    ax[1].hist(blue_collar_sample, alpha=.3, bins=20)
    ax[1].axvline(blue_collar_sample.mean(), color='black',
        linewidth=5) # sampling distribution mean
    ax[1].axvline(np.percentile(blue_collar_sample, p/2.),
        color='red', linewidth=3, alpha=0.99)
# 95% CI Lower limit (if bootstrapping)
    ax[1].axvline(np.percentile(blue_collar_sample,
        100-p/2.), color='red', linewidth=3, alpha=0.99)
# 95% CI Upper Limit (if bootstrapping)
    ax[1].title.set_text(str(blue_collar_label) +
        "\nn = {} Resamples".format(n_replicas))
    if n_replicas &gt; 1:
        print("Lower confidence interval limit: ",
            np.percentile(round(professional_sample,4),
            p/2.))
        print("Upper confidence interval limit: ",
            np.percentile(round(professional_sample,4),
            100-p/2.))
        print("Mean: ", round(professional_sample,
            4).mean())
        print("Standard Error: ",
            round(professional_sample.std() /
            np.sqrt(n_replicas), 4) )
        print("Lower confidence interval limit: ",
            np.percentile(round(blue_collar_sample,4),
            p/2.))
        print("Upper confidence interval limit: ",
            np.percentile(round(blue_collar_sample,4),
            100-p/2.))
        print("Mean: ", round(blue_collar_sample,4).mean())
        print("Standard Error: ",
            round(blue_collar_sample.std() /
            np.sqrt(n_replicas), 4) )
    else:
        print("At least two samples required to create the following statistics:\nConfidence Intervals\nMean\nStandard Error")</pre>
<p>In the original <a id="_idIndexMarker179"/>dataset, there are 18 income data points for <code>professional</code> job types and 21 data points for <code>blue-collar</code> job types. The 95% confidence interval for the professional job type ranges from 29.50 to 79.15 with an average of 60.06. That interval ranges from 7.00 to 64.00 for blue-collar job types with a mean of 23.76. Based on <em class="italic">Figure 2</em><em class="italic">.18</em>, there is a reasonable overlap between the income differences, which causes the overlapping confidence intervals. Consequently, it<a id="_idIndexMarker180"/> would be reasonable to assume there is no statistically significant difference in incomes between blue-collar and professional job types. However, this dataset has a very limited volume of samples:</p>
<pre class="source-code">
n_replicas=0
plot_distributions(n_replicas=n_replicas,
professional_sample=df_professional['income'],
    blue_collar_sample=df_blue_collar['income'],
    professional_label="Professional",
    blue_collar_label="Blue Collar")</pre>
<div><div><img alt="Figure 2.18 – Original data distributions with 95th percentile lines" height="660" src="img/B18945_02_018.jpg" width="825"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.18 – Original data distributions with 95th percentile lines</p>
<p>In the following code, using <code>pandas</code>’ <code>.sample()</code> function, we randomly resample 50% (<code>frac=0.5</code>) of the income values from each distribution 1,000 times and calculate a new mean each time, appending it to the Python lists ending with <code>_bootstrap_means</code>. Using those lists, we derive new 95% confidence intervals. <em class="italic">Figure 2</em><em class="italic">.19</em> shows, with respect to the standard deviations and income values in the dataset, the new sample distributions using the average of each resampled subset. The <code>replace=True</code> argument allows for resampling the same record multiple times (in the event that should randomly occur), which is a requirement of bootstrapping.</p>
<p>After<a id="_idIndexMarker181"/> performing the bootstrapping procedure, we can see income has started to distribute in a roughly standard normal, Gaussian form. Notably, from this experiment, the confidence intervals no longer overlap. The implication of the separation of the confidence intervals between the professional and blue-collar groups is that with a 95% level of confidence, it can be shown there is a statistically significant difference between the incomes of the two job types. The confidence interval for the professional income levels is now 48.66 to 69.89 with a mean of 60.04, and for blue-collar, 14.60 to 35.90 with a mean of 23.69:</p>
<pre class="source-code">
n_replicas = 1000
professional_bootstrap_means = pd.Series(
    [df_professional.sample(frac=0.5, replace=True)
    ['income'].mean() for i in range(n_replicas)])
blue_collar_bootstrap_means = pd.Series(
    [df_blue_collar.sample(frac=0.5, replace=True)
    ['income'].mean() for i in range(n_replicas)])</pre>
<div><div><img alt="Figure 2.19 – Distributions of the 95% confidence interval for 1,000 bootstrapped sampling means" height="660" src="img/B18945_02_019.jpg" width="825"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.19 – Distributions of the 95% confidence interval for 1,000 bootstrapped sampling means</p>
<p>Here, you <a id="_idIndexMarker182"/>can notice the distribution more closely clusters around the mean with tighter confidence intervals.</p>
<p>As mentioned before, bootstrapping can be used to obtain different statistical parameters of the distribution beyond the confidence intervals.</p>
<h2 id="_idParaDest-43"><a id="_idTextAnchor047"/>Standard error</h2>
<p>Another <a id="_idIndexMarker183"/>commonly used metric is the standard error,  σ _ √ _ n . We  can calculate this using the last variables, <code>p</code><code>rofessional_bootstrap_means</code>, and <code>blue_collar_bootstrap_means</code>, as these contain the new distributions of means obtained through the bootstrapping process. We can also see that standard error – calculated by dividing the standard deviation by the square root of the number of samples (or in our case, <code>n_replicas</code>, representing the count of averages obtained from each random re-subsample) – decreases as the volume resamples increases. We use the following code to calculate the standard error of the professional and blue-collar type income bootstrapped means. The following table, <em class="italic">Figure 2</em><em class="italic">.20</em>, shows that the standard error reduces as <em class="italic">n</em> increases:</p>
<pre class="source-code">
scipy.stats.sem(professional_bootstrap_means)
scipy.stats.sem(blue_collar_bootstrap_means)</pre>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">n</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Professional </strong><strong class="bold">Standard Error</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Blue-Collar </strong><strong class="bold">Standard Error</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>10 replicas</p>
</td>
<td class="No-Table-Style">
<p>0.93</p>
</td>
<td class="No-Table-Style">
<p>2.09</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>10,000 replicas</p>
</td>
<td class="No-Table-Style">
<p>0.03</p>
</td>
<td class="No-Table-Style">
<p>0.04</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.20 – Table of standard errors for n = 10 and n = 10,000 bootstrap replicas</p>
<p>Another <a id="_idIndexMarker184"/>use case for bootstrapping is Pearson’s correlation, which we will discuss in the following section.</p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor048"/>Correlation coefficients (Pearson’s correlation)</h2>
<p>Typically, this<a id="_idIndexMarker185"/> is difficult to find using a small sample size since correlation depends on the covariance of two variables. As <a id="_idIndexMarker186"/>the variables overlap more significantly, their correlation is higher. However, if the overlap is the result of a small sample size or sampling error, this correlation may be representative. <em class="italic">Figure 2</em><em class="italic">.21</em> shows a table of correlation at different counts of bootstrap subsamples. As the distributions form more native distinctions, the correlation diminishes from a small positive correlation to an amount approximating zero.</p>
<p>To test correlation on a sample of 10 records from the original dataset, see the following:</p>
<pre class="source-code">
df_prof_corr = df_professional.sample(n=10)
df_blue_corr = df_blue_collar.sample(n=10)
corr, _ = scipy.stats.pearsonr(df_prof_corr['income'],
    df_blue_corr['income'])</pre>
<p>To test correlation on samples of bootstrapped means:</p>
<pre class="source-code">
n_replicas = n_replicas
professional_bootstrap_means = pd.Series([df_prof_corr.sample(frac=0.5,replace=False).income.mean()for i in range(n_replicas)])
blue_collar_bootstrap_means = pd.Series([df_blue_corr.sample(frac=0.5, replace=False).income.mean() for i in range(n_replicas)])
corr, _ = scipy.stats.pearsonr(
    professional_bootstrap_means,
    blue_collar_bootstrap_means)
print(corr)</pre>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">n</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Pearson’s </strong><strong class="bold">Correlation Coefficient</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>10 samples from original data</p>
</td>
<td class="No-Table-Style">
<p>0.32</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>10 replicas</p>
</td>
<td class="No-Table-Style">
<p>0.22</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>10,000 replicas</p>
</td>
<td class="No-Table-Style">
<p>-0.003</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.21 – Table of Pearson’s correlation coefficients alongside the original samples</p>
<p>It is <a id="_idIndexMarker187"/>common <a id="_idIndexMarker188"/>to run around 1,000 to 10,000 bootstrap replicas. However, this depends on the type of data being bootstrapped. For example, if bootstrapping data from a human genome sequence dataset, it may be useful to bootstrap a sample 10 million times, but if bootstrapping a simple dataset, it may be useful to bootstrap 1,000 times or less. Ultimately, the researcher should perform a visual inspection of the distributions of the <a id="_idIndexMarker189"/>means to determine whether the results appear logical compared to what is expected. As common with <a id="_idIndexMarker190"/>statistics, it is best to have some domain knowledge or subject-matter expertise to help validate findings, as this will likely be the best for deciding bootstrap replication counts.</p>
<p>Bootstrapping is also used in machine learning, where it underlies the concept of <strong class="bold">bootstrap aggregation</strong>, also <a id="_idIndexMarker191"/>called <strong class="bold">bagging</strong>, a process that combines outputs of <a id="_idIndexMarker192"/>predictive models built upon bootstrap subsample distributions. <strong class="bold">Random Forest</strong> is <a id="_idIndexMarker193"/>one popular algorithm that performs this operation. The purpose of bootstrapping in bagging algorithms is to preserve the low-bias behavior of non-parametric (more to be discussed on this in later chapters) classification, but also reduce variance, thus using bootstrapping as a way to minimize the significance of the bias-variance trade-off in modeling errors.</p>
<p>In the following section, we will consider another non-parametric test called permutation testing using resampling data.</p>
<h1 id="_idParaDest-45"><a id="_idTextAnchor049"/>Permutations</h1>
<p>Before jumping<a id="_idIndexMarker194"/> into this testing analysis, we will review some basic knowledge of permutations and combinations.</p>
<h2 id="_idParaDest-46"><a id="_idTextAnchor050"/>Permutations and combinations</h2>
<p>Permutations and combinations <a id="_idIndexMarker195"/>are two mathematical techniques for taking a set of objects to create subsets from a population but in two different ways. The order of objects matters in permutations but does not matter in combinations.</p>
<p>In order to understand these concepts easily, we will consider two examples. There are 10 people at an evening party. The organizer of the party wants to give 3 prizes of $1,000, $500, and $200 randomly to 3 people. The question is <em class="italic">how many ways are there to distribute the prizes?</em> Another example is that the organizer will give 3 equal prizes of $500 to 3 people out of 10 at the party. The organizer really does not care which prize is<a id="_idIndexMarker196"/> given to whom among the 3 selected people. Huy, Paul, and Stuart are our winners in <a id="_idIndexMarker197"/>these two examples but, in the first example, different situations may play out, for instance, if Paul wins the $200 prize, $500 prize, or $1,000 prize.</p>
<table class="No-Table-Style" id="table003">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">$</strong><strong class="bold">1,000</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">$</strong><strong class="bold">500</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">$</strong><strong class="bold">200</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Huy</p>
</td>
<td class="No-Table-Style">
<p>Paul</p>
</td>
<td class="No-Table-Style">
<p>Stuart</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Paul</p>
</td>
<td class="No-Table-Style">
<p>Huy</p>
</td>
<td class="No-Table-Style">
<p>Stuart</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Paul</p>
</td>
<td class="No-Table-Style">
<p>Stuart</p>
</td>
<td class="No-Table-Style">
<p>Huy</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Huy</p>
</td>
<td class="No-Table-Style">
<p>Stuart</p>
</td>
<td class="No-Table-Style">
<p>Paul</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Stuart</p>
</td>
<td class="No-Table-Style">
<p>Huy</p>
</td>
<td class="No-Table-Style">
<p>Paul</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Stuart</p>
</td>
<td class="No-Table-Style">
<p>Paul</p>
</td>
<td class="No-Table-Style">
<p>Huy</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.22 – Table of distributed prizes given to Huy, Paul, and Stuart</p>
<p>However, in the second example, because the 3 prizes have the same value of $500, the order of prize arrangements does not matter.</p>
<p>Let us take a closer look at these two permutations and combinations examples. The first example is a permutation example. Since the pool has 10 people, we have 10 possibilities in choosing one person from the pool to give the $1,000 prize. If this person is chosen to win the $1,000 prize, then there are only 9 possibilities in choosing another person to give the $500 prize, and finally, we have 8 possibilities in choosing a person from the pool to give the $200 prize. Then, we have 10*9*8 = 720 ways to distribute the prizes. The mathematical formula for the permutations is this:</p>
<p>P(n, r) =  n ! _ (n − r) ! </p>
<p>Here, P(n, r) is the number of permutations, n is the total number of objects in a set, and r is the number of objects that can be chosen from the set. In this example, n = 10 and r = 3 so then we see this:</p>
<p>P(10,3) =  10 ! _ (10 − 3) ! = 10*9 * 8*7 * 6*5 * 4*3 * 2*1  ____________  7*6 * 5*4 * 3*2 * 1  = 10*9 * 8 = 720</p>
<p>There<a id="_idIndexMarker198"/> are 720 ways to select 3 people from the 10 people at the party to whom to distribute the 3 prizes of $1,000, $500, and $200.</p>
<p>In Python, there<a id="_idIndexMarker199"/> is a package called <code>itertools</code> to help us to find permutations directly. Readers can<a id="_idIndexMarker200"/> check out the following link – <a href="https://docs.python.org/3/library/itertools.xhtml">https://docs.python.org/3/library/itertools.xhtml</a> – for more information related to this package. We need to import this package into the Python environment for permutations:</p>
<pre class="source-code">
from itertools import permutations
# list of 10 people in the party
people = ['P1','P2','P3','P4','P5','P6','P7','P8','P9','P10']
# all the ways that the 3 prizes are distributed
perm = permutations(people, 3)
list_perm = list(perm)
print(f"There are {len(list_perm)} ways to distribute the prizes!")</pre>
<p>In the preceding Python code, we created a list, <code>people</code>, containing 10 people, <code>P1</code> to <code>P10</code>, and then use the <code>permutations</code> function from <code>itertools</code> to get all the ways to distribute the prizes. This method takes a list of 10 people as input and returns an object list of tuples containing all the possibilities in choosing 3 people from this pool of 10 people to whom to distribute the prizes of $1,000, $500, and $200. Because there are 720 ways to distribute the prizes, here we will just print the 10 first ways that the Python code produced:</p>
<pre class="source-code">
print(f"The 10 first ways to distribute the prizes: \n
    {list_perm[:10]} ")</pre>
<p>The output <a id="_idIndexMarker201"/>of the preceding code is the 10 first ways to distribute the prizes:</p>
<p><code>[('P1', 'P2', 'P3'), ('P1', 'P2', 'P4'), ('P1', 'P2', 'P5'), ('P1', 'P2', 'P6'), ('P1', '</code><code>P2', 'P7')]</code></p>
<p>If we have 10 different gifts, each<a id="_idIndexMarker202"/> person who participates in the party can take one gift home. How many ways are there to distribute these gifts? There are 3,628,800 ways. That is a really big number! The reader can check with the following code:</p>
<pre class="source-code">
#list of 10 people in the party
people = ['P1','P2','P3','P4','P5','P6','P7','P8','P9','P10']
# all the ways that the 10 different gifts are distributed
perm = permutations(people)
list_perm = list(perm)
print(f"There are {len(list_perm)}
    ways to distributed the gifts!")</pre>
<p>Going back to the second example, because the 3 prizes have the same value of $500, the order of the 3 selected people does not matter. Then, if the 3 selected people are Huy, Paul, and Stuart, as in <em class="italic">Figure 2</em><em class="italic">.22</em>, there are 6 ways to distribute the prizes in the first example. Then, there is only 1 way to distribute the same amount of $500 to Huy, Paul, and Stuart. The mathematical formula of combinations is this:</p>
<p>C(n, r) =  n ! _ r !(n − r) ! </p>
<p>Here, C(n, r) is the <a id="_idIndexMarker203"/>number of combinations, n is the total number of objects in a set, and r is the number of objects that can be chosen from the set. Similarly, we can calculate that there are</p>
<p> 10 ! _ 3 !(10 − 3) !  =  10.9 . 8 _ 1.2 . 3  =  720 _ 6  = 120</p>
<p>ways to distribute 3 prizes of $500.</p>
<p>In Python, we also<a id="_idIndexMarker204"/> use the <code>itertools</code> package but, instead of the <code>permutations</code> function, we import the <code>combinations</code> function:</p>
<pre class="source-code">
 from itertools import combinations
# list of 10 people in the party
people = ['P1','P2','P3','P4','P5','P6','P7','P8','P9','P10']
# all the ways that the 3 prizes are distributed
comb = combinations(people, 3)
list_comb = list(comb)
print(f"There are {len(list_comb)} ways to distribute the prizes!")</pre>
<h2 id="_idParaDest-47"><a id="_idTextAnchor051"/>Permutation testing</h2>
<p>Permutation testing<a id="_idIndexMarker205"/> is a non-parametric test that does not make the required assumption of normally distributed data. Both bootstrapping and permutations are useful for resampling techniques but best for different uses, one for estimating statistical parameters (bootstrapping) and another for hypothesis testing. Permutation testing is used to test the null hypothesis between two samples generated from the same population. It has different <a id="_idIndexMarker206"/>names such as <strong class="bold">exact testing</strong>, <strong class="bold">randomization testing</strong>, and <strong class="bold">re-randomization testing</strong>.</p>
<p>First, we <a id="_idIndexMarker207"/>go to<a id="_idIndexMarker208"/> see a simple <a id="_idIndexMarker209"/>example for better understanding before implementing the code in Python. We suppose that there are 2 groups of people, one group representing children (A) and another group representing people over 40 years old (B) as follows:</p>
<p><code>A</code> = [3,5,4] and <code>B</code> = [43,41,56,78,54]</p>
<p>The mean difference in age between the two samples A and B is</p>
<p> 43 + 41 + 56 + 78 + 54  ________________ 5  −  3 + 5 + 4 _ 3  = 50.4</p>
<p>We merge A and B into a single set, denoted as P as follows:</p>
<p><code>P = </code>[3,5, 4,43,41,56,78,54].</p>
<p>Then, we take a permutation of P, for example, the following:</p>
<p><code>P_new = [3,54, 78, 41, 4, 43, </code><code>5, 56]</code></p>
<p>Next, we redivide <code>P_new</code> into 2 subsets called <code>A_new</code> and <code>B_new</code>, which have the same size as A and B, respectively:</p>
<p><code>A_new</code> = [3,54,78] and <code>B_new</code> = [41,4,43,5,56]</p>
<p>Then, the mean difference in age between <code>A_new</code> and <code>B_new</code> is 15.2, which is lower than the original mean difference in age between A and B (50.4). In other words, the permutated <code>P_new</code> does not contribute to the p-value. We can observe that only one permutation drawn from all possible permutations of P is greater than or equal to the original mean difference itself, P. Now we will implement the code in Python:</p>
<pre class="source-code">
import numpy as np
# create permutation testing function
def permutation_testing(A,B,n_iter=1000):
#A, B are 2 lists of samples to test the hypothesis,
#n_iter is number of iterations with the default is 1000
    differences = []
    P = np.array(A+B)
    original_mean = np.array(A).mean()- np.array(B).mean()
    for i in range(n_iter):
      np.random.shuffle(P)#create a random permutation of P
      A_new = P[:len(A)] # having the same size of A
      B_new = P[-len(B):] # having the same size of B
      differences.append(A_new.mean()-B_new.mean())
    #Calculate p_value
    p_value = round(1-(float(len(np.where(
        differences&lt;=original_mean)[0]))/float(n_iter)),2)
    return p_value</pre>
<p>In the preceding<a id="_idIndexMarker210"/> Python code, A and B are two samples and we want to know whether they are from the same larger population; <code>n_ter</code> is the number of iterations that we want to perform; here, 1,000 is the default number of iterations.</p>
<p>Let’s perform permutation testing for the two groups of people in the example with 10,000 iterations:</p>
<pre class="source-code">
A = [3,5,4]
B = [43,41,56,78,54]
permutation_testing(A,B,n_iter=10000)</pre>
<p>The p-value obtained is 0.98. That means that we fail to reject the null hypothesis, or there is not <a id="_idIndexMarker211"/>enough evidence to confirm that samples A and B are from the same larger population.</p>
<p>Next, we will explore an important and necessary step in many statistical tests requiring the normal distribution assumption.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor052"/>Transformations</h1>
<p>In this section, we <a id="_idIndexMarker212"/>will consider three transformations:</p>
<ul>
<li>Log transformation</li>
<li>Square root transformation</li>
<li>Cube root transformation</li>
</ul>
<p>First, we will <a id="_idIndexMarker213"/>import the <code>numpy</code> package to create a random <a id="_idIndexMarker214"/>sample drawn <a id="_idIndexMarker215"/>from a Beta distribution. The documentation on Beta distributions can be found here:</p>
<p><a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.beta.xhtml">https://numpy.org/doc/stable/reference/random/generated/numpy.random.beta.xhtml</a></p>
<p>The sample, <code>df</code>, has 10,000 values. We also use <code>matplotlib.pyplot</code> to create different histogram plots. Second, we transform the original data by using a log transformation, square root transformation, and cube root transformation, and we draw four histograms:</p>
<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
np.random.seed(42) # for reproducible purpose
# create a random data
df = np.random.beta(a=1, b=10, size = 10000)
df_log = np.log(df) #log transformation
df_sqrt = np.sqrt(df) # Square Root transformation
df_cbrt = np.cbrt(df) # Cube Root transformation
plt.figure(figsize = (10,10))
plt.subplot(2,2,1)
plt.hist(df)
plt.title("Original Data")
plt.subplot(2,2,2)
plt.hist(df_log)
plt.title("Log Transformation")
plt.subplot(2,2,3)
plt.hist(df_sqrt)
plt.title("Square Root Transformation")
plt.subplot(2,2,4)
plt.hist(df_cbrt)
plt.title("Cube Root Transformation")
plt.show()</pre>
<p>The following is the<a id="_idIndexMarker216"/> output of the code:</p>
<div><div><img alt="Figure 2.23 – Histograms of the original and transformed data" height="1044" src="img/B18945_02_023.jpg" width="1063"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.23 – Histograms of the original and transformed data</p>
<p>Using <a id="_idIndexMarker217"/>transformation, we<a id="_idIndexMarker218"/> can see the transformed histograms are more normally distributed than the original one. It seems that the best transformation in this example is cube root transformation. With real-world data, it is important to determine whether a transformation is needed, and, if so, which transformation should be used.</p>
<p>Other data transformation methods, for example, finding duplicate data, dealing with missing values, and feature scaling will be discussed in hands-on, real-world use cases in Python in the following chapters.</p>
<h1 id="_idParaDest-49"><a id="_idTextAnchor053"/>Summary</h1>
<p>In the first section of this chapter, we learned about types of data and how to visualize these types of data. Then, we covered how to describe and measure attributes of data distribution. We learned about the standard normal distribution, why it’s important, and how the central limit theorem is applied in practice by demonstrating bootstrapping. We also learned how bootstrapping can make use of non-normally distributed data to test hypotheses using confidence intervals. Next, we covered mathematical knowledge as permutations and combinations and introduced permutation testing as another non-parametric test in addition to bootstrapping. We finished the chapter with different data transformation methods that are useful in many situations when performing statistical tests requiring normally distributed data.</p>
<p>In the next chapter, we will take a detailed look at hypothesis testing and discuss how to draw statistical conclusions from the results of the tests. We will also look at errors that can occur in statistical tests and how to select statistical power.</p>
<h1 id="_idParaDest-50"><a id="_idTextAnchor054"/>References</h1>
<ul>
<li>[<em class="italic">1</em>] Skewness – <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm">https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm</a></li>
<li>[<em class="italic">2</em>] Kurtosis – <a href="https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Kurtosis%20is%20a%20measure%20of,would%20be%20the%20extreme%20case">https://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm#:~:text=Kurtosis%20is%20a%20measure%20of,would%20be%20the%20extreme%20case</a>.</li>
<li>[<em class="italic">3</em>] Normal Distribution – <em class="italic">C.F. GAUSS AND THE METHOD OF LEAST SQUARES</em>, <em class="italic">ŚLĄSKI PRZEGLĄD STATYSTYCZNY Silesian Statistical Review</em>, Nr 12(18), O. Sheynin, Sep. 1999</li>
</ul>
</div>
</div></body></html>