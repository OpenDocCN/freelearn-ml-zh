- en: Chapter 1. Introducing the Probability Theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian inference is a method of learning about the relationship between variables
    from data, in the presence of uncertainty, in real-world problems. It is one of
    the frameworks of probability theory. Any reader interested in Bayesian inference
    should have a good knowledge of probability theory to understand and use Bayesian
    inference. This chapter covers an overview of probability theory, which will be
    sufficient to understand the rest of the chapters in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'It was Pierre-Simon Laplace who first proposed a formal definition of probability
    with mathematical rigor. This definition is called the *Classical Definition*
    and it states the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | *The theory of chance consists in reducing all the events of the same
    kind to a certain number of cases equally possible, that is to say, to such as
    we may be equally undecided about in regard to their existence, and in determining
    the number of cases favorable to the event whose probability is sought. The ratio
    of this number to that of all the cases possible is the measure of this probability,
    which is thus simply a fraction whose numerator is the number of favorable cases
    and whose denominator is the number of all the cases possible.* |   |'
  prefs: []
  type: TYPE_TB
- en: '|   | --*Pierre-Simon Laplace, A Philosophical Essay on Probabilities* |'
  prefs: []
  type: TYPE_TB
- en: 'What this definition means is that, if a random experiment can result in ![Introducing
    the Probability Theory](img/image00166.jpeg) mutually exclusive and equally likely
    outcomes, the probability of the event ![Introducing the Probability Theory](img/image00167.jpeg)
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing the Probability Theory](img/image00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Introducing the Probability Theory](img/image00169.jpeg) is the number
    of occurrences of the event ![Introducing the Probability Theory](img/image00167.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this concept, let us take a simple example of a rolling dice.
    If the dice is a fair dice, then all the faces will have an equal chance of showing
    up when the dice is rolled. Then, the probability of each face showing up is 1/6\.
    However, when one rolls the dice 100 times, all the faces will not come in equal
    proportions of 1/6 due to random fluctuations. The estimate of probability of
    each face is the number of times the face shows up divided by the number of rolls.
    As the denominator is very large, this ratio will be close to 1/6.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the long run, this classical definition treats the probability of an uncertain
    event as the relative frequency of its occurrence. This is also called a **frequentist**
    approach to probability. Although this approach is suitable for a large class
    of problems, there are cases where this type of approach cannot be used. As an
    example, consider the following question: *Is Pataliputra the name of an ancient
    city or a king?* In such cases, we have a degree of belief in various plausible
    answers, but it is not based on counts in the outcome of an experiment (in the
    Sanskrit language *Putra* means son, therefore some people may believe that Pataliputra
    is the name of an ancient king in India, but it is a city).'
  prefs: []
  type: TYPE_NORMAL
- en: Another example is, *What is the chance of the Democratic Party winning the
    election in 2016 in America?* Some people may believe it is 1/2 and some people
    may believe it is 2/3\. In this case, probability is defined as the **degree of
    belief** of a person in the outcome of an uncertain event. This is called the
    **subjective** definition of probability.
  prefs: []
  type: TYPE_NORMAL
- en: One of the limitations of the classical or frequentist definition of probability
    is that it cannot address subjective probabilities. As we will see later in this
    book, Bayesian inference is a natural framework for treating both frequentist
    and subjective interpretations of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In both classical and Bayesian approaches, a probability distribution function
    is the central quantity, which captures all of the information about the relationship
    between variables in the presence of uncertainty. A probability distribution assigns
    a probability value to each measurable subset of outcomes of a random experiment.
    The variable involved could be discrete or continuous, and univariate or multivariate.
    Although people use slightly different terminologies, the commonly used probability
    distributions for the different types of random variables are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Probability mass function** (**pmf**) for discrete numerical random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical distribution** for categorical random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability density function** (**pdf**) for continuous random variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the well-known distribution functions is the normal or Gaussian distribution,
    which is named after Carl Friedrich Gauss, a famous German mathematician and physicist.
    It is also known by the name *bell curve* because of its shape. The mathematical
    form of this distribution is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00170.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Probability distributions](img/image00171.jpeg) is the mean or location
    parameter and ![Probability distributions](img/image00172.jpeg) is the standard
    deviation or scale parameter (![Probability distributions](img/image00173.jpeg)
    is called variance). The following graphs show what the distribution looks like
    for different values of location and scale parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00174.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: One can see that as the mean changes, the location of the peak of the distribution
    changes. Similarly, when the standard deviation changes, the width of the distribution
    also changes.
  prefs: []
  type: TYPE_NORMAL
- en: Many natural datasets follow normal distribution because, according to the **central
    limit theorem**, any random variable that can be composed as a mean of independent
    random variables will have a normal distribution. This is irrespective of the
    form of the distribution of this random variable, as long as they have finite
    mean and variance and all are drawn from the same original distribution. A normal
    distribution is also very popular among data scientists because in many statistical
    inferences, theoretical results can be derived if the underlying distribution
    is normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us look at the multidimensional version of normal distribution. If
    the random variable is an N-dimensional vector, *x* is denoted by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the corresponding normal distribution is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Probability distributions](img/image00177.jpeg) corresponds to the mean
    (also called location) and ![Probability distributions](img/image00178.jpeg) is
    an *N x N* covariance matrix (also called scale).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of the multidimensional normal distribution,
    let us take the case of two dimensions. In this case, ![Probability distributions](img/image00179.jpeg)
    and the covariance matrix is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00180.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Probability distributions](img/image00181.jpeg) and ![Probability distributions](img/image00182.jpeg)
    are the variances along ![Probability distributions](img/image00183.jpeg) and
    ![Probability distributions](img/image00184.jpeg) directions, and ![Probability
    distributions](img/image00185.jpeg) is the correlation between ![Probability distributions](img/image00183.jpeg)
    and ![Probability distributions](img/image00184.jpeg). A plot of two-dimensional
    normal distribution for ![Probability distributions](img/image00186.jpeg), ![Probability
    distributions](img/image00187.jpeg), and ![Probability distributions](img/image00188.jpeg)
    is shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If ![Probability distributions](img/image00190.jpeg), then the two-dimensional
    normal distribution will be reduced to the product of two one-dimensional normal
    distributions, since ![Probability distributions](img/image00178.jpeg) would become
    diagonal in this case. The following 2D projections of normal distribution for
    the same values of ![Probability distributions](img/image00181.jpeg) and ![Probability
    distributions](img/image00182.jpeg) but with ![Probability distributions](img/image00188.jpeg)
    and ![Probability distributions](img/image00191.jpeg) illustrate this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/image00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The high correlation between *x* and *y* in the first case forces most of the
    data points along the 45 degree line and makes the distribution more anisotropic;
    whereas, in the second case, when the correlation is zero, the distribution is
    more isotropic.
  prefs: []
  type: TYPE_NORMAL
- en: We will briefly review some of the other well-known distributions used in Bayesian
    inference here.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, one would be interested in finding the probability of the occurrence
    of a set of random variables when other random variables in the problem are held
    fixed. As an example of population health study, one would be interested in finding
    what is the probability of a person, in the age range 40-50, developing heart
    disease with high blood pressure and diabetes. Questions such as these can be
    modeled using conditional probability, which is defined as the probability of
    an event, given that another event has happened. More formally, if we take the
    variables *A* and *B*, this definition can be rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional probability](img/image00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional probability](img/image00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following Venn diagram explains the concept more clearly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional probability](img/image00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In Bayesian inference, we are interested in conditional probabilities corresponding
    to multivariate distributions. If ![Conditional probability](img/image00196.jpeg)
    denotes the entire random variable set, then the conditional probability of ![Conditional
    probability](img/image00197.jpeg), given that ![Conditional probability](img/image00198.jpeg)
    is fixed at some value, is given by the ratio of joint probability of ![Conditional
    probability](img/image00196.jpeg) and joint probability of ![Conditional probability](img/image00198.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional probability](img/image00199.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the case of two-dimensional normal distribution, the conditional probability
    of interest is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional probability](img/image00200.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It can be shown that (exercise 2 in the *Exercises* section of this chapter)
    the RHS can be simplified, resulting in an expression for ![Conditional probability](img/image00201.jpeg)
    in the form of a normal distribution again with the mean ![Conditional probability](img/image00202.jpeg)
    and variance ![Conditional probability](img/image00203.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the definition of the conditional probabilities ![Bayesian theorem](img/image00204.jpeg)
    and ![Bayesian theorem](img/image00205.jpeg), it is easy to show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian theorem](img/image00206.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Rev. Thomas Bayes (1701–1761) used this rule and formulated his famous Bayes
    theorem that can be interpreted if ![Bayesian theorem](img/image00207.jpeg) represents
    the initial degree of belief (or prior probability) in the value of a random variable
    *A* before observing *B*; then, its posterior probability or degree of belief
    after accounted for *B* will get updated according to the preceding equation.
    So, the Bayesian inference essentially corresponds to updating beliefs about an
    uncertain system after having made some observations about it. In the sense, this
    is also how we human beings learn about the world. For example, before we visit
    a new city, we will have certain prior knowledge about the place after reading
    from books or on the Web.
  prefs: []
  type: TYPE_NORMAL
- en: However, soon after we reach the place, this belief will get updated based on
    our initial experience of the place. We continuously update the belief as we explore
    the new city more and more. We will describe Bayesian inference more in detail
    in [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing Bayesian Inference"),
    *Introducing Bayesian Inference*.
  prefs: []
  type: TYPE_NORMAL
- en: Marginal distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many situations, we are interested only in the probability distribution
    of a subset of random variables. For example, in the heart disease problem mentioned
    in the previous section, if we want to infer the probability of people in a population
    having a heart disease as a function of their age only, we need to integrate out
    the effect of other random variables such as blood pressure and diabetes. This
    is called **marginalization**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Marginal distribution](img/image00208.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Or:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Marginal distribution](img/image00209.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that marginal distribution is very different from conditional distribution.
    In conditional probability, we are finding the probability of a subset of random
    variables with values of other random variables fixed (conditioned) at a given
    value. In the case of marginal distribution, we are eliminating the effect of
    a subset of random variables by integrating them out (in the sense averaging their
    effect) from the joint distribution. For example, in the case of two-dimensional
    normal distribution, marginalization with respect to one variable will result
    in a one-dimensional normal distribution of the other variable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Marginal distribution](img/image00210.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The details of this integration is given as an exercise (exercise 3 in the *Exercises*
    section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Expectations and covariance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having known the distribution of a set of random variables ![Expectations and
    covariance](img/image00211.jpeg), what one would be typically interested in for
    real-life applications is to be able to estimate the average values of these random
    variables and the correlations between them. These are computed formally using
    the following expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Expectations and covariance](img/image00212.jpeg)![Expectations and covariance](img/image00213.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, in the case of two-dimensional normal distribution, if we are
    interested in finding the correlation between the variables ![Expectations and
    covariance](img/image00183.jpeg) and ![Expectations and covariance](img/image00184.jpeg),
    it can be formally computed from the joint distribution using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Expectations and covariance](img/image00214.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Binomial distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A binomial distribution is a discrete distribution that gives the probability
    of heads in *n* independent trials where each trial has one of two possible outcomes,
    heads or tails, with the probability of heads being *p*. Each of the trials is
    called a Bernoulli trial. The functional form of the binomial distribution is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binomial distribution](img/image00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Binomial distribution](img/image00216.jpeg) denotes the probability
    of having *k* heads in *n* trials. The mean of the binomial distribution is given
    by *np* and variance is given by *np(1-p)*. Have a look at the following graphs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binomial distribution](img/image00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graphs show the binomial distribution for two values of *n*; 100
    and 1000 for *p = 0.7*. As you can see, when *n* becomes large, the Binomial distribution
    becomes sharply peaked. It can be shown that, in the large *n* limit, a binomial
    distribution can be approximated using a normal distribution with mean *np* and
    variance *np(1-p)*. This is a characteristic shared by many discrete distributions
    that, in the large *n* limit, they can be approximated by some continuous distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Beta distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Beta distribution denoted by ![Beta distribution](img/image00218.jpeg)
    is a function of the power of ![Beta distribution](img/image00219.jpeg), and its
    reflection ![Beta distribution](img/image00220.jpeg) is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Beta distribution](img/image00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Beta distribution](img/image00222.jpeg) are parameters that determine
    the shape of the distribution function and ![Beta distribution](img/image00223.jpeg)
    is the Beta function given by the ratio of Gamma functions: ![Beta distribution](img/image00224.jpeg).'
  prefs: []
  type: TYPE_NORMAL
- en: The Beta distribution is a very important distribution in Bayesian inference.
    It is the conjugate prior probability distribution (which will be defined more
    precisely in the next chapter) for binomial, Bernoulli, negative binomial, and
    geometric distributions. It is used for modeling the random behavior of percentages
    and proportions. For example, the Beta distribution has been used for modeling
    **allele** frequencies in population genetics, time allocation in project management,
    the proportion of minerals in rocks, and heterogeneity in the probability of HIV
    transmission.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Gamma distribution denoted by ![Gamma distribution](img/image00225.jpeg)
    is another common distribution used in Bayesian inference. It is used for modeling
    the waiting times such as survival rates. Special cases of the Gamma distribution
    are the well-known Exponential and Chi-Square distributions.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian inference, the Gamma distribution is used as a conjugate prior for
    the inverse of variance of a one-dimensional normal distribution or parameters
    such as the rate (![Gamma distribution](img/image00226.jpeg)) of an exponential
    or Poisson distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical form of a Gamma distribution is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gamma distribution](img/image00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Gamma distribution](img/image00228.jpeg) and ![Gamma distribution](img/image00229.jpeg)
    are the shape and rate parameters, respectively (both take values greater than
    zero). There is also a form in terms of the scale parameter ![Gamma distribution](img/image00230.jpeg),
    which is common in **econometrics**. Another related distribution is the Inverse-Gamma
    distribution that is the distribution of the reciprocal of a variable that is
    distributed according to the Gamma distribution. It's mainly used in Bayesian
    inference as the conjugate prior distribution for the variance of a one-dimensional
    normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Dirichlet distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Dirichlet distribution is a multivariate analogue of the Beta distribution.
    It is commonly used in Bayesian inference as the conjugate prior distribution
    for multinomial distribution and categorical distribution. The main reason for
    this is that it is easy to implement inference techniques, such as Gibbs sampling,
    on the Dirichlet-multinomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dirichlet distribution of order ![Dirichlet distribution](img/image00231.jpeg)
    is defined over an open ![Dirichlet distribution](img/image00232.jpeg) dimensional
    simplex as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dirichlet distribution](img/image00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Dirichlet distribution](img/image00234.jpeg), ![Dirichlet distribution](img/image00235.jpeg),
    and ![Dirichlet distribution](img/image00236.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: Wishart distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Wishart distribution is a multivariate generalization of the Gamma distribution.
    It is defined over symmetric non-negative matrix-valued random variables. In Bayesian
    inference, it is used as the conjugate prior to estimate the distribution of inverse
    of the covariance matrix ![Wishart distribution](img/image00237.jpeg) (or precision
    matrix) of the normal distribution. When we discussed Gamma distribution, we said
    it is used as a conjugate distribution for the inverse of the variance of the
    one-dimensional normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical definition of the Wishart distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Wishart distribution](img/image00238.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Wishart distribution](img/image00239.jpeg) denotes the determinant of
    the matrix ![Wishart distribution](img/image00240.jpeg) of dimension ![Wishart
    distribution](img/image00241.jpeg) and ![Wishart distribution](img/image00242.jpeg)
    is the degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: A special case of the Wishart distribution is when ![Wishart distribution](img/image00243.jpeg)
    corresponds to the well-known Chi-Square distribution function with ![Wishart
    distribution](img/image00244.jpeg) degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: Wikipedia gives a list of more than 100 useful distributions that are commonly
    used by statisticians (reference 1 in the *Reference* section of this chapter).
    Interested readers should refer to this article.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By using the definition of conditional probability, show that any multivariate
    joint distribution of N random variables ![Exercises](img/image00197.jpeg) has
    the following trivial factorization:![Exercises](img/image00245.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bivariate normal distribution is given by:![Exercises](img/image00246.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Exercises](img/image00247.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: By using the definition of conditional probability, show that the conditional
    distribution ![Exercises](img/image00201.jpeg) can be written as a normal distribution
    of the form ![Exercises](img/image00248.jpeg) where ![Exercises](img/image00249.jpeg)
    and ![Exercises](img/image00203.jpeg).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By using explicit integration of the expression in exercise 2, show that the
    marginalization of bivariate normal distribution will result in univariate normal
    distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following table, a dataset containing the measurements of petal and
    sepal sizes of 15 different Iris flowers are shown (taken from the Iris dataset,
    UCI machine learning dataset repository). All units are in cms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| Sepal Length | Sepal Width | Petal Length | Petal Width | Class of Flower
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 5.1 | 3.5 | 1.4 | 0.2 | Iris-setosa |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 4.9 | 3 | 1.4 | 0.2 | Iris-setosa |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 4.7 | 3.2 | 1.3 | 0.2 | Iris-setosa |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 4.6 | 3.1 | 1.5 | 0.2 | Iris-setosa |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 5 | 3.6 | 1.4 | 0.2 | Iris-setosa |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 7 | 3.2 | 4.7 | 1.4 | Iris-versicolor |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 6.4 | 3.2 | 4.5 | 1.5 | Iris-versicolor |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 6.9 | 3.1 | 4.9 | 1.5 | Iris-versicolor |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 5.5 | 2.3 | 4 | 1.3 | Iris-versicolor |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 6.5 | 2.8 | 4.6 | 1.5 | Iris-versicolor |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 6.3 | 3.3 | 6 | 2.5 | Iris-virginica |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 5.8 | 2.7 | 5.1 | 1.9 | Iris-virginica |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 7.1 | 3 | 5.9 | 2.1 | Iris-virginica |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 6.3 | 2.9 | 5.6 | 1.8 | Iris-virginica |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 6.5 | 3 | 5.8 | 2.2 | Iris-virginica |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Answer the following questions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the probability of finding flowers with a sepal length more than 5 cm
    and a sepal width less than 3 cm?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the probability of finding flowers with a petal length less than 1.5
    cm; given that petal width is equal to 0.2 cm?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the probability of finding flowers with a sepal length less than 6 cm
    and a petal width less than 1.5 cm; given that the class of the flower is Iris-versicolor?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/List_of_probability_distributions](http://en.wikipedia.org/wiki/List_of_probability_distributions)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Feller W. *An Introduction to Probability Theory and Its Applications*. Vol.
    1\. Wiley Series in Probability and Mathematical Statistics. 1968\. ISBN-10: 0471257087'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jayes E.T. *Probability Theory: The Logic of Science*. Cambridge University
    Press. 2003\. ISBN-10: 0521592712'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Radziwill N.M. *Statistics (The Easier Way) with R: an informal text on applied
    statistics*. Lapis Lucera. 2015\. ISBN-10: 0692339426'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize this chapter, we discussed elements of probability theory; particularly
    those aspects required for learning Bayesian inference. Due to lack of space,
    we have not covered many elementary aspects of this subject. There are some excellent
    books on this subject, for example, books by William Feller (reference 2 in the
    *References* section of this chapter), E. T. Jaynes (reference 3 in the *References*
    section of this chapter), and M. Radziwill (reference 4 in the *References* section
    of this chapter). Readers are encouraged to read these to get a more in-depth
    understanding of probability theory and how it can be applied in real-life situations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the R programming language that is the
    most popular open source framework for data analysis and Bayesian inference in
    particular.
  prefs: []
  type: TYPE_NORMAL
