- en: Predicting Failures of Banks - Multivariate Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to apply different algorithms with the aim of
    obtaining a good model using combinations of our predictors. The most common algorithm
    that's used in credit risk applications, such as credit scoring and rating, is
    logistic regression. In this chapter, we will see how other algorithms can be
    applied to solve some of the weaknesses of logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularized methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing a random forest model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning in neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematically, a binary logistic model has a dependent variable with two categorical
    values. In our example, these values relate to whether or not a bank is solvent.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a logistic model, **log odds** refers to the logarithm of the odds for a
    class, which is a linear combination of one or more independent variables, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8748302-fdf3-43b2-adf6-f6774fcc2739.png)'
  prefs: []
  type: TYPE_IMG
- en: The coefficients (beta values, *β*) of the logistic regression algorithm must
    be estimated using maximum likelihood estimation. Maximum likelihood estimation
    involves getting values for the regression coefficients that minimize the error
    in the probabilities that are predicted by the model and the real observed case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is very sensitive to the presence of outlier values, so
    high correlations in variables should be avoided. Logistic regression in R can
    be applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code runs without problems, but a warning message appears. If the variables
    are highly correlated or collinearity exists, it is expected that the model parameters
    and the variance are inflated.
  prefs: []
  type: TYPE_NORMAL
- en: The high variance is not due to accurate or good predictors, but is instead
    due to a misspecified model with redundant predictors. Thus, the maximum likelihood
    is increased by simply adding more parameters, which results in overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe the parameters of the model with the `summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can see that most of the variables in the last column of the preceding table
    are insignificant. In cases like this, the number of variables should be reduced
    in the regression, or another approach should be followed, such as a penalized
    or regularization method.
  prefs: []
  type: TYPE_NORMAL
- en: Regularized methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three common approaches to using regularized methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Lasso
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see how these methods can be implemented in R. For
    these models, we will use the `h2o` package. This provides a predictive analysis
    platform to be used in machine learning that is open source, based on in-memory
    parameters, and distributed, fast, and scalable. It helps in creating models that
    are built on big data and is most suitable for enterprise applications as it enhances
    production quality.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the `h2o` package, please visit its documentation at
    [https://cran.r-project.org/web/packages/h2o/index.html](https://cran.r-project.org/web/packages/h2o/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: This package is very useful because it summarizes several common machine learning
    algorithms in one package. Moreover, these algorithms can be executed in parallel
    on our own computer, as it is very fast. The package includes generalized linear
    naïve Bayes, distributed random forest, gradient boosting, and deep learning,
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: It is not necessary to have a high level of programming knowledge, because the
    package comes with a user interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how the package works. First, the package should be loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `h2o.init` method to initialize H2O. This method accepts other options
    that can be found in the package documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step toward building our model involves placing our data in the H2O
    cluster/Java process. Before this step, we will ensure that our target is considered
    as a `factor` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s upload our data to the `h2o` cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you close R and restart it later, you will need to upload the datasets again,
    as in the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check that the data has been uploaded correctly with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The package contains an easy interface that allows us to create different models
    when we run it in our browser. In general, the interface can be launched by writing
    the following address in our web browser, `http://localhost:54321/flow/index.html`.
    You will be faced with a page like the one that''s shown in the following screenshot.
    In the Model tab, we can see a list with all of the available models that are
    implemented in this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f048b93-f077-4cfa-a381-12ba40c141bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we are going to develop regularization models. For that, Generalized
    Linear Modelling… must be selected. This module includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poisson regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binomial regression (classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multinomial classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamma regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ordinal regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in the following screenshot, we should fill in the necessary parameters
    to train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd371ff1-ed26-4cdd-9a97-a8398f634f8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will fill in the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: 'model_id: Here, we can specify the name that can be used as a reference by
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'training_frame: The dataset that we wish to use to build and train the model
    can be mentioned here, as this will be our training dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'validation_frame: Here, the dataset that will be used to check the accuracy
    of the model is mentioned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'nfolds: For validation, we require a certain number of folds to be mentioned
    here. In our case, the nfolds value is `5`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'seed: This specifies the seed that will be used by the algorithm. We will use
    a **Random Number Generator** (**RNG**) for the components in the algorithm that
    require random numbers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'response_column: This is the column to use as the dependent variable. In our
    case, the column is named Default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ignored_columns: In this section, it is possible to ignore variables in the
    training process. In our case, all of the variables are considered relevant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ignore_const_cols: This is a flag that indicates that the package should avoid
    constant variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'family: This specifies the model type. In our case, we want to train a regression
    model, so the family should be fixed as binomial, because our target variable
    has two possible values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'solver: This specifies the solver to use. We don''t change this value because
    no significant differences have been observed regardless of whether one solver
    or another is chosen. Hence, we will keep it as the default value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'alpha: Here, you have to choose values for the regularization distribution
    from L1 to L2\. If you select 1, it will be a Lasso regression. If you select
    0, it will be a Ridge regression. If any value in between 0 and 1 is selected,
    you will have a mixture of both Lasso and Ridge. In our case, we will select 1\.
    One of the main advantages of the Lasso model is in the reduction of the number
    of variables because the trained models makes the coefficient of non-relevant
    variables zero, resulting in models that are simple, but accurate at the same
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'lambda_search: This parameter starts a search of the regularization strength.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'standardize: If this flag is marked, it means that numeric columns will be
    transformed to have a zero mean and zero unit variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the Build model button trains the model. Although other options can
    be selected, the preceding specifications are sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d64f8f47-f7f4-4f04-aed7-de9bf5fb2b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the model has been trained quickly. The View button provides
    us with some interesting details about the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Receiver Operating Characteristic** (**ROC**) curve for training and validation
    samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardized coefficient magnitudes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gains/Lift table for cross-validation, training, and validation samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see some of the main results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/508719aa-acca-45b9-a847-ee4f09b73369.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, our Lasso model is trained with 108 different variables, but
    only 56 result in a model that has a coefficient greater than zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model provides an almost perfect classification. In the training sample,
    the **Area under the curve** (**AUC**) reaches 99.51%. This value is slightly
    lower in the validation sample, with a value of 98.65%. The standardized variables
    are also relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ade899-eea3-4f92-b42c-62fbaf23cd3a.png)'
  prefs: []
  type: TYPE_IMG
- en: If a variable is shown in blue, this indicates that the coefficient is positive.
    If it is negative, the color is orange.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, UBPRE626 looks like an important variable. It shows us how many
    times the total loans and lease-financing receivables surpassed the actual total
    of the equity capital. A positive sign here means a higher ratio, which also implies
    a higher probability that a bank will fail in its operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The top five relevant variables according to this figure are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'UBPRE626: The number of times net loans and lease-financing receivables exceed
    the total equity capital'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UBPRE545: The total of the due and non-accrual loans and leases, divided by
    the allowance for the loan and lease losses'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UBPRE170: The total equity capital'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UBPRE394: Other construction and land development loans, divided by the average
    gross loans and leases'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UBPRE672: One quarter of the annualized realized gains (or losses) of the securities,
    divided by average assets'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When looking at credit risks, it is important to understand which variables
    are most significant and the economic relevance of these variables. For example,
    it would not make sense if the higher the non-performing loans or loans with problems,
    the higher the solvency of a bank. We aren't concerned about the economic sense
    of the variables in our model, but this is a key issue for some models that are
    developed in financial institutions. If the variables don't have the expected
    sign, they have to be removed from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, is necessary to test different combinations of parameters until
    you obtain the best model. For example, in the recently trained regularized model,
    we could have tried different values of the alpha parameter. To test different
    parameters at the same time, you need to execute the algorithms using code. Let''s
    have a look at how to do this. We will train the regularized models again, but
    using some code this time. First, we remove all the objects, including the recently
    created model, from the `h2o` system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we upload our training and validation samples again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s code our model. A grid of empty parameters is created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we assign different parameters to be tested in this grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the parameters are exactly the same as those we used to train
    the previous model. The only difference is that we now use different alpha values
    at the same time, which corresponds to a Ridge regression, with a Lasso and an
    elastic net. The model is trained using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the previous code, the different models in the grid should be
    ordered by the AUC metric. Thus, we are interested in the first model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at some details about this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The model with the best performance is a Ridge model. The performance of the
    model can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `AUC` and the `Gini` index, which are the main metrics of performance, are
    only slightly higher than in the Lasso that we trained initially—at least in the
    training sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'The performance of the model in the test sample is also high:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Again, the results do not differ significantly in comparison with the Lasso
    model. Nevertheless, the number of coefficients in the Lasso model is lower, which
    makes it easier to interpret and more parsimonious.
  prefs: []
  type: TYPE_NORMAL
- en: 'The total number of coefficients in the Ridge regression is equal to the number
    of variables in the dataset and the intercept of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will store the predictions of each model in a new data frame. We can
    combine the results of the different models to obtain an additional model. Initially,
    our data frame will contain only the ID of each bank and the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s calculate the model predictions and store them in the summary data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the previous code to calculate the performance of the model, we
    also obtain a confusion matrix. For example, in the test sample, we obtain the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This package classifies a bank as a failed bank if its probability of defaulting
    is higher than 0.5, and is a successful bank otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: According to this assumption, `40` banks are misclassified (`28`+`12`). Nevertheless,
    the cutoff of 0.5 is not actually correct, because the proportion of failed versus
    non-failed banks in the sample is different.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proportion of failed banks is actually only 4.696%, as shown in the following
    code :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, it is more appropriate to consider a bank as failed if the probability
    of a bank defaulting is higher than this proportion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, the new confusion table for the test sample is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: According to this table, the model misclassifies 86 banks (`78`+`8`). Almost
    all of the failed banks have been correctly classified. It will be difficult to
    obtain a better algorithm than this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model can be saved locally using `h2o.saveModel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We remove the irrelevant objects in the workspace and save it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that if you close R and load this workspace again, you should convert
    your train and test samples into `h2o` format again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Testing a random forest model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A random forest is an ensemble of decision trees. In a decision tree, the training
    sample, which is based on the independent variables, will be split into two or
    more homogeneous sets. This algorithm deals with both categorical and continuous
    variables. The best attribute is selected using a recursive selection method and
    is split to form the leaf nodes. This continues until a criterion that's meant
    to stop the loop is met. Every tree that's created by the expansion of leaf nodes
    is considered to be a weak learner. This weak learner is built on top of the rows
    and columns of the subsets. The higher the number of trees, the lower the variance.
    Both classification and regression random forests calculate the average prediction
    of all of the trees to make a final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: When a random forest is trained, some different parameters can be set. Among
    the most common parameters are the number of trees, the maximum number of variables,
    the size of terminal nodes, and the depth in each tree. Several tests should be
    carried out to find a balance between performance and overfitting. For example,
    the higher the number and the depth of the trees, the better the accuracy on a
    training set, but this increases the risk of overfitting. To obtain this balance,
    several parameters and combinations of parameters should be tested on the validation
    set, and then they should be cross-validated during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, this algorithm is easy to implement in the `h2o` package using the visual
    guide in a browser. The grid of the parameters should be implemented by coding
    it. The code is almost the same as the one in the previous model. This time, however,
    the process is more time-consuming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the combinations in the number of trees (`ntrees`), the depth
    (`max_depth`), and the number of variables to be considered in each tree (`mtries`)
    are tested. The resulting models are ordered using the AUC metric.
  prefs: []
  type: TYPE_NORMAL
- en: '`27` different models have been trained according to the preceding specifications.
    The first model, or the model that has the best accuracy, is selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance of this model is obtained for train and test samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the code is exactly the same as the previous model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we find the performance of the test or validation sample using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are almost perfect in both samples. The importance of the variable
    can be obtained as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2105e9a6-9c14-4225-a3cc-fb03192e5ff4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like Ridge regression, the probability of bankruptcy will be stored for
    both the train and validation samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can compute the confusion matrix. Remember that the cutoff to classify
    a bank regarding its probability of bankruptcy is determined based on the observed
    proportion of bad banks in the total sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: If the random forest and Ridge regression models are compared, we can see that
    random forest only misclassifies five failed banks, while there are 12 misclassified
    banks in the Ridge regression. Nevertheless, the random forest classifies more
    solvent banks as failed banks than Ridge regression, meaning that it has a high
    level of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The irrelevant objects are, again, removed from the workspace. Moreover, we
    save a backup of our workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gradient boosting** means combining weak and average predictors to acquire
    one strong predictor. This ensures robustness. It is similar to a random forest,
    which is mainly based on decision trees. The difference is that the sample is
    not modified from one tree to another; only the weights of the different observations
    are modified.'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting trains trees sequentially by using information from previously trained
    trees. For this, we first need to create decision trees using the training dataset.
    Then, we need to create another model that does nothing but rectify the errors
    that occurred in the training model. This process is repeated sequentially until
    the specified number of trees, or some other stopping rule, is reached.
  prefs: []
  type: TYPE_NORMAL
- en: More specific details about the algorithm can be found in the documentation
    of the `h2o` package. While training the algorithm, we will need to define parameters
    such as the number of trees that we will combine and the minimum observation in
    each node, just like we did for the random forests.
  prefs: []
  type: TYPE_NORMAL
- en: The shrinkage parameter, or the rate at which boosting learns, can alter the
    performance of the model. We will need to consider the results of many experiments
    to determine the optimal parameters to ensure a high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our grid of parameters collects different combinations of the number of trees
    and the `max_depth` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Different models will be trained by executing the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The grids are ordered by `AUC`. The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Most models obtain a perfect classification. This might be a sign of overfitting.
    Let''s take a look at the performance of the first model on the validation sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are very good, even in the test sample. The first model is selected
    and the predictions are stored, as in the previous models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the confusion table in the test sample is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'A total of 14 failed banks and 25 non-failed banks are misclassified. Let''s
    save the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Deep learning in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For machine learning, we need systems that can process nonlinear and unrelated
    sets of data. This is very important so that we can make predictions for bankruptcy
    problems, since the relationship between the default and explanatory variables
    will rarely be linear. Therefore, using neural networks is the best possible solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**) have long since been used to solve
    bankruptcy problems. An ANN is a computer system that has a number of interconnected
    processors. These processors provide outputs by processing information and by
    responding dynamically to the inputs that are provided. A prominent and basic
    example of ANN is the **multilayer perceptron** (**MLP**). An MLP can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06c35f9e-3a7d-40f6-8910-7f225859bd6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Except for the input nodes, each node is a neuron that uses a nonlinear activation
    function, which was sent in.
  prefs: []
  type: TYPE_NORMAL
- en: As is evident from its diagram, an MLP is nothing but a **feed-forward neural
    network**. This means that the input information that's provided will only move
    in a forward direction. This type of network usually consists of one input, one
    hidden layer, and one output layer. The input layer represents the input data
    of the model, or the variables. In our case, these are the financial variables.
    No calculations are made in this layer. Hidden layers are where intermediate processing
    or computation is done. They perform computation and then transfer the weights
    (the signals or information) from the input layer to the following layer. Finally,
    the output layer takes inputs from the hidden layer and calculates the outputs
    of the network. The input nodes use a nonlinear activation function to send information
    from one layer to the next. The purpose of the activation function is to transform
    the input signal into an output signal that models complex nonlinear patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons networks learn by modifying the weights after every set of data
    is processed. These weights specify the number of errors that occur when processing
    the input, which is obtained by comparing the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: Is deep learning different from MLP? MLP is just one type of deep learning algorithm.
    In many cases, deep learning is different from an MLP network, but only because
    of the complexity in the calculations and the number of hidden layers. Deep learning
    can be considered an MLP with two or more hidden layers. When two or more hidden
    layers are included, the learning process should be different as well, because
    the backpropagation learning rule that's used in MLP fails. The perceptron update
    rule is prone to vanishing and exploding gradients, making it difficult to train
    networks with more than one or two layers.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing a multilayer network, ensure that you determine the appropriate
    number of required layers for better accuracy and precision. Often, for many models,
    just one hidden layer is enough to solve the problem of classification. Nevertheless,
    the use of many hidden layers has demonstrated its usefulness in areas such as
    speech recognition or object detection, among others. Another thing to consider
    is the number of neurons in every hidden layer. This is a very important aspect.
    Mistakes in estimating these values can lead to problems such as overfitting,
    when too many neurons are added, and underfitting, when not enough neurons are
    added.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `h2o` package helps us train the neural networks. Deep learning models
    have many input parameters. In this exercise, the following parameters will be
    tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will test three structures: first, a network containing only a hidden
    layer with 25 neurons, then a network with three hidden layers with 32 neurons
    in each layer, and finally, a two hidden layer network with 64 neurons in each
    layer.'
  prefs: []
  type: TYPE_NORMAL
- en: A neural network learns, and neurons progressively specialize in values for
    specific variables. If neurons are too specialized in the training set, there
    is a high risk of overfitting. To avoid overfitting, the `input_dropout_ratio`
    command is included. The dropout technique is a regularization approach for neural
    network models to improve the generalization of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the dropout approach randomly selects neurons and ignores them
    during training. In practice, at each training step, a different network is created
    because some of the random units are removed and trained using backpropagation,
    as usual. This forces the network to learn several independent representations
    of the patterns with identical input and output, improving the generalization.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain more information related to the dropout approach, I recommend the
    original paper, *Dropout:* **A Sim*ple Way to Prevent Neural Networks from Overfitting*,
    by Nitish Srivastava et al. It is available at [https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The suggested values for the input layer dropout ratio are 0.1 or 0.2\. Finally,
    with the `rate` command, we can specify the learning rate. Remember, if the learning
    rate is set too high, the model may become less stable, and if it is set too low,
    then the convergence will be very slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write some training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters in the preceding code can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`epochs`: The value that''s specified here determines the number of times the
    dataset has to be streamed while learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_metric`: This specifies the metric to use for early stopping, which
    in our case is AUC.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_tolerance` and `stopping rounds`: These determine a tolerance value
    before the model stops learning, and a stopping value that prevents the model
    from learning when the `stopping_metric` doesn''t improve after the number of
    rounds specified, respectively. When cross-fold validation is specified (as in
    our case), this option will apply on all cross-validation models. In our case,
    we set the options of `stopping_tolerance=1e-2` and `stopping_rounds = 2`, which
    means that the model won''t be trained after 2 rounds of iterations or if there
    isn''t an improvement of at least 2% ( `1e-2`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score_duty_cycle`: This indicates how much time to spend scoring versus training.
    The values are percentages ranging from 0 to 1\. Lower values indicate more training.
    The default value of this option is 0.1, which indicates that 10% of the time
    should be spent on scoring and the remaining 90% should be spent on training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1` and `l2`: The value that''s added here is the regularization index, which
    ensures better generalization and stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation`: Activation functions such as `tanh`, `tanh with dropout`, `Maxout`,
    and others can be mentioned here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nfolds`: This indicates the number of folds for cross-validation. The training
    process is very time-consuming because several configurations are tested. The
    performance of the different configurations can be obtained by running the following
    code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The best model is obtained with three hidden layers of 32 units, a dropout ratio
    of `0.25`, and a learning rate of `0.02`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best model is selected as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance is obtained for the test sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous models, the predictions on the training and validation samples
    are stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix is also obtained for the validation sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **support vector machine** (**SVM**) algorithm is a supervised learning
    technique. To understand this algorithm, take a look at the following diagram
    for the optimal hyperplane and maximum margin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c68909aa-0f0f-4048-be2f-80e8bc1047f3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this classification problem, we only have two classes that exist for many
    possible solutions to a problem. As shown in the preceding diagram, the SVM classifies
    these objects by calculating an optimal hyperplane and maximizing the margins
    between the classes. Both of these things will differentiate the classes to the
    maximum extent. Samples that are placed closest to the margin are known as **support
    vectors**. The problem is then treated as an optimization problem and can be solved
    by optimization techniques, the most common one being the use of Lagrange multipliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even in a separable linear problem, as shown in the preceding diagram, sometimes,
    it is not always possible to obtain a perfect separation. In these cases, the
    SVM model is the one that maximizes the margin while minimizing the number of
    misclassifications. In the real world, the problems are too far apart to be linearly
    separated, at least without a previous treatment or transformation of data. In
    the following diagram, the difference between a linear separable problem and a
    nonlinear separable problem is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09d1a29f-4e7b-4185-8f50-0290a6af9fcd.png)'
  prefs: []
  type: TYPE_IMG
- en: To handle nonlinear problems, a kernel function maps the data to different spaces.
    This means that data is transformed to a higher-dimensional space. This technique
    is known as the **kernel trick**, because sometimes it is possible to perform
    a linear separation between classes, making transformations in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages of the SVM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: SVM is simple
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM is a combination of statistical and machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM can be useful in solving financial problems like our problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting SVM parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's discuss some parameters that we might need so that we can use SVM.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM kernel parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main difficulties of SVM is selecting the kernel that transforms
    the data. The following are the most commonly used transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radial basis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The control on the liquidation and the transaction between the training error
    and the model complexity will be looked after by the cost parameter (`C`). If
    you have a relatively smaller number for `C`, there will be a higher number of
    training errors. If `C` is a bigger number, you could obtain a overfitted model,
    which means that your trained model has learned all of your training data but
    it is likely that the model does not work properly on any other sample. You can
    set the cost to any value between 1.001 and 100.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gamma parameter is needed while using a Gaussian kernel. This parameter
    will calculate the level of influence that each training sample can accomplish.
    Here, you may consider the lower values to be *far* and the higher values to be
    *close*.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma is actually the opposite of the support vectors that we have seen. Therefore,
    in an SVM, different values of all three parameters should be tested.
  prefs: []
  type: TYPE_NORMAL
- en: Training an SVM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The SVM algorithm is not available in the `h2o` package. To train the `SVM`
    classifier, we are going to use the `caret` package. Remember that our target
    value takes two different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the different values of this variable (`0` and `1`) do not display
    problems in other algorithms, in this case, we need to make a little transformation
    here. The categories in the target variable can only take values like `X0` or
    `X1`, so we need to transform them. Let''s write some code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'These values are also transformed in the test sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create a grid with different values of the cost and gamma parameters
    in a similar way to the `h2o` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will run the following code to train the different models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: To train an `SVM` classifier, the `train()` method should be passed with the
    `method` parameter as `svmRadial`, which is our selected kernel. `TuneGrid` represents
    the different combinations of the cost and gamma parameters. The accuracy of models
    is measured using the `ROC` metric. 5-fold cross validation is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been trained, we can view the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'In summary, the best parameters are the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we can access the model with the best parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance of the model is not directly obtained, as in the `h2o` package.
    This isn''t difficult to do, but we need to use the `ROCR` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/057e4d18-0ae2-43cd-9ea9-ebeecd282edd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Gini index can be calculated as `2*ROC -1`. We can use the `Hmisc` package
    to calculate the ROC and then calculate the Gini index, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Gini reaches 0.9766 in the test sample. As in the previous models, the confusion
    matrix is calculated using the validation or test sample. To do this, first, the
    probabilities are stored for both the train and test samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion table is now calculated on the test sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'SVM does a good job of classifying banks. Only 76 banks (`68`+`8`) are misclassified
    on the test sample. Now, a new backup of the workspace is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we have trained five different models. The predictions are stored
    in two data frames, one for training and the other for the validation samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s summarize the accuracy of the previously trained models. First, the
    predictive power of each classifier will be calculated using the Gini index. With
    the following code, the Gini index for the training and validation samples is
    calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are stored in a data frame called `gini_models`. The variation
    in the predictive power between the train and test samples is also calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: There are not really many significant differences between the models. The SVM
    is the model with the highest predictive power in the test sample. On the other
    hand, the deep learning model obtains the worst results.
  prefs: []
  type: TYPE_NORMAL
- en: These results indicate that it is not very difficult to find banks that will
    fail in less than a year from the current financial statement, which is how we
    defined our target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the predictive power of each model, depending on the number
    of banks that are correctly classified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create new data frames where the banks are classified as solvent
    or non-solvent banks, depending on the predicted probabilities, as we have done
    for each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, a function that counts the number of banks as correctly and non-correctly
    classified is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'By running the preceding function, we will see a summary of the performance
    of each model. First, the function is applied on the training sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can see the results of the different models in our test sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: According to the table that was measured on the test sample, `RF` is the most
    accurate classifier of the failed banks, but this also misclassifies `138` solvent
    banks as failed, providing false alerts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of the different models are correlated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: It might be interesting to combine the results of the different models to obtain
    a better model. Here, the concept of ensembles comes in handy. **Ensemble** is
    a technique that's used to combine different algorithms to make a more robust
    model. This combined model incorporates the predictions from all the base learners.
    The resulting model will have a higher level of accuracy than the accuracy that
    would be attained if the models were run separately. In fact, some of the previous
    models that we've developed are ensemble models, for example; the random forest
    or **Gradient Boosting Machine** (**GBM**). There are many options when creating
    an ensemble. In this section, we will look at different alternatives, from the
    simplest to those that are more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Average model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is simply defined as taking the average of the predictions from the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the final probability of the failure of a bank will be calculated as the
    simple average of the probabilities of failure of the previous five models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The predictive power of this simple ensemble is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a confusion matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: This combined model only misclassifies `7` failed banks and `62` non-failed
    banks. Apparently, this averaged model is better than the performance of all of
    the individual models.
  prefs: []
  type: TYPE_NORMAL
- en: To add some degree of conservatism, we might think that a better approach would
    be to assign the highest probability of failure from the different models. Nevertheless,
    this approach is not likely to be successful, because we have observed before
    that random forest creates false alarms for some banks.
  prefs: []
  type: TYPE_NORMAL
- en: Majority vote
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is defined as taking the prediction with the maximum vote while predicting
    the outcome of a classification problem. First, we need to assign a vote for each
    model. This step is already done in the `decisions_test` data frame. A bank will
    be classified as non-solvent if three of the five models classify it as such.
    Let''s see the results of this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The results seem not to be as good as the individual models or the ensemble
    that considered the average probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Model of models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This involves combining the individual output of the models (such as random
    forest or SVM) using another machine learning model (such as Lasso, GBM, or random
    forest). The top layer of the ensemble model can be any model, even if the same
    technique (such as random forest) is used in the bottom layer. The most complex
    algorithms (such as random forest, gradient boosting, SVM, and others) do not
    always show better performance than the simpler ones (such as trees or logistic
    regression).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, no additional examples and algorithms will be trained, as the
    previous results will be sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have learned how to develop a powerful model to predict bank failures,
    we will test a final option to develop different models. Specifically, we will
    try out **automatic machine learning** (**autoML**), which is included in the
    `h2o` package. The process that we have carried out to build many models and find
    the best one without any prior knowledge is done automatically by the `autoML`
    function. This function trains different models by trying different grids of parameters.
    Moreover, stacked ensembles or models based on previously trained models are trained
    to find more accurate or predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, using this function before launching any model is highly recommended
    to get an initial idea of a reference starting point. Using an automatic approach,
    we can assess the most reliable algorithms, the most important potential variables
    to be used, or a reference of the accuracy we could obtain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test this function, we will load a previous workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '`Data12.RData` contains train and test samples before launching any model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to load the `h2o` package as well. Moreover, all of the objects that
    were created in the `h2o` space will be removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Standardizing variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous models, we fixed a parameter that standardized the data. However,
    this option is not available in the `autoML` function. Thus, the variables will
    be standardized first. The columns will have zero mean and unit variance. We need
    to standardize the variables because otherwise the results will have dominating
    variables that seem to have a higher variance compared to other attributes as
    a consequence of their scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Standardization is done using the `caret` package. First, we choose the name
    of numeric columns to standardize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The variables are transformed with the `preProcess` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous function stores the parameters that are needed to make the standardization
    on any dataset. With the `predict` function, we can actually apply this transformation.
    Both the train and test samples must be transformed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to create the different models. The train and test samples
    are converted into `h2o` tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We need the name of both the target and the predictors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'After all of these basic preprocessing steps, it is time to create a model.
    The `h2o.automl` function implements the autoML method. The following parameters
    are needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: The names of the predictors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y`: The target column name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_frame`: The training dataset that is to be used for creating the
    model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`leaderboard_frame`: The validation dataset that''s used by `h2o` to ensure
    that the model doesn''t overfit the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are more parameters, but the preceding list contains the minimum requirements.
    It is also possible to exclude some algorithms, for example. In our case, we will
    fix the maximum number of models to be trained and the AUC as the stopping metric
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train some models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access the `Leaderboard` of the trained models as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the `Leaderboard`, a gradient boosting model is the best if accuracy
    is to be considered. Let''s obtain the prediction of this boosting model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to print the complete details of the model with the following
    code (the results are not printed in this book because of their length):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'It is possible to analyze the importance of individual models in a stacked
    model, as well. Let''s see the accuracy of the best model on the test sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: It is important to remember that the predict column is the predicted category
    according to the model, but we need to take into consideration 50% as a threshold
    in the predicted probability of bankruptcy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like in the previous algorithms, we should define the observed default rate
    in our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will add the observed class, and then the accuracy table will be calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The automatic model obtains very good performance, but it''s a little worse
    than the SVM model. Like the previous models in the `h2o` package, the model can
    be saved for future use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used different models and algorithms to try and optimize
    our model. All of the algorithms obtained good results. This would not have been
    the case in other problems. You can try using different algorithms in your problems
    and test the best combinations of parameters to solve your specific problem. A
    combination of different algorithms or ensembles might be a good option as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will continue by looking at other real problems—specifically,
    data visualization of economic imbalances in European countries.
  prefs: []
  type: TYPE_NORMAL
