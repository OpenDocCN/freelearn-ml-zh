<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Transforming Your Data</h1>
                </header>
            
            <article>
                
<p>Real-world datasets are very varied: variables can be textual, numerical, or categorical, and observations can be missing, false, or wrong (outliers). To perform a proper data analysis, we will understand how to correctly parse data, clean it, and create an output matrix optimally built for machine learning analysis. To extract knowledge, it is essential that the reader is able to create an observation matrix using different techniques of data analysis and cleaning.</p>
<p>In this chapter, we'll present Cloud Dataprep, a service useful to preprocess the data, extract features, and clean up the records. We'll also cover Cloud Dataflow, a service to implement streaming and batch processing. We'll go into some practical details with real-life examples. We'll start from discovering different ways to transform data and the degree of cleaning data. We will analyze the techniques available for preparing the most suitable data for analysis and modeling, which includes imputation of missing data, detecting and eliminating outliers, and adding derived variables. Then we will learn how to normalize the data, in which data units are eliminated, allowing us to easily compare data from different locations.</p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Different ways to transform data</li>
<li>How to organize data</li>
<li>Dealing with missing data</li>
<li>Detecting outliers</li>
<li>Data normalization</li>
</ul>
<p>At the end of the chapter, we will be able to perform data preparation so that its information content is best exposed to the regression tools. We'll learn how to apply transforming methods to our own data and how these techniques work. We'll discover how to clean the data, identify missing data, and work with outliers and missing entries. We'll also learn how to use normalization techniques to compare data from different locations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to clean and prepare the data</h1>
                </header>
            
            <article>
                
<p>A novice may think that once we complete collecting data and it is imported into Google Cloud, it is finally time to start the analysis process. Conversely, we must first proceed with the preparation of data (data wrangling).</p>
<div class="packt_tip">
<p>Data wrangling is the process of the transformation and mapping of data, turning raw data into formatted <span>data, </span>with the intent of making it more appropriate for subsequent analysis operations.</p>
</div>
<p>This process can take a long time and it is very cumbersome, in some cases taking up about 80 percent of the entire data analysis process.</p>
<p>However, it is a fundamental prerequisite for the rest of the data analysis workflow; so it is essential to acquire the best practices in such techniques. Before submitting our data to any machine learning algorithm, we must be able to evaluate the quality and accuracy of our observations. If we do not know how to switch from raw data to something that can be analyzed, we cannot go ahead.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud Dataprep</h1>
                </header>
            
            <article>
                
<p>For proper preparation of our data, it is necessary to perform a series of operations involving the use of different algorithms. As we have anticipated, this work can take a long time and uses many resources. Google, within the Cloud service, offers the ability to do this job in a simple and immediate way: Google Cloud Dataprep.</p>
<p>It is an intelligent data service that allows you to visually explore, clean up, and prepare for structured and unstructured data analysis. Google Cloud Dataprep is serverless and works on any scale. It is not necessary to distribute or manage any infrastructure.</p>
<p>Google Cloud Dataprep helps to quickly prepare data for immediate analysis or for training machine learning models. Normally, the data has to be manually cleaned up; however, Google Cloud Dataprep makes the process extremely simple by<span> </span><span>automatically</span> detecting schemas, types, joins, and anomalies such as missing values. With regard to machine learning, different ways of data cleaning are suggested that can make the process of data preparation quicker and less prone to errors.</p>
<p>In Google Cloud Dataprep, it is possible to define data preparation rules by interacting with a sample of the data. Use of the application is free. Once a data preparation flow has been defined, you can export the sample for free or run the stream as a Cloud Dataprep job, which will be subject to additional costs.</p>
<p>With the use of Google Cloud Dataprep, you can perform the following operations:</p>
<ul>
<li>Import data from different sources</li>
<li>Identify and remove or modify missing data</li>
<li>Identify anomalous values (outliers)</li>
<li>Perform searches from a dataset</li>
<li>Normalize the values in the fields in the dataset</li>
<li>Merge datasets with joins</li>
<li>Add one dataset to another through merge operations</li>
</ul>
<p>These operations can be performed without the need for technological infrastructure in a very short time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring Dataprep console</h1>
                </header>
            
            <article>
                
<p>The first time you log into the Google Cloud Dataprep console, you will be asked to accept the terms of service, log into your Google account, and choose a cloud storage bucket to use with Cloud Dataprep. You will also be asked to allow Trifacta, the third-party application host, to access project data. After completing these steps, you will be offered the Cloud Dataprep home page with the <span class="packt_screen">Flows</span> screen opened, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a0b5e5c5-28d7-46ec-b26e-7194ffabb172.png"/></div>
<p>For now, the console appears empty because we have not yet performed any operation; it will later be populated by our operations. At the top of the Google Cloud Dataprep console, you can find three links that open the following pages:</p>
<ul>
<li><strong>FLOWS</strong>: This page shows the flows you have access to and allows you to create, review, and manage them. A flow is an object that allows us to gather and organize datasets to generate results.</li>
<li><strong>DATASETS</strong>: In this page, we can review the import and reference datasets to which we have access.</li>
<li><strong>JOBS</strong>: In this page, we can track the status of all of our running, complete, or failed jobs.</li>
</ul>
<p>By default, a <span class="packt_screen">Flows</span> page is open. To create a new flow, click on the <span class="packt_screen">Create Flow</span> button. The following window opens, where we can set a name and description for the new flow:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aee494ae-1dfc-4270-8ef9-33f2643efdae.png" style=""/></div>
<p class="NormalPACKT"><span>After we have done this, on the flows page, the new flow just created appears with suggestions of datasets that can be added to this flow to start wrangling. To analyze how a Google Cloud Dataprep example works, we will use a specially designed file that contains the data for a small sample of observation; it lists the results of a test. We'll grab <kbd>CleaningData.csv</kbd>, a spreadsheet that contains some of the issues we just listed. After correctly identifying this file on our computer and having it uploaded, the following window will be displayed (transformer page):</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/90edebe7-dcf3-4f1e-978e-152ac39693ee.png"/></div>
<p>In this page, we can easily identify the data we have imported. At this point, we can program the transformations that we want to apply to this data. Once we have established what we want to do, we can immediately preview the results before making changes to the entire final dataset.</p>
<p>In the Transformer page, two panels are available: <span class="packt_screen">Grid</span> and <span class="packt_screen">Columns</span>. By default, the Transformer page displays the grid panel, in which the previews of the columns in a columnar grid are displayed. In the columns panel, additional statistical information on individual columns is returned. This panel is particularly useful for managing anomalous values, to review average, minimum, and maximum values.</p>
<p>From a first glance at the transformer page, you can get an idea of the data we have imported. In fact, we can view all fields and all observations.</p>
<div class="packt_tip">Here, we have uploaded a file with just a few fields and observations. More generally, in the lower left part, we are given a summary of the number of rows, columns, and data types.</div>
<p>From the analysis of the returned window, we can extract useful information. Start from the name of each column, to the left of which appears a symbol that identifies the type of data automatically attributed to it. In this way we can verify that Dataprep has cataloged the variables as follows:</p>
<ul>
<li><kbd>name</kbd>: Name (String)</li>
<li><kbd>gender</kbd>: Sex (Gender)</li>
<li><kbd>age</kbd>: Age (Integer)</li>
<li><kbd>right1</kbd>: Percentage of right answers (Integer)</li>
<li><kbd>wrong</kbd>: Percentage of wrong answers (Integer)</li>
</ul>
<p>It seems that the attribution of the data type is correct. This information is returned to us using the cursor. In fact, as you move the cursor around the page, the cursor changes when it is over a selected data element. Immediately down the column header, there are useful summary graphs. In the column's data quality bar, the categories of values are shown. The following three types of values are available:</p>
<ul>
<li>Valid</li>
<li>Mismatched</li>
<li>Missing</li>
</ul>
<p>Then, histograms are shown to provide us with statistics about the data contained in each column. Further information is returned to us by passing the cursor over it.</p>
<p>Since the values are few, we can also have a preview of all the data. Let's take a look to visually identify anomalies. Already, we can see that the age variable has one missing value. Missing values of any type of variable are indicated by the NA code, which means not available. The <strong>Not a Number</strong> (<strong>NaN</strong>) code, on the other hand, indicates invalid numeric values, such as a numeric value divided by zero. If a variable contains missing values, GCP cannot apply some functions to it. For this reason, it is necessary to process the missing values in advance.</p>
<div>
<div class="packt_tip">On the transformation page, you can see that when you hover your mouse over the areas and related panels, a light bulb icon appears next to the cursor to indicate that suggestions are available.</div>
</div>
<p>On the left of each column header, you can see a drop-down column menu. With the help of this menu, we can perform several actions on the column data such as changing its data type, depending on the column data type. <span>The following actions are available:</span></p>
<ul>
<li><span class="packt_screen">Rename</span>: Rename the column</li>
<li><span class="packt_screen">Change type</span>: Change the data type of the column</li>
<li><span class="packt_screen">Move</span>: Move the column to the beginning or end, or to a specified location in the dataset</li>
<li><span class="packt_screen">Edit column</span>: Perform a series of edits to the column</li>
<li><span class="packt_screen">Column Details</span>: Explore the interactive profile of the column details</li>
<li><span class="packt_screen">Show related steps</span>: Highlight steps in the <span class="packt_screen">Recipe</span> panel where the selected column is referenced</li>
<li><span class="packt_screen">Find</span>: Find and replace specific values or extract patterned values from the column</li>
<li><span class="packt_screen">Filter</span>: Filter the rows of the dataset based on literal or computed values from the column</li>
<li><span class="packt_screen">Clean</span>: Clean the mismatched or missing values in the column, replacing values with fixed or computed values or removing the rows altogether</li>
<li><span class="packt_screen">Formula</span>: Generate a new column containing the values computed from the source column based on the selected function</li>
<li><span class="packt_screen">Aggregate</span>: Generate a summary table based on computations aggregated across groups, or add summary data as a new column in the current table</li>
<li><span class="packt_screen">Restructure</span>: Change the structure of the dataset based on the values of the column</li>
<li><span class="packt_screen">Lookup</span>: Perform a lookup of the column values against a set of values in another column of another dataset</li>
<li><span class="packt_screen">Delete</span>: Remove the column from the dataset</li>
</ul>
<p>The following screenshot shows the <span class="packt_screen">Column</span> menu for the gender column:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d9022727-6992-42c2-904b-d939955357d5.png"/></div>
<p>When selecting a column, a new suggestion panel opens to the right of the Transformer page. This panel shows a series of suggestions relevant to the type of data contained in the specific column. Suggestions vary depending on the selected data. By hovering over any of the suggestions, you can preview the results in the data grid to ensure that the proposed transformation works for the dataset. In the next section, we will be able to expand the use of the suggestion panel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing empty cells</h1>
                </header>
            
            <article>
                
<p>From a visual analysis of the imported data, we detect an empty cell at the third row and second column; this indicates the presence of a missing value. It is necessary to eliminate this anomaly before you can analyze the dataset.</p>
<p>In this case, identifying the empty cell was particularly easy given the small amount of data; in the case of large datasets, visual analysis does not work. Therefore, to identify missing values, ​​we can analyze the data quality bar. Here, the missing values ​​are identified in black.</p>
<p>To get a preview on the number of missing data, we can move our cursor over the black part of the data quality bar; the number of missing data is returned, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e3566a19-5274-47b8-91e0-437ef7d89e47.png"/></div>
<p>We have confirmed the presence of a missing value in the gender field. Recall that a missing value is a value that contains no content or is non-existent. These missing values may be due to a series of occurrences:</p>
<ul>
<li>Errors in the creation of the dataset; the values have been entered incorrectly, leaving empty cells</li>
<li>The dataset contains fields created automatically with cells that do not contain values</li>
<li>The result of an impossible calculation</li>
</ul>
<p>Now, if we select the <span class="packt_screen">gender</span> column, a new suggestion panel opens to the right of the Transformer page. As stated in the previous section, this panel shows a series of suggestions relevant to the type of data contained in the specific column. Several suggestions are proposed:</p>
<ul>
<li>Delete columns</li>
<li>Rename</li>
<li>Aggregate and group data</li>
<li>Create a new column</li>
<li>Set</li>
<li>Values to columns</li>
</ul>
<p>When we hover over different suggestions, a mini-preview <span>appears </span>to the left of each suggestion, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/93e688d2-4b81-48f0-b484-1858ebb93b14.png" style=""/></div>
<p class="NormalPACKT"><span>In the suggestion panel, a specific text called <span class="packt_screen">missing value to </span>appears, meaning that the missing values identified in the column can be replaced with something else. By clicking on the </span><span>Set </span><span>item</span><span>, two buttons appear:</span> <span class="packt_screen">E</span><span class="packt_screen">dit</span> <span>and</span> <span class="packt_screen">Add</span><span>. By clicking on the</span> <span class="packt_screen">Edit</span> <span>button a new box is opened; a new formula is proposed in this box as follows:</span></p>
<pre class="CodeEndPACKT"><span>ifmissing($col, '')</span></pre>
<p class="NormalPACKT"><span>The missing function writes out a specified value if the source value is a null or missing value. Upon inserting the <kbd>NA</kbd> string between the quotation marks, every time a missing value is identified in the column, it will be replaced by the <kbd>NA</kbd> value. After having modified the formula, we will be able to see a preview of the column in real time as modified by the formula applied; this is shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b58b06cc-b3c5-46a6-8633-ba7a710510b3.png"/></div>
<p>Now, to apply this suggestion, just click on the <span class="packt_screen">Add</span> button in the suggestions panel. In this way, a new step will be added to the <span class="packt_screen">Recipe</span> panel.</p>
<div class="packt_tip">
<p>Note that the <span class="packt_screen">Recipe</span> panel allows us to review and modify the steps of the recipe we have created so far, but, at the same time, it allows us to add new ones. If the panel is not displayed, to make it appear, just click on the icon (<span class="packt_screen">Recipe</span>) at the top left of the <span class="packt_screen">Run Job</span> button.</p>
</div>
<p>To generate a new set of suggestions on different columns, click on <span class="packt_screen">Cancel</span>. Then, select a different set of columns or values within a column; in this way, a new suggestions panel will be opened.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Replacing incorrect values</h1>
                </header>
            
            <article>
                
<p>The next step will allow us to replace the incorrect value indicators. If we take a look at the data again, we will see that in the age column, the value <kbd>-19</kbd> is displayed. That's obviously an incorrect value since for that variable, permissible values are greater than zero (this is an age):</p>
<pre>24 -19 32 15 18 21 28 30 26 100 22 NA</pre>
<p>We can replace this value with the missing value indicator, <kbd>NA</kbd>. To do this, we will write the following formula in the set suggestions:</p>
<pre>IF($col&lt;0, 'NA',$col)</pre>
<p>A new step will be added to the <span class="packt_screen">Recipe</span> panel immediately after the one created in the previous section. At the same time, we can see a preview of the changes made on the dataset. The incorrect value is no longer present; in its place, there is a further <kbd>NA</kbd> value.</p>
<p>The same result can be obtained by operating directly in the <span class="packt_screen">Recipe</span> panel:</p>
<ol>
<li>Remember that to open the <span class="packt_screen">Recipe</span> panel, you can just click on the icon (<span class="packt_screen">Recipe</span>) at the top left of the <span class="packt_screen">Run</span> <span class="packt_screen">Job</span> button</li>
<li>Click on the <span class="packt_screen">New Step</span> button; the Transform Builder is opened</li>
<li>In the <span class="packt_screen">Transformation</span> drop-down menu, select <span class="packt_screen">Apply formula</span> in the list of available transforms</li>
<li>Specify the <span class="packt_screen">Columns</span> (<span class="packt_screen">age</span>)</li>
<li>Edit the formula required in the <span class="packt_screen">Formula</span> box</li>
<li>Click on the <span class="packt_screen">Add</span> button</li>
<li>A new step is added to the <span class="packt_screen">Recipe</span> panel</li>
</ol>
<p>In the following screenshot, we can see the Transform Builder:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ad88eb70-7ee6-4c82-882c-fefef615521c.png"/></div>
<div>
<p>The result obtained is identical to the result we obtained by writing the formula in the set suggestions; even in this case, a negative value will no longer be present in the age column.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mismatched values</h1>
                </header>
            
            <article>
                
<p><span>A mismatched value is any value that seems to be of a different data type than the type specified for the column. In the database we are analyzing at this point, there are several values that seem to deviate from the type attributed to the column. For example, in the right column is a cell that contains a dot; it is clear that this is an error in the phase of populating the database. It is equally clear that such a value can cause many problems during the analysis phase, which is why it must be appropriately dealt with.</span></p>
<p>As seen for missing values, mismatched values are <span>also </span>represented in the data quality bar at the top of each column. In the data quality bar, mismatched values are identified in red, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/18b327ab-a0fd-4219-951c-716eca582790.png"/></div>
<p>To fix mismatched data, there are several options available:</p>
<ul>
<li>Change the data type</li>
<li>Replace the values with constant values</li>
<li>Set the values with other columns' values</li>
<li>Transform the data with functions</li>
<li>Delete rows</li>
<li>Hide the column for now</li>
<li>Drop the column</li>
</ul>
<p>In this case, since two of the selected lines contain mismatched data in other columns too, we will eliminate all three columns. To do this, simply click on the <span class="packt_screen">Add</span> button in the <span class="packt_screen">Delete rows</span> area of the suggestions panel.</p>
<p>Analyzing the dataset, we can see that mismatched data is still present. In fact, the data quality bar in the age column has an area in red. We try to fix this problem too. This time it is not appropriate to delete the entire row. In fact, by analyzing the first column, we can see that the <kbd>NA</kbd> value refers to a line whose name is clearly referring to a female (Olivia). So the most appropriate solution is to replace this value with a known value, in this case with <kbd>'F'</kbd>.</p>
<p>To do this, we will write the following formula in the set item of the suggestions panel:</p>
<pre>ifmismatched($col, ['Gender'], 'F')</pre>
<p>A new step will be added to the <span class="packt_screen">Recipe</span> panel. Once again, we can see a preview of the changes made to the dataset. In fact, we can see that the wrong value is no longer present; in its place, there is an <kbd>'F'</kbd>.</p>
<p>We have so far adjusted several things, but at first glance, there is still something to be done. If we pay attention to the <span class="packt_screen">right1</span> column, which represents the percentage of correct answers provided, we notice that the range of values is as follows: -19 to 98. But -19 is obviously an incorrect value since for that variable, the permissible values are between 0 and 100 (this is a percentage). We can assume that a minus sign was added by mistake when creating the dataset. We can then modify this value, leaving only the value 19.</p>
<p>To do this, we perform the following steps:</p>
<ol>
<li>Open the <span class="packt_screen">Recipe</span> panel. Just click on the icon (<span class="packt_screen">Recipe</span>) at the top left of the <span class="packt_screen">Run Job</span> button.</li>
<li>Click on the <span class="packt_screen">New Step</span> button. The Transform Builder is opened.</li>
<li>In the Transformation drop-down menu, select <span class="packt_screen">Apply formula</span> in the list of available transforms.</li>
<li>Specify the <span class="packt_screen">Columns</span> (<span class="packt_screen">right</span>).</li>
<li>Edit the following formula in the formula box:</li>
<li><kbd>IF($col==-19,19,$col)</kbd></li>
<li>Click on the <span class="packt_screen">Add</span> button.</li>
<li>A new step is added to the <span class="packt_screen">Recipe</span> panel.</li>
</ol>
<p>The operations we have added to the <span class="packt_screen">Recipe</span> panel are five, as shown in this screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7c39eaff-8aa3-476d-b292-b8d4016d7185.png" style=""/></div>
<p>By analyzing the <span class="packt_screen">Recipe</span> panel, we then have a summary of the actions we have planned on the dataset. Moreover, a visual analysis of the dataset preview with the changes made does not show any anomalies to be fixed. In the data quality bar at the top of each column, no red/black zones are highlighted, although it's too early to party yet! The data preparation work is far from done.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding outliers in the data</h1>
                </header>
            
            <article>
                
<p>Outliers are the values that, compared to others, are particularly extreme (a value clearly distant from the other available observations). The presence of outliers causes a hindrance because they tend to distort the results of data analysis, in particular in descriptive statistics and correlations. It is ideal to identify these outliers in the data cleaning phase itself; however, they can also be dealt with in the next step of the data analysis. Outliers can be univariate when they have an extreme value for a single variable, or multivariate when they have an unusual combination of values for a number of variables.</p>
<p>Outliers are the extreme values of a distribution that are characterized by being extremely high or extremely low compared to the rest of the distribution, thus representing isolated cases in respect to the rest of the distribution.</p>
<p>There are different methods to detect outliers. Google Cloud Dataprep uses Tukey's method, which uses the <strong>interquartile range</strong> (<strong>IQR</strong>) approach. This method is not dependent on the distribution of the data and ignores the mean and the standard deviation, which are influenced by outliers.</p>
<p>As said before, to determine the outlier values, refer to the IQR given by the difference between the 25th percentile and the 75th percentile, that is, the amplitude of the range within which it falls. These 50 percent of observations occupy the central positions in the ordered series of data. An outlier is a value with positive deviation from the 75th percentile greater than two times the IQR or, symmetrically, a value with a negative deviation from the 25th percentile (in absolute value) greater than two times the IQR.</p>
<p>Practically, an outlier value is either of these two:</p>
<pre>&lt; (25th percentile) - (2 * IQR)<br/>&gt; (75th percentile) + (2 * IQR)</pre>
<p>To identify outliers in individual columns, Google Cloud Dataprep has visual functionality and statistical information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visual functionality</h1>
                </header>
            
            <article>
                
<p>Previously, we have talked about the histogram at the top of each column. This graph displays the count of each value detected in the column (for string data) or the count of values ​​within a numeric range (for numerical data).</p>
<p>The visual functionality offered by Google Cloud Dataprep refers precisely to the use of these histograms to identify unusual values ​​or outliers, which should be removed or corrected before performing any analysis on the entire dataset.</p>
<p>The type of histogram returned depends on the type of data contained in the column. In fact, for numeric data, each bar refers to a range of values ​​and the bars are sorted in numerical order. For categorical types, each vertical bar covers a single value, ordered by the values ​​that occur most frequently.</p>
<p>By moving the mouse over a bar of the histogram, a series of information is returned. In this way, we can highlight specific values, obtaining the count of a value and the percentage that that value represents in the total count of values ​​in the column. We can also select a specific bar; in this case, the rows containing them are highlighted and the suggestion panel is displayed for the management of these values.</p>
<p>To select different bars at the same time:</p>
<ul>
<li>Use <em>Ctrl</em> + click to select multiple bars</li>
<li>Click and drag over a range of bars</li>
</ul>
<p>To better understand the usefulness of this visual functionality, let us refer to an example. In particular, we will analyze the dataset already used in the previous sections, which we have already had the opportunity to modify. Let's take a look at the age column, as it is currently presented in the preview offer with the changes already made effective. We can see that the histogram has four bars grouped on the left and only one bar isolated on the right. This particular form of the histogram is identifying the presence of an outlier, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5a952016-9736-401b-8b8d-57f5a358f8c9.png" style=""/></div>
<p class="mce-root">In this case, the presence of the anomalous value is evident; more generally, the presence of bars isolated at the ends of the graph must alert us.</p>
<p class="mce-root">If a dataset contains multiple instances of outliers, it is necessary to further investigate. Generally, if the dataset contains a large number of outliers, it is necessary to review these values and their data in other columns before performing operations to modify or remove these rows, since the removal of these values can become statistically significant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical information</h1>
                </header>
            
            <article>
                
<p class="mce-root">The visual analysis we have done so far does not allow us to easily identify the presence of outliers in some cases. To obtain more information, we can examine detailed statistics of the values in the currently selected column, including data on outliers, available in the <span class="packt_screen">Column Details</span> panel. To open the <span class="packt_screen">Column Details</span> panel, just select <span class="packt_screen">Column Details</span> from the drop-down menu of a column; the following panel is opened:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a724a0ff-cc64-4690-890d-80a5c9779b4c.png"/></div>
<p>In this panel, a lot of information is available, some of which is as follows:</p>
<ul>
<li>Summary stats the count of valid (unique and outliers), mismatched, and missing values</li>
<li>Statistics means you get many options such as, min, max, average, lowest and highest quartiles, median, and standard deviation</li>
<li>Value histogram</li>
<li>Top values</li>
<li>Outliers</li>
</ul>
<p>In this case, any outliers are clearly identified and indicated. Furthermore, the histogram is now much more precise, clearly indicating the arrangement of the bars.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing outliers</h1>
                </header>
            
            <article>
                
<p>So far we have seen different techniques for identifying possible outliers. What should we do after identifying them? After identifying the values ​​that are outliers in the column, you need to determine whether these values ​​are valid or invalid for the dataset.</p>
<p>If these are invalid values ​​due to an error in the population phase of the dataset, then we must correct them. This operation may involve the replacement of this value with a presumably valid one or the removal of the entire row. In this latter case, we must pay attention to the weight that this action can have on the whole dataset.</p>
<p>To replace the value <kbd>100</kbd>, which seems to us an invalid value <span>in all respects </span>(maybe it was <kbd>10</kbd> and an extra zero was added), we can insert the following formula:</p>
<pre>IF(($col == 100),10, $col)</pre>
<p>Instead, if the removal of this record does not assume statistically significant importance, we can adopt a simple erasing instruction, as shown here:</p>
<pre>DELETE row: age == 100</pre>
<p>If the data seems valid (in reality, a 100-year age is possible for human beings), we can leave it as it is. Or we can convert it into a value that seems to us to be statistically more significant. For example, we can decide to replace this value with the average value of the entire column so as to preserve at least the information of this observation derived from the other columns. To replace 100 with the average value of the column, use the following formula:</p>
<pre>if($col &gt; 80, average($col), $col)</pre>
<p>Ultimately we choose the first option, so <kbd>100</kbd> will be replaced with <kbd>10</kbd> in the age column. To do this, we insert (as always) the formula just proposed in a new step, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/af8b0974-b7d0-4420-9d07-e9a64ada601d.png" style=""/></div>
<p><span>To view a preview of the change made, click on the</span> <span class="packt_screen">Add</span> <span>button and a new step will be added to our</span> <span class="packt_screen">Recipe</span> <span>panel. In the screenshot, we can also verify that now the range of values has been significantly reduced: 10 to 32 instead of 15 to 100. Not only that, but also the histogram no longer has bars</span> isolated <span>at the end.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Run Job</h1>
                </header>
            
            <article>
                
<p><span>We have planned several operations on our database: it is time to make these changes. To do this, just click on <span class="packt_screen">Run Job</span> on the Transformer page. In this way, the <span class="packt_screen">Run Job</span> page will be open, where we can specify transformation and profiling jobs for the currently loaded dataset. Available options include output formats and output destinations.</span></p>
<p>The <span class="packt_screen">Profile Results</span> option allows us to generate a visual result profile. The visual profile is very useful for examining the problems of our recipe and iterating, even if it is a process that requires a lot of resources. If the dataset we are processing is large, disabling the profiling of the results can improve the overall execution speed of the job.</p>
<p>After setting the available options correctly, we can queue the specified job for execution by simply clicking <span class="packt_screen">Run Job</span>. Once this is done, the job is queued for processing. At the end of the process, you can view the results of successful runs using the <span class="packt_screen">Dataset Details</span> page.</p>
<p>Flow processing times depend on the availability of the server and the size of the dataset. In our case, the dataset is really small, so we just have to wait for the service to become available. Click on <span class="packt_screen">View Results</span> to open the job on the <span class="packt_screen">Job Results</span> page, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><span class="packt_screen"><img src="assets/ccfb166f-33cb-472e-9e6f-5ac822879874.png"/></span></div>
<p>In the previous screenshot (<span class="packt_screen">Job Results</span> page), it is possible to review the effects of the transformation recipe on the entire dataset. Statistics and data histograms are available that provide general visibility on the quality of our transformation recipe.</p>
<p>At the top of the screenshot on the left, you can find a series of summary information on the data contained in the generated dataset. In particular, the counts of valid values, non-corresponding values, and missing values are shown. These values are shown in full for the entire dataset.</p>
<p><span>A series of summary information on the work performed </span>always appears in the upper part of the screenshot, but this time <span>you can find it </span>on the right.</p>
<p>In the lower part of the screenshot, we can visualize the details of the transformations made on the single columns. Depending on the data type of the column, the information about the variables is displayed.</p>
<p>Finally, in the upper part of the screenshot, on the right, there are two buttons:</p>
<ul>
<li><strong>View Dependencies</strong>: To see the recipes and datasets on which the job depends</li>
<li><strong>Export Results</strong>: To export the results</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scale of features</h1>
                </header>
            
            <article>
                
<p>Data scaling is a preprocessing technique usually employed before feature selection and classification. Many artificial intelligence-based systems use features that are generated by many different feature extraction algorithms, with different kinds of sources. These features may have different dynamic ranges. Popular distance measures, such as Euclidean distance, implicitly assign more weighting to features with large ranges than those with small ranges. Feature scaling is thus required to approximately equalize ranges of the features and make them have approximately the same effect in the computation of similarity.</p>
<p>In addition, in several data mining applications with huge numbers of features with large dynamic ranges, feature scaling may improve the performance of the fitting model. However, the appropriate choice of these techniques is an important issue. This is because applying scaling on the input could change the structure of data and thereby affect the outcome of multivariate analysis used in data mining.</p>
<p>So far, we have worked on the data to correct any errors or omissions. We can say that at this point all variables contained in the dataset are complete with consistent data. What about different variables characterized by different ranges and units? There can be variables in a data frame where values for one feature could range from 1 to 10 and values for another feature could range from 1 to 1,000.</p>
<p>In data frames such as these, owing to mere greater numeric range, the impact on response variables by the feature having greater numeric range could be more than the one having less numeric range, and this could, in turn, impact prediction accuracy. Our goal is to improve predictive accuracy and not allow a particular feature to impact the prediction due to a large numeric value range. Thus, we may need to scale values under different features such that they fall under a common range. Through this statistical procedure, it is possible to compare identical variables belonging to different distributions, but also different variables, or variables expressed in different units. Two methods are usually well known for rescaling data: normalization and standardization.</p>
<div class="packt_tip">
<p>Remember, it is good practice to rescale the data before training a machine learning algorithm. With rescaling, data units are eliminated, allowing you to easily compare data from different locations.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Min–max normalization</h1>
                </header>
            
            <article>
                
<p>Min-max normalization (usually called <strong>feature scaling</strong>) performs a linear transformation on the original data. This technique gets all the scaled data in the range (0, 1). The formula to achieve this is the following:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/545d977f-9de2-4280-b08a-0436149980c5.png" style="width:14.75em;height:3.50em;"/></div>
<p>Min-max normalization preserves the relationships among the original data values. The cost of having this bounded range is that we will end up with smaller standard deviations, which can suppress the effect of outliers.</p>
<p>To better understand how to perform a min-max normalization, just analyze an example. We will use a dataset contained in the <kbd>Airquality.csv</kbd> file.</p>
<div class="packt_infobox">
<p>This dataset is available at the UCI machine learning repository, a large collection of data, at the following link: <a href="https://archive.ics.uci.edu/ml/index.php" target="_blank">https://archive.ics.uci.edu/ml/index.php</a>.<br/>
<span>S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, <em>On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario</em>, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February 2008, Pages 750-757, ISSN 0925-4005 at: <a href="https://www.sciencedirect.com/science/article/pii/S0925400507007691" target="_blank">https://www.sciencedirect.com/science/article/pii/S0925400507007691</a>.</span></p>
</div>
<p>These are the daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973:</p>
<ul>
<li><strong>Ozone</strong>: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt island</li>
<li><strong>Solar.R</strong>: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at central park</li>
<li><strong>Wind</strong>: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia airport</li>
<li><strong>Temp</strong>: Maximum daily temperature in degrees Fahrenheit at LaGuardia airport</li>
</ul>
<p>The data was obtained from the New York State Department of Conservation (ozone data) and the <strong>National Weather Service</strong> or <strong>NWS</strong> (meteorological data).</p>
<p>A data frame consists of 154 observations on 6 variables:</p>
<table>
<tbody>
<tr>
<td>
<p><strong>Name</strong></p>
</td>
<td>
<p><strong>Type</strong></p>
</td>
<td>
<p><strong>Units</strong></p>
</td>
</tr>
<tr>
<td>
<p>Ozone</p>
</td>
<td>
<p> numeric</p>
</td>
<td>
<p> ppb</p>
</td>
</tr>
<tr>
<td>
<p>Solar</p>
</td>
<td>
<p> numeric</p>
</td>
<td>
<p> lang</p>
</td>
</tr>
<tr>
<td>
<p>Wind</p>
</td>
<td>
<p> numeric</p>
</td>
<td>
<p> mph</p>
</td>
</tr>
<tr>
<td>
<p>Temp</p>
</td>
<td>
<p> numeric</p>
</td>
<td>
<p> degrees F</p>
</td>
</tr>
<tr>
<td>
<p>Month</p>
</td>
<td>
<p> numeric</p>
</td>
<td>
<p> 1 to 12</p>
</td>
</tr>
<tr>
<td>
<p>Day</p>
</td>
<td>
<p> numeric</p>
</td>
<td>
<p> 1 to 31</p>
</td>
</tr>
</tbody>
</table>
<p>As can be seen, the six variables are characterized by different units of measurement. As we did in the previous sections, even in this case, we start preparing the data by creating a new flow from the flows screen, and then we import the <kbd>.csv</kbd> file into Google Cloud Dataprep. The following window opens:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/777479ea-23cf-46f0-b15b-b625bcf8053b.png"/></div>
<p>As we have anticipated, the four variables have different units of measure this implies that the ranges of values are very different. In fact, by analyzing the upper part of each column in the previous screenshot, we can obtain the following ranges:</p>
<ul>
<li><strong>Ozone</strong>: 1 to 168</li>
<li><strong>Solar.R</strong>: 7 to 334</li>
<li><strong>Wind</strong>: 2 to 21</li>
<li><strong>Temp</strong>: 56 to 97</li>
</ul>
<p>Before proceeding with standardization, we eliminate some problems highlighted in the data quality bar. Mismatched values have been identified. To be precise:</p>
<ul>
<li><strong>Ozone</strong>: 37 mismatched values</li>
<li><strong>Solar.R</strong>: 7 mismatched values</li>
</ul>
<p>First of all, we will try to fix these problems as we have learned to do in the previous section.</p>
<p>We start with the Ozone column; we proceed to click on the red area of the data quality bar. In this way, all 37 mismatched values are highlighted. At the same time, the suggestions panel is opened on the right of the window. In particular, the first suggestion advises us to delete the lines with mismatched values in Ozone. We simply click on the <span class="packt_screen">Add</span> button. The following line will be added to the <span class="packt_screen">Recipe</span> panel:</p>
<pre>Delete rows where ISMISMATCHED(Ozone, ['Integer'])</pre>
<p>We perform the same operation for the <kbd>Solar_R</kbd> column; the following line will be added to the <span class="packt_screen">Recipe</span> panel:</p>
<pre>Delete rows where ISMISMATCHED(Solar_R, ['Integer'])</pre>
<p>In both cases, the preview window shows us that no mismatched value is now present in the data. At this point, we can take care of the normalization. As we have specified earlier, the variables ranges are very varied. We want to eliminate this feature through min-max normalization. As stated earlier, to apply this procedure we have to calculate the minimum and maximum for each variable. To do this, we can apply two functions available in Google Cloud Dataprep: <kbd>MIN</kbd> and <kbd>MAX</kbd>.</p>
<div class="packt_tip">
<p>Google Cloud Dataprep support functions typically found in most desktop spreadsheet packages. Functions can be used to create formulas that manipulate data in the columns.</p>
</div>
<p>Previously, we have proposed the formula for normalization; to apply it to a column, we perform the following steps:</p>
<ol>
<li>In the <span class="packt_screen">Recipe</span> panel, click on the <span class="packt_screen">NEW STEP</span> button. Remember? To open the <span class="packt_screen">Recipe</span> panel, just click on the icon (<span class="packt_screen">Recipe</span>) at the top left of the <span class="packt_screen">Run Job</span> button.</li>
<li>From the Transformation drop-down menu, select <span class="packt_screen">Apply formula</span> item (this item sets the values of one or more columns to the result of a formula).</li>
<li>From the column box, select the <span class="packt_screen">Ozone</span> column, for example (the same procedure can be applied to all the dataset variables).</li>
<li>In the <span class="packt_screen">Formula</span> box, edit the following formula: <kbd>(Ozone-MIN(Ozone))/(MAX(Ozone)-MIN(Ozone))</kbd></li>
<li>Just click on the <span class="packt_screen">Add</span> button.</li>
</ol>
<p>The following statement is added to the <span class="packt_screen">Recipe</span> panel:</p>
<pre>Set Ozone to (Ozone-MIN(Ozone))/(MAX(Ozone)-MIN(Ozone))</pre>
<p>We follow the same procedure for the other three variables: <kbd>Solar_R</kbd>, <kbd>Wind</kbd>, and <kbd>Temp</kbd>. At the end, we will have added the following lines to the <span class="packt_screen">Recipe</span> panel:</p>
<pre>Set Solar_R to (Solar_R -MIN(Solar_R))/(MAX(Solar_R)-MIN(Solar_R))<br/>Set Wind to (Wind -MIN(Wind))/(MAX(Wind)-MIN(Wind))<br/>Set Temp to (Temp -MIN(Temp))/(MAX(Temp)-MIN(Temp))</pre>
<p>The Transformer page has been changed as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e6c98f0f-7342-4faa-b258-62bc105960d0.png"/></div>
<div>
<p>It is apparent that now the data is all between zero and one; this happens for each column of the dataset and then for each variable. The scale differences due to the different units of measurement have therefore been removed.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">z score standardization</h1>
                </header>
            
            <article>
                
<p><span>This technique consists of subtracting the mean of the column from each value in a column, and then dividing the result by the standard deviation of the column. The formula to achieve this is the following:</span></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/7a9d8cb9-10f7-43b5-b52f-865fbbb0b69e.png" style="width:14.58em;height:3.58em;"/></div>
<p>The result of standardization is that the features will be rescaled so that they’ll have the properties of a standard normal distribution, as follows:</p>
<ul>
<li><em>μ=0</em></li>
<li><em>σ=1</em></li>
</ul>
<p>μ is the mean and <em>σ</em> is the standard deviation from the mean.</p>
<p>In summary, the z score (also called the <strong>standard score</strong>) represents the number of standard deviations with which the value of an observation point or data differ than the mean value of what is observed or measured. Values more than the mean have positive z scores, while values less than the mean have negative z scores. The z score is a quantity without dimension, obtained by subtracting the population mean from a single rough score and then dividing the difference for the standard deviation of the population.</p>
<p>Once again, to standardize the data, we will use the same procedure used to min-max normalization. This time the two functions are changed as follows:</p>
<ul>
<li><kbd>AVERAGE</kbd>: Calculates the average for each column</li>
<li><kbd>STDEV</kbd>: Calculates the standard deviation for each column</li>
</ul>
<p>To perform a z score standardization, just analyze the same dataset used for min-max normalization. I refer to dataset called Airquality.csv, which contains daily readings of the following air quality values for May 1, 1973 (a Tuesday) to September 30, 1973.</p>
<p>To apply z score standardization to a dataset column, perform the following steps:</p>
<ol>
<li>In the <span class="packt_screen">Recipe</span> panel, click on the <span class="packt_screen">NEW STEP</span> button. Again, to open the <span class="packt_screen">Recipe</span> panel, just click on the icon (<span class="packt_screen">Recipe</span>) at the top left of the <span class="packt_screen">Run Job</span> button.</li>
<li>From the Transformation drop-down menu, select <span class="packt_screen">Apply formula</span> (this item sets the values of one or more columns to the result of a formula).</li>
<li>From the column box, select the <span class="packt_screen">Ozone</span> column (the same procedure can be applied to all the dataset variables).</li>
<li>In the <span class="packt_screen">Formula</span> box, edit the following formula:</li>
<li><kbd>(Ozone- AVERAGE(Ozone))/STDEV(Ozone)</kbd></li>
<li>Just click on the <span class="packt_screen">ADD</span> button.</li>
</ol>
<p>The following statement is added to the <span class="packt_screen">Recipe</span> panel:</p>
<pre>Set Ozone to (Ozone- AVERAGE(Ozone))/STDEV(Ozone)</pre>
<p>We follow the same procedure for the other three variables: <kbd>Solar_R</kbd>, <kbd>Wind</kbd>, and <kbd>Temp</kbd>. At the end, we will have added the following lines to the <span class="packt_screen">Recipe</span> panel:</p>
<pre>Set Solar_R to (Solar_R - AVERAGE(Solar_R))/STDEV(Solar_R)<br/>Set Wind to (Wind - AVERAGE(Wind))/STDEV(Wind)<br/>Set Temp to (Temp - AVERAGE(Temp))/STDEV(Temp)</pre>
<p>The transformer page has been changed, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/47b95f55-7b8c-43b2-be42-b1d71a872987.png"/></div>
<p>The modification made by the z score standardization is evident: the data ranges are quite similar. This happens for each column of the dataset, and then for each variable. The differences in scale due to the different units of measurement have therefore been removed.</p>
<p class="mce-root">According to the assumptions, all variables must have <kbd>average= 0</kbd> and <kbd>stdev =1</kbd>. Let's verify that. To do so, just use the statistical information available in the <span class="packt_screen">Column Details</span> panel, already used in the previous sections. To open the <span class="packt_screen">Column Details</span> panel, select <span class="packt_screen">Column Details</span> from the drop-down menu of a column; the following panel is opened:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3695a5e9-314b-41b8-9b9b-7034543dda15.png"/></div>
<div>
<p>So, we have verified that the <span class="packt_screen">Ozone</span> variable has an average of zero and a standard deviation of one. The same check can be executed for the other variables.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google Cloud Dataflow</h1>
                </header>
            
            <article>
                
<p><span>Google Cloud Dataflow is a fully managed service for creating data pipelines that transform, enrich, and analyze data in batch and streaming modes. Google Cloud Dataflow extracts useful information from data, reducing operating costs without the hassle of implementing, maintaining, or resizing the data infrastructure.</span></p>
<div>
<p>A pipeline is a set of data processing elements connected in series, in which the output of one element is the input of the next. The data pipeline is implemented to increase throughput, which is the number of instructions executed in a given amount of time, parallelizing the processing flows of multiple instructions.</p>
<p>By appropriately defining a process management flow, significant resources can be saved in extracting knowledge from the data. Thanks to a serverless approach to provisioning and managing resources, Dataflow offers virtually unlimited capacity to solve the most serious data processing problems, but you only pay for what you use.</p>
<p>Google Cloud Dataflow automates the provisioning and management of processing resources to reduce latency times and optimize utilization. It is no longer necessary to activate the instances manually or to reserve them. Automatic and optimized partitioning allows the pending job to be dynamically redistributed. You do not need to go for keyboard shortcuts or preprocess your input data. Cloud Dataflow supports rapid and simplified pipeline development using expressive Java and Python APIs in the Apache Beam SDK.</p>
<p>Cloud Dataflow jobs are billed per minute, based on the actual use of workers in batch mode or streaming of Cloud Dataflow. Jobs that use other GCP resources, such as Cloud Storage or Cloud Pub/Sub, are billed based on the price of the corresponding service.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, we explored Google Cloud Dataprep, a service useful to preprocess the data, extract features, and clean up records. We got into practical details with real-life examples. We started by taking a look at the Cloud application interface to discover some preliminary information needed to access the platform. We then analyzed the techniques available for the preparation of the most suitable data for analysis and modeling, which includes the imputation of missing data, detecting and eliminating outliers, and mismatched values treatment. We discovered different ways to transform data and the degree of cleaning data. Then we learned how to normalize our data, in which data units are eliminated, allowing us to easily compare data from different locations.</span></p>


            </article>

            
        </section>
    </body></html>