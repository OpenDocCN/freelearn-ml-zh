- en: Mobile Application Using Google Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in [Chapter 1](51fcaf51-eb68-4493-afc2-0b02f1c1d50e.xhtml), *Introduction
    to Machine Learning on Mobile,* we know that machine learning in mobile applications
    can be implemented either on-device or it can be implemented using machine learning
    cloud provider services. There are various machine learning cloud providers:'
  prefs: []
  type: TYPE_NORMAL
- en: Clarifai
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft Azure Cognitive Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM Watson
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Machine Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to dive deeply into Google Cloud Vision to understand
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Features of Google Cloud Vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to utilize the Google Cloud Vision label-detection technique in an Android
    Mobile application to determine what is the picture taken by the camera. That
    is, we basically feed an image into Google Cloud Vision and see how it labels
    the image. Google Vision is going to predict the image that it receives from the
    mobile application and provide a label for the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features of Google Cloud Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google Cloud Vision API comprises various complex and powerful machine learning
    models that help to perform image analysis. It classifies images into various
    categories using an easy-to-use REST API. The important features provided by Google
    Cloud Vision include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Label detection**: This enables us to classify images into thousands of categories.
    The images can be categorized into various common category labels, such as *animals*
    and *fruits*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image attribute detection**: This enables us to detect individual objects
    from within images. It can also detect attributes such as prominent color.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face detection**: This enables us to detect faces from within images. If
    there are multiple faces in the images, each can be detected individually. It
    can also detect the prominent attributes associated with a face, such as wearing
    a helmet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logo detection**: This enables us to detect printed words from images. Prominent
    logos are trained which can be detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Landmark detection**: It is trained to detect prominent landmarks – natural
    and man-made–so these are detected through Google Vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optical character recognition**: This helps to detect the text within images
    even if they aren''t in English. This supports a wide range of languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explicit content detection**: This helps to identify the type of content
    or sentiment of the content, such as *violent* or *humorous*. It enables us to
    perform sentiment analysis of images by leveraging the metadata information that
    can be built.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search web**: This searches the web for similar images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these features provided by Google Cloud Vision can be used by invoking simple
    RESTful APIs provided by Google. However, for their use, there is a price attached
    to using each feature. A combination of features can also be used. The pricing
    details can be found on the Google Cloud Vision website: [https://cloud.google.com/vision/](https://cloud.google.com/vision/).
  prefs: []
  type: TYPE_NORMAL
- en: Sample mobile application using Google Cloud Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to try a sample Android mobile application using
    Google Cloud Vision. We are going to capture an image from the camera of the cell
    phone, upload the image to Google Cloud Vision, and see what it predicts the image
    to be. This is going to use the label detection feature of Google Cloud Vision,
    which determines the label of the uploaded image.
  prefs: []
  type: TYPE_NORMAL
- en: How does label detection work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Vision API can detect and extract information about entities within an image,
    across a broad group of categories. Labels can identify objects, locations, activities,
    animal species, products, and more. Labels are returned in English only.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image whose label is to be determined and the features of the Google Vision
    that we intend to use needs to be sent in the request API. The feature can be
    any of the features listed in the *Features of Google Cloud Vision* section, such
    as label detection or logo detection. If there is any additional image context
    that needs to be sent across along with the image, it can be sent as an additional
    parameter. The request API JSON format is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The image object can be a base64-encoded string or it can be a URL of the image
    that needs to be analyzed. The URL can be a Google Cloud Storage image location,
    or a publicly accessible image URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response for the request is going to be a list of annotations based on
    the features requested. In our case, it is going to be label annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The returned `EntityAnnotation` object is going to contain the label of the
    image, the prediction score, and other useful information. All labels that match
    the input image object are returned as an array list with the prediction score,
    based on which we could perform the required inference needed in our application.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the basics of how label detection works, let's start
    creating the Android application.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to get started start exploring the Google Vision and to write a program
    using the services exposed by Google vision, the following are required to be
    setup, so we can get our hands dirty:'
  prefs: []
  type: TYPE_NORMAL
- en: A Google Cloud Platform account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Project on Google Cloud Console
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Android Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mobile phone running Android 5.0 or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section gives details about the key activities we need to do before we
    can start using the Google Cloud Vision API from our mobile application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Google Cloud Vision API should be enabled in the Google Cloud Console and
    an API key should be created that will be used in the mobile application code.
    Please perform the following steps to get the Cloud Vision API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open [cloud.google.com/vision](http://Google%20Cloud%20Vision).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to Console. If you do no have a trial account, it will ask you to create
    one and complete the process.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Enable billing so we get $300 free credit. Once we have the account, we can
    go to Console and complete the process of creating the key.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From the Console, create a project.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Open that project. Go to API services | Library search for cloud vision API.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on it and enable it.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to API Services | Credentials.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to Credentials | API Key.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the API key.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the API key; this will be used in the mobile application code.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the dependencies required in the mobile client application to use the Google
    Cloud Vision API. The Google API Client will be needed and hence this needs to
    be added to the client project. These will need to be specified in the Gradle
    build file. The sample Gradle file with the key dependencies is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the Application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see the key flows of the source code to understand
    how the Google Vision API works from an Android mobile application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vision class represents the Google API Client for Cloud Vision. The first
    step is to initialize the Vision class. We do it through the Builder, to which
    we specify the transport mechanism and the JSON factory to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to assign the API key to the Vision Builder so it can start
    interacting with the cloud APIs. The key we have created is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to get the Vision instance through which the cloud APIs can
    be invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are going to capture a picture and send the picture to the cloud API
    to detect its label. The code to capture the picture through the camera is the
    usual Android stuff. The following code provides details on how the image is converted
    into a Vision Request for label detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Google Cloud Vision will be called as an **async task**. The response received
    from the API will be analyzed to provide data in user-readable format. The following
    code provides details of the response received from Google Vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The labels returned for the image can be viewed by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section displays the Android application screen when a mobile phone has
    been captured and sent to the vision API. Possible labels are listed in the output
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30a0c8ed-bd27-429a-a9a1-fb23cce4d065.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how Google Cloud Vision works, and can how to
    invoke it from a mobile application without much effort. We saw how easy it is
    to make complex machine learning predictions without the hassles of model selection
    and training. In the next chapter, we will explore the future of machine learning
    in the field of mobile applications.
  prefs: []
  type: TYPE_NORMAL
