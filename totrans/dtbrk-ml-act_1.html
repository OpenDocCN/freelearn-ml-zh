<html><head></head><body>
		<div id="_idContainer022">
			<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor016"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor017"/>Getting Started with This Book and Lakehouse Concepts</h1>
			<p class="author-quote">“Give me six hours to chop down a tree, and I will spend the first four sharpening the axe.”</p>
			<p class="author-quote">– Abraham Lincoln</p>
			<p>We will start with a basic overview of how <strong class="bold">Databrick</strong>’s <strong class="bold">Data Intelligence Platform</strong> (<strong class="bold">DI</strong>) is an open platform on a <strong class="bold">lakehouse</strong> architecture and the advantages of this in developing <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications. For brevity, we will use terms such as <em class="italic">Data Intelligence Platform</em> and <em class="italic">Databricks</em> interchangeably throughout the book. This chapter will introduce the different projects and associated datasets we’ll use throughout the book. Each project intentionally highlights a function or component of the DI Platform. Use the example projects as hands-on lessons for each platform element we cover. We progress through these projects in the last section of each chapter – namely, applying <span class="No-Break">our learning.</span></p>
			<p>Here is what you will learn in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>The components of the Data <span class="No-Break">Intelligence Platform</span></li>
				<li>Advantages of the <span class="No-Break">Databricks Platform</span></li>
				<li>Applying <span class="No-Break">our learning</span></li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor018"/>The components of the Data Intelligence Platform</h1>
			<p>The Data Intelligence Platform <a id="_idIndexMarker000"/>allows your entire organization to leverage your data and AI. It’s built on a lakehouse architecture to provide an<a id="_idIndexMarker001"/> open, unified foundation for all data and governance layers. It is powered by a <strong class="bold">Data Intelligence Engine</strong>, which understands the context of your data. For practical purposes, let’s talk about the components of the Databricks Data <span class="No-Break">Intelligence Platform:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer008">
					<img alt="Figure 1.1 – The components of the Databricks Data Intelligence Platform" src="image/B16865_01_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – The components of the Databricks Data Intelligence Platform</p>
			<p>Let’s check out the following list with the<a id="_idIndexMarker002"/> descriptions of the items in <span class="No-Break">the figure:</span><a id="_idTextAnchor019"/></p>
			<ul>
				<li><strong class="bold">Delta Lake</strong>: The data layout within the Data Intelligence Platform is automatically optimized based on common data <span class="No-Break">usage patterns</span></li>
				<li><strong class="bold">Unity Catalog</strong>: A unified governance model to secure, manage, and share your <span class="No-Break">data assets</span></li>
				<li><strong class="bold">Data Intelligence Engine</strong>: This uses AI to enhance the <span class="No-Break">platform’s capabilities</span></li>
				<li><strong class="bold">Databricks AI</strong>: ML tools to support end-to-end<a id="_idIndexMarker003"/> ML solutions and <strong class="bold">generative AI</strong> capabilities, including creating, tuning, and <span class="No-Break">serving LLMs</span></li>
				<li><strong class="bold">Delta live tables</strong>: Enables automated data ingestion and <span class="No-Break"><strong class="bold">data quality</strong></span></li>
				<li><strong class="bold">Workflows</strong>: A fully integrated orchestration service to automate, manage, and monitor multi-task workloads, queries, <span class="No-Break">and pipelines</span></li>
				<li><strong class="bold">Databricks SQL (DBSQL)</strong>: An SQL-first interface, similar to how you would interact with a data warehouse, and <a id="_idIndexMarker004"/>with functionality such as text-to-SQL, which lets you use natural language to <span class="No-Break">generate queries</span></li>
			</ul>
			<p>Now that we have our elements defined, let’s discuss how they help us achieve our <span class="No-Break">ML goa<a id="_idTextAnchor020"/>ls.</span></p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor021"/>The advantages of the Databricks Platform</h1>
			<p>Databricks’ implementation <a id="_idIndexMarker005"/>of a lakehouse architecture is unique. Databricks’ foundation is built on a Delta-formatted data lake that Unity Catalog governs. Therefore, it combines a data lake’s scalability and cost-effectiveness with a data warehouse’s governance. This means not only are table-level permissions managed through <strong class="bold">access control lists</strong> (<strong class="bold">ACLs</strong>) but file and object-level access are also <a id="_idIndexMarker006"/>regulated. This change in architecture from a data lake and/or a data warehouse to a unified platform is ideal – a lakehouse facilitates a wide range of new use cases for analytics, business intelligence, and data science projects across an organization. See the <em class="italic">Introduction to Data Lakes</em> blog post in the <em class="italic">Further reading</em> section for more information on <span class="No-Break">lakehouse benefits.</span></p>
			<p>This section will discuss<a id="_idIndexMarker007"/> the importance of open source frameworks and two critical advantages they provide – transparency <span class="No-Break">and flexibility.</span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor022"/>Open source features</h2>
			<p>How open source features relate to the Data Intelligence Platform is unique. This uniqueness lies in the concepts of openness and transparency, often referred to as the “glass box” approach by Databricks. It means that when you use the platform to create assets, there’s no inscrutable <a id="_idIndexMarker008"/>black box that forces you to depend on a specific vendor for usage, understanding, or storage. A genuinely open lakehouse architecture uses open data file formats to make accessing, sharing, and removing your data simple. Databricks has optimized the managed version of Apache Spark to leverage the open data format Delta (which we’ll cover in more detail shortly). This is one of the reasons why the Delta format is ideal for most use cases. However, nothing <a id="_idIndexMarker009"/>stops you from using something such as the CSV or Parquet format. Furthermore, Databricks introduced <strong class="bold">Delta Lake Universal Format</strong> (<strong class="bold">Delta Lake UniForm</strong>) to easily integrate with other file formats such as Iceberg or Hudi. For more details, check out the <em class="italic">Further reading</em> section at the end of <span class="No-Break">this chapter.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em> illustrates the coming together of data formats <span class="No-Break">with UniForm.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer009">
					<img alt="Figure 1.2 – Delta Lake UniForm makes consuming Hudi and Iceberg file formats as easy as consumin﻿g Delta" src="image/B16865_01_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Delta Lake UniForm makes consuming Hudi and Iceberg file formats as easy as consumin<a id="_idTextAnchor023"/>g Delta</p>
			<p>The ability to use third-party and open source software fuels rapid innovation. New advances in data processing and ML can be quickly tested and integrated into your workflow. In contrast, proprietary systems often have longer wait times for vendors to incorporate updates. Waiting for a vendor to capitalize on open source innovation may seem rare, but it is the rule rather<a id="_idIndexMarker010"/> than the exception. This is especially true for data science. The speed of software and algorithmic advances is incredible. Evidence of this frantic pace of innovation can be seen daily on the Hugging Face community website. Developers share libraries and models on Hugging Face; hundreds of libraries are updated daily on the <span class="No-Break">site alone.</span></p>
			<p>Delta, Spark, the Pandas API on Spark (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.3</em>), and MLflow are notable examples of consistent innovation, largely driven by their transparency as open source projects. We mention these specifically because they were all initially created by either the founders of Databricks or company members following <span class="No-Break">its formation.</span></p>
			<p>ML developers benefit significantly from this transparency, as it provides them with unparalleled flexibility, easy integration, and robust support from the open source community – all without the overhead of maintaining an open source <span class="No-Break">full stack.</span></p>
			<p>Starting development as a contractor using Databricks is super-fast compared to when companies require a fresh development environment to be set up. Some companies require a service request to install Python libraries. This can be a productivity killer for data scientists. In Databricks, many of your favorite libraries are pre-installed and ready to use, and of course, you can easily install your own libraries <a id="_idTextAnchor024"/><span class="No-Break">as well.</span></p>
			<p>Additionally, there is a large and vibrant community of Databricks users. The Databricks community website is an excellent resource to ask and answer questions about anything related to Databricks. We’ve included a link in the <em class="italic">Further reading</em> section at the end of <span class="No-Break">this cha<a id="_idTextAnchor025"/>pter.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer010">
					<img alt="Figure 1.3 – The pandas API on Spark" src="image/B16865_01_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – The pandas API on Spark</p>
			<p>The pandas API on Spark is nearly identical syntax to standard pandas, making distributed computing with Spark easier to learn for those who have written pandas code <span class="No-Break">in Python</span></p>
			<p>While continuing with a<a id="_idIndexMarker011"/> focus on transparency, let’s move on to <span class="No-Break">Databricks<a id="_idTextAnchor026"/> </span><span class="No-Break"><strong class="bold">AutoML</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-20">Databrick<a id="_idTextAnchor027"/>s AutoML</h2>
			<p>Databricks refers to its AutoML solution as a <strong class="bold">glass box</strong>. This terminology highlights the fact that there is nothing<a id="_idIndexMarker012"/> hidden from the <a id="_idIndexMarker013"/>user. This feature in the Data Intelligence Platform leverages an open<a id="_idIndexMarker014"/> source library, Hyperopt, in conjunction with Spark for hyperparameter tuning. It intelligently explores different model types in addition to optimizing the parameters in a distributed fashion. The use of Hyperopt allows each run within the AutoML experiment to inform the next run, reducing the overall number of runs needed to reach an optimal solution compared to a grid search. Each run in the experiment has an associated notebook with the code for the model. This method increases productivity, reduces unnecessary computing, and lets scientists perform experiments instead of writing boilerplate code. Once AutoML has converged on the algorithmically optimal solution, there is a “best notebook” for the best<a id="_idIndexMarker015"/> scoring model. We’ll expand on AutoML in several chapters<a id="_idIndexMarker016"/> throughou<a id="_idTextAnchor028"/>t <span class="No-Break">this book.</span></p>
			<h2 id="_idParaDest-21">Reusability and re<a id="_idTextAnchor029"/>producibility</h2>
			<p>As data scientists, transparency is especially important. We do not trust black box models. How do you use them without understanding them? A model is only as good as the data going in. In addition to <a id="_idIndexMarker017"/>not trusting the models, black boxes create concerns about our research’s reproducibility and model <span class="No-Break">drivers’ explainability.</span></p>
			<p>When we create a model, who does it belong to? Can we get access to it? Can we tweak, test, and, most importantly, reuse it? The amount of time put into the model’s creation is not negligible. Databricks AutoML gives you everything to explain, reproduce, and reuse the models it creates. In fact, you can take the model code or model object and run it on a laptop or wherever. This open source, glass-box, reproducible, and reusable methodology is our<a id="_idTextAnchor030"/> kind <span class="No-Break">of open.</span></p>
			<h2 id="_idParaDest-22">Open file formats give y<a id="_idTextAnchor031"/>ou flexibility</h2>
			<p>Flexibility is also an essential aspect of the Databricks platform, so let’s dive into the file format Delta, an open source project that makes it easy to adapt to many different use cases. For those familiar with Parquet, you can think of Delta as Parquet-plus – Delta files are Parquet files with a transaction log. The transaction log is a game changer. The increased reliability and <a id="_idIndexMarker018"/>optimizations make Delta the foundation of Databricks’ lakehouse architecture. The data lake side of the lakehouse is vital to data science, streaming, and unstructured and semi-structured data formats. Delta has also made the warehouse side possible. There are entire books on Delta; see the <em class="italic">Further reading</em> section for some examples. We are focusing on the fact that it is an open file format with key features that support building <a id="_idTextAnchor032"/><span class="No-Break">data products.</span></p>
			<h3>Integration and control</h3>
			<p>Having an open file format is essential to maintain ownership of your data. Not only do you want to be able to read, alter, and open your data files, but you also want to keep them in your cloud tenant. Maintaining control over your data is possible in the Databricks Data Intelligence<a id="_idIndexMarker019"/> Platform. There is no need to put the data files into a proprietary format or lock them away in a vendor’s cloud. Take a look at <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em> to see how Delta is part of the <span class="No-Break">larger ecosystem.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer011">
					<img alt="Figure 1.4 – The Delta Kernel connection ecosystem" src="image/B16865_01_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – The Delta Kernel connection ecosystem</p>
			<p>The Delta Kernel introduces a fresh approach, offering streamlined, focused, and reliable APIs that abstract away the intricacies of the Delta protocol. By simply updating the Kernel version, connector developers can seamlessly access the latest Delta features without needing to m<a id="_idTextAnchor033"/>odify <span class="No-Break">any code.</span></p>
			<h3>Time-travel versioning in Delta</h3>
			<p>The freedom and flexibility of open file formats make it possible to integrate with new and existing external tooling. Delta Lake, in particular, offers unique support to create data products thanks to features such as time-travel versioning, exceptional speed, and the ability to update and merge changes. Time travel, in this context, refers to the capability of querying different versions of your data table, allowing you to revisit the state of the table before your most recent changes or transformations (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em>). The more obvious use is to back up after making a mistake rather than writing out multiple copies of the table as a safety measure. A possibly less obvious use for time travel is reproducible research. You can access the data your model was trained on in the previous week without<a id="_idIndexMarker020"/> creating an additional copy of the data. Throughout the book, we will detail features of the Data Intelligence Platform you can use to facilitate reproducible research. The following figure shows you how the previous version of a table, relative to a timestamp or a version number, can <span class="No-Break">be queried.</span></p>
			<p class="IMG---Figure"/>
			<div>
				<div class="IMG---Figure" id="_idContainer012">
					<img alt="Figure 1.5 – A code example of the querying techniques to view previous versi﻿ons of a table" src="image/B16865_01_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – A code example of the querying techniques to view previous versi<a id="_idTextAnchor034"/>ons of a table</p>
			<h3>The speed of Databricks’ optimized combination</h3>
			<p>Next, let us discuss the speed of Databricks’ lakehouse architecture. In November 2021, Databricks set a new world <a id="_idIndexMarker021"/>record for the gold standard performance benchmark for data warehousing. The Barcelona Computing Group shared their research supporting this finding. This record-breaking speed resulted from the Databricks’ engines (Spark and Photon) paired with Delta (see the <em class="italic">Databricks Sets Official Data Warehousing Performance Record</em> link in the <em class="italic">Further </em><span class="No-Break"><em class="italic">re<a id="_idTextAnchor035"/>ading</em></span><span class="No-Break"> section).</span></p>
			<h3>The additional benefits of Delta</h3>
			<p>Delta’s impressive features <a id="_idIndexMarker022"/>include <strong class="bold">change data feed</strong> (<strong class="bold">CDF</strong>), <strong class="bold">change data capture</strong> (<strong class="bold">CDC</strong>), and <strong class="bold">schema evolution</strong>. Each <a id="_idIndexMarker023"/>plays a specific role<a id="_idIndexMarker024"/> in data transformation<a id="_idIndexMarker025"/> in support <span class="No-Break">of ML.</span></p>
			<p>Starting with Delta’s CDF capability, it is exactly what it sounds like – a feed of the changed data. Let’s say you have a model looking for fraud, and that model needs to know how many transaction requests have occurred in the last 10 minutes. It is not feasible to rewrite the entire table each time a value for an account needs to be updated. The feature value, or in this case, the number of transactions that occurred in the last 10 minutes, needs to be updated only when the value has changed. The use of CDF in this example enables updates to <a id="_idIndexMarker026"/>be passed to an <strong class="bold">online feature store</strong>; see <em class="italic">Chapters 5</em> and <em class="italic">6</em> for <span class="No-Break">more details.</span></p>
			<p>Finally, let’s talk about<a id="_idIndexMarker027"/> change data capture, a game-changer in the world of data management. Unlike traditional filesystems, CDC in Delta has been purposefully designed to handle data updates efficiently. Let’s take a closer look at CDC and explore its capabilities through two <span class="No-Break">practical scenarios:</span></p>
			<ul>
				<li><strong class="bold">Scenario 1 – effortless record updates</strong>: Picture a scenario involving Rami, one of your customers. He initially made a purchase in Wisconsin but later relocated to Colorado, where he continued to make purchases. In your records, it’s essential to reflect Rami’s new address in Colorado. Here’s where Delta’s CDC shines. It effortlessly updates Rami’s customer record without treating him as a new customer. CDC excels at capturing and applying updates seamlessly, ensuring data integrity without <span class="No-Break">any hassles.</span></li>
				<li><strong class="bold">Scenario 2 – adapting to<a id="_idTextAnchor036"/> evolving data sources</strong>: Now, consider a situation where your data source experiences unexpected changes, resulting in adding a new column containing information about your customers. Let’s say this new column provides insights into the colors of items purchased by customers. This is valuable data that you wouldn’t want to lose. Delta’s CDC, combined with its schema evolution feature, comes to <span class="No-Break">the rescue.</span></li>
			</ul>
			<p>Schema evolution, explored in depth in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, enables Delta to gracefully adapt to schema changes without causing any disruptions. When dealing with a new data column, Delta smoothly incorporates this information, ensuring your data remains up to date while retaining its full historical context. This ensures that you can leverage valuable insights for both presen<a id="_idTextAnchor037"/>t and <span class="No-Break">future analyses.</span></p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor038"/>Applying our learning</h1>
			<p>This book is heavily project-based. Each chapter starts with an overview of the important concepts and Data Intelligence Platform features that will prepare you for the main event – the <em class="italic">Applying our learning</em> sections. Every <em class="italic">Applying our learning</em> section has a <em class="italic">Technical requirements</em> section so that you know what technical resources you will need, in addition to your Databricks workspace and GitHub repository, to complete the project work in th<a id="_idTextAnchor039"/>e <span class="No-Break">respective chapter.</span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor040"/>Technical requirements</h2>
			<p>Here are the technical requirements needed to get started with the hands-on examples used throughout <span class="No-Break">this book:</span></p>
			<ul>
				<li>We use Kaggle for two of our datasets. If you do not already have an account, you wi<a id="_idTextAnchor041"/>ll need to <span class="No-Break">create one.</span></li>
				<li>Throughout the book, we will refer to code in GitHub. Create an account if you do<a id="_idTextAnchor042"/> not already <span class="No-Break">have one.</span></li>
			</ul>
			<h2 id="_idParaDest-25">Get<a id="_idTextAnchor043"/>ting to know your data</h2>
			<p>There are four main projects that progress sequentially throughout the book. In each subsequent chapter, the code will expand upon the code in previous chapters. We chose these projects to highlight a variety of Data Intelligence Platform features across different ML projects. Specifically, we include streaming data into your lakehouse architecture, forecasting <a id="_idIndexMarker028"/>sales, building a <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) model for computer vision, and building a chatbot using <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>) techniques. Read through<a id="_idIndexMarker029"/> the descriptions of each project to get an idea of what it will cover. If some of the concepts and features are unfamiliar, don’t worry! We’ll explain them in<a id="_idTextAnchor044"/> the <span class="No-Break">following chapters.</span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor045"/>Project – streaming transactions</h2>
			<p>This first project is a data solution for the streaming transactions dataset we will generate. The transaction data will include<a id="_idIndexMarker030"/> information such as the customer ID and transaction time, which we’ll use to simulate transactions streaming in real time; see the sample data in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer013">
					<img alt="Figure 1.6 – A sample of the synthetic streaming transactions data" src="image/B16865_01_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – A sample of the synthetic streaming transactions data</p>
			<p>Our goal with this project is to demonstrate how flexible the Data Intelligence Platform is compared to proprietary data warehouses of the past, which were more rigid for data ingestion. Additionally, we want to<a id="_idIndexMarker031"/> highlight important Databricks capabilities such as <strong class="bold">Spark Structured Streaming</strong>, <strong class="bold">Auto Loader</strong>, schema <a id="_idIndexMarker032"/>evolution, Delta Live Tables, and <span class="No-Break"><strong class="bold">Lakehouse Monitoring</strong></span><span class="No-Break">.</span></p>
			<p>When we generate the <a id="_idIndexMarker033"/>transactions, we also generate a label based on statistical distributions (note that the label is random and only used for learning purposes). This is the label we will be predicting. Our journey includes generating transaction records as multiline JSON files, formatting the files to a Delta table, creating a streaming feature for our ML model, wrapping a <strong class="bold">Light Gradient-Boosting Machine</strong> (<strong class="bold">LightGBM</strong> or <strong class="bold">LGBM</strong>) model in a Python function (<strong class="source-inline">pyfunc</strong>) with the preprocessing steps, and <a id="_idIndexMarker034"/>deploying the model wrapper via a workflow. Take a look through the project pipeline in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.7</em> to understand how we’ll progress through <span class="No-Break">this project.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer014">
					<img alt="Figure 1.7 – The project pipeline for the streaming transactions project&#13;&#10;" src="image/B16865_01_7.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer015">
					<img alt="Figure 1.7 – The project pipeline for the streaming transactions project&#13;&#10;" src="image/B16865_01_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – The project pipeline for the streaming transactions project</p>
			<p>That concludes the streaming transaction<a id="_idIndexMarker035"/> project explanation. Next, we will look <a id="_idTextAnchor046"/>at the <span class="No-Break">forecasting project.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor047"/>Project – Favorita sales forecasting</h2>
			<p>This is a typical forecasting project. Our <a id="_idIndexMarker036"/>dataset is hosted on the Kaggle website (see the <em class="italic">Further reading</em> section). We will use the data to build a model to predict the total sales amount for a family of goods at a specific Favorita store in Ecuador. The data includes train, test, and supplementary data. This project will use Databricks’ AutoML for data exploration and to create a baseline model. Take a look through the project pipeline in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.8</em> to understand how we’ll progress through this project. The store sales dataset is a rich time-series dataset, and we encourage you to build on the <a id="_idIndexMarker037"/>project framework we provide using your favorite <span class="No-Break">time-series library.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer016">
					<img alt="Figure 1.8 – The project pipeline for the Favorita store sales project&#13;&#10;" src="image/B16865_01_9.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer017">
					<img alt="Figure 1.8 – The project pipeline for the Favorita store sales project&#13;&#10;" src="image/B16865_01_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – The project pipeline for the Favorita store sales project</p>
			<p>That concludes the forecasting project explanation. Next, we w<a id="_idTextAnchor048"/>ill look at the <span class="No-Break">DL project.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor049"/>Project – multilabel image classification</h2>
			<p>This project is a DL data solution<a id="_idIndexMarker038"/> that uses another Kaggle dataset. We will use these datasets images to fine-tune a deep-learning model, using PyTorch and Lightning to predict a corresponding label. We will implement MLflow code for experiment and model tracking, Spark for fast training and <strong class="bold">inference</strong>, and Delta for data version control. We will deploy the model as we would for a real-time scenario by creating a model wrapper, similar to the <a id="_idIndexMarker039"/>wrapper we use for the streaming transactions project. Take a look through the project pipeline in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.9</em> to understand how we’ll progress<a id="_idTextAnchor050"/> through <span class="No-Break">this project.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer018">
					<img alt="Figure 1.9 – The project pipeline for the multilabel image classification project&#13;&#10;" src="image/B16865_01_11.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer019">
					<img alt="Figure 1.9 – The project pipeline for the multilabel image classification project&#13;&#10;" src="image/B16865_01_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – The project pipeline for the multilabel image classification project</p>
			<p>That concludes the image classification project explanation. Next, we will l<a id="_idTextAnchor051"/>ook at the <span class="No-Break">chatbot project.</span></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor052"/>Project – a retrieval augmented generation chatbot</h2>
			<p>This project is a RAG chatbot. The dataset we use comes from the <em class="italic">arXiv</em> website. We have selected a few research<a id="_idIndexMarker040"/> articles about the impact of generative AI on humans and labor. We will download and store them in a volume in <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. After we download the PDF documents, we will extract and prepare the text through a process of chunking and tokenization, creating embeddings of the documents to be referenced in the chatbot. We will use Databricks Vector Search to store the embeddings. Then, we will use the new Foundation Model API to generate answers when text is retrieved. The final bot will be deployed as an application using <strong class="bold">Databricks Model Serving</strong>. This example<a id="_idIndexMarker041"/> allows you to build a chatbot from start to finish! Take a look through the project pipeline in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.10</em> to understand how we’ll progres<a id="_idTextAnchor053"/>s through <span class="No-Break">this project.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer020">
					<img alt="Figure 1.10 – The project pipeline for the chatbot project&#13;&#10;" src="image/B16865_01_13.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer021">
					<img alt="Figure 1.10 – The project pipeline for the chatbot project&#13;&#10;" src="image/B16865_01_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – The project pipeline for the chatbot project</p>
			<p>The four projects are hands-on examples<a id="_idIndexMarker042"/> that can be implemented on Databricks. <em class="italic">Databricks ML in Action</em> provides best practices and recommendations from an ML perspective based on our experiences and supplements online documentation. All the code and solutions presented in this book have been developed and tested on the full version of Databricks. However, we understand that accessibility matters. There is also a free community version of the Databricks Data Intelligence Platform available, enabling everyone to follow along with the examples to a certain point be<a id="_idTextAnchor054"/>fore considering <span class="No-Break">an upgrade.</span></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor055"/>Summary</h1>
			<p>In this chapter, we introduced you to <em class="italic">Databricks ML in Action</em>. We emphasized that the Databricks Data Intelligence Platform is designed with openness, flexibility, and tooling freedom in mind, which greatly accelerates productivity. Additionally, we’ve given you a sneak peek at the projects and the associated datasets that will be central to <span class="No-Break">this book.</span></p>
			<p>Now that you’ve gained a foundational understanding of the Data Intelligence Platform, it’s time to take the next step. In the upcoming chapter, we’ll guide you through setting up your environment and provide instructions on downloading the project data. This will prepare you for the practical, hands-on ML experiences tha<a id="_idTextAnchor056"/>t lie ahead in <span class="No-Break">this journey.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor057"/>Questions</h1>
			<p> Let’s test ourselves on what we’ve learned by going through the <span class="No-Break">following questions:</span></p>
			<ol>
				<li>How will you use this book? Do you plan to go cover to cover or pick certain sections out? Have you chosen sections <span class="No-Break">of interest?</span></li>
				<li>We covered why transparency in modeling is critical to success. How does Databricks’ glass-box approach to AutoML <span class="No-Break">support this?</span></li>
				<li>Databricks has developed a new way of uniting the open data formats, called UniForm. Which data formats does <span class="No-Break">UniForm unite?</span></li>
				<li>Delta is the foundation of the lakehouse architecture. What is one of the benefits of <span class="No-Break">using Delta?</span></li>
				<li>What is the main advantage of using the Delta file format for large-scale data processing over simple <a id="_idTextAnchor058"/>Parquet <a id="_idTextAnchor059"/>files <span class="No-Break">in Databricks?</span></li>
			</ol>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor060"/>Answers</h1>
			<p>After putting thought into the questions, c<a id="_idTextAnchor061"/>ompare your answers <span class="No-Break">to ours:</span></p>
			<ol>
				<li>We cannot answer this question, but we hope you learn something you <a id="_idTextAnchor062"/>can use in your <span class="No-Break">career soon!</span></li>
				<li>The glass-box approach supports transparency by providing the code run for each run in the experiment and the best run, thus enabling reus<a id="_idTextAnchor063"/>ability <span class="No-Break">and reproducibility.</span></li>
				<li>Apache Iceberg, Apache Hudi, and Linux Foundation Delta Lake (an open source/<a id="_idTextAnchor064"/>unmanaged version <span class="No-Break">of Delta).</span></li>
				<li>There <a id="_idTextAnchor065"/>are several. Here are <span class="No-Break">a few:</span><ul><li>Open <a id="_idTextAnchor066"/>protoc<a id="_idTextAnchor067"/>ol (no <span class="No-Break">vendor lock-i<a id="_idTextAnchor068"/>n)</span></li><li><span class="No-Break">Speed</span></li><li>Cha<a id="_idTextAnchor069"/>nge <span class="No-Break">data capture</span></li><li><span class="No-Break">Time travel</span></li></ul></li>
				<li>While Parquet also provides columnar storage and has efficient read/write operations, its lack of ACID transaction capabiliti<a id="_idTextAnchor070"/>es distinguishes <span class="No-Break">Delta Lake.</span></li>
			</ol>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor071"/>Further reading</h1>
			<p>In this chapter, we introduced vital technologies. Look at these resources to go deeper into the areas that interest <span class="No-Break">you most:</span></p>
			<ul>
				<li>YouTube video – <em class="italic">Introduction to Databricks Data Intelligence </em><span class="No-Break"><em class="italic">Platform</em></span><span class="No-Break">: </span><a href="https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO"><span class="No-Break">https://youtu.be/E885Ld3N2As?si=1NPg85phVH8RhayO</span></a></li>
				<li><em class="italic">Introduction to Data </em><span class="No-Break"><em class="italic">Lakes</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/discover/data-lakes"><span class="No-Break">https://www.databricks.com/discover/data-lakes</span></a></li>
				<li><em class="italic">5 Steps to a Successful Data Lakehouse</em> by Bill Inmon, father of the data <span class="No-Break">warehouse: </span><a href="https://www.databricks.com/resources/ebook/building-the-data-lakehouse"><span class="No-Break">https://www.databricks.com/resources/ebook/building-the-data-lakehouse</span></a></li>
				<li><em class="italic">Delta Lake: Up &amp; Running</em> by <span class="No-Break">O’Reilly: </span><a href="https://www.databricks.com/resources/ebook/delta-lake-running-oreilly"><span class="No-Break">https://www.databricks.com/resources/ebook/delta-lake-running-oreilly</span></a></li>
				<li><em class="italic">Delta Lake: The Definitive </em><span class="No-Break"><em class="italic">Guide</em></span><span class="No-Break">: </span><a href="https://www.oreilly.com/library/view/delta-lake-the/9781098151935/"><span class="No-Break">https://www.oreilly.com/library/view/delta-lake-the/9781098151935/</span></a></li>
				<li><em class="italic">Comparing Apache Spark and </em><span class="No-Break"><em class="italic">Databricks</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/spark/comparing-databricks-to-apache-spark"><span class="No-Break">https://www.databricks.com/spark/comparing-databricks-to-apache-spark</span></a></li>
				<li><em class="italic">Databricks </em><span class="No-Break"><em class="italic">MLflow</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/product/managed-mlflow"><span class="No-Break">https://www.databricks.com/product/managed-mlflow</span></a></li>
				<li><em class="italic">Databricks Community Edition </em><span class="No-Break"><em class="italic">FAQ</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis"><span class="No-Break">https://www.databricks.com/product/faq/community-edition#:~:text=What%20is%20the%20difference%20between,ODBC%20integrations%20for%20BI%20analysis</span></a></li>
				<li><em class="italic">Delta 2.0 - The Foundation of your Data Lakehouse is </em><span class="No-Break"><em class="italic">Open</em></span><span class="No-Break">: </span><a href="https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/"><span class="No-Break">https://delta.io/blog/2022-08-02-delta-2-0-the-foundation-of-your-data-lake-is-open/</span></a></li>
				<li><em class="italic">Delta Lake </em><span class="No-Break"><em class="italic">Integrations</em></span><span class="No-Break">: </span><a href="https://delta.io/integrations/"><span class="No-Break">https://delta.io/integrations/</span></a></li>
				<li><em class="italic">Delta vs </em><span class="No-Break"><em class="italic">Iceberg</em></span><span class="No-Break">: </span><a href="https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d"><span class="No-Break">https://databeans-blogs.medium.com/delta-vs-iceberg-performance-as-a-decisive-criteria-add7bcdde03d</span></a></li>
				<li><span class="No-Break"><em class="italic">UniForm</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability"><span class="No-Break">https://www.databricks.com/blog/delta-uniform-universal-format-lakehouse-interoperability</span></a></li>
				<li><em class="italic">Delta Kernel: Simplifying Building Connectors for </em><span class="No-Break"><em class="italic">Delta</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/"><span class="No-Break">https://www.databricks.com/dataaisummit/session/delta-kernel-simplifying-building-connectors-delta/</span></a></li>
				<li><em class="italic">Databricks Community </em><span class="No-Break"><em class="italic">Website</em></span><span class="No-Break">: </span><a href="https://community.cloud.databricks.com"><span class="No-Break">https://community.cloud.databricks.com</span></a></li>
				<li><em class="italic">Podcast: Delta Lake Discussions with Denny </em><span class="No-Break"><em class="italic">Lee</em></span><span class="No-Break">: </span><a href="https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4"><span class="No-Break">https://open.spotify.com/show/6YvPDkILtWfnJNTzJ9HsmW?si=214eb7d808d84aa4</span></a></li>
				<li><em class="italic">Databricks Sets Official Data Warehousing Performance </em><span class="No-Break"><em class="italic">Record</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html"><span class="No-Break">https://www.databricks.com/blog/2021/11/02/databricks-sets-official-data-warehousing-performance-record.html</span></a></li>
				<li><em class="italic">LightGBM</em>: <a href="https://github.com/microsoft/LightGBM"><span class="No-Break">https://github.com/microsoft/LightGBM</span></a><span class="No-Break"> </span><a href="https://lightgbm.readthedocs.io/en/latest/"><span class="No-Break">https://lightgbm.readthedocs.io/en/latest/</span></a></li>
				<li><em class="italic">Kaggle</em> | <em class="italic">Store </em><span class="No-Break"><em class="italic">Sales</em></span><span class="No-Break">: </span><a href="https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview"><span class="No-Break">https://www.kaggle.com/competitions/store-sales-time-series-forecasting/overview</span></a></li>
				<li><em class="italic">Kaggle</em> | <em class="italic">Multi Label Image </em><span class="No-Break"><em class="italic">Classification</em></span><span class="No-Break">: </span><a href="https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-image-classification-dataset"><span class="No-Break">https://www.kaggle.com/datasets/meherunnesashraboni/multi-label-<span id="_idTextAnchor072"/>image-classification-dataset</span></a></li>
				<li><em class="italic">Store Sales - Time Series </em><span class="No-Break"><em class="italic">Forecasting</em></span><span class="No-Break">: </span><a href="http://Kaggle.com/competitions/store-sales-time-series-forecasting/overview"><span class="No-Break">Kaggle.com/competitions/store-sales-time-series-forecasting/overview</span></a></li>
				<li><em class="italic">arXiv </em><span class="No-Break"><em class="italic">website</em></span><span class="No-Break">: </span><a href="https://arxiv.org"><span class="No-Break">https://arxiv.org</span></a></li>
			</ul>
		</div>
	</body></html>