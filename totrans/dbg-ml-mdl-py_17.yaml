- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Human-in-the-Loop Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类在环机器学习
- en: Machine learning modeling is more than just machine learning developers and
    engineers sitting behind their computers to build and revise components of a machine
    learning life cycle. Incorporating feedback from domain experts, or even the non-expert
    crowd, is key in bringing more reliable and application-oriented models to production.
    This concept, which is called human-in-the-loop machine learning, is about benefiting
    from human intelligence and expert knowledge in different stages of a life cycle
    to further improve the performance and reliability of our models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习建模不仅仅是机器学习开发者和工程师坐在电脑后面构建和修改机器学习生命周期的组件。融入领域专家或非专家群体的反馈对于将更可靠和应用导向的模型投入生产至关重要。这个被称为人类在环机器学习的概念，是关于在不同生命周期阶段利用人类智能和专业知识来进一步提高我们模型的性能和可靠性。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Humans in the machine learning life cycle
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习生命周期中的人类
- en: Human-in-the-loop modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类在环建模
- en: By the end of this chapter, you will know about the benefits and challenges
    of incorporating human intelligence in your machine learning modeling projects.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解在机器学习建模项目中融入人类智能的好处和挑战。
- en: Humans in the machine learning life cycle
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习生命周期中的人类
- en: 'Developing and improving different components of a machine learning life cycle
    to bring a reliable and high-performance model to production is a collaborative
    effort that can benefit from expert and non-expert human feedback (*Figure 17**.1*):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 开发和改进机器学习生命周期的不同组件，以将可靠且高性能的模型投入生产是一个需要专家和非专家人类反馈的协作努力（*图17.1*）：
- en: "![Figure 17.1 – Human\uFEFFs in the machine learning life cycle](img/B16369_17_01.jpg)"
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图17.1 – 机器学习生命周期中的人类](img/B16369_17_01.jpg)'
- en: Figure 17.1 – Humans in the machine learning life cycle
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1 – 机器学习生命周期中的人类
- en: For example, a radiologist can help in annotating radiological images while
    most people with good vision capabilities can easily label cat and dog images.
    But incorporating human feedback is not limited to data annotation at the beginning
    of a life cycle.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，放射科医生可以在标注放射学图像时提供帮助，而大多数具有良好视觉能力的人可以轻松地标注猫和狗的图像。但融入人类反馈并不限于生命周期开始时的数据标注。
- en: We can benefit from human intelligence and expertise to improve data preparation,
    feature engineering, and representation learning aspects of a life cycle, as well
    as model training and testing, and eventually model deployment and monitoring.
    In each of these stages, human feedback can be incorporated either passively or
    actively, which allows us to bring a better model into production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用人类智能和专业知识来改进生命周期中的数据准备、特征工程和表示学习等方面，以及模型训练和测试，最终实现模型的部署和监控。在这些每个阶段，人类反馈都可以被动或主动地融入，这使我们能够将更好的模型投入生产。
- en: Passive human-in-the-loop is about collecting feedback and information from
    experts and non-experts and benefitting from that the next time we revise components
    of the corresponding machine learning modeling system. In this process, the feedback
    and extra information help in identifying opportunities for improving the components
    of the life cycle and identifying data and concept drift to bring a better model
    into production. In active human-in-the-loop machine learning, the infrastructure
    and one or all of the life cycle components need to be designed in a way that
    the extra human-in-the-loop information and data can be actively and continuously
    incorporated to improve data analysis and modeling.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 被动的人类在环是关于收集专家和非专家的反馈和信息，并在下一次修改相应的机器学习建模系统的组件时从中受益。在这个过程中，反馈和额外信息有助于识别改进生命周期组件的机会，以及识别数据漂移和概念漂移，以将更好的模型投入生产。在主动的人类在环机器学习中，基础设施和生命周期的一个或所有组件需要以某种方式设计，以便额外的人类在环信息和数据可以主动和持续地融入，以改进数据分析建模。
- en: First, we will review expert feedback collection and how to effectively benefit
    from it in improving our models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将回顾专家反馈收集以及如何有效地从中受益来改进我们的模型。
- en: Expert feedback collection
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家反馈收集
- en: The ultimate goal of building a piece of technology on top of one or multiple
    machine learning modes is to provide a tool for users, experts, or non-experts
    for a specific objective, such as healthcare image classification, stock price
    prediction, credit risk estimation, and product recommendation in platforms such
    as Amazon. For example, we can collect feedback for data annotation or later in
    the production stage for drift detection. We can then use this feedback to improve
    our models. However, this feedback could extend beyond the purposes of just data
    annotation or identifying data and concept drift.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个或多个机器学习模型之上构建一项技术的最终目标是提供一个工具，供用户、专家或非专家用于特定目标，例如医疗图像分类、股价预测、信用风险评估以及亚马逊等平台上的产品推荐。例如，我们可以收集数据标注或生产阶段后期用于漂移检测的反馈。然后我们可以利用这些反馈来改进我们的模型。然而，这种反馈可能超出了仅仅数据标注或识别数据和概念漂移的目的。
- en: 'We can incorporate expert feedback for four major purposes: data generation
    and annotation, data filtering, model selection, and model monitoring. Expert
    feedback collection for annotation and monitoring is generally similar to non-expert
    data collection except for the fact that in some applications, expertise is of
    necessity, such as in classifying radiological images.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将专家反馈纳入四个主要目的：数据生成和标注、数据过滤、模型选择和模型监控。专家反馈收集对于标注和监控通常与非专家数据收集相似，除了在某些应用中，专业知识是必需的，例如在分类放射学图像时。
- en: For model selection, we can use expert feedback to not need to rely exclusively
    on the performance measures we use for model performance assessment and, consequently,
    select the best model, but to detect red flags according to wrong predictions
    or rely on explainability information for our models, such as if features that
    contribute the most in terms of predictions are of lowest relevance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型选择，我们可以利用专家反馈，不仅依赖于我们用于模型性能评估的性能指标，而且根据错误预测检测红旗，或者根据模型的解释性信息进行选择，例如，如果对预测贡献最大的特征具有最低的相关性。
- en: We can also benefit from experts’ feedback in monitoring our models. Drift detection,
    as discussed in [*Chapter 11*](B16369_11.xhtml#_idTextAnchor300), *Avoiding and
    Detecting Data and Concept Drifts*, is crucial to ensure the reliability of our
    models in production. In many applications, users of our models could be experts
    in specific domains, such as healthcare and drug discovery. In such cases, we
    need to make sure we continuously collect their feedback and use this to detect
    and eliminate drifts in our models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从专家的反馈中受益，以监控我们的模型。正如在[*第11章*](B16369_11.xhtml#_idTextAnchor300)“避免和检测数据及概念漂移”中讨论的那样，漂移检测对于确保我们的模型在生产中的可靠性至关重要。在许多应用中，我们模型的使用者可能是特定领域的专家，例如医疗保健和药物发现。在这种情况下，我们需要确保我们持续收集他们的反馈，并利用这些反馈来检测和消除模型中的漂移。
- en: 'Collecting feedback from experts as users of our machine learning models should
    not be limited to getting their binary response of “good” versus “bad.” We need
    to provide enough information about our models and their predictions and ask experts
    to provide their feedback, as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们机器学习模型的使用者那里收集专家反馈不应仅限于获取他们“好”与“坏”的二进制响应。我们需要提供关于我们的模型及其预测的足够信息，并要求专家提供以下反馈：
- en: '**Provide sufficient information**: When asking for feedback from expert users
    of our models, we need to provide sufficient information to get better and more
    relevant feedback. For example, in addition to the performance of our model in
    testing and production, or wrong and correct predictions for a specific set of
    data points, we can also provide explainability information on how the model came
    up with its decision for those data points. This type of information could help
    the users provide better feedback that will help us in improving our models.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供足够的信息**：当我们向我们的模型专家用户请求反馈时，我们需要提供足够的信息以获得更好和更相关的反馈。例如，除了我们的模型在测试和生产中的性能，或针对一组特定数据点的错误和正确预测之外，我们还可以提供关于模型如何针对这些数据点做出决策的解释性信息。这类信息可以帮助用户提供更好的反馈，从而帮助我们改进模型。'
- en: '**Don’t ask for translations**: Many of the users of our models might have
    limited statistical and machine learning modeling knowledge. So, asking them to
    convert their opinions and ideas into technical terms would limit efficient feedback
    collection. You need to provide sufficient information and ask for their feedback
    and have a back-and-forth conversation to convert their insights into actionable
    items for model improvement.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不要要求翻译**：我们模型的许多用户可能对统计和机器学习建模知识有限。因此，要求他们将自己的意见和想法转换为技术术语将限制有效的反馈收集。你需要提供足够的信息，并要求他们的反馈，进行双向对话，将他们的见解转化为改进模型的行动项。'
- en: '**Design for automated feedback collection**: Although it is better to not
    ask for translations, as pointed out earlier, you can move toward more automated
    feedback collection using clear and detailed questions and proper infrastructure
    design to collect the feedback and incorporate it into your models. For example,
    you can use machine learning explainability and ask whether the most informative
    features used by the model for predicting the output of a specific set of data
    points are relevant to the task or not.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设计自动化反馈收集**：虽然最好不要求翻译，如前所述，你可以通过使用清晰详细的问题和适当的基础设施设计来收集反馈并将其纳入模型，从而朝着更自动化的反馈收集迈进。例如，你可以使用机器学习可解释性，并询问模型用于预测特定数据点集输出的最有信息量的特征是否与任务相关。'
- en: Human-in-the-loop has its own challenges, such as in preserving privacy when
    third-party companies are needed to monitor models and pipelines, or when there
    would be specific legal barriers in sharing data coming from collaborators and
    business partners with others in our teams and organizations. We need to keep
    these challenges in mind when we’re designing so that we can benefit from human
    feedback in our machine learning life cycles.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中的人类建模有其自身的挑战，例如，当需要第三方公司监控模型和管道时，或者在分享来自合作者和商业伙伴的数据时存在特定的法律障碍，我们团队和组织中的其他人。在设计时，我们需要牢记这些挑战，以便我们可以在机器学习生命周期中从人类反馈中受益。
- en: Although we can collect feedback in different stages of the machine learning
    life cycle to improve our models, there are techniques such as active learning
    (which we will cover next) that can help us bring a better model with lower cost
    into production.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以在机器学习生命周期的不同阶段收集反馈来改进我们的模型，但还有一些技术，如主动学习（我们将在下一节中介绍），可以帮助我们以更低的成本将更好的模型投入生产。
- en: Human-in-the-loop modeling
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境中的人类建模
- en: 'Despite more high-quality annotated data points being more valuable, the cost
    of annotating data, specifically when domain expertise is of necessity, could
    be very high. Active learning is a strategy that helps us in generating and labeling
    data to improve the performance of our models at a lower cost. In an active learning
    setting, we aim to benefit from a model with a limited amount of data and iteratively
    select new data points to be labeled, or their continuous value identified, with
    the aim of achieving higher performance (Wu et al., 2022; Ren et al., 2021; Burbidge
    et al., 2007). The model queries new instances to be annotated by experts or non-experts,
    or their labels or continuous values are identified via any computational or experimental
    technique. However, instead of the instances being selected randomly, there are
    techniques for new instance selection to help us in achieving better models with
    a lower number of instances and iterations (*Table 17.1*). Each of these techniques
    has its advantages and disadvantages. For example, *uncertainty sampling* is simple
    but its effect on performance might be limited if uncertainty in the predicted
    output of instances is not highly correlated with model error:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管更多高质量的标注数据点更有价值，但标注数据的成本可能非常高，尤其是在需要领域专业知识的情况下。主动学习是一种帮助我们以较低成本生成和标注数据以改进模型性能的策略。在主动学习环境中，我们旨在利用有限的数据量，迭代选择新的数据点进行标注，或识别其连续值，以达到更高的性能（Wu
    et al., 2022; Ren et al., 2021; Burbidge et al., 2007）。模型会查询需要由专家或非专家标注的新实例，或者通过任何计算或实验技术识别其标签或连续值。然而，与随机选择实例不同，有新技术用于选择新实例，以帮助我们以更少的实例和迭代次数实现更好的模型（*表17.1*）。每种技术都有其优缺点。例如，*不确定性采样*简单，但如果实例预测输出的不确定性没有高度相关于模型错误，其对性能的影响可能有限：
- en: '| **Data-Centric** | **Model-Centric** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **数据中心** | **模型中心** |'
- en: '| **Uncertainty sampling**Selecting instances with the most uncertainty (in
    inference), which could be instances closest to the decision boundary in classification
    problems | **Expected** **model change**Selecting instances that know their labels
    results in the biggest impact on the current model |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **不确定性采样**选择具有最大不确定性的实例（在推理中），这些实例可能是分类问题中离决策边界最近的实例 | **预期模型变化**选择已知标签的实例，对当前模型的影响最大
    |'
- en: '| **Density-weighted** **uncertainty sampling**Selecting instances that not
    only have the highest uncertainty but also are representative of many other data
    points that rely on the density of data in feature space | **Estimation of** **error
    reduction**Selecting instances that know their labels would result in the biggest
    future error reduction |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **密度加权不确定性采样**选择不仅具有最高不确定性，而且代表了许多其他依赖特征空间数据密度的数据点的实例 | **误差减少估计**选择已知标签的实例，将导致最大的未来误差减少
    |'
- en: '| **Query-by-committee**Multiple models (the committee) get trained and instances
    with the highest disagreement in their prediction get selected | **Variance reduction**Selecting
    instances that know their labels would result in the most reduction in the model’s
    uncertainty about its parameters |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **委员会查询**多个模型（委员会）被训练，并选择预测中分歧最大的实例 | **方差减少**选择已知标签的实例，将导致模型对其参数的不确定性减少最多
    |'
- en: Table 17.1 – Active learning techniques for instance election to be annotated
    in each step
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表17.1 – 每个步骤中用于实例选择的主动学习技术
- en: In this chapter, we focused on introducing concepts and techniques behind human-in-the-loop.
    However, there are Python libraries such as `modAL` ([https://modal-python.readthedocs.io/en/latest/](https://modal-python.readthedocs.io/en/latest/))
    that can help you in implementing some of these techniques in your projects to
    bring human feedback into your machine learning life cycle.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于介绍人类在循环背后的概念和技术。然而，有一些Python库，如`modAL` ([https://modal-python.readthedocs.io/en/latest/](https://modal-python.readthedocs.io/en/latest/))，可以帮助你在项目中实现一些这些技术，将人类反馈引入你的机器学习生命周期。
- en: Summary
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about some of the important concepts in human-in-the-loop
    machine learning, which can help you in better establishing collaboration between
    you and your team with experts or non-experts so that you can incorporate their
    feedback into your machine learning modeling projects.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了一些人类在循环机器学习中的重要概念，这些概念可以帮助你在与专家或非专家的团队合作中更好地建立协作，以便你可以将他们的反馈纳入你的机器学习建模项目中。
- en: This was the last chapter of this book. I hope you learned enough about different
    approaches to improve your machine learning models and build better ones so that
    you can start your journey toward becoming an expert in this domain.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章。我希望你学到了足够多的关于提高机器学习模型和构建更好模型的不同方法，以便你可以开始你的旅程，成为这个领域的专家。
- en: Questions
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Is human-in-the-loop machine learning limited to data annotation and labeling?
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类在循环机器学习是否仅限于数据标注和标签化？
- en: What is the difference between uncertainty sampling and density-weighted uncertainty
    sampling in selecting instances in each step of an active learning process?
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主动学习过程的每个步骤中，不确定性采样和密度加权不确定性采样在实例选择上的区别是什么？
- en: References
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amershi, Saleema, et al. *Power to the people: The role of humans in interactive
    machine learning*. Ai Magazine 35.4 (2014): 105-120.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amershi, Saleema, 等人。*赋予人民力量：人类在交互式机器学习中的作用*. 人工智能杂志35.4（2014）：105-120。
- en: 'Wu, Xingjiao, et al. *A survey of human-in-the-loop for machine learning*.
    Future Generation Computer Systems 135 (2022): 364-381.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu, Xingjiao, 等人。*人机交互机器学习综述*. 未来一代计算机系统135（2022）：364-381。
- en: 'Ren, Pengzhen, et al. *A survey of deep active learning*. ACM computing surveys
    (CSUR) 54.9 (2021): 1-40.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren, Pengzhen, 等人。*深度主动学习综述*. ACM 计算调查（CSUR）54.9（2021）：1-40。
- en: 'Burbidge, Robert, Jem J. Rowland, and Ross D. King. *Active learning for regression
    based on query by committee*. Intelligent Data Engineering and Automated Learning-IDEAL
    2007: 8th International Conference, Birmingham, UK, December 16-19, 2007\. Proceedings
    8\. Springer Berlin Heidelberg, 2007.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burbidge, Robert, Jem J. Rowland, 和 Ross D. King. *基于委员会查询的回归主动学习*. 智能数据工程与自动学习-IDEAL
    2007：第8届国际会议，英国伯明翰，2007年12月16-19日。第8卷。Springer Berlin Heidelberg，2007。
- en: Cai, Wenbin, Ya Zhang, and Jun Zhou. *Maximizing expected model change for active
    learning in regression*. 2013 IEEE 13th international conference on data mining.
    IEEE, 2013.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai, Wenbin, Ya Zhang, 和 Jun Zhou. *最大化回归中主动学习的预期模型变化*. 2013年第13届国际数据挖掘会议。IEEE，2013。
- en: 'Roy, Nicholas, and Andrew McCallum. *Toward optimal active learning through
    monte carlo estimation of error reduction*. ICML, Williamstown 2 (2001): 441-448.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy, Nicholas, 和 Andrew McCallum. *通过蒙特卡洛估计误差减少实现最优主动学习*. ICML，威廉斯塔特2（2001）：441-448。
- en: 'Donmez, Pinar, Jaime G. Carbonell, and Paul N. Bennett. *Dual strategy active
    learning*. Machine Learning: ECML 2007: 18th European Conference on Machine Learning,
    Warsaw, Poland, September 17-21, 2007\. Proceedings 18\. Springer Berlin Heidelberg,
    2007.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donmez, Pinar, Jaime G. Carbonell, 和 Paul N. Bennett. *双重策略主动学习*. 机器学习：ECML
    2007：第18届欧洲机器学习会议，波兰华沙，2007年9月17-21日。第18卷。Springer Berlin Heidelberg，2007。
- en: Assessments
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: '[*Chapter 1*](B16369_01.xhtml#_idTextAnchor015) – Beyond Code Debugging'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第一章*](B16369_01.xhtml#_idTextAnchor015) – 超越代码调试'
- en: 'Yes – here is an example that was provided in this chapter:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的——这里有一个在本章中提供的例子：
- en: '[PRE0]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here are their definitions:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是它们的定义：
- en: '`AttributeError`: This type of error is raised when an attribute is used for
    an object that it is not defined for. For example, `isnull` is not defined for
    a list. So, `my_list. isnull()` results in `AttributeError`.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AttributeError`：当对一个未为其定义属性的对象使用属性时，会引发此类错误。例如，`isnull`未在列表中定义。因此，`my_list.
    isnull()`会导致`AttributeError`。'
- en: '`NameError`: This error is raised when you try to call a function, class, or
    other names and modules that are not defined in your code. For example, if you
    haven’t defined a `neural_ network` class in your code but call it in your code
    as `neural_network()`, you will get a `NameError` message.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NameError`：当你尝试调用未在代码中定义的函数、类或其他名称和模块时，会引发此错误。例如，如果你没有在代码中定义`neural_network`类，但在代码中调用它为`neural_network()`，你将得到`NameError`消息。'
- en: Higher dimensionality makes a sparser feature space and could reduce the confidence
    of the model in identifying generalizable decision boundaries in a classification
    setting.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高维性使得特征空间更稀疏，可能会降低模型在分类设置中识别可推广决策边界的信心。
- en: When you get an error message in Python, it usually provides you with the necessary
    information to find the issue. This information creates a report-like message
    about the lines of your code that the error occurred in, the types of errors,
    and the function or class calls that resulted in such errors. This report-like
    message is called a **traceback** in Python.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你在Python中遇到错误信息时，它通常会提供必要的信息来帮助你找到问题。这些信息创建了一个类似报告的消息，关于代码中发生错误的行，错误类型，以及导致这些错误的功能或类调用。这种类似报告的消息在Python中称为**回溯**。
- en: '**Incremental programming**: Writing code for every small component, then testing
    it and writing test codes using PyTest, for example, could help you avoid issues
    with each function or class you wrote. It also helps you ensure the outputs of
    one module that feed another module as its input are compatible.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增量编程**：为每个小组件编写代码，然后测试它，例如使用PyTest编写测试代码，这可以帮助你避免每个编写的功能或类的问题。它还帮助你确保作为另一个模块输入的模块的输出是兼容的。'
- en: '**Logging**: When you develop functions and classes in Python, you can benefit
    from logging to log information, errors, and other kinds of messages to help you
    in identifying potential sources of issues when you get an error message.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**日志记录**：当你用Python开发函数和类时，你可以从日志记录中受益，将信息、错误和其他类型的消息记录下来，以帮助你识别在收到错误信息时的潜在问题来源。'
- en: For example, if you use experts, such as radiologists, to annotate medical images
    for a cancer diagnosis, then the confidence on the label of images could be different.
    And these confidences could be considered in the modeling phase either in the
    data collection process, such as by asking more experts to annotate the same images,
    or in the modeling process, such as by assigning a weight to each image based
    on the confidence in labeling. The features of your data could also have different
    qualities. For example, you might have highly sparse features that have mostly
    zero values across the data points or features that might have different levels
    of confidence. For example, a measurement feature will have lower confidence if
    you use a measurement tape to capture millimeter differences between the sizes
    of objects, such as dice, compared to using the same tape to capture differences
    between bigger objects, such as furniture.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，如果你使用专家，如放射科医生，为癌症诊断标注医学图像，那么对图像标签的信心可能不同。这些信心可以在建模阶段考虑，无论是在数据收集过程中，例如通过要求更多专家标注相同的图像，还是在建模过程中，例如通过根据标签的信心为每个图像分配权重。你的数据特征也可能具有不同的质量。例如，你可能具有高度稀疏的特征，这些特征在数据点中大部分为零值，或者具有不同置信水平的特征。例如，如果你使用卷尺来捕捉物体尺寸（如骰子）之间的毫米差异，而不是使用相同的卷尺来捕捉更大物体（如家具）之间的差异，那么测量特征将具有较低的置信度。
- en: You can control underfitting and overfitting by controlling model complexity.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过控制模型复杂度来控制欠拟合和过拟合。
- en: Yes, it is possible. The data that’s used for training and testing machine learning
    models could become out of date. For example, the changes in the trends of the
    clothing market could make predictions of a model for clothing recommendation
    unreliable.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，这是可能的。用于训练和测试机器学习模型的数据可能会过时。例如，服装市场趋势的变化可能会使服装推荐模型的预测变得不可靠。
- en: By playing with model hyperparameters alone, you can’t develop the best possible
    model. In the same way, by increasing the quality and quantity of your data and
    keeping your model hyperparameters the same, you also can’t achieve the best performance
    possible. So, data and hyperparameters work hand in hand.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅通过调整模型超参数，你无法开发出最佳可能的模型。同样，通过增加数据和超参数的质量和数量，同时保持模型超参数不变，你也不能达到最佳性能。因此，数据和超参数是相辅相成的。
- en: '[*Chapter 2*](B16369_02.xhtml#_idTextAnchor076) – Machine Learning Life Cycle'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第2章*](B16369_02.xhtml#_idTextAnchor076) – 机器学习生命周期'
- en: Examples of cleaning processes are filling in missing values in your data and
    removing outliers.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清洗过程的例子包括在数据中填充缺失值和移除异常值。
- en: One-hot encoding generates a new feature for each category of categorical features.
    Label encoding keeps the same features and just replaces each category with a
    number assigned to that category.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: One-hot编码为每个分类特征类别生成一个新特征。标签编码保持相同的特征，只是将每个类别替换为分配给该类别的数字。
- en: The simplest way of detecting outliers is by using quantiles of the distribution
    of variable values. Data points that are beyond the upper and lower bounds are
    considered outliers. The lower and upper bounds can be calculated as *Q1-a.IQR*
    and *Q3+a.IQR*, where *a* can be a real value between 1.5 and 3\. The common value
    of *a* that is also used by default in drawing a boxplot is 1.5, but having higher
    values makes the process of outlier identification less stringent and lets fewer
    data points be detected as outliers.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测异常值的最简单方法是通过使用变量值分布的分位数。超出上下界限的数据点被认为是异常值。下限和上限可以计算为*Q1-a.IQR*和*Q3+a.IQR*，其中*a*可以是一个介于1.5和3之间的实数值。*a*的常用值也是默认用于绘制箱线图的1.5，但使用更高的值会使异常值识别过程不那么严格，并允许检测到更少的数据点作为异常值。
- en: If you want to deploy a model in doctors’ personal computers in hospitals to
    be used directly by clinicians, you need to consider all difficulties and planning
    needed to set up the proper production environment and all the software dependencies.
    You also need to make sure their local system has the necessary hardware requirements.
    These are not among the considerations if you want to deploy a model behind chatbots
    in a banking system.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想在医院医生的电脑上部署一个模型，以便临床医生直接使用，你需要考虑所有必要的困难和规划，以设置适当的生产环境以及所有软件依赖项。你还需要确保他们的本地系统满足必要的硬件要求。如果你想在银行系统中部署在聊天机器人背后的模型，这些就不是需要考虑的因素。
- en: '[*Chapter 3*](B16369_03.xhtml#_idTextAnchor119) – Debugging toward Responsible
    AI'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第3章*](B16369_03.xhtml#_idTextAnchor119) – 负责任的人工智能调试'
- en: '**Data collection bias**: Data that is collected could have biases such as
    gender bias, as in Amazon’s applicant sorting examples, race bias, as in COMPAS,
    socioeconomic biases, as in hospitalization examples, or other kinds of biases.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集偏差**：收集的数据可能存在偏差，例如性别偏差，如在亚马逊的应聘者排序示例中，种族偏差，如在COMPAS中，社会经济偏差，如在住院示例中，或其他类型的偏差。'
- en: '**Sampling bias**: Another source of data bias could be in the process of sampling
    data points or sampling the population in the data collection stage of the life
    cycle. For example, in sampling students to fill in a survey, our sampling process
    could be biased toward girls or boys, rich or poor student families, or high-
    versus low-grade students.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**抽样偏差**：数据偏差的另一个来源可能是数据收集阶段的生命周期中数据点的抽样或人群的抽样过程。例如，在抽样学生填写调查时，我们的抽样过程可能偏向于女孩或男孩，富裕或贫穷的学生家庭，或高年级与低年级学生。'
- en: '**Perfect-knowledge white-box attacks**: The attacker knows everything about
    the system.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完美知识白盒攻击**：攻击者了解系统的所有信息。'
- en: '**Zero-knowledge black-box attacks**: The attacker doesn’t have any knowledge
    of the system itself but collects information through predictions of the model
    in production.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**零知识黑盒攻击**：攻击者对系统本身没有任何了解，但通过在生产中对模型进行预测来收集信息。'
- en: The encryption process transforms the information, data, or algorithm into a
    new (that is, encrypted) form. The encrypted data can be decrypted (that is, become
    human-readable or machine understandable) if the individual has access to the
    encryption key (that is, a password-style key necessary for the decryption process).
    In this way, getting access to the data and algorithm without the encryption key
    will be almost impossible or very difficult.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加密过程将信息、数据或算法转换成新的（即，加密）形式。如果个人有权访问加密密钥（即，解密过程中必要的密码式密钥），则加密数据可以被解密（即，成为可读或机器可理解的）。这样，在没有加密密钥的情况下获取数据和算法将几乎不可能或非常困难。
- en: '**Differential privacy** tries to ensure that the removal or addition of individual
    data points does not affect the outcome of modeling. It attempts to learn from
    patterns within groups of data points. For example, by adding random noise from
    a normal distribution, it tries to make the features of individual data points
    obscure. The effect of noise in learning could be eliminated based on the law
    of large numbers if a large number of data points will be accessible.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**差分隐私**试图确保删除或添加单个数据点不会影响建模的结果。它试图从数据点的组中学习模式。例如，通过添加来自正态分布的随机噪声，它试图使单个数据点的特征变得模糊。如果可以访问大量数据点，则可以根据大数定律消除学习中的噪声效应。'
- en: '**Federated learning** relies on the idea of decentralizing learning, data
    analysis, and inference, thus allowing the user’s data to be kept within individual
    devices or local databases.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**联邦学习**依赖于分散学习、数据分析和推理的想法，从而允许用户的数据保留在单个设备或本地数据库中。'
- en: Transparency helps in building trust in users and could potentially increase
    the number of users that trust and use your models.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 透明度有助于在用户中建立信任，并可能增加信任并使用您模型的用户数量。
- en: '[*Chapter 4*](B16369_04.xhtml#_idTextAnchor159) – Detecting Performance and
    Efficiency Issues in Machine Learning Models'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第4章*](B16369_04.xhtml#_idTextAnchor159) – 在机器学习模型中检测性能和效率问题'
- en: In a primary diagnostic test, with more accurate follow-up tests, we want to
    make sure we do not lose any patients with the disease we are testing for. So,
    we need to aim to decrease false negatives, while trying to decrease false positives
    at the same time. So, we can aim to maximize **recall** while controlling for
    **precision** and **specificity**.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初步诊断测试中，随着更精确的后续测试，我们希望确保我们不会失去任何我们正在测试的疾病患者。因此，我们需要旨在减少假阴性，同时尝试减少假阳性。因此，我们可以旨在最大化**召回率**，同时控制**精确度**和**特异性**。
- en: In such cases, you want to make sure you have the **precision** to control risks
    and suggest good investment opportunities. This might result in a lower **recall**,
    which is okay as a bad investment could result in a significant loss of capital
    for individual investors. Here, we don’t want to consider the details of investment
    risk management and want to have a high-level understanding of how to select a
    good performance measure. If you are an expert in this field, consider your knowledge
    and select a good performance measure that satisfies the requirements you are
    aware of.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，您需要确保您有 **精确度** 来控制风险并建议良好的投资机会。这可能会导致较低的 **召回率**，这是可以接受的，因为不良投资可能导致个人投资者资本的重大损失。在这里，我们不考虑投资风险管理的细节，而希望对如何选择良好的性能指标有一个高层次的理解。如果您是这个领域的专家，考虑您的知识，并选择一个满足您已知要求的良好性能指标。
- en: ROC-AUC is a summary measure. Two models with the same ROC-AUCs could have different
    predictions for individual data points.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ROC-AUC 是一个汇总指标。具有相同 ROC-AUC 的两个模型可能对个别数据点的预测不同。
- en: '**MCC** focuses on predicted labels, while **log-loss** cares about predicted
    probabilities for the tested data points. So, a lower **log-loss** does not necessarily
    result in a lower **MCC**.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MCC** 关注预测标签，而 **log-loss** 关注测试数据点的预测概率。因此，较低的 **log-loss** 并不一定导致较低的 **MCC**。'
- en: Not necessarily. *R*2 doesn’t take into account data dimensionality (that is,
    the number of features, inputs, or independent variables). A model with more features
    could result in a higher *R*2, while it might not necessarily be a better model.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不一定。*R*2 并不考虑数据维度（即特征、输入或独立变量的数量）。具有更多特征的模型可能导致更高的 *R*2，但这不一定是一个更好的模型。
- en: It depends on the performance measure and test data used for assessing the generalizability
    of the model. We need to use the right performance measure for our objective in
    production, and use a set of data points for model testing that will be more reflective
    of unseen data in production.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这取决于用于评估模型泛化能力的性能指标和测试数据。我们需要为生产中的目标使用正确的性能指标，并使用一组数据点进行模型测试，这组数据点将更能反映生产中未见数据。
- en: '[*Chapter 5*](B16369_05.xhtml#_idTextAnchor183) – Improving the Performance
    of Machine Learning Models'
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第五章*](B16369_05.xhtml#_idTextAnchor183) – 提高机器学习模型的性能'
- en: Adding more training data points could help to reduce variance while adding
    more features could help to reduce bias. However, there is no guarantee of a reduction
    of variance through the addition of new data points or whether new features will
    be helpful in reducing variance.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加更多的训练数据点可以帮助减少方差，而增加更多的特征可以帮助减少偏差。然而，通过添加新的数据点来减少方差，或者新特征是否有助于减少方差，都没有保证。
- en: '**Assigning weights during optimization**: You can assign a weight to each
    data point, according to the confidence of class labels, when training machine
    learning models.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化过程中的权重分配**：在训练机器学习模型时，您可以根据类别标签的置信度，为每个数据点分配一个权重。'
- en: '**Ensemble learning**: If you consider a distribution of the quality or confidence
    score of each data point, then you can build different models using data points
    from each part of this distribution and then combine the predictions of the models
    for example using their weighted average.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成学习**：如果您考虑每个数据点的质量或置信度分数的分布，那么您可以使用分布的每个部分的数据点构建不同的模型，然后结合这些模型的预测，例如使用它们的加权平均。'
- en: '**Transfer learning**: You can train a model on a large dataset with different
    levels of label confidence (see *Figure 5**.3*), excluding very low-confidence
    data and then fine-tune it on the very high-confidence part of your dataset.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**：您可以在具有不同标签置信度级别的大数据集上训练一个模型（参见 *图5**.3*），排除非常低置信度的数据，然后在数据集非常高置信度的部分进行微调。'
- en: By increasing confidence in identifying the decision boundary, in a classification
    setting, where the minority class is sparse.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提高识别决策边界的置信度，在分类设置中，少数类是稀疏的。
- en: If we use Borderline-SMOTE, the new synthetically generated data points would
    be close to the majority-class data points, which helps in identifying a generalizable
    decision boundary.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们使用 Borderline-SMOTE，新合成的数据点将接近多数类数据点，这有助于识别一个可推广的决策边界。
- en: In DSMOTE, **DBSCAN** is used to divide data points of the minority class into
    three groups of core samples, borderline samples, and noise (that is, outlying)
    samples, and then the core and borderline samples only get used for oversampling.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DSMOTE 中，**DBSCAN** 用于将少数类的数据点划分为三个组：核心样本、边界样本和噪声（即异常）样本，然后仅使用核心和边界样本进行过采样。
- en: Searching over the whole possible combinations of hyperparameters is not necessary,
    as explained in this chapter.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章所述，搜索所有可能的超参数组合并不是必要的。
- en: Yes, L1 regularization can eliminate the contribution of features to the regularization
    process.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，L1 正则化可以消除特征对正则化过程的贡献。
- en: Yes, it is possible.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，这是可能的。
- en: '[*Chapter 6*](B16369_06.xhtml#_idTextAnchor201) – Interpretability and Explainability
    in Machine Learning Modeling'
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第6章*](B16369_06.xhtml#_idTextAnchor201) – 机器学习建模中的可解释性和可解释性'
- en: Explainability can help improve performance, such as by reducing the sensitivity
    of models to small feature value changes, increasing data efficiency in model
    training, trying to help in proper reasoning in models, and avoiding spurious
    correlations.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可解释性可以帮助提高性能，例如通过减少模型对特征值小变化的敏感性，提高模型训练中的数据效率，试图帮助模型进行适当的推理，以及避免虚假相关性。
- en: '**Local explainability** helps us understand the behavior of a model close
    to a data point in feature space. Although these models meet local fidelity criteria,
    features that have been identified to be locally important might not be globally
    important, and vice versa.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**局部可解释性**帮助我们理解模型在特征空间中接近数据点的行为。尽管这些模型满足局部保真度标准，但被识别为局部重要的特征可能不是全局重要的，反之亦然。'
- en: '**Global explainability** techniques try to go beyond local explainability
    and provide global explanations to the models.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局可解释性**技术试图超越局部可解释性，并为模型提供全局解释。'
- en: Linear models, although interpretable, usually have low performance. Instead,
    we could benefit from more complex models, with higher performance, and use explainability
    techniques to understand how the model comes up with its predictions.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性模型虽然可解释，但通常性能较低。相反，我们可以从更复杂、性能更高的模型中受益，并使用可解释性技术来理解模型是如何得出其预测的。
- en: Yes, it does. Explainability techniques could help us understand what models
    are major contributors to predictions for one set of data points.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，确实如此。可解释性技术可以帮助我们了解哪些模型是预测一组数据点的主要贡献者。
- en: SHAP can determine how each feature contributes to a model’s prediction. As
    features work cooperatively in determining the decision boundaries of classification
    models and eventually affecting model predictions, SHAP tries to first identify
    the marginal contribution of each feature and then provide Shapely values as an
    estimate of each feature in cooperation with the whole feature set to predict
    a model.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SHAP 可以确定每个特征对模型预测的贡献。由于特征在确定分类模型的决策边界并最终影响模型预测方面是协同工作的，SHAP 尝试首先识别每个特征的边际贡献，然后提供
    Shapely 值作为每个特征与整个特征集合作预测模型的估计。
- en: LIME is an alternative to SHAP for **local explainability** that explains the
    predictions of any classifier or regressor, in a model-agnostic way, by approximating
    a model locally with an interpretable model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 是 SHAP 的替代品，用于 **局部可解释性**，它以模型无关的方式解释任何分类器或回归器的预测，通过在局部近似一个可解释的模型。
- en: Counterfactual examples, or explanations, help us identify what needs to be
    changed in an instance to change the outcome of a classification model. These
    counterfactuals could help in identifying actionable paths in many applications,
    such as finance, retail, marketing, recruiting, and healthcare. For example, we
    can use them to suggest to a bank customer how they can change the rejection to
    their loan application.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反事实示例或解释有助于我们确定在实例中需要改变什么才能改变分类模型的输出。这些反事实可以帮助在许多应用中识别可操作的路径，例如金融、零售、营销、招聘和医疗保健。例如，我们可以用它们来建议银行客户如何改变其贷款申请被拒绝的情况。
- en: As presented in the *Counterfactual generation using Diverse Counterfactual
    Explanations (DiCE)* section, not all counterfactuals are feasible according to
    the definition and meaning of each feature. For example, if we want to suggest
    to a 30-year-old individual to change their outcome, suggesting that they need
    to wait until they get to their 50s is not an effective and actionable suggestion.
    Also, suggesting a change of `hours_per_week` of work from 38 to >80 is not feasible.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如在*使用多样化的反事实解释（DiCE）生成反事实*部分所述，并非所有反事实都符合每个特征的定义和意义。例如，如果我们想建议一个30岁的人改变他们的结果，建议他们等到50多岁才这样做并不是一个有效且可行的建议。同样，建议将`hours_per_week`的工作时间从38小时增加到>80小时也是不可行的。
- en: '[*Chapter 7*](B16369_07.xhtml#_idTextAnchor218) – Decreasing Bias and Achieving
    Fairness'
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第7章*](B16369_07.xhtml#_idTextAnchor218) – 减少偏差和实现公平性'
- en: No. There might be proxies in our models for sensitive attributes, but not in
    our models.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不一定。我们的模型中可能有敏感属性的代理，但不是我们的模型中。
- en: Salary and income (in some countries), occupation, a history of a felony charge.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 薪酬和收入（在某些国家），职业，犯罪指控的历史。
- en: Not necessarily. Satisfying fairness according to **demographic parity** wouldn’t
    necessarily result in a model being fair according to **equalized odds**.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不一定。根据**人口比例**来满足公平性并不一定会导致模型根据**均衡机会**来表现公平。
- en: '**Demographic parity** is a group fairness definition to ensure that a model’s
    predictions are not dependent on a given sensitive attribute, such as ethnicity
    or sex.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人口比例**是一个群体公平性定义，旨在确保模型的预测不依赖于给定的敏感属性，如种族或性别。'
- en: '**Equalized odds** is satisfied when a given prediction is independent of the
    group of a given sensitive attribute and the real output.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**均衡机会**在给定预测与给定敏感属性组独立且与真实输出无关时得到满足。'
- en: Not necessarily. For example, there could be feature proxies for `'sex'` among
    top contributors to model predictions.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不一定。例如，模型预测的主要贡献者中可能有 `'sex'` 的特征代理。
- en: We can use explainability techniques to identify potential biases in our models
    and then plan to improve them toward fairness. For example, we can identify fairness
    issues between male and female groups.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用可解释性技术来识别模型中的潜在偏差，然后计划改进它们以实现公平性。例如，我们可以识别男性和女性群体之间的公平性问题。
- en: '[*Chapter 8*](B16369_08.xhtml#_idTextAnchor243) – Controlling Risks Using Test-Driven
    Development'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第8章*](B16369_08.xhtml#_idTextAnchor243) – 使用测试驱动开发控制风险'
- en: '`pytest` is a simple-to-use Python library you can use to design unit tests.
    The designed tests can then be simply used to test changes in your code and control
    risks of potential mistakes throughout the development process and future changes
    in your code.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pytest`是一个简单易用的Python库，你可以用它来设计单元测试。设计的测试可以简单地用来测试代码的变化，并在整个开发过程和未来代码的变化中控制潜在错误的危险。'
- en: In programming for data analysis and machine learning modeling, we need to use
    data that is in different variables or data objects, comes from a file in your
    local machine or the cloud, is queried from a database, or comes from a URL to
    our tests. Fixtures help us in these processes by removing the need to repeat
    the same code across our tests. Attaching a fixture function to a test will run
    it and return data to the test before each test runs. We can use the examples
    provided on the `pytest` documentation page for fixtures ([https://docs.pytest.org/en/7.1.x/how-to/fixtures.html](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html)).
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据分析和机器学习建模的编程中，我们需要使用来自不同变量或数据对象的数据，这些数据可能来自本地机器或云端的文件，也可能来自数据库的查询，或者来自我们的测试的URL。固定装置（Fixtures）通过消除在测试中重复相同代码的需要来帮助我们完成这些过程。将固定装置函数附加到测试上会在每个测试运行之前运行它，并将数据返回给测试。我们可以使用`pytest`文档页面上的示例（[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html)）。
- en: Differential testing attempts to check two versions of a piece of software,
    as base and test versions, on the same input and compare the outputs. This process
    helps identify whether the outputs are the same and identify unexpected differences.
    In differential testing, the base version is already verified and considered as
    the approved version while the test version needs to be checked against the base
    version in producing the correct output.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 差分测试试图检查同一输入上的软件的两个版本，即基版和测试版，并比较输出。这个过程有助于确定输出是否相同，并识别意外的差异。在差分测试中，基版已经过验证，被认为是批准的版本，而测试版需要与基版进行比较，以产生正确的输出。
- en: '`mlflow` is a widely used machine learning experiment tracking library that
    we can use in Python. Keeping track of our machine learning experiments will help
    us to reduce the risks of invalid conclusions and selecting unreliable models.
    Experiment tracking in machine learning is about saving information about the
    experiments, such as the data that has been used, the testing performance and
    the metric used for performance assessment, and the algorithms and the hyperparameters
    used for modeling.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mlflow`是一个广泛使用的机器学习实验跟踪库，我们可以在Python中使用它。跟踪我们的机器学习实验将帮助我们减少无效结论的风险和选择不可靠模型的风险。机器学习中的实验跟踪是关于保存有关实验的信息，例如已使用的数据、测试性能和用于性能评估的指标，以及用于建模的算法和超参数。'
- en: '[*Chapter 9*](B16369_09.xhtml#_idTextAnchor261) – Testing and Debugging for
    Production'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第9章*](B16369_09.xhtml#_idTextAnchor261) – 生产测试和调试'
- en: '**Data drift**: Data drift happens if the characteristics and meaning of features
    or independent variables in production differ from the modeling stage. Imagine
    you used a third-party tool to generate a score for the health or financial situation
    of people. The algorithm behind that tool could change over time, and its range
    and meaning will not be the same when your model gets used in production. If you
    have not updated your model accordingly, then your model will not work as expected
    as the meaning of the value of the features will not be the same between the data
    used for training and the user data after deployment.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据漂移**：如果生产中特征或独立变量的特征和意义与建模阶段不同，就会发生数据漂移。想象一下，你使用第三方工具为人们的健康或财务状况生成分数。该工具背后的算法可能会随时间变化，并且当你的模型在生产中使用时，其范围和意义将不会相同。如果你没有相应地更新你的模型，那么你的模型将不会按预期工作，因为特征值的含义在用于训练的数据和部署后的用户数据之间将不同。'
- en: '**Concept drift**: Concept drift is about any change in the definition of output
    variables. For example, real decision boundaries between training data and production
    could be different because of concept drift, meaning the effort in training might
    result in a decision boundary far from reality in production.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念漂移**：概念漂移是指输出变量定义的任何变化。例如，由于概念漂移，训练数据和生产数据之间的实际决策边界可能不同，这意味着训练中的努力可能导致生产中的决策边界远离现实。'
- en: '**Model assertions** can help you detect issues early on, such as input data
    drift or other unexpected behaviors that might affect the model’s performance.
    We can consider model assertions as a set of rules that get checked during the
    model’s training, validation, or even during deployment to ensure that the model’s
    predictions meet the predefined conditions. Model assertions can help us in many
    ways, such as detecting and diagnosing issues with the model or input data, allowing
    us to address them before they impact the model’s performance.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型断言**可以帮助你早期发现问题，例如输入数据漂移或其他可能影响模型性能的意外行为。我们可以将模型断言视为在模型训练、验证甚至部署期间进行检查的一组规则，以确保模型的预测满足预定义的条件。模型断言可以从许多方面帮助我们，例如检测和诊断模型或输入数据的问题，使我们能够在它们影响模型性能之前解决这些问题。'
- en: 'Here are some examples of the components of integration testing:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有一些集成测试组件的例子：
- en: '**Testing data pipelines**: We need to evaluate that the data preprocessing
    components before model training, such as data wrangling, are consistent between
    the training and deployment stages.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据管道**：我们需要评估在模型训练之前的数据预处理组件，如数据整理，在训练和部署阶段之间是否一致。'
- en: '**Testing APIs**: If our machine learning model is exposed through an API,
    we can test the API endpoints to ensure they handle requests and responses correctly.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试API**：如果我们的机器学习模型通过API公开，我们可以测试API端点以确保它们正确处理请求和响应。'
- en: '**Testing model deployment**: We can use integration testing to assess the
    model’s deployment process, whether it’s deployed as a standalone service, within
    a container, or embedded in an application. This process helps us ensure that
    the deployment environment provides the necessary resources, such as CPU, memory,
    and storage, and that the model can be updated if needed.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试模型部署**：我们可以使用集成测试来评估模型的部署过程，无论它是作为独立服务、容器内还是嵌入在应用程序中部署。这个过程帮助我们确保部署环境提供必要的资源，例如CPU、内存和存储，并且如果需要，模型可以更新。'
- en: '**Testing interactions with other components**: We need to verify that our
    machine learning model works seamlessly with databases, user interfaces, or third-party
    services. This may include testing how the model’s predictions are stored, displayed,
    or used within the application.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试与其他组件的交互**：我们需要验证我们的机器学习模型能够与数据库、用户界面或第三方服务无缝工作。这可能包括测试模型预测在应用程序中如何存储、显示或使用。'
- en: '**Testing end-to-end functionality**: We can use end-to-end tests that simulate
    real-world scenarios and user interactions to validate that the model’s predictions
    are accurate, reliable, and useful in the context of the overall application.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试端到端功能**：我们可以使用模拟真实场景和用户交互的端到端测试来验证模型的预测在整体应用程序的上下文中是准确、可靠和有用的。'
- en: 'IaC and configuration management tools such as Chef, Puppet, and Ansible can
    be used to automate the deployment, configuration, and management of software
    and hardware infrastructures. These tools could help us ensure consistency and
    reliability across different environments. First, we need to define two important
    terminologies, client and server, before describing what these IaC tools are for
    us:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IaC 和配置管理工具，如 Chef、Puppet 和 Ansible，可以用于自动化软件和硬件基础设施的部署、配置和管理。这些工具可以帮助我们确保在不同环境中的一致性和可靠性。首先，在描述这些
    IaC 工具对我们有什么用之前，我们需要定义两个重要的术语：客户端和服务器：
- en: '**Chef** ([https://www.chef.io/products/chef-infrastructure-management](https://www.chef.io/products/chef-infrastructure-management)):
    Chef is an open source configuration management tool that relies on a client-server
    model, where the Chef server stores the desired configuration, and the Chef client
    applies it to the nodes.'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chef** ([https://www.chef.io/products/chef-infrastructure-management](https://www.chef.io/products/chef-infrastructure-management)):
    Chef 是一款开源配置管理工具，它依赖于客户端-服务器模型，其中 Chef 服务器存储期望的配置，Chef 客户端将其应用于节点。'
- en: '**Puppet** ([https://www.puppet.com/](https://www.puppet.com/)): Puppet is
    another open source configuration management tool that works in a client-server
    model or as a standalone application. Puppet enforces desired configurations across
    nodes by periodically pulling them from the Puppet master server.'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Puppet** ([https://www.puppet.com/](https://www.puppet.com/)): Puppet 是另一款开源配置管理工具，它可以在客户端-服务器模式下工作，也可以作为独立应用程序运行。Puppet
    通过定期从 Puppet 主服务器拉取配置来强制执行节点上的期望配置。'
- en: '**Ansible** ([https://www.ansible.com/](https://www.ansible.com/)): Ansible
    is an open source and easy-to-use configuration management, orchestration, and
    automation tool that employs an agentless architecture to communicate and apply
    configurations to nodes.'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ansible** ([https://www.ansible.com/](https://www.ansible.com/)): Ansible
    是一款开源且易于使用的配置管理、编排和自动化工具，它采用无代理架构来与节点通信并应用配置。'
- en: '[*Chapter 10*](B16369_10.xhtml#_idTextAnchor286) – Versioning and Reproducible
    Machine Learning Modeling'
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第 10 章*](B16369_10.xhtml#_idTextAnchor286) – 版本控制和可重复的机器学习建模'
- en: '**MLflow**: We introduced MLflow for experiment tracking and model monitoring
    in previous chapters, but you can also use it for data versioning ([https://mlflow.org/](https://mlflow.org/)).'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**MLflow**: 我们在之前的章节中介绍了 MLflow 用于实验跟踪和模型监控，但您也可以用它进行数据版本化 ([https://mlflow.org/](https://mlflow.org/))。'
- en: '**DVC**: An open source version control system for managing data, code, and
    ML models. It is designed to handle large datasets and integrates with Git ([https://dvc.org/](https://dvc.org/)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**DVC**: 一个用于管理数据、代码和机器学习模型的开源版本控制系统。它旨在处理大型数据集，并与 Git 集成 ([https://dvc.org/](https://dvc.org/))。'
- en: '**Pachyderm**: A data versioning platform that provides reproducibility, provenance,
    and scalability in machine learning workflows ([https://www.pachyderm.com/](https://www.pachyderm.com/)).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pachyderm**: 一个提供机器学习工作流程可重复性、溯源和可扩展性的数据版本化平台 ([https://www.pachyderm.com/](https://www.pachyderm.com/))。'
- en: No. Different versions of the same data file could be stored with the same name
    and restored and retrieved when needed.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: No. 同一个数据文件的多个版本可以以相同的名称存储，并在需要时恢复和检索。
- en: A simple change of the random state when splitting data into training and test
    sets or during model initialization could result in different parameter values
    and performances for training and evaluation sets.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将数据分割为训练集和测试集或模型初始化时简单更改随机状态可能会导致训练集和评估集的参数值和性能不同。
- en: '[*Chapter 11*](B16369_11.xhtml#_idTextAnchor300) – Avoiding and Detecting Data
    and Concept Drifts'
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第 11 章*](B16369_11.xhtml#_idTextAnchor300) – 避免和检测数据漂移和概念漂移'
- en: '**Magnitude**: We might face different magnitudes of difference in the data
    distribution resulting in drift in our machine learning models. Small changes
    in the data distribution may be difficult to detect, while large changes may be
    more noticeable.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**幅度**：我们可能会遇到数据分布中不同的幅度差异，导致我们的机器学习模型发生漂移。数据分布中的小变化可能难以检测，而大变化可能更明显。'
- en: '**Frequency**: Drifts might occur in different frequencies.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**频率**：漂移可能发生在不同的频率上。'
- en: The Kolmogorov–Smirnov test can be used for data drift detection.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kolmogorov-Smirnov测试可用于数据漂移检测。
- en: '[*Chapter 12*](B16369_12.xhtml#_idTextAnchor320) – Going Beyond ML Debugging
    with Deep Learning'
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第12章*](B16369_12.xhtml#_idTextAnchor320) – 深度学习超越ML调试'
- en: Yes, in the forward pass, parameters that are already calculated get used for
    output generation; then, the difference between the real and predicted output
    gets used in the backpropagation process to update the weights.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，在正向传递中，已经计算出的参数被用于输出生成；然后，实际输出和预测输出之间的差异被用于反向传播过程中更新权重。
- en: In stochastic gradient descent, one data point is used per iteration to optimize
    and update the model weights, while in mini-batch gradient descent, a mini-batch
    (small subset) of data points gets used.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随机梯度下降中，每个迭代使用一个数据点来优化和更新模型权重，而在小批量梯度下降中，使用数据点的小批量（小子集）。
- en: Each batch or mini-batch is a small subset of data points in the training set
    that gets used to calculate the loss and update the model’s weights. In each epoch,
    multiple batches get iterated to cover all training data.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个批次或小批量是训练集中用于计算损失和更新模型权重的数据点的小子集。在每个epoch中，多个批次被迭代以覆盖所有训练数据。
- en: The sigmoid and softmax functions are commonly used in output layers to transform
    the scores of the output neurons to values of between zero and one for classification
    models. This is called the probability of predictions.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sigmoid和softmax函数通常用于输出层，将输出神经元的分数转换为介于零和一之间的值，用于分类模型。这被称为预测的概率。
- en: '[*Chapter 13*](B16369_13.xhtml#_idTextAnchor342) – Advanced Deep Learning Techniques'
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第13章*](B16369_13.xhtml#_idTextAnchor342) – 高级深度学习技术'
- en: CNNs can be used for image classification or segmentation – for example, for
    radiological images to identify malignancies (tumor regions). On the other hand,
    GNNs can be used in social and biological networks.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNNs可用于图像分类或分割——例如，用于放射学图像以识别恶性肿瘤（肿瘤区域）。另一方面，GNNs可用于社交和生物网络。
- en: Yes, it does.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，确实如此。
- en: It might result in more mistakes.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可能会导致更多的错误。
- en: To handle this challenge, a common ID, such as 0, gets used before or after
    IDs of tokens of words in each sequence of words or sentences in a process called
    padding.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，在称为填充的过程中，在单词序列或句子中的每个单词的标记ID之前或之后使用一个常见的ID，例如0。
- en: The classes we build for CNNs and GNNs have similar code structures.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为CNNs和GNNs构建的类别具有相似的代码结构。
- en: Edge features help you include some vital information, depending on the application.
    For example, in chemistry, you can determine the type of chemical bond as an edge
    feature, while the nodes could be the atoms in the graphs.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边缘特征有助于你包含一些关键信息，具体取决于应用。例如，在化学中，你可以将化学键的类型作为边缘特征，而节点可以是图中的原子。
- en: '[*Chapter 14*](B16369_14.xhtml#_idTextAnchor379) – Introduction to Recent Advancements
    in Machine Learning'
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第14章*](B16369_14.xhtml#_idTextAnchor379) – 机器学习最新进展简介'
- en: Transformer-based text generation, VAEs, and GANs.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于Transformer的文本生成，VAEs和GANs。
- en: Different versions of LLaMA and GPT.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLaMA和GPT的不同版本。
- en: The generator, which could be a neural network architecture for generating desired
    data types, such as images, generates images aiming to fool the discriminator
    into recognizing the generated data as real data. The discriminator learns to
    remain good at recognizing generated data compared to real data.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器，它可以是用于生成所需数据类型的神经网络架构，例如图像，生成图像的目的是欺骗判别器将其识别为真实数据。判别器学习在识别生成数据与真实数据相比保持良好。
- en: You can improve your prompting by being specific about the question and specifying
    for whom the data is being generated.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过具体说明问题和指定数据生成对象来提高你的提示。
- en: In RLHF, the reward is calculated based on the feedback of humans, either experts
    or non-experts, depending on the problem. But the reward is not like a predefined
    mathematical formula considering the complexity of problems such as language modeling.
    The feedback provided by humans results in improving the model step by step.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在RLHF中，奖励是根据人类反馈计算的，无论是专家还是非专家，这取决于问题。但奖励并不是像考虑语言模型等问题的复杂性那样预定义的数学公式。人类提供的反馈导致模型逐步改进。
- en: The idea of contrastive learning is to learn representations that result in
    similar data points being closer to each other compared to dissimilar data points.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对比学习的想法是学习表示，使得相似数据点彼此更接近，而不同数据点则更远。
- en: '[*Chapter 15*](B16369_15.xhtml#_idTextAnchor406) – Correlation versus Causality'
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第15章*](B16369_15.xhtml#_idTextAnchor406) – 相关性 versus 因果性'
- en: Yes. You can have features that are highly correlated with the output in supervised
    learning that aren’t causal.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的。在监督学习中，你可以有与输出高度相关的特征，而这些特征并不是因果的。
- en: One way to establish causality is to conduct experiments, as in **experimental
    design**, where we measure the effect of changes in the causal feature on the
    target variable. However, such experimental studies may not always be feasible
    or ethical. In **observational studies**, we use observational data, instead of
    controlled experiments, and try to identify causal relationships by controlling
    confounding variables.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立因果关系的一种方法是通过实验，如在**实验设计**中，我们测量因果特征变化对目标变量的影响。然而，这种实验研究可能并不总是可行或道德的。在**观察性研究**中，我们使用观察数据，而不是控制实验，并试图通过控制混杂变量来识别因果关系。
- en: '**Instrumental variables** is used in causal aim to overcome a common problem
    in observational studies where the treatment and outcome variables are jointly
    determined by other variables, or confounders, that are not included in the model.
    This approach starts with identifying an instrument that is correlated with the
    treatment variable and uncorrelated with the outcome variable, except through
    its effect on the treatment variable.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**工具变量**用于因果分析，以克服观察性研究中常见的共同问题，即治疗变量和结果变量由其他变量（或混杂因素）共同决定，而这些变量未包含在模型中。这种方法从识别一个与治疗变量相关且与结果变量不相关（除了通过其对治疗变量的影响）的工具变量开始。'
- en: The directions, from a feature to the outcome, don’t necessarily mean causality.
    But *Bayesian* networks can be used for estimating the causal effects of variables
    on the outcome while controlling the confounding variables.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征到结果的方向并不一定意味着因果性。但**贝叶斯**网络可以用来估计变量对结果的影响，同时控制混杂变量。
- en: '[*Chapter 16*](B16369_16.xhtml#_idTextAnchor429) – Security and Privacy in
    Machine Learning'
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第16章*](B16369_16.xhtml#_idTextAnchor429) – 机器学习中的安全和隐私'
- en: '**Advanced Encryption Standard** (**AES**): AES is one of the strongest encryption
    algorithms that protects data. AES accepts different key sizes: 128, 192, or 256
    bits.'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高级加密标准**（**AES**）：AES是保护数据的最强大的加密算法之一。AES接受不同的密钥大小：128位、192位或256位。'
- en: '**Triple Data Encryption Standard** (**DES**): Triple DES is an encryption
    method that uses a 56-bit key to encrypt data blocks.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**三重数据加密标准**（**DES**）：三重DES是一种使用56位密钥加密数据块的加密方法。'
- en: '**Blowfish**: Blowfish is a symmetric-key encryption technique used as an alternative
    to the DES encryption algorithm. Blowfish is fast and highly effective for data
    encryption. It splits data, for example, strings and messages, into blocks of
    64 bits and encrypts them individually.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**Blowfish**：Blowfish是一种对称密钥加密技术，用作DES加密算法的替代方案。Blowfish加密速度快，对数据加密非常有效。它将数据，例如字符串和消息，分成64位的块，并分别加密它们。'
- en: We can use a model for inference on encrypted data without the need for decryption.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在不需要解密的情况下使用模型对加密数据进行推理。
- en: The objective of **differential privacy** (**DP**) is to ensure that the removal
    or addition of individual data points does not affect the outcome of the modeling.
    For example, by adding random noise to a normal distribution, it tries to make
    the features of individual data points obscure.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**差分隐私**（**DP**）的目标是确保删除或添加单个数据点不会影响建模的结果。例如，通过向正态分布添加随机噪声，它试图使单个数据点的特征变得模糊不清。'
- en: The challenge of using FL or DP in practice goes beyond programming or infrastructure
    design. In spite of this great alternative to storing user data locally, there
    are still ethical, legal, and business challenges when benefitting from FL in
    different applications.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实践中使用联邦学习（FL）或差分隐私（DP）的挑战不仅限于编程或基础设施设计。尽管存储用户数据本地有这样一个很好的替代方案，但在从不同应用中受益于FL时，仍然存在伦理、法律和商业挑战。
- en: '[*Chapter 17*](B16369_17.xhtml#_idTextAnchor447) – Human-in-the-Loop Machine
    Learning'
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[*第17章*](B16369_17.xhtml#_idTextAnchor447) – 循环中的人类机器学习'
- en: No. For example, you can bring human experts into the loop through *active learning*.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，例如，您可以通过**主动学习**将人类专家引入循环。
- en: In **uncertainty sampling**, data points get selected solely based on uncertainty
    in inference. But in **density-weighted uncertainty sampling**, instances get
    selected not only based on their highest uncertainty but also to be representative
    of the many other data points that rely on the density of data in the feature
    space.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**不确定性采样**中，数据点仅根据推理中的不确定性被选中。但在**密度加权不确定性采样**中，实例不仅基于它们最高的不确定性被选中，还要代表依赖于特征空间中数据密度的许多其他数据点。
