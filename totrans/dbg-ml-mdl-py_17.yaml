- en: '17'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human-in-the-Loop Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning modeling is more than just machine learning developers and
    engineers sitting behind their computers to build and revise components of a machine
    learning life cycle. Incorporating feedback from domain experts, or even the non-expert
    crowd, is key in bringing more reliable and application-oriented models to production.
    This concept, which is called human-in-the-loop machine learning, is about benefiting
    from human intelligence and expert knowledge in different stages of a life cycle
    to further improve the performance and reliability of our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Humans in the machine learning life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human-in-the-loop modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know about the benefits and challenges
    of incorporating human intelligence in your machine learning modeling projects.
  prefs: []
  type: TYPE_NORMAL
- en: Humans in the machine learning life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Developing and improving different components of a machine learning life cycle
    to bring a reliable and high-performance model to production is a collaborative
    effort that can benefit from expert and non-expert human feedback (*Figure 17**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 17.1 – Human\uFEFFs in the machine learning life cycle](img/B16369_17_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 17.1 – Humans in the machine learning life cycle
  prefs: []
  type: TYPE_NORMAL
- en: For example, a radiologist can help in annotating radiological images while
    most people with good vision capabilities can easily label cat and dog images.
    But incorporating human feedback is not limited to data annotation at the beginning
    of a life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: We can benefit from human intelligence and expertise to improve data preparation,
    feature engineering, and representation learning aspects of a life cycle, as well
    as model training and testing, and eventually model deployment and monitoring.
    In each of these stages, human feedback can be incorporated either passively or
    actively, which allows us to bring a better model into production.
  prefs: []
  type: TYPE_NORMAL
- en: Passive human-in-the-loop is about collecting feedback and information from
    experts and non-experts and benefitting from that the next time we revise components
    of the corresponding machine learning modeling system. In this process, the feedback
    and extra information help in identifying opportunities for improving the components
    of the life cycle and identifying data and concept drift to bring a better model
    into production. In active human-in-the-loop machine learning, the infrastructure
    and one or all of the life cycle components need to be designed in a way that
    the extra human-in-the-loop information and data can be actively and continuously
    incorporated to improve data analysis and modeling.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will review expert feedback collection and how to effectively benefit
    from it in improving our models.
  prefs: []
  type: TYPE_NORMAL
- en: Expert feedback collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ultimate goal of building a piece of technology on top of one or multiple
    machine learning modes is to provide a tool for users, experts, or non-experts
    for a specific objective, such as healthcare image classification, stock price
    prediction, credit risk estimation, and product recommendation in platforms such
    as Amazon. For example, we can collect feedback for data annotation or later in
    the production stage for drift detection. We can then use this feedback to improve
    our models. However, this feedback could extend beyond the purposes of just data
    annotation or identifying data and concept drift.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can incorporate expert feedback for four major purposes: data generation
    and annotation, data filtering, model selection, and model monitoring. Expert
    feedback collection for annotation and monitoring is generally similar to non-expert
    data collection except for the fact that in some applications, expertise is of
    necessity, such as in classifying radiological images.'
  prefs: []
  type: TYPE_NORMAL
- en: For model selection, we can use expert feedback to not need to rely exclusively
    on the performance measures we use for model performance assessment and, consequently,
    select the best model, but to detect red flags according to wrong predictions
    or rely on explainability information for our models, such as if features that
    contribute the most in terms of predictions are of lowest relevance.
  prefs: []
  type: TYPE_NORMAL
- en: We can also benefit from experts’ feedback in monitoring our models. Drift detection,
    as discussed in [*Chapter 11*](B16369_11.xhtml#_idTextAnchor300), *Avoiding and
    Detecting Data and Concept Drifts*, is crucial to ensure the reliability of our
    models in production. In many applications, users of our models could be experts
    in specific domains, such as healthcare and drug discovery. In such cases, we
    need to make sure we continuously collect their feedback and use this to detect
    and eliminate drifts in our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Collecting feedback from experts as users of our machine learning models should
    not be limited to getting their binary response of “good” versus “bad.” We need
    to provide enough information about our models and their predictions and ask experts
    to provide their feedback, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Provide sufficient information**: When asking for feedback from expert users
    of our models, we need to provide sufficient information to get better and more
    relevant feedback. For example, in addition to the performance of our model in
    testing and production, or wrong and correct predictions for a specific set of
    data points, we can also provide explainability information on how the model came
    up with its decision for those data points. This type of information could help
    the users provide better feedback that will help us in improving our models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Don’t ask for translations**: Many of the users of our models might have
    limited statistical and machine learning modeling knowledge. So, asking them to
    convert their opinions and ideas into technical terms would limit efficient feedback
    collection. You need to provide sufficient information and ask for their feedback
    and have a back-and-forth conversation to convert their insights into actionable
    items for model improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Design for automated feedback collection**: Although it is better to not
    ask for translations, as pointed out earlier, you can move toward more automated
    feedback collection using clear and detailed questions and proper infrastructure
    design to collect the feedback and incorporate it into your models. For example,
    you can use machine learning explainability and ask whether the most informative
    features used by the model for predicting the output of a specific set of data
    points are relevant to the task or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human-in-the-loop has its own challenges, such as in preserving privacy when
    third-party companies are needed to monitor models and pipelines, or when there
    would be specific legal barriers in sharing data coming from collaborators and
    business partners with others in our teams and organizations. We need to keep
    these challenges in mind when we’re designing so that we can benefit from human
    feedback in our machine learning life cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Although we can collect feedback in different stages of the machine learning
    life cycle to improve our models, there are techniques such as active learning
    (which we will cover next) that can help us bring a better model with lower cost
    into production.
  prefs: []
  type: TYPE_NORMAL
- en: Human-in-the-loop modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite more high-quality annotated data points being more valuable, the cost
    of annotating data, specifically when domain expertise is of necessity, could
    be very high. Active learning is a strategy that helps us in generating and labeling
    data to improve the performance of our models at a lower cost. In an active learning
    setting, we aim to benefit from a model with a limited amount of data and iteratively
    select new data points to be labeled, or their continuous value identified, with
    the aim of achieving higher performance (Wu et al., 2022; Ren et al., 2021; Burbidge
    et al., 2007). The model queries new instances to be annotated by experts or non-experts,
    or their labels or continuous values are identified via any computational or experimental
    technique. However, instead of the instances being selected randomly, there are
    techniques for new instance selection to help us in achieving better models with
    a lower number of instances and iterations (*Table 17.1*). Each of these techniques
    has its advantages and disadvantages. For example, *uncertainty sampling* is simple
    but its effect on performance might be limited if uncertainty in the predicted
    output of instances is not highly correlated with model error:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data-Centric** | **Model-Centric** |'
  prefs: []
  type: TYPE_TB
- en: '| **Uncertainty sampling**Selecting instances with the most uncertainty (in
    inference), which could be instances closest to the decision boundary in classification
    problems | **Expected** **model change**Selecting instances that know their labels
    results in the biggest impact on the current model |'
  prefs: []
  type: TYPE_TB
- en: '| **Density-weighted** **uncertainty sampling**Selecting instances that not
    only have the highest uncertainty but also are representative of many other data
    points that rely on the density of data in feature space | **Estimation of** **error
    reduction**Selecting instances that know their labels would result in the biggest
    future error reduction |'
  prefs: []
  type: TYPE_TB
- en: '| **Query-by-committee**Multiple models (the committee) get trained and instances
    with the highest disagreement in their prediction get selected | **Variance reduction**Selecting
    instances that know their labels would result in the most reduction in the model’s
    uncertainty about its parameters |'
  prefs: []
  type: TYPE_TB
- en: Table 17.1 – Active learning techniques for instance election to be annotated
    in each step
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focused on introducing concepts and techniques behind human-in-the-loop.
    However, there are Python libraries such as `modAL` ([https://modal-python.readthedocs.io/en/latest/](https://modal-python.readthedocs.io/en/latest/))
    that can help you in implementing some of these techniques in your projects to
    bring human feedback into your machine learning life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about some of the important concepts in human-in-the-loop
    machine learning, which can help you in better establishing collaboration between
    you and your team with experts or non-experts so that you can incorporate their
    feedback into your machine learning modeling projects.
  prefs: []
  type: TYPE_NORMAL
- en: This was the last chapter of this book. I hope you learned enough about different
    approaches to improve your machine learning models and build better ones so that
    you can start your journey toward becoming an expert in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is human-in-the-loop machine learning limited to data annotation and labeling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between uncertainty sampling and density-weighted uncertainty
    sampling in selecting instances in each step of an active learning process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amershi, Saleema, et al. *Power to the people: The role of humans in interactive
    machine learning*. Ai Magazine 35.4 (2014): 105-120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu, Xingjiao, et al. *A survey of human-in-the-loop for machine learning*.
    Future Generation Computer Systems 135 (2022): 364-381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren, Pengzhen, et al. *A survey of deep active learning*. ACM computing surveys
    (CSUR) 54.9 (2021): 1-40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burbidge, Robert, Jem J. Rowland, and Ross D. King. *Active learning for regression
    based on query by committee*. Intelligent Data Engineering and Automated Learning-IDEAL
    2007: 8th International Conference, Birmingham, UK, December 16-19, 2007\. Proceedings
    8\. Springer Berlin Heidelberg, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai, Wenbin, Ya Zhang, and Jun Zhou. *Maximizing expected model change for active
    learning in regression*. 2013 IEEE 13th international conference on data mining.
    IEEE, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roy, Nicholas, and Andrew McCallum. *Toward optimal active learning through
    monte carlo estimation of error reduction*. ICML, Williamstown 2 (2001): 441-448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Donmez, Pinar, Jaime G. Carbonell, and Paul N. Bennett. *Dual strategy active
    learning*. Machine Learning: ECML 2007: 18th European Conference on Machine Learning,
    Warsaw, Poland, September 17-21, 2007\. Proceedings 18\. Springer Berlin Heidelberg,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B16369_01.xhtml#_idTextAnchor015) – Beyond Code Debugging'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Yes – here is an example that was provided in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are their definitions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AttributeError`: This type of error is raised when an attribute is used for
    an object that it is not defined for. For example, `isnull` is not defined for
    a list. So, `my_list. isnull()` results in `AttributeError`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NameError`: This error is raised when you try to call a function, class, or
    other names and modules that are not defined in your code. For example, if you
    haven’t defined a `neural_ network` class in your code but call it in your code
    as `neural_network()`, you will get a `NameError` message.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher dimensionality makes a sparser feature space and could reduce the confidence
    of the model in identifying generalizable decision boundaries in a classification
    setting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you get an error message in Python, it usually provides you with the necessary
    information to find the issue. This information creates a report-like message
    about the lines of your code that the error occurred in, the types of errors,
    and the function or class calls that resulted in such errors. This report-like
    message is called a **traceback** in Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Incremental programming**: Writing code for every small component, then testing
    it and writing test codes using PyTest, for example, could help you avoid issues
    with each function or class you wrote. It also helps you ensure the outputs of
    one module that feed another module as its input are compatible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Logging**: When you develop functions and classes in Python, you can benefit
    from logging to log information, errors, and other kinds of messages to help you
    in identifying potential sources of issues when you get an error message.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you use experts, such as radiologists, to annotate medical images
    for a cancer diagnosis, then the confidence on the label of images could be different.
    And these confidences could be considered in the modeling phase either in the
    data collection process, such as by asking more experts to annotate the same images,
    or in the modeling process, such as by assigning a weight to each image based
    on the confidence in labeling. The features of your data could also have different
    qualities. For example, you might have highly sparse features that have mostly
    zero values across the data points or features that might have different levels
    of confidence. For example, a measurement feature will have lower confidence if
    you use a measurement tape to capture millimeter differences between the sizes
    of objects, such as dice, compared to using the same tape to capture differences
    between bigger objects, such as furniture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can control underfitting and overfitting by controlling model complexity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, it is possible. The data that’s used for training and testing machine learning
    models could become out of date. For example, the changes in the trends of the
    clothing market could make predictions of a model for clothing recommendation
    unreliable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By playing with model hyperparameters alone, you can’t develop the best possible
    model. In the same way, by increasing the quality and quantity of your data and
    keeping your model hyperparameters the same, you also can’t achieve the best performance
    possible. So, data and hyperparameters work hand in hand.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B16369_02.xhtml#_idTextAnchor076) – Machine Learning Life Cycle'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Examples of cleaning processes are filling in missing values in your data and
    removing outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One-hot encoding generates a new feature for each category of categorical features.
    Label encoding keeps the same features and just replaces each category with a
    number assigned to that category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The simplest way of detecting outliers is by using quantiles of the distribution
    of variable values. Data points that are beyond the upper and lower bounds are
    considered outliers. The lower and upper bounds can be calculated as *Q1-a.IQR*
    and *Q3+a.IQR*, where *a* can be a real value between 1.5 and 3\. The common value
    of *a* that is also used by default in drawing a boxplot is 1.5, but having higher
    values makes the process of outlier identification less stringent and lets fewer
    data points be detected as outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to deploy a model in doctors’ personal computers in hospitals to
    be used directly by clinicians, you need to consider all difficulties and planning
    needed to set up the proper production environment and all the software dependencies.
    You also need to make sure their local system has the necessary hardware requirements.
    These are not among the considerations if you want to deploy a model behind chatbots
    in a banking system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B16369_03.xhtml#_idTextAnchor119) – Debugging toward Responsible
    AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data collection bias**: Data that is collected could have biases such as
    gender bias, as in Amazon’s applicant sorting examples, race bias, as in COMPAS,
    socioeconomic biases, as in hospitalization examples, or other kinds of biases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sampling bias**: Another source of data bias could be in the process of sampling
    data points or sampling the population in the data collection stage of the life
    cycle. For example, in sampling students to fill in a survey, our sampling process
    could be biased toward girls or boys, rich or poor student families, or high-
    versus low-grade students.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Perfect-knowledge white-box attacks**: The attacker knows everything about
    the system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Zero-knowledge black-box attacks**: The attacker doesn’t have any knowledge
    of the system itself but collects information through predictions of the model
    in production.'
  prefs: []
  type: TYPE_NORMAL
- en: The encryption process transforms the information, data, or algorithm into a
    new (that is, encrypted) form. The encrypted data can be decrypted (that is, become
    human-readable or machine understandable) if the individual has access to the
    encryption key (that is, a password-style key necessary for the decryption process).
    In this way, getting access to the data and algorithm without the encryption key
    will be almost impossible or very difficult.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Differential privacy** tries to ensure that the removal or addition of individual
    data points does not affect the outcome of modeling. It attempts to learn from
    patterns within groups of data points. For example, by adding random noise from
    a normal distribution, it tries to make the features of individual data points
    obscure. The effect of noise in learning could be eliminated based on the law
    of large numbers if a large number of data points will be accessible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Federated learning** relies on the idea of decentralizing learning, data
    analysis, and inference, thus allowing the user’s data to be kept within individual
    devices or local databases.'
  prefs: []
  type: TYPE_NORMAL
- en: Transparency helps in building trust in users and could potentially increase
    the number of users that trust and use your models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16369_04.xhtml#_idTextAnchor159) – Detecting Performance and
    Efficiency Issues in Machine Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a primary diagnostic test, with more accurate follow-up tests, we want to
    make sure we do not lose any patients with the disease we are testing for. So,
    we need to aim to decrease false negatives, while trying to decrease false positives
    at the same time. So, we can aim to maximize **recall** while controlling for
    **precision** and **specificity**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In such cases, you want to make sure you have the **precision** to control risks
    and suggest good investment opportunities. This might result in a lower **recall**,
    which is okay as a bad investment could result in a significant loss of capital
    for individual investors. Here, we don’t want to consider the details of investment
    risk management and want to have a high-level understanding of how to select a
    good performance measure. If you are an expert in this field, consider your knowledge
    and select a good performance measure that satisfies the requirements you are
    aware of.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ROC-AUC is a summary measure. Two models with the same ROC-AUCs could have different
    predictions for individual data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**MCC** focuses on predicted labels, while **log-loss** cares about predicted
    probabilities for the tested data points. So, a lower **log-loss** does not necessarily
    result in a lower **MCC**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not necessarily. *R*2 doesn’t take into account data dimensionality (that is,
    the number of features, inputs, or independent variables). A model with more features
    could result in a higher *R*2, while it might not necessarily be a better model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It depends on the performance measure and test data used for assessing the generalizability
    of the model. We need to use the right performance measure for our objective in
    production, and use a set of data points for model testing that will be more reflective
    of unseen data in production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16369_05.xhtml#_idTextAnchor183) – Improving the Performance
    of Machine Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adding more training data points could help to reduce variance while adding
    more features could help to reduce bias. However, there is no guarantee of a reduction
    of variance through the addition of new data points or whether new features will
    be helpful in reducing variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assigning weights during optimization**: You can assign a weight to each
    data point, according to the confidence of class labels, when training machine
    learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensemble learning**: If you consider a distribution of the quality or confidence
    score of each data point, then you can build different models using data points
    from each part of this distribution and then combine the predictions of the models
    for example using their weighted average.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transfer learning**: You can train a model on a large dataset with different
    levels of label confidence (see *Figure 5**.3*), excluding very low-confidence
    data and then fine-tune it on the very high-confidence part of your dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: By increasing confidence in identifying the decision boundary, in a classification
    setting, where the minority class is sparse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we use Borderline-SMOTE, the new synthetically generated data points would
    be close to the majority-class data points, which helps in identifying a generalizable
    decision boundary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In DSMOTE, **DBSCAN** is used to divide data points of the minority class into
    three groups of core samples, borderline samples, and noise (that is, outlying)
    samples, and then the core and borderline samples only get used for oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: Searching over the whole possible combinations of hyperparameters is not necessary,
    as explained in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, L1 regularization can eliminate the contribution of features to the regularization
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, it is possible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16369_06.xhtml#_idTextAnchor201) – Interpretability and Explainability
    in Machine Learning Modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explainability can help improve performance, such as by reducing the sensitivity
    of models to small feature value changes, increasing data efficiency in model
    training, trying to help in proper reasoning in models, and avoiding spurious
    correlations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Local explainability** helps us understand the behavior of a model close
    to a data point in feature space. Although these models meet local fidelity criteria,
    features that have been identified to be locally important might not be globally
    important, and vice versa.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Global explainability** techniques try to go beyond local explainability
    and provide global explanations to the models.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear models, although interpretable, usually have low performance. Instead,
    we could benefit from more complex models, with higher performance, and use explainability
    techniques to understand how the model comes up with its predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, it does. Explainability techniques could help us understand what models
    are major contributors to predictions for one set of data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SHAP can determine how each feature contributes to a model’s prediction. As
    features work cooperatively in determining the decision boundaries of classification
    models and eventually affecting model predictions, SHAP tries to first identify
    the marginal contribution of each feature and then provide Shapely values as an
    estimate of each feature in cooperation with the whole feature set to predict
    a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LIME is an alternative to SHAP for **local explainability** that explains the
    predictions of any classifier or regressor, in a model-agnostic way, by approximating
    a model locally with an interpretable model.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual examples, or explanations, help us identify what needs to be
    changed in an instance to change the outcome of a classification model. These
    counterfactuals could help in identifying actionable paths in many applications,
    such as finance, retail, marketing, recruiting, and healthcare. For example, we
    can use them to suggest to a bank customer how they can change the rejection to
    their loan application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As presented in the *Counterfactual generation using Diverse Counterfactual
    Explanations (DiCE)* section, not all counterfactuals are feasible according to
    the definition and meaning of each feature. For example, if we want to suggest
    to a 30-year-old individual to change their outcome, suggesting that they need
    to wait until they get to their 50s is not an effective and actionable suggestion.
    Also, suggesting a change of `hours_per_week` of work from 38 to >80 is not feasible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16369_07.xhtml#_idTextAnchor218) – Decreasing Bias and Achieving
    Fairness'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No. There might be proxies in our models for sensitive attributes, but not in
    our models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Salary and income (in some countries), occupation, a history of a felony charge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not necessarily. Satisfying fairness according to **demographic parity** wouldn’t
    necessarily result in a model being fair according to **equalized odds**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Demographic parity** is a group fairness definition to ensure that a model’s
    predictions are not dependent on a given sensitive attribute, such as ethnicity
    or sex.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Equalized odds** is satisfied when a given prediction is independent of the
    group of a given sensitive attribute and the real output.'
  prefs: []
  type: TYPE_NORMAL
- en: Not necessarily. For example, there could be feature proxies for `'sex'` among
    top contributors to model predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use explainability techniques to identify potential biases in our models
    and then plan to improve them toward fairness. For example, we can identify fairness
    issues between male and female groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16369_08.xhtml#_idTextAnchor243) – Controlling Risks Using Test-Driven
    Development'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`pytest` is a simple-to-use Python library you can use to design unit tests.
    The designed tests can then be simply used to test changes in your code and control
    risks of potential mistakes throughout the development process and future changes
    in your code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In programming for data analysis and machine learning modeling, we need to use
    data that is in different variables or data objects, comes from a file in your
    local machine or the cloud, is queried from a database, or comes from a URL to
    our tests. Fixtures help us in these processes by removing the need to repeat
    the same code across our tests. Attaching a fixture function to a test will run
    it and return data to the test before each test runs. We can use the examples
    provided on the `pytest` documentation page for fixtures ([https://docs.pytest.org/en/7.1.x/how-to/fixtures.html](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Differential testing attempts to check two versions of a piece of software,
    as base and test versions, on the same input and compare the outputs. This process
    helps identify whether the outputs are the same and identify unexpected differences.
    In differential testing, the base version is already verified and considered as
    the approved version while the test version needs to be checked against the base
    version in producing the correct output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mlflow` is a widely used machine learning experiment tracking library that
    we can use in Python. Keeping track of our machine learning experiments will help
    us to reduce the risks of invalid conclusions and selecting unreliable models.
    Experiment tracking in machine learning is about saving information about the
    experiments, such as the data that has been used, the testing performance and
    the metric used for performance assessment, and the algorithms and the hyperparameters
    used for modeling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B16369_09.xhtml#_idTextAnchor261) – Testing and Debugging for
    Production'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data drift**: Data drift happens if the characteristics and meaning of features
    or independent variables in production differ from the modeling stage. Imagine
    you used a third-party tool to generate a score for the health or financial situation
    of people. The algorithm behind that tool could change over time, and its range
    and meaning will not be the same when your model gets used in production. If you
    have not updated your model accordingly, then your model will not work as expected
    as the meaning of the value of the features will not be the same between the data
    used for training and the user data after deployment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Concept drift**: Concept drift is about any change in the definition of output
    variables. For example, real decision boundaries between training data and production
    could be different because of concept drift, meaning the effort in training might
    result in a decision boundary far from reality in production.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model assertions** can help you detect issues early on, such as input data
    drift or other unexpected behaviors that might affect the model’s performance.
    We can consider model assertions as a set of rules that get checked during the
    model’s training, validation, or even during deployment to ensure that the model’s
    predictions meet the predefined conditions. Model assertions can help us in many
    ways, such as detecting and diagnosing issues with the model or input data, allowing
    us to address them before they impact the model’s performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are some examples of the components of integration testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Testing data pipelines**: We need to evaluate that the data preprocessing
    components before model training, such as data wrangling, are consistent between
    the training and deployment stages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing APIs**: If our machine learning model is exposed through an API,
    we can test the API endpoints to ensure they handle requests and responses correctly.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing model deployment**: We can use integration testing to assess the
    model’s deployment process, whether it’s deployed as a standalone service, within
    a container, or embedded in an application. This process helps us ensure that
    the deployment environment provides the necessary resources, such as CPU, memory,
    and storage, and that the model can be updated if needed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing interactions with other components**: We need to verify that our
    machine learning model works seamlessly with databases, user interfaces, or third-party
    services. This may include testing how the model’s predictions are stored, displayed,
    or used within the application.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing end-to-end functionality**: We can use end-to-end tests that simulate
    real-world scenarios and user interactions to validate that the model’s predictions
    are accurate, reliable, and useful in the context of the overall application.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IaC and configuration management tools such as Chef, Puppet, and Ansible can
    be used to automate the deployment, configuration, and management of software
    and hardware infrastructures. These tools could help us ensure consistency and
    reliability across different environments. First, we need to define two important
    terminologies, client and server, before describing what these IaC tools are for
    us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chef** ([https://www.chef.io/products/chef-infrastructure-management](https://www.chef.io/products/chef-infrastructure-management)):
    Chef is an open source configuration management tool that relies on a client-server
    model, where the Chef server stores the desired configuration, and the Chef client
    applies it to the nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Puppet** ([https://www.puppet.com/](https://www.puppet.com/)): Puppet is
    another open source configuration management tool that works in a client-server
    model or as a standalone application. Puppet enforces desired configurations across
    nodes by periodically pulling them from the Puppet master server.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ansible** ([https://www.ansible.com/](https://www.ansible.com/)): Ansible
    is an open source and easy-to-use configuration management, orchestration, and
    automation tool that employs an agentless architecture to communicate and apply
    configurations to nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B16369_10.xhtml#_idTextAnchor286) – Versioning and Reproducible
    Machine Learning Modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MLflow**: We introduced MLflow for experiment tracking and model monitoring
    in previous chapters, but you can also use it for data versioning ([https://mlflow.org/](https://mlflow.org/)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**DVC**: An open source version control system for managing data, code, and
    ML models. It is designed to handle large datasets and integrates with Git ([https://dvc.org/](https://dvc.org/)).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pachyderm**: A data versioning platform that provides reproducibility, provenance,
    and scalability in machine learning workflows ([https://www.pachyderm.com/](https://www.pachyderm.com/)).'
  prefs: []
  type: TYPE_NORMAL
- en: No. Different versions of the same data file could be stored with the same name
    and restored and retrieved when needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A simple change of the random state when splitting data into training and test
    sets or during model initialization could result in different parameter values
    and performances for training and evaluation sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B16369_11.xhtml#_idTextAnchor300) – Avoiding and Detecting Data
    and Concept Drifts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Magnitude**: We might face different magnitudes of difference in the data
    distribution resulting in drift in our machine learning models. Small changes
    in the data distribution may be difficult to detect, while large changes may be
    more noticeable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Frequency**: Drifts might occur in different frequencies.'
  prefs: []
  type: TYPE_NORMAL
- en: The Kolmogorov–Smirnov test can be used for data drift detection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B16369_12.xhtml#_idTextAnchor320) – Going Beyond ML Debugging
    with Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes, in the forward pass, parameters that are already calculated get used for
    output generation; then, the difference between the real and predicted output
    gets used in the backpropagation process to update the weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In stochastic gradient descent, one data point is used per iteration to optimize
    and update the model weights, while in mini-batch gradient descent, a mini-batch
    (small subset) of data points gets used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each batch or mini-batch is a small subset of data points in the training set
    that gets used to calculate the loss and update the model’s weights. In each epoch,
    multiple batches get iterated to cover all training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sigmoid and softmax functions are commonly used in output layers to transform
    the scores of the output neurons to values of between zero and one for classification
    models. This is called the probability of predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B16369_13.xhtml#_idTextAnchor342) – Advanced Deep Learning Techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs can be used for image classification or segmentation – for example, for
    radiological images to identify malignancies (tumor regions). On the other hand,
    GNNs can be used in social and biological networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, it does.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It might result in more mistakes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To handle this challenge, a common ID, such as 0, gets used before or after
    IDs of tokens of words in each sequence of words or sentences in a process called
    padding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The classes we build for CNNs and GNNs have similar code structures.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edge features help you include some vital information, depending on the application.
    For example, in chemistry, you can determine the type of chemical bond as an edge
    feature, while the nodes could be the atoms in the graphs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B16369_14.xhtml#_idTextAnchor379) – Introduction to Recent Advancements
    in Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer-based text generation, VAEs, and GANs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different versions of LLaMA and GPT.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generator, which could be a neural network architecture for generating desired
    data types, such as images, generates images aiming to fool the discriminator
    into recognizing the generated data as real data. The discriminator learns to
    remain good at recognizing generated data compared to real data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can improve your prompting by being specific about the question and specifying
    for whom the data is being generated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In RLHF, the reward is calculated based on the feedback of humans, either experts
    or non-experts, depending on the problem. But the reward is not like a predefined
    mathematical formula considering the complexity of problems such as language modeling.
    The feedback provided by humans results in improving the model step by step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The idea of contrastive learning is to learn representations that result in
    similar data points being closer to each other compared to dissimilar data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B16369_15.xhtml#_idTextAnchor406) – Correlation versus Causality'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes. You can have features that are highly correlated with the output in supervised
    learning that aren’t causal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One way to establish causality is to conduct experiments, as in **experimental
    design**, where we measure the effect of changes in the causal feature on the
    target variable. However, such experimental studies may not always be feasible
    or ethical. In **observational studies**, we use observational data, instead of
    controlled experiments, and try to identify causal relationships by controlling
    confounding variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Instrumental variables** is used in causal aim to overcome a common problem
    in observational studies where the treatment and outcome variables are jointly
    determined by other variables, or confounders, that are not included in the model.
    This approach starts with identifying an instrument that is correlated with the
    treatment variable and uncorrelated with the outcome variable, except through
    its effect on the treatment variable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The directions, from a feature to the outcome, don’t necessarily mean causality.
    But *Bayesian* networks can be used for estimating the causal effects of variables
    on the outcome while controlling the confounding variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B16369_16.xhtml#_idTextAnchor429) – Security and Privacy in
    Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Advanced Encryption Standard** (**AES**): AES is one of the strongest encryption
    algorithms that protects data. AES accepts different key sizes: 128, 192, or 256
    bits.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Triple Data Encryption Standard** (**DES**): Triple DES is an encryption
    method that uses a 56-bit key to encrypt data blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Blowfish**: Blowfish is a symmetric-key encryption technique used as an alternative
    to the DES encryption algorithm. Blowfish is fast and highly effective for data
    encryption. It splits data, for example, strings and messages, into blocks of
    64 bits and encrypts them individually.'
  prefs: []
  type: TYPE_NORMAL
- en: We can use a model for inference on encrypted data without the need for decryption.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The objective of **differential privacy** (**DP**) is to ensure that the removal
    or addition of individual data points does not affect the outcome of the modeling.
    For example, by adding random noise to a normal distribution, it tries to make
    the features of individual data points obscure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The challenge of using FL or DP in practice goes beyond programming or infrastructure
    design. In spite of this great alternative to storing user data locally, there
    are still ethical, legal, and business challenges when benefitting from FL in
    different applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B16369_17.xhtml#_idTextAnchor447) – Human-in-the-Loop Machine
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No. For example, you can bring human experts into the loop through *active learning*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **uncertainty sampling**, data points get selected solely based on uncertainty
    in inference. But in **density-weighted uncertainty sampling**, instances get
    selected not only based on their highest uncertainty but also to be representative
    of the many other data points that rely on the density of data in the feature
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
