- en: Virtualizing Big Data on vSphere
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn to leverage shared storage in modern big data
    platforms. We'll evaluate current in-memory big data apps on vSphere virtualization
    platform. The in-memory feature of these platforms makes them less dependent on
    I/O and storage protocols. We will go through with administrator productivity
    and his control while creation of a Hadoop cluster and show the use of a Hadoop
    management tool for installation of software onto virtual machines. Further, we
    will learn about the ability to scale in and out, such that any workloads on the
    platform can expand to utilize all available cluster resources by pooling of resources
    to be shared by multiple virtual Hadoop clusters, resulting in higher average
    resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Big data infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download VMware vSphere Big Data Extensions 2.3.2 from [https://my.vmware.com/web/vmware/details?downloadGroup=BDE_232&productId=676&rPId=28154](https://my.vmware.com/web/vmware/details?downloadGroup=BDE_232&productId=676&rPId=28154).
  prefs: []
  type: TYPE_NORMAL
- en: Big data infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A cloud implementation always has a service catalog with all the services that
    are available for consumption. It also has service-design, catalog-management,
    and knowledge-management systems. These services will provide an organization
    with the ability to accelerate the operations and build an agile cloud services
    framework. We have to define some roles, responsibilities, and functionalities
    to manage the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service owner**: Responsible for the value of a service and managing the
    service backlog'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service backlog manager**: Responsible for defining the service''s priority
    with all backlogs, including functional, non-functional, and technical requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service release manager**: Responsible for planning, scheduling and controlling
    the builds, tests, and releases by delivering new features as well as taking care
    of existing services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop as a service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'VMware vSphere **Big Data Extensions** (**BDE**) is the platform that runs
    scale-out clustered Hadoop applications. It provides the agility to change the
    configuration through a single console and the flexibility to scale up and out
    for both compute and storage resources with better reliability and security on
    the vSphere platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ceab812f-bdaa-4231-84d1-23f5e3bd2b6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We categorize the Hadoop journey in three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stage 1**: This is what we call the piloting stage; typically the size of
    the cluster we see is under 20 nodes. In this stage, the customer is dabbling
    with Hadoop, trying to understand the value of Hadoop, and in many cases, prove
    the value of Hadoop in providing new business insights. Usually, the journey starts
    with a line of business, where someone wants to apply Hadoop on one or two use
    cases, often on data the enterprise is collecting but not doing much with. For
    example, one of the oil and gas companies we spoke with was collecting all these
    sensor data from oil wells and drill platforms. With Hadoop, they were to do some
    interesting analyses and get some very interesting insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stage 2**: Once the initial value is proven for Hadoop on big data, enterprises
    typically move into codifying these use cases and running them regularly as a
    production workload. One common phenomenon we see at this stage is that, as people
    hear about this production Hadoop cluster, they want to leverage it to explore
    their data; more and more jobs are added to the cluster, and the cluster starts
    to expand and grow. Another common thing we see is that it''s not just about core
    Hadoop components of MapReduce and **Hadoop Distributed File System** (**HDFS**).
    The other non-core Hadoop components of Hive, Pig, HBase, and others are often
    added to the cluster. Typically, we see the production cluster ranging from dozens
    of nodes to hundreds of nodes, and it may grow very rapidly. Here, there are typically
    dedicated Hadoop administrators to ensure the health of the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stage 3**: At this stage, customers are using Hadoop extensively throughout
    their organization, and have built mission-critical business workflows around
    it. For example, for an e-commerce retailer, the recommendation engine is now
    a critical part of their business, and Hadoop is a critical part of the workflow.
    Often, at this stage, we see enterprises expanding beyond Hadoop, and adding other
    big data technology and services into the mix. Often, **massively-parallel processing**
    (**MPP**) databases, NoSQL databases, and more non-core Hadoop components are
    part of the big data production system. In terms of Hadoop nodes, typically we
    see hundreds to thousands of nodes. On the extreme end, companies such as Yahoo
    and Facebook have several thousands of nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the BDE appliance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VMware enables you to easily and efficiently deploy and use Hadoop on existing
    virtual infrastructure through vSphere BDE. BDE makes Hadoop virtualization-aware,
    improves performance in virtual environments, and enables the deployment of highly-available
    Hadoop clusters in minutes. vSphere BDE automates the deployment of a Hadoop cluster,
    and thus provides better Hadoop manageability and usability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started with these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select File in the VMware vSphere Client and go to Deploy VMware-BigDataExtensions-x.x_OVF10.ova.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Select the source location dialog box, click the Local file radio button,
    click Browse..., browse to the location of the identity appliance, click Open,
    and then click Next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1002aa18-4bad-4c78-a098-3777c38cabeb.png)'
  prefs: []
  type: TYPE_IMG
- en: In the Review details dialog box, review the summary details and click Next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Accept EULAs dialog box, accept the license agreement by clicking the
    Accept button and then click Next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Select name and folder dialog box, enter a unique name for the virtual
    appliance in the Name text box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the folder or data center location where we have to deploy virtual appliances
    and then click Next. For QA deployment, East FP | Pumulus | QA folder is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Select a resource dialog box, select the cluster where you want to deploy
    the virtual appliance and click Next. For QA deployment, the ECHADMIN01 cluster
    is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the required resource pool in the Resource Pool dialog box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the QumulusQA VMs resource group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Select storage dialog box, select the disk format that you want to use
    for the virtual appliance from the Select virtual disk format drop-down list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the datastore you wish to place the virtual appliance on by clicking
    on it in the list. Click Next. For QA deployment, the ECHADMIN01-DEV and QA-VMX
    datastore cluster is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Disk Format dialog box, select Thin Provision and click Next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Setup networks dialog box, select the network that you want to connect
    the virtual appliance to using the Destination drop-down list and then click Next.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For QA deployment, xx.xxx.0.0/22 is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Ready to complete dialog box, select the Power on after deployment check
    box and click Finish.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring the VMware BDE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will deploy the vApp, power on, and then browse the console of the management
    server. There are four Hadoop clusters configured in this vSphere environment.
    The columnar view on the right indicates each cluster's name, status, which Hadoop
    distribution is running, the resource pool it belongs to, and the list of nodes.
    Resource pools manage how Hadoop consumes the underlying physical resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps to configure BDE on vSphere are given below :'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in as Serengeti and change the Serengeti user password with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the management console and SSH with the serengeti user. Configure the
    YUM repository by running the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If we can''t connect using `wget`, download `.rpm` and then winscp ( open source
    tool ) them over. To create the repo, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The BDE plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will get to the BDE plugin by clicking the Home icon and then choosing Big
    Data Extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a web browser and navigate to `https://xx.xxx.x.xx:8443/register-plugin`.
    Remember that the IP address will be user-specific.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the Install radial button, fill out the form with vCenter information,
    and click Submit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/257e099c-f4b7-4fa5-88ba-74afd6d25265.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Big Data Extensions in the vSphere Web Client, then click on the Connect
    Server… hyperlink in the Summary tab and navigate through the inventory tree to
    find the management server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c4d6d31-8975-4da9-92f4-015bdc74c8d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Click on OK to accept the certificate. The server is now connected in the Summary
    tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To set up the Hadoop YUM repository, SSH into the YUM repo server as the root
    user. Type the commands shown in the VMware KB article ([https://kb.vmware.com/s/article/2091054](https://kb.vmware.com/s/article/2091054)) to
    configure **Hortonworks Data Platform** (**HDP**) 2 YUM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Browse to the new repo at `http://puppet2.qvcdev.qvc.net/hdp/2/`. We will utilize
    an existing YUM repo server for this environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring distributions on BDE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now log in through SSH into the Serengeti Management Server with the
    Serengeti user account:'
  prefs: []
  type: TYPE_NORMAL
- en: Use PuTTY to SSH to the management server, then double-click the PuTTY icon
    on the desktop
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the SerengetiCLI session and then click Open
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `qumulushdp` distro is added into `/opt/ erengeti/www/distros/manifest`
    successfully.
  prefs: []
  type: TYPE_NORMAL
- en: The old manifest is backed up to `/opt/ serengeti/www/distros/manifest.bak`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Log into the vCenter Web Client and click on Big Data Extension from the tree
    on the left
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Big Data Clusters and then click the icon to add a new cluster (a green
    +)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can see the name of the new HDP distro now in the Hadoop distribution. Keep
    in mind that the name will match the parameter specified when we ran `./config-distro.rb`
    (`pumulushdp`).
  prefs: []
  type: TYPE_NORMAL
- en: The Hadoop plugin in vRO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now see how vRO integrates the BDE plugin and runs the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log into the vRO configuration page at `https://xx.xxx.x.xx:8283/config_general/General.action`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/41961ed7-a408-4250-a7e4-d932ad0da7b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the Plug-ins tab on the left:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/39a36f0d-fcd9-413e-8fbb-f71197e8c5e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scroll toward the bottom and click the magnifying glass. Look for the magnifying
    glass and select the required plugin file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/fd9733ed-f90c-4b6a-a16d-2eec203608ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Click the Upload and install button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept the license agreement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the VMware vRealize Orchestrator console through which we can manage
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c54b84cf-5511-4560-8b72-cd52da709eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: Click Startup Options to restart the vRO service and restart the vRO configuration
    server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log into the vRO client, and, under Run, select Workflows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under Library, you should see Hadoop Cluster As A Service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expand Hadoop Cluster As A Service and then expand Configuration. Right-click
    Configure The Serengeti Host and click Start Workflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/eb006753-f8a1-48e3-a100-9e160e1198ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Type in the URL for the Serengeti Management Server as `https://xx.xxx.x.xx:8443/serengeti`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the username for an administrative user on vCenter in UPN format (for
    example, `user@domain.com`). Enter the password for the administrative user and
    click Submit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Connection url for a Serengeti host: For example, `` `https://10103.3.18:8443/serengeti`
    ``'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'vCenter Server username: For example, `vrasvcqa@qvcdev.qvc.net`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2b1f469a-2c88-4416-b023-306587c51eeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We get a question about importing the certificate. On the last page of the
    form, select Install certificate… from the dropdown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ca86e5e8-7f67-4f63-be1a-28aab0a7db43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click Next and then click Submit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6ac7d6cc-7075-4a14-a7f4-c33323bdefdc.png)'
  prefs: []
  type: TYPE_IMG
- en: The Serengeti host is now fully configured.
  prefs: []
  type: TYPE_NORMAL
- en: We can provision the cluster using VRO as the workflow "Configure The Serengeti
    Host " has rest host connection and operation timeout value hardcoded as 30.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Workflow creation settings; the user can
    create different workflows as per their requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b3761d3-385c-41f4-a371-ca37d145ef63.png)'
  prefs: []
  type: TYPE_IMG
- en: We have to select the network or datastore resource option in BDE Cluster Blueprint
    in vRA. There should be a drop-down option to select a certain BDE Resource on
    the Web Client side. This needs to be customized on the vRA blueprint forms. Configure
    the Serengeti host to add a timeout value for the connection and operation. We
    also have an option to select BDE cluster sizes (Small, Medium, Large) from the
    vSphere web console. This needs to be customized on the blueprint side from vRA
    Blueprint.
  prefs: []
  type: TYPE_NORMAL
- en: Open source software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Organizations need great skill sets to accept open source compared to traditional
    proprietary solutions as there's a big difference between building a solution
    from scratch with all integrated support and utilizing tried-and-tested vendor
    solutions. For many enterprises, these challenges are too daunting, and erode
    the value of the open source choice for them. Business strategies, investments,
    and many other factors come into play. In these situations, enterprises find that
    a commercially-supported distribution of open source solutions, or a proprietary
    solution, better supports their strategy. Customers build digital and online selling
    channels as a pillar of their go to market strategy, develop their own proprietary
    implementation of OpenStack aligned to the unique demands of their business use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Customers have invested in the time, talent, and resources to refine OpenStack
    to meet their specific needs. A major sports retailer had chosen open source based
    solution rather then to implement a commercial distribution of OpenStack .VMware
    Integrated OpenStack help customers to save their time and resources, investing
    their technical talent in refining the outbound customer-facing portions of their
    strategy. Open source is undeniably a strategic part of every company''s software
    portfolio today. While open source software has its strong suits, being production-ready
    is not one of its top attributes. There''s still a lot of work to do: getting
    that code to meet the standards of a commercial, sold product is not an insignificant
    investment, and requires specialized skills.'
  prefs: []
  type: TYPE_NORMAL
- en: From selection to testing, integration, and security, some assembly is required.
    For most enterprises, that's not an investment they want to make; they are better
    served investing in their core competence, not in becoming an expert in a single
    implementation of an open source project. That's where commercial providers, such
    as VMware, step in to provide the pragmatic, practical open-source-based software
    that enterprises can rely on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Open vSwitch** (**OVS**) is another example of VMware''s contributions. The
    code was transferred to the Linux Foundation collaborative projects for ongoing
    community support with VMware, and continues to play an active role as VMware
    engineers are responsible for as much as 70% of the active commits to OVS. These
    contributions are considered personal, and community support across the industry
    continues to grow. VMware is making strategic investments in the IoT space with
    EdgeX and **n****etwork functions virtualization** (**NFV**) with expertise in
    the **o****pen network automation platform** (**ONAP**).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clarity** is an excellent example of creating software internally and choosing
    to open source it to benefit a broader community. Clarity is a UX/UI design framework
    as it helps both developers and designers with the visual aspects of applications.
    Clarity was developed internally in VMware to meet the UI/UX needs of products
    but it''s not contingent upon VMware products to work or to deliver value. It
    can be applied and used in nearly any environment, so the choice was made to open
    source it. Clarity has taken off as it has an active community, has been downloaded
    more than 100,000 times, and has nearly 1,000,000 views on its homepage. Our open
    source projects also include tools and kits that help a developer to be more efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenge handshake authentication protocol (CHAP)  is a tool that analyzes
    un-instrumented ELF core files for leaks, memory growth, and corruption.
  prefs: []
  type: TYPE_NORMAL
- en: 'VMware products are based in open source, which we support and contribute to
    but we are not an open source software company. VMware software, whether propriety
    or based on open source, is production-ready: it is fully supported, fully tested,
    and optimized—it''s secure and ready to deploy.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering solutions with CapEx and OpEx
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can see with open source solution that CapEx cost reduces as license costs
    potentially diminish, while OpEx cost rises as
  prefs: []
  type: TYPE_NORMAL
- en: the support and skilled technical manpower required to deploy and maintain the
    open source solution. We see CapEx rise in popularity, which reflects the license
    and support contract costs for a **commercial off-the-shelf** (**COTS**) software
    solution, while OpEx falls as the burden for patching, upgrading, enhancing, and
    securing the software rests with the vendor, not the enterprise IT department.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not a 1:1 tradeoff, but something you must consider across your enterprise
    and it''s not a short-term decision; it has long-term, structural, and strategic
    implications. If you are struggling to hire or retain staff, converting to an
    open source solution where you are relying on your intellectual property and technical
    prowess to make the solution work could put you in a very precarious situation.
    You may be held captive by consultants or outsourcing companies that promise to
    *hold your hand* as you approach production operations. Those costs rarely fall
    over time. Another option to consider is a hybrid solution: commercially-supported
    open source distributions or commercialized versions of open source projects.
    Another option to explore is the two-tiered option: some companies offer a *community
    version*, which is their open source project offered at no cost, and offer a second
    version, typically labeled *enterprise,* which is a sold product that offers a
    more robust version of the software and is fully supported. We can go with open
    source to build our strategy and make the right decisions for business. Therefore,
    to start with the basics, we must know where and how our application developers
    or IT staff are leveraging open source and understand the decisions behind their
    choice, including the benefits as well as the gaps.'
  prefs: []
  type: TYPE_NORMAL
- en: As our team starts to engage in open source projects, arm them with guidelines
    so that they feel confident in their contributions. We should have a single point
    of contact for questions about IP, license types and compliance, and best practices
    with a safety first option. If we want to create a new open source project or
    engage at a deeper level in an existing project, be sure to understand the strategic
    intent. This is a long-term commitment and requires an investment in talent and
    time, or our efforts will flounder as they're time-consuming, distracting, cost
    money, and can be annoying. We have to evaluate the choice between an open source
    solution and a proprietary, vendor-backed and -sold solution, as it is a strategic
    choice, not just a purchasing decision. We need to weigh the pros and cons of
    CapEx and OpEx, and carefully evaluate our long-term commitment and ability to
    hire staff. We can discuss to understand the costs and benefits along with the
    technology curve.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of virtualizing Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The benefits of virtualizing Hadoop are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**On-demand provisioning**: Automate the cluster-deployment process as per
    the defined policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous availability**: vSphere''s built-in HA protection protects the
    single point of failure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Resources (CPU, memory, network, and storage) can be scaled
    up and down on demand as per your requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-tenant environment**: Different tenants running Hadoop can be isolated
    within a shared infrastructure as per security compliance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case – security and configuration isolation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Hadoop authentication and authorization model is weak. Sensitive data is
    hard to protect. It has multiple MapReduce workloads for production batch analysis,
    ad hoc analysis, and experiment tasks with different SLAs for different jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to take the following into consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: Where it makes sense, HDFS is consolidated to minimize data duplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-priority jobs get more resources to ensure they are completed on time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each type of job can get as many resources as possible at any time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid CPU and memory contention so better utilize resources to get a job done
    on time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our objective is to integrate Hadoop workloads and other workloads by having
    a big data shared infrastructure. The Hadoop MapReduce framework uses HDFS as
    an underlying filesystem to process large sets of data and use their own storage
    mechanism. We also have other technologies, such as HBase and Pivotal.
  prefs: []
  type: TYPE_NORMAL
- en: Case study – automating application delivery for a major media provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer had a mandate that any application must be deployable to any number
    of backend infrastructures, spanning multiple private clouds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A specific application (with a footprint of over 10,000 servers) needed better
    provisioning procedures and tools to ensure the mandated goal was met
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer's current provisioning model required an extensive overhaul, as spin-up
    time was several weeks if not months, largely consisting of manual procedures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: Implemented an automated workflow-based approach using a version-controlled,
    software-defined infrastructure via the **Business Process Management** (**BPM**)
    platform/workflow engine and underlying Java services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraged Puppet architecture for build processes and packaging, and bakery
    workflow for images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provided insight into the operations via a Ruby-based console and reporting
    UX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Jira into the provisioning workflow will make delivery ease-of-use
    from familiar tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioned application servers and requisite number of Memcached and related
    instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new system validates a newly-provisioned infrastructure, provides an automated
    cleanup of any failures, and automatically switches routing rules to serve up
    the new infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provided customer with tools and patterns necessary for repeatable operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Created better build procedures and processes, which yield more stable infrastructure
    changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment time for the target infrastructure was decreased from weeks/months
    to 90 minutes for 270 servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop is still quite new for many enterprises, and different companies are
    at different stages in their Hadoop adoption journey. Having worked with several
    customers on this, it's evident that, depending on the stage the customer is at,
    there are distinct Hadoop use cases and requirements. Virtualization can help
    to address all the key requirements of each stages. Multiple Hadoop clusters can
    be used by different departments within a company.
  prefs: []
  type: TYPE_NORMAL
- en: It is difficult to manage multiple clusters for different departments in a company
    and keep them all running well. We have multiple use cases running data mining,
    recommendation engines, and for our online service, we have one shared dataset,
    rather than duplicated data everywhere. We are now managing a single cluster rather
    than multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to support cloud-native application development
    by providing developers with access to traditional and modern application-development
    frameworks and resources, including container services and open APIs, on a common
    vSphere platform. This enables microservice-based architectures for faster and
    frequent development without compromising security, reliability, or governance.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following resources for more information on the topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Adobe Deploys Hadoop as a Service on VMware vSphere* at [http://www.vmware.com/files/pdf/products/vsphere/VMware-vSphere-Adobe-Deploys-HAAS-CS.pdf](http://www.vmware.com/files/pdf/products/vsphere/VMware-vSphere-Adobe-Deploys-HAAS-CS.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Virtualizing Hadoop in Large-Scale Infrastructures, technical white paper
    by EMC* at [https://community.emc.com/docs/DOC-41473](https://community.emc.com/docs/DOC-41473)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Virtualized Hadoop Performance with VMware vSphere 6 on High-Performance Servers*
    at [http://www.vmware.com/resources/techresources/10452](http://www.vmware.com/resources/techresources/10452)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Benchmarking Case Study of Virtualized Hadoop Performance on vSphere* at [http://www.vmware.com/resources/techresources/10222](http://www.vmware.com/resources/techresources/10222)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transaction Processing Council—TPCx-HS Benchmark Results (Cloudera on VMware
    performance, submitted by Dell)* at [http://www.tpc.org/tpcx-hs/results/tpcxhs_results.asp](http://www.tpc.org/tpcx-hs/results/tpcxhs_results.asp)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ESG Lab Review: VCE vBlock/systems with EMC Isilon for Enterprise Hadoop* at [http://www.esg-global.com/lab-reports/esg-lab-review-vce-vblock-systems-with-emc-isilon-for-enterprise-hadoop/](http://www.esg-global.com/lab-reports/esg-lab-review-vce-vblock-systems-with-emc-isilon-for-enterprise-hadoop/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VMware BDE Documentation site: vSphere Big Data Extensions (BDE)* at [https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html](https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VMware vSphere Big Data Extensions—Administrator''s and User''s Guide and
    Command-line Interface User''s Guide* at [https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html](https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Blog articles on BDE Version 2.1* at [http://blogs.vmware.com/vsphere/2014/10/whats-new-vsphere-big-data-extensions-version-2-1.html](http://blogs.vmware.com/vsphere/2014/10/whats-new-vsphere-big-data-extensions-version-2-1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*VMware Big Data Extensions (BDE) Community Discussion* at [https://communities.vmware.com/message/2308400](https://communities.vmware.com/message/2308400)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Apache Hadoop Storage Provisioning Using VMware vSphere Big Data Extensions* at [https://www.vmware.com/files/pdf/VMware-vSphere-BDE-Storage-Provisioning.pdf](https://www.vmware.com/files/pdf/VMware-vSphere-BDE-Storage-Provisioning.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hadoop Virtualization Extensions* at [http://www.vmware.com/files/pdf/Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf](http://www.vmware.com/files/pdf/Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Container Orchestration on vSphere with Big Data Extensions* at [https://labs.vmware.com/flings/big-data-extensions-for-vsphere-standard-edition](https://labs.vmware.com/flings/big-data-extensions-for-vsphere-standard-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
