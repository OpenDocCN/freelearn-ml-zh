<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Association Rules based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Association Rules based learning</h1></div></div></div><p>We have covered Decision tree, instance and kernel-based supervised and unsupervised learning methods in the previous chapters. We also explored the most commonly used algorithms across these learning algorithms in the previous chapters. In this chapter, we will cover association rule based learning and, in specific, Apriori and FP-Growth algorithms among others. We will learn the basics of this technique and get hands-on implementation guidance using Apache Mahout, R, Julia, Apache Spark, and Python. The following figure depicts different learning models covered in this book. The techniques highlighted in orange will be dealt with in detail in this chapter.</p><div class="mediaobject"><img src="graphics/B03980_07_01.jpg" alt="Association Rules based learning"/></div><p>The following topics are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the basics and core principles of association rules based learning models</li><li class="listitem" style="list-style-type: disc">Core use cases for association rule such as the Market Basket problem</li><li class="listitem" style="list-style-type: disc">Key terms such as itemsets, lift, support, confidence and frequent itemsets, and rule generation techniques</li><li class="listitem" style="list-style-type: disc">A deep dive into association rule based algorithms such as Apriori and FP-Growth; comparing and contrasting Apriori and FP-Growth in the context of large datasets</li><li class="listitem" style="list-style-type: disc">Overview and purpose of some advanced association rules concepts such as correlation and sequential rules</li><li class="listitem" style="list-style-type: disc">A sample implementation for Apache Mahout, R, Apache Spark, Julia and Python (scikit-learn) libraries and modules.</li></ul></div><div class="section" title="Association rules based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec37"/>Association rules based learning</h1></div></div></div><p>Association rule-based Machine learning deals with finding frequent patterns, associations, and transactions that can be used for classification and prediction requirements. The association rule based learning process is as follows: given a set of transactions, finding rules and using these rules to predict the occurrence of an item based on the occurrences of other items in the transaction is Association rule based learning. The following diagram represents the scope of Machine learning:</p><div class="mediaobject"><img src="graphics/B03980_07_02.jpg" alt="Association rules based learning"/></div><div class="section" title="Association rule – a definition"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec84"/>Association rule – a definition</h2></div></div></div><p>An association rule<a id="id823" class="indexterm"/> is a representation of a pattern that describes the probability with which an event occurs, given the occurrence of another event. Usually, the syntax for association rules follows the <span class="emphasis"><em>if...then</em></span> statements that relate two sets of unrelated data from the repository. In short, it helps find the relationship between objects that are frequently used together. The goal of association rules is to find all the sets of items that have greater support than minimum support using the large dataset to predict the rules that have confidence greater than the minimum confidence. One of the most common examples where association rule is used is the Market Basket example. To elaborate the Market basket example, if a customer buys an iPad, he or she is likely to buy an iPad case as well.</p><p>Two important criteria are used in association rules, <span class="strong"><strong>Support</strong></span> <a id="id824" class="indexterm"/>and <a id="id825" class="indexterm"/>
<span class="strong"><strong>Confidence</strong></span>. Every association rule should have a minimum Confidence and minimum Support at the same time. This is usually user-defined.</p><p>Now, let's look <a id="id826" class="indexterm"/>at what Support, Confidence, and lift measures are. Let's consider the same example as explained previously, <span class="emphasis"><em>If X then Y</em></span>. where <span class="emphasis"><em>X</em></span> is buying an iPad and <span class="emphasis"><em>Y</em></span> is buying an iPad case.</p><p>Then Support is defined as the frequency with which <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> are purchased together over the total number of purchases or transactions.</p><div class="mediaobject"><img src="graphics/B03980_07_13.jpg" alt="Association rule – a definition"/></div><p>Confidence can be defined as the frequency with which <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span> are purchased together over the frequency with which <span class="emphasis"><em>X</em></span> is purchased in isolation.</p><div class="mediaobject"><img src="graphics/B03980_07_14.jpg" alt="Association rule – a definition"/></div><p>Lift is defined as the Support over the Support for <span class="emphasis"><em>X</em></span> times the Support for <span class="emphasis"><em>Y</em></span>.</p><div class="mediaobject"><img src="graphics/B03980_07_15.jpg" alt="Association rule – a definition"/></div><div class="mediaobject"><img src="graphics/B03980_07_03.jpg" alt="Association rule – a definition"/></div><p>Before<a id="id827" class="indexterm"/> understanding the significance of these measures, let's look at the terms used in this context as an example. A collection of items in a warehouse called itemset are represented as <span class="emphasis"><em>I = { i<sub>1</sub>, i<sub>2</sub>, …. i<sub>n</sub>}</em></span>, a set of all transactions where each transaction consists of a subset of itemset is represented as <span class="emphasis"><em>T = { t<sub>1</sub>, t<sub>2</sub>, …. t<sub>n</sub>}</em></span>, where <span class="emphasis"><em>t</em></span><sub>x</sub> is a subset of <span class="emphasis"><em>I</em></span> with a <a id="id828" class="indexterm"/>
<span class="strong"><strong>Unique Transaction Identifier</strong></span> (<span class="strong"><strong>UTI</strong></span>).</p><p>Let's represent items, transactions, and measures using an example now.</p><p>Consider five items and five transactions as depicted here:</p><p>
<span class="emphasis"><em>I = {iPad(A), iPad case(B), iPad scratch guard(C), Apple care (D), iPhone (E)}</em></span>
</p><div class="mediaobject"><img src="graphics/B03980_07_04.jpg" alt="Association rule – a definition"/></div><p>
<span class="emphasis"><em>T = {{ iPad, iPad case, iPad scratch guard }, { iPad, iPad scratch guard, Apple care }, { iPad case, iPad scratch guard, Apple care }, { iPad, Apple care, iPhone }, { iPad case, iPad scratch guard, iPhone }}</em></span>
</p><p>The table below <a id="id829" class="indexterm"/>shows the support, confidence and lift values for each of the identified rules.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>#</p>
</th><th style="text-align: left" valign="bottom">
<p>Rule</p>
</th><th style="text-align: left" valign="bottom">
<p>Support</p>
</th><th style="text-align: left" valign="bottom">
<p>Confidence</p>
</th><th style="text-align: left" valign="bottom">
<p>Lift</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>If iPad (<span class="emphasis"><em>A</em></span>) is purchased, iPhone (<span class="emphasis"><em>D</em></span>) is also purchased</p>
</td><td style="text-align: left" valign="top">
<p>2/5</p>
</td><td style="text-align: left" valign="top">
<p>2/3</p>
</td><td style="text-align: left" valign="top">
<p>10/9</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>If iPad scratch guard(<span class="emphasis"><em>C</em></span>) is purchased, iPad (<span class="emphasis"><em>A</em></span>) is also purchased</p>
</td><td style="text-align: left" valign="top">
<p>2/5</p>
</td><td style="text-align: left" valign="top">
<p>2/4</p>
</td><td style="text-align: left" valign="top">
<p>5/6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>3</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>If iPad (<span class="emphasis"><em>A</em></span>) is purchased, iPad scratch guard (<span class="emphasis"><em>C</em></span>) is also purchased</p>
</td><td style="text-align: left" valign="top">
<p>2/5</p>
</td><td style="text-align: left" valign="top">
<p>2/3</p>
</td><td style="text-align: left" valign="top">
<p>5/6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>4</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>If iPad case(<span class="emphasis"><em>B</em></span>) and iPad scratch guard (<span class="emphasis"><em>C</em></span>) are purchased, then apple care (<span class="emphasis"><em>D</em></span>) is also purchased</p>
</td><td style="text-align: left" valign="top">
<p>1/5</p>
</td><td style="text-align: left" valign="top">
<p>1/3</p>
</td><td style="text-align: left" valign="top">
<p>5/9</p>
</td></tr></tbody></table></div><p>From these itemsets, based on the support and confidence computations, frequent itemset(s) can be determined. The goal of association rule mining is to find the rules that satisfy the criteria given here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">support ≥ minsup (minimum support) threshold</li><li class="listitem" style="list-style-type: disc">confidence ≥ minconf (minimum confidence) threshold</li></ul></div><p>The following are the steps involved in frequent itemset generation and mining association rules:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">List all the possible association rules.</li><li class="listitem">Compute the support and confidence for each rule.</li><li class="listitem">Prune the rules that fail to satisfy the minsup and minconf threshold values.</li></ol></div><p>This approach is called the brute force approach and is usually known to be computationally prohibitive.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip05"/>Tip</h3><p>Rules originating from the same itemset usually have the same support, but vary with confidence. The minimum support (minsup) and the minimum confidence (minconf) are the values that are agreed upon during the problem definition statement. For example, minimum support and confidence can take percentage values like 75% and 85% respectively.</p></div></div><p>To avoid all <a id="id830" class="indexterm"/>expensive computations, we can simplify this process into two steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Frequent itemset generation</strong></span>: This requires generating all the itemsets with support ≥ minsup</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rule generation</strong></span>: From the identified frequent itemsets, generate rules with the highest confidence</li></ul></div><p>When there are five items, there are 32 candidate itemsets. The following figure depicts the itemset combination for five items: <span class="strong"><strong>A</strong></span>, <span class="strong"><strong>B</strong></span>, <span class="strong"><strong>C</strong></span>, <span class="strong"><strong>D</strong></span>, and <span class="strong"><strong>E</strong></span>:</p><div class="mediaobject"><img src="graphics/B03980_07_05.jpg" alt="Association rule – a definition"/></div><p>The possible <a id="id831" class="indexterm"/>number of itemsets and rules, given the number of items is defined here:</p><p>Given <span class="emphasis"><em>d</em></span> unique items:</p><p>
<span class="emphasis"><em>Total number of possible itemsets = 2</em></span><sup>d</sup></p><p>The standard formula for computing total possible association rules is defined here:</p><div class="mediaobject"><img src="graphics/B03980_07_16.jpg" alt="Association rule – a definition"/></div><p>For example, if <span class="emphasis"><em>d</em></span> is equivalent to 6, then the <span class="emphasis"><em>total number of possible itemsets = 2</em></span><sup>d</sup><span class="emphasis"><em> = 64</em></span>
</p><p>Thus, the <span class="emphasis"><em>total number of possible association rules = 602 rules</em></span>
</p><p>The following graph <a id="id832" class="indexterm"/>shows the relationship between the number of items and possible association rules.</p><div class="mediaobject"><img src="graphics/B03980_07_06.jpg" alt="Association rule – a definition"/></div><p>Efficient ways of generating frequent itemsets and association rules determine the efficiency of the association rule algorithms. In the next sections, we will cover the Apriori and FP-Growth algorithms in detail.</p></div><div class="section" title="Apriori algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec85"/>Apriori algorithm</h2></div></div></div><p>In this section, we will cover the<a id="id833" class="indexterm"/> Apriori algorithm step-by-step using an example. The Apriori algorithm is as stated here:</p><div class="mediaobject"><img src="graphics/B03980_07_07.jpg" alt="Apriori algorithm"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>Apriori principle—for all the frequent itemsets, the subsets must also be frequent.</p></div></div><p>Consider the five items (from the example in the previous section)</p><p>
<span class="emphasis"><em>I = {iPad(A), iPad case(B), iPad scratch guard(C), Apple care (D), iPhone (E)}</em></span>, and the following nine<a id="id834" class="indexterm"/> transactions. Let's assume that the minimum Support count is two:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>TID</p>
</th><th style="text-align: left" valign="bottom">
<p>The purpose or meaning in the context of Machine learning</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), and iPhone(<span class="emphasis"><em>E</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>) and Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>3</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>) and iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>4</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), and Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>5</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>) and Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>6</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>) and iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>7</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>) and Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>8</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>), and iPhone (<span class="emphasis"><em>E</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>9</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), and iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td></tr></tbody></table></div><p>Let's debug the<a id="id835" class="indexterm"/> previous algorithm using the previous datasets:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get the number of occurrences for each item from the previous transactions (<span class="emphasis"><em>C</em></span><sub>1</sub>):<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support count</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr></tbody></table></div><p>Determine Frequent 1—Itemsets (<span class="emphasis"><em>L</em></span><sub>1</sub>) from <span class="emphasis"><em>C</em></span><sub>1</sub>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support count</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr></tbody></table></div></li><li class="listitem">Generate 2—Itemset <a id="id836" class="indexterm"/>candidates (<span class="emphasis"><em>C</em></span><sub>2</sub>) and scan the dataset for Support count:<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support count</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad case(B)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B), iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B), Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B), iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad scratch guard(C), Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad scratch guard(C), iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{Apple care(D), iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div></li><li class="listitem">Determine <a id="id837" class="indexterm"/>Frequent 2—Itemsets (<span class="emphasis"><em>L</em></span><sub>2</sub>) from <span class="emphasis"><em>C</em></span><sub>2</sub>:<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support count</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad case(B)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B), iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B), Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B), iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr></tbody></table></div></li><li class="listitem">Generate 3—Itemset candidates (<span class="emphasis"><em>C</em></span><sub>3</sub>).</li><li class="listitem">Finally, scan the dataset for Support count and frequent 3—Itemset identification.</li></ol></div><p>This is similar to the <a id="id838" class="indexterm"/>previously followed steps, but we will demonstrate how pruning can be applied to identify the frequent itemset, based on the Apriori principle effectively. First, we identify the possible subset itemsets. We then check whether there are any of the subset itemsets that do not belong to the frequent itemset list. If not found, we eliminate that 3—Itemset possibility.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>C3</p>
</th><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th colspan="3" style="text-align: center" valign="bottom">
<p>Possible subset itemsets</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,B,C}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,B}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em> {A,C}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em> {B,C}</em></span>✓</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,B,D}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,B}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,D}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,D}</em></span>✓</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>3</strong></span>✕</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,C,D}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,C}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{A,D}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{C,D}</em></span>✕</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>4</strong></span>✕</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,C,D}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,C}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,D}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{C,D}</em></span>✕</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>5</strong></span>✕</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,C,E}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,C}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,E}</em></span>✕</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{C,E}</em></span>✕</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>6</strong></span>✕</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,D,E}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,D}</em></span>✓</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{B,E}</em></span>✕</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{D,E}</em></span>✕</p>
</td></tr></tbody></table></div><p>In the previous table, the ✕ itemsets are pruned using the Apriori technique, and the data from step 4 (<span class="emphasis"><em>L</em></span><sub>2</sub>) is used. The itemsets are represented using the item codes <span class="emphasis"><em>A</em></span>, <span class="emphasis"><em>B</em></span>, <span class="emphasis"><em>C</em></span>, <span class="emphasis"><em>D</em></span>, and <span class="emphasis"><em>E</em></span> instead of the actual names for ease of understanding. The 3—itemset candidates can be identified as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>C<sub>3</sub></p>
</th><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support Count</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad case (B), iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad case (B), Apple care(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr></tbody></table></div><p>Thus, the Frequent 3—Itemsets are:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>L<sub>3</sub></p>
</th><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support Count </p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad case (B), iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A), iPad case (B), Apple care(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr></tbody></table></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Generate 4— Itemset candidates (<span class="emphasis"><em>C</em></span><sub>4</sub>).</li><li class="listitem">Finally, scan the dataset for the Support count and frequent 3—Itemset identification (<span class="emphasis"><em>L</em></span><sub>4</sub>).</li></ol></div><p>As we can see, the <a id="id839" class="indexterm"/>pruning stops here, as there are no further <span class="emphasis"><em>C</em></span><sub>3</sub> options available.</p><p>The Apriori algorithm is not efficient as it requires multiple dataset scans. However, there are some techniques to improve the efficiency. Some of them are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If a transaction does not contain any frequent item-sets, it is not useful and need not participate in the subsequent scans</li><li class="listitem" style="list-style-type: disc">Any itemset that is frequent in the dataset should be frequent in at least one partition of the dataset</li><li class="listitem" style="list-style-type: disc">Application of sampling, to include a subset of the whole data set with a lower support threshold, will yield more efficiency</li></ul></div><div class="section" title="Rule generation strategy"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec92"/>Rule generation strategy</h3></div></div></div><p>Let's say we have a <a id="id840" class="indexterm"/>frequent itemset <span class="emphasis"><em>{A, B, C, D}</em></span>, and the possible candidate rules are:</p><p>ABC→D</p><p>ABD→C</p><p>ACD→B</p><p>BCD→A</p><p>AB→CD</p><p>AC→BD</p><p>AD→BC</p><p>BC→AD</p><p>BD→AC</p><p>CD→AB</p><p>A→BCD</p><p>B→ACD</p><p>C→ABD</p><p>D→ABC</p><p>The standard formula is, for every k items in the frequent itemset, <span class="emphasis"><em>2k-2</em></span> possible candidate rules can be defined. Only the rules with high confidence can be retained. The following figure depicts marking the low confidence rules and knocking them off:</p><div class="mediaobject"><img src="graphics/B03980_07_08.jpg" alt="Rule generation strategy"/></div><div class="section" title="Rules for defining appropriate minsup"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec25"/>Rules for defining appropriate minsup</h4></div></div></div><p>Some important guidelines to be <a id="id841" class="indexterm"/>followed for defining the minsup threshold for the association rule based mining are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Too high minsup: This will lead to missing itemsets with rare items</li><li class="listitem" style="list-style-type: disc">Too low minsup: This will result in computational expense as more scans will be needed</li></ul></div></div><div class="section" title="Apriori – the downside"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec26"/>Apriori – the downside</h4></div></div></div><p>It is now clear that in <a id="id842" class="indexterm"/>Apriori algorithm, for every <span class="emphasis"><em>k</em></span> itemsets we will need to use <span class="emphasis"><em>(k-1)</em></span> frequent itemsets and when the database scans are done, the pattern matching approach is used. The primary bottlenecks are two huge candidate sets and multiple database scans. Let's see an example—if there are 10<sup>4</sup> frequent 1-itemsets, then this will result in 10<sup>7</sup> candidate 2-itemsets. And for every <span class="emphasis"><em>n</em></span> itemsets, the longest pattern length, <span class="emphasis"><em>n + 1</em></span> scans are required.</p><p>The solution for this would be to avoid the candidate itemset generation completely, and one way of solving this is to compress a large dataset or database into a compact <span class="strong"><strong>frequent pattern tree</strong></span> (<span class="strong"><strong>FP-tree</strong></span>)<a id="id843" class="indexterm"/> that <a id="id844" class="indexterm"/>will avoid expensive scans.</p><p>There are several ways of optimizing the Apriori implementation and here are some of the important ones:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Approach 1</strong></span>—<span class="strong"><strong>Has-based itemset counting</strong></span>: There is a threshold value set for every k itemset <a id="id845" class="indexterm"/>bucket, and if the count of the itemset for that itemset is lower than the threshold, this bucket will not be processed. This in-turn reduces the itemset buckets that are to be considered for processing, thus improving the efficiency.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Approach 2</strong></span>—<span class="strong"><strong>Transaction elimination / counting</strong></span>: In case a transaction does not contain the <a id="id846" class="indexterm"/>target k itemset, this transaction does not add value or make sense for being processed. So, this approach is about identifying these transactions and eliminating them from being processed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Approach 3</strong></span>—<span class="strong"><strong>Partitioning</strong></span>: Any itemset that is potentially frequent in the dataset will need to be<a id="id847" class="indexterm"/> frequent in the partitions of the dataset as well; in the absence of which, the itemset could potentially be excluded from being processed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Approach 4</strong></span>—<span class="strong"><strong>Sampling</strong></span>: This is a simpler way to consider a sample or a subset of the bigger <a id="id848" class="indexterm"/>universe of data and run the mining process. This would reduce the k, and thus the frequent k-itemsets.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Approach 5</strong></span>—<span class="strong"><strong>Dynamic itemset counting</strong></span>: This is one of the most effective methods, and involves <a id="id849" class="indexterm"/>including a new itemset only if it is frequent in all its subset itemsets.</li></ul></div><p>Although, there are optimization techniques for Apriori; it poses inefficiency as a result of expensive scans that are inherent, which will need to be addressed. This brings us to the next algorithm of association rule based learning, the <a id="id850" class="indexterm"/>
<span class="strong"><strong>FP-growth</strong></span> algorithm.</p></div></div></div><div class="section" title="FP-growth algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec86"/>FP-growth algorithm</h2></div></div></div><p>The FP-growth algorithm<a id="id851" class="indexterm"/> is an efficient and scalable alternative to mining frequent patterns, and thus association rule mining. It addresses most of the performance bottlenecks that an Apriori algorithm would undergo. It allows frequent itemset generation without having to actually generate the candidate itemsets. This algorithm has two steps primarily:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Building a compact data structure from the database called FP-tree</li><li class="listitem" style="list-style-type: disc">Extracting frequent itemsets directly from the FP-tree</li></ul></div><p>Let's consider the same example we used in the Apriori algorithm. There is a total of five items (from the example in the previous section):</p><p>
<span class="emphasis"><em>I is {iPad(A), iPad case(B), iPad scratch guard(C), Apple care (D), iPhone (E)}</em></span>, and the following nine transactions. Let's assume that the minimum support count is two:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>TID</p>
</th><th style="text-align: left" valign="bottom">
<p>Transaction Itemsets</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad(A), iPad case(B), and iPhone(E)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad case(B), Apple care(D)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>3</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad case(B), iPad scratch guard(C)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>4</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad(A), iPad case(B), and Apple care(D)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>5</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad(A), Apple care(D)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>6</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad case(B), iPad scratch guard(C)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>7</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad(A), Apple care(D)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>8</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad(A), iPad case(B), iPad scratch guard(C), and iPhone (E)</em></span>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>9</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>iPad(A), iPad case(B), and iPad scratch guard(C)</em></span>
</p>
</td></tr></tbody></table></div><p>We will now look at building an FP-tree for this database:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Identify/calculate the minimum support count. Since it needs to be 30%, the minimum support count is calculated as follows:<p>Minimum support count = 30/100 * 9 = 2.7 ~ 3</p></li><li class="listitem">Calculate the frequency of occurrence for 1-itemset. Additionally, based on the support count, add <a id="id852" class="indexterm"/>priority:<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Itemset</p>
</th><th style="text-align: left" valign="bottom">
<p>Support count</p>
</th><th style="text-align: left" valign="bottom">
<p>Priority</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad(A)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad case(B)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPad scratch guard(C)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{Apple care(D)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>{iPhone(E)}</em></span>
</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td></tr></tbody></table></div></li><li class="listitem">Order the items for each transaction as per the priority:<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>TID</p>
</th><th style="text-align: left" valign="bottom">
<p>Transaction Itemsets</p>
</th><th style="text-align: left" valign="bottom">
<p>Re-ordered Itemsets based on priority</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>1</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), and iPhone(<span class="emphasis"><em>E</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad(<span class="emphasis"><em>A</em></span>), and iPhone(<span class="emphasis"><em>E</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>2</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>3</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>4</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), and Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad(<span class="emphasis"><em>A</em></span>), and Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>5</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>6</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>7</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), Apple care(<span class="emphasis"><em>D</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>8</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>), and iPhone (<span class="emphasis"><em>E</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad(<span class="emphasis"><em>A</em></span>), iPad scratch guard(<span class="emphasis"><em>C</em></span>), and iPhone (<span class="emphasis"><em>E</em></span>)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>9</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>iPad(<span class="emphasis"><em>A</em></span>), iPad case(<span class="emphasis"><em>B</em></span>), and iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td><td style="text-align: left" valign="top">
<p>iPad case(<span class="emphasis"><em>B</em></span>), iPad(<span class="emphasis"><em>A</em></span>), and iPad scratch guard(<span class="emphasis"><em>C</em></span>)</p>
</td></tr></tbody></table></div></li><li class="listitem">Create FP-tree for <a id="id853" class="indexterm"/>transaction for <span class="strong"><strong>TID</strong></span> = <span class="strong"><strong>1</strong></span>, and the ordered itemset is iPad case(<span class="emphasis"><em>B</em></span>), iPad(<span class="emphasis"><em>A</em></span>), and iPhone(<span class="emphasis"><em>E</em></span>).<div class="mediaobject"><img src="graphics/B03980_07_09.jpg" alt="FP-growth algorithm"/></div></li><li class="listitem">Now, scan the database for <span class="strong"><strong>TID</strong></span> = <span class="strong"><strong>2</strong></span>, iPad case (<span class="emphasis"><em>B</em></span>) and Apple care(<span class="emphasis"><em>D</em></span>). The updated FP-tree will look like this:<div class="mediaobject"><img src="graphics/B03980_07_10.jpg" alt="FP-growth algorithm"/></div></li><li class="listitem">Scan all the<a id="id854" class="indexterm"/> transactions in the order of L and update the FP-tree accordingly. The final FP-tree will be as shown next. Note that every time an item is encountered again in the transaction, the count value on the node is incremented.<div class="mediaobject"><img src="graphics/B03980_07_11.jpg" alt="FP-growth algorithm"/></div></li><li class="listitem">Generate a conditional FP-tree for each of the transactions and define the conditional pattern base.</li><li class="listitem">Finally, generate<a id="id855" class="indexterm"/> the frequent patterns. The result for the given dataset is shown here:<div class="informalexample"><pre class="programlisting">E: {B, E: 2}, {A, E: 2}, {B, A, E: 2}
D: {B, D: 2}
C: {B, C: 4}, {A, C: 4}, {B, A, C: 2}
A: {B, A: 4}</pre></div></li></ol></div></div><div class="section" title="Apriori versus FP-growth"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec87"/>Apriori versus FP-growth</h2></div></div></div><p>The following <a id="id856" class="indexterm"/>graph shows the relationship <a id="id857" class="indexterm"/>between the algorithms with different minsup threshold values:</p><div class="mediaobject"><img src="graphics/B03980_07_12.jpg" alt="Apriori versus FP-growth"/><div class="caption"><p>Image source: An article by Prof. Pier Luca Lanzi</p></div></div><p>The advantages of the FP-growth algorithm are detailed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The complete information for frequent pattern mining is preserved, without breaking the pattern in a long transaction</li><li class="listitem" style="list-style-type: disc">Data is compacted by eliminating irrelevant information as infrequent itemsets are avoided upfront</li><li class="listitem" style="list-style-type: disc">The FP-growth algorithm works in a divide-and-conquer mode, where the dataset is decomposed as per the frequent itemset patterns uncovered so far. This reduces searches to the subset of datasets as against the complete database</li><li class="listitem" style="list-style-type: disc">The<a id="id858" class="indexterm"/> candidate itemsets<a id="id859" class="indexterm"/> are not generated in this case and hence, will not need to be tested</li></ul></div></div></div></div>
<div class="section" title="Implementing Apriori and FP-growth"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec38"/>Implementing Apriori and FP-growth</h1></div></div></div><p>Refer to the source code provided for this chapter for implementing the Apriori classifier (source code path <code class="literal">.../chapter7/...</code> under each of the folders for the technology.)</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec88"/>Using Mahout</h2></div></div></div><p>Refer to the code <a id="id860" class="indexterm"/>files folder <code class="literal">.../mahout/chapter7/aprioriexample/</code>.</p><p>Refer to the code<a id="id861" class="indexterm"/> files folder <code class="literal">.../mahout/chapter7/fpgrowthexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec89"/>Using R</h2></div></div></div><p>Refer to the code<a id="id862" class="indexterm"/> files folder <code class="literal">.../r/chapter7/aprioriexample/</code>.</p><p>Refer to the code<a id="id863" class="indexterm"/> files folder <code class="literal">.../r/chapter7/fpgrowthexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec90"/>Using Spark</h2></div></div></div><p>Refer to the code<a id="id864" class="indexterm"/> files folder <code class="literal">.../spark/chapter7/aprioriexample/</code>.</p><p>Refer to the<a id="id865" class="indexterm"/> code files folder <code class="literal">.../spark/chapter7/fpgrowthexample/</code>.</p></div><div class="section" title="Using Python (Scikit-learn)"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec91"/>Using Python (Scikit-learn)</h2></div></div></div><p>Refer to the code <a id="id866" class="indexterm"/>files<a id="id867" class="indexterm"/> folder <code class="literal">.../python-scikit-learn/ chapter7/aprioriexample/</code>.</p><p>Refer to the code files folder <code class="literal">.../python-scikit-learn/chapter7/fpgrowthexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec92"/>Using Julia</h2></div></div></div><p>Refer to the code <a id="id868" class="indexterm"/>files folder <code class="literal">.../julia/chapter7/aprioriexample/</code>.</p><p>Refer to the code<a id="id869" class="indexterm"/> files folder <code class="literal">.../julia/chapter7/fpgrowthexample/</code>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Summary</h1></div></div></div><p>In this chapter, you have learned the association rule based learning methods and, Apriori and FP-growth algorithms. With a common example, you learned how to do frequent pattern mining using Apriori and FP-growth algorithms with a step-by-step debugging of the algorithm. We also compared and contrasted the algorithms and their performance. We have example implementations for Apriori using Mahout, R, Python, Julia, and Spark. In the next chapter, we will cover the Bayesian methods and specifically, the Naïve-Bayes algorithm.</p></div></body></html>