- en: Clustering Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法
- en: In this chapter, we are going to introduce some fundamental clustering algorithms,
    discussing both their strengths and weaknesses. The field of unsupervised learning,
    as well as any other machine learning approach, must be always based on the concept
    of Occam's razor. Simplicity must always be preferred when performance meets the
    requirements. However, in this case, the ground truth can be unknown. When a clustering
    algorithm is adopted as an exploratory tool, we can only assume that the dataset
    represents a precise data generating process. If this assumption is correct, the
    best strategy is to determine the number of clusters to maximize the internal
    cohesion (denseness) and the external separation. This means that we expect to
    find blobs (or isles) whose samples share some common and partially unique features.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些基本的聚类算法，讨论它们的优点和缺点。无监督学习领域，以及任何其他机器学习方法，都必须始终基于奥卡姆剃刀的概念。当性能满足要求时，必须始终优先考虑简单性。然而，在这种情况下，真实情况可能是未知的。当采用聚类算法作为探索性工具时，我们只能假设数据集代表一个精确的数据生成过程。如果这个假设是正确的，最佳策略是确定簇的数量以最大化内部凝聚力（密度）和外部分离。这意味着我们期望找到具有一些共同和部分独特特征的团块（或岛屿）。
- en: 'In particular, the algorithms we are going to present are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们要介绍的一些算法是：
- en: '**k-Nearest Neighbors** (**KNN**) based on KD Trees and Ball Trees'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于KD树和球树的k-最近邻算法**（**KNN**）'
- en: K-means and K-means++
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means和K-means++
- en: Fuzzy C-means
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊C均值
- en: Spectral clustering based on the Shi-Malik algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Shi-Malik算法的谱聚类
- en: k-Nearest Neighbors
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-最近邻
- en: 'This algorithm belongs to a particular family called **instance-based** (the
    methodology is called **instance-based learning**). It differs from other approaches
    because it doesn''t work with an actual mathematical model. On the contrary, the
    inference is performed by direct comparison of new samples with existing ones
    (which are defined as instances). KNN is an approach that can be easily employed
    to solve clustering, classification, and regression problems (even if, in this
    case, we are going to consider only the first technique). The main idea behind
    the clustering algorithm is very simple. Let''s consider a data generating process
    *p[data]* and a finite a dataset drawn from this distribution:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法属于一个称为**基于实例**的特定家族（该方法称为**基于实例学习**）。它与其他方法的不同之处在于它不使用实际的数学模型。相反，推理是通过直接比较新样本与现有样本（定义为实例）来进行的。KNN是一种可以轻松应用于解决聚类、分类和回归问题的方法（即使在这种情况下，我们只考虑第一种技术）。聚类算法背后的主要思想非常简单。让我们考虑一个数据生成过程*p[data]*和从这个分布中抽取的有限数据集：
- en: '![](img/ec298b90-b592-4e50-9c92-e68076fde214.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec298b90-b592-4e50-9c92-e68076fde214.png)'
- en: 'Each sample has a dimensionality equal to *N*. We can now introduce a distance
    function *d(x[1], x[2])*, which in the majority of cases can be generalized with
    the Minkowski distance:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本具有等于*N*的维度。我们现在可以引入一个距离函数*d(x[1], x[2])**，在大多数情况下，它可以被Minkowski距离泛化：
- en: '![](img/70d36b7b-9ee9-4eba-aa27-4360263526e7.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/70d36b7b-9ee9-4eba-aa27-4360263526e7.png)'
- en: 'When *p = 2, d[p]* represents the classical Euclidean distance, that is normally
    the default choice. In particular cases, it can be useful to employ other variants,
    such as *p = 1* (which is the Manhattan distance) or *p > 2*. Even if all the
    properties of a metric function remain unchanged, different values of *p* yield
    results that can be *semantically* diverse. As an example, we can consider the
    distance between points *x[1] = (0, 0)* and *x[2] = (15, 10)* as a function of
    *p*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当*p = 2*时，*d[p]*代表经典的欧几里得距离，这通常是默认选择。在特定情况下，使用其他变体可能是有用的，例如*p = 1*（这是曼哈顿距离）或*p
    > 2*。即使度量函数的所有属性保持不变，不同的*p*值会产生语义上不同的结果。例如，我们可以考虑点*x[1] = (0, 0)*和*x[2] = (15,
    10)*作为*p*的函数的距离：
- en: '![](img/7d5b4c92-d564-4b54-85bb-3d61b20fd323.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7d5b4c92-d564-4b54-85bb-3d61b20fd323.png)'
- en: Minkowski distance between (0, 0) and (15, 10) as a function of parameter p
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (0, 0)和(15, 10)之间的Minkowski距离作为参数p的函数
- en: 'The distance decreases monotonically with *p* and converges to the largest
    component absolute difference, *|x[1]^((j)) - x[2]^((j))|*, when *p → ∞*. Therefore,
    whenever it''s important to weight all the components in the same way in order
    to have a consistent metric, small values of *p* are preferable (for example,
    *p=1* or *2*). This result has also been studied and formalized by Aggarwal, Hinneburg,
    and Keim (in *On the Surprising Behavior of Distance Metrics in High Dimensional
    Space*,*Aggarwal C. C.*, *Hinneburg A.*, *Keim D. A.*, *ICDT 2001*), who proved
    a fundamental inequality. If we consider a generic distribution *G* of *M* points
    *x[i] ∈ (0, 1)^d*, a distance function based on the *L[p]* norm, and the maximum
    *D[max]^p* and minimum *D[min]^p* distances (computed using the *L*[*p* ]norm)
    between two points, *x[j]* and *x*[*k*] drawn from *G* and *(0, 0)*, the following
    inequality holds:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 距离随着 *p* 的增加单调递减，并收敛到最大成分绝对差，*|x[1]^((j)) - x[2]^((j))|*，当 *p → ∞* 时。因此，为了保持一致的距离度量，在所有成分上施加相同的权重非常重要，因此较小的
    *p* 值更可取（例如，*p=1* 或 *2*）。这一结果已经被 Aggarwal、Hinneburg 和 Keim 在 *On the Surprising
    Behavior of Distance Metrics in High Dimensional Space*（Aggarwal C. C.，Hinneburg
    A.，Keim D. A.，ICDT 2001）一文中研究并形式化，他们证明了基本的不等式。如果我们考虑一个由 *M* 个点 *x[i] ∈ (0, 1)^d*
    组成的通用分布 *G*，一个基于 *L[p]* 范数的距离函数，以及两个点 *x[j]* 和 *x[k]* 之间的最大 *D[max]^p* 和最小 *D[min]^p*
    距离（使用 *L*[*p*] 范数计算），这两个点从 *G* 和 *(0, 0)* 中抽取，以下不等式成立：
- en: '![](img/56a0f65a-f7ba-4ec2-8edd-b701125f9255.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56a0f65a-f7ba-4ec2-8edd-b701125f9255.png)'
- en: 'It''s clear that when the input dimensionality is very high and *p >> 2*, the
    expected value, *E[D[max]^p - D[min]^p]*, becomes bounded between two constants,
    *k[1] (C[p]d^(1/p-1/2))* and *k[2 ] ((M-1)C[p]d^(1/p-1/2)) → 0*, reducing the
    actual effect of almost any distance. In fact, given two generic couples of points
    *(x[1], x[2])* and *(x[3], x[4])*drawn from *G*, the natural consequence of the
    following inequality is that *d[p](x[1], x[2]) ≈ d[p](x[3], x[4]**)* when *p →
    ∞*, independently of their relative positions. This important result confirms
    the importance of choosing the right metric according to the dimensionality of
    the dataset and that *p = 1* is the best choice when *d >> 1*, while *p >> 1*
    can produce inconsistent results due the ineffectiveness of the metric. To see
    direct confirmation of this phenomenon, it''s possible to run the following snippet,
    which computes the average difference between maximum and minimum distances considering
    `100` sets containing `100` samples drawn from a uniform distribution, *G ∼ U(0,
    1)*. In the snippet, the case of `d=2`, `100`, `1000` is analyzed with Minkowski
    metrics with `P`= `1`, `2`, `10`, `100` (the final values depend on the random
    seed and how many times the experiment is repeated):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，当输入维度非常高且 *p >> 2* 时，期望值 *E[D[max]^p - D[min]^p]* 介于两个常数 *k[1] (C[p]d^(1/p-1/2))*
    和 *k[2] ((M-1)C[p]d^(1/p-1/2)) → 0* 之间，这几乎消除了任何距离的实际影响。实际上，给定从 *G* 中抽取的两个通用点对
    *(x[1], x[2])* 和 *(x[3], x[4])*，以下不等式的自然结果是，当 *p → ∞* 时，*d[p](x[1], x[2]) ≈ d[p](x[3],
    x[4]*)，而不论它们的相对位置如何。这一重要结果证实了根据数据集的维度选择正确度量的重要性，并且当 *d >> 1* 时，*p = 1* 是最佳选择，而
    *p >> 1* 由于度量的无效性可能会产生不一致的结果。为了直接证实这一现象，可以运行以下代码片段，该片段计算了包含从均匀分布 *G ∼ U(0, 1)*
    中抽取的 `100` 个样本的 `100` 个集合之间最大和最小距离的平均差异。在该片段中，分析了 `d=2`、`100`、`1000` 的情况，使用 Minkowski
    度量，`P` 分别为 `1`、`2`、`10`、`100`（最终值取决于随机种子和实验重复的次数）：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A particular case, that is a direct consequence of the previous inequality is
    when the largest absolute difference between components determines the most important
    factor of a distance, large values of *p* can be employed. For example, if we
    consider three points, *x[1]** = (0, 0)*, *x**[2]** = (15, 10)*, and *x[3] = (15,
    0)*, *d[2](x[1], x[2]) ≈ 18* and *d[2](x[1], x[3]) = 15*. So, if we set a threshold
    at *d = 16* centered at *x[1]*, *x*[*2* ]is outside the boundaries. If instead *p
    = 15*, both distances become close to *15* and the two points (*x[2]* and *x[3]*)
    are inside the boundaries. A particular use of large values of *p* is when it's
    important to take into account the inhomogeneity among components. For example,
    some feature vectors can represent the age and height of a set of people. Considering
    a test person *x = (30, 175)*, with large *p* values, the distances between *x*
    and two samples *(35, 150)* and *(25, 151)* are almost identical (about *25.0*),
    and the only dominant factor becomes the height difference (independent from the
    age).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特殊情况，即前述不等式的直接结果是，当组件之间最大的绝对差异决定了距离的最重要因素时，可以采用大的 *p* 值。例如，如果我们考虑三个点，*x[1]*
    = (0, 0)，*x[2]* = (15, 10)，和 *x[3]* = (15, 0)，则 *d[2](x[1], x[2]) ≈ 18* 和 *d[2](x[1],
    x[3]) = 15*。因此，如果我们设置一个以 *x[1]* 为中心的阈值 *d = 16*，则 *x[2]* 在边界之外。如果 *p = 15*，两个距离都接近
    *15*，两个点 (*x[2]* 和 *x[3]*) 在边界内。当需要考虑组件之间的不均匀性时，大 *p* 值的特定用途。例如，一些特征向量可以表示一组人的年龄和身高。考虑一个测试人员
    *x = (30, 175)*，在大的 *p* 值下，*x* 与两个样本 *(35, 150)* 和 *(25, 151)* 之间的距离几乎相同（大约 *25.0*），唯一的决定性因素成为身高差异（独立于年龄）。
- en: 'The KNN algorithm determines the *k* closest samples of each training point.
    When a new sample is presented, the procedure is repeated with two possible variants:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 算法确定每个训练点的 *k* 个最近样本。当呈现新样本时，会重复执行此过程，有两种可能的变体：
- en: With a predefined value of *k*, the KNN are computed
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预定义的值 *k* 下，计算 KNN
- en: With a predefined radius/threshold *r*, all the neighbors whose distance is
    less than or equal to the radius are computed
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预定义的半径/阈值 *r* 下，计算所有距离小于或等于半径的邻居
- en: 'The philosophy of KNN is that similar samples can share their features. For
    example, a recommendation system can cluster users using this algorithm and, given
    a new user, find the most similar ones (based, for example, on the products they
    bought) to recommend the same category of items. In general, a similarity function
    is defined as the reciprocal of a distance (there are some exceptions, such as
    the cosine similarity):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 的哲学是相似的样本可以共享其特征。例如，一个推荐系统可以使用此算法对用户进行聚类，并针对新用户找到最相似的用户（例如，基于他们购买的产品）以推荐相同类别的物品。一般来说，相似度函数定义为距离的倒数（有一些例外，如余弦相似度）：
- en: '![](img/ca9f5020-6d8d-41d1-8dc7-a6e6e42ab4a9.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca9f5020-6d8d-41d1-8dc7-a6e6e42ab4a9.png)'
- en: Two different users, *A* and *B*, who are classified as neighbors, will differ
    under some viewpoints, but, at the same time, they will share some peculiar features.
    This statement authorizes us to increase the homogeneity by *suggesting the differences*.
    For example, if *A* liked book *b[1]* and *B* liked *b[2]*, we can recommend *b*[*1* ]to
    *B* and *b[2]* to *A*. If our hypothesis was correct, the similarity between *A*
    and *B* will be increased; otherwise, the two users will move towards other clusters
    that better represent their behavior.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 两位不同的用户，*A* 和 *B*，被分类为邻居，在某些观点上会有所不同，但与此同时，他们也会共享一些独特的特征。这个陈述使我们能够通过 *建议差异*
    来增加同质性。例如，如果 *A* 喜欢书籍 *b[1]* 而 *B* 喜欢书籍 *b[2]*，我们可以向 *B* 推荐 *b[1]*，向 *A* 推荐 *b[2]*。如果我们假设是正确的，那么
    *A* 和 *B* 之间的相似性将会增加；否则，这两个用户将向其他更好地代表他们行为的聚类移动。
- en: Unfortunately, the *vanilla* algorithm (in Scikit-Learn it is called the **brute-force**
    algorithm) can become extremely slow with a large number of samples because it's
    necessary to compute all the pairwise distances in order to answer any query.
    With *M* points, this number is equal to *M²*, which is often unacceptable (if
    *M* = 1,000, each query needs to compute a million distances). More precisely,
    as the computation of a distance in an N-dimensional space requires *N* operations,
    the total complexity becomes *O(M²N)*, which can be reasonable only for small
    values of both *M* and *N*. That's why some important strategies have been implemented
    to reduce the computational complexity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，*vanilla* 算法（在 Scikit-Learn 中称为**暴力**算法）在样本数量较多时可能会变得非常慢，因为它需要计算所有成对距离以回答任何查询。当有
    *M* 个点时，这个数字等于 *M²*，这通常是不可以接受的（如果 *M* = 1,000，每个查询都需要计算一百万个距离）。更精确地说，在一个 N 维空间中计算距离需要
    *N* 次操作，总复杂度变为 *O(M²N)*，这只有在 *M* 和 *N* 都很小的情况下才是合理的。这就是为什么已经实施了一些重要的策略来降低计算复杂度。
- en: KD Trees
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KD 树
- en: As all KNN queries can be considered search problems, one of the most efficient
    way to reduce the overall complexity is to reorganize the dataset into a tree
    structure. In a binary tree (one-dimensional data), the average computational
    complexity of a query is *O(log M)*, because we assume we have almost the same
    number of elements in each branch (if the tree is completely unbalanced, all the
    elements are inserted sequentially and the resulting structure has a single branch, 
    so the complexity becomes *O(M))*. In general, the real complexity is slightly
    higher than *O(log M)*, but the operation is always much more efficient than a
    vanilla search, which is *O(M²)*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有 KNN 查询都可以被视为搜索问题，降低整体复杂度最有效的方法之一是将数据集重新组织成树结构。在一个二叉树（一维数据）中，查询的平均计算复杂度为
    *O(log M)*，因为我们假设每个分支中几乎有相同数量的元素（如果树完全不平衡，所有元素都是顺序插入的，并且结果结构只有一个分支，因此复杂度变为 *O(M)*）。一般来说，实际的复杂度略高于
    *O(log M)*，但操作总是比普通的搜索更有效，普通搜索的复杂度为 *O(M²)*。
- en: 'However, we normally work with N-dimensional data and the previous structure
    cannot be immediately employed. KD Trees extend the concept of a binary for *N
    >* *1*. In this case, a split cannot be immediately performed and a different
    strategy must be chosen. The easiest way to solve this problem is to select a
    feature at each level *(1, 2, ..., N)* and repeat the process until the desired
    depth is reached. In the following diagram, there''s an example of KD Trees with
    three-dimensional points:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通常处理 N 维数据，并且之前的结构不能立即应用。KD 树扩展了二叉树的概念，用于 *N >* *1*。在这种情况下，不能立即进行分割，必须选择不同的策略。解决这个问题的最简单方法是，在每一层
    *(1, 2, ..., N)* 选择一个特征，并重复这个过程，直到达到所需的深度。在下面的图中，有一个三维点的 KD 树示例：
- en: '![](img/74005f85-2f8e-424a-be82-9c6f72f74ae3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74005f85-2f8e-424a-be82-9c6f72f74ae3.png)'
- en: Example of three-dimensional KD Tree
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 三维 KD 树的示例
- en: The root is point **(5, 3, 7)**. The first split is performed considering the
    first feature, so two children are **(2, 1, 1)** and **(8, 4, 3)**. The second
    one operates on the second feature and so on. The average computational complexity
    is *O(N log M)*, but if the distribution is very asymmetric, the probability that
    the tree becomes unbalanced is very high. To mitigate this issue, it's possible
    to select the feature corresponding to the median of the (sub-)dataset and to
    continue splitting with this criterion. In this way, the tree is guaranteed to
    be balanced. However, the average complexity is always proportional to the dimensionality
    and this can dramatically affect the performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点是点 **(5, 3, 7)**。第一次分割是考虑第一个特征，因此有两个子节点 **(2, 1, 1)** 和 **(8, 4, 3)**。第二次分割操作在第二个特征上，以此类推。平均计算复杂度为
    *O(N log M)*，但如果分布非常不对称，树变得不平衡的概率非常高。为了减轻这个问题，可以选择对应于（子）数据集中位数的特征，并以此标准继续分割。这样，树可以保证是平衡的。然而，平均复杂度总是与维数成正比，这可能会严重影响性能。
- en: For example, if *M* = 10,000 and *N* = 10, using the *log[10]*, *O(**N log M)
    = O(40)*, while, with *N* = 1,000, the complexity becomes *O(40,000)*. Generally,
    KD Trees suffers the *curse of dimensionality* and when *N* becomes large, the
    average complexity is about *O(MN)*, which is always better than the *vanilla*
    algorithm, but often too expensive for real-life applications.  Therefore, KD
    Trees is really effective only when the dimensionality is not too high. In all
    other cases, the probability of having an unbalanced tree and the resulting computational
    complexity suggest employing a different method.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果*M* = 10,000且*N* = 10，使用*log[10]*，则*O(N log M) = O(40)*，而，当*N* = 1,000时，复杂度变为*O(40,000)*。通常，KD树受到*维数灾难*的影响，当*N*变得很大时，平均复杂度约为*O(MN)*，这总是比*普通*算法好，但通常对于实际应用来说太昂贵。因此，KD树只有在维度不是太高的情况下才真正有效。在所有其他情况下，不平衡树的概率和由此产生的计算复杂度表明应采用不同的方法。
- en: Ball Trees
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 球树
- en: 'An alternative to KD Trees is provided by **Ball Trees**. The idea is to rearrange
    the dataset in a way that is almost insensitive to high-dimensional samples. A
    ball is defined as a set of points whose distance from a center sample is less
    than or equal to a fixed radius:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**球树**提供了KD树的替代方案。其思路是将数据集重新排列，使其对高维样本几乎不敏感。球被定义为从中心样本到其距离小于或等于固定半径的点集：'
- en: '![](img/13fcde74-8f62-45d7-a0db-f74613cc6670.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13fcde74-8f62-45d7-a0db-f74613cc6670.png)'
- en: 'Starting from the first main ball, it''s possible to build smaller ones nested
    into the parent ball and stop the process when the desired depth has been reached.
    A fundamental condition is that a point can always belong to a single ball. In
    this way, considering the cost of the N-dimensional distance, the computational
    complexity is *O(N log M)* and doesn''t suffer the curse of dimensionality like
    KD Trees. The structure is based on hyperspheres, whose boundaries are defined
    by the equations (given a center point *x* and a radius *R[i]*):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一个主要球开始，可以构建嵌套在父球中的更小的球，并在达到所需的深度时停止这个过程。一个基本条件是，一个点始终只能属于一个球。这样，考虑到N维距离的成本，计算复杂度为*O(N
    log M)*，并且不像KD树那样受到维数灾难的影响。该结构基于超球面，其边界由以下方程定义（给定中心点*x*和半径*R[i]*）：
- en: '![](img/22f759e3-1db7-473c-a070-6d18716385e2.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22f759e3-1db7-473c-a070-6d18716385e2.png)'
- en: 'Therefore, the only operation needed to find the right ball is measuring the
    distance between a sample and the centers starting from the smallest balls. If
    a point is outside the ball, it''s necessary to move upwards and check the parents,
    until the ball containing the sample is found. In the following diagram, there''s
    an example of Ball Trees with two levels:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，找到正确球所需的唯一操作是从最小的球开始测量样本与中心之间的距离。如果一个点在球外，则需要向上移动并检查父节点，直到找到包含样本的球。在以下图中，有一个具有两个层次的球树示例：
- en: '![](img/7b0af978-f4d6-4bdc-99e5-396ba696ef36.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b0af978-f4d6-4bdc-99e5-396ba696ef36.png)'
- en: Example of Ball Trees with seven bidimensional points and two levels
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 具有七个二维点和两个层次的球树示例
- en: In this example, the seven bidimensional points are split first into two balls
    containing respectively three and four points. At the second level, the second
    ball is split again into two smaller balls containing two points each. This procedure
    can be repeated until a fixed depth is reached or by imposing the maximum number
    of elements that a leaf must contain (in this case, it can be equal to *3*).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，七个二维点首先被分成包含三个和四个点的两个球。在第二层，第二个球再次被分成包含两个点的两个更小的球。这个过程可以重复进行，直到达到固定的深度或通过规定叶子必须包含的最大元素数量（在这种情况下，它可以等于*3*）。
- en: Both KD Trees and Ball Trees can be efficient structures to reduce the complexity
    of KNN queries. However, when fitting a model, it's important to consider both
    the *k parameter* (which normally represents the average or the standard number
    of neighbors computed in a query) and the maximum tree depth. These particular
    structures are not employed for common tasks (such as sorting) and their efficiency
    is maximized when all the requested neighbors can be found in the same sub-structure
    (with a size *K<< M*, to avoid an implicit fallback to the *vanilla* algorithm).
    In other words, the tree has the role of reducing the dimensionality of the search
    space by partitioning it into reasonably small regions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: KD树和球树都可以是有效的结构，用于减少KNN查询的复杂性。然而，在拟合模型时，重要的是要考虑*k*参数（通常表示查询中计算的邻居的平均数或标准数）和最大树深度。这些特定的结构不用于常见任务（如排序），并且当所有请求的邻居都可以在同一个子结构中找到时（大小*K<<
    M*，以避免隐式回退到*vanilla*算法）它们的效率最大化。换句话说，树通过将其划分为合理的小区域来减少搜索空间的维度。
- en: At the same time, if the number of samples contained in a leaf is small, the
    number of tree nodes grows and the complexity is subsequently increased. The negative
    impact is doubled because on average it's necessary to explore more nodes and
    if *k* is much greater than the number of elements contained in a node, it's necessary
    to merge the samples belonging to different nodes. On the other side, a very large
    number of samples per node leads to a condition that is close to the *vanilla*
    algorithm. For example, if *M* = 1,000 and each node contains 250 elements, once
    the right node is computed, the number of distances to compute is comparable with
    the initial dataset size and no real advantage is achieved by employing a tree
    structure. An acceptable practice is to set the size of a life equal to *5 ÷ 10*
    times the average value of *k*, to maximize the probability to find all the neighbors
    inside the same leaf. However, every specific problem must be analyzed (while
    also benchmarking the performances) in order to find the most appropriate value.
    If different values for *k* are necessary, it's important to consider the relative
    frequencies of the queries. For example, if a program needs 10 *5-NN* queries
    and 1 *50-NN* query, it's probably better to set a leaf size equal to 25, even
    if the *50-NN* query will be more expensive. In fact, setting a good value for
    a second query (for example, 200) will dramatically increase the complexity of
    the first 10 queries, driving to a performance loss.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，如果一个叶子节点中包含的样本数量较少，树节点的数量会增长，从而复杂性随之增加。负面影响加倍，因为平均来说需要探索更多的节点，如果*k*远大于节点中包含的元素数量，则需要合并属于不同节点的样本。另一方面，每个节点包含的样本数量非常多，会导致接近*vanilla*算法的状态。例如，如果*M*
    = 1,000，并且每个节点包含250个元素，一旦计算出了正确的节点，需要计算的距离数量与初始数据集的大小相当，采用树结构并没有真正获得优势。一种可接受的做法是将叶子的大小设置为平均*k*值的5到10倍，以最大化找到所有邻居都在同一叶子中的概率。然而，为了找到最合适的值，必须分析（同时基准测试性能）每个具体问题。如果需要不同的*k*值，重要的是考虑查询的相对频率。例如，如果一个程序需要10个*5-NN*查询和1个*50-NN*查询，可能最好将叶子大小设置为25，即使*50-NN*查询会更昂贵。实际上，为第二个查询（例如，200）设置一个良好的值将大大增加前10个查询的复杂性，导致性能损失。
- en: Example of KNN with Scikit-Learn
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中KNN的示例
- en: 'In order to test the KNN algorithm, we are going to use the MNIST handwritten
    digit dataset provided directly by Scikit-Learn. It is made up of 1,797 8 × 8
    grayscale images representing the digits from 0 to 9\. The first step is loading
    it and normalizing all the values to be bounded between 0 and 1:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试KNN算法，我们将使用Scikit-Learn直接提供的MNIST手写数字数据集。它由1,797个8×8灰度图像组成，代表从0到9的数字。第一步是加载它并将所有值归一化，使其介于0和1之间：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The dictionary `digits` contains both the images, `digits[''images'']`, and
    the flattened 64-dimensional arrays, `digits[''data'']`. Scikit-Learn implements
    different classes (for example, it''s possible to work directly with KD Trees
    and Ball Trees using the KDTree and BallTree classes) that can be used in the
    context of KNN (as clustering, classification, and regression algorithms). However,
    we''re going to employ the main class, `NearestNeighbors`, which allows performing
    clustering and queries based either on the number of neighbors or on the radius
    of a ball centered on a sample:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 字典 `digits` 包含图像 `digits['images']` 和展平的 64 维数组 `digits['data']`。Scikit-Learn
    实现了不同的类（例如，可以使用 KDTree 和 BallTree 类直接与 KD 树和 Ball 树一起工作），这些类可以在 KNN 的上下文中使用（作为聚类、分类和回归算法）。然而，我们将使用主要的类
    `NearestNeighbors`，它允许基于邻居的数量或以样本为中心的球体的半径进行聚类和查询：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have chosen to have a default number of neighbors equal to `50` and an algorithm
    based on a `ball_tree`. The leaf size (`leaf_size`) parameter has been kept to
    its default value equal to `30`. We have also employed the default metric (Euclidean),
    but it's possible to change it using the `metric` and `p` parameters (which is
    the order of the Minkowski metric). Scikit-Learn supports all the metrics implemented
    by SciPy in the `scipy.spatial.distance` package. However, in the majority of
    cases, it's sufficient to use a Minkowski metric and adjust the value of `p` if
    the results are not acceptable with any number of neighbors. Other metrics, such
    as the cosine distance, can be employed when the similarity must not be affected
    by the Euclidean distance, but only by the angle between two vectors pointing
    at the samples. Applications that use this metric include, for example, deep learning
    models for natural language processing, where the words are embedded into feature
    vectors whose semantic similarity is proportional to their Cosine distance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择默认的邻居数量为 `50`，以及基于 `ball_tree` 的算法。叶子大小（`leaf_size`）参数已被保留为其默认值 `30`。我们还使用了默认的度量标准（欧几里得），但可以通过
    `metric` 和 `p` 参数（这是 Minkowski 度量标准的阶数）来更改它。Scikit-Learn 支持SciPy在 `scipy.spatial.distance`
    包中实现的全部度量标准。然而，在大多数情况下，使用 Minkowski 度量标准并调整 `p` 的值就足够了，如果任何数量的邻居的结果不可接受。其他度量标准，如余弦距离，可以在相似性不能受欧几里得距离影响，而只能由指向样本的两个向量之间的角度影响时使用。使用此度量标准的应用包括，例如，用于自然语言处理的深度学习模型，其中单词被嵌入到特征向量中，其语义相似性与它们的余弦距离成正比。
- en: 'We can now query the model in order to find 50 neighbors of a sample. For our
    purposes, we have selected the sample with index 100, which represents a 4 (the
    images have a very low resolution, but it''s always possible to distinguish the
    digit):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以查询模型以找到样本的 50 个邻居。为了我们的目的，我们选择了索引为 100 的样本，它代表数字 4（图像分辨率非常低，但总是可以区分数字）：
- en: '![](img/6ff7c51f-cf3f-4943-ba4c-66f73b368ebb.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ff7c51f-cf3f-4943-ba4c-66f73b368ebb.png)'
- en: Sample digit used to query the KNN model
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于查询 KNN 模型的样本数字
- en: 'The query can be performed using the instance method `kneighbors`, which allows
    specifying the number of neighbors (`n_neighbors` parameter the default is the
    value selected during the instantiation of the class) and whether we want to also
    get the distances of each neighbor (the `return_distance` parameter). In this
    example, we are also interested in evaluating *how far* the neighbors are from
    the center, so we set `return_distance=True`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 查询可以通过实例方法 `kneighbors` 来执行，该方法允许指定邻居的数量（`n_neighbors` 参数，默认值是在类实例化时选择的值）以及我们是否想要获取每个邻居的距离（`return_distance`
    参数）。在这个例子中，我们还对评估邻居与中心的距离有多远感兴趣，因此我们将 `return_distance=True` 设置为：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first neighbor is always the center, so its distance is `0`. The other
    ones range from 0.9 to 1.9\. Considering that, in this case, the maximum possible
    distance is 8 (between a 64-dimensional vector *a = (1, 1, ..., 1)* and the null
    vector), the result could be acceptable. In order to get confirmation, we can
    plot the neighbors as bidimensional 8 × 8 arrays (the returned array, `neighbors`,
    contains the indexes of the samples). The result is shown in the following screenshot:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个邻居总是中心，因此其距离为 `0`。其他邻居的距离从 0.9 到 1.9。考虑到在这种情况下，可能的最大距离是 8（在 64 维向量 *a = (1,
    1, ..., 1)* 和零向量之间），结果可能是可接受的。为了得到确认，我们可以将邻居作为二维 8 × 8 数组绘制出来（返回的数组 `neighbors`
    包含样本的索引）。结果如下截图所示：
- en: '![](img/da803e16-8f68-468b-95ff-b477a2594189.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da803e16-8f68-468b-95ff-b477a2594189.png)'
- en: 50 neighbors selected by the KNN model
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 模型选择的 50 个邻居
- en: As it's possible to see, there are no errors, but all the shapes are slightly
    different. In particular, the last one, which is also the farthest, has a lot
    of white pixels (corresponding to the value 1.0), explaining the reason of a distance
    equal to about 2.0\. I invite the reader to test the `radius_neighbors` method
    until spurious values appear among the results. It's also interesting to try this
    algorithm with the Olivetti faces dataset, whose complexity is higher and many
    more geometrical parameters can influence the similarity.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，没有错误，但所有形状都略有不同。特别是最后一个，也是最远的，有很多白色像素（对应于值1.0），解释了距离约为2.0的原因。我邀请读者测试`radius_neighbors`方法，直到结果中出现虚假值。尝试使用Olivetti人脸数据集运行此算法也很有趣，其复杂性更高，许多更多的几何参数可以影响相似性。
- en: K-means
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means
- en: 'When we discussed the Gaussian mixture algorithm, we defined it as *Soft K-means*.
    The reason is that each cluster was represented by three elements: mean, variance,
    and weight. Each sample always belongs to all clusters with a probability provided
    by the Gaussian distributions. This approach can be very useful when it''s possible
    to manage the probabilities as weights, but in many other situations, it''s preferable
    to determine a single cluster per sample. Such an approach is called hard clustering
    and K-means can be considered the hard version of a Gaussian mixture. In fact,
    when all variances *Σ[i] → 0*, the distributions degenerate to Dirac''s Deltas,
    which represent perfect spikes centered at a specific point. In this scenario,
    the only possibility to determine the most appropriate cluster is to find the
    shortest distance between a sample point and all the centers (from now on, we
    are going to call them *centroids*). This approach is also based on an important
    double principle that should be taken into account in every clustering algorithm.
    The clusters must be set up to maximize:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论高斯混合算法时，我们将其定义为*软K-means*。原因是每个簇由三个元素表示：均值、方差和权重。每个样本总是以高斯分布提供的概率属于所有簇。当可能管理概率作为权重时，这种方法非常有用，但在许多其他情况下，确定每个样本的单个簇更可取。这种方法被称为硬聚类，K-means可以被认为是高斯混合的硬版本。实际上，当所有方差*Σ[i]→
    0*时，分布退化为Dirac的δ函数，它们代表以特定点为中心的完美尖峰。在这种情况下，确定最合适的簇的唯一可能性是找到样本点与所有中心（从现在起，我们将它们称为*质心*）之间的最短距离。这种方法也基于一个重要的双重原则，应该在每个聚类算法中加以考虑。簇必须设置以最大化：
- en: The intra-cluster cohesion
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇内内聚性
- en: The inter-cluster separation
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇间分离
- en: 'This means that we expect to label high-density regions that are well separated
    from each other. When this is not possible, the criterion must try to minimize
    the intra-cluster average distance between samples and centroid. This quantity
    is also called *inertia* and it''s defined as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们期望标记彼此之间距离较远的密集区域。当这不可能实现时，标准必须尝试最小化样本和质心之间的簇内平均距离。这个量也称为*惯性*，其定义为：
- en: '![](img/b5909e8f-cde4-4615-be26-c9981119b84d.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b5909e8f-cde4-4615-be26-c9981119b84d.png)'
- en: High levels of inertia imply low cohesion because there are probably too many
    points belongings to clusters whose centroids are too far away. The problem can
    be solved by minimizing the previous quantity. However, the computational complexity
    needed to find the global minimum is exponential (K-means belongs to the class
    of NP-Hard problems). The alternative approach employed by the K-means algorithm,
    also known as **Lloyd's algorithm**, is iterative and starts from selecting *k*
    random centroids (in the next section, we're going to analyze a more efficient
    method) and adjusting them until their configuration becomes stable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 高惯性意味着低内聚性，因为可能存在太多属于簇的点，而这些簇的质心距离太远。可以通过最小化前面的量来解决此问题。然而，找到全局最小值所需的计算复杂度是指数级的（K-means属于NP-Hard问题类别）。K-means算法采用的替代方法，也称为**Lloyd算法**，是迭代的，并从选择*k*个随机质心开始（在下一节中，我们将分析一种更有效的方法），并调整它们直到它们的配置变得稳定。
- en: 'The dataset to cluster (with *M* samples) is represented as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要聚类的数据集（包含*M*个样本）表示为：
- en: '![](img/cf1fde0d-5770-444d-8b44-b80daebea84a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf1fde0d-5770-444d-8b44-b80daebea84a.png)'
- en: 'An initial guess for the centroids is:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 质心的初始猜测为：
- en: '![](img/b8e117ce-ddea-4bc9-8eb8-63cc585ac54b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8e117ce-ddea-4bc9-8eb8-63cc585ac54b.png)'
- en: 'There are no particular restrictions on the initial values. However, the choice
    can influence both the convergence speed and the minimum that is found. The iterative
    procedure will loop over the dataset, computing the Euclidean distance between
    *x*[*i* ]and each *μ[j]* and assigning a cluster based on the criterion:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 初始值没有特定的限制。然而，选择可以影响收敛速度和找到的最小值。迭代过程将遍历数据集，计算 *x*[*i* ]和每个 *μ[j]* 之间的欧几里得距离，并根据以下标准分配簇：
- en: '![](img/a387604a-6b26-42a4-994a-6b7b4e32e3c2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a387604a-6b26-42a4-994a-6b7b4e32e3c2.png)'
- en: 'Once all the samples have been clustered, the new centroids are computed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有样本都被聚类，新的质心将被计算：
- en: '![](img/11d25a25-0e01-491b-8c64-8ef6a3dd42f8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/11d25a25-0e01-491b-8c64-8ef6a3dd42f8.png)'
- en: The quantity *N[Cj]* represents the number of points belonging to cluster *j*.
    At this point, the inertia is recomputed and the new value is compared with the
    previous one. The procedure will stop either after a fixed number of iterations
    or when the variations in the inertia become smaller than a predefined threshold.
    Lloyd's algorithm is very similar to a particular case of the EM algorithm. In
    fact, the first step of each iteration is the computation of an *expectation*
    (the centroid configuration), while the second step maximizes the intra-cluster
    cohesion by minimizing the inertia.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 量 *N[Cj]* 表示属于聚类 *j* 的点的数量。在此点，重新计算惯性并与前一个值比较。该过程将在达到固定迭代次数或惯性变化小于预定义阈值后停止。Lloyd
    算法与 EM 算法的一个特例非常相似。实际上，每个迭代的第一个步骤是计算一个 *期望*（质心配置），而第二个步骤通过最小化惯性来最大化簇内凝聚力。
- en: 'The complete vanilla K-means algorithm is:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 vanilla K-means 算法是：
- en: Set a maximum number of iterations *N*[*max*].
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置最大迭代次数 *N*[*max*]。
- en: Set a tolerance *Thr*.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个容差 *Thr*。
- en: Set the value of *k* (number of expected clusters).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *k*（期望的聚类数量）的值。
- en: Initialize vector *C^((0))* with random values. They can be points belonging
    to the dataset or sampled from a suitable distribution.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化向量 *C^((0))*。它们可以是数据集中的点或从合适的分布中采样。
- en: Compute the initial inertia *S*^(*(0)*)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算初始惯性 *S*^(*(0)*)
- en: Set *N = 0*.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *N = 0*。
- en: 'While *N < N[max]* or *||S^((t)) - S^((t-1))|| > Thr*:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *N < N[max]* 或 *||S^((t)) - S^((t-1))|| > Thr* 时：
- en: '*N = N + 1*'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*N = N + 1*'
- en: 'For *x[i]* in *X*:'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *X* 中的 *x[i]*：
- en: Assign *x[i]* to a cluster using the shortest distance between *x[i]* and *μ*[*j*]
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *x[i]* 和 *μ*[*j*] 之间的最短距离将 *x[i]* 分配到簇
- en: Recompute the centroid vector *C*^(*(t)*)
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算质心向量 *C*^(*(t)*)
- en: Recompute the inertia *S*^(*(t)*)
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算惯性 *S*^(*(t)*)
- en: The algorithm is quite simple and intuitive, and there are many real-life applications
    based on it. However, there are two important elements to consider. The first
    one is the convergence speed. It's easy to show that every initial guess drives
    to a convergence point, but the number of iterations is dramatically influenced
    by this choice and there's no guarantee to find the global minimum. If the initial
    centroids are close to the final ones, the algorithm needs only a few steps to
    correct the values, but when the choice is totally random, it's not uncommon to
    need a very high number of iterations. If there are *N* samples and *k* centroids,
    *Nk* distances must be computed at each iteration, leading to an inefficient result.
    In the next paragraph, we'll show how it's possible to initialize the centroids
    to minimize the convergence time.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法相当简单直观，并且基于它的现实生活应用有很多。然而，有两个重要元素需要考虑。第一个是收敛速度。很容易证明每个初始猜测都会驱动到一个收敛点，但迭代次数会受到这个选择的影响，并且无法保证找到全局最小值。如果初始质心接近最终质心，算法只需要几步就能纠正值，但如果选择完全随机，则可能需要非常高的迭代次数。如果有
    *N* 个样本和 *k* 个质心，则必须在每次迭代中计算 *Nk* 个距离，导致结果效率低下。在下一段中，我们将展示如何初始化质心来最小化收敛时间。
- en: Another important aspect is that, contrary to KNN, K-means needs to predefine
    the number of expected clusters. In some cases, this is a secondary problem because
    we already know the most appropriate value for k. However, when the dataset is
    high-dimensional and our knowledge is limited, this choice could be hazardous.
    A good approach to solve the issue is to analyze the final inertia for a different
    number of clusters. As we expect to maximize the intra-cluster cohesion, a small
    number of clusters will lead to an increased inertia. We try to pick the highest
    point below a maximum tolerable value. Theoretically, we can also pick *k = N*.
    In this case, the inertia becomes zero because each point represents the centroid
    of its cluster, but a large value for *k* transforms the clustering scenario into
    a fine-grained partitioning that might not be the best strategy to capture the
    feature of a consistent group. It's impossible to define a rule for the upper
    bound *k[max]*, but we assume that this value is always much less than *N*. The
    best choice is achieved by selecting *k* to minimize the inertia, selecting the
    values from a set bounded, for example, between *2* and *k[max]*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要方面是，与 KNN 不同，K-means 需要预先定义期望的簇数量。在某些情况下，这是一个次要问题，因为我们已经知道 k 的最合适值。然而，当数据集是高维的且我们的知识有限时，这个选择可能会很危险。解决这个问题的好方法是对不同数量的簇分析最终的惯性。我们期望最大化簇内凝聚力，簇的数量较少会导致惯性增加。我们试图选择低于最大可容忍值的最高点。理论上，我们也可以选择
    *k = N*。在这种情况下，惯性变为零，因为每个点代表其簇的质心，但 *k* 的一个较大值将聚类场景转换为细粒度划分，这可能不是捕捉一致群体特征的最好策略。无法为上界
    *k[max]* 定义一个规则，但我们假设这个值总是远小于 *N*。最佳选择是通过选择 *k* 来最小化惯性，从例如介于 *2* 和 *k[max]* 之间的集合中选择值。
- en: K-means++
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means++
- en: We have said that a good choice for the initial centroids can improve the convergence
    speed and leads to a minimum that is closer to the global optimum of the inertia
    S. Arthur and Vassilvitskii (in *The Advantages of Careful Seeding, **Arthur,
    D., Vassilvitskii S., k-means++:* *Proceedings of the Eighteenth Annual ACM-SIAM
    Symposium on Discrete Algorithms*) proposed a method called K-means++, which allows
    increasing the accuracy of the initial centroid guess considering the most likely
    final configuration.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经说过，一个好的初始质心选择可以改善收敛速度，并使最小值更接近惯性 S 的全局最优解。Arthur 和 Vassilvitskii（在 *The
    Advantages of Careful Seeding, **Arthur, D., Vassilvitskii S., k-means++:* *Proceedings
    of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms*）提出了一种称为 K-means++
    的方法，该方法允许通过考虑最可能的最终配置来提高初始质心猜测的准确性。
- en: 'In order to expose the algorithm, it''s useful to introduce a function, *D(x,
    i)*, which is defined as:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示算法，引入一个函数，*D(x, i)*，定义为：
- en: '![](img/e91fd7c6-9b48-409b-8445-b5850263705f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e91fd7c6-9b48-409b-8445-b5850263705f.png)'
- en: '*D(x, i)* defines the shortest distance between each sample and one of the
    centroids already selected. As the process is incremental, this function must
    be recomputed after all steps. For our purposes, let''s also define an auxiliary
    probability distribution (we omit the index variable for simplicity):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*D(x, i)* 定义了每个样本与已选中的一个质心之间的最短距离。由于这个过程是增量式的，这个函数必须在所有步骤之后重新计算。为了我们的目的，我们还可以定义一个辅助概率分布（为了简单起见，我们省略了索引变量）：'
- en: '![](img/52eda745-0e5f-4253-8612-9d9bff3f739c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/52eda745-0e5f-4253-8612-9d9bff3f739c.png)'
- en: 'The first centroid *μ[0]* is sampled from *X* using a uniform distribution.
    The next steps are:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个质心 *μ[0]* 是使用均匀分布从 *X* 中采样的。接下来的步骤是：
- en: Compute *D(x, i)* for all *x ∈ X* considering the centroids already selected
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有 *x ∈ X*，考虑已选中的质心计算 *D(x, i)*
- en: Compute *G(x)*
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *G(x)*
- en: Select the next centroid *μ[i]* from *X* with a probability *G(x)*
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *X* 中以概率 *G(x)* 选择下一个质心 *μ[i]*
- en: 'In the aforementioned paper, the authors showed a very important property.
    If we define *S^** as the global optimum of *S*, a K-means++ initialization determines
    an upperbound for the expected value of the actual inertia:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述论文中，作者展示了一个非常重要的性质。如果我们定义 *S^** 为 *S* 的全局最优解，K-means++ 初始化确定了实际惯性期望值的上界：
- en: '![](img/414461c4-d6c3-4280-82bf-0c33ce065114.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/414461c4-d6c3-4280-82bf-0c33ce065114.png)'
- en: This condition is often expressed by saying that K-means++ is *O(log k)*-competitive.
    When *k* is sufficiently small, the probability of finding a local minimum close
    to the global one increases. However, K-means++ is still a probabilistic approach
    and different initializations on the same dataset lead to different initial configurations.
    A good practice is to run a limited number of initializations (for example, ten)
    and pick the one associated with the smallest inertia. When training complexity
    is not a primary issue, this number can be increased, but different experiments
    showed that the improvement achievable with a very large number of trials is negligible
    when compared to the actual computational cost. The default value in Scikit-Learn
    is ten and the author suggests to keep this value in the majority of cases. If
    the result continues to be poor, it's preferable to pick another method. Moreover,
    there are problems that cannot be solved using K-means (even with the best possible
    initialization), because one of the assumptions of the algorithm is that each
    cluster is a hypersphere and the distances are measured using a Euclidean function.
    In the following sections, we're going to analyze other algorithms that are not
    constrained to work with such limitations and can easily solve clustering problems
    using asymmetric cluster geometries.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种条件通常通过说K-means++是*O(log k)*-竞争性的来表示。当*k*足够小的时候，找到接近全局最小值的局部最小值的概率增加。然而，K-means++仍然是一种概率方法，在相同的数据集上不同的初始化会导致不同的初始配置。一个好的做法是运行有限数量的初始化（例如，十个）并选择与最小惯性相关联的那个。当训练复杂性不是主要问题时，这个数字可以增加，但不同的实验表明，与实际的计算成本相比，一个非常大的试验数量所能实现的改进是可以忽略不计的。Scikit-Learn中的默认值是十个，作者建议在大多数情况下保持这个值。如果结果仍然很差，最好选择另一种方法。此外，有些问题无法使用K-means（即使是使用最佳可能的初始化）来解决，因为算法的一个假设是每个群集都是一个超球体，距离是通过欧几里得函数来测量的。在接下来的章节中，我们将分析其他不受此类限制的算法，这些算法可以轻松地使用非对称群集几何来解决聚类问题。
- en: Example of K-means with Scikit-Learn
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn的K-means示例
- en: 'In this example, we continue using the MNIST dataset (the `X_train` array is
    the same defined in the paragraph dedicated to KNN), but we want also to analyze
    different clustering evaluation methods. The first step is visualizing the inertia
    corresponding to different numbers of clusters. We are going to use the `KMeans` class,
    which accepts the `n_clusters` parameter and employs the K-means++ initialization
    as the default method (as explained in the previous section, in order to find
    the best initial configuration, Scikit-Learn performs several attempts and selects
    the configuration with the lowest inertia; it''s possible to change the number
    of attempts through the `n_iter` parameter):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们继续使用MNIST数据集（`X_train`数组与KNN段落中定义的相同），但我们还想分析不同的聚类评估方法。第一步是可视化不同群集数量对应的惯性。我们将使用`KMeans`类，它接受`n_clusters`参数，并使用K-means++初始化作为默认方法（如前所述，为了找到最佳初始配置，Scikit-Learn执行多次尝试并选择具有最低惯性的配置；可以通过`n_iter`参数更改尝试次数）：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are supposing to analyze the range [`2`, `20`]. After each training session,
    the final inertia can be retrieved using the `inertia_` instance variable. The
    following graph shows the plot of the values as a function of the number of clusters:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设要分析的范围是`[2`, `20`]。在每次训练会话之后，可以使用`inertia_`实例变量检索最终的惯性。以下图表显示了值作为群集数量的函数的绘制：
- en: '![](img/d09423dd-2a55-40ea-b9b1-e14ef6cf4059.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d09423dd-2a55-40ea-b9b1-e14ef6cf4059.png)'
- en: Inertia as a function of the number of clusters
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 群集数量的惯性函数
- en: 'As expected, the function is decreasing, starting from a value of about 7,500
    and reaching about 3,700 with **20** clusters. In this case, we know that the
    real number is **10**, but it''s possible to discover it by observing the trend.
    The slope is quite high before **10**, but it starts decreasing more and more
    slowly after this threshold. This is a signal that informs us that some clusters
    are not well separated, even if their internal cohesion is high. In order to confirm
    this hypothesis, we can set `n_clusters=10` and, first of all, check the centroids
    at the end of the training process:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，函数是递减的，从大约7,500的值开始，达到大约3,700，**20**个簇。在这种情况下，我们知道实际数字是**10**，但可以通过观察趋势来发现它。在**10**之前，斜率相当高，但超过这个阈值后开始越来越慢地下降。这是一个信号，告诉我们一些簇没有很好地分离，即使它们的内部凝聚力很高。为了证实这个假设，我们可以将`n_clusters`设置为**10**，首先检查训练过程结束时的重心：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The centroids are available through the `cluster_centers_` instance variable.
    In the following screenshot, there''s a plot of the corresponding bidimensional
    arrays:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重心可以通过`cluster_centers_`实例变量获取。在下面的截图中，有一个对应的双维数组的图示：
- en: '![](img/89dd07e7-a96b-41dc-aabe-580fbdaa62d4.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/89dd07e7-a96b-41dc-aabe-580fbdaa62d4.png)'
- en: K-means centroid at the end of the training process
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: K-means重心位于训练过程结束时
- en: 'All the digits are present and there are no duplicates. This confirms that
    the algorithm has successfully separated the sets, but the final inertia (which
    is about 4,500) informs us that there are probably wrong assignments. To obtain
    confirmation, we can plot the dataset using a dimensionality-reduction method,
    such as t-SNE (see [Chapter 3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based
    Semi-Supervised Learning* for further details):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数字都存在，没有重复。这证实了算法已经成功分离了集合，但最终的惯性（大约为4,500）告诉我们可能存在错误分配。为了获得确认，我们可以使用如t-SNE（见[第3章](c23d1792-167f-416e-a848-fa7a10777697.xhtml)，*基于图的无监督学习*以获取更多详细信息）这样的降维方法绘制数据集：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, we can plot the bidimensional dataset with the corresponding
    cluster labels:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以绘制带有相应簇标签的双维数据集：
- en: '![](img/40643b02-fd5f-442d-849b-1231625cd23a.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/40643b02-fd5f-442d-849b-1231625cd23a.png)'
- en: t-SNE representation of the MNIST dataset; the labels correspond to the clusters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的t-SNE表示；标签对应于簇
- en: 'The plot confirms that the dataset is made up of well-separated blobs, but
    a few samples are assigned to the wrong cluster (this is not surprising considering
    the similarity between some pairs of digits). An important observation can further
    explain the trend of the inertia. In fact, the point where the slope changes almost
    abruptly corresponds to 9 clusters. Observing the t-SNE plot, we can immediately
    discover the reason: the cluster corresponding to the digit **7** is indeed split
    into 3 blocks. The main one contains the majority of samples, but there are another
    2 smaller blobs that are wrongly *attached* to clusters **1** and **9**. This
    is not surprising, considering that the digit **7** can be very similar to a distorted
    **1** or **9**. However, these two spurious blobs are always at the boundaries
    of the wrong clusters (remember that the geometric structures are hyperspheres),
    confirming that the metric has successfully detected a low similarity. If a group
    of wrongly assigned samples were in the middle of a cluster, it would have meant
    that the separation failed dramatically and another method should be employed.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 该图证实数据集由分离良好的团块组成，但有一些样本被分配到了错误的簇（考虑到一些数字对之间的相似性，这是不奇怪的）。一个重要的观察可以进一步解释惯性趋势。事实上，斜率几乎突然变化的点对应于9个簇。观察t-SNE图，我们可以立即发现原因：对应数字**7**的簇确实被分成了3块。主要的一块包含大多数样本，但有另外2个较小的团块错误地**附着**在簇**1**和**9**上。考虑到数字**7**可以非常类似于扭曲的**1**或**9**，这是不奇怪的。然而，这两个虚假的团块总是位于错误簇的边界上（记住几何结构是超球体），这证实了度量已经成功检测到低相似度。如果一组错误分配的样本位于簇的中间，那就意味着分离失败得很严重，应该采用另一种方法。
- en: Evaluation metrics
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: In many cases, it's impossible to evaluate the performance of a clustering algorithm
    using only a visual inspection. Moreover, it's important to use standard objective
    metrics that allow for comparing different approaches. We are now going to introduce
    some methods based on the knowledge of the ground truth (the correct assignment
    for each sample) and one common strategy employed when the true labels are unknown.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，仅通过视觉检查无法评估聚类算法的性能。此外，使用标准目标指标来比较不同方法非常重要。我们现在将介绍一些基于地面真实知识（每个样本的正确分配）的方法，以及当真实标签未知时采用的一种常见策略。
- en: 'Before discussing the scoring functions, we need to introduce a standard notation.
    If there are *k* clusters, we define the true labels as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论评分函数之前，我们需要介绍一种标准符号。如果有 *k* 个簇，我们定义真实标签为：
- en: '![](img/200a6236-e235-436e-88cf-31d7b3d481ce.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/200a6236-e235-436e-88cf-31d7b3d481ce.png)'
- en: 'In the same way, we can define the predicted labels:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以定义预测标签：
- en: '![](img/89db817b-4b6b-43bd-995b-3a2191f0ec35.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89db817b-4b6b-43bd-995b-3a2191f0ec35.png)'
- en: 'Both sets can be considered as sampled from two discrete random variables (for
    simplicity, we denote them with the same names), whose probability mass functions
    are *P[true](y)* and *P[pred](y)* with a generic *y* *∈ {y[1], y[2], ..., y[k]}*
    (*y[i]* represents the index of the *i^(th)* cluster). These two probabilities
    can be approximated with a frequency count; so, for example, the probability *P[true](1)*
    is computed as the number of samples whose true label is *1 n[true](1)* over the
    total number of samples *M*. In this way, we can define the entropies:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个集合都可以被视为来自两个离散随机变量的样本（为了简单起见，我们用相同的名称表示它们），其概率质量函数为 *P[true](y)* 和 *P[pred](y)*，其中
    *y* *∈ {y[1], y[2], ..., y[k]}* (*y[i]* 代表第 *i* 个簇的索引）。这两个概率可以用频率计数来近似；例如，概率 *P[true](1)*
    是指真实标签为 *1* 的样本数除以总样本数 *M*。这样，我们可以定义熵：
- en: '![](img/993508fd-4259-4fc0-848b-7e3104f105ae.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/993508fd-4259-4fc0-848b-7e3104f105ae.png)'
- en: 'These quantities describe the intrinsic uncertainty of the random variables.
    They are maximized when all the classes have the same probability, while, for
    example, they are null if all the samples belong to a single class (minimum uncertainty).
    We also need to know the uncertainty of a random variable *Y* given another one
    *X*. This can be achieved using the conditional entropy *H(Y|X)*. In this case,
    we need to compute the joint probability *p(x, y)* because the definition of *H(Y|X)*
    is:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这些量描述了随机变量的内在不确定性。当所有类别的概率相同时，它们达到最大值，例如，如果所有样本都属于单个类别（最小不确定性），则它们为零。我们还需要知道随机变量
    *Y* 在给定另一个随机变量 *X* 时的不确定性。这可以通过条件熵 *H(Y|X)* 来实现。在这种情况下，我们需要计算联合概率 *p(x, y)*，因为
    *H(Y|X)* 的定义是：
- en: '![](img/1dc59e67-4559-4193-bd3b-8abea170fd2b.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1dc59e67-4559-4193-bd3b-8abea170fd2b.png)'
- en: 'In order to approximate the previous expression, we can define the function
    *n(i[true], j[pred])*, which counts the number of samples with the true label
    *i* assigned to cluster *j*. In this way, if there are *M* samples, the approximated
    conditional entropies become:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了近似前面的表达式，我们可以定义函数 *n(i[true], j[pred])*，它计算将真实标签 *i* 分配给簇 *j* 的样本数量。这样，如果有
    *M* 个样本，近似条件熵变为：
- en: '![](img/1d0accfc-3ec0-4701-b822-b7c83718e106.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d0accfc-3ec0-4701-b822-b7c83718e106.png)'
- en: Homogeneity score
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同质性得分
- en: 'This score is useful to check whether the clustering algorithm meets an important
    requirement: a cluster should contain only samples belonging to a single class.
    It''s defined as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分有助于检查聚类算法是否满足一个重要要求：一个簇应只包含属于单个类别的样本。它被定义为：
- en: '![](img/b43c6cea-4195-4e57-bbde-3147395cdb21.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b43c6cea-4195-4e57-bbde-3147395cdb21.png)'
- en: 'It''s bounded between *0* and *1*, with low values indicating a low homogeneity.
    In fact, when the knowledge of *Y[pred]* reduces the uncertainty of *Y[true]*,
    *H(Y[true]|Y[pred])* becomes smaller (*h → 1*) and viceversa. For our example,
    the homogeneity score can be computed as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它的值介于 *0* 和 *1* 之间，低值表示同质性低。事实上，当 *Y[pred]* 的知识减少了 *Y[true]* 的不确定性时，*H(Y[true]|Y[pred])*
    变得较小 (*h → 1*)，反之亦然。对于我们的例子，同质性得分可以计算如下：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `digits['target']` array contains the true labels while `Y` contains the
    predictions (all the functions we are going to use accept the true labels as the
    first parameter and the predictions as the second one). The homogeneity score
    confirms that the clusters are rather homogeneous, but there's still a moderate
    level of uncertainty because some clusters contain wrong assignments. This method,
    together with the other ones, can be used to search for the right number of clusters
    and tune up all supplementary hyperparameters (such as the number of iterations
    or the metric function).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`digits[''target'']` 数组包含真实标签，而 `Y` 包含预测（我们将要使用的所有函数都接受真实标签作为第一个参数，预测作为第二个参数）。同质性得分证实簇相对同质，但仍然存在一定程度的不确定性，因为一些簇包含错误的分配。这种方法，连同其他方法，可以用来寻找正确的簇数并调整所有辅助超参数（如迭代次数或度量函数）。'
- en: Completeness score
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整性得分
- en: 'This score is complementary to the previous one. Its purpose is to provide
    a piece of information about the assignment of samples belonging to the same class.
    More precisely, a good clustering algorithm should assign all samples with the
    same true label to the same cluster. From our previous analysis, we know that,
    for example, the digit 7 has been wrongly assigned to both clusters 9 and 1; therefore,
    we expect a non-perfect completeness score. The definition is symmetric to the
    homogeneity score:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分是前一个得分的补充。它的目的是提供关于属于同一类的样本分配的一些信息。更确切地说，一个好的聚类算法应该将所有具有相同真实标签的样本分配到同一个簇中。从我们之前的分析中，我们知道，例如，数字7被错误地分配到了簇9和簇1中；因此，我们预计完整性得分不会完美。定义是对称的，与同质性得分相同：
- en: '![](img/ad248be3-4fcd-4d42-b5bd-1ccbdf582556.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad248be3-4fcd-4d42-b5bd-1ccbdf582556.png)'
- en: 'The rationale is very intuitive. When *H(Y[pred]|Y[true]**)* is low *(c → 1)*,
    it means that the knowledge of the ground truth reduces the uncertainty about
    the predictions. Therefore, if we know that all the sample of subset *A* have
    the same label *y[i]*, we are quite sure that all the corresponding predictions
    have been assigned to the same cluster. The completeness score for our example
    is:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 理由非常直观。当 *H(Y[pred]|Y[true]**) 低 *(c → 1)* 时，这意味着关于基真的知识减少了预测的不确定性。因此，如果我们知道子集
    *A* 中的所有样本都具有相同的标签 *y[i]*，我们相当确信所有相应的预测都被分配到了同一个簇中。我们示例的完整性得分为：
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Again, the value confirms our hypothesis. The residual uncertainty is due to
    a lack of completeness because a few samples with the same label have been split
    into blocks that are assigned to wrong clusters. It's obvious that a perfect scenario
    is characterized by having both homogeneity and completeness scores equal to 1.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这个值证实了我们的假设。剩余的不确定性是由于不完整性造成的，因为一些具有相同标签的样本被分割成块，并被分配到错误的簇中。很明显，一个完美的场景是同质性和完整性得分都等于1。
- en: Adjusted Rand Index
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整后的兰德指数
- en: 'This score is useful to compare the original label distribution with the clustering
    prediction. Ideally, we''d like to reproduce the exact ground truth distribution,
    but in general, this is very difficult in real-life scenarios. A way to measure
    the discrepancy is provided by the Adjusted Rand Index. In order to compute this
    score, we need to define the auxiliary variables:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分可以用来比较原始标签分布与聚类预测。理想情况下，我们希望重现精确的基真分布，但在现实生活中，这通常是非常困难的。调整后的兰德指数提供了一种衡量差异的方法。为了计算这个得分，我们需要定义辅助变量：
- en: '*a*: Number of sample pairs *(y[i], y[j])* that have the same true label and
    that are assigned to the same cluster'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*：具有相同真实标签且被分配到同一簇的样本对数 *(y[i], y[j]*)*'
- en: '*b*: Number of sample pairs *(y**[i], y[j]**)* that have a different true label
    and that are assigned to different clusters'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*：具有不同真实标签且被分配到不同簇的样本对数 *(y[i], y[j]*)*'
- en: 'The Rand Index is defined as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 兰德指数定义为：
- en: '![](img/992c113a-e970-4932-a0cc-e028e4932e15.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/992c113a-e970-4932-a0cc-e028e4932e15.png)'
- en: 'The Adjusted Rand Index is the Rand Index corrected for chance and it''s defined
    as:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的兰德指数是修正了偶然性的兰德指数，其定义为：
- en: '![](img/760ff071-962c-48e9-89fe-f22ed58b9a1e.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/760ff071-962c-48e9-89fe-f22ed58b9a1e.png)'
- en: 'The *R[A]* measure is bounded between *-1* and *1*. A value close to *-1* indicates
    a prevalence of wrong assignments, while a value close to *1* indicates that the
    clustering algorithm is correctly reproducing the ground truth distribution. The
    Adjusted Rand Score for our example is:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*R[A]*度量介于*-1*和*1*之间。一个接近*-1*的值表明错误分配占主导地位，而一个接近*1*的值表明聚类算法正确地再现了真实情况的分布。我们示例的调整后兰德指数如下：'
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This value confirms that the algorithm is working well (because it's positive),
    but it can be further optimized by trying to reduce the number of wrong assignments.
    The Adjusted Rand Score is a very powerful tool when the ground truth is known
    and can be employed as a single method to optimize all the hyperparameters.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值证实了算法运行良好（因为它为正值），但可以通过尝试减少错误分配的数量来进一步优化。当真实情况已知时，调整后兰德指数是一个非常强大的工具，可以作为单一方法来优化所有超参数。
- en: Silhouette score
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮廓得分
- en: 'This measure doesn''t need to know the ground truth and can be used to check,
    at the same time, the intra-cluster cohesion and the inter-cluster separation.
    In order to define the Silhouette score, we need to introduce two auxiliary functions.
    The first one is the average intra-cluster distance of a sample *x[i]* belonging
    to a cluster *C[j]*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量不需要知道真实情况，可以同时用来检查簇内凝聚力和簇间分离。为了定义轮廓得分，我们需要引入两个辅助函数。第一个是样本*x[i]*属于簇*C[j]*的平均簇内距离：
- en: '![](img/50c668b9-bd9f-4a5a-a257-bfb2bc5e6cc0.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50c668b9-bd9f-4a5a-a257-bfb2bc5e6cc0.png)'
- en: 'In the previous expression, *n(k)* is the number of samples assigned to the
    cluster *C[j]* and *d(a, b)* is a standard distance function (in the majority
    of cases, the Euclidean distance is chosen). We need also to define the lowest
    inter-cluster distance which can be interpreted as the average nearest-cluster
    distance. In the sample *x[i] ∈ C[j]*, let''s call *C[t]* the nearest cluster;
    therefore, the function is defined as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，*n(k)*是分配到簇*C[j]*的样本数量，*d(a, b)*是一个标准距离函数（在大多数情况下，选择欧几里得距离）。我们还需要定义最低的簇间距离，这可以解释为平均最近簇距离。在样本*x[i]
    ∈ C[j]*中，我们称*C[t]*为最近的簇；因此，该函数定义为：
- en: '![](img/8c3bf91c-6358-45db-8b22-adb7bb65fe27.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c3bf91c-6358-45db-8b22-adb7bb65fe27.png)'
- en: 'The Silhouette score for sample *x[i]* is:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 样本*x[i]*的轮廓得分是：
- en: '![](img/a7a3b0de-56da-4336-8f76-608d444341a6.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7a3b0de-56da-4336-8f76-608d444341a6.png)'
- en: 'The value of *s(x[i])*, like for the Adjusted Rand Index, is bounded between
    *-1* and *1*. A value close to *-1* indicates that *b(x[i]) << a(x[i])*, so the
    average intra-cluster distance is greater than the average nearest-cluster index
    and sample *x[i]* is wrongly assigned. Viceversa, a value close to *1* indicates
    that the algorithm achieved a very good level of internal cohesion and inter-cluster
    separation (because *a(x[i]) << b(x[i]**)*). Contrary to the other measure, the Silhouette
    score isn''t a cumulative function and must be computed for each sample. A feasible
    strategy is to analyze the average value, but in this way, it''s not possible
    to determine which clusters have the highest impact on the result. Another approach
    (the most common), is based on Silhouette plots, which display the score for each
    cluster in descending order. In the following snippet, we create plots for four
    different values of `n_clusters` (`3`, `5`, `10`, `12`):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*s(x[i])*的值，类似于调整后的兰德指数，介于*-1*和*1*之间。一个接近*-1*的值表明*b(x[i]) << a(x[i])*，因此平均簇内距离大于平均最近簇索引，样本*x[i]*被错误分配。反之，一个接近*1*的值表明算法达到了非常好的内部凝聚力和簇间分离水平（因为*a(x[i])
    << b(x[i]*)*）。与其它度量不同，轮廓得分不是一个累积函数，必须为每个样本单独计算。一种可行的策略是分析平均值，但这样无法确定哪些簇对结果影响最大。另一种方法（最常见的方法），是基于轮廓图，它按降序显示每个簇的得分。在下面的代码片段中，我们为四个不同的`n_clusters`值（`3`、`5`、`10`、`12`）创建图表：'
- en: '[PRE10]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result is shown in the following graph:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![](img/477a571a-9618-442a-8274-fd035aa2d80b.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/477a571a-9618-442a-8274-fd035aa2d80b.png)'
- en: Silhouette plots for different number of clusters
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 不同簇数的轮廓图
- en: 'The analysis of a Silhouette plot should follow some common guidelines:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 分析轮廓图应遵循一些常见的指导原则：
- en: The width of each block must be proportional to the number of samples that are
    expected to belong to the corresponding cluster. If the label distribution is
    uniform, all the blocks must have a similar width. Any asymmetry indicates wrong
    assignments. For example, in our case, we know that the right number of clusters
    is ten, but a couple of blocks are thinner than the other ones. This means that
    a cluster contains fewer samples than expected and the remaining ones have been
    assigned to wrong partitions.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个块的宽度必须与预期属于相应聚类的样本数量成比例。如果标签分布均匀，所有块必须具有相似的宽度。任何不对称性都表明有错误的分配。例如，在我们的案例中，我们知道正确的聚类数量是十个，但有一两个块比其他块要薄。这意味着一个聚类包含的样本比预期的要少，其余的样本被分配到了错误的分区。
- en: The shape of a block shouldn't be sharp and peaked (like a knife) because it
    means that many samples have a low Silhouette score. The ideal (realistic) scenario
    is made up of shapes similar to cigars with a minimum difference between the highest
    and lowest values. Unfortunately, this is not always possible to achieve, but
    it's always preferable to tune up the algorithm if the shapes are like the ones
    plotted in the first diagram (three clusters).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块的形状不应该尖锐和峰值（如刀片），因为这意味着许多样本具有较低的轮廓分数。理想的（现实的）场景是由类似雪茄的形状组成，最高值和最低值之间的差异最小。不幸的是，这并不总是可能实现，但如果形状类似于第一张图中绘制的形状（三个聚类），则始终
    preferable 调整算法。
- en: The maximum Silhouette score should be close to *1*. Lower values (like in our
    example) indicate the presence of partial overlaps and wrong assignments. Negative
    values must be absolutely avoided (or limited to a very small number of samples)
    because they show a failure in the clustering process. Moreover, it's possible
    to prove that convex clusters (like K-means hyperspheres) lead to higher values.
    This is due to the properties of the commons distance functions (like the Euclidean
    distance) that can suggest a low internal cohesion whenever the shape of a cluster
    is concave (think about a circle and a half-moon). In this case, the process of
    embedding the shape into a convex geometry leads to a lower density and this negatively
    affects the Silhouette score.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大轮廓分数应该接近*1*。较低的值（如我们的例子所示）表明存在部分重叠和错误的分配。必须绝对避免（或限制到非常少的样本）负值，因为它们表明聚类过程失败。此外，可以证明凸聚类（如K-means超球体）会导致更高的值。这是由于常用的距离函数（如欧几里得距离）的性质，当聚类形状是凹的时（想想一个圆和一个半月形），它可以暗示低内部凝聚力。在这种情况下，将形状嵌入到凸几何形状中的过程会导致较低的密度，这会负面影响轮廓分数。
- en: In our particular case,  we cannot accept having a number of clusters different
    from ten. However, the corresponding Silhouette plot is not perfect. We know the
    reasons for such imperfections (the structure of the samples and the high similarity
    of different digits) and it's quite difficult to avoid them using an algorithm
    like K-means. The reader can try to improve the performances by increasing the
    number of iterations, but in these cases, if the result doesn't meet the requirements,
    it's preferable to adopt another method (like the spectral clustering method,
    which can manage asymmetric clusters and more complex geometries).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们特定的案例中，我们不能接受有与十个不同的聚类数量。然而，相应的轮廓图并不完美。我们知道这种不完美的原因（样本的结构和不同数字之间的高度相似性）并且使用像K-means这样的算法很难避免它们。读者可以尝试通过增加迭代次数来提高性能，但如果结果不符合要求，最好采用另一种方法（如谱聚类方法，它可以处理非对称聚类和更复杂的几何形状）。
- en: Fuzzy C-means
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模糊C均值
- en: 'We have already talked about the difference between hard and soft clustering,
    comparing K-means with Gaussian mixtures. Another way to address this problem
    is based on the concept of **fuzzy logic**, which was proposed for the first time
    by Lotfi Zadeh in 1965 (for further details, a very good reference is *An Introduction
    to Fuzzy Sets*,* Pedrycz W.*, *Gomide F*., *The MIT Press*). Classic logic sets
    are based on the law of excluded middle that, in a clustering scenario, can be
    expressed by saying that a sample *x[i]* can belong only to a single cluster *c[j]*.
    Speaking more generally, if we split our universe into labeled partitions, a hard
    clustering approach will assign a label to each sample, while a fuzzy (or soft)
    approach allows managing a membership degree (in Gaussian mixtures, this is an
    actual probability), *w[ij]* which expresses how strong the relationship is between
    sample *x[i]* and cluster *c[j]*. Contrary to other methods, by employing fuzzy
    logic it''s possible to define asymmetric sets that are not representable with
    continuous functions (such as trapezoids). This allows for achieving further flexibility
    and an increased ability to adapt to more complex geometries. In the following
    graph, there''s an example of fuzzy sets:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了硬聚类和软聚类的区别，比较了K-means和高斯混合模型。解决这个问题的另一种方法是基于**模糊逻辑**的概念，该概念首次由Lotfi Zadeh于1965年提出（更多细节，请参考*《模糊集导论》*，作者：Pedrycz
    W.，Gomide F.，出版社：麻省理工学院出版社）。经典逻辑集基于排中律，在聚类场景中可以表述为样本 *x[i]* 只能属于单个聚类 *c[j]*。更普遍地说，如果我们将我们的宇宙划分为标记分区，硬聚类方法将为每个样本分配一个标签，而模糊（或软）方法允许管理一个成员度（在高斯混合中，这是一个实际的概率），*w[ij]*，它表达了样本
    *x[i]* 与聚类 *c[j]* 之间关系的强度。与其他方法不同，通过采用模糊逻辑，可以定义不对称的集合，这些集合不能用连续函数（如梯形）表示。这允许实现更大的灵活性，并提高了适应更复杂几何形状的能力。在下面的图中，有一个模糊集的示例：
- en: '![](img/4b18aefc-8dab-441f-84bc-0b28dcdb99e0.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4b18aefc-8dab-441f-84bc-0b28dcdb99e0.png)'
- en: Example of fuzzy sets representing the seniority level of an employee according
    to years of experience
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 根据工作经验表示员工资历级别的模糊集示例
- en: The graph represents the seniority level of an employee given his/her years
    of experience. As we want to cluster the entire population into three groups (**Junior**,
    **Middle level**, and **Senior**), three fuzzy sets have been designed. We have
    assumed that a young employee is keen and can quickly reach a **Junior** level
    after an initial apprenticeship period. The possibility to work with complex problems
    allows him/her to develop skills that are fundamental to allowing the transition
    between the **Junior** and **Middle** levels. After about **10** years, the employee
    can begin to consider himself/herself as a s*enior apprentice* and, after about
    25 years, the experience is enough to qualify him/her as a full **Senior** until
    the end of his/her career. As this is an imaginary example, we haven't tuned all
    the values up, but it's easy to compare, for example, employee A with 9 years
    of experience with another employee B with 18 years of experience. The former
    is about 50% **Junior** (decreasing), 90% **Middle level** (reaching its climax),
    and 10% **Senior** (increasing). The latter, instead, is 0% **Junior** (ending
    plateau), 30% **Middle level** (decreasing), and 60% **Senior** (increasing).
    In both cases, the values are not normalized so always sum up to 1 because we
    are more interested in showing the process and the proportions. The fuzziness
    level is lower in extreme cases, while it becomes higher when two sets intersect.
    For example, at about 15%, the **Middle level** and **Senior** are about 50%.
    As we're going to discuss, it's useful to avoid a very high fuzziness when clustering
    a dataset because it can lead to a lack of precision as the boundaries *fade out*,
    becoming completely fuzzy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Fuzzy C-means is a generalization of a standard K-means, with a soft assignment
    and more *flexible* clusters. The dataset to cluster (containing *M* samples)
    is represented by:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c07db279-134e-436f-ab76-279f2f5608da.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'If we assume we have *k* clusters, it''s necessary to define a matrix *W ∈ ℜ^(M ×
    k)* containing the membership degrees for each sample:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55c6d6da-90b6-40cd-9856-c9eb530a9f1d.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'Each degree *w[ij] ∈ [0, 1]* and all rows must be normalized so that they always
    sum up to *1*. In this way, the membership degrees can be considered as probabilities
    (with the same semantics) and it''s easier to make decisions with a prediction
    result. If a hard assignment is needed, it''s possible to employ the same approach
    normally used with Gaussian mixtures: the winning cluster is selected by applying
    the *argmax* function. However, it''s a good practice to employ soft clustering
    only when it''s possible to manage the vectorial output. For example, the probabilities/membership
    degrees can be fed into a classifier in order to yield more complex predictions.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'As with K-means, the problem can be expressed as the minimization of a *generalized
    inertia*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca5be8b-4cc2-478c-9707-13b0386c7184.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: The constant *m (m > 1)* is an exponent employed to re-weight the membership
    degrees. A value very close to *1* doesn't affect the actual values. Greater *m*
    values reduce their magnitude. The same parameter is also used when recomputing
    the centroids and the new membership degrees and can drive to a different clustering
    result. It's rather difficult to define a global acceptable value; therefore,
    a good practice is to start with an average *m* (for example, 1.5) and perform
    a grid search (it's possible to sample from a Gaussian or uniform distribution)
    until the desired accuracy has been achieved.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing the previous expression is even more difficult than with a standard
    inertia; therefore, a *pseudo-Lloyd''s algorithm* is employed. After a random
    initialization, the algorithm proceeds, alternating two steps (like an EM procedure)
    in order to determine the centroids, and recomputing the membership degrees to
    maximize the internal cohesion. The centroids are determined by a weighted average:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/895f3187-9892-4204-a793-64b072ae7f6c.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Contrary to K-means, the sum is not limited to the points belonging to a specific
    cluster because the weight factor will force the farthest points (*w[ij] ≈ 0.0*)
    to produce a contribution close to *0*. At the same time, as this is a soft-clustering
    algorithm, no exclusions are imposed, to allow a sample to belong to any number
    of clusters with different membership degrees. Once the centroids have been recomputed,
    the membership degrees must be updated using this formula:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b92fce2-7280-4c79-97f7-8a8ec3db26b8.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'This function behaves like a similarity. In fact, when sample *x[i]* is very
    close to centroid *μ[j]* (and relatively far from *μ[p]* with *p ≠ j*), the denominator
    becomes small and *w[ij]* increases. The exponent *m* directly influences the
    fuzzy partitioning, because when *m ≈ 1* *(m > 1)*, the denominator is a sum of
    *quasi-*squared terms and the closest centroid can dominate the sum, yielding
    to a higher preference for a specific cluster. When *m >> 1*, all the terms in
    the sum tend to *1*, producing a more flat weight distribution with no well-defined
    preference. It''s important to understand that, even when working with soft clustering,
    a fuzziness excess leads to inaccurate decisions because there are no factors
    that push a sample to clearly belong to a specific cluster. This means that problem
    is either ill-posed or, for example, the number of expected clusters is too high
    and doesn''t represent the real underlying data structure. A good way to measure
    how much this algorithm is similar to a hard-clustering approach (such as K-means)
    is provided by the normalized **Dunn''s partitioning coefficient**:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8895e7e0-576a-4fa9-87bd-4cfaad6a5d5a.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: When *P[c]* is bounded between *0* and *1*, when it's close to *0*, it means
    that the membership degrees have a flat distribution and the level of fuzziness
    is the highest possible. On the other side, if it's close to *1*, each row of
    *W* has a single dominant value, while all the others are negligible. This scenario
    resembles a hard-clustering approach. Higher *P[c]* values are normally preferable
    because, even without renouncing to a degree of fuzziness, it allows making more
    precise decisions. Considering the previous example, *P[c]* tends to *1* when
    the sets don't intersect, while it becomes 0 (complete fuzziness) if, for example,
    the three seniority levels are chosen to be identical and overlapping. Of course,
    we are interested in avoiding such extreme scenarios by limiting the number of
    borderline cases. A grid search can be performed by analyzing different numbers
    of clusters and *m* values (in the example, we're going to do it with the MNIST
    handwritten digit dataset). A reasonable rule of thumb is to accept *P[c]* values
    higher than *0.8*, but in some cases, that can be impossible. If we are sure that
    the problem is well-posed, the best approach is to choose the configuration that
    maximizes *P[c]*, considering, however, that a final value less than *0.3*-*0.5*
    will lead to a very high level of uncertainty because the clusters are extremely
    overlapping.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete **Fuzzy C-means** algorithm is:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Set a maximum number of iteration *N*[*max*]
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a tolerance *Thr*
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the value of *k* (number of expected clusters)
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the matrix *W^((0))* with random values and normalize each row, dividing
    it by its sum
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *N = 0*
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *N < N[max]* or *||W^((t)) - W^((t-1))|| > Thr*:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*N = N + 1*'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j = 1* to *k*:'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the centroid vectors *μ[j]*
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the weight matrix *W^((t))*
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the rows of *W^((t))*
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of fuzzy C-means with Scikit-Fuzzy
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Fuzzy ([http://pythonhosted.org/scikit-fuzzy/](http://pythonhosted.org/scikit-fuzzy/))
    is a Python package based on SciPy that allows implementing all the most important
    fuzzy logic algorithms (including fuzzy C-means). In this example, we continue
    using the MNIST dataset, but with a major focus on fuzzy partitioning. To perform
    the clustering, Scikit-Fuzzy implements the `cmeans` method (in the `skfuzzy.cluster`
    package) which requires a few mandatory parameters: `data`, which must be an array
    *D ∈ ℜ^(N × M)* (*N* is the number of features; therefore, the array used with
    Scikit-Learn must be transposed); `c`, the number of clusters; the coefficient
    `m`, `error`, which is the maximum tolerance; and `maxiter`, which is the maximum
    number of iterations. Another useful parameter (not mandatory) is the `seed` parameter
    which allows specifying the random seed to be able to easily reproduce the experiments.
    I invite the reader to check the official documentation for further information.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of this example is performing the clustering:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `cmeans` function returns many values, but for our purposes, the most important
    are: the first one, which is the array containing the cluster centroids; the second
    one, which is the final membership degree matrix; and the last one, the partition
    coefficient. In order to analyze the result, we can start with the partition coefficient:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This value informs us that the clustering is not very far from a hard assignment,
    but there''s still a residual fuzziness. In this particular case, such a situation
    may be reasonable because we know that many digits are partially distorted and
    may appear very similar to other ones (such as 1, 7, and 9). However, I invite
    the reader to try different values for `m` and check how the partition coefficient
    changes. We can now display the centroids:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e91fecf-26c7-4540-8336-10a38cdff61f.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Centroids obtained by fuzzy C-means
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'All the different digit classes have been successfully found, but now, contrary
    to K-means, we can check the fuzziness of a *problematic* digit (representing
    a 7, with index 7), as shown in the following diagram:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef72db33-3a1f-4635-86bc-21046f0489da.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
- en: Sample digit (a 7) selected to test the fuzziness
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'The membership degrees associated with the previous sample are:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The corresponding plot is:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1402c1fd-4710-4810-aa75-e6b65900da8b.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Fuzzy membership plot corresponding to a digit representing a 7
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the choice of *m* has forced the algorithm to reduce the fuzziness.
    However, it's still possible to see three smaller peaks corresponding to the clusters
    centered respectively on 1, 8, and 5 (remember that the cluster indexes correspond
    to digits shown previously in the centroid plot). I invite the reader to analyze
    the fuzzy partitioning of different digits and replot it with different values
    of the `m` parameter. It will be possible to observe an increased fuzziness (corresponding
    also to smaller partitioning coefficients) with larger *m* values. This effect
    is due to a stronger overlap among clusters (observable also by plotting the centroids)
    and could be useful when it's necessary to detect the distortion of a sample.
    In fact, even if the main peak indicates the right cluster, the secondary ones,
    in descending order, inform us how much the sample is similar to other centroids
    and, therefore, if it contains features that are characteristics of other subsets.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to Scikit-Learn, in order to perform predictions, Scikit-Fuzzy implements
    the `cmeans_predict` method (in the same package), which requires the same parameters
    of `cmeans`, but instead of the number of clusters, `c` needs the final centroid
    array (the name of the parameter is `cntr_trained`). The function returns as a
    first value the corresponding membership degree matrix (the other ones are the
    same as `cmeans`). In the following snippet, we repeat the prediction for the
    same sample digit (representing a `7`):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Scikit-Fuzzy can be installed using the `pip install -U scikit-fuzzy` command.
    For further instructions, please visit [http://pythonhosted.org/scikit-fuzzy/install.html](http://pythonhosted.org/scikit-fuzzy/install.html)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most common problems of K-means and other similar algorithms is
    the assumption we have only hyperspherical clusters. This condition can be acceptable
    when the dataset is split into blobs that can be easily embedded into a regular
    geometric structure. However, it fails whenever the sets are not separable using
    regular shapes. Let''s consider, for example, the following bidimensional dataset:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b518a8fc-a97e-4537-aa69-aab5e242a7a7.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Sinusoidal dataset
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are going to see in the example, any attempt to separate the upper sinusoid
    from the lower one using K-means will fail. The reason is quite obvious: a circle
    that contains the upper set will also contain part of the (or the whole) lower
    set. Considering the criterion adopted by K-means and imposing two clusters, the
    inertia will be minimized by a vertical separation corresponding to about *x[0]
    = 0*. Therefore, the resulting clusters are completely mixed and only a dimension
    is contributing to the final configuration. However, the two sinusoidal sets are
    well-separated and it''s not difficult to check that, selecting a point *x[i]*
    from the lower set, it''s always possible to find a ball containing only samples
    belonging to the same set. We have already discussed this kind of problem when
    Label Propagation algorithms were discussed and the logic behind **spectral clustering**
    is essentially the same (for further details, I invite the reader to check [Chapter
    2](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based Semi-Supervised Learning*).'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a dataset *X* sampled from a data generating process
    *p[data]*:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de0b8670-6120-4780-a06d-e9f5fed4fec7.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: 'We can build a graph *G = {V, E}*, where the vertices are the points and the
    edges are determined using an *affinity matrix* *W*. Each element *w[ij]* must
    express the affinity between sample *x[i]* and sample *x[j]*. *W* is normally
    built using two different approaches:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'KNN: In this case, we can build the number of neighbors to take into account
    for each point *x[i]*. *W* can be built as a *connectivity matrix* (expressing
    only the existence of a connection between two samples) if we adopt the criterion:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0c812c71-cd10-4c28-a03f-d1402b1bbe6a.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, it''s possible to build a *distance matrix*:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8461b117-636f-467c-9d5e-9e12604f3d72.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: '**Radial basis function** (**RBF**): The previous methods can lead to graphs
    which are not fully connected because samples can exist that have no neighbors.
    In order to obtain a fully connected graph, it''s possible to employ an RBF (this
    approach has also been used in the Kohonen map algorithm):'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a556e026-41e4-4fa9-b3fe-b29c1f4677d3.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: The *γ* parameter allows controlling the amplitude of the Gaussian function,
    reducing or increasing the number of samples with a high weight (so *actual neighbors*).
    However, a weight is assigned to all points and the resulting graph will always
    be connected (even if many elements are close to zero).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, the elements of *W* will represent a measure of affinity (or
    *closeness*) between points and no restrictions are imposed on the global geometry
    (contrary to K-means). In particular, using a KNN connectivity matrix, we are
    implicitly segmenting the original dataset into smaller regions with a high level
    of internal cohesion. The problem that we need to solve now is to find out a way
    to merge all the regions belonging to the same cluster. The approach we are going
    to present here has been proposed by *Normalized Cuts and Image Segmentation*,* J.
    Shi* and *J. Malik*,* IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    *Vol. 22*, *08/2000*, and it''s based on the normalized graph Laplacian:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78efd83b-f387-4263-af2c-bc0c8b5f32ee.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: 'The matrix *D*, called the degree matrix, is the same as discussed in [Chapter
    3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based Semi-Supervised Learning* and
    it''s defined as:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce6d4ad0-cc93-4eca-9301-48fac0602e25.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'It''s possible to prove the following properties (the formal proofs are omitted
    but they can be found in texts such as *Functions and Graphs Vol. 2*, *Gelfand
    I. M*., *Glagoleva E. G.*, *Shnol E. E.*, *The MIT Press*:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues *λ[i]* and the eigenvectors *v[i]* of *L[n]* can be found by
    solving the problem *Lv =* *λDv*, where *L* is the unnormalized graph Laplacian
    *L = D - W*
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L[n]* always has an eigenvalue equal to *0* (with a multiplicity *k*) with
    a corresponding eigenvector *v[o] = (1, 1, ..., 1)*'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As *G* is undirected and all *w[ij] ≥ 0*, the number of connected components
    *k* of *G* is equal to the multiplicity of the null eigenvalue
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, the normalized graph Laplacian encodes the information about
    the number of connected components and provides us with a new reference system
    where the clusters can be separated using regular geometric shapes (normally hyperspheres).
    To better understand how this approach works without a non-trivial mathematical
    approach, it's important to expose another property of *L[n]*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: From linear algebra, we know that each eigenvalue *λ* of a matrix *M ∈ ℜ^(n ×
    n)* spans a corresponding eigenspace, which is a subset of *ℜ^n* containing all
    eigenvectors associated with *λ* plus the null vector. Moreover, given a set *S*
    ⊆ *ℜn*and a countable subset *C* (it's possible to extend the definition to generic
    subsets but in our context the datasets are always countable), we can define a
    vector *v ∈ ℜ^n* as an *indicator vector*, if *v^((i)) = 1* if the vector *c[i] ∈
    S* and *v^((i)) = 0* otherwise. If we consider the null eigenvalues of *L[n]*
    and we assume that their number is *k* (corresponding to the multiplicity of the
    eigenvalue *0*), it's possible to prove that the corresponding eigenvectors are
    indicator vectors for eigenspaces spanned by each of them. From the previous statements,
    we know that these eigenspaces correspond to the connected components of the graph
    G; therefore, performing a standard clustering (like K-means or K-means++) with
    the points projected into these subspaces allows for an easy separation with symmetric
    shapes.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'As *L[n] ∈ ℜ^(M × M)*, its eigenvectors *v[i ]∈ ℜ^M*. Selecting the first *k*
    eigenvectors, it''s possible to build a matrix *A ∈ ℜ^(M × k)*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd015f48-904f-49bc-90c6-5ccf2d551d67.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: Each row of *A*, *a[j] ∈ ℜ^k* can be considered as the projection of an original
    sample *x[j]* in the low-dimensional subspace spanned by each of the null eigenvalues
    of *L[n]*. At this point, the separability of the new dataset *A = {a[j]}* depends
    only on the structure of the graph *G* and, in particular, on the number of neighbors
    or the *γ* parameter for RBFs. As in many other similar cases, it's impossible
    to define a standard value suitable for all problems, above all when the dimensionality
    doesn't allow a visual inspection. A reasonable approach should start with a small
    number of neighbors (for example, five) or *γ = 1.0* and increase the values until
    a performance metric (such as the Adjusted Rand Index) reaches its maximum. Considering
    the nature of the problems, it can also be useful to measure the homogeneity and
    the completeness because these two measures are more sensitive to irregular geometric
    structures and can easily show when the clustering is not separating the sets
    correctly. If the ground truth is unknown, the Silhouette score can be employed
    to assess the intra-cluster cohesion and the inter-cluster separation as functions
    of all hyperparameters (number of clusters, number of neighbors, or *γ*).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete **Shi-Malik spectral clustering** algorithm is:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Select a graph construction a method between KNN (1) and RBF (2):'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select parameter *k*
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select parameter *γ*
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the expected number of clusters *N*[*k*].
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrices *W* and *D*.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the normalized graph Laplacian *L[n]*.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the first k eigenvectors of *L[n]*.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the matrix *A*.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cluster the rows of *A* using K-means++ (or any other symmetric algorithm).
    The output of this process is this set of clusters: *C[km]^((1))*, *C[km]^((2))*,
    ..., *C[km]^((Nk))* .'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of spectral clustering with Scikit-Learn
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to use the sinusoidal dataset previously shown.
    The first step is creating it (with 1,000 samples):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'At this point, we can try to cluster it using K-means (with `n_clusters=2`):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result is shown in the following graph:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d66eb730-ffca-42d9-a3a7-c09c5906b581.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: K-means clustering result using the sinusoidal dataset
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, K-means isn''t able to separate the two sinusoids. The reader
    is free to try with different parameters, but the result will always be unacceptable
    because K-means bidimensional clusters are circles and no valid configurations
    exist. We can now employ spectral clustering using an affinity matrix based on
    the KNN algorithm (in this case, Scikit-Learn can produce a warning because the
    graph is not fully connected, but this normally doesn''t affect the results).
    Scikit-Learn implements the `SpectralClustering` class, whose most important parameters
    are `n_clusters`, the number of expected clusters; `affinity`, which can be either
    `''rbf''` or `''nearest_neighbors''`; `gamma` (only for RBF); and `n_neighbors`
    (only for KNN). For our test, we have chosen to have `20` neighbors:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result of the spectral clustering is shown in the following graph:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c68127e-0864-46fc-ad88-8bce7d0efe54.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: Spectral clustering result using the sinusoidal dataset
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the algorithm was able to separate the two sinusoids perfectly.
    As an exercise, I invite the reader to apply this method to the MNIST dataset,
    using both an RBF (with different gamma values) and KNN (with different numbers
    of neighbors). I also suggest to replot the t-SNE diagram and compare all the
    assignment errors. As the clusters are strictly non-convex, we don''t expect a
    high Silhouette score. Other useful exercises can be: drawing the Silhouette plot
    and checking the result, assigning ground truth labels, and measuring the homogeneity
    and the completeness.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we presented some fundamental clustering algorithms. We started
    with KNN, which is an instance-based method that restructures the dataset to find
    the most similar samples given a query point. We discussed three approaches: a
    naive one, which is also the most expensive in terms of computational complexity,
    and two strategies based respectively on the construction of a KD Tree and a Ball
    Tree. These two data structures can dramatically improve performance even when
    the number of samples is very large.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'The next topic was a classic algorithm: K-means, which is a symmetric partitioning
    strategy, comparable to a Gaussian mixture with variances close to zero, that
    can solve many real-life problems. We discussed both a vanilla algorithm, which
    wasn''t able to find a valid sub-optimal solution, and an optimized initialization
    method, called K-means++, which was able to speed up the convergence towards solutions
    quite close to the global minimum. In the same section, we also presented some
    evaluation methods that can be employed to assess the performance of a generic
    clustering algorithm.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: We also presented a soft-clustering method called fuzzy C-means, which resembles
    the structure of a standard K-means, but allows managing membership degrees (analogous
    to probabilities) that encode the similarity of a sample with all cluster centroids.
    This kind of approach allows processing the membership vectors in a more complex
    pipeline, where the output of a clustering process, for example, is fed into a
    classifier.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important limitations of K-means and similar algorithms is the
    symmetric structure of the clusters. This problem can be solved with methods such
    as spectral clustering, which is a very powerful approach based on the dataset
    graph and is quite similar to non-linear dimensionality reduction methods. We
    analyzed an algorithm proposed by Shi and Malik, showing how it can easily separate
    a non-convex dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 8](78baef9c-5391-4898-91bf-8df25330a163.xhtml),
    *Ensemble Learning*, we're going to discuss some common ensemble learning methods,
    which are based on the use of a large set of weak classifiers. We focused on their
    peculiarities, comparing the performances of different ensembles with single strong
    classifiers.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
