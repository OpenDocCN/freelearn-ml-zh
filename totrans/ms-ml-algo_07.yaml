- en: Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce some fundamental clustering algorithms,
    discussing both their strengths and weaknesses. The field of unsupervised learning,
    as well as any other machine learning approach, must be always based on the concept
    of Occam's razor. Simplicity must always be preferred when performance meets the
    requirements. However, in this case, the ground truth can be unknown. When a clustering
    algorithm is adopted as an exploratory tool, we can only assume that the dataset
    represents a precise data generating process. If this assumption is correct, the
    best strategy is to determine the number of clusters to maximize the internal
    cohesion (denseness) and the external separation. This means that we expect to
    find blobs (or isles) whose samples share some common and partially unique features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the algorithms we are going to present are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**k-Nearest Neighbors** (**KNN**) based on KD Trees and Ball Trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means and K-means++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuzzy C-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering based on the Shi-Malik algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-Nearest Neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This algorithm belongs to a particular family called **instance-based** (the
    methodology is called **instance-based learning**). It differs from other approaches
    because it doesn''t work with an actual mathematical model. On the contrary, the
    inference is performed by direct comparison of new samples with existing ones
    (which are defined as instances). KNN is an approach that can be easily employed
    to solve clustering, classification, and regression problems (even if, in this
    case, we are going to consider only the first technique). The main idea behind
    the clustering algorithm is very simple. Let''s consider a data generating process
    *p[data]* and a finite a dataset drawn from this distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec298b90-b592-4e50-9c92-e68076fde214.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each sample has a dimensionality equal to *N*. We can now introduce a distance
    function *d(x[1], x[2])*, which in the majority of cases can be generalized with
    the Minkowski distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70d36b7b-9ee9-4eba-aa27-4360263526e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When *p = 2, d[p]* represents the classical Euclidean distance, that is normally
    the default choice. In particular cases, it can be useful to employ other variants,
    such as *p = 1* (which is the Manhattan distance) or *p > 2*. Even if all the
    properties of a metric function remain unchanged, different values of *p* yield
    results that can be *semantically* diverse. As an example, we can consider the
    distance between points *x[1] = (0, 0)* and *x[2] = (15, 10)* as a function of
    *p*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d5b4c92-d564-4b54-85bb-3d61b20fd323.png)'
  prefs: []
  type: TYPE_IMG
- en: Minkowski distance between (0, 0) and (15, 10) as a function of parameter p
  prefs: []
  type: TYPE_NORMAL
- en: 'The distance decreases monotonically with *p* and converges to the largest
    component absolute difference, *|x[1]^((j)) - x[2]^((j))|*, when *p → ∞*. Therefore,
    whenever it''s important to weight all the components in the same way in order
    to have a consistent metric, small values of *p* are preferable (for example,
    *p=1* or *2*). This result has also been studied and formalized by Aggarwal, Hinneburg,
    and Keim (in *On the Surprising Behavior of Distance Metrics in High Dimensional
    Space*,*Aggarwal C. C.*, *Hinneburg A.*, *Keim D. A.*, *ICDT 2001*), who proved
    a fundamental inequality. If we consider a generic distribution *G* of *M* points
    *x[i] ∈ (0, 1)^d*, a distance function based on the *L[p]* norm, and the maximum
    *D[max]^p* and minimum *D[min]^p* distances (computed using the *L*[*p* ]norm)
    between two points, *x[j]* and *x*[*k*] drawn from *G* and *(0, 0)*, the following
    inequality holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56a0f65a-f7ba-4ec2-8edd-b701125f9255.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s clear that when the input dimensionality is very high and *p >> 2*, the
    expected value, *E[D[max]^p - D[min]^p]*, becomes bounded between two constants,
    *k[1] (C[p]d^(1/p-1/2))* and *k[2 ] ((M-1)C[p]d^(1/p-1/2)) → 0*, reducing the
    actual effect of almost any distance. In fact, given two generic couples of points
    *(x[1], x[2])* and *(x[3], x[4])*drawn from *G*, the natural consequence of the
    following inequality is that *d[p](x[1], x[2]) ≈ d[p](x[3], x[4]**)* when *p →
    ∞*, independently of their relative positions. This important result confirms
    the importance of choosing the right metric according to the dimensionality of
    the dataset and that *p = 1* is the best choice when *d >> 1*, while *p >> 1*
    can produce inconsistent results due the ineffectiveness of the metric. To see
    direct confirmation of this phenomenon, it''s possible to run the following snippet,
    which computes the average difference between maximum and minimum distances considering
    `100` sets containing `100` samples drawn from a uniform distribution, *G ∼ U(0,
    1)*. In the snippet, the case of `d=2`, `100`, `1000` is analyzed with Minkowski
    metrics with `P`= `1`, `2`, `10`, `100` (the final values depend on the random
    seed and how many times the experiment is repeated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A particular case, that is a direct consequence of the previous inequality is
    when the largest absolute difference between components determines the most important
    factor of a distance, large values of *p* can be employed. For example, if we
    consider three points, *x[1]** = (0, 0)*, *x**[2]** = (15, 10)*, and *x[3] = (15,
    0)*, *d[2](x[1], x[2]) ≈ 18* and *d[2](x[1], x[3]) = 15*. So, if we set a threshold
    at *d = 16* centered at *x[1]*, *x*[*2* ]is outside the boundaries. If instead *p
    = 15*, both distances become close to *15* and the two points (*x[2]* and *x[3]*)
    are inside the boundaries. A particular use of large values of *p* is when it's
    important to take into account the inhomogeneity among components. For example,
    some feature vectors can represent the age and height of a set of people. Considering
    a test person *x = (30, 175)*, with large *p* values, the distances between *x*
    and two samples *(35, 150)* and *(25, 151)* are almost identical (about *25.0*),
    and the only dominant factor becomes the height difference (independent from the
    age).
  prefs: []
  type: TYPE_NORMAL
- en: 'The KNN algorithm determines the *k* closest samples of each training point.
    When a new sample is presented, the procedure is repeated with two possible variants:'
  prefs: []
  type: TYPE_NORMAL
- en: With a predefined value of *k*, the KNN are computed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a predefined radius/threshold *r*, all the neighbors whose distance is
    less than or equal to the radius are computed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The philosophy of KNN is that similar samples can share their features. For
    example, a recommendation system can cluster users using this algorithm and, given
    a new user, find the most similar ones (based, for example, on the products they
    bought) to recommend the same category of items. In general, a similarity function
    is defined as the reciprocal of a distance (there are some exceptions, such as
    the cosine similarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca9f5020-6d8d-41d1-8dc7-a6e6e42ab4a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Two different users, *A* and *B*, who are classified as neighbors, will differ
    under some viewpoints, but, at the same time, they will share some peculiar features.
    This statement authorizes us to increase the homogeneity by *suggesting the differences*.
    For example, if *A* liked book *b[1]* and *B* liked *b[2]*, we can recommend *b*[*1* ]to
    *B* and *b[2]* to *A*. If our hypothesis was correct, the similarity between *A*
    and *B* will be increased; otherwise, the two users will move towards other clusters
    that better represent their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the *vanilla* algorithm (in Scikit-Learn it is called the **brute-force**
    algorithm) can become extremely slow with a large number of samples because it's
    necessary to compute all the pairwise distances in order to answer any query.
    With *M* points, this number is equal to *M²*, which is often unacceptable (if
    *M* = 1,000, each query needs to compute a million distances). More precisely,
    as the computation of a distance in an N-dimensional space requires *N* operations,
    the total complexity becomes *O(M²N)*, which can be reasonable only for small
    values of both *M* and *N*. That's why some important strategies have been implemented
    to reduce the computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: KD Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As all KNN queries can be considered search problems, one of the most efficient
    way to reduce the overall complexity is to reorganize the dataset into a tree
    structure. In a binary tree (one-dimensional data), the average computational
    complexity of a query is *O(log M)*, because we assume we have almost the same
    number of elements in each branch (if the tree is completely unbalanced, all the
    elements are inserted sequentially and the resulting structure has a single branch, 
    so the complexity becomes *O(M))*. In general, the real complexity is slightly
    higher than *O(log M)*, but the operation is always much more efficient than a
    vanilla search, which is *O(M²)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we normally work with N-dimensional data and the previous structure
    cannot be immediately employed. KD Trees extend the concept of a binary for *N
    >* *1*. In this case, a split cannot be immediately performed and a different
    strategy must be chosen. The easiest way to solve this problem is to select a
    feature at each level *(1, 2, ..., N)* and repeat the process until the desired
    depth is reached. In the following diagram, there''s an example of KD Trees with
    three-dimensional points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74005f85-2f8e-424a-be82-9c6f72f74ae3.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of three-dimensional KD Tree
  prefs: []
  type: TYPE_NORMAL
- en: The root is point **(5, 3, 7)**. The first split is performed considering the
    first feature, so two children are **(2, 1, 1)** and **(8, 4, 3)**. The second
    one operates on the second feature and so on. The average computational complexity
    is *O(N log M)*, but if the distribution is very asymmetric, the probability that
    the tree becomes unbalanced is very high. To mitigate this issue, it's possible
    to select the feature corresponding to the median of the (sub-)dataset and to
    continue splitting with this criterion. In this way, the tree is guaranteed to
    be balanced. However, the average complexity is always proportional to the dimensionality
    and this can dramatically affect the performance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if *M* = 10,000 and *N* = 10, using the *log[10]*, *O(**N log M)
    = O(40)*, while, with *N* = 1,000, the complexity becomes *O(40,000)*. Generally,
    KD Trees suffers the *curse of dimensionality* and when *N* becomes large, the
    average complexity is about *O(MN)*, which is always better than the *vanilla*
    algorithm, but often too expensive for real-life applications.  Therefore, KD
    Trees is really effective only when the dimensionality is not too high. In all
    other cases, the probability of having an unbalanced tree and the resulting computational
    complexity suggest employing a different method.
  prefs: []
  type: TYPE_NORMAL
- en: Ball Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An alternative to KD Trees is provided by **Ball Trees**. The idea is to rearrange
    the dataset in a way that is almost insensitive to high-dimensional samples. A
    ball is defined as a set of points whose distance from a center sample is less
    than or equal to a fixed radius:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13fcde74-8f62-45d7-a0db-f74613cc6670.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Starting from the first main ball, it''s possible to build smaller ones nested
    into the parent ball and stop the process when the desired depth has been reached.
    A fundamental condition is that a point can always belong to a single ball. In
    this way, considering the cost of the N-dimensional distance, the computational
    complexity is *O(N log M)* and doesn''t suffer the curse of dimensionality like
    KD Trees. The structure is based on hyperspheres, whose boundaries are defined
    by the equations (given a center point *x* and a radius *R[i]*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22f759e3-1db7-473c-a070-6d18716385e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the only operation needed to find the right ball is measuring the
    distance between a sample and the centers starting from the smallest balls. If
    a point is outside the ball, it''s necessary to move upwards and check the parents,
    until the ball containing the sample is found. In the following diagram, there''s
    an example of Ball Trees with two levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b0af978-f4d6-4bdc-99e5-396ba696ef36.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Ball Trees with seven bidimensional points and two levels
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the seven bidimensional points are split first into two balls
    containing respectively three and four points. At the second level, the second
    ball is split again into two smaller balls containing two points each. This procedure
    can be repeated until a fixed depth is reached or by imposing the maximum number
    of elements that a leaf must contain (in this case, it can be equal to *3*).
  prefs: []
  type: TYPE_NORMAL
- en: Both KD Trees and Ball Trees can be efficient structures to reduce the complexity
    of KNN queries. However, when fitting a model, it's important to consider both
    the *k parameter* (which normally represents the average or the standard number
    of neighbors computed in a query) and the maximum tree depth. These particular
    structures are not employed for common tasks (such as sorting) and their efficiency
    is maximized when all the requested neighbors can be found in the same sub-structure
    (with a size *K<< M*, to avoid an implicit fallback to the *vanilla* algorithm).
    In other words, the tree has the role of reducing the dimensionality of the search
    space by partitioning it into reasonably small regions.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, if the number of samples contained in a leaf is small, the
    number of tree nodes grows and the complexity is subsequently increased. The negative
    impact is doubled because on average it's necessary to explore more nodes and
    if *k* is much greater than the number of elements contained in a node, it's necessary
    to merge the samples belonging to different nodes. On the other side, a very large
    number of samples per node leads to a condition that is close to the *vanilla*
    algorithm. For example, if *M* = 1,000 and each node contains 250 elements, once
    the right node is computed, the number of distances to compute is comparable with
    the initial dataset size and no real advantage is achieved by employing a tree
    structure. An acceptable practice is to set the size of a life equal to *5 ÷ 10*
    times the average value of *k*, to maximize the probability to find all the neighbors
    inside the same leaf. However, every specific problem must be analyzed (while
    also benchmarking the performances) in order to find the most appropriate value.
    If different values for *k* are necessary, it's important to consider the relative
    frequencies of the queries. For example, if a program needs 10 *5-NN* queries
    and 1 *50-NN* query, it's probably better to set a leaf size equal to 25, even
    if the *50-NN* query will be more expensive. In fact, setting a good value for
    a second query (for example, 200) will dramatically increase the complexity of
    the first 10 queries, driving to a performance loss.
  prefs: []
  type: TYPE_NORMAL
- en: Example of KNN with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to test the KNN algorithm, we are going to use the MNIST handwritten
    digit dataset provided directly by Scikit-Learn. It is made up of 1,797 8 × 8
    grayscale images representing the digits from 0 to 9\. The first step is loading
    it and normalizing all the values to be bounded between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The dictionary `digits` contains both the images, `digits[''images'']`, and
    the flattened 64-dimensional arrays, `digits[''data'']`. Scikit-Learn implements
    different classes (for example, it''s possible to work directly with KD Trees
    and Ball Trees using the KDTree and BallTree classes) that can be used in the
    context of KNN (as clustering, classification, and regression algorithms). However,
    we''re going to employ the main class, `NearestNeighbors`, which allows performing
    clustering and queries based either on the number of neighbors or on the radius
    of a ball centered on a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have chosen to have a default number of neighbors equal to `50` and an algorithm
    based on a `ball_tree`. The leaf size (`leaf_size`) parameter has been kept to
    its default value equal to `30`. We have also employed the default metric (Euclidean),
    but it's possible to change it using the `metric` and `p` parameters (which is
    the order of the Minkowski metric). Scikit-Learn supports all the metrics implemented
    by SciPy in the `scipy.spatial.distance` package. However, in the majority of
    cases, it's sufficient to use a Minkowski metric and adjust the value of `p` if
    the results are not acceptable with any number of neighbors. Other metrics, such
    as the cosine distance, can be employed when the similarity must not be affected
    by the Euclidean distance, but only by the angle between two vectors pointing
    at the samples. Applications that use this metric include, for example, deep learning
    models for natural language processing, where the words are embedded into feature
    vectors whose semantic similarity is proportional to their Cosine distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now query the model in order to find 50 neighbors of a sample. For our
    purposes, we have selected the sample with index 100, which represents a 4 (the
    images have a very low resolution, but it''s always possible to distinguish the
    digit):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ff7c51f-cf3f-4943-ba4c-66f73b368ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample digit used to query the KNN model
  prefs: []
  type: TYPE_NORMAL
- en: 'The query can be performed using the instance method `kneighbors`, which allows
    specifying the number of neighbors (`n_neighbors` parameter the default is the
    value selected during the instantiation of the class) and whether we want to also
    get the distances of each neighbor (the `return_distance` parameter). In this
    example, we are also interested in evaluating *how far* the neighbors are from
    the center, so we set `return_distance=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The first neighbor is always the center, so its distance is `0`. The other
    ones range from 0.9 to 1.9\. Considering that, in this case, the maximum possible
    distance is 8 (between a 64-dimensional vector *a = (1, 1, ..., 1)* and the null
    vector), the result could be acceptable. In order to get confirmation, we can
    plot the neighbors as bidimensional 8 × 8 arrays (the returned array, `neighbors`,
    contains the indexes of the samples). The result is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da803e16-8f68-468b-95ff-b477a2594189.png)'
  prefs: []
  type: TYPE_IMG
- en: 50 neighbors selected by the KNN model
  prefs: []
  type: TYPE_NORMAL
- en: As it's possible to see, there are no errors, but all the shapes are slightly
    different. In particular, the last one, which is also the farthest, has a lot
    of white pixels (corresponding to the value 1.0), explaining the reason of a distance
    equal to about 2.0\. I invite the reader to test the `radius_neighbors` method
    until spurious values appear among the results. It's also interesting to try this
    algorithm with the Olivetti faces dataset, whose complexity is higher and many
    more geometrical parameters can influence the similarity.
  prefs: []
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we discussed the Gaussian mixture algorithm, we defined it as *Soft K-means*.
    The reason is that each cluster was represented by three elements: mean, variance,
    and weight. Each sample always belongs to all clusters with a probability provided
    by the Gaussian distributions. This approach can be very useful when it''s possible
    to manage the probabilities as weights, but in many other situations, it''s preferable
    to determine a single cluster per sample. Such an approach is called hard clustering
    and K-means can be considered the hard version of a Gaussian mixture. In fact,
    when all variances *Σ[i] → 0*, the distributions degenerate to Dirac''s Deltas,
    which represent perfect spikes centered at a specific point. In this scenario,
    the only possibility to determine the most appropriate cluster is to find the
    shortest distance between a sample point and all the centers (from now on, we
    are going to call them *centroids*). This approach is also based on an important
    double principle that should be taken into account in every clustering algorithm.
    The clusters must be set up to maximize:'
  prefs: []
  type: TYPE_NORMAL
- en: The intra-cluster cohesion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inter-cluster separation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means that we expect to label high-density regions that are well separated
    from each other. When this is not possible, the criterion must try to minimize
    the intra-cluster average distance between samples and centroid. This quantity
    is also called *inertia* and it''s defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5909e8f-cde4-4615-be26-c9981119b84d.png)'
  prefs: []
  type: TYPE_IMG
- en: High levels of inertia imply low cohesion because there are probably too many
    points belongings to clusters whose centroids are too far away. The problem can
    be solved by minimizing the previous quantity. However, the computational complexity
    needed to find the global minimum is exponential (K-means belongs to the class
    of NP-Hard problems). The alternative approach employed by the K-means algorithm,
    also known as **Lloyd's algorithm**, is iterative and starts from selecting *k*
    random centroids (in the next section, we're going to analyze a more efficient
    method) and adjusting them until their configuration becomes stable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset to cluster (with *M* samples) is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf1fde0d-5770-444d-8b44-b80daebea84a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An initial guess for the centroids is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8e117ce-ddea-4bc9-8eb8-63cc585ac54b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are no particular restrictions on the initial values. However, the choice
    can influence both the convergence speed and the minimum that is found. The iterative
    procedure will loop over the dataset, computing the Euclidean distance between
    *x*[*i* ]and each *μ[j]* and assigning a cluster based on the criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a387604a-6b26-42a4-994a-6b7b4e32e3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once all the samples have been clustered, the new centroids are computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11d25a25-0e01-491b-8c64-8ef6a3dd42f8.png)'
  prefs: []
  type: TYPE_IMG
- en: The quantity *N[Cj]* represents the number of points belonging to cluster *j*.
    At this point, the inertia is recomputed and the new value is compared with the
    previous one. The procedure will stop either after a fixed number of iterations
    or when the variations in the inertia become smaller than a predefined threshold.
    Lloyd's algorithm is very similar to a particular case of the EM algorithm. In
    fact, the first step of each iteration is the computation of an *expectation*
    (the centroid configuration), while the second step maximizes the intra-cluster
    cohesion by minimizing the inertia.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete vanilla K-means algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set a maximum number of iterations *N*[*max*].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a tolerance *Thr*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the value of *k* (number of expected clusters).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize vector *C^((0))* with random values. They can be points belonging
    to the dataset or sampled from a suitable distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the initial inertia *S*^(*(0)*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *N = 0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *N < N[max]* or *||S^((t)) - S^((t-1))|| > Thr*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*N = N + 1*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *x[i]* in *X*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign *x[i]* to a cluster using the shortest distance between *x[i]* and *μ*[*j*]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the centroid vector *C*^(*(t)*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the inertia *S*^(*(t)*)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm is quite simple and intuitive, and there are many real-life applications
    based on it. However, there are two important elements to consider. The first
    one is the convergence speed. It's easy to show that every initial guess drives
    to a convergence point, but the number of iterations is dramatically influenced
    by this choice and there's no guarantee to find the global minimum. If the initial
    centroids are close to the final ones, the algorithm needs only a few steps to
    correct the values, but when the choice is totally random, it's not uncommon to
    need a very high number of iterations. If there are *N* samples and *k* centroids,
    *Nk* distances must be computed at each iteration, leading to an inefficient result.
    In the next paragraph, we'll show how it's possible to initialize the centroids
    to minimize the convergence time.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect is that, contrary to KNN, K-means needs to predefine
    the number of expected clusters. In some cases, this is a secondary problem because
    we already know the most appropriate value for k. However, when the dataset is
    high-dimensional and our knowledge is limited, this choice could be hazardous.
    A good approach to solve the issue is to analyze the final inertia for a different
    number of clusters. As we expect to maximize the intra-cluster cohesion, a small
    number of clusters will lead to an increased inertia. We try to pick the highest
    point below a maximum tolerable value. Theoretically, we can also pick *k = N*.
    In this case, the inertia becomes zero because each point represents the centroid
    of its cluster, but a large value for *k* transforms the clustering scenario into
    a fine-grained partitioning that might not be the best strategy to capture the
    feature of a consistent group. It's impossible to define a rule for the upper
    bound *k[max]*, but we assume that this value is always much less than *N*. The
    best choice is achieved by selecting *k* to minimize the inertia, selecting the
    values from a set bounded, for example, between *2* and *k[max]*.
  prefs: []
  type: TYPE_NORMAL
- en: K-means++
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have said that a good choice for the initial centroids can improve the convergence
    speed and leads to a minimum that is closer to the global optimum of the inertia
    S. Arthur and Vassilvitskii (in *The Advantages of Careful Seeding, **Arthur,
    D., Vassilvitskii S., k-means++:* *Proceedings of the Eighteenth Annual ACM-SIAM
    Symposium on Discrete Algorithms*) proposed a method called K-means++, which allows
    increasing the accuracy of the initial centroid guess considering the most likely
    final configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to expose the algorithm, it''s useful to introduce a function, *D(x,
    i)*, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e91fd7c6-9b48-409b-8445-b5850263705f.png)'
  prefs: []
  type: TYPE_IMG
- en: '*D(x, i)* defines the shortest distance between each sample and one of the
    centroids already selected. As the process is incremental, this function must
    be recomputed after all steps. For our purposes, let''s also define an auxiliary
    probability distribution (we omit the index variable for simplicity):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52eda745-0e5f-4253-8612-9d9bff3f739c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first centroid *μ[0]* is sampled from *X* using a uniform distribution.
    The next steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute *D(x, i)* for all *x ∈ X* considering the centroids already selected
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *G(x)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the next centroid *μ[i]* from *X* with a probability *G(x)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the aforementioned paper, the authors showed a very important property.
    If we define *S^** as the global optimum of *S*, a K-means++ initialization determines
    an upperbound for the expected value of the actual inertia:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/414461c4-d6c3-4280-82bf-0c33ce065114.png)'
  prefs: []
  type: TYPE_IMG
- en: This condition is often expressed by saying that K-means++ is *O(log k)*-competitive.
    When *k* is sufficiently small, the probability of finding a local minimum close
    to the global one increases. However, K-means++ is still a probabilistic approach
    and different initializations on the same dataset lead to different initial configurations.
    A good practice is to run a limited number of initializations (for example, ten)
    and pick the one associated with the smallest inertia. When training complexity
    is not a primary issue, this number can be increased, but different experiments
    showed that the improvement achievable with a very large number of trials is negligible
    when compared to the actual computational cost. The default value in Scikit-Learn
    is ten and the author suggests to keep this value in the majority of cases. If
    the result continues to be poor, it's preferable to pick another method. Moreover,
    there are problems that cannot be solved using K-means (even with the best possible
    initialization), because one of the assumptions of the algorithm is that each
    cluster is a hypersphere and the distances are measured using a Euclidean function.
    In the following sections, we're going to analyze other algorithms that are not
    constrained to work with such limitations and can easily solve clustering problems
    using asymmetric cluster geometries.
  prefs: []
  type: TYPE_NORMAL
- en: Example of K-means with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we continue using the MNIST dataset (the `X_train` array is
    the same defined in the paragraph dedicated to KNN), but we want also to analyze
    different clustering evaluation methods. The first step is visualizing the inertia
    corresponding to different numbers of clusters. We are going to use the `KMeans` class,
    which accepts the `n_clusters` parameter and employs the K-means++ initialization
    as the default method (as explained in the previous section, in order to find
    the best initial configuration, Scikit-Learn performs several attempts and selects
    the configuration with the lowest inertia; it''s possible to change the number
    of attempts through the `n_iter` parameter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We are supposing to analyze the range [`2`, `20`]. After each training session,
    the final inertia can be retrieved using the `inertia_` instance variable. The
    following graph shows the plot of the values as a function of the number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d09423dd-2a55-40ea-b9b1-e14ef6cf4059.png)'
  prefs: []
  type: TYPE_IMG
- en: Inertia as a function of the number of clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the function is decreasing, starting from a value of about 7,500
    and reaching about 3,700 with **20** clusters. In this case, we know that the
    real number is **10**, but it''s possible to discover it by observing the trend.
    The slope is quite high before **10**, but it starts decreasing more and more
    slowly after this threshold. This is a signal that informs us that some clusters
    are not well separated, even if their internal cohesion is high. In order to confirm
    this hypothesis, we can set `n_clusters=10` and, first of all, check the centroids
    at the end of the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The centroids are available through the `cluster_centers_` instance variable.
    In the following screenshot, there''s a plot of the corresponding bidimensional
    arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89dd07e7-a96b-41dc-aabe-580fbdaa62d4.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means centroid at the end of the training process
  prefs: []
  type: TYPE_NORMAL
- en: 'All the digits are present and there are no duplicates. This confirms that
    the algorithm has successfully separated the sets, but the final inertia (which
    is about 4,500) informs us that there are probably wrong assignments. To obtain
    confirmation, we can plot the dataset using a dimensionality-reduction method,
    such as t-SNE (see [Chapter 3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based
    Semi-Supervised Learning* for further details):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can plot the bidimensional dataset with the corresponding
    cluster labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40643b02-fd5f-442d-849b-1231625cd23a.png)'
  prefs: []
  type: TYPE_IMG
- en: t-SNE representation of the MNIST dataset; the labels correspond to the clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot confirms that the dataset is made up of well-separated blobs, but
    a few samples are assigned to the wrong cluster (this is not surprising considering
    the similarity between some pairs of digits). An important observation can further
    explain the trend of the inertia. In fact, the point where the slope changes almost
    abruptly corresponds to 9 clusters. Observing the t-SNE plot, we can immediately
    discover the reason: the cluster corresponding to the digit **7** is indeed split
    into 3 blocks. The main one contains the majority of samples, but there are another
    2 smaller blobs that are wrongly *attached* to clusters **1** and **9**. This
    is not surprising, considering that the digit **7** can be very similar to a distorted
    **1** or **9**. However, these two spurious blobs are always at the boundaries
    of the wrong clusters (remember that the geometric structures are hyperspheres),
    confirming that the metric has successfully detected a low similarity. If a group
    of wrongly assigned samples were in the middle of a cluster, it would have meant
    that the separation failed dramatically and another method should be employed.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, it's impossible to evaluate the performance of a clustering algorithm
    using only a visual inspection. Moreover, it's important to use standard objective
    metrics that allow for comparing different approaches. We are now going to introduce
    some methods based on the knowledge of the ground truth (the correct assignment
    for each sample) and one common strategy employed when the true labels are unknown.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before discussing the scoring functions, we need to introduce a standard notation.
    If there are *k* clusters, we define the true labels as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/200a6236-e235-436e-88cf-31d7b3d481ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, we can define the predicted labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89db817b-4b6b-43bd-995b-3a2191f0ec35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Both sets can be considered as sampled from two discrete random variables (for
    simplicity, we denote them with the same names), whose probability mass functions
    are *P[true](y)* and *P[pred](y)* with a generic *y* *∈ {y[1], y[2], ..., y[k]}*
    (*y[i]* represents the index of the *i^(th)* cluster). These two probabilities
    can be approximated with a frequency count; so, for example, the probability *P[true](1)*
    is computed as the number of samples whose true label is *1 n[true](1)* over the
    total number of samples *M*. In this way, we can define the entropies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/993508fd-4259-4fc0-848b-7e3104f105ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These quantities describe the intrinsic uncertainty of the random variables.
    They are maximized when all the classes have the same probability, while, for
    example, they are null if all the samples belong to a single class (minimum uncertainty).
    We also need to know the uncertainty of a random variable *Y* given another one
    *X*. This can be achieved using the conditional entropy *H(Y|X)*. In this case,
    we need to compute the joint probability *p(x, y)* because the definition of *H(Y|X)*
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1dc59e67-4559-4193-bd3b-8abea170fd2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to approximate the previous expression, we can define the function
    *n(i[true], j[pred])*, which counts the number of samples with the true label
    *i* assigned to cluster *j*. In this way, if there are *M* samples, the approximated
    conditional entropies become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d0accfc-3ec0-4701-b822-b7c83718e106.png)'
  prefs: []
  type: TYPE_IMG
- en: Homogeneity score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This score is useful to check whether the clustering algorithm meets an important
    requirement: a cluster should contain only samples belonging to a single class.
    It''s defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b43c6cea-4195-4e57-bbde-3147395cdb21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s bounded between *0* and *1*, with low values indicating a low homogeneity.
    In fact, when the knowledge of *Y[pred]* reduces the uncertainty of *Y[true]*,
    *H(Y[true]|Y[pred])* becomes smaller (*h → 1*) and viceversa. For our example,
    the homogeneity score can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `digits['target']` array contains the true labels while `Y` contains the
    predictions (all the functions we are going to use accept the true labels as the
    first parameter and the predictions as the second one). The homogeneity score
    confirms that the clusters are rather homogeneous, but there's still a moderate
    level of uncertainty because some clusters contain wrong assignments. This method,
    together with the other ones, can be used to search for the right number of clusters
    and tune up all supplementary hyperparameters (such as the number of iterations
    or the metric function).
  prefs: []
  type: TYPE_NORMAL
- en: Completeness score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This score is complementary to the previous one. Its purpose is to provide
    a piece of information about the assignment of samples belonging to the same class.
    More precisely, a good clustering algorithm should assign all samples with the
    same true label to the same cluster. From our previous analysis, we know that,
    for example, the digit 7 has been wrongly assigned to both clusters 9 and 1; therefore,
    we expect a non-perfect completeness score. The definition is symmetric to the
    homogeneity score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad248be3-4fcd-4d42-b5bd-1ccbdf582556.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The rationale is very intuitive. When *H(Y[pred]|Y[true]**)* is low *(c → 1)*,
    it means that the knowledge of the ground truth reduces the uncertainty about
    the predictions. Therefore, if we know that all the sample of subset *A* have
    the same label *y[i]*, we are quite sure that all the corresponding predictions
    have been assigned to the same cluster. The completeness score for our example
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Again, the value confirms our hypothesis. The residual uncertainty is due to
    a lack of completeness because a few samples with the same label have been split
    into blocks that are assigned to wrong clusters. It's obvious that a perfect scenario
    is characterized by having both homogeneity and completeness scores equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted Rand Index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This score is useful to compare the original label distribution with the clustering
    prediction. Ideally, we''d like to reproduce the exact ground truth distribution,
    but in general, this is very difficult in real-life scenarios. A way to measure
    the discrepancy is provided by the Adjusted Rand Index. In order to compute this
    score, we need to define the auxiliary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*: Number of sample pairs *(y[i], y[j])* that have the same true label and
    that are assigned to the same cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*: Number of sample pairs *(y**[i], y[j]**)* that have a different true label
    and that are assigned to different clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Rand Index is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/992c113a-e970-4932-a0cc-e028e4932e15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Adjusted Rand Index is the Rand Index corrected for chance and it''s defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/760ff071-962c-48e9-89fe-f22ed58b9a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *R[A]* measure is bounded between *-1* and *1*. A value close to *-1* indicates
    a prevalence of wrong assignments, while a value close to *1* indicates that the
    clustering algorithm is correctly reproducing the ground truth distribution. The
    Adjusted Rand Score for our example is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This value confirms that the algorithm is working well (because it's positive),
    but it can be further optimized by trying to reduce the number of wrong assignments.
    The Adjusted Rand Score is a very powerful tool when the ground truth is known
    and can be employed as a single method to optimize all the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This measure doesn''t need to know the ground truth and can be used to check,
    at the same time, the intra-cluster cohesion and the inter-cluster separation.
    In order to define the Silhouette score, we need to introduce two auxiliary functions.
    The first one is the average intra-cluster distance of a sample *x[i]* belonging
    to a cluster *C[j]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50c668b9-bd9f-4a5a-a257-bfb2bc5e6cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, *n(k)* is the number of samples assigned to the
    cluster *C[j]* and *d(a, b)* is a standard distance function (in the majority
    of cases, the Euclidean distance is chosen). We need also to define the lowest
    inter-cluster distance which can be interpreted as the average nearest-cluster
    distance. In the sample *x[i] ∈ C[j]*, let''s call *C[t]* the nearest cluster;
    therefore, the function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c3bf91c-6358-45db-8b22-adb7bb65fe27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Silhouette score for sample *x[i]* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7a3b0de-56da-4336-8f76-608d444341a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The value of *s(x[i])*, like for the Adjusted Rand Index, is bounded between
    *-1* and *1*. A value close to *-1* indicates that *b(x[i]) << a(x[i])*, so the
    average intra-cluster distance is greater than the average nearest-cluster index
    and sample *x[i]* is wrongly assigned. Viceversa, a value close to *1* indicates
    that the algorithm achieved a very good level of internal cohesion and inter-cluster
    separation (because *a(x[i]) << b(x[i]**)*). Contrary to the other measure, the Silhouette
    score isn''t a cumulative function and must be computed for each sample. A feasible
    strategy is to analyze the average value, but in this way, it''s not possible
    to determine which clusters have the highest impact on the result. Another approach
    (the most common), is based on Silhouette plots, which display the score for each
    cluster in descending order. In the following snippet, we create plots for four
    different values of `n_clusters` (`3`, `5`, `10`, `12`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/477a571a-9618-442a-8274-fd035aa2d80b.png)'
  prefs: []
  type: TYPE_IMG
- en: Silhouette plots for different number of clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'The analysis of a Silhouette plot should follow some common guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: The width of each block must be proportional to the number of samples that are
    expected to belong to the corresponding cluster. If the label distribution is
    uniform, all the blocks must have a similar width. Any asymmetry indicates wrong
    assignments. For example, in our case, we know that the right number of clusters
    is ten, but a couple of blocks are thinner than the other ones. This means that
    a cluster contains fewer samples than expected and the remaining ones have been
    assigned to wrong partitions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape of a block shouldn't be sharp and peaked (like a knife) because it
    means that many samples have a low Silhouette score. The ideal (realistic) scenario
    is made up of shapes similar to cigars with a minimum difference between the highest
    and lowest values. Unfortunately, this is not always possible to achieve, but
    it's always preferable to tune up the algorithm if the shapes are like the ones
    plotted in the first diagram (three clusters).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum Silhouette score should be close to *1*. Lower values (like in our
    example) indicate the presence of partial overlaps and wrong assignments. Negative
    values must be absolutely avoided (or limited to a very small number of samples)
    because they show a failure in the clustering process. Moreover, it's possible
    to prove that convex clusters (like K-means hyperspheres) lead to higher values.
    This is due to the properties of the commons distance functions (like the Euclidean
    distance) that can suggest a low internal cohesion whenever the shape of a cluster
    is concave (think about a circle and a half-moon). In this case, the process of
    embedding the shape into a convex geometry leads to a lower density and this negatively
    affects the Silhouette score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our particular case,  we cannot accept having a number of clusters different
    from ten. However, the corresponding Silhouette plot is not perfect. We know the
    reasons for such imperfections (the structure of the samples and the high similarity
    of different digits) and it's quite difficult to avoid them using an algorithm
    like K-means. The reader can try to improve the performances by increasing the
    number of iterations, but in these cases, if the result doesn't meet the requirements,
    it's preferable to adopt another method (like the spectral clustering method,
    which can manage asymmetric clusters and more complex geometries).
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy C-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already talked about the difference between hard and soft clustering,
    comparing K-means with Gaussian mixtures. Another way to address this problem
    is based on the concept of **fuzzy logic**, which was proposed for the first time
    by Lotfi Zadeh in 1965 (for further details, a very good reference is *An Introduction
    to Fuzzy Sets*,* Pedrycz W.*, *Gomide F*., *The MIT Press*). Classic logic sets
    are based on the law of excluded middle that, in a clustering scenario, can be
    expressed by saying that a sample *x[i]* can belong only to a single cluster *c[j]*.
    Speaking more generally, if we split our universe into labeled partitions, a hard
    clustering approach will assign a label to each sample, while a fuzzy (or soft)
    approach allows managing a membership degree (in Gaussian mixtures, this is an
    actual probability), *w[ij]* which expresses how strong the relationship is between
    sample *x[i]* and cluster *c[j]*. Contrary to other methods, by employing fuzzy
    logic it''s possible to define asymmetric sets that are not representable with
    continuous functions (such as trapezoids). This allows for achieving further flexibility
    and an increased ability to adapt to more complex geometries. In the following
    graph, there''s an example of fuzzy sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b18aefc-8dab-441f-84bc-0b28dcdb99e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of fuzzy sets representing the seniority level of an employee according
    to years of experience
  prefs: []
  type: TYPE_NORMAL
- en: The graph represents the seniority level of an employee given his/her years
    of experience. As we want to cluster the entire population into three groups (**Junior**,
    **Middle level**, and **Senior**), three fuzzy sets have been designed. We have
    assumed that a young employee is keen and can quickly reach a **Junior** level
    after an initial apprenticeship period. The possibility to work with complex problems
    allows him/her to develop skills that are fundamental to allowing the transition
    between the **Junior** and **Middle** levels. After about **10** years, the employee
    can begin to consider himself/herself as a s*enior apprentice* and, after about
    25 years, the experience is enough to qualify him/her as a full **Senior** until
    the end of his/her career. As this is an imaginary example, we haven't tuned all
    the values up, but it's easy to compare, for example, employee A with 9 years
    of experience with another employee B with 18 years of experience. The former
    is about 50% **Junior** (decreasing), 90% **Middle level** (reaching its climax),
    and 10% **Senior** (increasing). The latter, instead, is 0% **Junior** (ending
    plateau), 30% **Middle level** (decreasing), and 60% **Senior** (increasing).
    In both cases, the values are not normalized so always sum up to 1 because we
    are more interested in showing the process and the proportions. The fuzziness
    level is lower in extreme cases, while it becomes higher when two sets intersect.
    For example, at about 15%, the **Middle level** and **Senior** are about 50%.
    As we're going to discuss, it's useful to avoid a very high fuzziness when clustering
    a dataset because it can lead to a lack of precision as the boundaries *fade out*,
    becoming completely fuzzy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fuzzy C-means is a generalization of a standard K-means, with a soft assignment
    and more *flexible* clusters. The dataset to cluster (containing *M* samples)
    is represented by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c07db279-134e-436f-ab76-279f2f5608da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we assume we have *k* clusters, it''s necessary to define a matrix *W ∈ ℜ^(M ×
    k)* containing the membership degrees for each sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55c6d6da-90b6-40cd-9856-c9eb530a9f1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each degree *w[ij] ∈ [0, 1]* and all rows must be normalized so that they always
    sum up to *1*. In this way, the membership degrees can be considered as probabilities
    (with the same semantics) and it''s easier to make decisions with a prediction
    result. If a hard assignment is needed, it''s possible to employ the same approach
    normally used with Gaussian mixtures: the winning cluster is selected by applying
    the *argmax* function. However, it''s a good practice to employ soft clustering
    only when it''s possible to manage the vectorial output. For example, the probabilities/membership
    degrees can be fed into a classifier in order to yield more complex predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with K-means, the problem can be expressed as the minimization of a *generalized
    inertia*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cca5be8b-4cc2-478c-9707-13b0386c7184.png)'
  prefs: []
  type: TYPE_IMG
- en: The constant *m (m > 1)* is an exponent employed to re-weight the membership
    degrees. A value very close to *1* doesn't affect the actual values. Greater *m*
    values reduce their magnitude. The same parameter is also used when recomputing
    the centroids and the new membership degrees and can drive to a different clustering
    result. It's rather difficult to define a global acceptable value; therefore,
    a good practice is to start with an average *m* (for example, 1.5) and perform
    a grid search (it's possible to sample from a Gaussian or uniform distribution)
    until the desired accuracy has been achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing the previous expression is even more difficult than with a standard
    inertia; therefore, a *pseudo-Lloyd''s algorithm* is employed. After a random
    initialization, the algorithm proceeds, alternating two steps (like an EM procedure)
    in order to determine the centroids, and recomputing the membership degrees to
    maximize the internal cohesion. The centroids are determined by a weighted average:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/895f3187-9892-4204-a793-64b072ae7f6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Contrary to K-means, the sum is not limited to the points belonging to a specific
    cluster because the weight factor will force the farthest points (*w[ij] ≈ 0.0*)
    to produce a contribution close to *0*. At the same time, as this is a soft-clustering
    algorithm, no exclusions are imposed, to allow a sample to belong to any number
    of clusters with different membership degrees. Once the centroids have been recomputed,
    the membership degrees must be updated using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b92fce2-7280-4c79-97f7-8a8ec3db26b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This function behaves like a similarity. In fact, when sample *x[i]* is very
    close to centroid *μ[j]* (and relatively far from *μ[p]* with *p ≠ j*), the denominator
    becomes small and *w[ij]* increases. The exponent *m* directly influences the
    fuzzy partitioning, because when *m ≈ 1* *(m > 1)*, the denominator is a sum of
    *quasi-*squared terms and the closest centroid can dominate the sum, yielding
    to a higher preference for a specific cluster. When *m >> 1*, all the terms in
    the sum tend to *1*, producing a more flat weight distribution with no well-defined
    preference. It''s important to understand that, even when working with soft clustering,
    a fuzziness excess leads to inaccurate decisions because there are no factors
    that push a sample to clearly belong to a specific cluster. This means that problem
    is either ill-posed or, for example, the number of expected clusters is too high
    and doesn''t represent the real underlying data structure. A good way to measure
    how much this algorithm is similar to a hard-clustering approach (such as K-means)
    is provided by the normalized **Dunn''s partitioning coefficient**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8895e7e0-576a-4fa9-87bd-4cfaad6a5d5a.png)'
  prefs: []
  type: TYPE_IMG
- en: When *P[c]* is bounded between *0* and *1*, when it's close to *0*, it means
    that the membership degrees have a flat distribution and the level of fuzziness
    is the highest possible. On the other side, if it's close to *1*, each row of
    *W* has a single dominant value, while all the others are negligible. This scenario
    resembles a hard-clustering approach. Higher *P[c]* values are normally preferable
    because, even without renouncing to a degree of fuzziness, it allows making more
    precise decisions. Considering the previous example, *P[c]* tends to *1* when
    the sets don't intersect, while it becomes 0 (complete fuzziness) if, for example,
    the three seniority levels are chosen to be identical and overlapping. Of course,
    we are interested in avoiding such extreme scenarios by limiting the number of
    borderline cases. A grid search can be performed by analyzing different numbers
    of clusters and *m* values (in the example, we're going to do it with the MNIST
    handwritten digit dataset). A reasonable rule of thumb is to accept *P[c]* values
    higher than *0.8*, but in some cases, that can be impossible. If we are sure that
    the problem is well-posed, the best approach is to choose the configuration that
    maximizes *P[c]*, considering, however, that a final value less than *0.3*-*0.5*
    will lead to a very high level of uncertainty because the clusters are extremely
    overlapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete **Fuzzy C-means** algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set a maximum number of iteration *N*[*max*]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a tolerance *Thr*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the value of *k* (number of expected clusters)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the matrix *W^((0))* with random values and normalize each row, dividing
    it by its sum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *N = 0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *N < N[max]* or *||W^((t)) - W^((t-1))|| > Thr*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*N = N + 1*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j = 1* to *k*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the centroid vectors *μ[j]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the weight matrix *W^((t))*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the rows of *W^((t))*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of fuzzy C-means with Scikit-Fuzzy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-Fuzzy ([http://pythonhosted.org/scikit-fuzzy/](http://pythonhosted.org/scikit-fuzzy/))
    is a Python package based on SciPy that allows implementing all the most important
    fuzzy logic algorithms (including fuzzy C-means). In this example, we continue
    using the MNIST dataset, but with a major focus on fuzzy partitioning. To perform
    the clustering, Scikit-Fuzzy implements the `cmeans` method (in the `skfuzzy.cluster`
    package) which requires a few mandatory parameters: `data`, which must be an array
    *D ∈ ℜ^(N × M)* (*N* is the number of features; therefore, the array used with
    Scikit-Learn must be transposed); `c`, the number of clusters; the coefficient
    `m`, `error`, which is the maximum tolerance; and `maxiter`, which is the maximum
    number of iterations. Another useful parameter (not mandatory) is the `seed` parameter
    which allows specifying the random seed to be able to easily reproduce the experiments.
    I invite the reader to check the official documentation for further information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step of this example is performing the clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cmeans` function returns many values, but for our purposes, the most important
    are: the first one, which is the array containing the cluster centroids; the second
    one, which is the final membership degree matrix; and the last one, the partition
    coefficient. In order to analyze the result, we can start with the partition coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This value informs us that the clustering is not very far from a hard assignment,
    but there''s still a residual fuzziness. In this particular case, such a situation
    may be reasonable because we know that many digits are partially distorted and
    may appear very similar to other ones (such as 1, 7, and 9). However, I invite
    the reader to try different values for `m` and check how the partition coefficient
    changes. We can now display the centroids:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e91fecf-26c7-4540-8336-10a38cdff61f.png)'
  prefs: []
  type: TYPE_IMG
- en: Centroids obtained by fuzzy C-means
  prefs: []
  type: TYPE_NORMAL
- en: 'All the different digit classes have been successfully found, but now, contrary
    to K-means, we can check the fuzziness of a *problematic* digit (representing
    a 7, with index 7), as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef72db33-3a1f-4635-86bc-21046f0489da.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample digit (a 7) selected to test the fuzziness
  prefs: []
  type: TYPE_NORMAL
- en: 'The membership degrees associated with the previous sample are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding plot is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1402c1fd-4710-4810-aa75-e6b65900da8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Fuzzy membership plot corresponding to a digit representing a 7
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the choice of *m* has forced the algorithm to reduce the fuzziness.
    However, it's still possible to see three smaller peaks corresponding to the clusters
    centered respectively on 1, 8, and 5 (remember that the cluster indexes correspond
    to digits shown previously in the centroid plot). I invite the reader to analyze
    the fuzzy partitioning of different digits and replot it with different values
    of the `m` parameter. It will be possible to observe an increased fuzziness (corresponding
    also to smaller partitioning coefficients) with larger *m* values. This effect
    is due to a stronger overlap among clusters (observable also by plotting the centroids)
    and could be useful when it's necessary to detect the distortion of a sample.
    In fact, even if the main peak indicates the right cluster, the secondary ones,
    in descending order, inform us how much the sample is similar to other centroids
    and, therefore, if it contains features that are characteristics of other subsets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to Scikit-Learn, in order to perform predictions, Scikit-Fuzzy implements
    the `cmeans_predict` method (in the same package), which requires the same parameters
    of `cmeans`, but instead of the number of clusters, `c` needs the final centroid
    array (the name of the parameter is `cntr_trained`). The function returns as a
    first value the corresponding membership degree matrix (the other ones are the
    same as `cmeans`). In the following snippet, we repeat the prediction for the
    same sample digit (representing a `7`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Scikit-Fuzzy can be installed using the `pip install -U scikit-fuzzy` command.
    For further instructions, please visit [http://pythonhosted.org/scikit-fuzzy/install.html](http://pythonhosted.org/scikit-fuzzy/install.html)
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most common problems of K-means and other similar algorithms is
    the assumption we have only hyperspherical clusters. This condition can be acceptable
    when the dataset is split into blobs that can be easily embedded into a regular
    geometric structure. However, it fails whenever the sets are not separable using
    regular shapes. Let''s consider, for example, the following bidimensional dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b518a8fc-a97e-4537-aa69-aab5e242a7a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Sinusoidal dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are going to see in the example, any attempt to separate the upper sinusoid
    from the lower one using K-means will fail. The reason is quite obvious: a circle
    that contains the upper set will also contain part of the (or the whole) lower
    set. Considering the criterion adopted by K-means and imposing two clusters, the
    inertia will be minimized by a vertical separation corresponding to about *x[0]
    = 0*. Therefore, the resulting clusters are completely mixed and only a dimension
    is contributing to the final configuration. However, the two sinusoidal sets are
    well-separated and it''s not difficult to check that, selecting a point *x[i]*
    from the lower set, it''s always possible to find a ball containing only samples
    belonging to the same set. We have already discussed this kind of problem when
    Label Propagation algorithms were discussed and the logic behind **spectral clustering**
    is essentially the same (for further details, I invite the reader to check [Chapter
    2](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based Semi-Supervised Learning*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a dataset *X* sampled from a data generating process
    *p[data]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de0b8670-6120-4780-a06d-e9f5fed4fec7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can build a graph *G = {V, E}*, where the vertices are the points and the
    edges are determined using an *affinity matrix* *W*. Each element *w[ij]* must
    express the affinity between sample *x[i]* and sample *x[j]*. *W* is normally
    built using two different approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: 'KNN: In this case, we can build the number of neighbors to take into account
    for each point *x[i]*. *W* can be built as a *connectivity matrix* (expressing
    only the existence of a connection between two samples) if we adopt the criterion:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0c812c71-cd10-4c28-a03f-d1402b1bbe6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, it''s possible to build a *distance matrix*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8461b117-636f-467c-9d5e-9e12604f3d72.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Radial basis function** (**RBF**): The previous methods can lead to graphs
    which are not fully connected because samples can exist that have no neighbors.
    In order to obtain a fully connected graph, it''s possible to employ an RBF (this
    approach has also been used in the Kohonen map algorithm):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a556e026-41e4-4fa9-b3fe-b29c1f4677d3.png)'
  prefs: []
  type: TYPE_IMG
- en: The *γ* parameter allows controlling the amplitude of the Gaussian function,
    reducing or increasing the number of samples with a high weight (so *actual neighbors*).
    However, a weight is assigned to all points and the resulting graph will always
    be connected (even if many elements are close to zero).
  prefs: []
  type: TYPE_NORMAL
- en: 'In both cases, the elements of *W* will represent a measure of affinity (or
    *closeness*) between points and no restrictions are imposed on the global geometry
    (contrary to K-means). In particular, using a KNN connectivity matrix, we are
    implicitly segmenting the original dataset into smaller regions with a high level
    of internal cohesion. The problem that we need to solve now is to find out a way
    to merge all the regions belonging to the same cluster. The approach we are going
    to present here has been proposed by *Normalized Cuts and Image Segmentation*,* J.
    Shi* and *J. Malik*,* IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    *Vol. 22*, *08/2000*, and it''s based on the normalized graph Laplacian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78efd83b-f387-4263-af2c-bc0c8b5f32ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix *D*, called the degree matrix, is the same as discussed in [Chapter
    3](c23d1792-167f-416e-a848-fa7a10777697.xhtml), *Graph-Based Semi-Supervised Learning* and
    it''s defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce6d4ad0-cc93-4eca-9301-48fac0602e25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s possible to prove the following properties (the formal proofs are omitted
    but they can be found in texts such as *Functions and Graphs Vol. 2*, *Gelfand
    I. M*., *Glagoleva E. G.*, *Shnol E. E.*, *The MIT Press*:'
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues *λ[i]* and the eigenvectors *v[i]* of *L[n]* can be found by
    solving the problem *Lv =* *λDv*, where *L* is the unnormalized graph Laplacian
    *L = D - W*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L[n]* always has an eigenvalue equal to *0* (with a multiplicity *k*) with
    a corresponding eigenvector *v[o] = (1, 1, ..., 1)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As *G* is undirected and all *w[ij] ≥ 0*, the number of connected components
    *k* of *G* is equal to the multiplicity of the null eigenvalue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, the normalized graph Laplacian encodes the information about
    the number of connected components and provides us with a new reference system
    where the clusters can be separated using regular geometric shapes (normally hyperspheres).
    To better understand how this approach works without a non-trivial mathematical
    approach, it's important to expose another property of *L[n]*.
  prefs: []
  type: TYPE_NORMAL
- en: From linear algebra, we know that each eigenvalue *λ* of a matrix *M ∈ ℜ^(n ×
    n)* spans a corresponding eigenspace, which is a subset of *ℜ^n* containing all
    eigenvectors associated with *λ* plus the null vector. Moreover, given a set *S*
    ⊆ *ℜn*and a countable subset *C* (it's possible to extend the definition to generic
    subsets but in our context the datasets are always countable), we can define a
    vector *v ∈ ℜ^n* as an *indicator vector*, if *v^((i)) = 1* if the vector *c[i] ∈
    S* and *v^((i)) = 0* otherwise. If we consider the null eigenvalues of *L[n]*
    and we assume that their number is *k* (corresponding to the multiplicity of the
    eigenvalue *0*), it's possible to prove that the corresponding eigenvectors are
    indicator vectors for eigenspaces spanned by each of them. From the previous statements,
    we know that these eigenspaces correspond to the connected components of the graph
    G; therefore, performing a standard clustering (like K-means or K-means++) with
    the points projected into these subspaces allows for an easy separation with symmetric
    shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As *L[n] ∈ ℜ^(M × M)*, its eigenvectors *v[i ]∈ ℜ^M*. Selecting the first *k*
    eigenvectors, it''s possible to build a matrix *A ∈ ℜ^(M × k)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd015f48-904f-49bc-90c6-5ccf2d551d67.png)'
  prefs: []
  type: TYPE_IMG
- en: Each row of *A*, *a[j] ∈ ℜ^k* can be considered as the projection of an original
    sample *x[j]* in the low-dimensional subspace spanned by each of the null eigenvalues
    of *L[n]*. At this point, the separability of the new dataset *A = {a[j]}* depends
    only on the structure of the graph *G* and, in particular, on the number of neighbors
    or the *γ* parameter for RBFs. As in many other similar cases, it's impossible
    to define a standard value suitable for all problems, above all when the dimensionality
    doesn't allow a visual inspection. A reasonable approach should start with a small
    number of neighbors (for example, five) or *γ = 1.0* and increase the values until
    a performance metric (such as the Adjusted Rand Index) reaches its maximum. Considering
    the nature of the problems, it can also be useful to measure the homogeneity and
    the completeness because these two measures are more sensitive to irregular geometric
    structures and can easily show when the clustering is not separating the sets
    correctly. If the ground truth is unknown, the Silhouette score can be employed
    to assess the intra-cluster cohesion and the inter-cluster separation as functions
    of all hyperparameters (number of clusters, number of neighbors, or *γ*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete **Shi-Malik spectral clustering** algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select a graph construction a method between KNN (1) and RBF (2):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select parameter *k*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select parameter *γ*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the expected number of clusters *N*[*k*].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the matrices *W* and *D*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the normalized graph Laplacian *L[n]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the first k eigenvectors of *L[n]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the matrix *A*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cluster the rows of *A* using K-means++ (or any other symmetric algorithm).
    The output of this process is this set of clusters: *C[km]^((1))*, *C[km]^((2))*,
    ..., *C[km]^((Nk))* .'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of spectral clustering with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to use the sinusoidal dataset previously shown.
    The first step is creating it (with 1,000 samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can try to cluster it using K-means (with `n_clusters=2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d66eb730-ffca-42d9-a3a7-c09c5906b581.png)'
  prefs: []
  type: TYPE_IMG
- en: K-means clustering result using the sinusoidal dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, K-means isn''t able to separate the two sinusoids. The reader
    is free to try with different parameters, but the result will always be unacceptable
    because K-means bidimensional clusters are circles and no valid configurations
    exist. We can now employ spectral clustering using an affinity matrix based on
    the KNN algorithm (in this case, Scikit-Learn can produce a warning because the
    graph is not fully connected, but this normally doesn''t affect the results).
    Scikit-Learn implements the `SpectralClustering` class, whose most important parameters
    are `n_clusters`, the number of expected clusters; `affinity`, which can be either
    `''rbf''` or `''nearest_neighbors''`; `gamma` (only for RBF); and `n_neighbors`
    (only for KNN). For our test, we have chosen to have `20` neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the spectral clustering is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c68127e-0864-46fc-ad88-8bce7d0efe54.png)'
  prefs: []
  type: TYPE_IMG
- en: Spectral clustering result using the sinusoidal dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the algorithm was able to separate the two sinusoids perfectly.
    As an exercise, I invite the reader to apply this method to the MNIST dataset,
    using both an RBF (with different gamma values) and KNN (with different numbers
    of neighbors). I also suggest to replot the t-SNE diagram and compare all the
    assignment errors. As the clusters are strictly non-convex, we don''t expect a
    high Silhouette score. Other useful exercises can be: drawing the Silhouette plot
    and checking the result, assigning ground truth labels, and measuring the homogeneity
    and the completeness.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we presented some fundamental clustering algorithms. We started
    with KNN, which is an instance-based method that restructures the dataset to find
    the most similar samples given a query point. We discussed three approaches: a
    naive one, which is also the most expensive in terms of computational complexity,
    and two strategies based respectively on the construction of a KD Tree and a Ball
    Tree. These two data structures can dramatically improve performance even when
    the number of samples is very large.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next topic was a classic algorithm: K-means, which is a symmetric partitioning
    strategy, comparable to a Gaussian mixture with variances close to zero, that
    can solve many real-life problems. We discussed both a vanilla algorithm, which
    wasn''t able to find a valid sub-optimal solution, and an optimized initialization
    method, called K-means++, which was able to speed up the convergence towards solutions
    quite close to the global minimum. In the same section, we also presented some
    evaluation methods that can be employed to assess the performance of a generic
    clustering algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: We also presented a soft-clustering method called fuzzy C-means, which resembles
    the structure of a standard K-means, but allows managing membership degrees (analogous
    to probabilities) that encode the similarity of a sample with all cluster centroids.
    This kind of approach allows processing the membership vectors in a more complex
    pipeline, where the output of a clustering process, for example, is fed into a
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important limitations of K-means and similar algorithms is the
    symmetric structure of the clusters. This problem can be solved with methods such
    as spectral clustering, which is a very powerful approach based on the dataset
    graph and is quite similar to non-linear dimensionality reduction methods. We
    analyzed an algorithm proposed by Shi and Malik, showing how it can easily separate
    a non-convex dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 8](78baef9c-5391-4898-91bf-8df25330a163.xhtml),
    *Ensemble Learning*, we're going to discuss some common ensemble learning methods,
    which are based on the use of a large set of weak classifiers. We focused on their
    peculiarities, comparing the performances of different ensembles with single strong
    classifiers.
  prefs: []
  type: TYPE_NORMAL
