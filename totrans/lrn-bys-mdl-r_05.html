<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Bayesian Regression Models" id="aid-173721"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Bayesian Regression Models</h1></div></div></div><p>In the previous chapter, we covered the theory of Bayesian linear regression in some detail. In this chapter, we will take a sample problem and illustrate how it can be applied to practical situations. For this purpose, we will use the <a id="id265" class="indexterm"/>
<span class="strong"><strong>generalized linear model</strong></span> (<span class="strong"><strong>GLM</strong></span>) packages in R. Firstly, we will give a brief introduction to the concept of GLM to the readers.</p><div class="section" title="Generalized linear regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Generalized linear regression</h1></div></div></div><p>Recall<a id="id266" class="indexterm"/> that in linear regression, we assume the following functional form between the dependent variable <span class="emphasis"><em>Y</em></span> and independent variable <span class="emphasis"><em>X</em></span>:</p><div class="mediaobject"><img src="../Images/image00438.jpeg" alt="Generalized linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00439.jpeg" alt="Generalized linear regression"/></span> is a set of basis functions and <span class="inlinemediaobject"><img src="../Images/image00440.jpeg" alt="Generalized linear regression"/></span> is the parameter vector. Usually, it is assumed that <span class="inlinemediaobject"><img src="../Images/image00441.jpeg" alt="Generalized linear regression"/></span>, so <span class="inlinemediaobject"><img src="../Images/image00442.jpeg" alt="Generalized linear regression"/></span> represents an intercept or a bias term. Also, it is assumed that <span class="inlinemediaobject"><img src="../Images/image00443.jpeg" alt="Generalized linear regression"/></span> is a noise term distributed according to the normal distribution with mean zero and variance <span class="inlinemediaobject"><img src="../Images/image00444.jpeg" alt="Generalized linear regression"/></span>. We also showed that this results in the following equation:</p><div class="mediaobject"><img src="../Images/image00445.jpeg" alt="Generalized linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>One can <a id="id267" class="indexterm"/>generalize the preceding equation to incorporate not only the normal distribution for noise but any distribution in the exponential family (reference 1 in the <span class="emphasis"><em>References</em></span> section of this chapter). This is done by defining the following equation:</p><div class="mediaobject"><img src="../Images/image00446.jpeg" alt="Generalized linear regression"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>g</em></span> is called a link function. The well-known models, such as logistic regression, log-linear models, Poisson regression, and so on, are special cases of GLM. For example, in the case of ordinary linear regression, the link function would be <span class="inlinemediaobject"><img src="../Images/image00447.jpeg" alt="Generalized linear regression"/></span>. For logistic regression, it is <span class="inlinemediaobject"><img src="../Images/image00448.jpeg" alt="Generalized linear regression"/></span>, which is the inverse of the logistic function, and for Poisson regression, it is <span class="inlinemediaobject"><img src="../Images/image00449.jpeg" alt="Generalized linear regression"/></span>.</p><p>In the Bayesian formulation of GLMs, unlike ordinary linear regression, there are no closed-form analytical solutions. One needs to specify prior probabilities for the regression coefficients. Then, their posterior probabilities are typically obtained through Monte Carlo simulations.</p></div></div>
<div class="section" title="The arm package" id="aid-181NK1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec36"/>The arm package</h1></div></div></div><p>In this <a id="id268" class="indexterm"/>chapter, for the purpose of illustrating Bayesian regression models, we will use the <span class="strong"><strong>arm</strong></span> package of R. This package was developed by Andrew Gelman and co-workers, and it can be downloaded from the website at <a class="ulink" href="http://CRAN.R-project.org/package=arm">http://CRAN.R-project.org/package=arm</a>.</p><p>The arm package has the <code class="literal">bayesglm</code> function that implements the Bayesian generalized linear model with an independent normal, t, or Cauchy prior distributions, for the model coefficients. We will <a id="id269" class="indexterm"/>use this function to build Bayesian regression models.</p></div>
<div class="section" title="The Energy efficiency dataset" id="aid-190861"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>The Energy efficiency dataset</h1></div></div></div><p>We will use the <a id="id270" class="indexterm"/>Energy efficiency dataset from the UCI Machine Learning repository for the illustration of Bayesian regression (reference 2 in the <span class="emphasis"><em>References</em></span> section of this chapter). The dataset can be downloaded from the website at <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Energy+efficiency">http://archive.ics.uci.edu/ml/datasets/Energy+efficiency</a>. The dataset contains the measurements of energy efficiency of buildings with different building parameters. There are two energy efficiency parameters measured: heating load (<span class="emphasis"><em>Y1</em></span>) and cooling load (<span class="emphasis"><em>Y2</em></span>).</p><p>The building parameters used are: relative compactness (<span class="emphasis"><em>X1</em></span>), surface area (<span class="emphasis"><em>X2</em></span>), wall area (<span class="emphasis"><em>X3</em></span>), roof area (<span class="emphasis"><em>X4</em></span>), overall height (<span class="emphasis"><em>X5</em></span>), orientation (<span class="emphasis"><em>X6</em></span>), glazing area (<span class="emphasis"><em>X7</em></span>), and glazing area distribution (<span class="emphasis"><em>X8</em></span>). We will try to predict heating load as a function of all the building parameters using both ordinary regression and Bayesian regression, using the <code class="literal">glm</code> functions of the arm package. We will show that, for the same dataset, Bayesian regression gives significantly smaller prediction intervals.</p></div>
<div class="section" title="Regression of energy efficiency with building parameters" id="aid-19UOO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Regression of energy efficiency with building parameters</h1></div></div></div><p>In this <a id="id271" class="indexterm"/>section, we will do a linear regression of the building's energy efficiency measure, heating load (<span class="emphasis"><em>Y1</em></span>) as a function of the building parameters. It would be useful to do a preliminary descriptive analysis to find which building variables are statistically significant. For this, we will first create bivariate plots of <span class="emphasis"><em>Y1</em></span> and all the <span class="emphasis"><em>X</em></span> variables. We will also compute the Spearman correlation between <span class="emphasis"><em>Y1</em></span> and all the <span class="emphasis"><em>X</em></span> variables. The R script for performing these tasks is as follows:</p><div class="informalexample"><pre class="programlisting">&gt;library(ggplot2)
&gt;library(gridExtra)

&gt;df &lt;- read.csv("ENB2012_data.csv",header = T)
&gt;df &lt;- df[,c(1:9)]
&gt;str(df)
&gt;df[,6] &lt;- as.numeric(df[,6])
&gt;df[,8] &lt;- as.numeric(df[,8])

&gt;attach(df)
&gt;bp1 &lt;- ggplot(data = df,aes(x = X1,y = Y1)) + geom_point()
&gt;bp2 &lt;- ggplot(data = df,aes(x = X2,y = Y1)) + geom_point()
&gt;bp3 &lt;- ggplot(data = df,aes(x = X3,y = Y1)) + geom_point()
&gt;bp4 &lt;- ggplot(data = df,aes(x = X4,y = Y1)) + geom_point()
&gt;bp5 &lt;- ggplot(data = df,aes(x = X5,y = Y1)) + geom_point()
&gt;bp6 &lt;- ggplot(data = df,aes(x = X6,y = Y1)) + geom_point()
&gt;bp7 &lt;- ggplot(data = df,aes(x = X7,y = Y1)) + geom_point()
&gt;bp8 &lt;- ggplot(data = df,aes(x = X8,y = Y1)) + geom_point()
&gt;grid.arrange(bp1,bp2,bp3,bp4,bp5,bp6,bp7,bp8,nrow = 2,ncol = 4)
&gt;detach(df)</pre></div><div class="mediaobject"><img src="../Images/image00450.jpeg" alt="Regression of energy efficiency with building parameters"/></div><p style="clear:both; height: 1em;"> </p><div class="informalexample"><pre class="programlisting">&gt;cor.val &lt;- cor(df[,1:8],df[,9],method = "spearman")
&gt;cor.val           
[,1]
X1  0.622134697
X2 -0.622134697
X3  0.471457650
X4 -0.804027000
X5  0.861282577
X6 -0.004163071
X7  0.322860320
X8  0.068343464</pre></div><p>From the <a id="id272" class="indexterm"/>b-plots and correlation coefficient values, we can conclude that variables X6 and X8 do not have a significant influence on Y1 and, hence, can be dropped from the model.</p><div class="section" title="Ordinary regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec33"/>Ordinary regression</h2></div></div></div><p>Before <a id="id273" class="indexterm"/>we look at the Bayesian linear regression, let's do an ordinary linear regression. The following R code fits the linear regression model using the <code class="literal">lm</code> function in the base R on the training data and predicts the values of <span class="emphasis"><em>Y1</em></span> on the test dataset:</p><div class="informalexample"><pre class="programlisting">&gt;#Removing X6 and X8 since they don't have significant correlation with Y1
&gt;df &lt;- df[,c(1,2,3,4,5,7,9)]
&gt;str(df)

&gt;Splitting data set to Train and Test set in the ratio 80:20
&gt;set.seed(123)
&gt;samp &lt;- sample.int(nrow(df),as.integer(nrow(df)*0.2),replace = F)
&gt;dfTest &lt;- df[samp,]
&gt;dfTrain &lt;- df[-samp,]
&gt;xtest &lt;- dfTest[,1:6]
&gt;ytest &lt;- dfTest[,7]

&gt;library(arm)
&gt;attach(dfTrain)

&gt;#Ordinary Multivariate Regression
&gt;fit.ols &lt;- lm(Y1 ~ X1 + X2 + X3 + X4 + X5 + X7,data = dfTrain)
&gt;summary(fit.ols)
&gt;fit.coeff &lt;- fit.ols$coefficients
&gt;ypred.ols &lt;- predict.lm(fit.ols,xtest,interval = "prediction",se.fit = T)
&gt;ypred.ols$fit
&gt;yout.ols &lt;- as.data.frame(cbind(ytest,ypred.ols$fit))
&gt;ols.upr &lt;- yout.ols$upr
&gt;ols.lwr &lt;- yout.ols$lwr</pre></div></div><div class="section" title="Bayesian regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec34"/>Bayesian regression</h2></div></div></div><p>To <a id="id274" class="indexterm"/>perform Bayesian linear regression, we use the <code class="literal">bayesglm()</code> function of the arm package. As we described in the introduction, for the GLM, if we choose family as <code class="literal">gaussian</code> (the same as normal distribution) and link function as <code class="literal">identity</code>, then the GLM is equivalent to ordinary linear regression. Hence, if we use the <code class="literal">bayesglm()</code> function with the <code class="literal">gaussian</code> family and the <code class="literal">identity</code> link function, then we are performing a Bayesian linear regression.</p><p>For the Bayesian model, we need to specify a prior distribution. For the Gaussian distribution, the default settings are <code class="literal">prior.mean = 0</code>, <code class="literal">prior.scale = NULL</code>, and <code class="literal">prior.df = Inf</code>. The following R code can be used for Bayesian linear regression:</p><div class="informalexample"><pre class="programlisting">&gt;fit.bayes &lt;- bayesglm(Y1 ~ X1 + X2 + X3 + X4 + X5 + X7,family=gaussian(link=identity),data=dfTrain,prior.df = Inf,prior.mean = 0,prior.scale = NULL,maxit = 10000)
&gt;ypred.bayes &lt;- predict.glm(fit.bayes,newdata = xtest,se.fit = T)
&gt;ypred.bayes$fit</pre></div><p>To compare the results of the ordinary regression and Bayesian regression, we plot the prediction on test data with prediction errors for both methods on a single graph. For this purpose, we will use the <a id="id275" class="indexterm"/>
<span class="strong"><strong>ggplot2</strong></span> package:</p><div class="informalexample"><pre class="programlisting">&gt;library(ggplot2)
&gt;library(gridExtra)
&gt;yout.ols &lt;- as.data.frame(cbind(ytest,ypred.ols$fit))
&gt;ols.upr &lt;- yout.ols$upr
&gt;ols.lwr &lt;- yout.ols$lwr

&gt;p.ols &lt;- ggplot(data = yout.ols,aes(x = yout.ols$ytest,y = yout.ols$fit)) + geom_point() + ggtitle("Ordinary Regression Prediction on Test Data") + labs(x = "Y-Test",y = "Y-Pred")
&gt;p.ols + geom_errorbar(ymin = ols.lwr,ymax = ols.upr)yout.bayes &lt;- as.data.frame(cbind(ytest,ypred.bayes$fit))
&gt;names(yout.bayes) &lt;- c("ytest","fit")
&gt;critval &lt;- 1.96 #approx for 95% CI
&gt;bayes.upr &lt;- ypred.bayes$fit + critval * ypred.bayes$se.fit
&gt;bayes.lwr &lt;- ypred.bayes$fit - critval * ypred.bayes$se.fit

&gt;p.bayes &lt;- ggplot(data = yout.bayes,aes(x = yout.bayes$ytest,y = yout.bayes$fit)) + geom_point() + ggtitle("Bayesian Regression Prediction on Test Data") + labs(x = "Y-Test",y = "Y-Pred")
&gt;p.bayes + geom_errorbar(ymin = bayes.lwr,ymax = bayes.upr)


&gt;p1 &lt;-  p.ols + geom_errorbar(ymin = ols.lwr,ymax = ols.upr)
&gt;p2 &lt;-  p.bayes + geom_errorbar(ymin = bayes.lwr,ymax = bayes.upr)

&gt;grid.arrange(p1,p2,ncol = 2)</pre></div><div class="mediaobject"><img src="../Images/image00451.jpeg" alt="Bayesian regression"/></div><p style="clear:both; height: 1em;"> </p><p>One can see that<a id="id276" class="indexterm"/> the Bayesian approach gives much more compact, 95% confident prediction intervals compared to ordinary regression. This is happening because, in the Bayesian approach, one computes a distribution of parameters. The prediction is made using a set of values sampled from the posterior distribution and averaged to get the final prediction and confidence interval.</p></div></div>
<div class="section" title="Simulation of the posterior distribution" id="aid-1AT9A1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Simulation of the posterior distribution</h1></div></div></div><p>If one wants to find out<a id="id277" class="indexterm"/> the posterior of the model parameters, the <code class="literal">sim( )</code> function of the arm package becomes handy. The following R script will simulate the posterior distribution of parameters and produce a set of histograms:</p><div class="informalexample"><pre class="programlisting">&gt;posterior.bayes &lt;- as.data.frame(coef(sim(fit.bayes)))
&gt;attach(posterior.bayes)

&gt;h1 &lt;- ggplot(data = posterior.bayes,aes(x = X1)) + geom_histogram() + ggtitle("Histogram X1")
&gt;h2 &lt;- ggplot(data = posterior.bayes,aes(x = X2)) + geom_histogram() + ggtitle("Histogram X2")
&gt;h3 &lt;- ggplot(data = posterior.bayes,aes(x = X3)) + geom_histogram() + ggtitle("Histogram X3")
&gt;h4 &lt;- ggplot(data = posterior.bayes,aes(x = X4)) + geom_histogram() + ggtitle("Histogram X4")
&gt;h5 &lt;- ggplot(data = posterior.bayes,aes(x = X5)) + geom_histogram() + ggtitle("Histogram X5")
&gt;h7 &lt;- ggplot(data = posterior.bayes,aes(x = X7)) + geom_histogram() + ggtitle("Histogram X7")
&gt;grid.arrange(h1,h2,h3,h4,h5,h7,nrow = 2,ncol = 3)

&gt;detach(posterior.bayes)</pre></div><div class="mediaobject"><img src="../Images/image00452.jpeg" alt="Simulation of the posterior distribution"/></div><p style="clear:both; height: 1em;"> </p></div>
<div class="section" title="Exercises" id="aid-1BRPS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Use<a id="id278" class="indexterm"/> the multivariate dataset named Auto MPG from the UCI Machine Learning repository (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter). The dataset can be downloaded from the website at <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Auto+MPG">https://archive.ics.uci.edu/ml/datasets/Auto+MPG</a>. The dataset describes automobile fuel consumption in <a id="id279" class="indexterm"/><span class="strong"><strong>miles per gallon</strong></span> (<span class="strong"><strong>mpg</strong></span>) for cars running in American cities. From the folder containing the datasets, download two files: <code class="literal">auto-mpg.data</code> and <code class="literal">auto-mpg.names</code>. The <code class="literal">auto-mpg.data</code> file contains the data and it is in space-separated format. The <code class="literal">auto-mpg.names</code> file has several details about the dataset, including variable names for each column. Build a regression model for the fuel efficiency, as a function displacement (<span class="emphasis"><em>disp</em></span>), horse power (<span class="emphasis"><em>hp</em></span>), weight (<span class="emphasis"><em>wt</em></span>), and acceleration (<span class="emphasis"><em>accel</em></span>), using both OLS and Bayesian GLM. Predict the values for mpg in the test dataset using both the OLS model and Bayesian GLM model (using the <code class="literal">bayesglm</code> function). Find the <span class="strong"><strong>Root Mean Square Error</strong></span> (<span class="strong"><strong>RMSE</strong></span>) <a id="id280" class="indexterm"/>values for OLS and Bayesian GLM and compare the accuracy and prediction intervals for both the methods.</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-1CQAE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Friedman J., Hastie T., and Tibshirani R. <span class="emphasis"><em>The Elements of Statistical Learning – Data Mining, Inference, and Prediction</em></span>. Springer Series in Statistics. 2009</li><li class="listitem">Tsanas A. and Xifara A. "Accurate Quantitative Estimation of Energy Performance of Residential Buildings Using Statistical Machine Learning Tools". Energy and Buildings. Vol. 49, pp. 560-567. 2012</li><li class="listitem">Quinlan R. "Combining Instance-based and Model-based Learning". In: Tenth International Conference of Machine Learning. 236-243. University of Massachusetts, Amherst. Morgan Kaufmann. 1993. Original dataset is from StatLib library maintained by Carnegie Mellon University.</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-1DOR01"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Summary</h1></div></div></div><p>In this chapter, we illustrated how Bayesian regression is more useful for prediction with a tighter confidence interval using the Energy efficiency dataset and the <code class="literal">bayesglm</code> function of the arm package. We also learned how to simulate the posterior distribution using the <code class="literal">sim</code> function in the same R package. In the next chapter, we will learn about Bayesian classification.</p></div></body></html>