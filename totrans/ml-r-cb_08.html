<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Ensemble Learning"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Ensemble Learning </h1></div></div></div><p class="calibre7">In this chapter, we will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Classifying data with the bagging method</li><li class="listitem">Performing cross-validation with the bagging method</li><li class="listitem">Classifying data with the boosting method</li><li class="listitem">Performing cross-validation with the boosting method </li><li class="listitem">Classifying data with gradient boosting</li><li class="listitem">Calculating the margins of a classifier </li><li class="listitem">Calculating the error evolution of the ensemble method</li><li class="listitem">Classifying the data with random forest</li><li class="listitem">Estimating the prediction errors of different classifiers</li></ul></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Ensemble Learning">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec90" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">Ensemble learning is a method to combine results produced by different learners into one format, with the aim of producing better classification results and regression results. In previous chapters, we discussed several classification methods. These methods take different approaches but they all have the same goal, that is, finding an optimum classification model. However, a single classifier may be imperfect, which may misclassify data in certain categories. As not all classifiers are imperfect, a better approach is to average the results by voting. In other words, if we average the prediction results of every classifier with the same input, we may create a superior model compared to using an<a id="id647" class="calibre1"/> individual method.</p><p class="calibre7">In ensemble learning, bagging, boosting, and random forest are the three most common methods:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Bagging <a id="id648" class="calibre1"/>is a voting method, which first<a id="id649" class="calibre1"/> uses Bootstrap to generate a different training set, and then uses the training set to make different base learners. The<a id="id650" class="calibre1"/> bagging method employs a combination <a id="id651" class="calibre1"/>of base learners to make a better prediction.</li><li class="listitem">Boosting<a id="id652" class="calibre1"/> is similar to the bagging method. However, what makes boosting different is that it first constructs<a id="id653" class="calibre1"/> the base learning in sequence, where each successive learner is built for the prediction residuals of the preceding learner. With the means to create a complementary learner, it uses the mistakes made by previous learners to train the next base learner.</li><li class="listitem">Random forest<a id="id654" class="calibre1"/> uses the classification <a id="id655" class="calibre1"/>results voted from many classification trees. The idea is simple; a single classification tree will obtain a single classification result with a single input vector. However, a random forest grows many classification trees, obtaining multiple results from a single input. Therefore, a random forest will use the majority of votes from all the decision trees to classify data or use an average output for regression.</li></ul></div><p class="calibre7">In the following recipes, we will discuss how to use bagging and boosting to classify data. We can then perform cross-validation to estimate the error rate of each classifier. In addition to this, we'll introduce the use of a margin to measure the certainty of a model. Next, we cover random forests, similar to the bagging and boosting methods, and introduce how to train the model to classify data and use margins to estimate the model certainty. Lastly, we'll demonstrate how to estimate the error rate of each classifier, and use the error rate to compare the performance of different classifiers.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with the bagging method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec91" class="calibre1"/>Classifying data with the bagging method</h1></div></div></div><p class="calibre7">The <code class="email">adabag</code> package implements both boosting and bagging methods. For the bagging method, the package implements Breiman's Bagging algorithm, which first generates multiple <a id="id656" class="calibre1"/>versions of classifiers, and then obtains an aggregated classifier. In this recipe, we will illustrate how to use the bagging <a id="id657" class="calibre1"/>method from <code class="email">adabag</code> to generate a classification model using the telecom <code class="email">churn</code> dataset.</p></div>

<div class="book" title="Classifying data with the bagging method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec309" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we continue to use the telecom <code class="email">churn</code> dataset as the input data source for the bagging method. For those who have not prepared the dataset, please refer to <a class="calibre1" title="Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic" href="part0060_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre8">Classification (I) – Tree, Lazy, and Probabilistic</em></span>, for detailed information.</p></div></div>

<div class="book" title="Classifying data with the bagging method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec310" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to generate a classification model for the telecom <code class="email">churn</code> dataset:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you need to install and load the <code class="email">adabag</code> package (it might take a while to install <code class="email">adabag</code>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("adabag")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(adabag)</strong></span>
</pre></div></li><li class="listitem" value="2">Next, you <a id="id658" class="calibre1"/>can use the <code class="email">bagging</code> function to train a training dataset (the result may vary during the training process):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.bagging = bagging(churn ~ ., data=trainset, mfinal=10)</strong></span>
</pre></div></li><li class="listitem" value="3">Access the variable importance from the bagging result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.bagging$importance</strong></span>
<span class="strong"><strong class="calibre2">           international_plan number_customer_service_calls </strong></span>
<span class="strong"><strong class="calibre2">                   10.4948380                    16.4260510 </strong></span>
<span class="strong"><strong class="calibre2">        number_vmail_messages               total_day_calls </strong></span>
<span class="strong"><strong class="calibre2">                    0.5319143                     0.3774190 </strong></span>
<span class="strong"><strong class="calibre2">             total_day_charge             total_day_minutes </strong></span>
<span class="strong"><strong class="calibre2">                    0.0000000                    28.7545042 </strong></span>
<span class="strong"><strong class="calibre2">              total_eve_calls              total_eve_charge </strong></span>
<span class="strong"><strong class="calibre2">                    0.1463585                     0.0000000 </strong></span>
<span class="strong"><strong class="calibre2">            total_eve_minutes              total_intl_calls </strong></span>
<span class="strong"><strong class="calibre2">                   14.2366754                     8.7733895 </strong></span>
<span class="strong"><strong class="calibre2">            total_intl_charge            total_intl_minutes </strong></span>
<span class="strong"><strong class="calibre2">                    0.0000000                     9.7838256 </strong></span>
<span class="strong"><strong class="calibre2">            total_night_calls            total_night_charge </strong></span>
<span class="strong"><strong class="calibre2">                    0.4349952                     0.0000000 </strong></span>
<span class="strong"><strong class="calibre2">          total_night_minutes               voice_mail_plan </strong></span>
<span class="strong"><strong class="calibre2">                    2.3379622                     7.7020671 </strong></span>
</pre></div></li><li class="listitem" value="4">After generating the classification model, you can use the predicted results from the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.predbagging= predict.bagging(churn.bagging, newdata=testset)</strong></span>
</pre></div></li><li class="listitem" value="5">From the predicted results, you can obtain a classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.predbagging$confusion</strong></span>
<span class="strong"><strong class="calibre2">               Observed Class</strong></span>
<span class="strong"><strong class="calibre2">Predicted Class yes  no</strong></span>
<span class="strong"><strong class="calibre2">            no   35 866</strong></span>
<span class="strong"><strong class="calibre2">            yes 106  11</strong></span>
</pre></div></li><li class="listitem" value="6">Finally, you can retrieve the average error of the bagging result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.predbagging$error</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.0451866</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with the bagging method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec311" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Bagging is <a id="id659" class="calibre1"/>derived from the name Bootstrap aggregating, which is a stable, accurate, and easy to implement model for data classification and regression. The definition of bagging is as follows: given a training dataset of size <span class="strong"><em class="calibre8">n</em></span>, bagging performs Bootstrap sampling and generates <span class="strong"><em class="calibre8">m</em></span> new training sets, <span class="strong"><em class="calibre8">Di</em></span>, each of size <span class="strong"><em class="calibre8">n</em></span>. Finally, we can fit <span class="strong"><em class="calibre8">m</em></span> Bootstrap samples to <span class="strong"><em class="calibre8">m</em></span> models and combine the result by averaging the output (for regression) or voting (for classification):</p><div class="mediaobject"><img src="../images/00132.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">An illustration of bagging method</p></div></div><p class="calibre10"> </p><p class="calibre7">The advantage of using bagging is that it is a powerful learning method, which is easy to understand and implement. However, the main drawback of this technique is that it is hard to analyze the result.</p><p class="calibre7">In this recipe, we use the boosting method from <code class="email">adabag</code> to classify the telecom churn data. Similar to other classification methods discussed in previous chapters, you can train a boosting classifier with a formula and a training dataset. Additionally, you can set the number of iterations to 10 in the <code class="email">mfinal</code> argument. Once the classification model is built, you can <a id="id660" class="calibre1"/>examine the importance of each attribute. Ranking the attributes by importance reveals that the number of customer service calls play a crucial role in the classification model.</p><p class="calibre7">Next, with a fitted model, you can apply the <code class="email">predict.bagging</code> function to predict the labels of the testing dataset. Therefore, you can use the labels of the testing dataset and predicted results to generate a classification table and obtain the average error, which is 0.045 in this example.</p></div></div>

<div class="book" title="Classifying data with the bagging method">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec312" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Besides <code class="email">adabag</code>, the <code class="email">ipred</code> package provides a bagging method for a classification tree. We demonstrate<a id="id661" class="calibre1"/> here how to use the bagging method of the <code class="email">ipred</code> package to train a classification model:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you need to install and load the <code class="email">ipred</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("ipred")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(ipred)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use the <code class="email">bagging</code> method to fit the classification method:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.bagging = bagging(churn ~ ., data = trainset, coob = T)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.bagging</strong></span>

<span class="strong"><strong class="calibre2">Bagging classification trees with 25 bootstrap replications </strong></span>

<span class="strong"><strong class="calibre2">Call: bagging.data.frame(formula = churn ~ ., data = trainset, coob = T)</strong></span>

<span class="strong"><strong class="calibre2">Out-of-bag estimate of misclassification error:  0.0605 </strong></span>
</pre></div></li><li class="listitem" value="3">Obtain an out of bag estimate of misclassification of the errors:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; mean(predict(churn.bagging) != trainset$churn)</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.06047516</strong></span>
</pre></div></li><li class="listitem" value="4">You can then use the <code class="email">predict</code> function to obtain the predicted labels of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.prediction = predict(churn.bagging, newdata=testset, type="class")</strong></span>
</pre></div></li><li class="listitem" value="5">Obtain the classification table from the labels of the testing dataset and prediction result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; prediction.table = table(churn.prediction, testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">                </strong></span>
<span class="strong"><strong class="calibre2">churn.prediction yes  no</strong></span>
<span class="strong"><strong class="calibre2">             no   31 869</strong></span>
<span class="strong"><strong class="calibre2">             yes 110   8</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing cross-validation with the bagging method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec92" class="calibre1"/>Performing cross-validation with the bagging method</h1></div></div></div><p class="calibre7">To assess the<a id="id662" class="calibre1"/> prediction power of a <a id="id663" class="calibre1"/>classifier, you can run a cross-validation method to test the robustness of the classification model. In this recipe, we will introduce how to use <code class="email">bagging.cv</code> to perform cross-validation with the bagging method.</p></div>

<div class="book" title="Performing cross-validation with the bagging method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec313" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we continue to use the telecom <code class="email">churn</code> dataset as the input data source to perform a k-fold cross-validation with the bagging method.</p></div></div>

<div class="book" title="Performing cross-validation with the bagging method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec314" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to retrieve the minimum estimation errors by performing cross-validation with the bagging method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, we use <code class="email">bagging.cv</code> to make a 10-fold classification on the training dataset with 10 iterations:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.baggingcv = bagging.cv(churn ~ ., v=10, data=trainset, mfinal=10)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then obtain the confusion matrix from the cross-validation results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.baggingcv$confusion</strong></span>
<span class="strong"><strong class="calibre2">               Observed Class</strong></span>
<span class="strong"><strong class="calibre2">Predicted Class  yes   no</strong></span>
<span class="strong"><strong class="calibre2">            no   100 1938</strong></span>
<span class="strong"><strong class="calibre2">            yes  242   35</strong></span>
</pre></div></li><li class="listitem" value="3">Lastly, you can retrieve the minimum estimation errors from the cross-validation results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.baggingcv$error</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.05831533</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing cross-validation with the bagging method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec315" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The <code class="email">adabag</code> package provides a function to perform the k-fold validation with either the bagging or <a id="id664" class="calibre1"/>boosting method. In this <a id="id665" class="calibre1"/>example, we use <code class="email">bagging.cv</code> to make the k-fold cross-validation with the bagging method. We first perform a 10-fold cross validation with 10 iterations by specifying <code class="email">v=10</code> and <code class="email">mfinal=10</code>. Please note that this is quite time consuming due to the number of iterations. After the cross-validation process is complete, we can obtain the confusion matrix and average errors (0.058 in this case) from the cross-validation results.</p></div></div>

<div class="book" title="Performing cross-validation with the bagging method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec316" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For those interested in tuning the parameters of <code class="email">bagging.cv</code>, please view the <code class="email">bagging.cv</code> document by using the <code class="email">help</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(bagging.cv)</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with the boosting method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec93" class="calibre1"/>Classifying data with the boosting method</h1></div></div></div><p class="calibre7">Similar to the bagging method, boosting starts with a simple or weak classifier and gradually <a id="id666" class="calibre1"/>improves it by reweighting the misclassified samples. Thus, the new classifier can learn from previous classifiers. The <code class="email">adabag</code> package <a id="id667" class="calibre1"/>provides implementation of the <span class="strong"><strong class="calibre2">AdaBoost.M1</strong></span> and <span class="strong"><strong class="calibre2">SAMME</strong></span> algorithms. Therefore, one can use the boosting method in <code class="email">adabag</code> to perform<a id="id668" class="calibre1"/> ensemble learning. In this recipe, we will use the boosting method in <code class="email">adabag</code> to classify the telecom <code class="email">churn</code> dataset.</p></div>

<div class="book" title="Classifying data with the boosting method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec317" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom churn dataset as the input data source to perform classifications with the boosting method. Also, you need to have the <code class="email">adabag</code> package loaded in R before commencing the recipe.</p></div></div>

<div class="book" title="Classifying data with the boosting method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec318" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to classify the telecom <code class="email">churn</code> dataset with the boosting method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can use the boosting function from the <code class="email">adabag</code> package to train the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.boost = boosting(churn ~.,data=trainset,mfinal=10, coeflearn="Freund", boos=FALSE , control=rpart.control(maxdepth=3))</strong></span>
</pre></div></li><li class="listitem" value="2">You <a id="id669" class="calibre1"/>can then make a prediction based on the boosted model and testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.boost.pred = predict.boosting(churn.boost,newdata=testset)</strong></span>
</pre></div></li><li class="listitem" value="3">Next, you can retrieve the classification table from the predicted results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.boost.pred$confusion</strong></span>
<span class="strong"><strong class="calibre2">               Observed Class</strong></span>
<span class="strong"><strong class="calibre2">Predicted Class yes  no</strong></span>
<span class="strong"><strong class="calibre2">            no   41 858</strong></span>
<span class="strong"><strong class="calibre2">            yes 100  19</strong></span>
</pre></div></li><li class="listitem" value="4"> Finally, you can obtain the average errors from the predicted results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.boost.pred$error</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.0589391</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with the boosting method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec319" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The idea of boosting is to "boost" weak learners (for example, a single decision tree) into strong learners. Assuming that we have <span class="strong"><em class="calibre8">n</em></span> points in our training dataset, we can assign a weight, <span class="strong"><em class="calibre8">Wi</em></span> (0 &lt;= i &lt;n), for each point. Then, during the iterative learning process (we assume the number of iterations is <span class="strong"><em class="calibre8">m</em></span>), we can reweigh each point in accordance with the classification result in each iteration. If the point is correctly classified, we should decrease the weight. Otherwise, we increase the weight of the point. When the iteration process is finished, we can then obtain the <span class="strong"><em class="calibre8">m</em></span> fitted model, <span class="strong"><em class="calibre8">f<sub class="calibre25">i</sub>(x)</em></span> (0 &lt;= i &lt;n). Finally, we can obtain the final prediction through the weighted average of each tree's prediction, where the weight, b, is based on the quality of each tree:</p><div class="mediaobject"><img src="../images/00133.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">An illustration of boosting method</p></div></div><p class="calibre10"> </p><p class="calibre7">Both bagging and boosting are ensemble methods, which combine the prediction power of each <a id="id670" class="calibre1"/>single learner into a strong learner. The difference between bagging and boosting is that the bagging method combines independent models, but boosting performs an iterative process to reduce the errors of preceding models by predicting them with successive models.</p><p class="calibre7">In this recipe, we demonstrate how to fit a classification model within the boosting method. Similar to bagging, one has to specify the formula and the training dataset used to train the classification model. In addition, one can specify parameters, such as the number of iterations (<code class="email">mfinal</code>), the weight update coefficient (<code class="email">coeflearn</code>), the weight of how each observation is used (<code class="email">boos</code>), and the control for <code class="email">rpart</code> (a single decision tree). In this recipe, we set the iteration to 10, using <code class="email">Freund</code> (the AdaBoost.M1 algorithm implemented method) as <code class="email">coeflearn</code>, <code class="email">boos</code> set to false and max depth set to <code class="email">3</code> for <code class="email">rpart</code> configuration.</p><p class="calibre7">We use the boosting method to fit the classification model and then save it in <code class="email">churn.boost</code>. We can then obtain predicted labels using the <code class="email">prediction</code> function. Furthermore, we can use the <code class="email">table</code> function to retrieve a classification table based on the predicted labels and testing the dataset labels. Lastly, we can get the average errors of the predicted results.</p></div></div>

<div class="book" title="Classifying data with the boosting method">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec320" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">In addition to using the boosting function in the <code class="email">adabag</code> package, one can also use the <code class="email">caret</code> package to perform a classification with the boosting method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, load the  <code class="email">mboost</code> and <code class="email">pROC</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(mboost)</strong></span>
<span class="strong"><strong class="calibre2">&gt; install.packages("pROC")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(pROC)</strong></span>
</pre></div></li><li class="listitem" value="2">We can then set the training control with the <code class="email">trainControl</code> function and use the <code class="email">train</code> function to train the classification model with adaboost:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ctrl = trainControl(method = "repeatedcv", repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ada.train = train(churn ~ ., data = trainset, method = "ada", metric = "ROC", trControl = ctrl)</strong></span>
</pre></div></li><li class="listitem" value="3">Use <a id="id671" class="calibre1"/>the <code class="email">summary</code> function to obtain the details of the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ada.train$result</strong></span>
<span class="strong"><strong class="calibre2">   nu maxdepth iter       ROC      Sens        Spec      ROCSD     SensSD      SpecSD</strong></span>
<span class="strong"><strong class="calibre2">1 0.1        1   50 0.8571988 0.9152941 0.012662155 0.03448418 0.04430519 0.007251045</strong></span>
<span class="strong"><strong class="calibre2">4 0.1        2   50 0.8905514 0.7138655 0.006083679 0.03538445 0.10089887 0.006236741</strong></span>
<span class="strong"><strong class="calibre2">7 0.1        3   50 0.9056456 0.4036134 0.007093780 0.03934631 0.09406015 0.006407402</strong></span>
<span class="strong"><strong class="calibre2">2 0.1        1  100 0.8550789 0.8918487 0.015705276 0.03434382 0.06190546 0.006503191</strong></span>
<span class="strong"><strong class="calibre2">5 0.1        2  100 0.8907720 0.6609244 0.009626724 0.03788941 0.11403364 0.006940001</strong></span>
<span class="strong"><strong class="calibre2">8 0.1        3  100 0.9077750 0.3832773 0.005576065 0.03601187 0.09630026 0.003738978</strong></span>
<span class="strong"><strong class="calibre2">3 0.1        1  150 0.8571743 0.8714286 0.016720505 0.03481526 0.06198773 0.006767313</strong></span>
<span class="strong"><strong class="calibre2">6 0.1        2  150 0.8929524 0.6171429 0.011654617 0.03638272 0.11383803 0.006777465</strong></span>
<span class="strong"><strong class="calibre2">9 0.1        3  150 0.9093921 0.3743697 0.007093780 0.03258220 0.09504202 0.005446136</strong></span>
</pre></div></li><li class="listitem" value="4">Use the <code class="email">plot</code> function to plot the ROC curve within different iterations:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(ada.train)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00134.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">The repeated cross validation plot</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">Finally, we<a id="id672" class="calibre1"/> can make predictions using the <code class="email">predict</code> function and view the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ada.predict = predict(ada.train, testset, "prob")</strong></span>
<span class="strong"><strong class="calibre2">&gt; ada.predict.result = ifelse(ada.predict[1] &gt; 0.5, "yes", "no")</strong></span>

<span class="strong"><strong class="calibre2">&gt; table(testset$churn, ada.predict.result)</strong></span>
<span class="strong"><strong class="calibre2">     ada.predict.result</strong></span>
<span class="strong"><strong class="calibre2">       no yes</strong></span>
<span class="strong"><strong class="calibre2">  yes  40 101</strong></span>
<span class="strong"><strong class="calibre2">  no  872   5</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing cross-validation with the boosting method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec94" class="calibre1"/>Performing cross-validation with the boosting method</h1></div></div></div><p class="calibre7">Similar to the <code class="email">bagging</code> function, <code class="email">adabag</code> provides a cross-validation function for the boosting <a id="id673" class="calibre1"/>method, named <code class="email">boosting.cv</code>. In<a id="id674" class="calibre1"/> this recipe, we will demonstrate how to perform cross-validation using <code class="email">boosting.cv</code> from the package, <code class="email">adabag</code>.</p></div>

<div class="book" title="Performing cross-validation with the boosting method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec321" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we continue to use the telecom <code class="email">churn</code> dataset as the input data source to perform a k-fold cross-validation with the <code class="email">boosting</code> method.</p></div></div>

<div class="book" title="Performing cross-validation with the boosting method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec322" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform <a id="id675" class="calibre1"/>the following steps to retrieve<a id="id676" class="calibre1"/> the minimum estimation errors via cross-validation with the <code class="email">boosting</code> method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can use <code class="email">boosting.cv</code> to cross-validate the training dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.boostcv = boosting.cv(churn ~ ., v=10, data=trainset, mfinal=5,control=rpart.control(cp=0.01))</strong></span>
</pre></div></li><li class="listitem" value="2">You can then obtain the confusion matrix from the boosting results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.boostcv$confusion</strong></span>
<span class="strong"><strong class="calibre2">               Observed Class</strong></span>
<span class="strong"><strong class="calibre2">Predicted Class  yes   no</strong></span>
<span class="strong"><strong class="calibre2">            no   119 1940</strong></span>
<span class="strong"><strong class="calibre2">            yes  223   33</strong></span>
</pre></div></li><li class="listitem" value="3">Finally, you can retrieve the average errors of the boosting method:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.boostcv$error</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.06565875</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing cross-validation with the boosting method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec323" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Similar to <code class="email">bagging.cv</code>, we can perform cross-validation with the boosting method using <code class="email">boosting.cv</code>. If <code class="email">v</code> is set to <code class="email">10</code> and <code class="email">mfinal</code> is set to <code class="email">5</code>, the <code class="email">boosting</code> method will perform 10-fold cross-validations with five iterations. Also, one can set the control of the <code class="email">rpart</code> fit within the parameter. We can set the complexity parameter to 0.01 in this example. Once the training is complete, the confusion matrix and average errors of the boosted results will be obtained.</p></div></div>

<div class="book" title="Performing cross-validation with the boosting method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec324" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For those who require more information on tuning the parameters of <code class="email">boosting.cv</code>, please view the <code class="email">boosting.cv</code> document by using the <code class="email">help</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(boosting.cv)</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with gradient boosting"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec95" class="calibre1"/>Classifying data with gradient boosting</h1></div></div></div><p class="calibre7">Gradient boosting ensembles weak learners and creates a new base learner that maximally<a id="id677" class="calibre1"/> correlates with the negative gradient of the loss function. One may apply this method on either regression or classification problems, and it will <a id="id678" class="calibre1"/>perform well in different datasets. In this recipe, we will introduce how to use <code class="email">gbm</code> to classify a telecom <code class="email">churn</code> dataset.</p></div>

<div class="book" title="Classifying data with gradient boosting">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec325" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we continue to use the telecom <code class="email">churn</code> dataset as the input data source for the <code class="email">bagging</code> method. For those who have not prepared the dataset, please refer to <a class="calibre1" title="Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic" href="part0060_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre8">Classification (I) – Tree, Lazy, and Probabilistic</em></span>, for detailed information.</p></div></div>

<div class="book" title="Classifying data with gradient boosting">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec326" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to calculate and classify data with the gradient boosting method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the package, <code class="email">gbm</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("gbm")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(gbm)</strong></span>
</pre></div></li><li class="listitem" value="2">The <code class="email">gbm</code> function only uses responses ranging from <code class="email">0</code> to <code class="email">1</code>; therefore, you should transform yes/no responses to numeric responses (0/1):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; trainset$churn = ifelse(trainset$churn == "yes", 1, 0)</strong></span>
</pre></div></li><li class="listitem" value="3">Next, you can use the <code class="email">gbm</code> function to train a training dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.gbm = gbm(formula = churn ~ .,distribution = "bernoulli",data = trainset,n.trees = 1000,interaction.depth = 7,shrinkage = 0.01, cv.folds=3)</strong></span>
</pre></div></li><li class="listitem" value="4">Then, you can obtain the summary information from the fitted model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(churn.gbm)</strong></span>
<span class="strong"><strong class="calibre2">                                         var    rel.inf</strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes          total_day_minutes 28.1217147</strong></span>
<span class="strong"><strong class="calibre2">total_eve_minutes                total_eve_minutes 16.8097151</strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls number_customer_service_calls 12.7894464</strong></span>
<span class="strong"><strong class="calibre2">total_intl_minutes             total_intl_minutes  9.4515822</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls                   total_intl_calls  8.1379826</strong></span>
<span class="strong"><strong class="calibre2">international_plan               international_plan  8.0703900</strong></span>
<span class="strong"><strong class="calibre2">total_night_minutes             total_night_minutes  4.0805153</strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages         number_vmail_messages  3.9173515</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_plan                  voice_mail_plan  2.5501480</strong></span>
<span class="strong"><strong class="calibre2">total_night_calls              total_night_calls  2.1357970</strong></span>
<span class="strong"><strong class="calibre2">total_day_calls                     total_day_calls  1.7367888</strong></span>
<span class="strong"><strong class="calibre2">total_eve_calls                     total_eve_calls  1.4398047</strong></span>
<span class="strong"><strong class="calibre2">total_eve_charge                 total_eve_charge  0.5457486</strong></span>
<span class="strong"><strong class="calibre2">total_night_charge              total_night_charge  0.2130152</strong></span>
<span class="strong"><strong class="calibre2">total_day_charge                total_day_charge  0.0000000</strong></span>
<span class="strong"><strong class="calibre2">total_intl_charge                 total_intl_charge  0.0000000</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00135.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Relative influence plot of fitted model</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="5">You <a id="id679" class="calibre1"/>can obtain the best iteration using cross-validation:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.iter = gbm.perf(churn.gbm,method="cv")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00136.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The performance measurement plot</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">Then, you<a id="id680" class="calibre1"/> can retrieve the odd value of the log returned from the Bernoulli loss function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.predict = predict(churn.gbm, testset, n.trees = churn.iter)</strong></span>
<span class="strong"><strong class="calibre2">&gt; str(churn.predict)</strong></span>
<span class="strong"><strong class="calibre2"> num [1:1018] -3.31 -2.91 -3.16 -3.47 -3.48 ...</strong></span>
</pre></div></li><li class="listitem" value="7">Next, you can plot the ROC curve and get the best cut off that will have the maximum accuracy:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.roc = roc(testset$churn, churn.predict)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(churn.roc)</strong></span>
<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">roc.default(response = testset$churn, predictor = churn.predict)</strong></span>
<span class="strong"><strong class="calibre2">Data: churn.predict in 141 controls (testset$churn yes) &gt; 877 cases (testset$churn no).</strong></span>
<span class="strong"><strong class="calibre2">Area under the curve: 0.9393</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00137.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The ROC curve of fitted model</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="8">You <a id="id681" class="calibre1"/>can retrieve the best cut off with the <code class="email">coords</code> function and use this cut off to obtain the predicted label:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; coords(churn.roc, "best")</strong></span>
<span class="strong"><strong class="calibre2">  threshold specificity sensitivity </strong></span>
<span class="strong"><strong class="calibre2"> -0.9495258   0.8723404   0.9703535 </strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.predict.class = ifelse(churn.predict &gt; coords(churn.roc, "best")["threshold"], "yes", "no")</strong></span>
</pre></div></li><li class="listitem" value="9">Lastly, you can obtain the classification table from the predicted results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table( testset$churn,churn.predict.class)</strong></span>
<span class="strong"><strong class="calibre2">     churn.predict.class</strong></span>
<span class="strong"><strong class="calibre2">       no yes</strong></span>
<span class="strong"><strong class="calibre2">  yes  18 123</strong></span>
<span class="strong"><strong class="calibre2">  no  851  26</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with gradient boosting">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec327" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The algorithm of gradient boosting involves, first, the process computes the deviation of residuals for each partition, and then, determines the best data partitioning in each stage. Next, the successive model will fit the residuals from the previous stage and build a new model to reduce the residual variance (an error). The reduction of the residual variance follows the functional gradient descent technique, in which it minimizes the residual variance by going down its derivative, as show here:</p><div class="mediaobject"><img src="../images/00138.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">Gradient descent method</p></div></div><p class="calibre10"> </p><p class="calibre7">In this recipe, we use the gradient boosting method from <code class="email">gbm</code> to classify the telecom churn<a id="id682" class="calibre1"/> dataset. To begin the classification, we first install and load the <code class="email">gbm</code> package. Then, we use the <code class="email">gbm</code> function to train the classification model. Here, as our prediction target is the <code class="email">churn</code> attribute, which is a binary outcome, we therefore set the distribution as <code class="email">bernoulli</code> in the <code class="email">distribution</code> argument. Also, we set the 1000 trees to fit in the <code class="email">n.tree</code> argument, the maximum depth of the variable interaction to <code class="email">7</code> in <code class="email">interaction.depth</code>, the learning rate of the step size reduction to 0.01 in <code class="email">shrinkage</code>, and the number of cross-validations to <code class="email">3</code> in <code class="email">cv.folds</code>. After the model is fitted, we can use the summary function to obtain the relative influence information of each variable in the table and figure. The relative influence shows the reduction attributable to each variable in the sum of the square error. Here, we can find <code class="email">total_day_minutes</code> is the most influential one in reducing the loss function.</p><p class="calibre7">Next, we use the <code class="email">gbm.perf</code> function to find the optimum iteration. Here, we estimate the optimum number with cross-validation by specifying the <code class="email">method</code> argument to <code class="email">cv</code>. The function further generates two plots, where the black line plots the training error and the green one plots the validation error. The error measurement here is a <code class="email">bernoulli</code> distribution, which we have defined earlier in the training stage. The blue dash line on the plot shows where the optimum iteration is.</p><p class="calibre7">Then, we use the <code class="email">predict</code> function to obtain the odd value of a log in each testing case returned from the Bernoulli loss function. In order to get the best prediction result, one can set the <code class="email">n.trees</code> argument to an optimum iteration number. However, as the returned value is an odd value log, we still have to determine the best cut off to determine the label. Therefore, we use the <code class="email">roc</code> function to generate an ROC curve and get the cut off with the maximum accuracy.</p><p class="calibre7">Finally, we can use the function, <code class="email">coords</code>, to retrieve the best cut off threshold and use the <code class="email">ifelse</code> function to determine the class label from the odd value of the log. Now, we can use the <code class="email">table</code> function to generate the classification table and see how accurate the classification model is.</p></div></div>

<div class="book" title="Classifying data with gradient boosting">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec328" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">In addition<a id="id683" class="calibre1"/> to using the boosting function in the <code class="email">gbm</code> package, one can also use the <code class="email">mboost</code> package to perform classifications <a id="id684" class="calibre1"/>with the gradient boosting method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">mboost</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("mboost")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(mboost)</strong></span>
</pre></div></li><li class="listitem" value="2">The <code class="email">mboost</code> function only uses numeric responses; therefore, you should transform yes/no responses to numeric responses (0/1):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; trainset$churn = ifelse(trainset$churn == "yes", 1, 0)</strong></span>
</pre></div></li><li class="listitem" value="3">Also, you should remove nonnumerical attributes, such as <code class="email">voice_mail_plan</code> and <code class="email">international_plan</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; trainset$voice_mail_plan = NULL</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset$international_plan = NULL</strong></span>
</pre></div></li><li class="listitem" value="4">We can then use <code class="email">mboost</code> to train the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.mboost = mboost(churn ~ ., data=trainset,  control = boost_control(mstop = 10))</strong></span>
</pre></div></li><li class="listitem" value="5">Use the <code class="email">summary</code> function to obtain the details of the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(churn.mboost)</strong></span>

<span class="strong"><strong class="calibre2">   Model-based Boosting</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">mboost(formula = churn ~ ., data = trainset, control = boost_control(mstop = 10))</strong></span>


<span class="strong"><strong class="calibre2">   Squared Error (Regression) </strong></span>

<span class="strong"><strong class="calibre2">Loss function: (y - f)^2 </strong></span>

<span class="strong"><strong class="calibre2">Number of boosting iterations: mstop = 10 </strong></span>
<span class="strong"><strong class="calibre2">Step size:  0.1 </strong></span>
<span class="strong"><strong class="calibre2">Offset:  1.147732 </strong></span>
<span class="strong"><strong class="calibre2">Number of baselearners:  14 </strong></span>

<span class="strong"><strong class="calibre2">Selection frequencies:</strong></span>
<span class="strong"><strong class="calibre2">            bbs(total_day_minutes) bbs(number_customer_service_calls) </strong></span>
<span class="strong"><strong class="calibre2">                0.6                                0.4 </strong></span>
</pre></div></li><li class="listitem" value="6">Lastly, use<a id="id685" class="calibre1"/> the <code class="email">plot</code> function to draw a partial contribution plot of each attribute:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; par(mfrow=c(1,2))</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(churn.mboost)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00139.jpeg" alt="There's more..." class="calibre9"/><div class="caption"><p class="calibre12">The partial contribution plot of important attributes</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Calculating the margins of a classifier"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec96" class="calibre1"/>Calculating the margins of a classifier</h1></div></div></div><p class="calibre7">A margin<a id="id686" class="calibre1"/> is a measure of the certainty of classification. This method <a id="id687" class="calibre1"/>calculates the difference between the support of a<a id="id688" class="calibre1"/> correct class and the maximum support of an incorrect class. In this recipe, we will demonstrate how to calculate the margins of the generated classifiers.</p></div>

<div class="book" title="Calculating the margins of a classifier">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec329" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by storing a fitted bagging model in the variables, <code class="email">churn.bagging</code> and <code class="email">churn.predbagging</code>. Also, put the fitted boosting classifier in both <code class="email">churn.boost</code> and <code class="email">churn.boost.pred</code>.</p></div></div>

<div class="book" title="Calculating the margins of a classifier">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec330" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform<a id="id689" class="calibre1"/> the following steps to calculate the margin of<a id="id690" class="calibre1"/> each ensemble learner:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, use the <code class="email">margins</code> function to calculate the margins of the boosting classifiers:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; boost.margins = margins(churn.boost, trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; boost.pred.margins = margins(churn.boost.pred, testset)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use the <code class="email">plot</code> function to plot a marginal cumulative distribution graph of the boosting classifiers:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(sort(boost.margins[[1]]), (1:length(boost.margins[[1]]))/length(boost.margins[[1]]), type="l",xlim=c(-1,1),main="Boosting: Margin cumulative distribution graph", xlab="margin", ylab="% observations", col = "blue")</strong></span>
<span class="strong"><strong class="calibre2">&gt; lines(sort(boost.pred.margins[[1]]), (1:length(boost.pred.margins[[1]]))/length(boost.pred.margins[[1]]), type="l", col = "green")</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(v=0, col="red",lty=2)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00140.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The margin cumulative distribution graph of using the boosting method</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">You can then calculate the percentage of negative margin matches training errors <a id="id691" class="calibre1"/>and the percentage of negative <a id="id692" class="calibre1"/>margin matches test errors:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; boosting.training.margin = table(boost.margins[[1]] &gt; 0)</strong></span>
<span class="strong"><strong class="calibre2">&gt; boosting.negative.training = as.numeric(boosting.training.margin[1]/boosting.training.margin[2])</strong></span>
<span class="strong"><strong class="calibre2">&gt; boosting.negative.training</strong></span>
<span class="strong"><strong class="calibre2"> [1] 0.06387868</strong></span>

<span class="strong"><strong class="calibre2">&gt; boosting.testing.margin = table(boost.pred.margins[[1]] &gt; 0)</strong></span>
<span class="strong"><strong class="calibre2">&gt; boosting.negative.testing = as.numeric(boosting.testing.margin[1]/boosting.testing.margin[2])</strong></span>
<span class="strong"><strong class="calibre2">&gt; boosting.negative.testing</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.06263048</strong></span>
</pre></div></li><li class="listitem" value="4">Also, you can calculate the margins of the bagging classifiers. You might see the warning message showing "<code class="email">no non-missing argument to min</code>". The message simply indicates that the min/max function is applied to the numeric of the 0 length argument:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; bagging.margins = margins(churn.bagging, trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; bagging.pred.margins = margins(churn.predbagging, testset)</strong></span>
</pre></div></li><li class="listitem" value="5">You can then use the <code class="email">plot</code> function to plot a margin cumulative distribution graph of the bagging classifiers:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(sort(bagging.margins[[1]]), (1:length(bagging.margins[[1]]))/length(bagging.margins[[1]]), type="l",xlim=c(-1,1),main="Bagging: Margin cumulative distribution graph", xlab="margin", ylab="% observations", col = "blue")</strong></span>

<span class="strong"><strong class="calibre2">&gt; lines(sort(bagging.pred.margins[[1]]), (1:length(bagging.pred.margins[[1]]))/length(bagging.pred.margins[[1]]), type="l", col = "green")</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(v=0, col="red",lty=2)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00141.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The margin cumulative distribution graph of the bagging method</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">Finally, you<a id="id693" class="calibre1"/> can then compute the percentage<a id="id694" class="calibre1"/> of negative margin matches training errors and the percentage of negative margin matches test errors:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; bagging.training.margin = table(bagging.margins[[1]] &gt; 0)</strong></span>
<span class="strong"><strong class="calibre2">&gt; bagging.negative.training = as.numeric(bagging.training.margin[1]/bagging.training.margin[2])</strong></span>
<span class="strong"><strong class="calibre2">&gt; bagging.negative.training</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.1733401</strong></span>

<span class="strong"><strong class="calibre2">&gt; bagging.testing.margin = table(bagging.pred.margins[[1]] &gt; 0)</strong></span>
<span class="strong"><strong class="calibre2">&gt; bagging.negative.testing = as.numeric(bagging.testing.margin[1]/bagging.testing.margin[2])</strong></span>
<span class="strong"><strong class="calibre2">&gt; bagging.negative.testing</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.04303279</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Calculating the margins of a classifier">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec331" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">A margin is the measurement of certainty of the classification; it is computed by the support <a id="id695" class="calibre1"/>of the correct class and the maximum support <a id="id696" class="calibre1"/>of the incorrect class. The formula of margins can be formulated as:</p><div class="mediaobject"><img src="../images/00142.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">Here, the margin of the xi sample equals the support of a correctly classified sample (<span class="strong"><em class="calibre8">c</em></span> denotes the correct class) minus the maximum support of a sample that is classified to class <span class="strong"><em class="calibre8">j</em></span> (where <span class="strong"><em class="calibre8">j≠c</em></span> and <span class="strong"><em class="calibre8">j=1…k</em></span>). Therefore, correctly classified examples will have positive margins and misclassified examples will have negative margins. If the margin value is close to one, it means that correctly classified examples have a high degree of confidence. On the other hand, examples of uncertain classifications will have small margins.</p><p class="calibre7">The <code class="email">margins</code> function calculates the margins of AdaBoost.M1, AdaBoost-SAMME, or the bagging classifier, which returns a vector of a margin. To visualize the margin distribution, one can use a margin cumulative distribution graph. In these graphs, the x-axis shows the margin and the y-axis shows the percentage of observations where the margin is less than or equal to the margin value of the x-axis. If every observation is correctly classified, the graph will show a vertical line at the margin equal to 1 (where margin = 1).</p><p class="calibre7">For the margin cumulative distribution graph of the boosting classifiers, we can see that there are two lines plotted on the graph, in which the green line denotes the margin of the testing dataset, and the blue line denotes the margin of the training set. The figure shows about 6.39 percent of negative margins match the training error, and 6.26 percent of negative margins match the test error. On the other hand, we can find that 17.33% of negative margins match the training error and 4.3 percent of negative margins match the test error in the margin cumulative distribution graph of the bagging classifiers. Normally, the percentage of negative margins matching the training error should be close to the percentage of negative margins that match the test error. As a result of this, we should then examine the reason why the percentage of negative margins that match the training error is much higher than the negative margins that match the test error.</p></div></div>

<div class="book" title="Calculating the margins of a classifier">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec332" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">If you are interested in more details on margin distribution graphs, please refer to the following source: <span class="strong"><em class="calibre8">Kuncheva LI (2004)</em></span>, <span class="strong"><em class="calibre8">Combining Pattern Classifiers: Methods and Algorithms</em></span>, <span class="strong"><em class="calibre8">John Wiley &amp; Sons</em></span>.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Calculating the error evolution of the ensemble method"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec97" class="calibre1"/>Calculating the error evolution of the ensemble method</h1></div></div></div><p class="calibre7">The <code class="email">adabag</code> package provides the <code class="email">errorevol</code> function for a user to estimate the ensemble <a id="id697" class="calibre1"/>method errors in accordance <a id="id698" class="calibre1"/>with the number of iterations. In this recipe, we will demonstrate how to use <code class="email">errorevol</code> to show the evolution of errors of each ensemble classifier.</p></div>

<div class="book" title="Calculating the error evolution of the ensemble method">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec333" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by storing the fitted bagging model in the variable, <code class="email">churn.bagging</code>. Also, put the fitted boosting classifier in <code class="email">churn.boost</code>.</p></div></div>

<div class="book" title="Calculating the error evolution of the ensemble method">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec334" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to calculate the error evolution of each ensemble learner:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, use the <code class="email">errorevol</code> function to calculate the error evolution of the boosting classifiers:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; boosting.evol.train = errorevol(churn.boost, trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; boosting.evol.test = errorevol(churn.boost, testset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(boosting.evol.test$error, type = "l", ylim = c(0, 1),</strong></span>
<span class="strong"><strong class="calibre2">+       main = "Boosting error versus number of trees", xlab = "Iterations",</strong></span>
<span class="strong"><strong class="calibre2">+       ylab = "Error", col = "red", lwd = 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; lines(boosting.evol.train$error, cex = .5, col = "blue", lty = 2, lwd = 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; legend("topright", c("test", "train"), col = c("red", "blue"), lty = 1:2, lwd = 2)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00143.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Boosting error versus number of trees</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">Next, use<a id="id699" class="calibre1"/> the <code class="email">errorevol</code> function<a id="id700" class="calibre1"/> to calculate the error evolution of the bagging classifiers:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; bagging.evol.train = errorevol(churn.bagging, trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; bagging.evol.test = errorevol(churn.bagging, testset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(bagging.evol.test$error, type = "l", ylim = c(0, 1),</strong></span>
<span class="strong"><strong class="calibre2">+       main = "Bagging error versus number of trees", xlab = "Iterations",</strong></span>
<span class="strong"><strong class="calibre2">+       ylab = "Error", col = "red", lwd = 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; lines(bagging.evol.train$error, cex = .5, col = "blue", lty = 2, lwd = 2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; legend("topright", c("test", "train"), col = c("red", "blue"), lty = 1:2, lwd = 2)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00144.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Bagging error versus number of trees</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Calculating the error evolution of the ensemble method">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec335" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The <code class="email">errorest</code> function calculates the error evolution of AdaBoost.M1, AdaBoost-SAMME, or the bagging classifiers and returns a vector of error evolutions. In this recipe, we <a id="id701" class="calibre1"/>use the boosting and bagging <a id="id702" class="calibre1"/>models to generate error evolution vectors and graph the error versus number of trees.</p><p class="calibre7">The resulting graph reveals the error rate of each iteration. The trend of the error rate can help measure how fast the errors reduce, while the number of iterations increases. In addition to this, the graphs may show whether the model is over-fitted.</p></div></div>

<div class="book" title="Calculating the error evolution of the ensemble method">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec336" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">If the ensemble model is over-fitted, you can use the <code class="email">predict.bagging</code> and <code class="email">predict.boosting</code> functions to prune the ensemble model. For more information, please use the help function to refer to <code class="email">predict.bagging</code> and <code class="email">predict.boosting</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(predict.bagging)</strong></span>
<span class="strong"><strong class="calibre2">&gt; help(predict.boosting)</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with random forest"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec98" class="calibre1"/>Classifying data with random forest</h1></div></div></div><p class="calibre7">Random forest is another useful ensemble learning method that grows multiple decision<a id="id703" class="calibre1"/> trees during the training process. Each decision tree will output its own prediction results corresponding to the input. The forest will use the voting mechanism to select the most voted class as the prediction result. In this recipe, we will illustrate how to classify data using the <code class="email">randomForest</code> package.</p></div>

<div class="book" title="Classifying data with random forest">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec337" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this<a id="id704" class="calibre1"/> recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to perform classifications with the random forest method.</p></div></div>

<div class="book" title="Classifying data with random forest">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec338" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to classify data with random forest:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you have to install and load the <code class="email">randomForest</code> package;<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("randomForest")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(randomForest)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then fit the random forest classifier with a training set:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.rf = randomForest(churn ~ ., data = trainset, importance = T)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.rf</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2"> randomForest(formula = churn ~ ., data = trainset, importance = T) </strong></span>
<span class="strong"><strong class="calibre2">               Type of random forest: classification</strong></span>
<span class="strong"><strong class="calibre2">                     Number of trees: 500</strong></span>
<span class="strong"><strong class="calibre2">No. of variables tried at each split: 4</strong></span>

<span class="strong"><strong class="calibre2">        OOB estimate of  error rate: 4.88%</strong></span>
<span class="strong"><strong class="calibre2">Confusion matrix:</strong></span>
<span class="strong"><strong class="calibre2">    yes   no class.error</strong></span>
<span class="strong"><strong class="calibre2">yes 247   95 0.277777778</strong></span>
<span class="strong"><strong class="calibre2">no   18 1955 0.009123163</strong></span>
</pre></div></li><li class="listitem" value="3">Next, make predictions based on the fitted model and testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.prediction = predict(churn.rf, testset)</strong></span>
</pre></div></li><li class="listitem" value="4">Similar to other classification methods, you can obtain the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table(churn.prediction, testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">                </strong></span>
<span class="strong"><strong class="calibre2">churn.prediction yes  no</strong></span>
<span class="strong"><strong class="calibre2">             yes 110   7</strong></span>
<span class="strong"><strong class="calibre2">             no   31 870</strong></span>
</pre></div></li><li class="listitem" value="5">You <a id="id705" class="calibre1"/>can use the <code class="email">plot</code> function to plot the mean square error of the forest object:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(churn.rf)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00145.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The mean square error of the random forest</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">You can then examine the importance of each attribute within the fitted classifier:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; importance(churn.rf)</strong></span>
<span class="strong"><strong class="calibre2">                                      yes         no</strong></span>
<span class="strong"><strong class="calibre2">international_plan            66.55206691 56.5100647</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_plan               19.98337191 15.2354970</strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages         21.02976166 14.0707195</strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes             28.05190188 27.7570444</strong></span>
</pre></div></li><li class="listitem" value="7">Next, you can use the <code class="email">varImpPlot</code> function to obtain the plot of variable importance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; varImpPlot(churn.rf)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00146.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The visualization of variable importance</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="8">You <a id="id706" class="calibre1"/>can also use the <code class="email">margin</code> function to calculate the margins and plot the margin cumulative distribution:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; margins.rf=margin(churn.rf,trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(margins.rf)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00147.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The margin cumulative distribution graph for the random forest method</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="9">Furthermore, you can use a histogram to visualize the margin distribution of the random forest:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; hist(margins.rf,main="Margins of Random Forest for churn dataset")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00148.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">The histogram of margin distribution</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="10">You<a id="id707" class="calibre1"/> can also use <code class="email">boxplot</code> to visualize the margins of the random forest by class:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; boxplot(margins.rf~trainset$churn, main="Margins of Random Forest for churn dataset by class")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00149.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Margins of the random forest by class</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with random forest">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec339" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The purpose of random forest is to ensemble weak learners (for example, a single decision tree) into a strong learner. The process of developing a random forest is very similar to the bagging method, assuming that we have a training set containing <span class="strong"><em class="calibre8">N</em></span> samples <a id="id708" class="calibre1"/>with <span class="strong"><em class="calibre8">M</em></span> features. The process first performs bootstrap sampling, which samples <span class="strong"><em class="calibre8">N</em></span> cases at random, with the replacement as the training dataset of each single decision tree. Next, in each node, the process first randomly selects <span class="strong"><em class="calibre8">m</em></span> variables (where <span class="strong"><em class="calibre8">m &lt;&lt; M</em></span>), then finds the predictor variable that provides the best split among m variables. Next, the process grows the full tree without pruning. In the end, we can obtain the predicted result of an example from each single tree. As a result, we can get the prediction result by taking an average or weighted average (for regression) of an output or taking a majority vote (for classification):</p><div class="mediaobject"><img src="../images/00132.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">A random forest<a id="id709" class="calibre1"/> uses two parameters: <span class="strong"><strong class="calibre2">ntree</strong></span> (the number of trees) and <span class="strong"><strong class="calibre2">mtry</strong></span> (the number of features used to find the best feature), while the bagging method <a id="id710" class="calibre1"/>only uses ntree as a parameter. Therefore, if we set mtry equal to the number of features within a training dataset, then the random forest is equal to the bagging method.</p><p class="calibre7">The main <a id="id711" class="calibre1"/>advantages of random forest are that it is easy to compute, it can efficiently process data, and is fault tolerant to missing or unbalanced data. The main disadvantage of random forest is that it cannot predict the value beyond the range of a training dataset. Also, it is prone to over-fitting of noisy data.</p><p class="calibre7">In this recipe, we employ the random forest method adapted from the <code class="email">randomForest</code> package to fit a classification model. First, we install and load <code class="email">randomForest</code> into an R session. We then use the random forest method to train a classification model. We set <code class="email">importance = T</code>, which will ensure that the importance of the predictor is assessed.</p><p class="calibre7">Similar to the bagging and boosting methods, once the model is fitted, one can perform predictions using a fitted model on the testing dataset, and furthermore, obtain the classification table.</p><p class="calibre7">In order to <a id="id712" class="calibre1"/>assess the importance of each attribute, the <code class="email">randomForest</code> package provides the importance and <code class="email">varImpPlot</code> functions to either list the importance of each attribute in the fitted model or visualize the importance using either mean decrease accuracy or mean decrease <code class="email">gini</code>.</p><p class="calibre7">Similar to <code class="email">adabag</code>, which contains a method to calculate the margins of the bagging and boosting methods, <code class="email">randomForest</code> provides the <code class="email">margin</code> function to calculate the margins of the forest object. With the <code class="email">plot</code>, <code class="email">hist</code>, and <code class="email">boxplot</code> functions, you can visualize the margins in different aspects to the proportion of correctly classified observations.</p></div></div>

<div class="book" title="Classifying data with random forest">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec340" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">Apart from the <code class="email">randomForest</code> package, the <code class="email">party</code> package also provides an implementation<a id="id713" class="calibre1"/> of random forest. In the following steps, we illustrate how to use the <code class="email">cforest</code> function within the <code class="email">party</code> package to perform classifications:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, install and load the <code class="email">party</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("party")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(party)</strong></span>
</pre></div></li><li class="listitem" value="2">You can then use the <code class="email">cforest</code> function to fit the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.cforest = cforest(churn ~ ., data = trainset, controls=cforest_unbiased(ntree=1000, mtry=5))</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.cforest</strong></span>

<span class="strong"><strong class="calibre2">   Random Forest using Conditional Inference Trees</strong></span>

<span class="strong"><strong class="calibre2">Number of trees:  1000 </strong></span>

<span class="strong"><strong class="calibre2">Response:  churn </strong></span>
<span class="strong"><strong class="calibre2">Inputs:  international_plan, voice_mail_plan, number_vmail_messages, total_day_minutes, total_day_calls, total_day_charge, total_eve_minutes, total_eve_calls, total_eve_charge, total_night_minutes, total_night_calls, total_night_charge, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls </strong></span>
<span class="strong"><strong class="calibre2">Number of observations:  2315 </strong></span>
</pre></div></li><li class="listitem" value="3">You <a id="id714" class="calibre1"/>can make predictions based on the built model and the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.cforest.prediction = predict(churn.cforest, testset, OOB=TRUE, type = "response")</strong></span>
</pre></div></li><li class="listitem" value="4">Finally, obtain the classification table from the predicted labels and the labels of the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table(churn.cforest.prediction, testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">                        </strong></span>
<span class="strong"><strong class="calibre2">churn.cforest.prediction yes  no</strong></span>
<span class="strong"><strong class="calibre2">                     yes  91   3</strong></span>
<span class="strong"><strong class="calibre2">                     no   50 874</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Estimating the prediction errors of different classifiers"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec99" class="calibre1"/>Estimating the prediction errors of different classifiers</h1></div></div></div><p class="calibre7">At the beginning of this chapter, we discussed why we use ensemble learning and how it can improve<a id="id715" class="calibre1"/> the prediction performance compared to using just a single classifier. We now validate whether the ensemble model performs better than a single decision tree by comparing the performance of each method. In order to compare the different classifiers, we can perform a <a id="id716" class="calibre1"/>10-fold cross-validation on each classification method to <a id="id717" class="calibre1"/>estimate test errors using <code class="email">erroreset</code> from the <code class="email">ipred</code> package.</p></div>

<div class="book" title="Estimating the prediction errors of different classifiers">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec341" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to estimate the prediction errors of the different classifiers.</p></div></div>

<div class="book" title="Estimating the prediction errors of different classifiers">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec342" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to estimate the prediction errors of each classification method:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can estimate the error rate of the bagging model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.bagging= errorest(churn ~ ., data = trainset, model = bagging)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.bagging</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">errorest.data.frame(formula = churn ~ ., data = trainset, model = bagging)</strong></span>

<span class="strong"><strong class="calibre2">   10-fold cross-validation estimator of misclassification error </strong></span>

<span class="strong"><strong class="calibre2">Misclassification error:  0.0583 </strong></span>
</pre></div></li><li class="listitem" value="2">You <a id="id718" class="calibre1"/>can then estimate the error rate of the boosting method:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("ada")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(ada)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.boosting= errorest(churn ~ ., data = trainset, model = ada)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.boosting</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">errorest.data.frame(formula = churn ~ ., data = trainset, model = ada)</strong></span>

<span class="strong"><strong class="calibre2">   10-fold cross-validation estimator of misclassification error </strong></span>

<span class="strong"><strong class="calibre2">Misclassification error:  0.0475 </strong></span>
</pre></div></li><li class="listitem" value="3">Next, estimate the error rate of the random forest model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.rf= errorest(churn ~ ., data = trainset, model = randomForest)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.rf</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">errorest.data.frame(formula = churn ~ ., data = trainset, model = randomForest)</strong></span>

<span class="strong"><strong class="calibre2">   10-fold cross-validation estimator of misclassification error </strong></span>

<span class="strong"><strong class="calibre2">Misclassification error:  0.051 </strong></span>
</pre></div></li><li class="listitem" value="4">Finally, make <a id="id719" class="calibre1"/>a prediction function using <code class="email">churn.predict</code>, and then use the function to estimate the error rate of the single decision tree:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.predict = function(object, newdata) {predict(object, newdata = newdata, type = "class")}</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.tree= errorest(churn ~ ., data = trainset, model = rpart,predict = churn.predict)</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.tree</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">errorest.data.frame(formula = churn ~ ., data = trainset, model = rpart, </strong></span>
<span class="strong"><strong class="calibre2">    predict = churn.predict)</strong></span>

<span class="strong"><strong class="calibre2">   10-fold cross-validation estimator of misclassification error </strong></span>

<span class="strong"><strong class="calibre2">Misclassification error:  0.0674 </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Estimating the prediction errors of different classifiers">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec343" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we estimate the error rates of four different classifiers using the <code class="email">errorest</code> function from the <code class="email">ipred</code> package. We compare the boosting, bagging, and random forest methods, and the single decision tree classifier. The <code class="email">errorest</code> function performs a 10-fold cross-validation on each classifier and calculates the misclassification error. The estimation results from the four chosen models reveal that the boosting method performs the best with the lowest error rate (0.0475). The random forest method has the second lowest error rate (0.051), while the bagging method has an error rate of 0.0583. The single decision tree classifier, <code class="email">rpart</code>, performs the worst among the four methods with an error rate equal to 0.0674. These results show that all three ensemble learning methods, boosting, bagging, and random forest, outperform a single decision tree classifier.</p></div></div>

<div class="book" title="Estimating the prediction errors of different classifiers">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec344" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">In this recipe we mentioned the <code class="email">ada</code> package, which contains a method to perform stochastic boosting. For those interested in this package, please refer to: <span class="strong"><em class="calibre8">Additive Logistic Regression: A Statistical View of Boosting by Friedman</em></span>, <span class="strong"><em class="calibre8">et al. (2000)</em></span>.</li></ul></div></div></div></body></html>