<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Real-Time Machine Learning Using Apache Spark</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will extend our deployment of machine learning models beyond batch processing in order to learn from data, make predictions, and identify trends in real time! We will develop and deploy a real-time stream processing and machine learning application comprised of the following high-level technologies:</p>
<ul>
<li>Apache Kafka producer application</li>
<li>Apache Kafka consumer application</li>
<li>Apache Spark's Structured Streaming engine</li>
<li>Apache Spark's machine learning library, <kbd>MLlib</kbd></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Distributed streaming platform</h1>
                </header>
            
            <article>
                
<p>So far in this book, we have been performing batch processing—that is, we have been provided with bounded raw data files and processed that data as a group. As we saw in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, stream processing differs from batch processing in the fact that data is processed as and when individual units, or streams, of data arrive. We also saw in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, how <strong>Apache Kafka</strong>, as a distributed <em>streaming platform</em>, allows us to move real-time data between systems and applications in a fault-tolerant and reliable manner via a logical streaming architecture comprising of the following components:</p>
<ul>
<li><strong>Producers</strong>: Applications that generate and send messages</li>
<li><strong>Consumers</strong>: Applications that subscribe to and consume messages</li>
</ul>
<ul>
<li><strong>Topics</strong>: Streams of records belonging to a particular category and stored as a sequence of ordered and immutable records partitioned and replicated across a distributed cluster</li>
<li><strong>Stream processors</strong>: Applications that process messages in a certain manner, such as data transformations and machine learning models</li>
</ul>
<p>A simplified illustration of this logical streaming architecture is shown in <em>Figure 8.1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-601 image-border" src="Images/92b5f057-8b17-4e0f-a931-9f5cb7851f82.png" style="width:20.17em;height:7.50em;" width="746" height="277"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.1: Apache Kafka logical streaming architecture</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Distributed stream processing engines</h1>
                </header>
            
            <article>
                
<p>Apache Kafka allows us to <em>move</em> real-time data <span>reliably</span> between systems and applications. But we still need some sort of processing engine to process and transform that real-time data in order ultimately <span>to </span>derive value from it based on the use case in question. Fortunately, there are a number of <em>stream processing engines</em> available to allow us to do this, including—but not limited—to the following:</p>
<ul>
<li><strong>Apache Spark:</strong> <a href="https://spark.apache.org/">https://spark.apache.org/</a></li>
<li><strong>Apache Storm:</strong> <a href="http://storm.apache.org/">http://storm.apache.org/</a></li>
<li><strong>Apache Flink: </strong><a href="https://flink.apache.org/">https://flink.apache.org/</a></li>
<li><strong>Apache Samza:</strong> <a href="http://samza.apache.org/">http://samza.apache.org/</a></li>
<li><strong>Apache Kafka (via its Streams API):</strong> <a href="https://kafka.apache.org/documentation/">https://kafka.apache.org/documentation/</a></li>
<li><strong>KSQL:</strong> <a href="https://www.confluent.io/product/ksql/">https://www.confluent.io/product/ksql/</a></li>
</ul>
<p>Though a detailed comparison of the available stream processing engines is beyond the scope of this book, you are encouraged to explore the preceding links and study the differing architectures available. For the purposes of this chapter, we will be using Apache Spark's Structured Streaming engine as our stream processing engine of choice.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Streaming using Apache Spark</h1>
                </header>
            
            <article>
                
<p>At the time of writing, there are two stream processing APIs available in Spark:</p>
<ul>
<li><strong>Spark Streaming (DStreams): </strong><a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li>
<li><strong>Structured Streaming:</strong> <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spark Streaming (DStreams)</h1>
                </header>
            
            <article>
                
<p><em>Spark Streaming (DStreams)</em> extends the core Spark API and works by dividing real-time data streams into <em>input batches</em> that are then processed by Spark's core API, resulting in a final stream of <em>processed batches</em>, as illustrated in <em>Figure 8.2</em>. A sequence of RDDs form what is known as a <em>discretized stream</em> (or DStream), which represents the continuous stream of data:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-602 image-border" src="Images/bfd910ee-aea3-4890-92e1-10daa23bb5c5.png" style="width:27.92em;height:5.50em;" width="992" height="195"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.2: Spark Streaming (DStreams)</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Structured Streaming</h1>
                </header>
            
            <article>
                
<p><em>Structured Streaming</em>, on the other hand, is a newer and highly optimized stream processing engine built on the Spark SQL engine in which streaming data can be stored and processed using Spark's Dataset/DataFrame API (see <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>). As of Spark 2.3, Structured Streaming offers the ability to process data streams using both micro-batch processing, with latencies as low as 100 milliseconds, and <em>continuous processing</em>, with latencies as low as 1 millisecond (thereby providing <em>true</em> real-time processing). Structured Streaming works by modelling data streams as an unbounded table that is being continuously appended. When a transformation or other type of query is processed on this unbounded table, a results table will be generated that is representative of that moment in time.</p>
<p>After a configurable trigger interval, new data in the data stream is modeled as new rows appended to this unbounded table and the results table is subsequently updated, as illustrated in <em>Figure</em> <em>8.3</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-603 image-border" src="Images/9b60d2e4-726c-40ca-ab4f-a886e3e85b3f.png" style="width:27.42em;height:21.58em;" width="836" height="659"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.3: Spark Structured Streaming logical model</div>
<p>As streaming data is exposed via the Dataset/DataFrame API, both SQL-like operations (including aggregations and joins) and RDD operations (including map and filtering) can easily be executed on real-time streams of data. Furthermore, Structured Streaming offers features that are designed to cater for data that arrives late, the management and monitoring of streaming queries, and the ability to recover from failures. As such, Structured Streaming is an extremely versatile, efficient, and reliable way to process streaming data with extremely low latencies, and is the stream processing engine that we will use for the remainder of this chapter.</p>
<div class="packt_tip">In general, it is advised that developers use this newer and highly optimized engine over Spark Streaming (DStreams). However, since it is a newer API, there may be certain features that are not yet available as of Spark 2.3.2, which will mean the continued occasional usage of the DStreams RDD-based approach while the newer API is being developed.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stream processing pipeline</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will develop an end-to-end stream processing pipeline that is capable of streaming data from a source system that generates continuous data, and thereafter able to publish those streams to an Apache Kafka distributed cluster. Our stream processing pipeline will then use Apache Spark to both consume data from Apache Kafka, using its Structured Streaming engine, and apply trained machine learning models to these streams in order to derive insights in real time using <kbd>MLlib</kbd>. The end-to-end stream processing pipeline that we will develop is illustrated in <em>Figure 8.4</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-604 image-border" src="Images/79b87bac-111a-4861-8808-9654b33c059c.png" style="width:51.58em;height:14.17em;" width="1565" height="431"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.4: Our end-to-end stream processing pipeline</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study – real-time sentiment analysis</h1>
                </header>
            
            <article>
                
<p>In the case study for this chapter, we will extend the sentiment analysis model that we developed in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, to operate in real time. In <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, we trained a decision tree classifier to predict and classify the underlying sentiment of tweets based on a training dataset of historic tweets about airlines. In this chapter, we will apply this trained decision tree classifier to real-time tweets in order to predict their sentiment and identify negative tweets so that airlines may act on them as soon as possible.</p>
<p>Our end-to-end stream processing pipeline can therefore be extended, as illustrated in <em>Figure 8.5</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-605 image-border" src="Images/07bd36e1-f7f5-4696-9710-517f5f63d164.png" style="width:47.58em;height:12.83em;" width="1965" height="530"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8.5: Our end-to-end stream processing pipeline for real-time sentiment analysis</div>
<p>The core stages of our stream processing pipeline for real-time sentiment analysis are as follows:</p>
<ol>
<li><strong>Kafka producer:</strong> We will develop a Python application, using the <kbd>pykafka</kbd> (an Apache Kafka client for Python) and <kbd>tweepy</kbd> (a Python library for accessing the Twitter API) libraries that we installed in <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up a Local Development Environment</em>, to capture tweets about airlines that are being tweeted in real time and to then publish those tweets to an Apache Kafka topic called <kbd>twitter</kbd>.</li>
<li><strong>Kafka consumer:</strong> We will then develop a Spark application, using its Structured Streaming API, to subscribe to and then consume tweets from the <kbd>twitter</kbd> topic into a Spark dataframe.</li>
<li><strong>Stream processor and</strong> <kbd>MLlib</kbd><strong>:</strong> We will then preprocess the raw textual content of the tweets stored in this Spark dataframe using the same pipeline of feature transformers and feature extractors that we studied and developed in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label"><em>Natural Language Processing Using Apache Spark</em>, </span></span>namely tokenization, removing stop words, stemming, and normalization—before applying the HashingTF transformer to generate feature vectors in real time.</li>
<li><strong>Trained decision tree classifier:</strong> Next, we will load the decision tree classifier that we trained in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank"/><a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark,</em> and persisted to the local filesystem of our single development node. Once loaded, we will apply this trained decision tree classifier to the Spark dataframe containing our preprocessed feature vectors derived from real time tweets in order to predict and classify their underlying sentiment.</li>
</ol>
<ol start="5">
<li><strong>Output sink:</strong> Finally, we will output the results of our sentiment analysis model applied to real-time tweets to a target destination, called an output <em>sink</em>. In our case, the output sink will be the <em>console</em> sink, one of the built-in output sinks provided natively by the Structured Streaming API. By using this sink, the output is printed to the console/<strong>standard output</strong> (<strong>stdout</strong><em>)</em> every time there is a trigger. From this console, we will be able to read both the raw textual content of the original tweets and the predicted sentiment classification from our model, namely negative or non-negative. To learn more about the various output sinks available, please visit <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-sinks</a>.</li>
</ol>
<p>The following subsections describe the technical steps that we will follow to develop, deploy, and run our end-to-end stream processing pipeline for real-time sentiment analysis.</p>
<div class="packt_infobox">Note that for the purposes of this case study, we will not be using Jupyter notebooks for development. This is because separate code files are required for the separate components, as described previously. This case study therefore provides another glimpse into how a production-grade pipeline should be developed and executed. Rather than instantiating a <kbd>SparkContext</kbd> explicitly within a notebook, we will instead submit our Python code files and all dependencies to <kbd>spark-submit</kbd> via the Linux command line.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Start Zookeeper and Kafka Servers</h1>
                </header>
            
            <article>
                
<p>The first step is to ensure that our single-node Kafka cluster is up and running. As described in <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up a Local Development Environment</em>, please execute the following commands to start Apache Kafka:</p>
<pre><strong>&gt; cd {KAFKA_HOME}</strong><br/><strong>&gt; bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</strong><br/><strong>&gt; bin/kafka-server-start.sh -daemon config/server.properties</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kafka topic</h1>
                </header>
            
            <article>
                
<p>Next, we need to create a Kafka topic to which our Python Kafka producer application (which we will develop later on) will publish real-time tweets about airlines. In our case, we will call the topic <kbd>twitter</kbd>. As demonstrated in <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>,<em> Setting Up a Local Development Environment</em>, this can be achieved as follows:</p>
<pre><strong>&gt; bin/kafka-topics.sh --create --zookeeper 192.168.56.10:2181 --replication-factor 1 --partitions 1 --topic twitter</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Twitter developer account</h1>
                </header>
            
            <article>
                
<p>In order for our Python Kafka producer application to capture tweets in real time, we require access to the Twitter API. As of July 2018, a Twitter <em>developer account</em>, in addition to a normal Twitter account, must be created and approved in order to access its API. In order to apply for a developer account, please go to <a href="https://apps.twitter.com/">https://apps.twitter.com/</a>, click on the <span class="packt_screen">Apply for a Developer Account</span> button, and fill in the required details.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Twitter apps and the Twitter API</h1>
                </header>
            
            <article>
                
<p>Once you have created your Twitter developer account, in order to use the Twitter API, a Twitter <em>app</em> must be created. A Twitter app provides authenticated and authorized access to the Twitter API based on the specific purpose of the app that you intend to create. In order to create a Twitter app for the purposes of our real-time sentiment analysis model, please go through the following instructions (valid at the time of writing):</p>
<ol>
<li>Navigate to <a href="https://developer.twitter.com/en/apps">https://developer.twitter.com/en/apps</a>.</li>
<li>Select the <span class="packt_screen">Create an App</span> button.</li>
<li>Provide the following mandatory app details:</li>
</ol>
<pre style="color: black;padding-left: 60px"><strong>- App Name</strong> (max 32 characters) e.g. "Airline Sentiment Analysis"<br/><strong>- Application Description</strong> (max 200 characters) e.g. "This App will collect tweets about airlines and apply our previously trained decision tree classifier to predict and classify the underlying sentiment of those tweets in real-time"<br/><strong>- Website URL</strong> (for attribution purposes only - if you do not have a personal website, then use the URL to your Twitter page, such as <a href="https://twitter.com/PacktPub">https://twitter.com/PacktPub</a>)<br/><strong>- Tell us how this app will be used</strong> (min 100 characters) e.g. "Internal training and development purposes only, including the deployment of machine learning models in real-time. It will not be visible to customers or 3rd parties."</pre>
<ol start="4">
<li>Click the <span class="packt_screen">Create</span> button to create your Twitter app.</li>
<li>Once your Twitter app has been created, navigate to the <span class="packt_screen">Keys and Tokens</span> tab.</li>
<li>Make a note of your <span class="packt_screen">Consumer API Key</span> and <span class="packt_screen">Consumer API Secret Key</span> strings respectively.</li>
<li>Then click the <span class="packt_screen">Create</span> button under <span class="packt_screen">Access Token &amp; Access Token Secret</span> to generate access tokens for your Twitter app. Set the access level to <span class="packt_screen">Read-only</span> as this Twitter app will only read tweets, and will not generate any of its own.</li>
<li>Make a note of the resulting <strong><span class="packt_screen">Access Token</span></strong> and <span class="packt_screen">Access Token Secret</span> strings respectively.</li>
</ol>
<p>The consumer API keys and access tokens will be used to provision our Python-based Kafka producer application read-only access to the stream of real-time tweets via the Twitter API, so it is important that you make a note of them.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application configuration</h1>
                </header>
            
            <article>
                
<p>We are now ready to start developing our end-to-end stream processing pipeline! First, let's create a configuration file in Python that will store all environmental and application-level options pertinent to our pipeline and local development node, as follows:</p>
<div class="mce-root packt_infobox">The following Python configuration file, called <kbd>config.py</kbd>, can be found in the GitHub repository accompanying this book.</div>
<pre>#!/usr/bin/python<br/><br/>""" config.py: Environmental and Application Settings """<br/><br/>""" ENVIRONMENT SETTINGS """<br/><br/># Apache Kafka<br/>bootstrap_servers = '192.168.56.10:9092'<br/>data_encoding = 'utf-8'<br/><br/>""" TWITTER APP SETTINGS """<br/><br/>consumer_api_key = 'Enter your Twitter App Consumer API Key here'<br/>consumer_api_secret = 'Enter your Twitter App Consumer API Secret Key here'<br/>access_token = 'Enter your Twitter App Access Token here'<br/>access_token_secret = 'Enter your Twitter App Access Token Secret here'<br/><br/>""" SENTIMENT ANALYSIS MODEL SETTINGS """<br/><br/># Name of an existing Kafka Topic to publish tweets to<br/>twitter_kafka_topic_name = 'twitter'<br/><br/># Keywords, Twitter Handle or Hashtag used to filter the Twitter Stream<br/>twitter_stream_filter = '@British_Airways'<br/><br/># Filesystem Path to the Trained Decision Tree Classifier<br/>trained_classification_model_path = '..chapter06/models/airline-sentiment-analysis-decision-tree-classifier'</pre>
<p>This Python configuration file defines the following pertinent options:</p>
<ul>
<li><kbd>bootstrap_servers</kbd>: A comma-delimited list of the hostname/IP address and port number pairings for the Kafka brokers. In our case, this is just the hostname/IP address of our single-node development environment at port <kbd>9092</kbd> by default.</li>
<li><kbd>consumer_api_key</kbd>: Enter the consumer API key associated with your Twitter app here.</li>
<li><kbd>consumer_api_secret</kbd>: Enter the consumer API secret key associated with your Twitter app here.</li>
<li><kbd>access_token</kbd>: Enter the access token associated with your Twitter app here.</li>
<li><kbd>access_token_secret</kbd>: Enter the access token secret associated with your Twitter app here.</li>
<li><kbd>twitter_kafka_topic_name</kbd>: The name of the Kafka topic to which our Kafka producer will publish tweets and from which our Structured Streaming Spark application will consume tweets.</li>
<li><kbd>twitter_stream_filter</kbd>: A keyword, Twitter handle, or hashtag to use in order to filter the stream of real-time tweets being captured from the Twitter API. In our case, we are filtering for real-time tweets directed at <kbd>@British_Airways</kbd>.</li>
<li><kbd>trained_classification_model_path</kbd>: The absolute path where we saved our trained decision tree classifier in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kafka Twitter producer application</h1>
                </header>
            
            <article>
                
<p>We are now ready to develop our Python-based Kafka producer application that will capture tweets about airlines that are being tweeted in real-time and then publish those tweets to the Apache Kafka <kbd>twitter</kbd> topic that we created previously. We will be using the following two Python libraries in order to develop our Kafka producer:</p>
<ul>
<li><kbd>tweepy</kbd>: This library allows us to access the Twitter API <span>programmatically </span>using Python and the consumer API keys and access tokens that we generated earlier</li>
<li><kbd>pykafka</kbd>: This library allow us to instantiate a Python-based Apache Kafka client through which we can communicate and transact with our single-node Kafka cluster.</li>
</ul>
<div class="packt_infobox">The following Python code file, called <kbd>kafka_twitter_producer.py</kbd>, can be found in the GitHub repository accompanying this book.</div>
<p><span>In regards to our Python-based Kafka producer application, we perform the following steps (numbered to correspond to the numbered comments in our Python code file):</span></p>
<ol>
<li>First, we import the required modules from the <kbd>tweepy</kbd> and <kbd>pykafka</kbd> libraries respectively, as shown in the following code. We also import the configuration from our <kbd>config.py</kbd> file, which we created earlier:</li>
</ol>
<pre style="padding-left: 60px">import config<br/>import tweepy<br/>from tweepy import OAuthHandler<br/>from tweepy import Stream<br/>from tweepy.streaming import StreamListener<br/>import pykafka</pre>
<ol start="2">
<li>Next, we instantiate a <kbd>tweepy</kbd> wrapper for the Twitter API using the consumer API keys and access tokens defined in <kbd>config.py</kbd> to provide us authenticated and authorized programmatic access to the Twitter API, as follows:</li>
</ol>
<pre style="padding-left: 60px">auth = OAuthHandler(config.consumer_api_key, <br/>   config.consumer_api_secret)<br/>auth.set_access_token(config.access_token, <br/>   config.access_token_secret)<br/>api = tweepy.API(auth)</pre>
<ol start="3">
<li>We then define a class in Python called <kbd>KafkaTwitterProducer</kbd>, which once instantiated, provides us with a <kbd>pykafka</kbd> client to our single-node Apache Kafka cluster, as shown in the following code. When this class is instantiated, it initially executes the code defined in the <kbd>__init__</kbd> function, which creates a <kbd>pykafka</kbd> client using the bootstrap servers, the locations of which may be found in <kbd>config.py</kbd>. It then creates a Kafka producer that associates the producer to the <kbd>twitter_kafka_topic_name</kbd> Kafka topic also defined in <kbd>config.py</kbd>. When data is captured by our <kbd>pykafka</kbd> producer, the <kbd>on_data</kbd> function is invoked, which physically publishes the data to the Kafka topic.</li>
</ol>
<p style="padding-left: 60px">If our <kbd>pykafka</kbd> producer encounters an error, then the <kbd>on_error</kbd> function is invoked, which, in our case, simply prints the error to the console and goes on to process the next message:</p>
<pre style="padding-left: 60px">class KafkaTwitterProducer(StreamListener):<br/><br/>    def __init__(self):<br/>        self.client = pykafka.KafkaClient(config.bootstrap_servers)<br/>        self.producer = self.client.topics[bytes(<br/>           config.twitter_kafka_topic_name, <br/>           config.data_encoding)].get_producer()<br/><br/>    def on_data(self, data):<br/>        self.producer.produce(bytes(data, config.data_encoding))<br/>        return True<br/><br/>    def on_error(self, status):<br/>        print(status)<br/>        return True</pre>
<ol start="4">
<li>Next, we instantiate a Twitter stream using the <kbd>Stream</kbd> module of the <kbd>tweepy</kbd> library. To achieve this, we simply pass our Twitter app authentication details and an instance of our <kbd>KafkaTwitterProducer</kbd> class to the Stream module:</li>
</ol>
<pre style="padding-left: 60px">print("Instantiating a Twitter Stream and publishing to the '%s' <br/>   Kafka Topic..." % config.twitter_kafka_topic_name)<br/>twitter_stream = Stream(auth, KafkaTwitterProducer())</pre>
<ol start="5">
<li>Now that we have instantiated a Twitter stream, the final step is to filter the stream to deliver tweets of interest, based on the <kbd>twitter_stream_filter</kbd> option found in <kbd>config.py</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">print("Filtering the Twitter Stream based on the query '%s'..." % <br/>   config.twitter_stream_filter)<br/>twitter_stream.filter(track=[config.twitter_stream_filter])</pre>
<p>We are now ready to run our Kafka producer application! Since it is a Python application, the easiest way to run it is simply <span>to </span>use the Linux command line, navigate to the directory containing <kbd>kafka_twitter_producer.py</kbd>, and execute it as follows:</p>
<pre><strong>&gt; python kafka_twitter_producer.py</strong><br/><strong>   $ Instantiating a Twitter Stream and publishing to the 'twitter' <br/>     Kafka Topic...</strong><br/><strong>   $ Filtering the Twitter Stream based on the query <br/>     '@British_Airways'...</strong></pre>
<p><span>To check that it is actually capturing and publishing real-time tweets to Kafka, as described in <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up a Local Development Environment</em>, you can start a command-line consumer application to consume messages from the Twitter topic and print them to the console, as follows:</span></p>
<pre><strong>&gt; cd {KAFKA_HOME}</strong><br/><strong>&gt; bin/kafka-console-consumer.sh --bootstrap-server 192.168.56.10:9092 --topic twitter</strong></pre>
<p><span>Hopefully, you will see </span><span>tweets</span><span> </span><span>printed to the console in real time. In our case, these tweets are all directed to</span> <kbd>"@British_Airways"</kbd><span>.</span></p>
<div class="mce-root packt_infobox">The tweets themselves are captured via the Twitter API in JSON format, and contain not only the raw textual content of the tweet, but also associated metadata, such as the tweet ID, the username of the tweeter, the timestamp, and so on. For a full description of the JSON schema, please visit <a href="https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html">https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object.html</a>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing and feature vectorization pipelines</h1>
                </header>
            
            <article>
                
<p class="mce-root">As described earlier, in order to be able to apply our trained decision tree classifier to these real-time tweets, we first need to preprocess and vectorize them exactly as we did with our training and test datasets in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>. However, rather than duplicating the preprocessing and vectorization pipeline logic within our Kafka consumer application itself, we will define our pipeline logic in a separate Python module and within Python <em>functions</em>. This way, any time we need to preprocess text as we did in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, we simply call the relevant Python function, thereby avoiding the need to duplicate the same code across different Python code files.</p>
<div class="packt_infobox">The following Python code file, called <kbd>model_pipelines.py</kbd>, can be found in the GitHub repository accompanying this book.</div>
<p><span>In the following Python module, we define two functions. The first function applies the exact same pipeline of</span> <kbd>MLlib</kbd> <span>and <kbd>spark-nlp</kbd> feature transformers that we studied in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, in order to preprocess the raw textual content of the tweets. The second function then takes a preprocessed Spark dataframe and applies the </span>HashingTF <span>transformer to it in order to generate feature vectors based on term frequencies, exactly as we studied in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>. The result is a Spark dataframe containing the original raw text of the tweet in a column called <kbd>text</kbd> and term frequency feature vectors in a column called <kbd>features</kbd>:</span></p>
<pre>#!/usr/bin/python<br/><br/>""" model_pipelines.py: Pre-Processing and Feature Vectorization Spark Pipeline function definitions """<br/><br/>from pyspark.sql.functions import *<br/>from pyspark.ml.feature import Tokenizer<br/>from pyspark.ml.feature import StopWordsRemover<br/>from pyspark.ml.feature import HashingTF<br/>from pyspark.ml import Pipeline, PipelineModel<br/><br/>from sparknlp.base import *<br/>from sparknlp.annotator import Tokenizer as NLPTokenizer<br/>from sparknlp.annotator import Stemmer, Normalizer<br/><br/>def preprocessing_pipeline(raw_corpus_df):<br/><br/>    # Native MLlib Feature Transformers<br/>    filtered_df = raw_corpus_df.filter("text is not null")<br/>    tokenizer = Tokenizer(inputCol = "text", outputCol = "tokens_1")<br/>    tokenized_df = tokenizer.transform(filtered_df)<br/>    remover = StopWordsRemover(inputCol = "tokens_1",<br/>       outputCol = "filtered_tokens")<br/><br/>    preprocessed_part_1_df = remover.transform(tokenized_df)<br/>    preprocessed_part_1_df = preprocessed_part_1_df<br/>       .withColumn("concatenated_filtered_tokens", concat_ws(" ",<br/>          col("filtered_tokens")))<br/><br/>    # spark-nlp Feature Transformers<br/>    document_assembler = DocumentAssembler()<br/>       .setInputCol("concatenated_filtered_tokens")<br/>    tokenizer = NLPTokenizer()<br/>       .setInputCols(["document"]).setOutputCol("tokens_2")<br/>    stemmer =    <br/>    Stemmer().setInputCols(["tokens_2"]).setOutputCol("stems")<br/>    normalizer = Normalizer().setInputCols(["stems"])<br/>       .setOutputCol("normalised_stems")<br/><br/>    preprocessing_pipeline = Pipeline(stages = [document_assembler,<br/>       tokenizer, stemmer, normalizer])<br/>    preprocessing_pipeline_model = preprocessing_pipeline<br/>       .fit(preprocessed_part_1_df)<br/>    preprocessed_df = preprocessing_pipeline_model<br/>       .transform(preprocessed_part_1_df)<br/>    preprocessed_df.select("id", "text", "normalised_stems")<br/><br/>    # Explode and Aggregate<br/>    exploded_df = preprocessed_df<br/>       .withColumn("stems", explode("normalised_stems"))<br/>       .withColumn("stems", col("stems").getItem("result"))<br/>       .select("id", "text", "stems")<br/><br/>    aggregated_df = exploded_df.groupBy("id")<br/>       .agg(concat_ws(" ", collect_list(col("stems"))), first("text"))<br/>       .toDF("id", "tokens", "text")<br/>       .withColumn("tokens", split(col("tokens"), " ")<br/>       .cast("array&lt;string&gt;"))<br/><br/>    # Return the final processed DataFrame<br/>    return aggregated_df<br/><br/>def vectorizer_pipeline(preprocessed_df):<br/><br/>    hashingTF = HashingTF(inputCol = "tokens", outputCol = "features",<br/>       numFeatures = 280)<br/>    features_df = hashingTF.transform(preprocessed_df)<br/><br/>    # Return the final vectorized DataFrame<br/>    return features_df</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Kafka Twitter consumer application</h1>
                </header>
            
            <article>
                
<p>We are finally ready to develop our Kafka consumer application using the Spark Structured Streaming engine in order to apply our trained decision tree classifier to the stream of real-time tweets in order to deliver real-time sentiment analysis!</p>
<div class="packt_infobox">The following Python code file, called <kbd>kafka_twitter_consumer.py</kbd>, can be found in the GitHub repository accompanying this book.</div>
<p><span>In regards to our Spark Structured-Streaming-based Kafka consumer application, we perform the following steps (numbered to correspond to the numbered comments in our Python code file):</span></p>
<ol start="1">
<li>First, we import the configuration from our <kbd>config.py</kbd> file. We also import the Python functions containing the logic for our preprocessing and vectorization pipelines that we created earlier, as follows:</li>
</ol>
<pre style="padding-left: 60px">import config<br/>import model_pipelines</pre>
<ol start="2">
<li>Unlike our Jupyter notebook case studies, there is no need explicitly to instantiate a <kbd>SparkContext</kbd> as this will be done for us when we execute our Kafka consumer application via <kbd>spark-submit</kbd> in the command line. In this case study, we create a <kbd>SparkSession</kbd>, as shown in the following code that acts as an entry point into the Spark execution environment—even if it is already running—and which subsumes <kbd>SQLContext</kbd>. We can therefore use <kbd>SparkSession</kbd> to undertake the same SQL-like operations over data that we have seen previously, while still using the Spark Dataset/DataFrame API:</li>
</ol>
<pre style="padding-left: 60px">spark = SparkSession.builder.appName("Stream Processing - Real-Time Sentiment Analysis").getOrCreate()</pre>
<ol start="3">
<li>In this step, we load the decision tree classifier that we trained in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank"/><a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, (which used the <em>HashingTF</em> feature extractor) from the local filesystem into a <kbd>DecisionTreeClassificationModel</kbd> object so that we can apply it later on, as shown in the following code. Note that the absolute path to the trained decision tree classifier has been defined in <kbd>config.py</kbd>:</li>
</ol>
<pre style="padding-left: 60px">decision_tree_model = DecisionTreeClassificationModel.load(<br/>   config.trained_classification_model_path)</pre>
<ol start="4">
<li>We are almost ready to start consuming messages from our single-node Kafka cluster. However, before doing so, we must note that Spark does not yet support the automatic inference and parsing of JSON key values into Spark dataframe columns. We must therefore explicitly define the JSON schema, or the subset of the JSON schema that we wish to retain, as follows:</li>
</ol>
<pre style="padding-left: 60px">schema = StructType([<br/>    StructField("created_at", StringType()), <br/>    StructField("id", StringType()), <br/>    StructField("id_str", StringType()), <br/>    StructField("text", StringType()), <br/>    StructField("retweet_count", StringType()), <br/>    StructField("favorite_count", StringType()), <br/>    StructField("favorited", StringType()), <br/>    StructField("retweeted", StringType()), <br/>    StructField("lang", StringType()), <br/>    StructField("location", StringType()) <br/>])</pre>
<ol start="5">
<li>Now that we have defined our JSON schema, we are ready to start consuming messages. To do this, we invoke the <kbd>readStream</kbd> method on our <kbd>SparkSession</kbd> instance to consume streaming data. We specify that the source of our stream will be a Kafka cluster using the <kbd>format</kbd> method, after which we define the Kafka bootstrap servers and the name of the Kafka topic to which we want to subscribe, both of which have been defined in <kbd>config.py</kbd>. Finally, we invoke the <kbd>load</kbd> method to stream the latest messages consumed from the <kbd>twitter</kbd> topic to an unbounded Spark dataframe called <kbd>tweets_df</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 90px">tweets_df = spark.readStream.format("kafka")<br/>  .option("kafka.bootstrap.servers", config.bootstrap_servers)<br/>  .option("subscribe", config.twitter_kafka_topic_name).load()</pre>
<ol start="6">
<li>Records stored in a Kafka topic are persisted in binary format. In order to process the JSON representing our tweets, which are stored in a Kafka record under a field called <kbd>value</kbd>, we must first <kbd>CAST</kbd> the contents of <kbd>value</kbd> as a string. We then apply our defined schema to this JSON string and extract the fields of interest, as shown in the following code. In our case, we are only interested in the tweet ID, stored in the JSON key called <kbd>id</kbd>, and its raw textual content, stored in the JSON key called <kbd>text</kbd>. The resulting Spark dataframe will therefore have two string columns, <kbd>id</kbd> and <kbd>text</kbd>, containing these respective fields of interest:</li>
</ol>
<pre style="padding-left: 60px">tweets_df = tweets_df.selectExpr(<br/>   "CAST(key AS STRING)", "CAST(value AS STRING) as json")<br/>   .withColumn("tweet", from_json(col("json"), schema=schema))<br/>   .selectExpr("tweet.id_str as id", "tweet.text as text")</pre>
<ol start="7">
<li>Now that we have consumed raw tweets from our Kafka topic and parsed them into a Spark dataframe, we can apply our preprocessing pipeline as we did in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>,<em> Natural Language Processing Using Apache Spark</em><a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank"/>. However, rather than duplicating the same code from <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, into our Kafka consumer application, we simply call the relevant function that we defined in <kbd>model_pipelines.py</kbd>, namely <kbd>preprocessing_pipeline()</kbd>, as shown in the following code. This preprocessing pipeline tokenizes the raw text, removes stop words, applies a stemming algorithm, and normalizes the resulting tokens:</li>
</ol>
<pre style="padding-left: 60px">preprocessed_df = model_pipelines.preprocessing_pipeline(tweets_df)</pre>
<ol start="8">
<li>Next, we generate feature vectors from these tokens, as we did in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>. We call the <kbd>vectorizer_pipeline()</kbd> function from <kbd>model_pipelines.py</kbd> to generate feature vectors based on term frequencies, as shown in the following code. The resulting Spark dataframe, called <kbd>features_df</kbd>, contains three pertinent columns, namely <kbd>id</kbd> (raw tweet ID), <kbd>text</kbd> (raw tweet text), and <kbd>features</kbd> (term-frequency feature vectors):</li>
</ol>
<pre style="padding-left: 60px">features_df = model_pipelines.vectorizer_pipeline(preprocessed_df)</pre>
<ol start="9">
<li>Now that we have generated feature vectors from our stream of tweets, we can apply our trained decision tree classifier to this stream in order to predict and classify their underlying sentiment. We do this as normal, by invoking the <kbd>transform</kbd> method on the <kbd>features_df</kbd> dataframe, resulting in a new Spark dataframe called <kbd>predictions_df</kbd>, containing the columns <kbd>id</kbd> and <kbd>text</kbd> as before, and a new column called <kbd>prediction</kbd> that contains our predicted classification, as shown in the following code. As described in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, a prediction of <kbd>1</kbd> implies non-negative sentiment, and a prediction of <kbd>0</kbd> implies negative sentiment:</li>
</ol>
<pre style="padding-left: 60px">predictions_df = decision_tree_model.transform(features_df)<br/>   .select("id", "text", "prediction")</pre>
<ol start="10">
<li>Finally, we write our predicted results dataframe to an output sink. In our case, we define the output sink as simply the console that is used to execute the Kafka consumer PySpark application—that is, the console from which we execute <kbd>spark-submit</kbd>. We achieve this by invoking the <kbd>writeStream</kbd> method on the relevant Spark dataframe and stating <kbd>console</kbd> as the <kbd>format</kbd> of choice. We start our output stream by invoking the <kbd>start</kbd> method, and invoke the <kbd>awaitTermination</kbd> method, which tells Spark to continue processing our streaming pipeline indefinitely until it is explicitly interrupted and stopped, as follows:</li>
</ol>
<pre style="padding-left: 90px">query = predictions_df.writeStream<br/>   .outputMode("complete")<br/>   .format("console")<br/>   .option("truncate", "false")<br/>   .start() <br/>query.awaitTermination()</pre>
<p style="padding-left: 90px"><span>Note that the </span><kbd>outputMode</kbd><span> method defines what gets written to the output sink, and can take one of the following options:</span></p>
<ul>
<li style="padding-left: 90px"><kbd>complete</kbd>: The entire (updated) results table is written to the output sink</li>
<li style="padding-left: 90px"><kbd>append</kbd>: Only the new rows appended to the results table since the last trigger is written to the output sink</li>
<li style="padding-left: 90px"><kbd>update</kbd>: Only the rows that were updated in the results table since the last trigger is written to the output<span> </span>sink</li>
</ul>
<p>We are now ready to run our Kafka consumer application! Since it is a Spark application, we can execute it via <em>spark-submit</em> on the Linux command line. To do this, navigate to the directory where we installed Apache Spark (see <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up a Local Development Environment</em>). Thereafter we can execute the <kbd>spark-submit</kbd> program by passing to it the following command-line arguments:</p>
<ul>
<li><kbd>--master</kbd>: The Spark master URL.</li>
<li><kbd>--packages</kbd>: The third-party libraries and dependencies required for the given Spark application to work. In our case, our Kafka consumer application is dependent on the availability of two third-party libraries, namely <kbd>spark-sql-kafka</kbd> (Spark Kafka integration) and <kbd>spark-nlp</kbd> (NLP algorithms, as studied in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>).</li>
</ul>
<ul>
<li><kbd>--py-files</kbd>: Since our Kafka consumer is a PySpark application, we can use this argument to pass a comma-delimited list of filesystem paths to any Python code files that our application is dependent on. In our case, our Kafka consumer application is dependent on <kbd>config.py</kbd> and <kbd>model_pipelines.py</kbd> respectively.</li>
<li>The final argument is the path to the Python code file containing our Spark Structured Streaming driver program, in our case, <kbd>kafka_twitter_consumer.py</kbd></li>
</ul>
<p>The final commands to execute therefore look as follows:</p>
<pre><strong>&gt; cd {SPARK_HOME}</strong><br/><strong>&gt; bin/spark-submit --master spark://192.168.56.10:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2,JohnSnowLabs:spark-nlp:1.7.0 --py-files chapter08/config.py,chapter08/model_pipelines.py chapter08/kafka_twitter_consumer.py</strong></pre>
<p>Assuming that the Python-based Kafka producer application is running as well, the results table should periodically be written to the console and contain the real-time prediction and classification of the underlying sentiment behind the stream of airline tweets being consumed <span><span>by</span></span> the Twitter API from across the world and written by real Twitter users! A selection of real-world classified tweets that was processed when filtering using <kbd>"@British_Airways"</kbd> is shown in the following table:</p>
<table style="border-collapse: collapse;width: 100%;border-color: #000000" border="1">
<tbody>
<tr>
<td style="padding: 5px;width: 75.999%"><strong>Tweet Raw Contents</strong></td>
<td style="padding: 5px;width: 18.001%"><strong>Predicted Sentiment</strong></td>
</tr>
<tr>
<td style="padding: 5px;width: 75.999%">@British_Airways @HeathrowAirport I mean I'm used to cold up in Shetland but this is a whole different kind of cold!!</td>
<td style="padding: 5px;width: 18.001%">Non-negative</td>
</tr>
<tr>
<td style="padding: 5px;width: 75.999%">@British_Airways have just used the app to check-in for our flight bari to lgw but the app shows no hold luggage</td>
<td style="padding: 5px;width: 18.001%">Negative</td>
</tr>
<tr>
<td style="padding: 5px;width: 75.999%">She looks more Beautiful at Night | A380 takeoff London Heathrow @HeathrowAviYT @HeathrowAirport @British_Airways</td>
<td style="padding: 5px;width: 18.001%">Non-negative</td>
</tr>
<tr>
<td style="padding: 5px;width: 75.999%">The @British_Airways #B747 landing into @HeathrowAirport</td>
<td style="padding: 5px;width: 18.001%">Non-negative</td>
</tr>
<tr>
<td style="padding: 5px;width: 75.999%">@British_Airways trying to check in online for my flight tomorrow and receiving a 'sorry we are unable to offer you online check-in for this flight' message. Any idea why??</td>
<td style="padding: 5px;width: 18.001%">Negative</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we developed Apache Kafka producer and consumer applications and utilized Spark's Structured Streaming engine to process streaming data consumed from a Kafka topic. In our real-world case study, we designed, developed, and deployed an end-to-end stream processing pipeline that was capable of consuming real tweets being authored across the world and then classified their underlying sentiment using machine learning, all of which was done in real time.</p>
<p>In this book, we went on both a theoretical and a hands-on journey through some of the most important and exciting technologies and frameworks that underpin the data-intelligence-driven revolution being seen across industry today. We started out by describing a new breed of distributed and scalable technologies that allow us to store, process, and analyze huge volumes of structured, semi-structured, and unstructured data. Using these technologies as a base, we established the context for artificial intelligence and how it relates to machine learning and deep learning. We then went on to explore some of the key concepts in, and applications of, machine learning, including supervised learning, unsupervised learning, natural language processing, and deep learning. We illustrated these key concepts through a wide variety of relevant and exciting use cases that were implemented using our big data processing engine of choice—Apache Spark. Finally, because timely decisions are critical to many businesses and organizations in the modern world, we extended our deployment of machine learning models beyond batch processing to real-time streaming applications!</p>


            </article>

            
        </section>
    </div>



  </body></html>