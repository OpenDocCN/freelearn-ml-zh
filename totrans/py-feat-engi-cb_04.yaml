- en: <st c="0">4</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Performing Variable Discretization</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="36">Discretization is the</st> <st c="59">process of transforming continuous
    variables into discrete features by creating a set of contiguous intervals, also</st>
    <st c="175">called</st> **<st c="182">bins</st>**<st c="186">, which span the
    range of the variable values.</st> <st c="233">Subsequently, these intervals are
    treated as</st> <st c="278">categorical data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="295">Many machine learning models, such as decision trees and Naïve Bayes,
    work better with discrete attributes.</st> <st c="404">In fact, decision tree-based
    models make decisions based on discrete partitions over the attributes.</st> <st
    c="505">During induction, a decision tree evaluates all possible feature values
    to find the best cut-point.</st> <st c="605">Therefore, the more values the feature
    has, the longer the induction time of the tree is.</st> <st c="695">In this sense,
    discretization can reduce the time it takes to train</st> <st c="763">the models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="774">Discretization has additional advantages.</st> <st c="817">Data
    is reduced and simplified; discrete features can be easier to understand by domain
    experts.</st> <st c="914">Discretization can change the distribution of skewed
    variables; when sorting observations across bins with equal-frequency, the values
    are spread more homogeneously across the range.</st> <st c="1097">Additionally,
    discretization can minimize the influence of outliers by placing them at lower
    or higher intervals, together with the remaining</st> **<st c="1239">inlier</st>**
    <st c="1245">values of the distribution.</st> <st c="1274">Overall, discretization
    reduces and simplifies data, making the learning process faster and potentially
    yielding more</st> <st c="1392">accurate results.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1409">Discretization can also lead to a loss of information, for example,
    by combining values that are strongly associated with different classes or target
    values into the same bin.</st> <st c="1586">Therefore, the aim of a discretization
    algorithm is to find the minimal number of intervals without incurring a significant
    loss of information.</st> <st c="1731">In practice, many discretization procedures
    require the user to input the number of intervals into which the values will be
    sorted.</st> <st c="1863">Then, the job of the algorithm is to find the cut points
    for those intervals.</st> <st c="1941">Among these procedures, we find the most
    widely used equal-width and equal-frequency discretization methods.</st> <st c="2050">Discretization
    methods based on decision trees are, otherwise, able to find the optimal number
    of partitions, as well as the</st> <st c="2175">cut points.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2186">Discretization procedures can be classified as</st> **<st c="2234">supervised</st>**
    <st c="2244">and</st> **<st c="2249">unsupervised</st>**<st c="2261">. Unsupervised
    discretization methods only use the variable’s distribution to determine the limits
    of the contiguous bins.</st> <st c="2384">On the other hand, supervised methods
    use target information to create</st> <st c="2455">the intervals.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2469">In this chapter, we will discuss widely used supervised and unsupervised
    discretization procedures that are available in established open source libraries.</st>
    <st c="2626">Among these, we will cover equal-width, equal-frequency, arbitrary,
    k-means, and decision tree-based discretization.</st> <st c="2743">More elaborate
    methods, such as ChiMerge and CAIM, are out of the scope of this chapter, as their
    implementation is not yet open</st> <st c="2872">source available.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2889">This chapter contains the</st> <st c="2916">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2934">Performing</st> <st c="2946">equal-width discretization</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2972">Implementing</st> <st c="2986">equal-frequency discretization</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3016">Discretizing the variable into</st> <st c="3048">arbitrary intervals</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3067">Performing discretization with</st> <st c="3099">k-means clustering</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3117">Implementing</st> <st c="3131">feature binarization</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3151">Using decision trees</st> <st c="3173">for discretization</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3191">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3214">In this chapter, we will use the numerical computing libraries</st>
    `<st c="3278">pandas</st>`<st c="3284">,</st> `<st c="3286">numpy</st>`<st c="3291">,</st>
    `<st c="3293">matplotlib</st>`<st c="3303">,</st> `<st c="3305">scikit-learn</st>`<st
    c="3317">, and</st> `<st c="3322">feature-engine</st>`<st c="3337">. We will also
    use the</st> `<st c="3360">yellowbrick</st>` <st c="3371">Python open source library,
    which you can install</st> <st c="3422">with</st> `<st c="3427">pip</st>`<st c="3430">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="3456">For more details about</st> `<st c="3480">yellowbrick</st>`<st
    c="3491">, visit the</st> <st c="3503">documentation here:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3522">https://www.scikit-yb.org/en/latest/index.html</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3569">Performing equal-width discretization</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3607">Equal-width</st> <st c="3619">discretization</st> <st c="3634">consists
    of dividing the range of</st> <st c="3668">observed values for a variable into</st>
    *<st c="3705">k</st>* <st c="3706">equally sized intervals, where</st> *<st c="3738">k</st>*
    <st c="3739">is supplied by the user.</st> <st c="3765">The interval width for
    the</st> *<st c="3792">X</st>* <st c="3793">variable is given by</st> <st c="3815">the
    followin</st><st c="3827">g:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">d</mi><mi
    mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">h</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold-italic">M</mi><mi mathvariant="bold-italic">a</mi><mi mathvariant="bold-italic">x</mi><mfenced
    open="(" close=")"><mi mathvariant="bold-italic">X</mi></mfenced><mo>−</mo><mi
    mathvariant="bold-italic">M</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">n</mi><mo>(</mo><mi
    mathvariant="bold-italic">X</mi><mo>)</mo></mrow><mi mathvariant="bold-italic">k</mi></mfrac></mrow></mrow></math>](img/20.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="3860">Then, if the</st> <st c="3873">values of the variable vary between
    0 and 100, we can create five bins like this:</st> *<st c="3955">width = (100-0)
    / 5 = 20</st>*<st c="3979">. The bins will be 0–20, 20–40, 40–60, and 80–100\.</st>
    <st c="4030">The first and final bins (0–20 and 80–100) can be expanded to accommodate
    values smaller than 0 or greater than 100 by extending the limits to minus and</st>
    <st c="4183">plus infinity.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4197">In this recipe, we will carry out equal-width discretization using</st>
    `<st c="4265">pandas</st>`<st c="4271">,</st> `<st c="4273">scikit-learn</st>`<st
    c="4285">,</st> <st c="4287">and</st> `<st c="4291">feature-engi</st><st c="4303">ne</st>`<st
    c="4306">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4307">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="4323">First, le</st><st c="4333">t’s import the necessary Python libraries
    and get the</st> <st c="4388">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4402">Let’s import the libraries</st> <st c="4430">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4622">Let’s load the predictor and target variables of the California</st>
    <st c="4687">housing dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4768">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4773">To avoid data leakage, we will find the intervals’ limits by using
    the variables in the train set.</st> <st c="4873">Then, we will use these limits
    to discretize the variables in train and</st> <st c="4945">test sets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4955">Let’s divide the data into train and</st> <st c="4993">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5093">Next, we will divide the continuous</st> `<st c="5130">HouseAge</st>`
    <st c="5138">variable into 10 intervals using</st> `<st c="5172">pandas</st>`
    <st c="5178">and the formula described at the beginning of</st> <st c="5225">the
    recipe.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="5236">Let’s capture the minimum and maximum values</st> <st c="5282">of</st>
    `<st c="5285">HouseAge</st>`<st c="5293">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5381">Let’s</st> <st c="5388">determine the interval width, which is
    the variable’s value range divided by the number</st> <st c="5476">of bins:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5526">If we execute</st> `<st c="5541">print(width)</st>`<st c="5553">,
    we will obtain</st> `<st c="5570">5</st>`<st c="5571">, which is the size of</st>
    <st c="5594">the intervals.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="5608">No</st><st c="5611">w we need to define the interval limits and
    store them in</st> <st c="5670">a list:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5744">If we now execute</st> `<st c="5763">print(interval_limits)</st>`<st
    c="5785">, we will see the</st> <st c="5803">interval limits:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5862">Let’s expand the limits of the first and last intervals to accommodate
    smaller or greater values that we could find in the test set or in future</st>
    <st c="6008">data sources:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6079">Let’s make a copy of the DataFrames so we don’t overwrite the original
    ones, which we will need for later steps in</st> <st c="6195">the recipe:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6254">Let’s sort</st> <st c="6266">the</st> `<st c="6270">HouseAge</st>`
    <st c="6278">variable into the intervals that we defined in</st> *<st c="6326">step
    6</st>*<st c="6332">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6534">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6539">We have set</st> `<st c="6552">include_lowest=True</st>` <st c="6571">to
    include the lowest value in the first interval.</st> <st c="6623">Note that we
    used the train set to find the intervals and then used those limits to sort the
    variable in</st> <st c="6728">both datasets.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6742">Let’s print the top</st> `<st c="6763">5</st>` <st c="6764">observations
    of the discretized and</st> <st c="6801">original variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6874">In the foll</st><st c="6886">owing output, we can see that the</st>
    `<st c="6921">52</st>` <st c="6923">value was allocated to the 46–infinite interval,
    the</st> `<st c="6977">43</st>` <st c="6979">value was allocated to the 41–46
    interval, and</st> <st c="7027">so on:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7168">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7173">The parentheses and brackets in the intervals indicate whether
    a value is included in the interval or not.</st> <st c="7281">For example, the
    (41, 46] interval contains all values greater than 41 and smaller than or equal</st>
    <st c="7378">to 46.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7384">Equal-width discretization</st> <st c="7411">allocates a different
    number of observations to</st> <st c="7460">each i</st><st c="7466">nterval.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7475">Let’s make a bar plot with the proportion of observations across
    the intervals of</st> `<st c="7558">HouseAge</st>` <st c="7566">in the train and</st>
    <st c="7584">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7969">In the following output, we can see that the proportion of observations
    per interval is approximately the same in the train and test sets, but different</st>
    <st c="8123">across inter</st><st c="8135">vals:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The proportion of observations per interval after the discretization](img/B22396_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8299">Figure 4.1 – The proportion of observations per interval after
    the discretization</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8380">With</st> `<st c="8386">feature-engine</st>`<st c="8400">, we</st>
    <st c="8405">can perform equal-width discretization in fewer lines of code and
    for many variables at</st> <st c="8493">a time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8500">First, let’s import</st> <st c="8521">the discretizer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8601">Let’s set up the discretizer to sort three continuous variables
    into</st> <st c="8671">eight intervals:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8793">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="8798">EqualWidthDiscretiser()</st>` <st c="8822">returns an integer
    indicating w</st><st c="8854">hether the value was sorted into the first, second,
    or eighth bin by default.</st> <st c="8933">That is the equivalent of ordinal
    encoding, which we described in the</st> *<st c="9003">Replacing categories with
    ordinal numbers</st>* <st c="9044">recipe of</st> [*<st c="9055">Chapter 2</st>*](B22396_02.xhtml#_idTextAnchor182)<st
    c="9064">,</st> *<st c="9066">Encoding Categorical Variables</st>*<st c="9096">.
    To carry out a different encoding with the</st> `<st c="9141">feature-engine</st>`
    <st c="9155">or</st> `<st c="9159">category</st>` `<st c="9167">encoders</st>`
    <st c="9177">Python libraries, cast the returned variables as objects by setting</st>
    `<st c="9245">return_object</st>` <st c="9258">to</st> `<st c="9262">True</st>`<st
    c="9266">. Alternatively, make the transformer return the interval limits by setting</st>
    `<st c="9342">return_boundaries</st>` <st c="9359">to</st> `<st c="9363">True</st>`<st
    c="9367">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9368">Let’s fit the</st> <st c="9383">discretizer to the train set so
    that it learns the cut points for</st> <st c="9449">each variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="9481">After fitting, we can inspect the cut points in the</st> `<st c="9534">binner_dict_</st>`
    <st c="9546">attribute by</st> <st c="9560">executing</st> `<st c="9570">print(disc.binner_dict_)</st>`<st
    c="9594">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="9595">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="9600">feature-engine</st>` <st c="9615">will automatically extend the
    limits of the lower and upper intervals to infinite to accommodate potential outliers
    in</st> <st c="9735">future data.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9747">Let’s discretize the variables in the train and</st> <st c="9796">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`<st c="9872">EqualWidthDiscretiser()</st>` <st c="9896">returns a DataFrame
    where the selected variables are discretized.</st> <st c="9963">If we run</st>
    `<st c="9973">test_t.head()</st>`<st c="9986">,</st> <st c="9988">we will see
    the following output where the original values of</st> `<st c="10050">MedInc</st>`<st
    c="10056">,</st> `<st c="10058">HouseAge</st>`<st c="10066">, and</st> `<st c="10072">AveRooms</st>`
    <st c="10080">are replaced by the</st> <st c="10101">interval nu</st><st c="10112">mbers:</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – A DataFrame with three discretized variables: HouseAge, MedInc,
    and AveRooms](img/B22396_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '<st c="10446">Figure 4.2 – A DataFrame with three discretized variables: HouseAge,
    MedInc, and AveRooms</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10535">Now, let’s</st> <st c="10546">make bar plots with the proportion
    of observations per interval to better understand the effect of</st> <st c="10646">equal-width
    discretization:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11271">The in</st><st c="11278">tervals contain</st> <st c="11294">a
    different number of observations, as shown in the</st> <st c="11347">following
    plots:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Bar plots with the proportion of observations per interval after
    the discretization](img/B22396_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="11674">Figure 4.3 – Bar plots with the proportion of observations per
    interval after the discretization</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11770">Now, let’s</st> <st c="11782">implement equal-width discretization</st>
    <st c="11819">with scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11837">Let’s import the classes</st> <st c="11863">from scikit-learn:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11979">Let’s set up an equal-width discretizer by setting its</st> `<st
    c="12035">strategy</st>` <st c="12043">to</st> `<st c="12047">uniform</st>`<st
    c="12054">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12129">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="12134">KBinsDiscretiser()</st>` <st c="12153">can return the bins as
    integers by setting</st> `<st c="12197">encoding</st>` <st c="12205">to</st> `<st
    c="12209">''ordinal''</st>` <st c="12218">or one-hot encoded by setting</st> `<st
    c="12249">encoding</st>` <st c="12257">to</st> `<st c="12261">''onehot-dense''</st>`<st
    c="12275">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12276">Let’s use</st> `<st c="12287">ColumnTransformer()</st>` <st c="12306">to
    restrict the discretization to the selected variables from</st> *<st c="12369">step
    13</st>*<st c="12376">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12495">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12500">With</st> `<st c="12506">remainder</st>` <st c="12515">set to</st>
    `<st c="12523">passthrough</st>`<st c="12534">,</st> `<st c="12536">ColumnTransformer()</st>`
    <st c="12555">returns all the variables in the input DataFrame after the transformation.</st>
    <st c="12631">To return only the transformed variables, set</st> `<st c="12677">remainder</st>`
    <st c="12686">to</st> `<st c="12690">drop</st>`<st c="12694">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12695">Let’s fit the discretizer to the train set so that it learns the</st>
    <st c="12761">interval limits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12793">Finall</st><st c="12800">y, let’s</st> <st c="12810">discretize
    the selected variables in the train and</st> <st c="12861">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12933">We can inspect the cut points learned by the transformer by</st>
    <st c="12994">executing</st> `<st c="13004">ct.named_transformers_["discretizer"].bin_edges_</st>`<st
    c="13052">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="13053">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="13058">ColumnTransformer()</st>` <st c="13078">will append</st> `<st
    c="13091">discretize</st>` <st c="13101">to the variables that were discretized
    and</st> `<st c="13145">remainder</st>` <st c="13154">to those that were</st>
    <st c="13174">not modified.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13187">We can check the output by</st> <st c="13215">executing</st> `<st
    c="13225">test_</st><st c="13230">t.head()</st>`<st c="13239">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13240">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13254">In this recipe, we sorted the variable values into equidistant
    intervals.</st> <st c="13329">To perform discretization with</st> `<st c="13360">pandas</st>`<st
    c="13366">, we first found the maximum and minimum values of the</st> `<st c="13421">HouseAge</st>`
    <st c="13429">variable using the</st> `<st c="13449">max()</st>` <st c="13454">and</st>
    `<st c="13459">min()</st>` <st c="13464">methods.</st> <st c="13474">Then, we
    estimated the interval width by dividing the value range by the number of arbitrary
    bins.</st> <st c="13573">With the width and the minimum and maximum values, we
    determined the interval limits and stored them in a list.</st> <st c="13685">We</st>
    <st c="13687">used this list with pandas</st> `<st c="13715">cut()</st>` <st c="13720">to
    sort the variable values into</st> <st c="13754">the intervals.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13768">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13773">Pandas</st> `<st c="13781">cut()</st>` <st c="13786">sorts the
    variable into intervals of equal size by default.</st> <st c="13847">It will extend
    the variable range by .1% on each side to include the minimum and maximum values.</st>
    <st c="13944">The reason why we generated the intervals manually is to accommodate
    potentially smaller or larger values than those seen in the dataset in future
    data sources when we deploy</st> <st c="14119">our model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14129">After discretization, we normally treat the intervals as categorical
    values.</st> <st c="14207">By default, pandas</st> `<st c="14226">cut()</st>`
    <st c="14231">returns the interval values as ordered integers, which is the equivalent
    of ordinal encoding.</st> <st c="14326">Alternatively, we can return the interval
    limits by setting the</st> `<st c="14390">labels</st>` <st c="14396">parameter</st>
    <st c="14407">to</st> `<st c="14410">None</st>`<st c="14414">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14415">To display the</st> <st c="14431">number of observations per interval,
    we created a bar plot.</st> <st c="14491">We used the pandas</st> `<st c="14510">value_counts()</st>`
    <st c="14524">function to obtain the fraction of observations per interval, which
    returns the result in pandas Series, where the index is the interval and the counts
    are the values.</st> <st c="14693">To plot these proportions, first, we concatenated
    the train and test set series using the pandas</st> `<st c="14790">concat()</st>`<st
    c="14798">function in a DataFrame, and then we assigned the</st> `<st c="14849">train</st>`
    <st c="14854">and</st> `<st c="14859">test</st>` <st c="14863">column names to
    it.</st> <st c="14884">Finally, we used</st> `<st c="14901">plot.bar()</st>` <st
    c="14911">to display a bar plot.</st> <st c="14935">We rotated the labels with
    Matplotlib’s</st> `<st c="14975">xticks()</st>`<st c="14983">function, and added
    the</st> *<st c="15008">x</st>* <st c="15009">and</st> *<st c="15014">y</st>*
    <st c="15015">legend with</st> `<st c="15028">xlabels()</st>` <st c="15037">and</st>
    `<st c="15042">ylabel()</st>`<st c="15050">, as well as the title</st> <st c="15073">with</st>
    `<st c="15078">title()</st>`<st c="15085">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15086">To perform equal-width discretization with</st> `<st c="15130">feature-engine</st>`<st
    c="15144">, we</st> <st c="15149">used</st> `<st c="15154">EqualWidth</st>` **<st
    c="15164">Discretiser()</st>**<st c="15178">, which takes the number of bins and
    the variables to discretize as arguments.</st> <st c="15257">With</st> `<st c="15262">fit()</st>`<st
    c="15267">, the discretizer learned the interval limits for each variable.</st>
    <st c="15332">With</st> `<st c="15337">transform()</st>`<st c="15348">, it sorted
    the values into</st> <st c="15376">each bin.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15385">EqualWidthDiscretiser()</st>` <st c="15409">returns the bins
    as sorted integers by default, which is the equivalent of ordinal encoding.</st>
    <st c="15503">To follow up the discretization with any other encoding procedure
    available in the</st> `<st c="15586">feature-engine</st>` <st c="15600">or</st>
    `<st c="15604">category encoders</st>` <st c="15621">libraries, we need to return
    the bins cast as objects by setting</st> `<st c="15687">return_object</st>` <st
    c="15700">to</st> `<st c="15704">True</st>` <st c="15708">when we set up</st>
    <st c="15724">the transformer.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15740">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15745">EqualWidthDiscretiser()</st>` <st c="15769">extends the values
    of the first and last interval to minus and plus infinity by default to automatically
    accommodate smaller and greater values than those seen in the</st> <st c="15937">training
    set.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15950">We followed the discretization with bar plots to display the fraction
    of observations per interval for each of the transformed variables.</st> <st c="16089">We
    could see that if the original variable was skewed, the bar plot was also skewed.</st>
    <st c="16174">Note how some of the intervals of the</st> `<st c="16212">MedInc</st>`
    <st c="16218">and</st> `<st c="16223">AveRooms</st>` <st c="16231">variables,
    which had skewed distributions, contained very few observations.</st> <st c="16308">In
    particular, even though we wanted to create eight bins for</st> `<st c="16370">AveRooms</st>`<st
    c="16378">, there were only enough values to create five, and most values of the
    variables were allocated to the</st> <st c="16481">first interval.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16496">Finally, we</st> <st c="16509">discretized</st> <st c="16520">three
    continuous variables into equal-width bins with</st> `<st c="16575">KBinsDiscretizer()</st>`
    <st c="16593">from scikit-learn.</st> <st c="16613">To create equal-width bins,
    we set the</st> `<st c="16652">strategy</st>` <st c="16660">argument to</st> `<st
    c="16673">uniform</st>`<st c="16680">. With</st> `<st c="16687">fit()</st>`<st
    c="16692">, the transformer learned the limits of the intervals, and with</st>
    `<st c="16756">transform()</st>`<st c="16767">, it sorted the values into</st>
    <st c="16795">each interval.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16809">We used the</st> `<st c="16822">ColumnTransformer()</st>` <st
    c="16841">to restrict the discretization to the selected variables, setting the
    transform output to pandas to obtain a DataFrame after the transformation.</st>
    `<st c="16987">KBinsDiscretizer()</st>` <st c="17005">can return the intervals
    as ordinal numbers, as we had it do in the recipe, or as one-hot-encoded variables.</st>
    <st c="17115">The behavior can be modified through the</st> `<st c="17156">encod</st><st
    c="17161">e</st>` <st c="17163">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17174">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="17183">For a comparison of equal-width discretization with more sophisticated
    methods, see Dougherty J, Kohavi R, Sahami M.</st> *<st c="17301">Supervised and
    unsupervised discretization of continuous features</st>*<st c="17366">. In: Proceedings
    of the 12th international conference on machine learning.</st> <st c="17442">San
    Francisco: Morgan Kaufma</st><st c="17470">nn; 1995.</st> <st c="17480">p.</st>
    <st c="17484">194–202.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17492">Implementing equal-frequency discretization</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="17536">Equal-width discretization</st> <st c="17563">is intuitive and
    easy to compute.</st> <st c="17598">However, if the variables are skewe</st><st
    c="17633">d, then there will be many empty bins or bins with only a few values,
    while most observations will be allocated to a few intervals.</st> <st c="17766">This
    could result in a loss of information.</st> <st c="17810">This problem can be
    solved by adaptively finding the interval cut-points so that each interval contains
    a similar fraction</st> <st c="17933">of observations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17949">Equal-frequency discretization</st> <st c="17980">divides the
    values of the variable into intervals that carry the same proportion of observations.</st>
    <st c="18079">The interval width is</st> <st c="18100">determined by</st> **<st
    c="18115">quantiles</st>**<st c="18124">. Quantiles are values that divide data
    into equal portions.</st> <st c="18185">For example, the median is a quantile
    that divides the data into two halves.</st> <st c="18262">Quartiles divide the
    data into four equal portions, and percentiles divide the data into 100 equal-sized
    portions.</st> <st c="18377">As a result, the intervals will most likely have
    different widths, but a similar number of observations.</st> <st c="18482">The
    number of intervals is defined by</st> <st c="18520">the user.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18529">In this recipe, we will perform equal-frequency discretization
    using</st> `<st c="18599">pandas</st>`<st c="18605">,</st> `<st c="18607">scikit-learn</st>`<st
    c="18619">,</st> <st c="18621">and</st> `<st c="18625">feature-engine</st>`<st
    c="18639">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18640">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18656">First, let’s</st> <st c="18669">import the necessary Python libraries
    and get the</st> <st c="18720">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18734">Let’s import the required Python libraries</st> <st c="18778">and
    functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18951">Let’s load the California housing dataset into</st> <st c="18999">a
    DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19076">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19081">To avoid data leakage, we will determine the interval boundaries
    or quantiles from the</st> <st c="19169">train set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19179">Let’s divide the data into train and</st> <st c="19217">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19317">Let’s make a copy of</st> <st c="19339">the DataFrames:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19402">We’ll use pandas</st> `<st c="19420">qcut()</st>`<st c="19426">to
    obtain a discretized copy of the</st> `<st c="19463">HouseAge</st>` <st c="19471">variable,
    which we will store as a new column in the training set, and the limits of eight</st>
    <st c="19563">equal-frequency intervals:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19696">If you execute</st> `<st c="19712">print(interval_limits)</st>`<st
    c="19734">, you’ll see the following interval limits:</st> `<st c="19778">array([
    1., 14., 18., 24., 29., 34., 37.,</st>` `<st c="19820">44., 52.])</st>`<st c="19830">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="19831">Let’s</st> <st c="19838">print the top five observations of the
    discretized and</st> <st c="19893">original variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19963">In the following output, we see that the</st> `<st c="20005">52</st>`
    <st c="20007">value was allocated to the 44–52 interval, the</st> `<st c="20055">43</st>`
    <st c="20057">value was allocated to the 37–44 interval, and</st> <st c="20105">so
    on:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: test_t["House_disc"] = pd.cut(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: x=X_test["HouseAge"],
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: bins=interval_limits,
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: include_lowest=True)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20456">Let’s</st> <st c="20463">make a bar plot with the proportion of
    observations per interval in the train and</st> <st c="20545">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="20959">In the following plot, we can see that the bins contain a similar
    fraction</st> <st c="21034">of observations:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The proportion of observations per interval of HouseAge after
    equal-frequency discretization](img/B22396_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21186">Figure 4.4 – The proportion of observations per interval of HouseAge
    after equal-frequency discretization</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21291">With</st> `<st c="21297">feature-engine</st>`<st c="21311">, we
    can apply equal-frequency discretization to</st> <st c="21360">multiple</st> <st
    c="21369">variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21379">Let’s</st> <st c="21386">import</st> <st c="21393">the discretizer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21477">Let’s set up the transformer to discretize three continuous variables
    into</st> <st c="21553">eight bins:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21695">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21700">With</st> `<st c="21706">return_boundaries=True</st>`<st c="21728">,
    the transformer will return the interval boundaries after the discretization.</st>
    <st c="21808">To return the interval number, set it</st> <st c="21846">to</st>
    `<st c="21849">False</st>`<st c="21854">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21855">Let’s fit the</st> <st c="21870">discretizer to the train set
    so that it learns the</st> <st c="21921">interval limits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22076">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="22081">feature-engine</st>` <st c="22096">will automatically extend
    the limits of the lower and upper intervals to infinite to accommodate potential
    outliers in</st> <st c="22216">future data.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22228">Let’s transform the variables in the train and</st> <st c="22276">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22353">Let’s make bar plots with the fraction of observations per interval
    to better understand the effect of</st> <st c="22457">equal-frequency discretization:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23092">In the following</st> <st c="23110">figure, we can se</st><st
    c="23127">e that the intervals have a similar fracti</st><st c="23170">on</st>
    <st c="23174">of observations:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.5 – The proportion of observations per interval after  equal-frequency\
    \ discretization of three va\uFEFFriables.](img/B22396_04_5.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="23721">Figure 4.5 – The proportion of observations per interval after
    equal-frequency discretization of three va</st><st c="23826">riables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23835">Now, let’s carry</st> <st c="23853">out equal-frequency discretization</st>
    <st c="23888">with scikit-learn:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23906">Let’s import</st> <st c="23920">the transformer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23987">Let’s set up the discretizer to sort variables into eight</st>
    <st c="24046">equal-frequency bins:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24141">Let’s fit the</st> <st c="24156">discretizer to a slice of the
    train set containing the variables from</st> *<st c="24226">step 10</st>* <st
    c="24233">so that it learns the</st> <st c="24256">interval limits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24301">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24306">scikit-learn’s</st> `<st c="24322">KBinsDiscretiser()</st>` <st
    c="24340">will discretize all the variables in the dataset.</st> <st c="24391">To
    discretize only a subset, we apply the transformer to the slice of the DataFrame
    that contains the variables of interest.</st> <st c="24516">Alternatively, we
    can restrict the discretization to a subset of variables by using the</st> `<st
    c="24604">ColumnTransformer()</st>`<st c="24623">, as we did in the</st> *<st
    c="24642">Performing equal-width</st>* *<st c="24665">discretization</st>* <st
    c="24679">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24687">Let’s make a copy of the DataFrames where we’ll store the</st>
    <st c="24746">discretized variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24816">Finally, let’s transform the variables in both the train and</st>
    <st c="24878">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24999">We can inspect the cut points by</st> <st c="25033">execu</st><st
    c="25038">ting</st> `<st c="25044">disc.bin_edges_</st>`<st c="25059">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25060">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="25074">In this recipe, we sorted the variable values into intervals with
    a similar proportion</st> <st c="25162">of observations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25178">We used pandas</st> `<st c="25194">qcut()</st>` <st c="25200">to
    identify the interval limits from the train set and sort the values of the</st>
    `<st c="25279">HouseAge</st>` <st c="25287">variable into those intervals.</st>
    <st c="25319">Next, we passed those interval limits to pandas</st> `<st c="25367">cut()</st>`
    <st c="25372">to discretize</st> `<st c="25387">HouseAge</st>` <st c="25395">in
    the test set.</st> <st c="25413">Note that pandas</st> `<st c="25430">qcut()</st>`<st
    c="25436">, like pandas</st> `<st c="25450">cut()</st>`<st c="25455">, returned
    the interval values as ordered integers, which is the equivalent of</st> <st c="25534">ordinal
    encoding,</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25551">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25556">With equal-frequency discretization, many occurrences of values
    within a small continuous range could cause observations with very similar values,
    resulting in different intervals.</st> <st c="25738">The problem with this is
    that it can introduce artificial distinctions between data points that are actually
    quite similar in nature, biasing models or subsequent</st> <st c="25901">data
    analysis.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25915">With Feature-engine’s</st> `<st c="25938">EqualFrequencyDiscretiser()</st>`<st
    c="25965">, we discretized three variables into eight bins.</st> <st c="26015">With</st>
    `<st c="26020">fit()</st>`<st c="26025">, the discretizer learned the interval
    limits and stored them in the</st> `<st c="26094">binner_dict_</st>` <st c="26106">attribute.</st>
    <st c="26118">With</st> `<st c="26123">transform()</st>`<st c="26134">, the observations
    were allocated to</st> <st c="26171">the bins.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26180">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="26185">EqualFrequencyDiscretiser()</st>` <st c="26213">returns an integer
    indicating whether the value was sorted into the first, second, or eighth bin
    by default.</st> <st c="26323">That is the equivalent of ordinal encoding, which
    we described in the</st> *<st c="26393">Replacing categories with ordinal numbers</st>*
    <st c="26434">recipe in</st> [*<st c="26445">Chapter 2</st>*](B22396_02.xhtml#_idTextAnchor182)<st
    c="26454">,</st> *<st c="26456">Encoding</st>* *<st c="26465">Categorical Variables</st>*<st
    c="26486">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26487">To follow up the discretization with a different type of encoding,
    we can return the variables cast as objects by setting</st> `<st c="26610">return_object</st>`
    <st c="26623">to</st> `<st c="26627">True</st>` <st c="26631">and then use any
    of the</st> `<st c="26656">feature-engine</st>` <st c="26670">or</st> `<st c="26674">category
    encoders</st>` <st c="26691">transformers</st> <st c="26704">. Alternatively,
    we can return the interval limits, as we did in</st> <st c="26769">this recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26781">Finally, we discretized variables into eight equal-frequency bins
    using</st> `<st c="26854">scikit-learn</st>`<st c="26866">’s</st> `<st c="26870">KBinsDiscretizer()</st>`<st
    c="26888">. With</st> `<st c="26895">fit()</st>`<st c="26900">, the transformer
    learned the cut points and stored them in its</st> `<st c="26964">bin_edges_</st>`
    <st c="26974">attribute.</st> <st c="26986">With</st> `<st c="26991">transform()</st>`<st
    c="27002">, it sorted the values into each interval.</st> <st c="27045">Note that,
    differently from</st> `<st c="27073">EqualFrequencyDiscretiser()</st>`<st c="27100">,</st>
    `<st c="27102">KBinsDiscretizer()</st>` <st c="27120">will transform all of the
    variables in the dataset.</st> <st c="27173">To avoid this, we only applied the
    discretizer on a slice of the data with the variables</st> <st c="27262">to modify.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27272">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27277">scikit-learn’s</st> `<st c="27293">KbinsDiscretizer</st>` <st
    c="27309">has the option to return the intervals as ordinal numbers or one-hot
    encoded.</st> <st c="27388">The behavior can be modified through</st> <st c="27424">the</st>
    `<st c="27429">encode</st>` <st c="27435">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27446">Discretizing the va</st><st c="27466">riable into arbitrar</st><st
    c="27487">y intervals</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="27499">In various</st> <st c="27510">industries</st><st c="27521">, it
    is common to group variable values into segments that make sense for the business.</st>
    <st c="27609">For example, we might want to group the variable age in intervals
    representing children, young adults, middle-aged people, and retirees.</st> <st
    c="27746">Alternatively, we might group ratings into bad, good, and excellent.</st>
    <st c="27815">On occasion, if we know that the variable is in a certain scale
    (for example, logarithmic), we might want to define the interval cut points within</st>
    <st c="27962">that scale.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27973">In this recipe, we will discretize a variable into pre-defined
    user intervals using</st> `<st c="28058">pan</st><st c="28061">das</st>` <st c="28065">and</st>
    `<st c="28070">feature-engine</st>`<st c="28084">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28085">How t</st><st c="28091">o do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="28102">First, let’s</st> <st c="28116">imp</st><st c="28119">ort the
    necessary Python libraries and get the</st> <st c="28167">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28181">Import Python libraries</st> <st c="28206">and classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28343">Let’s load the California housing dataset into a</st> `<st c="28393">pandas</st>`
    <st c="28399">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28475">Let’s plot a</st> <st c="28488">histogram of the</st> `<st c="28506">Population</st>`
    <st c="28516">variable to find out its</st> <st c="28542">value range:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28656">Population</st> <st c="28668">values vary between 0</st> <st c="28690">and</st>
    <st c="28694">approximately 40,000:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.6 – Histogram of the \uFEFFPopulation variable](img/B22396_04_6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="28831">Figure 4.6 – Histogram of the</st> <st c="28861">Population variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28880">Let’s cre</st><st c="28890">ate a list with arbitrary interval
    limits, setting the upper limit to infinity to accommodate</st> <st c="28985">bigger
    values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29045">Let’s create a list with the interval limits</st> <st c="29091">as
    strings:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29166">Let’s make</st> <st c="29177">a copy of the dataset and discretize
    the</st> `<st c="29219">Population</st>` <st c="29229">variable into the pre-defined
    limits from</st> *<st c="29272">step 4</st>*<st c="29278">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29397">Now, let’s</st> <st c="29409">discretize</st> `<st c="29420">Population</st>`
    <st c="29430">into pre-defined intervals and name the intervals with the labels
    that we defined in</st> *<st c="29516">step 5</st>* <st c="29522">for comparison:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29641">Let’s</st> <st c="29648">inspect the first five rows of the original
    and</st> <st c="29696">discretized variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '<st c="29786">In the last two columns of the DataFrame, we can see the discretized
    variables: the first one with the strings that we created in</st> *<st c="29917">step
    5</st>* <st c="29923">as values, and the second one with the</st> <st c="29963">interval
    limits:</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30182">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30187">We only need one of the variable versions, either the one with
    the value range or the one with the interval limits.</st> <st c="30304">In this
    recipe, I created both to highlight the different options offered</st> <st c="30378">by</st>
    `<st c="30381">pandas</st>`<st c="30387">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30388">Finally, we</st> <st c="30401">can count and plot the</st> <st
    c="30424">number of observations within</st> <st c="30454">each interval:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30627">In the foll</st><st c="30639">owing figure, we can see that t</st><st
    c="30671">he number of obser</st><st c="30690">vations per</st> <st c="30703">interval
    varies:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The proportion of observations per interval after the discretization.](img/B22396_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="30824">Figure 4.7 – The proportion of observations per interval after
    the discretization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30906">To wrap up</st> <st c="30917">the recipe, let’s discretize multiple
    variables</st> <st c="30966">ut</st><st c="30968">ilizing</st> `<st c="30977">feature-engine</st>`<st
    c="30991">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30993">Let’s</st> <st c="30998">import</st> <st c="31006">the transformer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31085">Let’s create a dictionary with the variables as keys and the interval
    limits</st> <st c="31163">as values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31269">Let’s set up the discretizer with the limits from</st> *<st c="31320">step
    11</st>*<st c="31327">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31413">Now, we can go ahead and discretize</st> <st c="31450">the variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31499">If we execute</st> `<st c="31514">X_t.head()</st>`<st c="31524">,
    we will see the following output, where the</st> `<st c="31570">Population</st>`
    <st c="31580">and</st> `<st c="31585">MedInc</st>` <st c="31591">var</st><st c="31595">iables
    have</st> <st c="31608">been discretized:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8 – A DataFrame containing the discretized variables](img/B22396_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="32062">Figure 4.8 – A DataFrame containing the discretized variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32123">The advantage</st> <st c="32137">of using</st> `<st c="32147">feature-engine</st>`
    <st c="32161">is that we</st> <st c="32173">can discretize multiple variables
    at the same time and apply arbitrary discretization as p</st><st c="32263">art
    of a</st> <st c="32273">scikit-learn</st> `<st c="32286">Pipeline</st>`<st c="32294">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32295">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="32311">In this recipe, we sorted the values of a variable into user-defin</st><st
    c="32378">ed intervals.</st> <st c="32393">First, we plotted a histogram of the</st>
    `<st c="32430">Population</st>` <st c="32440">variable to get an idea of its va</st><st
    c="32474">lue range.</st> <st c="32486">Next, we arbitrarily determined the limits
    of the intervals and captured them in a list.</st> <st c="32575">We created intervals
    that included 0–200, 200–500, 500–1000, 1000–2000, and more than 2,000 by setting
    the upper limit to infinite with</st> `<st c="32711">np.inf</st>`<st c="32717">.
    Next, we created a list with the interval names as strings.</st> <st c="32779">Using
    pandas</st> `<st c="32792">cut()</st>` <st c="32797">and passing the list with
    the interval limits, we sorted the variable values into the pre-defined bins.</st>
    <st c="32902">We executed the command twice; in the first run, we set the</st>
    `<st c="32962">labels</st>` <st c="32968">argument to</st> `<st c="32981">None</st>`<st
    c="32985">, returning the interval limits as a result.</st> <st c="33030">In the
    second run, we set the</st> `<st c="33060">labels</st>` <st c="33066">argument
    to the list of strings.</st> <st c="33100">We captured the returned output in
    two variables: the first one displays the interval limits as values and the second
    one has strings as values.</st> <st c="33245">Finally, we counted the number of
    observations per variable using</st> <st c="33311">pandas</st> `<st c="33318">value_counts()</st>`<st
    c="33332">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33333">Finally, we automated the procedure with</st> `<st c="33375">feature-engine</st>`<st
    c="33389">’s</st> `<st c="33393">ArbitraryDiscretiser()</st>`<st c="33415">. This
    transformer takes a dictionary with the variables to discretize as keys and the
    interval limits in a list as values, and then uses pandas</st> `<st c="33560">cut()</st>`
    <st c="33565">under the hood to discretize the variables.</st> <st c="33610">With</st>
    `<st c="33615">fit()</st>`<st c="33620">, the transformer does not learn any parameters
    but checks that the variables are numerical.</st> <st c="33713">With</st> `<st
    c="33718">transform()</st>`<st c="33729">, it discretizes</st> <st c="33746">the
    variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33760">Performing discretization</st> <st c="33787">with k-means clustering</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="33810">The aim o</st><st c="33820">f a discretization</st> <st c="33840">procedure
    is to find a set of cut points that</st> <st c="33885">partition a variable into
    a small number of intervals that have good class coherence.</st> <st c="33972">To
    create partitions that group similar observations, we can use clustering algorithms
    such</st> <st c="34064">as k-means.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34075">In</st> <st c="34078">discretization using k-means clustering,
    the partitions are the clusters identified by the k-means algorithm.</st> <st
    c="34189">The k-means clustering algorithm has two main steps.</st> <st c="34242">In
    the initialization step,</st> *<st c="34270">k</st>* <st c="34271">observations
    are chosen randomly as the initial centers of the</st> *<st c="34335">k</st>*
    <st c="34336">clusters, and the remaining data points are assigned to the closest
    cluster.</st> <st c="34414">The proximity to the cluster is measured by a distance
    measure, such as the Euclidean distance.</st> <st c="34510">In the iteration step,
    the centers of the clusters are re-computed as the average of all of the observations
    within the cluster, and the observations are reassigned to the newly created closest
    cluster.</st> <st c="34713">The iteration step continues until the optimal</st>
    *<st c="34760">k</st>* <st c="34761">centers</st> <st c="34770">are found.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34780">Discretization with k-means requires one parameter, which is</st>
    *<st c="34842">k</st>*<st c="34843">, the number of clusters.</st> <st c="34869">There
    are a few methods to determine the optimal number of clusters.</st> <st c="34938">One
    of them is the elbow method, which we will use in this recipe.</st> <st c="35005">This
    m</st><st c="35011">ethod consists of training s</st><st c="35040">everal k-means
    algorithms over the data using different values of</st> *<st c="35107">k</st>*<st
    c="35108">, and then determining the explained variation returned by the clustering.</st>
    <st c="35183">In the next step, we plot the explained variation as a function
    of the number of clusters,</st> *<st c="35274">k</st>*<st c="35275">, and pick
    the</st> *<st c="35290">elbow</st>* <st c="35295">of the curve as the number of
    clusters to use.</st> <st c="35343">The elbow is the inflection point that indicates
    that increasing the number of</st> *<st c="35422">k</st>* <st c="35423">further
    does not significantly increase the variance explained by the model.</st> <st
    c="35501">There are different metrics to quantify the explained variation.</st>
    <st c="35566">We will use the sum of the square distances from each point to its</st>
    <st c="35633">assigned center.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35649">In this recipe, we will use the Python library</st> `<st c="35697">yellowbrick</st>`
    <st c="35708">to determine the optimal number of clusters and then carry out k-means</st>
    <st c="35779">discretization</st> <st c="35795">with scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35813">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="35829">Let’s start by importing the necessary Python libraries and get
    the</st> <st c="35898">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35912">Import</st> <st c="35920">the required Python libraries</st> <st
    c="35950">and classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36224">Let’s</st> <st c="36231">load the California housing dataset into
    a</st> `<st c="36274">pandas</st>` <st c="36280">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36358">The k-means optimal clusters should be determined using the train
    set, so let’s divide the data into train and</st> <st c="36470">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36570">Let’s make a list with the variables</st> <st c="36608">to transform:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36668">Let’s set up a k-means</st> <st c="36692">clustering algorithm:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36747">Now, using Yellowbrick’s visualizer and the elbow method, let’s
    find the optimal number of clusters for</st> <st c="36852">each variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37066">In the</st> <st c="37074">following</st> <st c="37084">plots,
    we see that the optimal number of clusters is six for the first t</st><st c="37156">wo
    variables and seven for</st> <st c="37184">the third:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The number of clusters versus the explained variation for the
    MedInc, HouseAge, and AveRooms variables, from top to bottom](img/B22396_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="37557">Figure 4.9 – The number of clusters versus the explained variation
    for the MedInc, HouseAge, and AveRooms variables, from top to bottom</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37692">Let’s set</st> <st c="37703">up a discretizer that</st> <st c="37724">uses
    k-means clustering to create six partitions and returns the clusters as</st> <st
    c="37802">one-hot-encoded variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37954">Let’s fit</st> <st c="37965">the discretizer to the slice of the</st>
    <st c="38000">DataFrame that contains the variables to discretize so that it finds
    the clusters for</st> <st c="38087">each variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38130">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38135">In this recipe, we sort the values of all three of the variables
    into six clusters.</st> <st c="38220">To discretize</st> `<st c="38234">MedInc</st>`
    <st c="38240">and</st> `<st c="38245">HouseAge</st>` <st c="38253">into six partitions
    and</st> `<st c="38278">AveRooms</st>` <st c="38286">into seven, we would set
    up one instance of the discretizer for each variable group and use the</st> `<st
    c="38383">ColumnTransformer()</st>` <st c="38402">to restrict the discretization
    to</st> <st c="38437">each group.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38448">Let’s inspect the</st> <st c="38467">cut points:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38494">Each array contains the cut points for the six clusters for</st>
    `<st c="38555">MedInc</st>`<st c="38561">,</st> `<st c="38563">HouseAge</st>`<st
    c="38571">,</st> <st c="38573">and</st> `<st c="38577">AveRooms</st>`<st c="38585">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38873">Let’s obtain the discretized form of the variables in the train</st>
    <st c="38938">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39050">With</st> `<st c="39056">print(test_features)</st>`<st c="39076">,
    we can</st> <st c="39085">inspect the DataFrame that is returned by the discretizer.</st>
    <st c="39144">It contains 18 binary variables correspond</st><st c="39186">ing
    to the one-hot-encoded</st> <st c="39214">transformation</st> <st c="39229">of
    the six clusters returned for each of the three</st> <st c="39280">numerical variables:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40004">You</st> <st c="40008">can</st> <st c="40013">concatenate the
    result to the original DataFrame using</st> `<st c="40068">pandas</st>` <st c="40074">and
    then drop the original numerical variables.</st> <st c="40123">Alternatively,
    use the</st> `<st c="40146">ColumnTransformer()</st>` <st c="40165">class to restrict
    the discretization to the selected variables and add the result to th</st><st
    c="40253">e data by setting</st> `<st c="40272">remainder</st>` <st c="40281">to</st>
    `<st c="40285">"passthrough"</st>`<st c="40298">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40299">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="40315">In this recipe, we performed discretization</st> <st c="40359">with
    k-means clustering.</st> <st c="40385">First</st><st c="40390">, we identified
    the optimal number of clusters utilizing the elbow method by using</st> <st c="40473">Yellowbrick’s</st>
    `<st c="40487">KElbowVisualizer()</st>`<st c="40505">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40506">To perform k-means</st> <st c="40525">discretization, we used
    scikit-learn’s</st> `<st c="40565">KBinsDiscretizer()</st>`<st c="40583">, setting</st>
    `<st c="40593">strategy</st>` <st c="40601">to</st> `<st c="40605">kmeans</st>`
    <st c="40611">and the number of clusters to six in the</st> `<st c="40653">n_bins</st>`
    <st c="40659">argument.</st> <st c="40670">With</st> `<st c="40675">fit()</st>`<st
    c="40680">, the transformer learned the cluster boundaries using the k-means algorithm.</st>
    <st c="40758">With</st> `<st c="40763">transform()</st>`<st c="40774">, it sorted
    the variable values to their corresponding cluster.</st> <st c="40838">We set</st>
    `<st c="40845">encode</st>` <st c="40851">to</st> `<st c="40855">"onehot-dense"</st>`<st
    c="40869">; hence, after the discretization, the transformer applied one-hot encoding
    to the clusters.</st> <st c="40963">We also set the output of the discretizer
    to</st> `<st c="41008">pandas</st>`<st c="41014">, and with that, the transformer
    returned the one-hot encoded ver</st><st c="41079">sion of the clustered variables
    as</st> <st c="41115">a DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41127">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="41136">Discretization with k-means is described in the article found
    in</st> *<st c="41202">Palaniappan and Hong, Discretization of Continuous Valued
    Dimensions in OLAP Data Cube</st>*<st c="41288">s.</st> <st c="41292">International
    Journal of Computer Science and Network Security, VOL.8 No.11, November</st> <st
    c="41378">2008\.</st> [<st c="41384">http://paper.i</st><st c="41398">jcsns.org/07_book/200811/20081117.pdf</st>](http://paper.ijcsns.org/07_book/200811/20081117.pdf)<st
    c="41436">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41437">To learn more about the elbow method, visit Yellowbrick’s documentation
    and references</st> <st c="41525">at</st> [<st c="41528">https://www.scikit-yb.org/en/latest/api/cluster/elbow.html</st>](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html)<st
    c="41586">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41587">For other ways of determining the fit of k-means clustering, check
    out the additional visualizers in Yellowbrick</st> <st c="41701">at</st> [<st
    c="41704">https://www.</st><st c="41716">scikit-yb.org/en/latest/api/cluster/index.html</st>](https://www.scikit-yb.org/en/latest/api/cluster/index.html)<st
    c="41763">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="41764">Implementing feature binarization</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="41798">Some datasets</st> <st c="41812">contain sparse variables.</st>
    <st c="41839">Sparse variables are those where the majority of the values are
    0\.</st> <st c="41906">The classical example of sparse va</st><st c="41940">riables
    are those derived from text data through the bag-of-words model, where each variable
    is a word and each value represents the number of times the word appears in a
    certain document.</st> <st c="42130">Given that a document contains a limited
    number of words, whereas the feature space contains the words that appear across
    all documents, most documents, that is, most rows, will show a value of 0 for
    most columns.</st> <st c="42344">However, words are not the</st> <st c="42371">sole
    example.</st> <st c="42385">If we think about house details data, the</st> *<st
    c="42427">number of saunas</st>* <st c="42443">variable will also be 0 for most
    houses.</st> <st c="42485">In summary, some variables have very skewed distributions,
    where most observations show the same value, usually 0, and only a few observations
    show different, usually</st> <st c="42652">higher, values.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42667">For a simpler representation of these sparse or highly skewed
    variables, we can binarize them by clipping all values greater than 1 to 1\.</st>
    <st c="42806">In fact, binarization is commonly performed on text count data,
    where we consider the presence or absence of a feature rather than a quantified
    number of occurrences of</st> <st c="42975">a word.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42982">In this recipe,</st> <st c="42998">we will perform binarizat</st><st
    c="43024">ion</st> <st c="43029">using</st> `<st c="43035">scikit-learn</st>`<st
    c="43047">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43048">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43062">We will use a dataset consisting of a bag of words, which is available
    in the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Bag+of+Words).</st>
    <st c="43229">It is licensed under CC BY</st> <st c="43256">4.0 (</st>[<st c="43261">https://creativecommons.org/licenses/by/4.0/legalcode</st>](https://creativecommons.org/licenses/by/4.0/legalcode)<st
    c="43315">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43318">I downloaded and prepared a small bag of words representing a
    simplified version of one of those datasets.</st> <st c="43426">You will find
    this dataset in the accompanying</st> <st c="43473">GitHub repository:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43491">https://github.com/PacktPublishing/Python-Feature-Engineering-Coo</st><st
    c="43557">kbook-Third-Edition/tr</st><st c="43580">ee/main/ch04-discretization</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43608">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="43624">Let’s begin by importing the libraries and loading</st> <st c="43676">the
    data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43685">Let’s import the required Python libraries, classes,</st> <st
    c="43739">and datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="43901">Let’s load the bag of words dataset, which contains words as columns
    and different texts</st> <st c="43991">as rows:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44038">Let’s</st> <st c="44044">display histograms to visualize the sparsity
    of</st> <st c="44093">the variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44169">In the following histograms, we can see that the dif</st><st c="44222">ferent
    words appear zero times in</st> <st c="44257">most documents:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.10 – Histograms representing th\uFEFFe number of times each word\
    \ appears in a document](img/B22396_04_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="44633">Figure 4.10 – Histograms representing th</st><st c="44673">e number
    of times each word appears in a document</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44723">Let’s</st> <st c="44730">set up</st> `<st c="44737">binarizer</st>`
    <st c="44746">to clip all values greater than 1 to 1 and return DataFrames as</st>
    <st c="44811">a result:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44890">Let’s binarize</st> <st c="44906">the variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44959">Now we can explore the distribution of the binarized variables
    by displaying the histograms as</st> <st c="45055">in</st> *<st c="45058">step
    3</st>*<st c="45064">, or better, by creating</st> <st c="45089">bar plots.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="45099">Let’s</st> <st c="45105">create a bar plot with the number of
    observations per bin</st> <st c="45164">per variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45487">In the following plot, we can see the binarized v</st><st c="45537">ariables,
    where most occurrences show the</st> `<st c="45580">0</st>` <st c="45581">value:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.11 – Bar plots containing the number of documents that eithe\uFEFF\
    r show each one of the words or not](img/B22396_04_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="46228">Figure 4.11 – Bar plots containing the number of documents that
    eithe</st><st c="46297">r show each one of the words or not</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46333">That’s it;</st> <st c="46344">now</st> <st c="46348">we have a
    simpler representation of</st> <st c="46385">the data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46394">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="46408">In this recipe, we changed the representation of sparse variables
    to consider the presence or absence of an occurrence, which, in our case, is a
    word.</st> <st c="46560">The data consisted of a bag of words, where each variable
    (column) is a wor</st><st c="46635">d, each row is a document, and the values
    represent the number of times the word appears in a document.</st> <st c="46740">Most
    words do not appear in</st> <st c="46767">most documents; therefore, most values
    in the data are 0\.</st> <st c="46826">We corroborated the sparsity of our data</st>
    <st c="46867">with histograms.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46883">scikit-learn’s</st> `<st c="46899">Binarizer()</st>` <st c="46910">mapped
    values greater than the threshold, which, in our case, was 0, to the</st> `<st
    c="46987">1</st>` <st c="46988">value, while values less than or equal to the
    threshold were mapped to 0\.</st> `<st c="47063">Binarizer()</st>` <st c="47074">has
    the</st> `<st c="47083">fit()</st>` <st c="47088">and</st> `<st c="47093">transform()</st>`
    <st c="47104">methods, where</st> `<st c="47120">fit()</st>` <st c="47125">does
    not do anything and</st> `<st c="47151">transform()</st>` <st c="47162">binarizes</st>
    <st c="47173">the variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="47187">Binarizer()</st>` <st c="47199">modifies all variables in a
    dataset returning NumPy arrays by default.</st> <st c="47271">To return</st> `<st
    c="47281">pandas</st>` <st c="47287">DataFr</st><st c="47294">ames instead, we
    set the transform output</st> <st c="47337">to</st> `<st c="47340">pandas</st>`<st
    c="47346">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47347">Using decision trees for discretization</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="47387">In all previous</st> <st c="47403">recipes in this chapter, we
    determined the number of intervals arbitrarily, and then the discretization algorithm
    would find the interval limits one way or another.</st> <st c="47569">Decision
    trees can find the interval limits and th</st><st c="47619">e optimal number of</st>
    <st c="47640">bins autom</st><st c="47650">atically.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47660">Decision</st> <st c="47669">tree methods discretize continuous
    attributes during the learning process.</st> <st c="47745">At each node, a decision
    tree evaluates all possible values of a feature and selects the cut point that
    maximizes the class separation, or sample coherence, by utilizing a performance
    metric such as entropy or Gini impurity for classification, or the squared or
    absolute error for regression.</st> <st c="48038">As a result, the observations
    end up in certain leaves based on whether their feature values are greater or
    smaller than certain</st> <st c="48167">cut points.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48178">In the following figure, we can see the diagram of a decision
    tree that is trained to predict house prices based on the property’s average number</st>
    <st c="48325">of rooms:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – A diagram of a decision tree trained to predict house price
    based on the property’s average number of rooms](img/B22396_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="48749">Figure 4.12 – A diagram of a decision tree trained to predict
    house price based on the property’s average number of rooms</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48870">Based on this decision tree, houses with a smaller mean number
    of rooms than 5.5 will go to the first leaf, houses with a mean number of rooms
    between 5.5 and 6.37 will fall into the second leaf, houses with mean values between
    6.37 and 10.77 will end up in the third leaf, and</st> <st c="49149">houses with
    mean values greater than 10.77 will land in the</st> <st c="49209">fourth leaf.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49221">As you see, by design, decision trees</st> <st c="49259">can find
    the set of cut points that partition a variable into intervals with good</st>
    <st c="49342">class coherence.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49358">In this recipe,</st> <st c="49375">we will perform d</st><st c="49392">ecision
    tree-based discretization</st> <st c="49427">using Feature-engine.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49448">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="49464">Let’s begin by importing some libraries and loading</st> <st c="49517">the
    data:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49526">Let’s import the required Python libraries, classes,</st> <st
    c="49580">and datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="49853">Let’s load the California housing dataset into a</st> `<st c="49903">pandas</st>`
    <st c="49909">DataFrame and then split it into train and</st> <st c="49953">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="50117">Let’s make a list with the names of the variables</st> <st c="50168">to
    discretize:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="50215">If we execute</st> `<st c="50230">print(variables)</st>`<st c="50246">,
    we’ll see the following variable names:</st> `<st c="50288">['MedInc'</st>`<st
    c="50297">,</st> `<st c="50299">'HouseAge'</st>`<st c="50309">,</st> `<st c="50311">'AveRooms'</st>`<st
    c="50321">,</st> `<st c="50323">'AveBedrms'</st>`<st c="50334">,</st> `<st c="50336">'</st>``<st
    c="50337">Population'</st>`<st c="50348">,</st> `<st c="50350">'AveOccup']</st>`<st
    c="50361">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="50362">Let’s set</st> <st c="50373">up the transformer to discretize</st>
    <st c="50405">the variables from</st> *<st c="50425">step 3</st>*<st c="50431">.
    We want the transformer to optimize the hyperparameter’s maximum depth and minimum
    samples per leaf of each tree based on the negative mean square error metric using
    three-fold cross-validation.</st> <st c="50628">As the output of the discretization,
    we want the limits of</st> <st c="50687">the intervals:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="50923">Let’s fit the discretizer using the train set so that it finds
    the best decision trees for each of</st> <st c="51023">the variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51064">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51069">You can inspect the limits of the found intervals for each variable
    in the</st> `<st c="51145">binner_dict_</st>` <st c="51157">attribute by executing</st>
    `<st c="51181">disc.binner_dict_</st>`<st c="51198">. Note how the discretizer
    appended minus and plus infinity to the limits to accommodate smaller and greater
    values than those observed in the</st> <st c="51341">training set.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51354">Let’s discretize the variables and then display the first five
    rows of the transformed</st> <st c="51442">training set:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51547">In the following</st> <st c="51564">output, we can see the limits
    of the</st> <st c="51601">intervals to which each observation</st> <st c="51638">was
    allocated:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The first five rows of the transformed training set containing
    the discretized variables](img/B22396_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="52162">Figure 4.13 – The first five rows of the transformed training
    set containing the discretized variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52264">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52269">If you choose to return the interval limits and want to use these
    datasets to train machine learning models, you will need to follow up the discretization
    with one-hot encoding or ordinal encoding.</st> <st c="52468">Check the recipes
    in</st> [*<st c="52489">Chapter 2</st>*](B22396_02.xhtml#_idTextAnchor182)<st
    c="52498">,</st> *<st c="52500">Encoding Categorical Variables</st>*<st c="52530">,
    for</st> <st c="52536">more details.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52549">Instead of returning the interval limits, we can return the interval
    number to which</st> <st c="52634">each observation is allocated by setting</st>
    <st c="52675">up the transformer</st> <st c="52695">like this:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="52912">We can now fit and then transform the training and</st> <st c="52964">testing
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53056">If you now execute</st> `<st c="53076">train_t[variables].head()</st>`<st
    c="53101">, you will see integers as a result instead of the</st> <st c="53152">interval
    limits:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The first five rows of the transformed training set containing
    the discretized variables](img/B22396_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="53292">Figure 4.14 – The first five rows of the transformed training
    set containing the discretized variables</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53394">To wrap up the recipe, we will make the discretizer return the
    predictions of the trees as replacement values for the</st> <st c="53513">discretized
    variables:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53535">Let’s set up</st> <st c="53548">the transformer to return the</st>
    <st c="53579">predictions, then fit it to the training set, and finally transform</st>
    <st c="53647">both datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53962">Let’s explore the number of unique values of the</st> `<st c="54012">AveRooms</st>`
    <st c="54020">variable before and after</st> <st c="54047">the discretization:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54125">In the following output, we can see that the predictions of the
    decision trees are also discrete or finite because the trees contain a finite
    number of end leaves;</st> `<st c="54290">7</st>`<st c="54291">, while the original</st>
    <st c="54311">variable</st> <st c="54321">contained more than 6000</st> <st c="54346">different
    values:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54373">To better understand the structure of the tree, we can capture
    it into</st> <st c="54445">a variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54509">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54514">When we set the transformer to return integers or bin limits,
    we will obtain the bin limits in the</st> `<st c="54614">binner_dict_</st>` <st
    c="54626">attribute.</st> <st c="54638">If we set the transformer to return the
    tree predictions,</st> `<st c="54696">binner_dict_</st>` <st c="54708">will contain
    the trained tree for</st> <st c="54743">each variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54757">Now, we can display the</st> <st c="54782">tree structure:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="54888">In the following figure, we can see the values used by the tree
    to allocate samples to the different end leaves based on the mean number</st>
    <st c="55026">of rooms:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15 – The structure of the decision tree trained to discretize AveRooms](img/B22396_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="55862">Figure 4.15 – The structure of the decision tree trained to discretize
    AveRooms</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55941">To wrap up the recipe, we can plot the number of observations
    per bin for three of</st> <st c="56025">the</st> <st c="56029">variables:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="56457">We can s</st><st c="56466">ee the</st> <st c="56474">number of
    observations per bin in the</st> <st c="56512">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.16 – The proportion of observations \uFEFFper bin after discretizing\
    \ the variables with decision trees](img/B22396_04_16.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="56827">Figure 4.16 – The proportion of observations</st> <st c="56872">per
    bin after discretizing the variables with decision trees</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56932">As</st> <st c="56936">evidenced in</st> <st c="56949">the plots,
    discretization with decision tree</st><st c="56993">s</st> <st c="56996">returns
    a different fraction of observations at each node</st> <st c="57054">or bin.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57061">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57077">To perform discretization with decision trees, we used</st> <st
    c="57133">f</st>`<st c="57134">eature-engine</st>`<st c="57147">’s</st> `<st c="57151">Decision</st>`
    **<st c="57159">TreeDiscretiser()</st>**<st c="57177">. This transformer fitted
    a decision tree using each variable to discretize as input and optimized the hyperparameters
    of the model to find the best</st> <st c="57326">partitions</st> <st c="57337">based
    on a performance metric.</st> <st c="57368">It automatically found the optimal
    number of intervals, as well as their limits, returning</st> <st c="57458">either
    the limits, the bin number, or the predictions as</st> <st c="57516">a result.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57525">There’s more...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="57541">The implementation of</st> `<st c="57564">feat</st><st c="57568">ure-engine</st>`
    <st c="57579">is inspired by the winning solution of the KDD 2009 data science
    competition.</st> <st c="57658">The winners created new features by obtaining
    predictions of decision trees based on continuous features.</st> <st c="57764">You
    can find more details in the</st> *<st c="57797">Winning the KDD Cup Orange Challenge
    with Ensemble Selection</st>* <st c="57857">article on</st> *<st c="57869">page
    27</st>* <st c="57876">of the article series</st> <st c="57899">at</st> [<st c="57902">http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf</st>](http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf)<st
    c="57957">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57958">For a review of discretization techniques, you might find the
    following</st> <st c="58031">articles useful:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="58047">Dougherty et al,</st> *<st c="58065">Supervised and Unsupervised
    Discretization of Continuous Features, Machine Learning: Proceedings of the 12th
    International Conference</st>*<st c="58198">,</st> <st c="58200">1995, (</st>[<st
    c="58207">https://ai.stanford.edu/~ronnyk/disc.pdf</st>](https://ai.stanford.edu/~ronnyk/disc.pdf)<st
    c="58248">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="58251">Lu et al,</st> *<st c="58262">Discretization: An Enabling Technique,
    Data Mining, and Knowledge Discovery</st>*<st c="58337">, 6, 393–423,</st> <st
    c="58351">2002, (</st>[<st c="58358">https://www.researchgate.net/publication/220451974_Discretization_An_Enabling_Technique</st>](https://www.researchgate.net/publication/220451974_Discretization_An_Enabling_Technique)<st
    c="58446">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="58449">Garcia et al,</st> *<st c="58464">A Survey of Discretization
    Techniques: Taxonomy and Empirical Analysis in Supervised Learning, IEEE Transactions
    on Knowledge in Data</st> <st c="58597">Engineering 25 (4)</st>*<st c="58616">,</st>
    <st c="58618">2013, (</st>[<st c="58625">https://ieeexplore.ieee.org/document/6152258</st>](https://ieeexplore.ieee.org/document/6152258)<st
    c="58670">).</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
