- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-Parametric Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed parametric tests. Parametric tests are
    useful when test assumptions are met. However, there are cases where those assumptions
    are not met. In this chapter, we will discuss several non-parametric alternatives
    to the parametric tests presented in the previous chapter. We start by introducing
    the concept of a non-parametric test. Then, we will discuss several non-parametric
    tests that can be used when t-test or z-test assumptions are not met.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: When parametric test assumptions are violated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rank-sum test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The signed-rank test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kruskal-Wallis test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chi-square test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spearman’s correlation analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi-square power analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When parametric test assumptions are violated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed parametric tests. Parametric tests have
    strong statistical power but also require adherence to strong assumptions. When
    the assumptions are not satisfied, the test results are not valid. Fortunately,
    we have alternative tests that can be used when the assumptions of a parametric
    test are not satisfied. These tests are called **non-parametric** tests, meaning
    that they make *no assumptions about the underlying distribution of the data*.
    While non-parametric tests do not require distributional assumptions, these *tests
    will still require the samples to* *be independent*.
  prefs: []
  type: TYPE_NORMAL
- en: Permutation tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the first non-parametric test, let’s look more deeply at the definition
    of a p-value. A p-value is the *probability of obtaining a test statistic at least
    as extreme as the observed value* under the assumption of the null hypothesis.
    Then, to calculate a p-value, we need the null distribution and an observed statistic.
    The p-value is the proportion of samples with a test statistic more extreme than
    the observed statistic. It turns out that we can construct the null distribution
    using permutations from data. Let’s see how to construct the null distribution
    using the following dataset. This dataset could represent counts of machine failures
    at low and high temperatures. We assume that the samples are independent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To construct the null hypothesis, we first need to decide on a statistical
    measure. In this case, we will look for a difference in the mean of the two distributions.
    So, our statistical measure will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: _ x  lowtemp −  _ x  hightemp
  prefs: []
  type: TYPE_NORMAL
- en: Now, to calculate the distribution values, calculate the statistical measure
    for all permutations of the dataset. Here are a couple of examples of the permutations
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Label** | **Observed** | **P1** | **P2** | **P3** | **P4** | **P5** | **…**
    |'
  prefs: []
  type: TYPE_TB
- en: '| low | 0 | 1 | 0 | 1 | 0 | 1 | … |'
  prefs: []
  type: TYPE_TB
- en: '| low | 0 | 1 | 3 | 0 | 3 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| low | 0 | 0 | 0 | 1 | 1 | 3 | … |'
  prefs: []
  type: TYPE_TB
- en: '| low | 0 | 1 | 1 | 0 | 2 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| low | 0 | 0 | 1 | 2 | 0 | 1 | … |'
  prefs: []
  type: TYPE_TB
- en: '| low | 1 | 1 | 1 | 3 | 0 | 1 | … |'
  prefs: []
  type: TYPE_TB
- en: '| low | 1 | 0 | 1 | 1 | 1 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| high | 1 | 0 | 2 | 0 | 1 | 2 | … |'
  prefs: []
  type: TYPE_TB
- en: '| high | 2 | 3 | 0 | 0 | 0 | 1 | … |'
  prefs: []
  type: TYPE_TB
- en: '| high | 3 | 0 | 0 | 1 | 1 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| high | 1 | 2 | 0 | 0 | 0 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| mean difference | -1.46 | -0.68 | 0.5 | 0.89 | 0.5 | 0.11 | … |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.1 – First five permutations of the observed data and the mean difference
  prefs: []
  type: TYPE_NORMAL
- en: The table shows the observed data with five randomly generated permutations
    of the values. We calculate the difference in the mean for each permutation. The
    differences in means are the values of the null distribution. Once we have the
    distribution, we can calculate the p-value as the proportion of values more extreme
    than the observed value.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling of permutation calculations
  prefs: []
  type: TYPE_NORMAL
- en: In general, permutation tests can be expensive to calculate because the number
    of permutations grows quickly with the size of the distribution. For example,
    dataset sizes of 3, 5, and 7 samples correspond to permutation sizes of 6, 120,
    and 5,040\. The dataset shown here has more than 39 million permutations! The
    runtime performance of a permutation test on a large dataset will likely be slow
    due to the number of permutations necessary to compute the null distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform a permutation test in Python using the `permutation_test` function
    from `scipy`. This function calculates the test statistic, the null distribution,
    and the p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function produces the following distribution from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Null distribution from the permutation test](img/B18253_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Null distribution from the permutation test
  prefs: []
  type: TYPE_NORMAL
- en: The null distribution from the permutation test and the observed difference
    in the mean of the two groups is shown in *Figure 5**.2*. The p-value from the
    permutation test is 0.036\. The permutation test is the first of several non-parametric
    tests that will be covered in this section. Again, these types of tests are useful
    when the assumptions for parametric tests are not met. *However, if a parametric
    test can be used, it should be used*; non-parametric tests should not be used
    as default methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we introduced non-parametric tests with permutation tests.
    The permutation test is a widely applicable non-parametric test, but the computations
    for permutations grow quickly with the size of the dataset, which may make its
    use impractical in some situations. In the following sections, we will cover several
    other non-parametric tests that do not require computing a null distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The Rank-Sum test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the assumptions of the t-test are not met, the Rank-Sum test is often a
    good non-parametric alternative test. While the t-test can be used to test for
    the *difference between the means of two distributions*, the Rank-Sum test is
    used to test for the *difference between the locations of two distributions*.
    This difference in the test utility is due to the lack of parametric assumptions
    in the Rank-Sum test. The null hypothesis of the Rank-Sum test is that the distribution
    underlying the first sample is the same as the second sample. If the sample distributions
    appear to be similar, this allows us to use the Rank-Sum test to test for the
    difference in the locations of the two samples. As stated, the Rank-Sum test cannot
    specifically be used for testing the difference between means because it does
    not require assumptions about the sample distributions.
  prefs: []
  type: TYPE_NORMAL
- en: The test statistic procedure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The test procedure is straightforward. The process is outlined here and an
    example is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: Combine all sample values into one set and sort the samples in ascending order,
    keeping track of their labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign ranks to all samples starting with rank 1 for the lowest sample value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where ties occur, replace the rank of the tied values with the mean rank of
    the tied values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum the ranks for the smallest sample group, which is the test statistic T.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the test statistic is calculated, the p-value can be calculated. The p-value
    can be done with a normal approximation or with an exact method. Generally, the
    exact method is only used when the sample size is small (less than 8 samples).
  prefs: []
  type: TYPE_NORMAL
- en: Normal approximation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the T statistic is calculated for the Rank-Sum test, we can determine
    a p-value with an exact method or with a normal approximation. We will cover the
    approximation method here as the exact method requires a permutation test, which
    will require the use of software. We approximate the p-value with a z-score (recall
    the z-score from [*Chapter 3*](B18945_03.xhtml#_idTextAnchor055)*,* *Hypothesis
    Testing*):'
  prefs: []
  type: TYPE_NORMAL
- en: Z =  T − Mean(T) _ STDEV(T)
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: Mean(T) = n T _ R
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: STDEV(T) = s R √ _  n T n O _ n T + n O
  prefs: []
  type: TYPE_NORMAL
- en: In the equations here, n T is the number of samples in the group used to calculate
    T, n O is the number of samples in the other group,  _ R  is the mean of the corrected
    ranks, and s Ris the standard deviation of the corrected ranks. Once Z is calculated,
    the corresponding p-value can be looked up for the z-distribution. Having described
    the method, let’s look at an example. The data shown in the following table can
    be found in the accompanying Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Rank-Sum example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following table shows this process performed on a set of data for two groups
    labeled with “L” and “H,” where a Rank-Sum test will be used to test for the difference
    in location of the two sample distributions. The test statistic for this table
    is 42.5, which is the sum of the corrected ranks of group L. This test statistic
    corresponds to an approximate p-value of 0.00194 (a two-sided test). The data
    for this table was downloaded from [https://github.com/OpenIntroStat/openintro/raw/master/data/gpa_iq.rda](https://github.com/OpenIntroStat/openintro/raw/master/data/gpa_iq.rda).
  prefs: []
  type: TYPE_NORMAL
- en: '| **IQ** | **Group** | **Rank** | **Corrected Rank** |'
  prefs: []
  type: TYPE_TB
- en: '| 77 | L | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 79 | L | 2 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 93 | L | 3 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 96 | L | 4 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 104 | L | 5 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 105 | H | 6 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 106 | H | 7 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 107 | L | 8 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 109 | L | 9 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| **IQ** | **Group** | **Rank** | **Corrected Rank** |'
  prefs: []
  type: TYPE_TB
- en: '| 111 | H | 10 | 10.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 111 | L | 11 | 10.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 112 | H | 12 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| 116 | H | 13 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| 118 | H | 14 | 14 |'
  prefs: []
  type: TYPE_TB
- en: '| 124 | H | 15 | 15 |'
  prefs: []
  type: TYPE_TB
- en: '| 126 | H | 16 | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| 127 | H | 17 | 17 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | H | 18 | 18.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 128 | H | 19 | 18.5 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.3 – Corrected ranks for Rank-Sum test
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also perform this test with software using `mannwhitenyu` from `scipy`.
    For the following code sample, the values corresponding to L and H from the preceding
    table are contained in the `lower_score_iqs` and `higher_score_iqs` variables,
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we discussed the Rank-Sum test, which is a non-parametric alternative
    to the t-test. The Rank-Sum test is used to test for a difference in the locations
    of two distributions of sample data. In the next section, we will look at a similar
    rank-based test, which is used to compare paired data like the paired t-test from
    [*Chapter 4*](B18945_04.xhtml#_idTextAnchor070)*,* *Parametric Tests*.
  prefs: []
  type: TYPE_NORMAL
- en: The Signed-Rank test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wilcoxon Signed-Rank test is a non-parametric alternative version of the
    paired t-test that is used when the assumption of normality is violated. This
    test is robust to outliers because of the use of ranks and medians instead of
    means in the null and alternative hypotheses. As indicated by the name of the
    test, it uses the magnitudes of differences between two stages and their signs.
  prefs: []
  type: TYPE_NORMAL
- en: In research, a null hypothesis considers that the median difference between
    stage 1 and stage 2 is zero. Similarly, as in a paired t-test, for the alternative
    hypothesis, for a two-tailed test, the median difference between Stage 1 and Stage
    2 is considered not to be zero, or for a one-tailed test, the median difference
    between Stage 1 and Stage 2 is greater (or less) than zero.
  prefs: []
  type: TYPE_NORMAL
- en: Though the normality requirement is relaxed, the test requires independence
    between paired observations and these observations to be from the same population.
    In addition, the dependent variable is required to be continuous.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the test statistic, there are the following procedures:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the differences in each pair between the two stages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop pairs with zero difference, if they exist.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the absolute difference between each pair and rank them from smallest to
    largest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the signed-rank statistic S by summing the ranks with a positive sign.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us consider the simple example that follows. We consider generic data for
    two stages (before and after the treatment) of nine samples.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pair** | **Before** **the treatment** | **After** **the treatment** | **Absolute**
    **difference** | **Sign** | **Rank** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 37 | 38 | 1 | - | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 14 | 17 | 3 | - | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 22 | 19 | 3 | + | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12 | 7 | 5 | + | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 24 | 15 | 9 | + | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 35 | 25 | 10 | + | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 35 | 24 | 11 | + | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 51 | 38 | 13 | + | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 39 | 19 | 20 | + | 9 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.4 – Demonstration of rank calculation for sign-rank test
  prefs: []
  type: TYPE_NORMAL
- en: 'By observing the preceding table, we can see the second and the third pairs
    have the same absolute difference. Therefore, their ranks are computed by averaging
    their original rank, (2+3)/2 = 2.5\. The null and alternative hypotheses for a
    one-sided test are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : The median difference between before and after treatment is zero'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : The median difference between before treatment and after treatment is
    positive'
  prefs: []
  type: TYPE_NORMAL
- en: 'The signed-rank statistic is the sum of ranks for positive differences and
    it is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: S = 2.5 + 4 + 5 + 6 + 7 + 8 + 9 = 41.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean of S is:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean(S) =  n(n + 1) _ 4  =  9.10 _ 4  = 22.5
  prefs: []
  type: TYPE_NORMAL
- en: 'And the standard deviation of S is:'
  prefs: []
  type: TYPE_NORMAL
- en: SD(S) = √ ____________   n(n + 1)(2n + 1)  ____________ 24   = √ _  9 * 10 *
    19 _ 24   = 8.44.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the test statistic is calculated by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Z statistic = S − Mean(S) _ SD(S)  = 41.5 − 22.5 _ 8.44  = 2.2511.
  prefs: []
  type: TYPE_NORMAL
- en: Referring to [*Chapter 3*](B18945_03.xhtml#_idTextAnchor055)*, Hypothesis Testing*,
    we could use `scipy.stats.norm.sf()` to calculate the approximate one-sided `p-value
    0.012` using the Z statistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'At α = 0.05 – level of significance – with `p-value <` α, we reject the null
    hypothesis. There is strong evidence that the median before the treatment is greater
    than the median after the treatment. In Python, it is simple to implement the
    test as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The documentation for this test can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.xhtml](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: The Kruskal-Wallis test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another non-parametric test we will now discuss is the Kruskal-Wallis test.
    It is an alternative to the one-way ANOVA test when the normality assumption is
    not satisfied. It uses the medians instead of the means to test whether there
    are statistically significant differences between two or more independent groups.
    Let us consider a generic example of three independent groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The null and alternative hypotheses are stated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : The medians are equal among these three groups'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : The medians are not equal among these three groups'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, it is easy to implement by using the `scipy.stats.kruskal` function.
    The documentation can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.xhtml](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KruskalResult(statistic=5.7342701722574905, pvalue=0.056861597028239855)`'
  prefs: []
  type: TYPE_NORMAL
- en: As α = 0.05 – the level of significance – with `p-value >` α, we fail to reject
    the null hypothesis. There is no strong evidence to show that the medians are
    not equal across these three groups.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Researchers are often faced with the need to test hypotheses on categorical
    data. The parametric tests covered in [*Chapter 4*](B18945_04.xhtml#_idTextAnchor070)*,
    Parametric Tests*, are often not very helpful for this type of analysis. In the
    last chapter, we discussed using an F-test to compare sample variances. Extending
    that concept, we can consider the non-parametric and non-symmetric chi-square
    probability distribution, which is a distribution useful for comparing the means
    of sampling distribution variances to their population variances, specifically
    when the mean of a sampling distribution of sample variances is expected to equal
    the population variance under the null hypothesis. Because variance cannot be
    negative, the distribution starts at an origin of 0\. Here, we can see the **chi-square
    distribution**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Chi-square distribution with seven degrees of freedom](img/B18253_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Chi-square distribution with seven degrees of freedom
  prefs: []
  type: TYPE_NORMAL
- en: 'The shape of the chi-square distribution does not represent an assumption that
    percentiles are fixed to standard deviations, as with standard normal distribution;
    it is expected to change with each additional sample variance calculated. When
    the original population data from which the sampling distribution of variances
    is calculated can be assumed to be normally distributed, the chi-square standardized
    test statistic is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: χ 2 =  (n − 1) s 2 _ σ 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Where (n-1) is used to calculate the degrees of freedom, which are used to
    explain errors in sampling when building the test statistic. The critical values
    are based on table lookups using degrees of freedom and desired levels of significance.
    The null hypothesis is always H 0 : σ χ 2 = σ 0 2, where σ χ 2 is the observed
    distribution variance and σ 0 2 is the expected variance. The alternative hypothesis
    for a one-tailed test is H a : σ χ 2 ≥ σ 0 2 when testing the right tail and H a
    : σ χ 2 ≤ σ 0 2 when testing the left tail. The alternative hypothesis for a two-tailed
    test is H a : σ χ 2 ≠ σ 0 2\. This test, when the population data can be assumed
    to be normally distributed, is similar to the F-test in that the two variances
    are being compared and the result is 1 when both are the same. The difference
    is that the chi-square test statistic factors in the degrees of freedom and is
    therefore less sensitive to differences.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how this test is useful for comparing variances is helpful for
    understanding how the chi-square distribution also relates to frequencies of occurrences
    of categorical data. In the preceding chi-square statistic, we can relate the
    sample statistic for variance as an observed value whereas the population parameter
    for variance is what we expect to occur. When determining whether there is statistical
    significance in the occurrence of categorical factor levels, we can *compare the
    observed occurrences to expected occurrences*. We will illustrate examples for
    arguably the two most widely used versions of this test, the **chi-square goodness-of-fit
    test** and the **chi-square test of independence**, in the following sections.
    These tests are considered non-parametric as they do not require the assumptions
    stated in the first section of [*Chapter 4*](B18945_04.xhtml#_idTextAnchor070)*,*
    *Parametric Tests*.
  prefs: []
  type: TYPE_NORMAL
- en: Tailedness of chi-square goodness-of-fit and independence tests
  prefs: []
  type: TYPE_NORMAL
- en: The chi-square goodness-of-fit test and chi-square test of independence in the
    following two sections are always right-tailed tests. The null hypothesis in these
    tests states a difference of zero between the observed and expected frequencies.
    The alternative hypothesis is that the observed and expected frequencies are not
    the same. The closer to 0 the χ 2 test statistic, the more likely the observed
    and expected frequencies approximate each other.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square goodness-of-fit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **chi-square goodness-of-fit test** compares the count of occurrences of
    multiple factor levels for a single variable (factor) to determine whether the
    levels are statistically equal. For example, a vendor offers three models of phones
    – three levels (brands) of the single factor (phone) – to customers, who purchase
    in total an average of 90 phones per week. We can say the expected frequency is
    1/3 – so, 30 phones of each model are sold per week, on average. Pearson’s chi-square
    test statistic, which is calculated by measuring the observed frequencies against
    expected frequencies, is the test statistic used for the chi-square goodness-of-fit
    test. The linear equation for this test statistic is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: χ 2 = ∑ (O i − E i) 2 _ E i , degrees of freedom = k-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Where O i is the observed frequency, E i, is the expected frequency, and *k*
    is the number of factor levels. Using our phone example, we learn from the vendor
    the expected frequency of 1/3 is not actually what is observed; the vendor sells
    an average of 45, 30, and 15 phones of models A, B, and C, respectively. Suppose
    we want to know whether this observed frequency differs with statistical significance
    from the expected frequency. The null hypothesis is that the frequencies are equal.
    We formulate the **Pearson’s chi-square test statistic** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: χ2 =  (45 − 30) 2 _ 30  +  (30 − 30) 2 _ 30  +  (15 − 30) 2 _ 30  = 15
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to use a hypothesis test (right-tailed) with a 0.05 level of
    significance, thus forming the alternative hypothesis that the observed and expected
    frequencies are not equal. The chi-square critical value table shows that for
    a level of significance of 0.05 with df : 3 − 1 = 2 degrees of freedom, the critical
    value is 5.9915\. Because 15 > 5.9915, we may conclude to reject the null hypothesis
    based on the critical value test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform this operation in Python, we can use the `statsmodels.stats.gof`
    module’s `chisquare` function. Each group volume needs to be passed into a list
    or array, both for the observed frequencies and the expected frequencies. The
    `statsmodels` `chisquare` test will automatically calculate the k-1 degrees of
    freedom based on the counts of values in the observed (`f_obs`) list, but the
    degrees of freedom must be provided for the `scipy chi2.ppf` function, which provides
    the critical value, bypassing the need for a manual table lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As noted previously, since the p-value is below the 0.05 level of significance
    and the test statistic is larger than the critical value, we can assume we have
    a reasonable amount of evidence to reject the null hypothesis and conclude we
    have statistical significance indicating the phone models are not purchased in
    equal quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Chi-Square Test` `Statistic: 15.0000`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Chi-Square Critical` `Value: 5.9915`'
  prefs: []
  type: TYPE_NORMAL
- en: '`P-Value: 0.0006`'
  prefs: []
  type: TYPE_NORMAL
- en: We could then rerun the test for two frequencies at that point, comparing 45
    to 30 or 45 to 15, for example, with expected frequencies of 37.5 and 37.5 or
    30 and 30, respectively, to identify which phone may be the largest influencer
    on sales imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square test of independence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have a dataset of observed vehicle crashes in the state of Texas
    in 2021, *Restraint Use by Injury Severity and Seat Position* ([https://www.txdot.gov/data-maps/crash-reports-records/motor-vehicle-crash-statistics.xhtml](https://www.txdot.gov/data-maps/crash-reports-records/motor-vehicle-crash-statistics.xhtml)),
    and want to know whether using a seat belt resulted in a statistically significant
    difference in fatalities. We have the table of observed values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Restrained** | **Unrestrained** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| Fatal | 1,429 | 1,235 | 2,664 |'
  prefs: []
  type: TYPE_TB
- en: '| Not Fatal | 1,216,934 | 22,663 | 1,239,597 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 1,218,363 | 23,898 | 1,242,261 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.6 – Chi-square test of independence “observed” table
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us now create a table of expected values using this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: E ij =  T i T j _ N
  prefs: []
  type: TYPE_NORMAL
- en: 'Where T i is the total in the *i*th row, T j is the total in the *j*th column,
    and N is the total number of observations. This yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Restrained** | **Unrestrained** | **Total** |'
  prefs: []
  type: TYPE_TB
- en: '| Fatal | (2,664 *1,218,363)/1,242,261 = 2,612.75 | (2,664 *23,898)/1,242,261
    = 51.25 | 2,664 |'
  prefs: []
  type: TYPE_TB
- en: '| Not Fatal | (1,216,934* 1,239,597) / 1,242,261 = 1,214,324.31 | (1,239,597
    * 23,898) / 1,242,261 = 23,846.75 | 1,239,597 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 1,218,363 | 23,898 | 1,242,261 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.7 – Chi-square test of independence “expected” table
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified version of the Pearson’s chi-square test statistic we used in
    the goodness-of-fit test that extends to the chi-square test of independence follows:'
  prefs: []
  type: TYPE_NORMAL
- en: χ 2 = ∑ (O ij − E ij) 2 _ E ij , degrees of freedom = (r-1)(c-1)
  prefs: []
  type: TYPE_NORMAL
- en: The subscript *j* corresponds to columnar data and the subscript *i* corresponds
    to the row data for the observed and expected values, denoted as *O* and *E*,
    respectively. The values *r* and *c* in the degrees of freedom calculation correspond
    to the number of rows and the number of columns in the table (this is 2x2, so
    the degrees of freedom will equal (2-1)(2-1)=1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the values from the observed and expected tables, we find the chi-square
    test statistic to equal the following:'
  prefs: []
  type: TYPE_NORMAL
- en: χ 2 =  (1,429 − 2,612.75) 2  ______________ 2,612.75  +  (1,216,934 − 1,214,324.31) 2  ____________________  1,214,324.31 
    +  (1,235 − 51.25) 2  ____________ 51.25  +
  prefs: []
  type: TYPE_NORMAL
- en: (22, 663 − 23,846.75) 2  ________________  23,846.75  = 27,942.43
  prefs: []
  type: TYPE_NORMAL
- en: The table lookup test statistic using the one degree of freedom calculated here
    is 3.84\. We can reasonably conclude that since the test statistic of 27,942.43
    is greater than the critical value of 3.84, we can conclude that in the state
    of Texas in 2021, there was a statistically significant difference in the rate
    of fatalities in vehicle crashes where safety restraints were used compared to
    where they were not used.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square contingency tables
  prefs: []
  type: TYPE_NORMAL
- en: The test we performed uses two 2x2 tables called contingency tables. The chi-square
    test of independence is frequently referred to as the **chi-squared contingency
    test**. The failure to reject the null hypothesis is **contingent** upon the observed
    table’s values matching the expected table’s values within a level of statistical
    confidence. However, the chi-square test of independence can be extended to **tables
    of any combination of rows and columns**. However, as the rows and columns increase
    in value, the tables may be less useful to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform this test in Python, we can use the `chi2_contingency` function
    from the `scipy` `stats` module. Inputting only the observed frequencies as a
    2x2 `numpy` array, the `chi2_contingency` test provides the expected frequencies
    should the null hypothesis be true– the p-value, degrees of freedom used, and
    the chi-square test statistic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see the chi-square test statistic is much larger than the chi-square
    critical value at the 0.05 level of significance and the p-value is significant
    at p < 0.0000\. Therefore, we can conclude there is strong statistical evidence
    to suggest a large difference in car crash fatalities when using a safety restraint
    compared to not using one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Chi-Square Test` `Statistic: 27915.1221`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Chi-Square Critical` `Value: 3.8415`'
  prefs: []
  type: TYPE_NORMAL
- en: '`P-Value: 0.0000`'
  prefs: []
  type: TYPE_NORMAL
- en: Yates’ continuity correction
  prefs: []
  type: TYPE_NORMAL
- en: An additional argument can optionally be added to the `chi2_contingency` test
    with the `correction` `=` argument and a bool (`True` or `False`) input. This
    applies Yates’ continuity correction, which, per the `scipy.stats` documentation,
    adjusts each observed value by 0.5 toward the corresponding expected value. The
    purpose of the adjustment is to avoid incorrectly detecting the presence of statistical
    significance due to small expected sample sizes. Subjectively, this is useful
    for less than 10 samples in an expected frequency cell. However, many practitioners
    and researchers argue against its use.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square goodness-of-fit test power analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s use an example where a phone vendor sells four popular models of phones,
    models A, B, C, and D. We want to determine how many samples are required to produce
    a power of 0.8 so we can understand whether there is a statistically significant
    difference between the popularity of different phones so the vendor can more properly
    invest in phone acquisitions. In this case, the null hypothesis asserts that 25%
    of phones from each model were sold. In reality, 20% of phones sold were model
    A, 30% were model B, 19% were model C, and 31% were model D phones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Testing different values for the `nobs` argument (number of observations),
    we find that a minimum of 224 samples produces a power just greater than 0.801\.
    Adding more samples will only improve this. If the true distribution were more
    divergent from the hypothesized 25% even split, fewer samples would be required.
    However, since the splits are relatively close to 25%, a high volume of samples
    is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Spearman’s rank correlation coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 4*](B18945_04.xhtml#_idTextAnchor070)*, Parametric Tests*, we
    looked at the parametric correlation coefficient, Pearson’s correlation, where
    the coefficient is calculated from independently sampled, continuous data. However,
    **when we have ranked, ordinal data**, such as that from a satisfaction survey,
    we would not want to use Pearson’s correlation as it cannot be assumed to guarantee
    the preservation of order. As with Pearson’s correlation coefficient, **Spearman’s
    correlation coefficient** results in a coefficient, *r*, that ranges from -1 to
    1, with -1 being a strong inverse correlation and 1 being a strong direct correlation.
    Spearman’s is derived by dividing the covariance of the two variables’ ranks by
    the product of their standard deviations. The equation for the correlation coefficient,
    *r*, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: r s =  S xy _ √ _ S xx S yy
  prefs: []
  type: TYPE_NORMAL
- en: Where
  prefs: []
  type: TYPE_NORMAL
- en: S xy = ∑     (x i −  _ x )(y i −  _ y )
  prefs: []
  type: TYPE_NORMAL
- en: S xx = ∑     (x i −  _ x ) 2
  prefs: []
  type: TYPE_NORMAL
- en: S yy = ∑     (y i −  _ y ) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding correlation equation is safe to use under all ranking scenarios.
    However, when there are no ties or a minimal proportion of ties compared to the
    overall sample volume, the following equation may be used:'
  prefs: []
  type: TYPE_NORMAL
- en: r s =  6∑ d i 2 _ n(n 2 − 1), where d i = (x i − y i)
  prefs: []
  type: TYPE_NORMAL
- en: However, we will perform this test in Python. Therefore, the more complex and
    less error-prone formula will be applied regardless since computational power
    enables us to bypass the manual process entirely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have students being judged in a competition by two judges and there
    is a concern that one of the judges may be biased toward some of the participants
    based on confounding factors, such as family ties, rather than performance alone.
    We decide to run a correlation analysis on the scores to test the hypothesis the
    two judges scored similarly for each contestant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the following table of contestants:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Student 1** | **Student 2** | **Student 3** | **Student 4** | **Student
    5** | **Student 6** | **Student 7** |'
  prefs: []
  type: TYPE_TB
- en: '| Judge A | 1 | 3 | 5 | 7 | 8 | 3 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| Judge B | 2 | 5 | 3 | 9 | 6 | 1 | 7 |'
  prefs: []
  type: TYPE_TB
- en: Figure 5.8 – Judge rankings of student contestants for Spearman’s correlation
    analysis
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the p-value of 0.04 here, which is less than a significance level
    of 0.05, we can say the correlation produced is significant beyond a degree of
    random chance. Had the p-value been greater than 0.05, we could say with a 95%
    level of confidence that the correlation may have been purely spurious and may
    not have enough statistical evidence to support determinism. Spearman’s correlation
    (r) is 0.7748\. `r_squared` is therefore approximately 0.6003, meaning approximately
    60.3% of the variation in the score is determined by the judge’s scoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Spearman Correlation` `Coefficient: 0.7748`'
  prefs: []
  type: TYPE_NORMAL
- en: '`P-Value: 0.0408`'
  prefs: []
  type: TYPE_NORMAL
- en: Since the p-value is significant and the correlation coefficient is 0.77 – and
    a strong correlation coefficient starts at approximately 0.7 – we may conclude
    that the judges’ scores are directly correlated enough to assume there is no bias
    in scoring present, assuming a relatively objective method for ranking exists;
    something more subjective may not be as suitable for correlation analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed some of the most commonly used non-parametric
    hypothesis tests performed when required assumptions for parametric hypothesis
    testing cannot be prudently guaranteed. We discussed two-sample Wilcoxon Rank-Sum
    – also called Mann-Whitney U – tests to draw inferences from medians when two-sample
    t-testing cannot be performed. Next, we walked through the Wilcoxon Sign-Rank
    test’s paired comparison of medians when a paired t-test comparison of means cannot
    be performed. After, we looked at the non-parametric chi-square goodness-of-fit
    test and the chi-square Test of independence for comparing observed frequencies
    against expected frequencies, both useful for identifying the presence of statistically
    significant differences in counts of categorical data. Additionally, we discussed
    the Kruskal-Wallis test, a non-parametric alternative to the analysis of variance
    (ANOVA). Finally, we discussed Spearman’s correlation coefficient and how to derive
    correlation based on rank when parametric assumptions cannot safely support using
    Pearson’s correlation. Closing out the chapter, we provided an example of using
    power analysis for the chi-square test.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin our discussion of predictive analytics, starting
    with simple linear regression. There, we will discuss methods for fitting a model,
    interpreting linear coefficients, understanding the required assumptions of linear
    regression, assessing model performance, and considering modified versions of
    linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Part 2:Regression Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we discuss the types of problems that can be solved with regression,
    coefficients of correlation and determination, multivariate modeling, model selection
    and variable adjustment with regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'It includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18945_06.xhtml#_idTextAnchor104), *Simple Linear Regression*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18945_07.xhtml#_idTextAnchor118), *Multiple Linear Regression*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
