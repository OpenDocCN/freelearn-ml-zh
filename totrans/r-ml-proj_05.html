<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Customer Segmentation Using Wholesale Data</h1>
                </header>
            
            <article>
                
<p class="mce-root">In today's competitive world, the success of an organization largely depends on how much it understands its customers' behavior. Understanding each customer individually to better tailor the organizational effort to individual needs is a very expensive task. Based on the size of the organization, this task can be very challenging as well. As an alternative, organizations rely on something called <strong class="calibre3">segmentation</strong>, which attempts to categorize customers into groups based on identified similarities. This critical aspect of customer segmentation allows organizations to extend their efforts to the individual needs of various customer subsets (if not catering to individual needs), therefore reaping greater benefits.</p>
<p class="mce-root">In this chapter, we will learn about the concept and importance of customer segmentation. We'll then deep dive into learning the various <strong class="calibre3">machine learning</strong> (<strong class="calibre3">ML</strong>) methods to identify subgroups of customers based on customer characteristics. We'll implement several projects using the wholesale dataset to understand the ML techniques for segmentation. In the next section, we'll start by learning the foundations of customer segmentation and the need for ML techniques to achieve segmentation. We will cover the following topics as we progress:</p>
<ul class="calibre9">
<li class="calibre10">Understanding customer segmentation</li>
<li class="calibre10">Understanding the wholesale customer dataset and the segmentation problem</li>
<li class="calibre10">Identifying the customer segments in wholesale customer data using DIANA</li>
<li class="calibre10">Identifying the customer segments in wholesale customer data using AGNES</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding customer segmentation</h1>
                </header>
            
            <article>
                
<p class="mce-root">Customer segmentation, or market segmentation, at a basic level, is the partitioning of a broad range of potential customers in a given market into specific subgroups of customers, where each of the subgroups contains customers that share certain similarities. The following diagram depicts the formal definition of customer segmentation where customers are identified into three groups:</p>
<p class="CDPAlignCenter1"><img class="aligncenter46" src="assets/674f3a2b-403c-4d6b-adf4-f7668ed5c29f.png"/></p>
<div class="packtfigref">Illustration depicting customer segmentation definition</div>
<p class="mce-root">Customer segmentation needs the organizations to gather data about customers and analyze it to identify patterns that can be used to determine subgroups. The segmentation of customers could be achieved through multiple data points related to customers. The following are some of the data points:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Demographics</strong>: This data point includes race, ethnicity, age, gender, religion, level of education, income, life stage, marital status, occupation</li>
<li class="calibre10"><strong class="calibre1">Psychographics</strong>: <span>This data point includes l</span>ifestyle, values, socioeconomic standing, personality</li>
<li class="calibre10"><strong class="calibre1">Behavioral</strong>: <span>This data point includes p</span>roduct usage, loyalties, awareness, occasions, knowledge, liking, and purchase patterns</li>
</ul>
<p class="mce-root">With billions of people in the world, efficiently making use of customer segmentation will help organizations narrow down the pool and reach only the people that mean something to their business, ultimately driving conversions and revenue. The following are some of the specific objectives that organizations attempt to achieve through identifying segments in their customers:</p>
<ul class="calibre9">
<li class="calibre10">Identifying higher-percentage opportunities that the sales team can pursue</li>
<li class="calibre10">Identify<span>ing</span> customer groups that have a higher interest in the product, and customize the product according to the needs of high-interest customers</li>
<li class="calibre10">Develop<span>ing</span> very focused marketing messages to specific customer groups so as to drive higher-quality inbound interest in the product</li>
<li class="calibre10">Choos<span>ing</span> the best communication channel for various segments, which might be email, social media, radio, or another approach, depending on the segment</li>
<li class="calibre10">Concentrat<span>ing</span> on the most profitable customers</li>
<li class="calibre10">Upsell<span>ing</span> and cross-sell<span>ing</span> other products and services</li>
<li class="calibre10">Test pricing options</li>
<li class="calibre10">Identify<span>ing</span> new product or service opportunities</li>
</ul>
<p class="mce-root">When an organization needs to perform segmentation, it can typically look for common characteristics, such as shared needs, common interests, similar lifestyles, or even similar demographic profiles and come up with segments in customer data. Unfortunately, creating segments is not that simple. With the availability of big data, organizations now have hundreds of characteristics of customers they can look at in order to come up with segments. It is not feasible for a person or few people in an organization to go through hundreds of types of data, find relationships between each of them, and then establish segments based on several different values possible for each data point. That's where unsupervised ML, called <strong class="calibre3">clustering</strong>, comes to rescue.</p>
<p class="mce-root">Clustering is the mechanism of using ML algorithms to identify relationships in different types of data, thereby yielding new segments based on those relationships. Simply put, clustering finds the relationship between data points so they can be segmented.</p>
<p class="mce-root">The terms <strong class="calibre3">cluster analysis</strong> and <strong class="calibre3">customer segmentation</strong> are closely related and used interchangeably by <span class="calibre4">ML</span> practitioners. However, there is an important difference between these terms.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Clustering is a tool that helps organizations put together data based on similarities and statistical connections. Clustering is very helpful in guiding the development of suitable customer segments. It also provides useful statistical measures of the potential target customers. While the objective for an organization is to identify effective customer segments from data, simply applying a clustering technique on data and grouping the data in itself may or may not offer effective customer segments. This essentially means that the output obtained from clustering, that is, the <strong class="calibre3">clusters</strong>, need to be further analyzed to get insight into the meaning of each of the clusters, and then determine which clusters can be utilized for downstream activities, such as business promotions. The following is a flow diagram that helps us to understand the role of clustering in the customer-segmentation process:</p>
<p class="CDPAlignCenter1"><img class="aligncenter47" src="assets/966e98d2-c974-4edb-97be-fe67c9089fd6.png"/></p>
<div class="packtfigref">Role of clustering in customer segmentation</div>
<p class="mce-root">Now that we understand that clustering forms a stepping stone to performing customer segmentation, in the rest of the chapter, we will discuss various clustering techniques and implement projects around these techniques to create customer segments. For our projects, we make use of the wholesale customer dataset. Before delving into the projects, let's learn about the dataset and perform <strong class="calibre3">exploratory data analysis</strong> (<strong class="calibre3">EDA</strong>) to get a better understanding of the data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the wholesale customer dataset and the segmentation problem</h1>
                </header>
            
            <article>
                
<p class="mce-root">The UCI Machine Learning Repository offers the wholesale customer dataset at <a href="https://archive.ics.uci.edu/ml/datasets/wholesale+customers" class="calibre8">https://archive.ics.uci.edu/ml/datasets/wholesale+customers</a>. The dataset refers to clients of a wholesale distributor. It includes the annual spending in <strong class="calibre3">monetary units</strong> (<strong class="calibre3">m.u.</strong>) on diverse product categories. The goal of these projects is to apply clustering techniques to identify segments that are relevant for certain business activities, such as rolling out a marketing campaign. Before we actually use the clustering algorithms to get clusters, let's first read the data and perform some EDA to understand the data using the following code block:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<pre class="calibre16"># setting the working directory to a folder where dataset is located<br class="title-page-name"/>setwd('/home/sunil/Desktop/chapter5/')<br class="title-page-name"/># reading the dataset to cust_data dataframe<br class="title-page-name"/>cust_data = read.csv(file='Wholesale_customers_ data.csv', header = TRUE)<br class="title-page-name"/># knowing the dimensions of the dataframe<br class="title-page-name"/>print(dim(cust_data))<br class="title-page-name"/>Output : <br class="title-page-name"/>440 8<br class="title-page-name"/># printing the data structure<br class="title-page-name"/>print(str(cust_data))<br class="title-page-name"/>'data.frame': 440 obs. of 8 variables:<br class="title-page-name"/> $ Channel : int 2 2 2 1 2 2 2 2 1 2 ...<br class="title-page-name"/> $ Region : int 3 3 3 3 3 3 3 3 3 3 ...<br class="title-page-name"/> $ Fresh : int 12669 7057 6353 13265 22615 9413 12126 7579...<br class="title-page-name"/> $ Milk : int 9656 9810 8808 1196 5410 8259 3199 4956...<br class="title-page-name"/> $ Grocery : int 7561 9568 7684 4221 7198 5126 6975 9426...<br class="title-page-name"/> $ Frozen : int 214 1762 2405 6404 3915 666 480 1669...<br class="title-page-name"/> $ Detergents_Paper: int 2674 3293 3516 507 1777 1795 3140 3321...<br class="title-page-name"/> $ Delicassen : int 1338 1776 7844 1788 5185 1451 545 2566...<br class="title-page-name"/># Viewing the data to get an intuition of the data <br class="title-page-name"/>View(cust_data)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter48" src="assets/b7b0f664-aa72-4496-819a-c1aa3088fa7f.png"/></p>
<p class="mce-root">Now let's check whether there are any entries with missing fields in our dataset:</p>
<pre class="calibre16"># checking if there are any NAs in data<br class="title-page-name"/>print(apply(cust_data, 2, function (x) sum(is.na(x))))<br class="title-page-name"/>Output :<br class="title-page-name"/>Channel Region Fresh Milk <br class="title-page-name"/>0 0 0 0 <br class="title-page-name"/>Grocery Frozen Detergents_Paper Delicassen</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre class="calibre16"><br class="title-page-name"/>0 0 0 0 <br class="title-page-name"/># printing the summary of the dataset <br class="title-page-name"/>print(summary(cust_data))</pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">Channel Region Fresh Milk <br class="title-page-name"/> Min. :1.000 Min. :1.000 Min. : 3 Min. : 55 <br class="title-page-name"/> 1st Qu.:1.000 1st Qu.:2.000 1st Qu.: 3128 1st Qu.: 1533 <br class="title-page-name"/> Median :1.000 Median :3.000 Median : 8504 Median : 3627 <br class="title-page-name"/> Mean :1.323 Mean :2.543 Mean : 12000 Mean : 5796 <br class="title-page-name"/> 3rd Qu.:2.000 3rd Qu.:3.000 3rd Qu.: 16934 3rd Qu.: 7190 <br class="title-page-name"/> Max. :2.000 Max. :3.000 Max. :112151 Max. :73498 <br class="title-page-name"/> Grocery Frozen Detergents_Paper Delicassen <br class="title-page-name"/> Min. : 3.0 Min. : 3.0 Min. : 3 Min. : 25.0 <br class="title-page-name"/> 1st Qu.: 256.8 1st Qu.: 408.2 1st Qu.: 2153 1st Qu.: 742.2<br class="title-page-name"/> Median : 816.5 Median : 965.5 Median : 4756 Median : 1526.0<br class="title-page-name"/> Mean : 2881.5 Mean : 1524.9 Mean : 7951 Mean : 3071.9<br class="title-page-name"/> 3rd Qu.: 3922.0 3rd Qu.: 1820.2 3rd Qu.:10656 3rd Qu.: 3554.2<br class="title-page-name"/> Max. :40827.0 Max. :47943.0 Max. :92780 Max. :60869.0</pre>
<p class="mce-root">From the EDA, we see that there are <kbd class="calibre11">440</kbd> observations available in this dataset and there are eight variables. The dataset does not have any missing values. While the last six variables are goods that were brought by distributors from the wholesaler, the first two variables are factors (categorical variables) representing the location and channel of purchase. In our projects, we intend to identify the segments based on the sales into different products, therefore, the location and channel variables in the data are not very useful. Let's delete them from the dataset using the following code:</p>
<pre class="calibre16"># excluding the non-useful columns from the dataset<br class="title-page-name"/>cust_data&lt;-cust_data[,c(-1,-2)]<br class="title-page-name"/># verifying the dataset post columns deletion<br class="title-page-name"/>dim(cust_data)</pre>
<p class="mce-root">This gives us the following output:</p>
<pre class="calibre16">440 6</pre>
<p class="mce-root">We see that only six columns are retained, confirming that the deletion of non-required columns is successful. From the summary output in the EDA code, we can also observe that the scale across all the retained columns is the same so we do not have to explicitly normalize the data.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">It may be noted that most clustering algorithms involve computation of distance of some form (such as Euclidean, Manhattan, Grower). It is important that data is scaled across the columns in the dataset so as to ensure a variable does not end up as a dominating one in distance computation just because of high scale. In case of different scales observed in columns of the data, we will rely on techniques such as Z-transform or min-max transform. Applying one of these techniques on the data ensures that the columns of the dataset are scaled appropriately therefore leaving no dominating variables in the dataset to be used with clustering algorithms. Fortunately, we do not have this issue so we can continue with the dataset as it is.</p>
<p class="mce-root">Clustering algorithms impose identification of subgroups in the input dataset even if there are no clusters present. To ensure that we get meaningful clusters as output from the clustering algorithms, it is important to check whether clusters exist in the data at all. <strong class="calibre3">Clustering tendency</strong>, or the feasibility of the clustering analysis, is the process of identifying whether the clusters exist in the dataset. Given an input dataset, the process determines whether it has a non-random or non-uniform data structure distribution that will lead to meaningful clusters. The Hopkins statistic measure is used to determine cluster tendency. It takes a value between 0 and 1, and if the value of the Hopkins statistic is close to 0 (far below 0.5), it indicates the existence of valid clusters in the dataset. A Hopkins value closer to 1 indicates random structures in the dataset.</p>
<p class="mce-root">The <kbd class="calibre11">factoextra</kbd> library has a built-in <kbd class="calibre11">get_clust_tendency()</kbd> function that computes the Hopkins statistic on the input dataset. Let's apply this function on our wholesale dataset to determine whether the dataset is valid for clustering at all. The following code accomplishes the computation of the Hopkins statistic:</p>
<pre class="calibre16"># setting the working directory to a folder where dataset is located<br class="title-page-name"/>setwd('/home/sunil/Desktop/chapter5/')<br class="title-page-name"/># reading the dataset to cust_data dataframe<br class="title-page-name"/>cust_data = read.csv(file='Wholesale_customers_ data.csv', header = TRUE)<br class="title-page-name"/># removing the non-required columns<br class="title-page-name"/>cust_data&lt;-cust_data[,c(-1,-2)]<br class="title-page-name"/># inlcuding the facto extra library <br class="title-page-name"/>library(factoextra)<br class="title-page-name"/># computing and printing the hopikins statistic<br class="title-page-name"/>print(get_clust_tendency(cust_data, graph=FALSE,n=50,seed = 123))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">$hopkins_stat<br class="title-page-name"/>[1] 0.06354846</pre>
<p class="mce-root">The Hopkins statistic output for our dataset is very close to 0, so we can conclude that we have a dataset that is a good candidate for our clustering exercise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categories of clustering algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">There are numerous clustering algorithms available off the shelf in R. However, all these algorithms can be grouped into one of two categories:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Flat or partitioning algorithms</strong>: These algorithms rely on an input parameter that defines the number of clusters to be identified in the dataset. The input parameter sometimes comes up as input from business directly or it can be established through certain statistical methods. For example, the <strong class="calibre1">Elbow</strong> method.</li>
<li class="calibre10"><strong class="calibre1">Hierarchical algorithms</strong><span>: In these kinds of algorithms, the clusters are not identified in a single step. They involves multiple steps that run from a single cluster containing all the data points to <em class="calibre22">n</em> clusters containing single data point. Hierarchical algorithms can be further divided into the following two types:</span>
<ul class="calibre38">
<li class="calibre10"><strong class="calibre1">Divisive type</strong><span>: A top-down clustering method where all points are initially assigned to a single cluster. In the next step, the cluster is split into two clusters which are least similar. The process of splitting the clusters is recursively done until each point has its own cluster, for example, the <strong class="calibre1">DIvisive ANAlysis</strong> (<strong class="calibre1">DIANA</strong>) clustering algorithm.</span></li>
<li class="calibre10"><strong class="calibre1">Agglomerative type</strong>: A bottom-up approach where, in the initial run, each point in the dataset is assigned <em class="calibre22">n</em> unique clusters, where <em class="calibre22">n</em> is equal to the number of observations in the dataset. In the next iteration, most similar clusters are merged (based on the distance between the clusters). The recursive process of merging the clusters continues until we are left with just one cluster, for example, <strong class="calibre1">agglomerative nesting</strong> (<strong class="calibre1">AGNES</strong>) algorithm.</li>
</ul>
</li>
</ul>
<p class="mce-root">As discussed earlier, there are numerous clustering algorithms available and we will focus on implementing projects using one algorithm for each type of clustering. We will implement project with k-means that is a flat or partitioning type clustering algorithm. We will then do customer segmentation with DIANA and AGNES, which are divisive and agglomerative, respectively.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying the customer segments in wholesale customer data using k-means clustering</h1>
                </header>
            
            <article>
                
<p class="mce-root">The k-means algorithm is perhaps the most popular and commonly-used clustering method from partitioning clustering type. Though we usually call the clustering algorithm k-means, multiple implementations of this algorithm exist, namely the <strong class="calibre3">MacQueen</strong>, <strong class="calibre3">Lloyd and Forgy</strong>, and <strong class="calibre3">Hartigan-Wong</strong> algorithms. It has been studied and found that the Hartigan-Wong algorithm performs better than the other two algorithms in most situations. K-means in R makes use of the Hartigan-Wong implementation by default.</p>
<p class="mce-root">The k-means algorithm requires the k-value to be passed as a parameter. The parameter indicates the number of clusters to be made with the input data. It is often a challenge for practitioners to determine the optimal k-value. Sometimes, we can go to a business and ask them how many clusters they would expect in the data. The answer from the business directly translates to be the <em class="calibre15">k</em> parameter value to be fed to the algorithm. In most cases though, the business is clueless as to the number of clusters. In such a case, the onus will be on the ML practitioner to determine the k-value. Fortunately, there are several methods available to determine this value. These methods can be classified into the following two categories:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Direct methods</strong>: These methods rely on optimizing a criterion, such as <em class="calibre22">within cluster sums of squares</em> or <em class="calibre22">the average silhouette</em>. Examples of this method include the <strong class="calibre1">V Elbow method</strong> and the <strong class="calibre1">V Silhouette method</strong>.</li>
<li class="calibre10"><strong class="calibre1">Testing methods</strong>: These methods consists of comparing evidence against a null hypothesis. Gap statistic is one popular example of this method.</li>
</ul>
<p class="mce-root">In addition to Elbow, Silhouette, and gap statistic methods, there are more than 30 other indices and methods that have been published for identifying the optimal number of clusters. We will not delve into the theoretical details of these methods, as covering 30 methods in a single chapter is not practical. However, R offers an excellent library function, called <kbd class="calibre11">NbClust</kbd> that makes it easy for us to implement all these methods in one go. The <kbd class="calibre11">NbClust</kbd> function is so powerful that it determines the optimal clusters by varying all combinations of number of clusters, distance measures, and clustering methods and all in one go! Once the library function computes all 30 indices, the <em class="calibre15">majority rule</em> is applied on the output to determine the optimal number of clusters, that is, the k-value to be used as input to the algorithm. Let's implement <kbd class="calibre11">NbClust</kbd> for our wholesale dataset to determine the optimal k-value using the following code block:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<pre class="calibre16"># setting the working directory to a folder where dataset is located<br class="title-page-name"/>setwd('/home/sunil/Desktop/chapter5/')<br class="title-page-name"/># reading the dataset to cust_data dataframe<br class="title-page-name"/>cust_data = read.csv(file='Wholesale_customers_ data.csv', header = TRUE)<br class="title-page-name"/># removing the non-required columns<br class="title-page-name"/>cust_data&lt;-cust_data[,c(-1,-2)]<br class="title-page-name"/># including the NbClust library<br class="title-page-name"/>library(NbClust)<br class="title-page-name"/># Computing the optimal number of clusters through the NbClust function with distance as euclidean and using kmeans <br class="title-page-name"/>NbClust(cust_data,distance="euclidean", method="kmeans")</pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">******************************************************************* <br class="title-page-name"/>* Among all indices: <br class="title-page-name"/>* 1 proposed 2 as the best number of clusters <br class="title-page-name"/>* 11 proposed 3 as the best number of clusters <br class="title-page-name"/>* 2 proposed 4 as the best number of clusters <br class="title-page-name"/>* 1 proposed 5 as the best number of clusters <br class="title-page-name"/>* 4 proposed 8 as the best number of clusters <br class="title-page-name"/>* 1 proposed 10 as the best number of clusters <br class="title-page-name"/>* 1 proposed 12 as the best number of clusters <br class="title-page-name"/>* 1 proposed 14 as the best number of clusters <br class="title-page-name"/>* 1 proposed 15 as the best number of clusters <br class="title-page-name"/>                   ***** Conclusion ***** <br class="title-page-name"/>* According to the majority rule, the best number of clusters is 3 <br class="title-page-name"/>******************************************************************* </pre>
<p class="mce-root">As per the conclusion, we see the k-value that may be used for our problem is <kbd class="calibre11">3</kbd>. Additionally, plotting an elbow curve with the total within-groups sums of squares against the number of clusters in a k-means solution can be helpful in determining the optimal number of clusters. K-means is defined by the objective function, which tries to minimize the sum of all squared distances within a cluster (intra-cluster distance) for all clusters. In the elbow-curve plotting method, we compute the intra-cluster distance with different values of k, and the intra-cluster distance with different k's is plotted as a graph. A bend in the elbow curve suggests the k-value that is optimal for the dataset. The elbow curve can be obtained within R using the following code block:</p>
<pre class="calibre16"># computing the the intra-cluster distance with Ks ranging from 2 to 10<br class="title-page-name"/>library(purrr)<br class="title-page-name"/>tot_withinss &lt;- map_dbl(2:10, function(k){<br class="title-page-name"/>  model &lt;- kmeans(cust_data, centers = k, nstart = 50)<br class="title-page-name"/>  model$tot.withinss<br class="title-page-name"/>})<br class="title-page-name"/># converting the Ks and computed intra-cluster distances to a dataframe<br class="title-page-name"/>screeplot_df &lt;- data.frame(k = 2:10,</pre>
<pre class="calibre16"><br class="title-page-name"/>                           tot_withinss = tot_withinss)<br class="title-page-name"/># plotting the elbow curve<br class="title-page-name"/>library(ggplot2)<br class="title-page-name"/>print( ggplot(screeplot_df, aes(x = k, y = tot_withinss)) + <br class="title-page-name"/>         geom_line() + <br class="title-page-name"/>         scale_x_continuous(breaks = 1:10) + <br class="title-page-name"/>         labs(x = "k", y = "Within Cluster Sum of Squares") + <br class="title-page-name"/>         ggtitle("Total Within Cluster Sum of Squares by # of Clusters (k)") +<br class="title-page-name"/>         geom_point(data = screeplot_df[2,], aes(x = k, y = tot_withinss),<br class="title-page-name"/>                    col = "red2", pch = 4, size = 7))</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter49" src="assets/5f2649b3-bb6f-4598-97e9-646ff9060718.png"/></p>
<p class="mce-root">Even with the elbow curve method output, we see that the number of optimal clusters for our dataset is <kbd class="calibre11">3</kbd>.</p>
<p class="mce-root">We see from the <kbd class="calibre11">NbClust</kbd> function that we have used the Euclidean distance as the distance. There are a number of distance types (<kbd class="calibre11">euclidean</kbd>, <kbd class="calibre11">maximum</kbd>, <kbd class="calibre11">manhattan</kbd>, <kbd class="calibre11">canberra</kbd>, <kbd class="calibre11">binary</kbd>, <kbd class="calibre11">minkowski</kbd>) that we could have used as values for this distance parameter in the <kbd class="calibre11">NbClust</kbd> function. Let's understand what this distance actually means. We are already aware that each observation in our dataset is formed by values that represent features. This essentially means each observation of our dataset can be represented as points in multidimensional space. If we have to say that two observations are similar, we would expect the distance between the two points in the multidimensional space to be lower, that is, both these points in multidimensional space are close to each other. A high distance value between the two points indicates that they are very dissimilar.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">The Euclidean, Manhattan, and other types of distance measures are various ways in which distance can be measured given two points in a multidimensional space. Each of the distance measures involves a specific technique to compute the distance between the two points. The techniques involved in Manhattan and Euclidean, and the difference between their measures, are illustrated in the following screenshot:</p>
<p class="CDPAlignCenter1"><img class="aligncenter50" src="assets/1c1e5f14-ef4e-4c6e-bba1-564f421de849.png"/></p>
<div class="packtfigref">Difference between Manhattan and Euclidean distance measures</div>
<p class="mce-root">The Euclidean distance measures the shortest distance in the plane, whereas the Manhattan metric is the shortest path if one is allowed to move horizontally or vertically.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">For example, if <kbd class="calibre11">a</kbd> and <kbd class="calibre11">b</kbd> are two points where <kbd class="calibre11">a= (0,0)</kbd>, <kbd class="calibre11">b = (3,4)</kbd>, then take a look at the following:</p>
<ul class="calibre9">
<li class="calibre10"><kbd class="calibre11">dist_euclid (a,b) = sqrt(3^2+4^2) = 5</kbd></li>
<li class="calibre10"><kbd class="calibre11">dist_manhattan(a,b) = 3+4 = 7</kbd></li>
<li class="calibre10"><kbd class="calibre11">a=(a1,...,an), b=(b1,...,bn)</kbd> (i<span>n <em class="calibre22">n</em> dimensions and points</span>)</li>
<li class="calibre10"><kbd class="calibre11">dist_euclid (a,b) = sqrt((a1-b1)^2 + ... + (an-bn)^2)</kbd></li>
<li class="calibre10"><kbd class="calibre11">dist_manhattan(a,b) = sum(abs(a1-b1) + ... + abs(an-bn))</kbd></li>
</ul>
<p class="mce-root">Both measure the shortest paths, but the Euclidean metric doesn't have any restrictions while the Manhattan metric only allows paths constant in all but one dimension.</p>
<p class="mce-root">Likewise, the other distance measures also involve a certain unique to measure the similarity between given points. We will not be going through each one of the techniques in detail in this chapter, but the idea to get is that a distance measure basically defines the level of similarity between given observations. It may be noted that a distance measure is not just used in <kbd class="calibre11">NbClust</kbd> but in multiple ML algorithms, including k-means.</p>
<p class="mce-root">Now that we've learned the various ways to identify our k-value and have implemented them to identify the optimal number of clusters for our wholesale dataset, let's implement the k-means algorithm with the following code:</p>
<pre class="calibre16">library(cluster)<br class="title-page-name"/># runing kmeans in cust_data dataset to obtain 3 clusters<br class="title-page-name"/>kmeansout &lt;- kmeans(cust_data, centers = 3, nstart = 50) <br class="title-page-name"/>print (kmeansout)</pre>
<p class="mce-root">This will result in the following output:</p>
<pre class="calibre16">&gt; kmeansout<br class="title-page-name"/>K-means clustering with 3 clusters of sizes 330, 50, 60<br class="title-page-name"/>Cluster means:<br class="title-page-name"/>     Fresh Milk Grocery Frozen Detergents_Paper Delicassen<br class="title-page-name"/>1 8253.47 3824.603 5280.455 2572.661 1773.058 1137.497<br class="title-page-name"/>2 8000.04 18511.420 27573.900 1996.680 12407.360 2252.020<br class="title-page-name"/>3 35941.40 6044.450 6288.617 6713.967 1039.667 3049.467<br class="title-page-name"/>Clustering vector:<br class="title-page-name"/>  [1] 1 1 1 1 3 1 1 1 1 2 1 1 3 1 3 1 1 1 1 1 1 1 3 2 3 1 1 1 2 3 1 1 1 3 1 1 3 1 2 3 3 1 1 2 1 2 2 2 1 2 1 1 3 1<br class="title-page-name"/> [55] 3 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 2 2 3 1 3 1 1 2 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1<br class="title-page-name"/>[109] 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 3 3 1 1 2 1 1 1 3 1 1 1 1 1 2 1 1 1 1 1 1<br class="title-page-name"/>[163] 1 2 1 2 1 1 1 1 1 2 1 2 1 1 3 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 2 2 3 1 1 2 1 1 1 2 1 2 1 1 1 1</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre class="calibre16">[217] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 3 3 1 1 1 1 1 1 1 1 1 2 1 3 1 3 1 1 3 3 1 1 3 1 1 2 2 1 2 1<br class="title-page-name"/>[271] 1 1 1 3 1 1 3 1 1 1 1 1 3 3 3 3 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 2 1 1 2 1 2 1 1 2 1 3 2 1 1 1 1 1 1 2 1 1 1 1<br class="title-page-name"/>[325] 3 3 1 1 1 1 1 2 1 2 1 3 1 1 1 1 1 1 1 2 1 1 1 3 1 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3<br class="title-page-name"/>[379] 1 1 3 1 3 1 2 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 3 3 1 1 3 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 3 1 1 1 1 3 1 1 1 1<br class="title-page-name"/>[433] 1 1 1 3 3 2 1 1</pre>
<p class="mce-root">From the output k-means, we can observe and infer several things about the output clusters. Obviously, three clusters are formed and this is in line with our k parameter that was passed to the algorithm. We see that the first cluster has 330 observations in it, and the second and third clusters are small with just 50 and 60 observations. The k-means output also provides us with the cluster centroids. The <strong class="calibre3">centroid</strong> is a representative of all points in a particular cluster. As it is not feasible to study each of the individual observations assigned to a cluster and determine the business characteristics of the cluster, the cluster centroid may be used as a pseudo for the points in the cluster. The cluster centroid helps us to quickly arrive at a conclusion in terms of the definition of contents of the cluster. The k-means output also produced the cluster assignment for each observation. Each of the observations in our wholesale dataset is assigned to one of the three clusters (1,2,3).</p>
<p class="mce-root">It is possible to view the clustering results by using the <kbd class="calibre11">fviz_cluster()</kbd> function available in the <kbd class="calibre11">factoextra</kbd> library. The function provides a nice illustration of the clusters. If there are more than two dimensions (variables), <kbd class="calibre11">fviz_cluster</kbd> will perform <strong class="calibre3">principal component analysis</strong> (<strong class="calibre3">PCA</strong>) and plot the observations based on the first two principal components that explain the majority of the variance. The clusters visualization can be created though the following code:</p>
<pre class="calibre16">library(factoextra)<br class="title-page-name"/>fviz_cluster(kmout,data=cust_data)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">This will give the following graph as output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter51" src="assets/2fbbbb41-9ac6-4c9b-a97c-f552034ea036.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working mechanics of the k-means algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">The execution of the k-means algorithm involves the following steps:</p>
<ol class="calibre12">
<li class="calibre10">Randomly select <em class="calibre22">k</em> observations from the dataset as the initial cluster centroids.</li>
<li class="calibre10">For each observation in the dataset, perform the following:
<ol class="calibre13">
<li class="calibre10">Compute the distance between the observation and each of the cluster centroids.</li>
<li class="calibre10">Identify the cluster centroid that has minimum distance with the observation.</li>
<li class="calibre10">Assign the observation to such closest centroid.</li>
</ol>
</li>
</ol>
<ol start="3" class="calibre12">
<li class="calibre10">With all points assigned to one of the cluster centroids, compute new cluster centroids. This can be done by taking the mean of all the points assigned to a cluster.</li>
<li class="calibre10">Perform <em class="calibre22">step 2</em> and <em class="calibre22">step 3</em> repeatedly until the cluster centroids (mean) do not change or until a user-defined number of iterations is reached.</li>
</ol>
<p class="mce-root">One key thing to note in k-means is that the cluster centroids in the initial step are selected randomly and the initial cluster assignments are done based on the distance between the actual observations and the randomly-picked cluster centroids. This essentially means that if we were to pick observations as cluster centroids in the initial step other than the observations that were chosen, we would obtain different clusters than the one we have obtained. In technical terms, this is called a <strong class="calibre3">non-globally-optimal solution</strong> or a <strong class="calibre3">locally-optimal solution</strong>. The <kbd class="calibre11">cluster</kbd> library's k-means function has the <kbd class="calibre11">nstart</kbd> option, which works around this problem of the non-globally-optimal solution encountered with the k-means algorithm.</p>
<p class="mce-root">The <kbd class="calibre11">nstart</kbd> option enables the algorithm to try several random starts (instead of just one) by drawing a number of center observations from the datasets. It then checks the cluster sum of squares and proceeds with the best start, resulting in a more stable output. In our case, we set the <kbd class="calibre11">nstart</kbd> value as <kbd class="calibre11">50</kbd>, therefore the best start is chosen by k-means post checking it with 50 random initial sets of cluster centroids. <span class="calibre4">The following diagram depicts the high-level steps involved in the k-means clustering algorithm:</span></p>
<p class="CDPAlignCenter1"><img class="aligncenter52" src="assets/4fe651c3-aef1-42f3-902d-cc36a6222eb8.png"/></p>
<div class="packtfigref">Steps in k-means clustering</div>
<p class="mce-root">In supervised <span class="calibre4">ML</span> methods, such as classification, we have ground truth, therefore, we will be able to able to compare our predictions with the ground truth and measure to report the performance of our classification. Unlike the supervised ML method, in clustering, we do not have any ground truth. Therefore, computing the performance measurement with respect to clustering is a challenge.</p>
<p class="mce-root">As an alternative to performance measurement, we use a pseudo-measure called <strong class="calibre3">cluster quality</strong>. The cluster quality is generally computed through measures known as intra-cluster distance and inter-cluster distance, which are illustrated in the following diagram:</p>
<p class="CDPAlignCenter1"><img class="aligncenter53" src="assets/b460f049-6e7f-4fec-89d4-d2b37af5d12b.png"/></p>
<div class="packtfigref">Intra-cluster distance and inter-cluster distance defined</div>
<p class="mce-root">The goal of the clustering task is to obtain good-quality clusters. Clusters are termed as <strong class="calibre3">high-quality clusters</strong> if the distance within the observations is minimum and the distance between the clusters themselves is maximum.</p>
<p class="mce-root">There are multiple ways inter-cluster and intra-cluster distances can be measured:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Intra-cluster</strong>: This distance can be measured as (sum, minimum, maximum, or mean) of the (absolute/squared) distance between all pairs of points in the cluster (or) <em class="calibre22">diameter</em><span>–</span>two farthest points (or) between the centroid and all points in the cluster.</li>
<li class="calibre10"><strong class="calibre1">Inter-cluster</strong>: This distance is measured as sum of the (squared) distance between all pairs of clusters, where distance between two clusters itself is computed as one of the following:
<ul class="calibre38">
<li class="calibre10">Distance between their centroids</li>
<li class="calibre10">Distance between farthest pair of points</li>
<li class="calibre10">Distance between the closest pair of points belonging to the clusters</li>
</ul>
</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Unfortunately, there is no way to pinpoint the preferred inter-cluster distance and intra-cluster distance values. The <strong class="calibre3">Silhouette index</strong> is one metric that is based on inter-cluster distance and intra-cluster distance that can be readily computed and easily interpreted.</p>
<p class="mce-root">The Silhouette index is computed using the mean intra-cluster distance, <em class="calibre15">a</em>, and the mean nearest-cluster distance, <em class="calibre15">b</em>, for each of the observations participating in the clustering exercise. The Silhouette index for an observation is given by the following formula:</p>
<p class="CDPAlignCenter1"><img class="fm-editor-equation6" src="assets/e22c4857-0e38-4471-8e3b-5902a1a357e5.png"/></p>
<p class="CDPAlignLeft1">Here, <em class="calibre15">b</em> is the distance between an observation and the nearest cluster that the observation is not a part of.</p>
<p class="mce-root">Silhouette index value ranges between [-1, 1]. A value of +1 for an observation indicates that the observation is far away from its neighboring cluster and it is very close to the cluster it is assigned to. Similarly, a value of -1 tells us that the observation is close to its neighboring cluster than to the cluster it is assigned to. A value of 0 means it's at the boundary of the distance between the two clusters. A value of +1 is ideal and -1 is the least preferred. Hence, the higher the value, the better the quality of the cluster.</p>
<p class="mce-root">The <kbd class="calibre11">cluster</kbd> library offers the Silhouette function, which can be readily used on our k-means clustering output to understand the quality of the clusters that were formed. The following code computes the Silhouette index for our three clusters:</p>
<pre class="calibre16"># computing the silhouette index for the clusters<br class="title-page-name"/>si &lt;- silhouette(kmout$cluster, dist(cust_data, "euclidean"))<br class="title-page-name"/># printing the summary of the computed silhouette index <br class="title-page-name"/>print(summary(si))</pre>
<p class="mce-root">This will give us the following output:</p>
<pre class="calibre16">Silhouette of 440 units in 3 clusters from silhouette.default(x = kmout$cluster, dist = dist(cust_data, from "euclidean")) :<br class="title-page-name"/> Cluster sizes and average silhouette widths:<br class="title-page-name"/>       60 50 330 <br class="title-page-name"/>0.2524346 0.1800059 0.5646307 <br class="title-page-name"/>Individual silhouette widths:<br class="title-page-name"/>   Min. 1st Qu. Median Mean 3rd Qu. Max. <br class="title-page-name"/>-0.1544 0.3338 0.5320 0.4784 0.6743 0.7329 </pre>
<p class="mce-root">As we have seen, the Silhouette index can range from -1 to +1, and the latter is preferred. From the output, we see that the clusters are all good quality clusters, as the average width is a positive number closer to 1 than -1.</p>
<p class="mce-root"/>
<p class="mce-root">In fact, the Silhouette index can be used not just to measure the quality of clusters formed but also to compute the k-value. Similar to Elbow method, we can iterate through multiple values of k and then identify the k that yields the maximum Silhouette index values across the clusters. Clustering can then be performed using the k that was identified.</p>
<p class="mce-root">There are numerous cluster-quality measures described in the literature. The Silhouette index is just one measure we covered in this chapter because of its popularity in the <span class="calibre4">ML</span> community. The <kbd class="calibre11">clusterCrit</kbd> library offers a wide range of indices to measure the quality of clusters. We are not going to explore the other cluster-quality metrics here, but interested readers should refer to this library for further information on how to compute cluster quality.</p>
<p class="mce-root">So far, we have covered the k-means clustering algorithm to identify the clusters, but the original segmentation task we started with does not end here. Segmentation further spans to the task of understanding what each of the clusters formed from the clustering exercise mean to businesses. For example, we take our cluster centroids obtained from k-means and an attempt is made to identify what these are:</p>
<pre class="calibre16">Fresh Milk Grocery Frozen Detergents_Paper Delicatessen<br class="title-page-name"/>1 8253.47 3824.603 5280.455 2572.661 1773.058 1137.497<br class="title-page-name"/>2 8000.04 18511.420 27573.900 1996.680 12407.360 2252.020<br class="title-page-name"/>3 35941.40 6044.450 6288.617 6713.967 1039.667 3049.467</pre>
<p class="mce-root">Here are some sample insights into each cluster:</p>
<ul class="calibre9">
<li class="calibre10">Cluster 1 is low spenders (average spending: 22,841.744), with the majority of spending allocated to the fresh category</li>
<li class="calibre10">Cluster 2 is high spenders (average spending: 70,741.42), with the majority of spending in the grocery category</li>
<li class="calibre10">Cluster 3 is medium spenders (average spending : 59,077.568), with the majority of spending in the fresh category</li>
</ul>
<p class="mce-root">Now, based on the business objective, one or more clusters may be selected to target. For example, if the objective is to have high spenders spend more, promotions may be rolled out to cluster 2 individuals with spending in the <kbd class="calibre11">Frozen</kbd> and <kbd class="calibre11">Delicatessen</kbd> products less than the centroid values (that is, <kbd class="calibre11">Frozen</kbd>: <kbd class="calibre11">1,996.680</kbd> and <kbd class="calibre11">Delicatessen</kbd>: <kbd class="calibre11">2,252.020</kbd>).</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying the customer segments in the wholesale customer data using DIANA</h1>
                </header>
            
            <article>
                
<p class="mce-root">Hierarchical clustering algorithms are a good choice when we don't necessarily have circular (or hyperspherical) clusters in the data, and we essentially don't know the number of clusters in advance. With hierarchical clustering algorithm, unlike the flat or partitioning algorithms, there is no requirement to decide and pass the number of clusters to be formed prior to applying the algorithm on the dataset.</p>
<p class="mce-root">Hierarchical clustering results in a dendogram (tree diagram) that can be visually verified to easily determine the number of clusters. Visual verification enables us to perform cuts in the dendrogram at suitable places.</p>
<p class="mce-root">The results produced by this type of clustering algorithm are reproducible as the algorithm is not sensitive to the choice of the distance metric. In other words, irrespective of the distance metric chosen, we will get the same results. This type of clustering is also suitable for datasets of a higher complexity (quadratic) and in particular for exploring the hierarchical relationships that exist between the clusters.</p>
<p class="mce-root">Divisive hierarchical clustering, also known as <strong class="calibre3">DIvisive ANAlysis</strong> (<strong class="calibre3">DIANA</strong>), is a hierarchical clustering algorithm that follows a top-down approach to identify clusters in a given dataset. Here are the steps in DIANA to identify the clusters:</p>
<ol class="calibre12">
<li class="calibre10">All observations of the dataset are assigned to the root, so in the initial step only a single cluster is formed.</li>
<li class="calibre10">In each iteration, the most heterogeneous cluster is partitioned into two.</li>
<li class="calibre10"><strong class="calibre1">Step 2</strong> is repeated until all the observations are in their own cluster:</li>
</ol>
<p class="CDPAlignCenter1"><img class="aligncenter54" src="assets/b267fbb4-0e11-429f-9534-8ac5cc8b72ce.png"/></p>
<div class="packtfigref">Working of divisive hierarchical clustering algorithm</div>
<p class="mce-root"/>
<p class="mce-root">One obvious question that comes up is about the technique used by the algorithm to split the cluster into two. The answer is that it is performed according to some (dis)similarity measure. The Euclidean distance is used to measure the distance between two given points. This algorithm works by splitting the data on the basis of the farthest-distance measure of all the pairwise distances between the data points. Linkage defines the specific details of fartherness of the data points. The next figure illustrates the various linkages considered by DIANA for splitting the clusters. Here are some of the distances considered to split the groups:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">Single-link</strong>: Nearest distance or single linkage</li>
<li class="calibre10"><strong class="calibre1">Complete-link</strong>: Farthest distance or complete linkage</li>
<li class="calibre10"><strong class="calibre1">Average-link</strong>: Average distance or average linkage</li>
<li class="calibre10"><strong class="calibre1">Centroid-link</strong>: Centroid distance</li>
<li class="calibre10"><strong class="calibre1">Ward's method</strong>: Sum of squared <kbd class="calibre11">euclidean</kbd> distance is minimized</li>
</ul>
<p class="mce-root">Take a look at the following diagram to better understand the preceding distances:</p>
<p class="CDPAlignCenter1"><img class="aligncenter55" src="assets/4f0d00c3-798b-4422-9ab9-c5ab5cad4866.png"/></p>
<div class="packtfigref">Illustration depicting various linkage types used by DIANA</div>
<p class="mce-root">Generally, the linkage type to be used is passed as a parameter to the clustering algorithm. The <kbd class="calibre11">cluster</kbd> library offers the <kbd class="calibre11">diana</kbd> function to perform clustering. Let's apply it on our wholesale dataset with the following code:</p>
<pre class="calibre16"># setting the working directory to a folder where dataset is located<br class="title-page-name"/>setwd('/home/sunil/Desktop/chapter5/')<br class="title-page-name"/># reading the dataset to cust_data dataframe<br class="title-page-name"/>cust_data = read.csv(file='Wholesale_customers_ data.csv', header = TRUE)<br class="title-page-name"/># removing the non-required columns<br class="title-page-name"/>cust_data&lt;-cust_data[,c(-1,-2)]<br class="title-page-name"/># including the cluster library so as to make use of diana function<br class="title-page-name"/>library(cluster)<br class="title-page-name"/># Compute diana()<br class="title-page-name"/>cust_data_diana&lt;-diana(cust_data, metric = "euclidean",stand = FALSE)<br class="title-page-name"/># plotting the dendogram from diana output<br class="title-page-name"/>pltree(cust_data_diana, cex = 0.6, hang = -1,<br class="title-page-name"/>       main = "Dendrogram of diana")<br class="title-page-name"/># Divise coefficient; amount of clustering structure found<br class="title-page-name"/>print(cust_data_diana$dc)</pre>
<p class="mce-root">This will give us the following output:</p>
<pre class="calibre16">&gt; print(cust_data_diana$dc)<br class="title-page-name"/>[1] 0.9633628</pre>
<p class="mce-root">Take a look at the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter56" src="assets/ccfc0e26-aee4-4f83-b19d-16d56310d220.png"/></p>
<p class="mce-root"/>
<p class="mce-root">The <kbd class="calibre11">plot.hclust()</kbd> and <kbd class="calibre11">plot.dendrogram()</kbd> functions may also be used on the DIANA clustering output. <kbd class="calibre11">plot.dendrogram()</kbd> yields the dendogram that follows the natural structure of the splits as done by the DIANA algorithm. Use the following code to generate the dendrogram:</p>
<pre class="calibre16">plot(as.dendrogram(cust_data_diana), cex = 0.6,horiz = TRUE)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter57" src="assets/69e2660f-7ac4-44e3-a13c-640f2584e12b.png"/></p>
<p class="mce-root">In the dendrogram output, each leaf that appears on the right relates to each observation in the dataset. As we traverse from right to left, observations that are similar to each other are grouped into one branch, which are themselves fused at a higher level.</p>
<p class="mce-root">The higher level of the fusion, provided on the horizontal axis, indicates the similarity between two observations. The higher the fusion, the more similar the observations are. It may be noted that conclusions about the proximity of two observations can be drawn only based on the level where branches containing those two observations are first fused. In order to identify clusters, we can cut the dendrogram at a certain level. The level at which the cut is made defines the number of clusters obtained.</p>
<p class="mce-root">We can make use of the <kbd class="calibre11">cutree()</kbd> function to obtain the cluster assignment for each of the observations in our dataset. Execute the following code to obtain the clusters and review the clustering output:</p>
<pre class="calibre16"># obtain the clusters through cuttree<br class="title-page-name"/># Cut tree into 3 groups<br class="title-page-name"/>grp &lt;- cutree(cust_data_diana, k = 3)<br class="title-page-name"/># Number of members in each cluster<br class="title-page-name"/>table(grp)<br class="title-page-name"/># Get the observations of cluster 1<br class="title-page-name"/>rownames(cust_data)[grp == 1]</pre>
<p class="mce-root">This will give the following output:</p>
<pre class="calibre16">&gt; table(grp)<br class="title-page-name"/>grp<br class="title-page-name"/>  1 2 3 <br class="title-page-name"/>364 44 32 <br class="title-page-name"/>&gt; rownames(cust_data)[grp == 1]<br class="title-page-name"/>  [1] "1" "2" "3" "4" "5" "6" "7" "8" "9" "11" "12" "13" "14" "15" "16"</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre class="calibre16">"17" "18" "19" <br class="title-page-name"/> [19] "20" "21" "22" "25" "26" "27" "28" "31" "32" "33" "34" "35" "36" "37" "38" "41" "42" "43" <br class="title-page-name"/> [37] "45" "49" "51" "52" "54" "55" "56" "58" "59" "60" "61" "63" "64" "65" "67" "68" "69" "70" <br class="title-page-name"/> [55] "71" "72" "73" "74" "75" "76" "77" "79" "80" "81" "82" "83" "84" "85" "89" "90" "91" "92" <br class="title-page-name"/> [73] "94" "95" "96" "97" "98" "99" "100" "101" "102" "103" "105" "106" "107" "108" "109" "111" "112" "113"<br class="title-page-name"/> [91] "114" "115" "116" "117" "118" "119" "120" "121" "122" "123" "124" "127" "128" "129" "131" "132" "133" "134"<br class="title-page-name"/>[109] "135" "136" "137" "138" "139" "140" "141" "142" "144" "145" "147" "148" "149" "151" "152" "153" "154" "155"<br class="title-page-name"/>[127] "157" "158" "159" "160" "161" "162" "163" "165" "167" "168" "169" "170" "171" "173" "175" "176" "178" "179"<br class="title-page-name"/>[145] "180" "181" "183" "185" "186" "187" "188" "189" "190" "191" "192" "193" "194" "195" "196" "198" "199" "200"<br class="title-page-name"/>[163] "203" "204" "205" "207" "208" "209" "211" "213" "214" "215" "216" "218" "219" "220" "221" "222" "223" "224"<br class="title-page-name"/>[181] "225" "226" "227" "228" "229" "230" "231" "232" "233" "234" "235" "236" "237" "238" "239" "241" "242" "243"<br class="title-page-name"/>[199] "244" "245" "246" "247" "248" "249" "250" "251" "253" "254" "255" "257" "258" "261" "262" "263" "264" "265"<br class="title-page-name"/>[217] "266" "268" "269" "270" "271" "272" "273" "275" "276" "277" "278" "279" "280" "281" "282" "284" "287" "288"<br class="title-page-name"/>[235] "289" "291" "292" "293" "294" "295" "296" "297" "298" "299" "300" "301" "303" "304" "306" "308" "309" "311"<br class="title-page-name"/>[253] "312" "314" "315" "316" "317" "318" "319" "321" "322" "323" "324" "325" "327" "328" "329" "330" "331" "333"<br class="title-page-name"/>[271] "335" "336" "337" "338" "339" "340" "341" "342" "343" "345" "346" "347" "348" "349" "351" "353" "355" "356"<br class="title-page-name"/>[289] "357" "358" "359" "360" "361" "362" "363" "364" "365" "366" "367" "368" "369" "370" "372" "373" "374" "375"<br class="title-page-name"/>[307] "376" "377" "379" "380" "381" "382" "384" "385" "386" "387" "388" "389" "390" "391" "392" "393" "394" "395"<br class="title-page-name"/>[325] "396" "397" "398" "399" "400" "401" "402" "403" "404" "405" "406" "407" "409" "410" "411" "412" "413" "414"<br class="title-page-name"/>[343] "415" "416" "417" "418" "420" "421" "422" "423" "424" "425" "426" "427" "429" "430" "431" "432" "433" "434"<br class="title-page-name"/>[361] "435" "436" "439" "440"</pre>
<p class="mce-root">We can also visualize the clustering output through the <kbd class="calibre11">fviz_cluster</kbd> function in the <kbd class="calibre11">factoextra</kbd> library. Use the following code to get the required visualization:</p>
<pre class="calibre16">library(factoextra)<br class="title-page-name"/>fviz_cluster(list(data = cust_data, cluster = grp))</pre>
<p class="mce-root">This will give you the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter58" src="assets/a5b6f638-67e4-46b8-ac3a-e1a4603ef222.png"/></p>
<p class="mce-root">It is also possible to color-code the clusters within the dendogram itself. This can be accomplished with the following code:</p>
<pre class="calibre16">plot(as.hclust(cust_data_diana))<br class="title-page-name"/>rect.hclust(cust_data_diana, k = 4, border = 2:5)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter59" src="assets/61cd4544-de37-4978-8a15-ff63d74d7ae6.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Now that the clusters are identified, the steps we discussed to evaluate the cluster quality (through the Silhouette index) apply here as well. As we have already covered this topic under the k-means clustering algorithm, we are not going to repeat the steps here. The code and interpretation of the output remains the same as what was discussed under k-means.</p>
<p class="mce-root">As discussed earlier, the cluster's output is not the final point to customer segmentation exercise we have on hand. Similar to the discussion we had on under the k-means algorithm, we could analyze the DIANA cluster output to identify meaningful segments so as to roll out business objectives to those specifically-identified segments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying the customer segments in the wholesale customers data using AGNES</h1>
                </header>
            
            <article>
                
<p class="mce-root">AGNES is the reverse of DIANA in the sense that it follows a bottom-up approach to clustering the dataset. The following diagram illustrates the working principle of the AGNES algorithm for clustering:</p>
<p class="CDPAlignCenter1"><img class="aligncenter60" src="assets/56080695-a543-418e-ae0c-0429532fa2b6.png"/></p>
<div class="packtfigref">Working of agglomerative hierarchical clustering algorithm</div>
<p class="mce-root">Except for the bottom-up approach followed by AGNES, the implementation details behind the algorithm are the same as for DIANA; therefore, we won't repeat the discussion of the concepts here. The following code block clusters our wholesale dataset into three clusters with AGNES; it also creates a visualization of the clusters thus formed:</p>
<pre class="calibre16"># setting the working directory to a folder where dataset is located<br class="title-page-name"/>setwd('/home/sunil/Desktop/chapter5/')<br class="title-page-name"/># reading the dataset to cust_data dataframe<br class="title-page-name"/>cust_data = read.csv(file='Wholesale_customers_ data.csv', header = TRUE)<br class="title-page-name"/># removing the non-required columns<br class="title-page-name"/>cust_data&lt;-cust_data[,c(-1,-2)]<br class="title-page-name"/># including the cluster library so as to make use of agnes function<br class="title-page-name"/>library(cluster)<br class="title-page-name"/># Compute agnes()<br class="title-page-name"/>cust_data_agnes&lt;-agnes(cust_data, metric = "euclidean",stand = FALSE)<br class="title-page-name"/># plotting the dendogram from agnes output<br class="title-page-name"/>pltree(cust_data_agnes, cex = 0.6, hang = -1,<br class="title-page-name"/>       main = "Dendrogram of agnes")<br class="title-page-name"/># agglomerative coefficient; amount of clustering structure found<br class="title-page-name"/>print(cust_data_agnes$ac)<br class="title-page-name"/>plot(as.dendrogram(cust_data_agnes), cex = 0.6,horiz = TRUE)<br class="title-page-name"/># obtain the clusters through cuttree<br class="title-page-name"/># Cut tree into 3 groups<br class="title-page-name"/>grp &lt;- cutree(cust_data_agnes, k = 3)<br class="title-page-name"/># Number of members in each cluster<br class="title-page-name"/>table(grp)<br class="title-page-name"/># Get the observations of cluster 1<br class="title-page-name"/>rownames(cust_data)[grp == 1]<br class="title-page-name"/># visualization of clusters<br class="title-page-name"/>library(factoextra)<br class="title-page-name"/>fviz_cluster(list(data = cust_data, cluster = grp))<br class="title-page-name"/>library(factoextra)<br class="title-page-name"/>fviz_cluster(list(data = cust_data, cluster = grp))<br class="title-page-name"/>plot(as.hclust(cust_data_agnes))<br class="title-page-name"/>rect.hclust(cust_data_agnes, k = 3, border = 2:5)</pre>
<p class="mce-root">This is the output that you will obtain:</p>
<pre class="calibre16">[1] 0.9602911<br class="title-page-name"/>&gt; plot(as.dendrogram(cust_data_agnes), cex = 0.6,horiz = FALSE)</pre>
<p class="mce-root">Take a look at the following screenshot:</p>
<p class="CDPAlignCenter1"><img class="aligncenter61" src="assets/809fb5ed-f69d-40e4-9f90-966bb3953734.png"/></p>
<p class="mce-root">Take a look at the following code block:</p>
<pre class="calibre16">&gt; grp &lt;- cutree(cust_data_agnes, k = 3)<br class="title-page-name"/>&gt; # Number of members in each cluster<br class="title-page-name"/>&gt; table(grp)<br class="title-page-name"/>grp<br class="title-page-name"/>  1 2 3<br class="title-page-name"/>434 5 1 <br class="title-page-name"/>&gt; rownames(cust_data)[grp == 1]<br class="title-page-name"/>  [1] "1" "2" "3" "4" "5" "6" "7" "8" "9" "10" "11" "12" "13" "14" "15" "16" "17" "18" <br class="title-page-name"/> [19] "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30" "31" "32" "33" "34" "35" "36" <br class="title-page-name"/> [37] "37" "38" "39" "40" "41" "42" "43" "44" "45" "46" "47" "49" "50" "51" "52" "53" "54" "55" <br class="title-page-name"/> [55] "56" "57" "58" "59" "60" "61" "63" "64" "65" "66" "67" "68" "69" "70" "71" "72" "73" "74" <br class="title-page-name"/> [73] "75" "76" "77" "78" "79" "80" "81" "82" "83" "84" "85" "88" "89" "90" "91" "92" "93" "94" <br class="title-page-name"/> [91] "95" "96" "97" "98" "99" "100" "101" "102" "103" "104" "105" "106" "107" "108" "109" "110" "111" "112"<br class="title-page-name"/>[109] "113" "114" "115" "116" "117" "118" "119" "120" "121" "122" "123" "124" "125" "126" "127" "128" "129" "130"<br class="title-page-name"/>[127] "131" "132" "133" "134" "135" "136" "137" "138" "139" "140" "141" "142" "143" "144" "145" "146" "147" "148"<br class="title-page-name"/>[145] "149" "150" "151" "152" "153" "154" "155" "156" "157" "158" "159" "160" "161" "162" "163" "164" "165" "166"<br class="title-page-name"/>[163] "167" "168" "169" "170" "171" "172" "173" "174" "175" "176" "177" "178" "179" "180" "181" "183" "184" "185"<br class="title-page-name"/>[181] "186" "187" "188" "189" "190" "191" "192" "193" "194" "195" "196" "197" "198" "199" "200" "201" "202" "203"</pre>
<pre class="calibre16">[199] "204" "205" "206" "207" "208" "209" "210" "211" "212" "213" "214" "215" "216" "217" "218" "219" "220" "221"<br class="title-page-name"/>[217] "222" "223" "224" "225" "226" "227" "228" "229" "230" "231" "232" "233" "234" "235" "236" "237" "238" "239"<br class="title-page-name"/>[235] "240" "241" "242" "243" "244" "245" "246" "247" "248" "249" "250" "251" "252" "253" "254" "255" "256" "257"<br class="title-page-name"/>[253] "258" "259" "260" "261" "262" "263" "264" "265" "266" "267" "268" "269" "270" "271" "272" "273" "274" "275"<br class="title-page-name"/>[271] "276" "277" "278" "279" "280" "281" "282" "283" "284" "285" "286" "287" "288" "289" "290" "291" "292" "293"<br class="title-page-name"/>[289] "294" "295" "296" "297" "298" "299" "300" "301" "302" "303" "304" "305" "306" "307" "308" "309" "310" "311"<br class="title-page-name"/>[307] "312" "313" "314" "315" "316" "317" "318" "319" "320" "321" "322" "323" "324" "325" "326" "327" "328" "329"<br class="title-page-name"/>[325] "330" "331" "332" "333" "335" "336" "337" "338" "339" "340" "341" "342" "343" "344" "345" "346" "347" "348"<br class="title-page-name"/>[343] "349" "350" "351" "352" "353" "354" "355" "356" "357" "358" "359" "360" "361" "362" "363" "364" "365" "366"<br class="title-page-name"/>[361] "367" "368" "369" "370" "371" "372" "373" "374" "375" "376" "377" "378" "379" "380" "381" "382" "383" "384"<br class="title-page-name"/>[379] "385" "386" "387" "388" "389" "390" "391" "392" "393" "394" "395" "396" "397" "398" "399" "400" "401" "402"<br class="title-page-name"/>[397] "403" "404" "405" "406" "407" "408" "409" "410" "411" "412" "413" "414" "415" "416" "417" "418" "419" "420"<br class="title-page-name"/>[415] "421" "422" "423" "424" "425" "426" "427" "428" "429" "430" "431" "432" "433" "434" "435" "436" "437" "438"<br class="title-page-name"/>[433] "439" "440"</pre>
<p class="mce-root">Execute the following command:</p>
<pre class="calibre16">&gt; fviz_cluster(list(data = cust_data, cluster = grp))</pre>
<p class="mce-root">The preceding command generates the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter62" src="assets/f8337198-628a-46bc-a3fe-3a3ab237d047.png"/></p>
<p class="mce-root">Take a look at the following commands:</p>
<pre class="calibre16">&gt; plot(as.hclust(cust_data_agnes))<br class="title-page-name"/>&gt; rect.hclust(cust_data_agnes, k = 3, border = 2:5)</pre>
<p class="mce-root">The preceding commands generate the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter63" src="assets/6abc2b7f-b61c-40c4-8c72-7606a16b01f8.png"/></p>
<p class="mce-root">We can see from the AGNES clustering output that a large number of observations from the dataset are assigned to one cluster and very few observations were assigned to the other clusters. This is not a great output for our segmentation downstream exercise. To obtain better cluster assignments, you could try using other cluster-linkage methods aside from the default average linkage method currently used by the AGNES algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we learned about the concept of segmentation and its association with clustering, an ML unsupervised learning technique. We made use of the wholesale dataset available from the UCI repository and implemented clustering using the k-means, DIANA, and AGNES algorithms. During the course of this chapter, we also studied various aspects related to clustering, such as tendency to cluster, distance, linkage measures, and methods to identify the right number of clusters, and measuring the output of clustering. We also explored making use of the clustering output for customer-segmentation purposes.</p>
<p class="mce-root">Can computers see and identify objects and living creatures like humans do? Let's explore the answer to this question in the next chapter.</p>


            </article>

            
        </section>
    </body></html>