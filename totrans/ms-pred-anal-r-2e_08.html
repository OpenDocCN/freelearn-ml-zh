<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction"><div class="book" id="21PMQ2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Dimensionality Reduction</h1></div></div></div><p class="calibre8">Building a useful predictive model requires analyzing an appropriate number of observations (or cases). This number will vary, based upon your project or your objective. Strictly speaking, the more <span class="strong"><em class="calibre9">variations</em></span> (not necessarily the more <span class="strong"><em class="calibre9">data</em></span>) analyzed, the better the outcome or results of the model.</p><p class="calibre8">This chapter will discuss the concept of reducing the size or amount of the data being observed without affecting the outcome of the analysis (or the success of the project), through various common approaches such as <span class="strong"><em class="calibre9">correlation analysis</em></span>, <span class="strong"><em class="calibre9">principal component analysis</em></span>, <span class="strong"><em class="calibre9">independent component analysis</em></span>, <span class="strong"><em class="calibre9">common factor analysis</em></span>, and <span class="strong"><em class="calibre9">non-negative matrix factorization</em></span>.</p><p class="calibre8">Let us begin by clarifying what is meant by dimensional reduction.</p></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec62" class="calibre1"/>Defining DR</h1></div></div></div><p class="calibre8">It is a <a id="id616" class="calibre1"/>most commonly accepted rule of thumb that it is difficult to understand or visualize data represented in or by more than three dimensions.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Dimensional</strong></span> (-<span class="strong"><strong class="calibre2">ity</strong></span>) <span class="strong"><strong class="calibre2">reduction</strong></span> is the process of attempting to reduce the number of random variables (or data dimensions) under statistical consideration, or perhaps better put: finding a <a id="id617" class="calibre1"/>lower-dimensional representation of the feature-set that is of interest.</p><p class="calibre8">This allows the data scientist to:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Avoid what is referred to as the curse of dimensionality<div class="note" title="Note"><h3 class="title2"><a id="note25" class="calibre1"/>Note</h3><p class="calibre8">The curse of dimensionality refers to a phenomenon that arises when attempting to analyze data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings or everyday experience.</p></div></li><li class="listitem">Reduce the amount of time and memory required for the proper analysis of the data</li><li class="listitem">Allow the data to be more easily visualized</li><li class="listitem">Eliminate features irrelevant to the model's purpose</li><li class="listitem">Reduce model noise</li></ul></div><p class="calibre8">A useful (albeit perhaps over-used) conceptual example of data dimensional reduction is the case <a id="id618" class="calibre1"/>of a computer-generated face or faces or an image of a single human face, which is in fact made up of thousands of images of individual human faces. If we consider the attributes of each individual face, the data may become overwhelming; however, if we reduce the dimensionality of all those images into several principal components (eyes, nose, lips, and so on.), the data becomes somewhat more manageable.</p><p class="calibre8">The following sections outline some of the most common methods and strategies for dimension reduction.</p></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Correlated data analyses"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec65" class="calibre1"/>Correlated data analyses</h2></div></div></div><p class="calibre8">It is <a id="id619" class="calibre1"/>typical to think of the terms <span class="strong"><strong class="calibre2">dependence</strong></span> and <span class="strong"><strong class="calibre2">association</strong></span> as having the same meeting. These are used to qualify a relationship <a id="id620" class="calibre1"/>between two (or more) random variables or bivariate data.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note26" class="calibre1"/>Note</h3><p class="calibre8">A random variable is a variable quantity whose value depends on possible outcomes; bivariate data is data with two variables who may or may not have an exposed relationship.</p></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Correlated data</strong></span>, or data <a id="id621" class="calibre1"/>with correlation, describes a type of (typically linear) statistical relationship. A popular example of a correlation is product pricing, such as when the popularity <a id="id622" class="calibre1"/>of a product drives the manufacturer's pricing strategies.</p><p class="calibre8">Identifying correlations can be very useful as they can be a <span class="strong"><em class="calibre9">predictive relationship</em></span> that can be exploited or used to reduce dimensionality within a population or data file.</p><p class="calibre8">Common examples of correlation and predictive relationships typically involve the weather, but another idea might be found at <a class="calibre1" href="http://www.nfl.com/">http://www.nfl.com/</a>. If you are familiar with the national football league and have visited the site, then you'll know that each NFL team sells team-embolized merchandise and a team that's earned a winning season will most likely have higher product sales that year.</p><p class="calibre8">In this example, there is a <span class="strong"><em class="calibre9">causal relationship</em></span>, because a team's winning season <span class="strong"><em class="calibre9">causes</em></span> its fans to <a id="id623" class="calibre1"/>purchase more of their team merchandise. However, in general, the presence of a <span class="strong"><em class="calibre9">correlation</em></span> is not sufficient to infer the presence of (even) a <span class="strong"><em class="calibre9">causal relationship</em></span> (there will be more on this later in this chapter).</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Scatterplots"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec66" class="calibre1"/>Scatterplots</h2></div></div></div><p class="calibre8">As an <a id="id624" class="calibre1"/>aside, a <span class="strong"><em class="calibre9">scatterplot</em></span> is often used to graphically represent the relationship between two variables and therefore is a great choice for visualizing correlated data.</p><p class="calibre8">Using the R <code class="email">plot</code> function, you can easily generate a pretty nice visual of our winning team example, shown as follows:</p><div class="mediaobject"><img src="../images/00147.jpeg" alt="Scatterplots" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note27" class="calibre1"/>Note</h3><p class="calibre8">As other visualization options, boxplots and violin plots can also be used.</p></div><p class="calibre8">As mentioned in the preceding graph, <span class="strong"><em class="calibre9">correlation</em></span> describes or measures the level or extent to which two or more variables fluctuate <span class="strong"><em class="calibre9">together</em></span> (in the preceding example, <span class="strong"><strong class="calibre2">Games Won</strong></span> and <span class="strong"><strong class="calibre2">Merchandise Sold</strong></span>).</p><p class="calibre8">This <a id="id625" class="calibre1"/>measurement can be categorized as positive or negative, with a <span class="strong"><em class="calibre9">positive correlation</em></span> showing the extent to which those variables increase or decrease in parallel, and a <span class="strong"><em class="calibre9">negative correlation</em></span> showing the extent to which one variable increases as the other decreases.</p><p class="calibre8">A <span class="strong"><strong class="calibre2">correlation coefficient</strong></span> is a statistical measure of the degree to which changes in the value of one variable will or can predict change to the value of another variable.</p><p class="calibre8">When <a id="id626" class="calibre1"/>the variation of one variable <span class="strong"><em class="calibre9">reliably predicts</em></span> a similar variation in another variable, there's often a predisposition to think that this means that the change in one causes the change in the other. However, <span class="strong"><em class="calibre9">correlation</em></span> does not imply <span class="strong"><em class="calibre9">causation</em></span>. [There may be, for example, an unknown factor that influences both variables similarly].</p><p class="calibre8">To illustrate, think of a data correlation situation where television advertising has suggested that athletes who wear a certain brand of shoe run faster. However, those same athletes each employee personal trainers, which may be an influential factor.</p><p class="calibre8">So, correlation (or performing correlation analysis) is a <span class="strong"><em class="calibre9">statistical technique</em></span> that can show <a id="id627" class="calibre1"/>whether and how strongly pairs of variables are related. When variables are identified that are strongly related, it makes statistical sense to remove one of them from the analysis; however, when pairs of variables appear related but have a <span class="strong"><em class="calibre9">weaker</em></span> relationship, it's best to have both variables remain in the population.</p><p class="calibre8">For example, winning professional football teams and team merchandise sales are related in that teams with winning seasons frequently sell more merchandise. However, some teams have a stronger following then others and have high merchandise sales even when they lose more games than they win. Nonetheless, the average sales of a team winning more than 50% of its games is more than one losing 50% of their games, and teams winning more than 75% of their games exceed sales of those losing 75% of their games, and so on. So, what is the effect of winning games on a team's merchandise sales? It can be difficult to determine, but determining correlation between the data points can tell you just how much of the variation in a team's performance is related to their merchandise sales.</p><p class="calibre8">Although the correlation of <span class="strong"><em class="calibre9">games won</em></span> and <span class="strong"><em class="calibre9">merchandise sold</em></span> may be obvious, our example may contain <span class="strong"><em class="calibre9">unsuspected data</em></span> correlations. You may also suspect there are additional correlations, but are not sure which are the strongest.</p><p class="calibre8">A well-planned, thorough correlation analysis on the data can lead to a greater understanding of your data; however, just like all statistical techniques, correlation is only appropriate for certain types of data. <span class="strong"><strong class="calibre2">Correlation works for quantifiable data</strong></span> in which numbers <a id="id628" class="calibre1"/>are meaningful - usually quantities of some sort (such as products sold). It cannot be used for purely categorical data, such as an individual's gender, brand preference, or education level.</p><p class="calibre8">Let's go <a id="id629" class="calibre1"/>on and look at causation a bit more closely.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Causation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec67" class="calibre1"/>Causation</h2></div></div></div><p class="calibre8">It is <a id="id630" class="calibre1"/>very important to understand the concept of causation and how it compares to correlation.</p><p class="calibre8">Causation is defined in statistics as a variable that can cause change to or within another variable. The result of this effect can <span class="strong"><em class="calibre9">always</em></span> be predicted, providing a clear relationship between variables that can be established with certainty.</p><p class="calibre8">Causation involves correlation, but correlation does not imply causation. Every variable somehow linked to another may appear to imply causation. This is not always so; linking one thing with another does not always prove that the result has been caused by the other. The rule of thumb is: only if you can directly link a variance or change of a variable to that of another, can you say it's causation.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="The degree of correlation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch08lvl2sec68" class="calibre1"/>The degree of correlation</h2></div></div></div><p class="calibre8">To have <a id="id631" class="calibre1"/>a way of indicating or quantifying a statistical relationship between variables, we use a number referred to as a <span class="strong"><em class="calibre9">correlation coefficient</em></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem">It ranges from -1.0 to +1.0</li><li class="listitem">The closer it is to +1 or -1, the more closely the two variables are related</li><li class="listitem">If zero, there is <span class="strong"><em class="calibre9">no relationship</em></span> between the variables it represents</li><li class="listitem">If positive, it means that, as one variable increases, the other increases</li><li class="listitem">If negative, it means that, as one variable increases, the other decreases</li></ul></div></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Reporting on correlation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch08lvl2sec69" class="calibre1"/>Reporting on correlation</h2></div></div></div><p class="calibre8">While <a id="id632" class="calibre1"/>correlation coefficients are normally reported simply as-is (a value between -1 and +1), they are often <span class="strong"><em class="calibre9">squared</em></span> first to make them easier to understand.</p><p class="calibre8">If a correlation coefficient is <span class="strong"><em class="calibre9">r</em></span>, then <span class="strong"><em class="calibre9">r</em></span> <span class="strong"><em class="calibre9">squared</em></span> (you remove the decimal point) equals the percentage of the variation in one variable that is related to the variation in the other. Given this, a correlation coefficient of .5 would mean that the variation percentage is 25%.</p><p class="calibre8">In an earlier section of this chapter, we looked at a simple visualization of a correlation between <a id="id633" class="calibre1"/>the number of games won and the sales of a team's merchandise. In practice, when creating a <span class="strong"><em class="calibre9">correlation report</em></span>, you can also show a second figure – statistical significance. Adding significance will show the probability of error within the identified correlation information. Finally, since sample size can impact outcomes significantly, it is a good practice to also show the sample size.</p><p class="calibre8">In summary, identifying correlations within the data being observed is a common and accepted method for accomplishing dimensionality reduction. Another method is <span class="strong"><em class="calibre9">principal component analysis</em></span>, which we will cover in the next section.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Principal component analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_7"><a id="ch08lvl2sec70" class="calibre1"/>Principal component analysis</h2></div></div></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Principal component analysis</strong></span> (<span class="strong"><strong class="calibre2">PCA</strong></span>), is <a id="id634" class="calibre1"/>another <a id="id635" class="calibre1"/>popular statistical technique used for dimensional reduction in data.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note28" class="calibre1"/>Note</h3><p class="calibre8">PCA is also called the Karhunen-Loeve <code class="email">transform</code> method, and in fact, depending upon the audience, PCA has been said to be the most commonly used dimension reduction technique.</p></div><p class="calibre8">PCA is a technique that attempts to not only reduce data dimensionality but also retain as much of the variation in the data as possible.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note29" class="calibre1"/>Note</h3><p class="calibre8">Principal component analysis is an approach to factor analysis (which will be discussed later in this chapter) that considers the <span class="strong"><em class="calibre9">total variance</em></span> in the data file.</p></div><p class="calibre8">The process of PCA uses what is known as an <span class="strong"><em class="calibre9">orthogonal transformation</em></span> process to convert a set of observations of possibly <span class="strong"><em class="calibre9">correlated</em></span> variables into a set of values of linearly <span class="strong"><em class="calibre9">uncorrelated</em></span> variables. These variables are then called <span class="strong"><strong class="calibre2">principal components</strong></span>, or the data's <a id="id636" class="calibre1"/>principal modes of variation.</p><p class="calibre8">Through the PCA effort, the number of principal components (or variables) should be <span class="strong"><strong class="calibre2">less than or equal to</strong></span> the <span class="strong"><strong class="calibre2">number of original variables</strong></span> or the number of original observations, thereby <span class="strong"><em class="calibre9">reducing the data's independent dimensionality</em></span> (that is, dimensional reduction) or the number of independent dimensions.</p><p class="calibre8">These principal components are defined and arranged such that the first principal component accounts for as much of the variability in the data as possible, and each succeeding principal component has the next highest variance possible subject to the constraint that it is orthogonal to the preceding components.</p><p class="calibre8">The <a id="id637" class="calibre1"/>general concept or objective for performing a <span class="strong"><em class="calibre9">principal component analysis</em></span> is to observe that the same results will be obtained for affecting any of the independent variables upon the dependent variable, regardless of whether one models the effects of the variables individually.</p><p class="calibre8">PCA is mostly used as a tool when performing an <span class="strong"><em class="calibre9">exploratory data analysis</em></span>, or data profiling, since its operation can be thought of as revealing the internal structure of the data in a way <a id="id638" class="calibre1"/>that best explains the variance in the data, but it can also be helpful in predictive modeling.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note30" class="calibre1"/>Note</h3><p class="calibre8">
<span class="strong"><strong class="calibre2">Data profiling</strong></span> involves logically getting to know the data through query, experimentation, and review. Following the profiling process, you can then use the information you have collected to add context (and/or apply new perspectives) to the data. Adding context to data requires the manipulation of the data to perhaps reformat it, such as by adding calculations, aggregations or additional columns or re-ordering, and so on.</p></div><p class="calibre8">Principal <a id="id639" class="calibre1"/>component analysis or PCA is an alternative form of the common <span class="strong"><strong class="calibre2">factor analysis</strong></span> (which will be discussed later in this chapter) process. Factor analysis typically incorporates more domain-specific assumptions about the underlying structure of the data being observed and solves eigenvectors of a slightly different matrix.</p><p class="calibre8">To understand, at a very high level, think of PCA as continually fitting an <span class="strong"><em class="calibre9">n</em></span>-dimensional ellipsoid to a plotted data file, where each axis of the ellipsoid represents a <span class="strong"><em class="calibre9">principal component</em></span> of that data. If some axis of the ellipsoid is small, then the variance along that axis is also small, and by omitting that axis (and its corresponding principal component) from the data file, we will lose only a commensurately small amount of information about our data file; otherwise, the axis (the principal component) stays, representing some degree of variation to the mean of the data file overall.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Using R to understand PCA"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_8"><a id="ch08lvl2sec71" class="calibre1"/>Using R to understand PCA</h2></div></div></div><p class="calibre8">Perhaps <a id="id640" class="calibre1"/>with less of effort and logic required to understand PCA, as described in the preceding paragraph, you can use the generic R function <code class="email">princomp</code>.</p><p class="calibre8">The <code class="email">princomp</code> function is a way of simplifying a complex data file by exposing the sources of variations within the data using calculated standard deviations of the data file's principal components. This is best illustrated using the classic iris data file (provided when you install the R programming language).</p><p class="calibre8">The data <a id="id641" class="calibre1"/>file (partially shown in the following screenshot) contains flower attributes (petal width, petal length, sepal width, and sepal length) for over 150 species of <code class="email">iris</code>:</p><div class="mediaobject"><img src="../images/00148.jpeg" alt="Using R to understand PCA" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We should note that the <code class="email">princomp</code> function cannot handle string data (which is okay since our PCA analysis is only interested in identifying the numerical variations of principal components), so we can save the first five columns of data to a new object named <code class="email">pca</code> (dropping the column <code class="email">Species</code>) by using the following R line of code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">pca&lt;-princomp(iris[-5])</strong></span>
</pre></div><p class="calibre8">Next, we can use the R <code class="email">summary</code> command on the <code class="email">pca</code> object we just created. The output generated is shown as follows:</p><div class="mediaobject"><img src="../images/00149.jpeg" alt="Using R to understand PCA" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">You can see from the previous image that 92.4% of the variation in the dataset is explained by the <span class="strong"><em class="calibre9">first component alone</em></span>, and 97.8% is explained by the <span class="strong"><em class="calibre9">first two components!</em></span>
</p><p class="calibre8">To better <a id="id642" class="calibre1"/>understand, we can visualize the preceding observations by using the R <code class="email">screeplot</code> function with our <code class="email">pca</code> object, shown as follows:</p><div class="mediaobject"><img src="../images/00150.jpeg" alt="Using R to understand PCA" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <code class="email">screeplot</code> function generates a screen plot displaying the proportion of the total variation in a data file that is explained by each of the components in a principal component analysis, showing how many of the principal components are needed to summarize the data.</p><p class="calibre8">Our generated visualization is as follows:</p><div class="mediaobject"><img src="../images/00151.jpeg" alt="Using R to understand PCA" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The results of a PCA are usually deliberated in terms of the scores of each component (sometimes referred to as <span class="strong"><em class="calibre9">factor scores</em></span>), which are the transformed variable values corresponding <a id="id643" class="calibre1"/>to a particular data point in a data file, and loadings, which are the weight by which each standardized original variable should be multiplied to get the component score.</p><p class="calibre8">We can use the R <code class="email">loadings</code> and <code class="email">scores</code> commands to view our loadings and scores information, shown as follows:</p><div class="mediaobject"><img src="../images/00152.jpeg" alt="Using R to understand PCA" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Independent component analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_9"><a id="ch08lvl2sec72" class="calibre1"/>Independent component analysis</h2></div></div></div><p class="calibre8">Yet another <a id="id644" class="calibre1"/>concept concerning dimension reduction is ICA, or <span class="strong"><strong class="calibre2">independent component analysis</strong></span> (<span class="strong"><strong class="calibre2">ICA</strong></span>). This is a process where there is an attempt to uncover or verify <span class="strong"><em class="calibre9">statistically independent</em></span> variables or dimensions in high-dimensional data sources.</p><p class="calibre8">Using one selected ICA process, each variable or dimension in the data can be identified, examined for independence, and then selectively removed from, or retained in, the overall data analysis.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note31" class="calibre1"/>Note</h3><p class="calibre8">If the reader takes time to perform any additional research on ICA, he/she will encounter the common example application called the cocktail party problem, which is listening in on one person's speech in a noisy room.</p></div></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Defining independence"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_10"><a id="ch08lvl2sec73" class="calibre1"/>Defining independence</h2></div></div></div><p class="calibre8">ICA <a id="id645" class="calibre1"/>attempts to find independent components in data by maximizing the <span class="strong"><em class="calibre9">statistical independence</em></span> of the components. But just how is a variable's independence defined?</p><p class="calibre8">Components are <span class="strong"><em class="calibre9">determined to be independent</em></span> if the realization of one does not affect the probability distribution of the other.</p><p class="calibre8">There are many acceptable ways to determine independence, and your choice will determine the form of the ICA algorithm used.</p><p class="calibre8">The two most widely used definitions of independence (for ICA) are:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Minimization of mutual information</strong></span> (<span class="strong"><strong class="calibre2">MMI</strong></span>): Mutual information measures the <a id="id646" class="calibre1"/>information that two or more components share, measuring to what extent knowing one of these variables reduces uncertainty about the other. The less mutual information a component includes, the more independent the component is.</li><li class="listitem"><span class="strong"><strong class="calibre2">Non-Gaussianity Maximization</strong></span> (<span class="strong"><strong class="calibre2">NGM</strong></span>): Non-Gaussianity Maximization looks <a id="id647" class="calibre1"/>to avoid or reduce the average, or in other words, highlight variability (its level of independence) in a component.</li></ul></div></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="ICA pre-processing"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_11"><a id="ch08lvl2sec74" class="calibre1"/>ICA pre-processing</h2></div></div></div><p class="calibre8">Before <a id="id648" class="calibre1"/>applying any ICA logic to data, typical ICA methodologies use procedures such as <span class="strong"><em class="calibre9">centering</em></span> and <span class="strong"><em class="calibre9">whitening</em></span> as preprocessing steps in order to simplify and reduce the complexity of a problem and highlight features of data not readily explained by its average or co-variance.</p><p class="calibre8">In other words, before attempting to determine the level of a component independence, the data file may be reviewed and manipulated to make it more understandable using various preprocessing methods.</p><p class="calibre8">Centering is the most basic preprocess method commonly used, which, true to its name, involves centering a data point (or <span class="strong"><em class="calibre9">x</em></span>) by subtracting its mean, thus making it a zero-mean variable. Whitening, another preprocessing method, transforms a data point (<span class="strong"><em class="calibre9">x</em></span>) linearly so that <a id="id649" class="calibre1"/>its components are uncorrelated and equal unity.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Factor analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_12"><a id="ch08lvl2sec75" class="calibre1"/>Factor analysis</h2></div></div></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Factor analysis</strong></span> is <a id="id650" class="calibre1"/>yet another important <a id="id651" class="calibre1"/>statistical method used to determine and describe the data's (or the data components') variability between <span class="strong"><em class="calibre9">observed</em></span>, correlated variables versus (a potentially) lower number of <span class="strong"><em class="calibre9">unobserved</em></span> (or also referred to as <span class="strong"><em class="calibre9">latent</em></span>) variables, or the data <span class="strong"><em class="calibre9">factors</em></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note32" class="calibre1"/>Note</h3><p class="calibre8">
<span class="strong"><em class="calibre9">Observed variables</em></span> are those variables for which you should have clearly defined measurements in a data file, whereas <span class="strong"><em class="calibre9">unobserved variables</em></span> are those variables for which you do not have clearly defined measurements and that perhaps are inferred from certain observed variables within your data file.</p></div><p class="calibre8">In other words, we consider: Is it possible that variations in <span class="strong"><em class="calibre9">six observed variables</em></span> reflect the same variations found in only <span class="strong"><em class="calibre9">two unobserved variables</em></span>?</p><p class="calibre8">Factor analysis should/can be used when a data file includes a <span class="strong"><em class="calibre9">large number of observed</em></span> variables that seem to reflect <span class="strong"><em class="calibre9">a smaller number of unobserved variables</em></span>, giving us an opportunity for dimensional reduction, reducing the number of elements to be studied, and observing how they are interlinked.</p><p class="calibre8">Overall, factor analysis involves using techniques to help yield a smaller number of linear combinations of variables that, even though there is a reduced number of variables, account for and explain the majority of the variance in the data's components.</p><p class="calibre8">In short, performing a factor analysis on a data file attempts to search for <span class="strong"><em class="calibre9">cooperative variations</em></span> in previously unobserved variables.</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Explore and confirm"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_13"><a id="ch08lvl2sec76" class="calibre1"/>Explore and confirm</h2></div></div></div><p class="calibre8">Typically, one <a id="id652" class="calibre1"/>would begin the exercise of factor analysis with an exploration of the data file, exploring probable underlying factor structures <a id="id653" class="calibre1"/>of a set(s) of the <span class="strong"><em class="calibre9">data's observed variables</em></span> without imposing a predetermined outcome. This process is referred to as <span class="strong"><strong class="calibre2">exploratory factor analysis</strong></span> (<span class="strong"><strong class="calibre2">EFA</strong></span>).</p><p class="calibre8">During the <a id="id654" class="calibre1"/>exploratory factor analysis phase, we attempt to determine the number of unobserved or hidden variables and come up with a means of explaining variation within the data using a smaller number of hidden variables; in other words, we are condensing the information required for observation.</p><p class="calibre8">Once determinations are made (one or more hypothesis is formed), one would then want to <a id="id655" class="calibre1"/>confirm (or test or validate) the factor structure <a id="id656" class="calibre1"/>that was revealed during the EFA process. This <a id="id657" class="calibre1"/>step is known widely as <span class="strong"><strong class="calibre2">confirmatory factor analysis</strong></span> (<span class="strong"><strong class="calibre2">CFA</strong></span>).</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="Using R for factor analysis"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_14"><a id="ch08lvl2sec77" class="calibre1"/>Using R for factor analysis</h2></div></div></div><p class="calibre8">As usual, the R programming language provides various ways of performing a proper factor <a id="id658" class="calibre1"/>analysis.</p><p class="calibre8">For example, we have the R function <code class="email">factanal</code>. This function performs a maximum-likelihood factor analysis on a numeric matrix. In its simplest form, this function needs <span class="strong"><em class="calibre9">x</em></span> (your numeric matrix object) and the number of factors to be considered (or fitted):</p><div class="informalexample"><pre class="programlisting">factanal(x, factors = 3)</pre></div><p class="calibre8">We can run a simple example here for clarification based upon the R documentation and a number of generic functions.</p><p class="calibre8">First, a <span class="strong"><em class="calibre9">numeric matrix</em></span> is constructed by combining lists of random numeric values into six variables saved as R vectors (v1 through v6).</p><p class="calibre8">The R code is shown as follows:</p><div class="informalexample"><pre class="programlisting">&gt; v1 &lt;- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
&gt; v2 &lt;- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
&gt; v3 &lt;- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
&gt; v4 &lt;- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
&gt; v5 &lt;- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
&gt; v6 &lt;- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)</pre></div><p class="calibre8">The next step is to create a numeric matrix from our six variables, calling it <code class="email">m1</code>.</p><p class="calibre8">To accomplish this, we can use the R <code class="email">cbind</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; m1 &lt;- cbind(v1,v2,v3,v4,v5,v6)</pre></div><p class="calibre8">The following screenshot shows our code executed, along with a summary of our object <code class="email">m1</code>:</p><div class="mediaobject"><img src="../images/00153.jpeg" alt="Using R for factor analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The R function <code class="email">summary</code> provides us with some interesting details about our six variables, such as the minimum and maximum values, median, and the mean.</p><p class="calibre8">Now <a id="id659" class="calibre1"/>that we have a numeric matrix, we can <span class="strong"><em class="calibre9">review the variances</em></span> between our six variables by running the R <code class="email">cor</code> function.</p><p class="calibre8">The following screenshot shows the output that is generated from the <code class="email">cor</code> function:</p><div class="mediaobject"><img src="../images/00154.jpeg" alt="Using R for factor analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Interesting information, but let's move on.</p><p class="calibre8">Finally, we are now ready to use (the R function) <code class="email">factanal</code>.</p><p class="calibre8">Again, using <a id="id660" class="calibre1"/>the function's simplest form - simply providing the name of the data to analyze (<code class="email">m1</code>) and the number of factors to consider (let's use <code class="email">3</code>) - the following output is generated for us:</p><div class="mediaobject"><img src="../images/00155.jpeg" alt="Using R for factor analysis" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="The output"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_15"><a id="ch08lvl2sec78" class="calibre1"/>The output</h2></div></div></div><p class="calibre8">The R <a id="id661" class="calibre1"/>function <code class="email">factanal</code> first calculates <span class="strong"><em class="calibre9">uniqueness</em></span>. Uniqueness is the variance that is unique to each variable and not shared with other variables.</p><p class="calibre8">Factor <span class="strong"><em class="calibre9">loadings</em></span> are also calculated and displayed in a <span class="strong"><em class="calibre9">factor matrix</em></span>. The factor matrix is the matrix that contains the factor loadings of all the variables on all the factors extracted. The term factor loadings denotes the simple correlations between the factors and the variables, such as the correlation between the observed score and the unobserved score. Generally, the higher the better.</p><p class="calibre8">Notice the final message generated in the preceding screenshot:</p><p class="calibre8">
<span class="strong"><strong class="calibre2">The degrees of freedom for the model is 0 and the fit was 0.4755</strong></span>
</p><p class="calibre8">The <a id="id662" class="calibre1"/>number of <span class="strong"><em class="calibre9">degrees of freedom</em></span> can be defined as the minimum number of independent coordinates that can specify the position of the system completely (therefore, a zero is not so good). So, if we experiment a bit by increasing the numbers of factors to four, we see that <code class="email">factanal</code> is smart enough to tell us that, with only six variables, four is too many factors.</p><p class="calibre8">The following screenshot shows (globally) the output generated using four factors:</p><div class="mediaobject"><img src="../images/00156.jpeg" alt="The output" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Given the results shown, let us now try decreasing the number of factors to two:</p><div class="mediaobject"><img src="../images/00157.jpeg" alt="The output" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Notice <a id="id663" class="calibre1"/>that this time we see that two factors are sufficient.</p><p class="calibre8">Based upon the preceding, the results of our simple factor analysis seem to have improved but does this mean that the number of variables could correctly describe the data?</p><p class="calibre8">Obviously, more data, more variables and more experimentation are required!</p></div></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Dimensionality Reduction">
<div class="book" title="Defining DR">
<div class="book" title="NNMF"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_16"><a id="ch08lvl2sec79" class="calibre1"/>NNMF</h2></div></div></div><p class="calibre8">The <a id="id664" class="calibre1"/>term <span class="strong"><em class="calibre9">factorization</em></span> refers to the process or act of factoring, which is the breakdown of an object into a result (of other objects, or <span class="strong"><em class="calibre9">factors</em></span>) that, when multiplied together, equal the original. <span class="strong"><em class="calibre9">Matrix</em></span> factorization then, is factorizing a matrix, or finding two (or more) matrices that will equal the original matrix when you multiply them.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Non-negative matrix factorization</strong></span> (<span class="strong"><strong class="calibre2">NMF</strong></span>, <span class="strong"><strong class="calibre2">NNMF</strong></span>) is using an algorithm to factorize a matrix <a id="id665" class="calibre1"/>into (usually) two matrices <a id="id666" class="calibre1"/>with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to analyze.</p></div></div></div>
<div class="book" title="Summary" id="22O7C1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec63" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we introduced the idea of (and purpose for) data <span class="strong"><em class="calibre9">dimensional reduction</em></span>: reducing the total number of observations to consider when creating a predictive model.</p><p class="calibre8">The most common methods, strategies, and concepts for reduction were reviewed, including correlated data analysis, reporting on correlation, PCA, ICA, and factor analysis.</p><p class="calibre8">In the next chapter, we think about how several trained models can work together as an ensemble, in order to produce a single model that is more powerful than the individual models involved.</p></div></body></html>