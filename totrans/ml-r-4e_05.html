<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer207">
    <h1 class="chapterNumber">5</h1>
    <h1 class="chapterTitle" id="_idParaDest-105">Divide and Conquer – Classification Using Decision Trees and Rules</h1>
    <p class="normal">When deciding between job offers, many people begin by making lists of pros and cons, then eliminate options using simple rules. For instance, they may decide, “If I have to commute for more than an hour, I will be unhappy,” or “If I make less than $50K, I can’t support my family.” In this way, the complex decision of predicting one’s future career happiness can be reduced to a series of simple decisions.</p>
    <p class="normal">This chapter covers decision trees and rule learners—two machine learning methods that also make complex decisions from sets of simple choices. These methods present their knowledge in the form of logical structures that can be understood with no statistical knowledge. This aspect makes these models particularly useful for business strategy and process improvement.</p>
    <p class="normal">By the end of this chapter, you will have learned:</p>
    <ul>
      <li class="bulletList">How trees and rules “greedily” partition data into interesting segments</li>
      <li class="bulletList">The most common decision tree and classification rule learners, including the C5.0, 1R, and RIPPER algorithms</li>
      <li class="bulletList">How to use these algorithms for performing real-world classification tasks, such as identifying risky bank loans and poisonous mushrooms</li>
    </ul>
    <p class="normal">We will begin by examining decision trees and follow that with a look at classification rules. Then, we will summarize what we’ve learned by previewing later chapters, which discuss methods that use trees and rules as a foundation for more advanced machine learning techniques.</p>
    <h1 class="heading-1" id="_idParaDest-106">Understanding decision trees</h1>
    <p class="normal">Decision tree learners are <a id="_idIndexMarker496"/>powerful classifiers that utilize a <strong class="keyWord">tree structure</strong> to model the relationships among the features and the potential outcomes. As illustrated in the following figure, this structure earned its name because it mirrors the way a literal tree begins, with a wide trunk at the base that splits off into narrower and narrower branches as it works its way upward. In much the same way, a decision tree classifier uses a structure of branching decisions to channel examples into a final predicted class value.</p>
    <p class="normal">To better understand how this works in practice, let’s consider the following tree, which predicts whether a job offer should be accepted. A job offer under consideration begins at the <strong class="keyWord">root node</strong>, from <a id="_idIndexMarker497"/>where it then passes through the <strong class="keyWord">decision nodes</strong>, which require choices to be made based on the <a id="_idIndexMarker498"/>attributes of the job. These choices split the data<a id="_idIndexMarker499"/> across <strong class="keyWord">branches</strong> that indicate the potential outcomes of a decision. They are depicted here as yes or no outcomes, but in other cases, there may be more than two possibilities.</p>
    <p class="normal">If a final decision can be made, the<a id="_idIndexMarker500"/> tree terminates in <strong class="keyWord">leaf nodes</strong> (also known as <strong class="keyWord">terminal nodes</strong>) that denote the <a id="_idIndexMarker501"/>action to be taken as the result of the series of decisions. In the case of a predictive model, the leaf nodes provide the expected result given the series of events in the tree.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_05_01.png"/></figure>
    <p class="packt_figref">Figure 5.1: A decision tree depicting the process of determining whether to accept a new job offer</p>
    <p class="normal">A great benefit of decision tree algorithms is that the flowchart-like tree structure is not only for the machine’s internal use. After the model is created, many decision tree algorithms output the resulting structure in a human-readable format. This provides insight into how and why the <a id="_idIndexMarker502"/>model works or doesn’t work well for a particular task. This also makes decision trees particularly appropriate for applications in which the classification mechanism needs to be transparent for legal reasons, or if the results need to be shared with others to inform future business practices. With this in mind, some potential uses include the following:</p>
    <ul>
      <li class="bulletList">Credit scoring models in which the criteria that cause an applicant to be rejected need to be clearly documented and free from bias</li>
      <li class="bulletList">Marketing studies of customer behavior, such as satisfaction or churn, which will be shared with management or advertising agencies</li>
      <li class="bulletList">Diagnosis of medical conditions based on laboratory measurements, symptoms, or rates of disease progression</li>
    </ul>
    <p class="normal">Although the previous applications illustrate the value of trees in informing decision-making processes, this is not to suggest that their utility ends here. In fact, decision trees are one of the single most widely used machine learning techniques, and can be applied to model almost any type of data—often with excellent out-of-the-box performance.</p>
    <p class="normal">That said, despite their wide applicability, it is worth noting that there are some scenarios where trees may not be an ideal fit. This includes tasks where the data has many nominal features with many levels or a large number of numeric features. These cases may result in a very large number of decisions and an overly complex tree. They may also contribute to the tendency of decision trees to overfit data, though as we will soon see, even this weakness can be overcome by adjusting some simple parameters.</p>
    <h2 class="heading-2" id="_idParaDest-107">Divide and conquer</h2>
    <p class="normal">Decision trees are built using a <a id="_idIndexMarker503"/>heuristic called <strong class="keyWord">recursive partitioning</strong>. This approach is also<a id="_idIndexMarker504"/> commonly known as <strong class="keyWord">divide and conquer</strong> because it splits the data into subsets, which are then split repeatedly into even smaller subsets, and so on and so forth, until the process stops when the algorithm determines the data within the subsets are sufficiently homogenous, or another stopping criterion has been met.</p>
    <p class="normal">To see how splitting a dataset can create a decision tree, imagine a root node that will grow into a mature tree. At first, the root node represents the entire dataset, since no splitting has transpired. Here, the decision tree algorithm must choose a feature to split upon; ideally, it chooses the<a id="_idIndexMarker505"/> feature most predictive of the target class. </p>
    <p class="normal">The examples are then partitioned into groups according to the distinct values of this feature, and the first set of tree branches is formed.</p>
    <p class="normal">Working down each branch, the algorithm continues to divide and conquer the data, choosing the best candidate feature each time to create another decision node until a stopping criterion is reached. Divide and conquer might stop at a node if:</p>
    <ul>
      <li class="bulletList">All (or nearly all) of the examples at the node have the same class</li>
      <li class="bulletList">There are no remaining features to distinguish among the examples</li>
      <li class="bulletList">The tree has grown to a predefined size limit</li>
    </ul>
    <p class="normal">To illustrate the tree-building process, let’s consider a simple example. Imagine that you work for a Hollywood studio, where your role is to decide whether the studio should move forward with producing the screenplays pitched by promising new authors. After returning from a vacation, your desk is piled high with proposals. Without the time to read each proposal cover-to-cover, you decide to develop a decision tree algorithm to predict whether a potential movie would fall into one of three categories: <em class="italic">Critical Success</em>, <em class="italic">Mainstream Hit</em>, or <em class="italic">Box Office Bust</em>.</p>
    <p class="normal">To source data to create the decision tree model, you turn to the studio archives to examine the factors leading to the success or failure of the company’s 30 most recent releases. You quickly notice a relationship between the film’s estimated shooting budget, the number of A-list celebrities lined up for starring roles, and the film’s level of success. Excited about this finding, you produce a scatterplot to illustrate the pattern:</p>
    <figure class="mediaobject"><img alt="A picture containing shape  Description automatically generated" src="../Images/B17290_05_02.png"/></figure>
    <p class="packt_figref">Figure 5.2: A scatterplot depicting the relationship between a movie’s budget and celebrity count</p>
    <p class="normal">Using the divide and conquer strategy, you can build a simple decision tree from this data. First, to create the tree’s root node, you split the feature indicating the number of celebrities, partitioning<a id="_idIndexMarker506"/> the movies into groups with and without a significant number of A-list stars:</p>
    <figure class="mediaobject"><img alt="A picture containing shape  Description automatically generated" src="../Images/B17290_05_03.png"/></figure>
    <p class="packt_figref">Figure 5.3: The decision tree’s first split divides the data into films with high and low celebrity counts</p>
    <p class="normal">Next, among the group of movies with a larger number of celebrities, you make another split between movies with and without a high budget:</p>
    <figure class="mediaobject"><img alt="A picture containing shape  Description automatically generated" src="../Images/B17290_05_04.png"/></figure>
    <p class="packt_figref">Figure 5.4: The decision tree’s second split further divides the films with a high celebrity count into those with low and high budgets</p>
    <p class="normal">At this point, you’ve partitioned the data into three groups. The group in the top-left corner of the diagram is composed entirely of critically acclaimed films. This group is distinguished by a high number of celebrities and a relatively low budget. In the top-right corner, nearly all movies<a id="_idIndexMarker507"/> are box office hits with high budgets and many celebrities. The final group, which has little star power but budgets ranging from small to large, contains the flops.</p>
    <p class="normal">If desired, you could continue to divide and conquer the data by splitting it on increasingly specific ranges of budget and celebrity count until each of the currently misclassified values is correctly classified in its own tiny partition. However, it is not advisable to overfit a decision tree in this way. Although there is nothing stopping the algorithm from splitting the data indefinitely, overly specific decisions do not always generalize more broadly. Thus, you choose to avoid the problem of overfitting by stopping the algorithm here, since more than 80 percent of the examples in each group are from a single class. This is the stopping criterion for the decision tree model.</p>
    <div class="packt_tip">
      <p class="normal">You might have noticed that diagonal lines might have split the data even more cleanly. This is one limitation of the decision tree’s knowledge representation, which uses <strong class="keyWord">axis-parallel splits</strong>. The fact<a id="_idIndexMarker508"/> that each split considers one feature at a time prevents the decision tree from forming more complex decision boundaries. For example, a diagonal line could be created by a decision that asks, “Is the number of celebrities greater than the estimated budget?” If so, then “it will be a critical success.”</p>
    </div>
    <p class="normal">The model for predicting the future success of movies can be represented in a simple tree, as shown in the following diagram. Each step in the tree shows the fraction of examples falling into each class, which shows how the data becomes more homogeneous as the branches get closer to a leaf. To evaluate a new movie script, follow the branches through <a id="_idIndexMarker509"/>each decision until the script’s success or failure has been predicted. Using this approach, you will be able to quickly identify the most promising options among the backlog of scripts and get back to more important work, such as writing an Academy Awards acceptance speech!</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_05_05.png"/></figure>
    <p class="packt_figref">Figure 5.5: A decision tree built on historical movie data can forecast the performance of future movies</p>
    <p class="normal">Since real-world data contains more than two features, decision trees quickly become far more complex than this, with many more nodes, branches, and leaves. In the next section, you will learn about a popular algorithm to build decision tree models automatically.</p>
    <h2 class="heading-2" id="_idParaDest-108">The C5.0 decision tree algorithm</h2>
    <p class="normal">There are numerous<a id="_idIndexMarker510"/> implementations of decision trees, but one of the most well known is the C5.0 algorithm. This algorithm was developed by computer scientist J. Ross Quinlan as an improved version of his prior algorithm, C4.5, which itself is an<a id="_idIndexMarker511"/> improvement over his <strong class="keyWord">Iterative Dichotomiser 3</strong> (<strong class="keyWord">ID3</strong>) algorithm. Although Quinlan markets C5.0 to commercial clients (see <a href="http://www.rulequest.com/"><span class="url">http://www.rulequest.com/</span></a> for details), the source code for a single-threaded version of the algorithm was made public, and has therefore been incorporated into programs such as R.</p>
    <div class="note">
      <p class="normal">To further confuse matters, a popular Java-based open-source alternative to C4.5, titled <strong class="keyWord">J48</strong>, is included in R’s <code class="inlineCode">RWeka</code> package (introduced later in this chapter). As the differences between C5.0, C4.5, and J48 are minor, the principles in this chapter apply to any of these three methods and the algorithms should be considered synonymous.</p>
    </div>
    <p class="normal">The C5.0 algorithm has become the industry standard for producing decision trees because it does well for most types of problems directly out of the box. Compared to other advanced machine learning models, such as those described in <em class="chapterRef">Chapter 7</em>, <em class="italic">Black-Box Methods – Neural Networks and Support Vector Machines</em>, the decision trees built by C5.0 generally perform nearly as well but are much easier to understand and deploy. Additionally, as shown in the following table, the algorithm’s weaknesses are relatively minor and can be largely avoided.</p>
    <table class="table-container" id="table001-3">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">An all-purpose classifier that does well on many types of problems</li>
              <li class="bulletList">Highly automatic learning process, which can handle numeric or nominal features, as well as missing data</li>
              <li class="bulletList">Excludes unimportant features</li>
              <li class="bulletList">Can be used on both small and large datasets</li>
              <li class="bulletList">Results in a model that can be interpreted without a mathematical background (for relatively small trees)</li>
              <li class="bulletList">More efficient than other complex models</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Decision tree models are often biased toward splits on features having a large number of levels</li>
              <li class="bulletList">It is easy to overfit or underfit the model</li>
              <li class="bulletList">Can have trouble modeling some relationships due to reliance on axis-parallel splits</li>
              <li class="bulletList">Small changes in training data can result in large changes to decision logic</li>
              <li class="bulletList">Large trees can be difficult to interpret and the decisions they make may seem counterintuitive</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">To keep things simple, our earlier decision tree example ignored the mathematics involved with how a <a id="_idIndexMarker512"/>machine would employ a divide and conquer strategy. Let’s explore this in more detail to examine how this heuristic works in practice.</p>
    <h3 class="heading-3" id="_idParaDest-109">Choosing the best split</h3>
    <p class="normal">The first challenge that a decision tree will face is to identify which feature to split upon. In the previous example, we<a id="_idIndexMarker513"/> looked for a way to split the data such that the resulting partitions contained examples primarily of a single class. </p>
    <p class="normal">The degree to which a subset of examples contains only a single class is known as <strong class="keyWord">purity</strong>, and any subset<a id="_idIndexMarker514"/> composed of only a single class is <a id="_idIndexMarker515"/>called <strong class="keyWord">pure</strong>.</p>
    <p class="normal">There are various measurements of purity that can be used to identify the best decision tree splitting <a id="_idIndexMarker516"/>candidate. C5.0 uses <strong class="keyWord">entropy</strong>, a concept borrowed from information theory that quantifies the randomness, or disorder, within a set of class values. Sets with high entropy are very diverse and provide little information about other items that may also belong in the set, as there is no apparent commonality. The decision tree hopes to find splits that reduce entropy, ultimately increasing homogeneity within the groups.</p>
    <p class="normal">Typically, entropy is measured in <strong class="keyWord">bits</strong>. If there are <a id="_idIndexMarker517"/>only two possible classes, entropy values can range from 0 to 1. For <em class="italic">n</em> classes, entropy ranges from 0 to <em class="italic">log</em><sub class="subscript-italic" style="font-style: italic;">2</sub>(<em class="italic">n</em>). In each case, the minimum value indicates that the sample is completely homogenous, while the maximum value indicates that the data are as diverse as possible, and no group has even a small plurality.</p>
    <p class="normal">In mathematical notion, entropy is specified as:</p>
    <p class="center"><img alt="" src="../Images/B17290_05_001.png"/></p>
    <p class="normal">In this formula, for a given segment of data (<em class="italic">S</em>), the term <em class="italic">c</em> refers to the number of class levels, and <em class="italic">p</em><sub class="subscript-italic" style="font-style: italic;">i</sub> refers to the proportion of values falling into the <em class="italic">i</em>th class level.</p>
    <p class="normal">For example, suppose we have a partition of data with two classes: red (60 percent) and white (40 percent). We can calculate the entropy as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> <span class="hljs-operator">-</span><span class="hljs-number">0.60</span> <span class="hljs-operator">*</span> log2<span class="hljs-punctuation">(</span><span class="hljs-number">0.60</span><span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> <span class="hljs-number">0.40</span> <span class="hljs-operator">*</span> log2<span class="hljs-punctuation">(</span><span class="hljs-number">0.40</span><span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">[1] 0.9709506
</code></pre>
    <p class="normal">We can visualize the entropy for all possible two-class arrangements. If we know the proportion of examples in one class is <em class="italic">x</em>, then the proportion in the other class is <em class="italic">(1 – x)</em>. Using the <code class="inlineCode">curve()</code> function, we can then plot the entropy for all possible values of <em class="italic">x</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> curve<span class="hljs-punctuation">(</span><span class="hljs-operator">-</span>x <span class="hljs-operator">*</span> log2<span class="hljs-punctuation">(</span>x<span class="hljs-punctuation">)</span> <span class="hljs-operator">-</span> <span class="hljs-punctuation">(</span><span class="hljs-number">1</span> <span class="hljs-operator">-</span> x<span class="hljs-punctuation">)</span> <span class="hljs-operator">*</span> log2<span class="hljs-punctuation">(</span><span class="hljs-number">1</span> <span class="hljs-operator">-</span> x<span class="hljs-punctuation">),</span>
        col <span class="hljs-operator">=</span> <span class="hljs-string">"red"</span><span class="hljs-punctuation">,</span> xlab <span class="hljs-operator">=</span> <span class="hljs-string">"x"</span><span class="hljs-punctuation">,</span> ylab <span class="hljs-operator">=</span> <span class="hljs-string">"Entropy"</span><span class="hljs-punctuation">,</span> lwd <span class="hljs-operator">=</span> <span class="hljs-number">4</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This results in the following graph:</p>
    <figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" src="../Images/B17290_05_06.png"/></figure>
    <p class="packt_figref">Figure 5.6: The total entropy as the proportion of one class varies in a two-class outcome</p>
    <p class="normal">As illustrated by the peak at <em class="italic">x = 0.50</em>, a 50-50 split results in the maximum entropy. As one class increasingly dominates the other, the entropy reduces to zero.</p>
    <p class="normal">To use entropy to<a id="_idIndexMarker518"/> determine the optimal feature to split upon, the algorithm calculates the change in homogeneity that would result from a split on each possible feature, a <a id="_idIndexMarker519"/>measure known as <strong class="keyWord">information gain</strong>. The information gain for a feature <em class="italic">F</em> is calculated as the difference between the entropy in the segment before the split (<em class="italic">S</em><sub class="subscript">1</sub>) and the partitions resulting from the split (<em class="italic">S</em><sub class="subscript">2</sub>):</p>
    <p class="center">InfoGain(<em class="italic">F</em>) = Entropy(S<sub class="subscript">1</sub>) – Entropy(S<sub class="subscript">2</sub>)</p>
    <p class="normal">One complication is that after a split, the data is divided into more than one partition. Therefore, the function to calculate <em class="italic">Entropy(S</em><sub class="subscript-italic" style="font-style: italic;">2</sub><em class="italic">)</em> needs to consider the total entropy across all partitions<a id="_idIndexMarker520"/> resulting from the split. It does this by weighting each partition’s entropy according to the proportion of all records falling into that partition. This can be stated in a formula as:</p>
    <p class="center"><img alt="" src="../Images/B17290_05_002.png"/></p>
    <p class="normal">In simple terms, the total entropy resulting from a split is the sum of entropy of each of the <em class="italic">n</em> partitions weighted by the proportion of examples falling in the partition (<em class="italic">w</em><sub class="subscript-italic" style="font-style: italic;">i</sub>).</p>
    <p class="normal">The higher the information gain, the better a feature is at creating homogeneous groups after a split on that feature. If the information gain is zero, there is no reduction in entropy for splitting on this feature. On the other hand, the maximum information gain is equal to the entropy prior to the split. This would imply the entropy after the split is zero, which means that the split results in completely homogeneous groups.</p>
    <p class="normal">The previous formulas assume nominal features, but decision trees use information gain for splitting on numeric features as well. To do so, a common practice is to test various splits that divide the values into groups greater than or less than a threshold. This reduces the numeric feature into a two-level categorical feature that allows information gain to be calculated as usual. The numeric cut point yielding the largest information gain is chosen for the split.</p>
    <div class="note">
      <p class="normal">Though it is used by C5.0, information gain is not the only splitting criterion that can be used to <a id="_idIndexMarker521"/>build decision trees. Other commonly used <a id="_idIndexMarker522"/>criteria are the <strong class="keyWord">Gini index</strong>, <strong class="keyWord">chi-squared statistic</strong>, and <strong class="keyWord">gain ratio</strong>. For<a id="_idIndexMarker523"/> a review of these (and many more) criteria, refer to <em class="italic">An Empirical Comparison of Selection Measures for Decision-Tree Induction, Mingers, J, Machine Learning, 1989, Vol. 3, pp. 319-342</em>.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-110">Pruning the decision tree</h3>
    <p class="normal">As mentioned earlier, a decision tree can continue to grow indefinitely, choosing splitting features and dividing into smaller and smaller partitions until each example is perfectly classified or the algorithm runs out of features to split on. However, if the tree grows overly large, many of the<a id="_idIndexMarker524"/> decisions it makes will be overly specific and the model will be overfitted to the training data. The process of <strong class="keyWord">pruning</strong> a decision tree involves reducing its size such that it generalizes better to unseen data.</p>
    <p class="normal">One solution to this problem is to stop the tree from growing once it reaches a certain number of decisions or when the decision nodes contain only a small number of examples. This is called <strong class="keyWord">early stopping</strong> or <strong class="keyWord">pre-pruning</strong><a id="_idIndexMarker525"/> the decision tree. As the tree avoids doing needless work, this is an appealing strategy. However, one downside to this approach is that there is no way to know whether the tree will miss subtle but important patterns that it would have learned had it grown to a larger size.</p>
    <p class="normal">An alternative, called <strong class="keyWord">post-pruning</strong>, involves growing a tree that is intentionally too large and pruning leaf nodes to reduce the<a id="_idIndexMarker526"/> size of the tree to a more appropriate level. This is often a more effective approach than pre-pruning because it is quite difficult to determine the optimal depth of a decision tree without growing it first. Pruning the tree later allows the algorithm to be certain that all the important data structures were discovered.</p>
    <div class="note">
      <p class="normal">The implementation details of pruning operations are very technical and beyond the scope of this book. For a comparison of some of the available methods, see <em class="italic">A Comparative Analysis of Methods for Pruning Decision Trees, Esposito, F, Malerba, D, Semeraro, G, IEEE Transactions on Pattern Analysis and Machine Intelligence, 1997, Vol. 19, pp. 476-491</em>.</p>
    </div>
    <p class="normal">One of the benefits of the C5.0 algorithm is that it is opinionated about pruning—it takes care of many of the decisions automatically using reasonable defaults. Its overall strategy is to post-prune the tree. It first grows a large tree that overfits the training data. Later, the nodes and branches that have little effect on the classification errors are removed. In some cases, entire branches are moved further up the tree or replaced by simpler decisions. These <a id="_idIndexMarker527"/>processes of <a id="_idIndexMarker528"/>grafting branches are known as <strong class="keyWord">subtree raising</strong> and <strong class="keyWord">subtree replacement</strong>, respectively.</p>
    <p class="normal">Getting the right balance of overfitting and underfitting is a bit of an art, but if model accuracy is vital, it may be<a id="_idIndexMarker529"/> worth investing some time with various pruning options to see if it improves the test dataset performance. As you will soon see, one of the strengths of the C5.0 algorithm is that it is very easy to adjust the training options.</p>
    <h1 class="heading-1" id="_idParaDest-111">Example – identifying risky bank loans using C5.0 decision trees</h1>
    <p class="normal">The global financial crisis of 2007-2008 highlighted the importance of transparency and rigor in banking practices. As the <a id="_idIndexMarker530"/>availability of credit was limited, banks tightened their lending systems and turned to machine learning to more accurately identify risky loans.</p>
    <p class="normal">Decision trees are widely used in the banking industry due to their high accuracy and ability to formulate a statistical <a id="_idIndexMarker531"/>model in plain language. Since governments in many countries carefully monitor the fairness of lending practices, executives must be able to explain why one applicant was rejected for a loan while <a id="_idIndexMarker532"/>another was approved. This information is also useful for customers hoping to determine why their credit rating is unsatisfactory.</p>
    <p class="normal">It is likely that automated credit scoring models are used for credit card mailings and instant online approval processes. In this section, we will develop a simple credit approval model using C5.0 decision trees. We will also see how the model results can be tuned to minimize errors that result in a financial loss.</p>
    <h2 class="heading-2" id="_idParaDest-112">Step 1 – collecting data</h2>
    <p class="normal">The motivation for our credit <a id="_idIndexMarker533"/>model is to identify factors that are linked to a higher risk of loan default. To do this, we must obtain data on past bank loans as well as information about the loan applicants that would have been available at the time of credit application.</p>
    <p class="normal">Data with these characteristics are available in a dataset donated to the UCI Machine Learning Repository (<a href="http://archive.ics.uci.edu/ml"><span class="url">http://archive.ics.uci.edu/ml</span></a>) by Hans Hofmann of the University of Hamburg. The dataset contains information on loans obtained from a credit agency in Germany.</p>
    <div class="packt_tip">
      <p class="normal">The dataset presented in this chapter has been modified slightly from the original in order to eliminate some preprocessing steps. To follow along with the examples, download the <code class="inlineCode">credit.csv</code> file from the Packt Publishing GitHub repository for this chapter and save it to your R working directory.</p>
    </div>
    <p class="normal">The credit dataset includes 1,000 examples of loans, plus a set of numeric and nominal features indicating characteristics of the loan and the loan applicant. A class variable indicates whether the loan went into<a id="_idIndexMarker534"/> default. Let’s see if we can identify any patterns that predict this outcome.</p>
    <h2 class="heading-2" id="_idParaDest-113">Step 2 – exploring and preparing the data</h2>
    <p class="normal">As we have done previously, we will import the data using the <code class="inlineCode">read.csv()</code> function. Now, because the character data is <a id="_idIndexMarker535"/>entirely categorical, we can set the <code class="inlineCode">stringsAsFactors = TRUE</code> to automatically convert all <a id="_idIndexMarker536"/>character type columns to factors in the resulting data frame:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"credit.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">We can check the resulting object by examining the first few lines of output from the <code class="inlineCode">str()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>credit<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">'data.frame':1000 obs. of  17 variables:
 $ checking_balance : Factor w/ 4 levels "&lt; 0 DM","&gt; 200 DM",..
 $ months_loan_duration: int  6 48 12 42 24 36 24 36 12 30 ...
 $ credit_history : Factor w/ 5 levels "critical","good",..
 $ purpose : Factor w/ 6 levels "business","car",..
 $ amount : int  1169 5951 2096 7882 4870 9055 2835 6948 ...
</code></pre>
    <p class="normal">We see the expected 1,000 observations and 17 features, which are a combination of factor and integer data types.</p>
    <p class="normal">Let’s take a look at the <code class="inlineCode">table()</code> output for a couple of loan features that seem likely to predict a default. The applicant’s checking and savings account balances are recorded as categorical variables:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>checking_balance<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">    &lt; 0 DM   &gt; 200 DM 1 - 200 DM    unknown
       274         63        269        394
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>savings_balance<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">     &lt; 100 DM &gt; 1000 DM  100 - 500 DM 500 - 1000 DM   unknown
          603        48           103            63       183
</code></pre>
    <p class="normal">The checking and savings account balances <a id="_idIndexMarker537"/>may prove to be<a id="_idIndexMarker538"/> important predictors of loan default status. Note that since the loan data was obtained from<a id="_idIndexMarker539"/> Germany, the values use the <strong class="keyWord">Deutsche Mark</strong> (<strong class="keyWord">DM</strong>), which was the currency used in Germany prior to the adoption of the Euro.</p>
    <p class="normal">Some of the loan’s features are numeric, such as its duration and the amount of credit requested:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>months_loan_duration<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    4.0    12.0    18.0    20.9    24.0    72.0 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>amount<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    250    1366    2320    3271    3972   18424
</code></pre>
    <p class="normal">The loan amounts ranged from 250 DM to 18,420 DM across terms of 4 to 72 months. They had a median amount of 2,320 DM and a median duration of 18 months.</p>
    <p class="normal">The <code class="inlineCode">default</code> vector indicates whether the loan applicant was able to meet the agreed payment terms or if they went into default. A total of 30 percent of the loans in this dataset went into default:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>credit<span class="hljs-operator">$</span>default<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">no yes
700 300
</code></pre>
    <p class="normal">A high rate of default is undesirable for a bank because it means that the bank is unlikely to fully recover its investment. If we are successful, our model will identify applicants who are at high risk of default, allowing the bank to refuse the credit request before the money is given.</p>
    <h3 class="heading-3" id="_idParaDest-114">Data preparation – creating random training and test datasets</h3>
    <p class="normal">As we have done in previous<a id="_idIndexMarker540"/> chapters, we will split our data into two portions: a training dataset to build the decision tree and a test dataset to evaluate its performance on new data. We will use 90 percent of the data for training and 10 percent for testing, which will provide us with 100 records to simulate new applicants. A 90-10 split is used here rather than the more <a id="_idIndexMarker541"/>common 75-25 split due to the relatively small size of the credit dataset; given that predicting loan defaults is a challenging learning task, we need as much training data as possible while still holding out a sufficient test sample. </p>
    <div class="packt_tip">
      <p class="normal">More sophisticated approaches for training and evaluating models with relatively small datasets are introduced in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>.</p>
    </div>
    <p class="normal">As prior chapters used data that had been sorted in a random order, we simply divided the dataset into two portions by taking the first subset of records for training and the remaining subset for testing. In contrast, the credit dataset is not randomly ordered, making the prior approach unwise. Suppose that the bank had sorted the data by the loan amount, with the largest loans at the end of the file. If we used the first 90 percent for training and the remaining 10 percent for testing, we would be training a model on only the small loans and testing the model on the big loans. Obviously, this could be problematic.</p>
    <p class="normal">We’ll solve this problem by training the model on a <strong class="keyWord">random sample</strong> of the credit data. A random sample is simply a process<a id="_idIndexMarker542"/> that selects a subset of records at random. In R, the <code class="inlineCode">sample()</code> function is used to perform random sampling. However, before putting it in action, a <a id="_idIndexMarker543"/>common practice is to set a <strong class="keyWord">seed</strong> value, which causes the randomization process to follow a sequence that can be replicated later. It may seem that this defeats the purpose of generating random numbers, but there is a good reason for doing it this way. Providing a seed value via the <code class="inlineCode">set.seed()</code> function ensures that if the analysis is repeated in the future, an identical result is obtained.</p>
    <div class="packt_tip">
      <p class="normal">You may wonder how a so-called random process can be seeded to produce an identical result. This is because computers use a mathematical function called a <strong class="keyWord">pseudorandom number generator</strong> to create random number sequences that appear to act very random, but are<a id="_idIndexMarker544"/> actually quite predictable given knowledge of the previous values in the sequence. In practice, modern pseudorandom number sequences are virtually indistinguishable from true random sequences, but have the benefit that computers can generate them quickly and easily.</p>
    </div>
    <p class="normal">The following commands use <code class="inlineCode">sample()</code> with a seed value. Note that the <code class="inlineCode">set.seed()</code> function uses the arbitrary value <code class="inlineCode">9829</code>. Omitting<a id="_idIndexMarker545"/> this seed will cause your training and testing splits to differ from <a id="_idIndexMarker546"/>those shown in the remainder of this chapter. The following commands select 900 values at random out of the sequence of integers from 1 to 1,000:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> set.seed<span class="hljs-punctuation">(</span><span class="hljs-number">9829</span><span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> train_sample <span class="hljs-operator">&lt;-</span> sample<span class="hljs-punctuation">(</span><span class="hljs-number">1000</span><span class="hljs-punctuation">,</span> <span class="hljs-number">900</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">As expected, the resulting <code class="inlineCode">train_sample</code> object is a vector of 900 random integers:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> str<span class="hljs-punctuation">(</span>train_sample<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">int [1:900] 653 866 119 152 6 617 250 343 367 138 ...
</code></pre>
    <p class="normal">By using this vector to select rows from the credit data, we can split it into the 90 percent training and 10 percent test datasets we desired. Recall that the negation operator (the <code class="inlineCode">-</code> character) used in the selection of the test records tells R to select records that are not in the specified rows; in other words, the test data includes only the rows that are not in the training sample:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_train <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span>train_sample<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
<span class="hljs-operator">&gt;</span> credit_test  <span class="hljs-operator">&lt;-</span> credit<span class="hljs-punctuation">[</span><span class="hljs-operator">-</span>train_sample<span class="hljs-punctuation">,</span> <span class="hljs-punctuation">]</span>
</code></pre>
    <p class="normal">If randomization was done correctly, we should have about 30 percent of loans with default in each of the datasets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> prop.table<span class="hljs-punctuation">(</span>table<span class="hljs-punctuation">(</span>credit_train<span class="hljs-operator">$</span>default<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">       no       yes 
0.7055556 0.2944444 
</code></pre>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> prop.table<span class="hljs-punctuation">(</span>table<span class="hljs-punctuation">(</span>credit_test<span class="hljs-operator">$</span>default<span class="hljs-punctuation">))</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">  no  yes 
0.65 0.35 
</code></pre>
    <p class="normal">Both the training and test datasets have roughly similar distributions of loan defaults, so we can now build our decision tree. In the case that the proportions differ greatly, we may decide to resample the <a id="_idIndexMarker547"/>dataset, or attempt a more sophisticated sampling approach, such as<a id="_idIndexMarker548"/> those covered in <em class="chapterRef">Chapter 10</em>, <em class="italic">Evaluating Model Performance</em>.</p>
    <div class="packt_tip">
      <p class="normal">If your results do not match exactly, ensure that you ran the command <code class="inlineCode">set.seed(9829)</code> immediately prior to creating the <code class="inlineCode">train_sample</code> vector. Note that R’s default random number generator changed in R version 3.6.0 and your results will differ if this code is run on earlier versions. This also means that the results here are slightly different from those in prior editions of this book.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-115">Step 3 – training a model on the data</h2>
    <p class="normal">We will use the<a id="_idIndexMarker549"/> C5.0 algorithm in the <code class="inlineCode">C50</code> package for training our decision tree model. If you have not done so already, install the package with <code class="inlineCode">install.packages("C50")</code> and load it to your R session using <code class="inlineCode">library(C50)</code>.</p>
    <p class="normal">The following syntax box lists some of the most common parameters used when building decision trees. Compared to the machine learning approaches we have used previously, the C5.0 algorithm offers many more ways to tailor the model to a particular learning problem.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_05_07.png"/></figure>
    <p class="packt_figref">Figure 5.7: C5.0 decision tree syntax</p>
    <p class="normal">The <code class="inlineCode">C5.0()</code> function uses a new syntax known as the <strong class="keyWord">R formula interface</strong> to specify the model to be trained. The formula syntax uses the <code class="inlineCode">~</code> operator (known as the tilde) to express the relationship <a id="_idIndexMarker550"/>between a target variable and its predictors. The class variable to be learned goes to the left of the tilde and the predictor features are written on the right, separated by <code class="inlineCode">+</code> operators. </p>
    <p class="normal">If you would like to model the relationship between the target <code class="inlineCode">y</code> and predictors <code class="inlineCode">x1</code> and <code class="inlineCode">x2</code>, you would write the formula as <code class="inlineCode">y ~ x1 + x2</code>. To include all variables in the model, the period character is used. For example, <code class="inlineCode">y ~ .</code> specifies the relationship between <code class="inlineCode">y</code> and all other features in the dataset.</p>
    <div class="packt_tip">
      <p class="normal">The R formula interface is used across many R functions and offers some powerful features to describe the relationships among predictor variables. We will explore some of these features in later chapters. However, if you’re eager for a preview, feel free to read the documentation using the <code class="inlineCode">?formula</code> command.</p>
    </div>
    <p class="normal">For the first iteration of the credit approval model, we’ll use the default C5.0 settings, as shown in the following <a id="_idIndexMarker551"/>code. The target class is named <code class="inlineCode">default</code>, so we put it on the left-hand side of the tilde, which is followed by a period indicating that all other columns in the <code class="inlineCode">credit_train</code> data frame are to be used as predictors:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_model <span class="hljs-operator">&lt;-</span> C5.0<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit_train<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">credit_model</code> object now contains a C5.0 decision tree. We can see some basic data about the tree by typing its name:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_model
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
C5.0.formula(formula = default ~ ., data = credit_train)
Classification Tree
Number of samples: 900 
Number of predictors: 16 
Tree size: 67 
Non-standard options: attempt to group attributes
</code></pre>
    <p class="normal">The output shows some simple facts about the tree, including the function call that generated it, the number of features (labeled <code class="inlineCode">predictors</code>), and examples (labeled <code class="inlineCode">samples</code>) used to grow the tree. Also listed is the tree size of <code class="inlineCode">67</code>, which indicates that the tree is 67 decisions deep—quite a bit larger than the example trees we’ve considered so far!</p>
    <p class="normal">To see the tree’s decisions, we can call the <code class="inlineCode">summary()</code> function on the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>credit_model<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This results in the following output, which has been truncated to show only the first few lines:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>credit_model<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
C5.0.formula(formula = default ~ ., data = credit_train)
C5.0 [Release 2.07 GPL Edition]
-------------------------------
Class specified by attribute `outcome'
Read 900 cases (17 attributes) from undefined.data
Decision tree:
checking_balance in {&gt; 200 DM,unknown}: no (415/55)
checking_balance in {&lt; 0 DM,1 - 200 DM}:
:...credit_history in {perfect,very good}: yes (59/16)
    credit_history in {critical,good,poor}:
    :...months_loan_duration &gt; 27:
        :...dependents &gt; 1:
        :   :...age &lt;= 45: no (12/2)
        :   :   age &gt; 45: yes (2)
</code></pre>
    <p class="normal">The preceding output<a id="_idIndexMarker552"/> shows some of the first branches in the decision tree. The first three lines could be represented in plain language as:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">If the checking account balance is unknown or greater than 200 DM, then classify as “not likely to default”</li>
      <li class="numberedList">Otherwise, if the checking account balance is less than zero DM or between one and 200 DM...</li>
      <li class="numberedList">…and the credit history is perfect or very good, then classify as “likely to default”</li>
    </ol>
    <p class="normal">The numbers in parentheses indicate the number of examples meeting the criteria for that decision and the number incorrectly classified by the decision. For instance, on the first line, <code class="inlineCode">415/55</code> indicates that, of the 415 examples reaching the decision, 55 were incorrectly classified as “not likely to default.” In other words, 55 out of 415 applicants actually defaulted in spite of the model’s prediction to the contrary.</p>
    <div class="packt_tip">
      <p class="normal">Sometimes a tree results in decisions that make little logical sense. For example, why would an applicant whose credit history is perfect or very good be likely to default, while those whose checking balance is unknown are not likely to default? Contradictory rules like this occur sometimes. They might reflect a real pattern in the data, or they may be a statistical anomaly. In either case, it is important to investigate such strange decisions to see whether the tree’s logic makes sense for business use.</p>
    </div>
    <p class="normal">After the tree, the <code class="inlineCode">summary(credit_model)</code> output displays a confusion matrix, which is a cross-tabulation that<a id="_idIndexMarker553"/> indicates the model’s incorrectly classified records in the training data:</p>
    <pre class="programlisting con"><code class="hljs-con">Evaluation on training data (900 cases):
        Decision Tree   
      ----------------  
      Size      Errors  
        66  118(13.1%)   &lt;&lt;
       (a)   (b)    &lt;-classified as
      ----  ----
       604    31    (a): class no
        87   178    (b): class yes
</code></pre>
    <p class="normal">The <code class="inlineCode">Errors</code> heading shows that the model correctly classified all but 118 of the 900 training instances for an error rate of 13.1 percent. A total of 31 actual <code class="inlineCode">no</code> values were incorrectly classified as <code class="inlineCode">yes</code> (false positives), while 87 <code class="inlineCode">yes</code> values were misclassified as <code class="inlineCode">no</code> (false negatives). Given the tendency of decision trees to overfit to the training data, the error rate reported here, which is based on training data performance, may be overly optimistic. Therefore, it is especially important to apply the decision tree to an unseen test dataset, which we will do shortly.</p>
    <p class="normal">The output also includes a section labeled <code class="inlineCode">Attribute usage</code>, which provides a general sense of the most important predictors used in the decision tree model. The first few lines of this output are as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">Attribute usage:
    100.00%       checking_balance
    53.89%        credit_history
    47.33%        months_loan_duration
    26.11%        purpose
    24.33%        savings_balance
    18.22%        job
    12.56%        dependents
    12.11%        age
</code></pre>
    <p class="normal">The <strong class="keyWord">attribute usage</strong> statistics in decision tree output refer to the percentage of rows in the training data that use the listed feature to make a final prediction. For example, 100 percent of rows require the <code class="inlineCode">checking_balance</code> feature, because the checking account balance is used at the very first split in the tree. The second split uses <code class="inlineCode">credit_history</code>, but 46.11 percent of rows were already classified as non-default based on the checking account balance. This leaves only 53.89 percent of rows that need to consider the applicant’s credit history. At the bottom of this list, only 12.11 percent of examples require the applicant’s age to make a prediction, which suggests that the applicant’s age is less important than their checking account balance or credit history.</p>
    <p class="normal">This information, along<a id="_idIndexMarker554"/> with the tree structure itself, provides insight into how the model works. Both are readily understood, even without a statistics background. Of course, a model that cannot make accurate predictions is useless even if it is easy to understand, so we will now perform a more formal evaluation of its performance.</p>
    <div class="packt_tip">
      <p class="normal">C5.0 decision tree models can be visualized using the <code class="inlineCode">plot()</code> function, which relies on functionality in the <code class="inlineCode">partykit</code> package. Unfortunately, this is useful only for relatively small decision trees. For example, our decision tree can be visualized by typing <code class="inlineCode">plot(credit_model)</code>, but unless you have a very large display, the resulting plot will likely appear as a jumbled mess due to the large number of nodes and splits in the tree.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-116">Step 4 – evaluating model performance</h2>
    <p class="normal">To apply our decision<a id="_idIndexMarker555"/> tree to the test dataset, we use the <code class="inlineCode">predict()</code> function as shown in the following line of code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_pred <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>credit_model<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This creates a vector of predicted class values, which we can compare to the actual class values using the <code class="inlineCode">CrossTable()</code> function in the <code class="inlineCode">gmodels</code> package. Setting the <code class="inlineCode">prop.c</code> and <code class="inlineCode">prop.r</code> parameters to <code class="inlineCode">FALSE</code> removes the column and row percentages from the table. The remaining percentage (<code class="inlineCode">prop.t</code>) indicates the proportion of records in the cell out of the total number of records:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> library<span class="hljs-punctuation">(</span>gmodels<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> CrossTable<span class="hljs-punctuation">(</span>credit_test<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> credit_pred<span class="hljs-punctuation">,</span>
             prop.chisq <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> prop.c <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> prop.r <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span>
             dnn <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">'actual default'</span><span class="hljs-punctuation">,</span> <span class="hljs-string">'predicted default'</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">This results in the following table:</p>
    <pre class="programlisting con"><code class="hljs-con">               | predicted default 
actual default |        no |       yes | Row Total |
---------------|-----------|-----------|-----------|
            no |        56 |         9 |        65 |
               |     0.560 |     0.090 |           |
---------------|-----------|-----------|-----------|
           yes |        24 |        11 |        35 |
               |     0.240 |     0.110 |           |
---------------|-----------|-----------|-----------|
  Column Total |        80 |        20 |       100 |
---------------|-----------|-----------|-----------|
</code></pre>
    <p class="normal">Out of the 100 loan applications in the test set, our model correctly predicted that 56 did not default and 11 did default, resulting in an accuracy of 67 percent and an error rate of 33 percent. This is somewhat worse than its performance on the training data, but not unexpected, given that a model’s performance is often worse on unseen data. Also note that the model only correctly predicted 11 of the 35 actual loan defaults in the test data, or 31 percent. Unfortunately, this type of error is potentially a very costly mistake, as the bank loses money on each default. Let’s see if we can improve the result with a bit more effort.</p>
    <h2 class="heading-2" id="_idParaDest-117">Step 5 – improving model performance</h2>
    <p class="normal">Our model’s error rate is likely to be too high to deploy it in a real-time credit scoring application. In fact, if the <a id="_idIndexMarker556"/>model had predicted “no default” for every test case, it would have been correct 65 percent of the time—a result barely worse than our model but requiring much less effort! Predicting loan defaults using only 900 training examples seems to be a challenging problem.</p>
    <p class="normal">Making matters even worse, our<a id="_idIndexMarker557"/> model performed especially poorly at identifying applicants who do default on their loans. Luckily, there are a couple of simple ways to adjust the C5.0 algorithm that may help to improve the performance of the model, both overall and for the costlier type of mistakes.</p>
    <h3 class="heading-3" id="_idParaDest-118">Boosting the accuracy of decision trees</h3>
    <p class="normal">One way the C5.0 algorithm improved <a id="_idIndexMarker558"/>upon the C4.5 algorithm was through the addition of <strong class="keyWord">adaptive boosting</strong>. This is a process in which many decision trees are built, and the trees vote on the best class for each example.</p>
    <div class="note">
      <p class="normal">The idea of boosting is based largely upon research by Rob Schapire and Yoav Freund. For more information, try searching the web for their publications or their textbook <em class="italic">Boosting: Foundations and Algorithms, Cambridge, MA, The MIT Press, 2012</em>.</p>
    </div>
    <p class="normal">As boosting can be applied more generally to any machine learning algorithm, it is covered in more detail later in this book in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>. For now, it suffices to say that boosting is rooted in the notion that by combining several weak-performing learners, you can create a team that is much stronger than any of the learners alone. Each of the models has a unique set of strengths and weaknesses, and may be better or worse at certain problems. Using a combination of several learners with complementary strengths and weaknesses can therefore dramatically improve the accuracy of a classifier.</p>
    <p class="normal">The <code class="inlineCode">C5.0()</code> function makes it easy to add boosting to our decision tree. We simply need to add an additional <code class="inlineCode">trials</code> parameter indicating the number of separate decision trees to use in the boosted team. The <code class="inlineCode">trials</code> parameter sets an upper limit; the algorithm will stop adding trees if it recognizes that additional trials do not seem to be improving the accuracy. We’ll start with 10 trials, a number that has become the de facto standard, as research suggests that this reduces error rates on test data by about 25 percent. </p>
    <p class="normal">Aside from the new parameter, the command is similar to before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_boost10 <span class="hljs-operator">&lt;-</span> C5.0<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit_train<span class="hljs-punctuation">,</span>
                         trials <span class="hljs-operator">=</span> <span class="hljs-number">10</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">While examining the<a id="_idIndexMarker559"/> resulting model, we can see that the output now indicates the addition of boosting:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_boost10
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Number of boosting iterations: 10 
Average tree size: 57.3
</code></pre>
    <p class="normal">The new output shows that across the 10 iterations, our tree size shrunk. If you would like, you can see all 10 trees by typing <code class="inlineCode">summary(credit_boost10)</code> at the command prompt. Note that some of these trees, including the tree built for the first trial, have one or more subtrees such as the one denoted <code class="inlineCode">[S1]</code> in the following excerpt of the output:</p>
    <pre class="programlisting con"><code class="hljs-con">dependents &gt; 1: yes (8.8/0.8)
dependents &lt;= 1:
:...years_at_residence &lt;= 1: no (13.4/1.6)
    years_at_residence &gt; 1:
:...age &lt;= 23: yes (11.9/1.6)
    age &gt; 23: [S1]
</code></pre>
    <p class="normal">Notice the line that says that if <code class="inlineCode">age &gt; 23</code>, the result is <code class="inlineCode">[S1]</code>. To determine what this means, we must match <code class="inlineCode">[S1]</code> to the corresponding subtree slightly further down in the output, where we see that the final decision requires several more steps:</p>
    <pre class="programlisting con"><code class="hljs-con">SubTree [S1]
employment_duration in {&lt; 1 year,&gt; 7 years,4 - 7 years,
:                       unemployed}: no (27.7/6.3)
employment_duration = 1 - 4 years:
:...months_loan_duration &gt; 30: yes (7.2)
    months_loan_duration &lt;= 30:
    :...other_credit = bank: yes (2.4)
        other_credit in {none,store}: no (16.6/5.6)
</code></pre>
    <p class="normal">Such subtrees are the result of post-pruning options like subtree raising and subtree replacement, as mentioned earlier in this chapter.</p>
    <p class="normal">The tree’s <code class="inlineCode">summary()</code> output also<a id="_idIndexMarker560"/> shows the tree’s performance on the training data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> summary<span class="hljs-punctuation">(</span>credit_boost10<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">      (a)   (b)    &lt;-classified as
      ----  ----
       633     2    (a): class no
        17   248    (b): class yes
</code></pre>
    <p class="normal">The classifier made 19 mistakes on 900 training examples for an error rate of 2.1 percent. This is quite an improvement over the 13.1 percent training error rate we noted before boosting! However, it remains to be seen whether we see a similar improvement on the test data. Let’s take a look:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_boost_pred10 <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>credit_boost10<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> CrossTable<span class="hljs-punctuation">(</span>credit_test<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> credit_boost_pred10<span class="hljs-punctuation">,</span>
             prop.chisq <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> prop.c <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> prop.r <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span>
             dnn <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">'actual default'</span><span class="hljs-punctuation">,</span> <span class="hljs-string">'predicted default'</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">The resulting table is as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">               | predicted default 
actual default |        no |       yes | Row Total |
---------------|-----------|-----------|-----------|
            no |        58 |         7 |        65 |
               |     0.580 |     0.070 |           |
---------------|-----------|-----------|-----------|
           yes |        19 |        16 |        35 |
               |     0.190 |     0.160 |           |
---------------|-----------|-----------|-----------|
  Column Total |        77 |        23 |       100 |
---------------|-----------|-----------|-----------|
</code></pre>
    <p class="normal">Here, we reduced the total error rate from 33 percent prior to boosting to 26 percent in the boosted model. This may not seem like a large improvement, but it is not too far away from the 25 percent reduction we expected. That said, if boosting can be added this easily, why not apply it by default to every decision tree? The reason is twofold. First, if building a decision tree once takes a great deal of computation time, building many trees may be computationally impractical. Secondly, if the training data is very noisy, then boosting might not result in an improvement at all. Still, if greater accuracy is needed, it’s worth giving boosting a try.</p>
    <p class="normal">On the other hand, the model is still doing poorly at identifying the true defaults, predicting only 46 percent<a id="_idIndexMarker561"/> correctly (16 out of 35) compared to 31 percent (11 of 35) in the simpler model. Let’s investigate one more option to see if we can reduce these types of costly errors.</p>
    <h3 class="heading-3" id="_idParaDest-119">Making some mistakes cost more than others</h3>
    <p class="normal">Giving a loan to an<a id="_idIndexMarker562"/> applicant who is likely to default can be an expensive mistake. One solution to reduce the number of false negatives may be to reject a larger number of borderline applicants under the assumption that the interest that the bank would earn from a risky loan is far outweighed by the massive loss it would incur if the money is not paid back at all.</p>
    <p class="normal">The C5.0 algorithm allows us to assign a penalty to different types of errors in order to discourage a tree from making more costly mistakes. The penalties are designated in a <strong class="keyWord">cost matrix</strong>, which specifies<a id="_idIndexMarker563"/> how many times more costly each error is relative to any other.</p>
    <p class="normal">To begin constructing the cost matrix, we need to start by specifying the dimensions. Since the predicted and actual values can both take two values, <code class="inlineCode">yes</code> or <code class="inlineCode">no</code>, we need to describe a 2x2 matrix using a list of two vectors, each with two values. At the same time, we’ll also name the matrix dimensions to avoid confusion later:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> matrix_dimensions <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">list</span><span class="hljs-punctuation">(</span><span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"no"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"yes"</span><span class="hljs-punctuation">),</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"no"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"yes"</span><span class="hljs-punctuation">))</span>
<span class="hljs-operator">&gt;</span> <span class="hljs-built_in">names</span><span class="hljs-punctuation">(</span>matrix_dimensions<span class="hljs-punctuation">)</span> <span class="hljs-operator">&lt;-</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">"</span><span class="hljs-string">predicted"</span><span class="hljs-punctuation">,</span> <span class="hljs-string">"actual"</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">Examining the new object shows that our dimensions have been set up correctly:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> matrix_dimensions
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">$predicted
[1] "no"  "yes"
$actual
[1] "no"  "yes"
</code></pre>
    <p class="normal">Next, we need to assign the penalty for the various types of errors by supplying four values to fill the matrix. Since R fills a matrix by filling columns one by one from top to bottom, we need to supply the values in a specific order:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Predicted <code class="inlineCode">no</code>, actual <code class="inlineCode">no</code></li>
      <li class="numberedList">Predicted <code class="inlineCode">yes</code>, actual <code class="inlineCode">no</code></li>
      <li class="numberedList">Predicted <code class="inlineCode">no</code>, actual <code class="inlineCode">yes</code></li>
      <li class="numberedList">Predicted <code class="inlineCode">yes</code>, actual <code class="inlineCode">yes</code></li>
    </ol>
    <p class="normal">Suppose we believe that a<a id="_idIndexMarker564"/> loan default costs the bank four times as much as a missed opportunity. Our penalty values then could be defined as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> error_cost <span class="hljs-operator">&lt;-</span> matrix<span class="hljs-punctuation">(</span><span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">),</span> nrow <span class="hljs-operator">=</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span>
                  <span class="hljs-built_in">dimnames</span> <span class="hljs-operator">=</span> matrix_dimensions<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">This creates the following matrix:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> error_cost
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">         actual
predicted no yes
      no   0   4
      yes  1   0
</code></pre>
    <p class="normal">As defined by this matrix, there is no cost assigned when the algorithm classifies a <code class="inlineCode">no</code> or <code class="inlineCode">yes</code> correctly, but a false negative has a cost of <code class="inlineCode">4</code> versus a false positive’s cost of <code class="inlineCode">1</code>. To see how this impacts classification, let’s apply it to our decision tree using the <code class="inlineCode">costs</code> parameter of the <code class="inlineCode">C5.0()</code> function. We’ll otherwise use the same steps as before:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> credit_cost <span class="hljs-operator">&lt;-</span> C5.0<span class="hljs-punctuation">(</span>default <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> credit_train<span class="hljs-punctuation">,</span>
                        costs <span class="hljs-operator">=</span> error_cost<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> credit_cost_pred <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>credit_cost<span class="hljs-punctuation">,</span> credit_test<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> CrossTable<span class="hljs-punctuation">(</span>credit_test<span class="hljs-operator">$</span>default<span class="hljs-punctuation">,</span> credit_cost_pred<span class="hljs-punctuation">,</span>
             prop.chisq <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> prop.c <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span> prop.r <span class="hljs-operator">=</span> <span class="hljs-literal">FALSE</span><span class="hljs-punctuation">,</span>
             dnn <span class="hljs-operator">=</span> <span class="hljs-built_in">c</span><span class="hljs-punctuation">(</span><span class="hljs-string">'actual default'</span><span class="hljs-punctuation">,</span> <span class="hljs-string">'predicted default'</span><span class="hljs-punctuation">))</span>
</code></pre>
    <p class="normal">This produces the following confusion matrix:</p>
    <pre class="programlisting con"><code class="hljs-con">               | predicted default 
actual default |        no |       yes | Row Total |
---------------|-----------|-----------|-----------|
            no |        34 |        31 |        65 |
               |     0.340 |     0.310 |           |
---------------|-----------|-----------|-----------|
           yes |         5 |        30 |        35 |
               |     0.050 |     0.300 |           |
---------------|-----------|-----------|-----------|
  Column Total |        39 |        61 |       100 |
---------------|-----------|-----------|-----------|
</code></pre>
    <p class="normal">Compared to our boosted model, this version makes more mistakes overall: 36 percent error here versus 26 percent in the boosted case. However, the types of mistakes are very different. Where the previous models classified only 31 and 46 percent of defaults correctly, in this model, <em class="italic">30 / 35 = 86%</em> of the actual defaults were correctly predicted to be defaults. This trade-off <a id="_idIndexMarker565"/>resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.</p>
    <h1 class="heading-1" id="_idParaDest-120">Understanding classification rules</h1>
    <p class="normal">Classification rules represent knowledge<a id="_idIndexMarker566"/> in the form of logical if-else statements <a id="_idIndexMarker567"/>that assign a class to unlabeled examples. They are specified in terms of an <strong class="keyWord">antecedent</strong> and a <strong class="keyWord">consequent</strong>, which form <a id="_idIndexMarker568"/>a statement stating that “if <em class="italic">this</em> happens, then <em class="italic">that</em> happens.” The antecedent comprises certain combinations of feature values, while the consequent specifies the class value to assign if the rule’s conditions are met. A simple rule might state, “if the computer is making a clicking sound, then it is about to fail.”</p>
    <p class="normal">Rule learners are closely related siblings of decision tree learners and are often used for similar types of tasks. Like decision<a id="_idIndexMarker569"/> trees, they can be used for applications that generate knowledge for future action, such as:</p>
    <ul>
      <li class="bulletList">Identifying conditions that lead to hardware failure in mechanical devices</li>
      <li class="bulletList">Describing the key characteristics of groups of people for customer segmentation</li>
      <li class="bulletList">Finding conditions that precede large drops or increases in the prices of shares on the stock market</li>
    </ul>
    <p class="normal">Rule learners do have some distinct contrasts relative to decision trees. Unlike a tree, which must be followed through a series of branching decisions, rules are propositions that can be read much like independent statements of fact. Additionally, for reasons that will be discussed later, the results of a rule learner can be more simple, direct, and easier to understand than a decision tree built on the same data.</p>
    <div class="packt_tip">
      <p class="normal">You may have already realized that the branches of decision trees are almost identical to if-else statements of rule learning algorithms, and in fact, rules can be generated from trees. So, why bother with a separate group of rule learning algorithms? Read further to discover the nuances that differentiate the two approaches.</p>
    </div>
    <p class="normal">Rule learners are generally<a id="_idIndexMarker570"/> applied to problems where the features are primarily or entirely nominal. They do well at identifying rare events, even if the rare event occurs only for a very specific interaction among feature values.</p>
    <h2 class="heading-2" id="_idParaDest-121">Separate and conquer</h2>
    <p class="normal">Rule learning classification algorithms<a id="_idIndexMarker571"/> utilize a heuristic known as <strong class="keyWord">separate and conquer</strong>. The process involves identifying a rule that covers a subset of <a id="_idIndexMarker572"/>examples in the training data and then separating this partition from the remaining data. As rules are added, additional subsets of data are separated until the entire dataset has been covered and no more examples remain. Although separate and conquer is in many ways similar to the divide and conquer heuristic covered earlier, it differs in subtle ways that will become clear soon.</p>
    <p class="normal">One way to imagine the rule learning process of separate and conquer is to imagine drilling down into the data by creating increasingly specific rules to identify class values. Suppose you were tasked with creating rules to identify whether or not an animal is a mammal. You could depict the set of all animals as a large space, as shown in the following diagram:</p>
    <figure class="mediaobject"><img alt="Venn diagram  Description automatically generated with medium confidence" src="../Images/B17290_05_08.png"/></figure>
    <p class="packt_figref">Figure 5.8: A rule learning algorithm may help divide animals into groups of mammals and non-mammals</p>
    <p class="normal">A rule learner begins by using the available features to find homogeneous groups. For example, using a feature that indicates whether the species travels via land, sea, or air, the first rule might suggest that <a id="_idIndexMarker573"/>any land-based animals are mammals:</p>
    <figure class="mediaobject"><img alt="Diagram, venn diagram  Description automatically generated" src="../Images/B17290_05_09.png"/></figure>
    <p class="packt_figref">Figure 5.9: A potential rule considers animals that travel on land to be mammals</p>
    <p class="normal">Do you notice any problems with this rule? If you’re an animal lover, you might have realized that frogs are amphibians, not mammals. Therefore, our rule needs to be a bit more specific. Let’s drill down further by suggesting that mammals must walk on land and have a tail:</p>
    <figure class="mediaobject"><img alt="Diagram, venn diagram  Description automatically generated" src="../Images/B17290_05_10.png"/></figure>
    <p class="packt_figref">Figure 5.10: A more specific rule suggests that animals that walk on land and have tails are mammals</p>
    <p class="normal">As shown in the previous figure, the new rule results in a subset of animals that are entirely mammals. Thus, the subset of mammals can be separated from the other data and the frogs are returned to the pool of remaining animals—no pun intended!</p>
    <p class="normal">An additional rule can<a id="_idIndexMarker574"/> be defined to separate out the bats, the only remaining mammal. A potential feature distinguishing bats from the remaining animals would be the presence of fur. Using a rule built around this feature, we have then correctly identified all the animals:</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_05_11.png"/></figure>
    <p class="packt_figref">Figure 5.11: A rule stating that animals with fur are mammals perfectly classifies the remaining animals</p>
    <p class="normal">At this point, since all training instances have been classified, the rule learning process would stop. We learned a total of three rules:</p>
    <ul>
      <li class="bulletList">Animals that walk on land and have tails are mammals</li>
      <li class="bulletList">If the animal does not have fur, it is not a mammal</li>
      <li class="bulletList">Otherwise, the animal is a mammal</li>
    </ul>
    <p class="normal">The previous example illustrates how rules gradually consume larger and larger segments of data to eventually classify all instances. As the rules seem to cover portions of the data, separate and conquer<a id="_idIndexMarker575"/> algorithms are also known as <strong class="keyWord">covering algorithms</strong>, and the resulting rules are called covering rules. In the next section, we will learn how covering rules are applied in practice by examining a simple rule learning algorithm. We will then examine a more complex rule learner and apply both algorithms to a real-world problem.</p>
    <h2 class="heading-2" id="_idParaDest-122">The 1R algorithm</h2>
    <p class="normal">Suppose a television game show has an animal hidden behind a large curtain. You are asked to guess whether it is a mammal and if correct, you win a large cash prize. You are not given any clues about the<a id="_idIndexMarker576"/> animal’s characteristics, but you know<a id="_idIndexMarker577"/> that a very small portion of the world’s animals are mammals. Consequently, you guess “non-mammal.” What do you think about your chances of winning?</p>
    <p class="normal">Choosing this, of course, maximizes your odds of winning the prize, as it is the most likely outcome under the assumption the animal was chosen at random. Clearly, this game show is a bit ridiculous, but it demonstrates<a id="_idIndexMarker578"/> the simplest classifier, <strong class="keyWord">ZeroR</strong>, which is a rule learner that considers no features and literally learns no rules (hence the name). For every unlabeled example, regardless of the values of its features, it predicts the most common class. This algorithm has very little real-world utility, except that it provides a simple baseline for comparison to other, more sophisticated, rule learners.</p>
    <p class="normal">The <strong class="keyWord">1R algorithm</strong> (also known as <strong class="keyWord">One Rule</strong> or <strong class="keyWord">OneR</strong>), improves over ZeroR by selecting a single rule. Although this may seem overly simplistic, it tends to perform better than you might expect. As demonstrated in empirical studies, the accuracy of this algorithm can approach that of much more sophisticated algorithms for many real-world tasks.</p>
    <div class="note">
      <p class="normal">For an in-depth look at the surprising performance of 1R, see <em class="italic">Very Simple Classification Rules Perform Well on Most Commonly Used Datasets, Holte, RC, Machine Learning, 1993, Vol. 11, pp. 63-91</em>.</p>
    </div>
    <p class="normal">The strengths and weaknesses of the 1R algorithm are <a id="_idIndexMarker579"/>shown in the following <a id="_idIndexMarker580"/>table:</p>
    <table class="table-container" id="table002-3">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Generates a single, easy-to- understand, human-readable rule</li>
              <li class="bulletList">Often performs surprisingly well</li>
              <li class="bulletList">Can serve as a benchmark for more complex algorithms</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Uses only a single feature</li>
              <li class="bulletList">Probably overly simplistic</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The way this algorithm works is simple. For each feature, 1R divides the data into groups with similar values of the feature. Then, for each segment, the algorithm predicts the majority class. The error rate for the rule based on each feature is calculated and the rule with the fewest errors is chosen as the one rule.</p>
    <p class="normal">The following tables show how this would work for the animal data we looked at earlier:</p>
    <figure class="mediaobject"><img alt="Table  Description automatically generated" src="../Images/B17290_05_12.png"/></figure>
    <p class="packt_figref">Figure 5.12: The 1R algorithm chooses the single rule with the lowest misclassification rate</p>
    <p class="normal">For the <em class="italic">Travels By</em> feature, the dataset was divided into three groups: <em class="italic">Air</em>, <em class="italic">Land</em>, and <em class="italic">Sea</em>. Animals in the <em class="italic">Air</em> and <em class="italic">Sea</em> groups were predicted to be non-mammal, while animals in the <em class="italic">Land</em> group were <a id="_idIndexMarker581"/>predicted to be mammals. This resulted in two<a id="_idIndexMarker582"/> errors: bats and frogs.</p>
    <p class="normal">The <em class="italic">Has Fur</em> feature divided animals into two groups. Those with fur were predicted to be mammals, while those without fur were not. Three errors were counted: pigs, elephants, and rhinos. As the <em class="italic">Travels By</em> feature resulted in fewer errors, the 1R algorithm would return the following:</p>
    <ul>
      <li class="bulletList">If the animal travels by air, it is not a mammal</li>
      <li class="bulletList">If the animal travels by land, it is a mammal</li>
      <li class="bulletList">If the animal travels by sea, it is not a mammal</li>
    </ul>
    <p class="normal">The algorithm stops here, having found the single most important rule.</p>
    <p class="normal">Obviously, this rule learning algorithm may be too basic for some tasks. Would you want a medical diagnosis system to consider only a single symptom, or an automated driving system to stop or<a id="_idIndexMarker583"/> accelerate your car based on only a single factor? For these types of tasks, a more sophisticated rule learner might be useful. We’ll learn about one in the following section.</p>
    <h2 class="heading-2" id="_idParaDest-123">The RIPPER algorithm</h2>
    <p class="normal">Early rule learning algorithms were plagued by a couple of problems. First, they were notorious for being slow, which made <a id="_idIndexMarker584"/>them ineffective for the increasing<a id="_idIndexMarker585"/> number of large datasets. Second, they were often prone to being inaccurate on noisy data.</p>
    <p class="normal">A first step toward solving these problems was proposed in 1994 by Johannes Furnkranz and Gerhard Widmer. Their <strong class="keyWord">incremental reduced error pruning</strong> (<strong class="keyWord">IREP</strong>) <strong class="keyWord">algorithm</strong> uses a combination of pre-pruning and post-pruning<a id="_idIndexMarker586"/> methods that grow very complex rules and prune them before separating the instances from the full dataset. Although this strategy helped the performance of rule learners, decision trees often still performed better.</p>
    <div class="note">
      <p class="normal">For more information on IREP, see <em class="italic">Incremental Reduced Error Pruning, Furnkranz, J and Widmer, G, Proceedings of the 11th International Conference on Machine Learning, 1994, pp. 70-77</em>.</p>
    </div>
    <p class="normal">Rule learners took another step forward in 1995 when<a id="_idIndexMarker587"/> William W. Cohen introduced the <strong class="keyWord">repeated incremental pruning to produce error reduction</strong> (<strong class="keyWord">RIPPER</strong>) <strong class="keyWord">algorithm</strong>, which improved upon IREP to generate rules that match or exceed the performance of decision trees.</p>
    <div class="note">
      <p class="normal">For more detail on RIPPER, see <em class="italic">Fast Effective Rule Induction, Cohen, WW, Proceedings of the 12th International Conference on Machine Learning, 1995, pp. 115-123</em>.</p>
    </div>
    <p class="normal">As outlined in the following table, the strengths and weaknesses of RIPPER are generally comparable to decision trees. The chief benefit is that they may result in a slightly more parsimonious model.</p>
    <table class="table-container" id="table003-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Strengths</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Weaknesses</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <ul>
              <li class="bulletList">Generates easy-to-understand, human-readable rules</li>
              <li class="bulletList">Efficient on large and noisy datasets</li>
              <li class="bulletList">Generally, produces a simpler model than a comparable decision tree</li>
            </ul>
          </td>
          <td class="table-cell">
            <ul>
              <li class="bulletList">May result in rules that seem to defy common sense or expert knowledge</li>
              <li class="bulletList">Not ideal for working with numeric data</li>
              <li class="bulletList">Might not perform as well as more complex models</li>
            </ul>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Having evolved<a id="_idIndexMarker588"/> from several iterations of<a id="_idIndexMarker589"/> rule learning algorithms, the RIPPER <a id="_idIndexMarker590"/>algorithm is a patchwork of efficient heuristics for rule learning. Due to its complexity, a discussion of the implementation details is beyond the scope of this book. However, it can be understood in general terms as a three-step process:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Grow</li>
      <li class="numberedList">Prune</li>
      <li class="numberedList">Optimize</li>
    </ol>
    <p class="normal">The growing phase uses the separate and conquer technique to greedily add conditions to a rule until it perfectly classifies a subset of data or runs out of attributes for splitting. Like decision trees, the information gain criterion is used to identify the next splitting attribute. When increasing<a id="_idIndexMarker591"/> a rule’s specificity no longer reduces entropy, the rule is immediately pruned. Steps one and two are repeated until reaching a stopping criterion, at which point the entire set of rules is optimized using a variety of heuristics.</p>
    <p class="normal">The RIPPER algorithm can create much more complex rules than the 1R algorithm can, as it can consider more than one feature. This means that it can create rules with multiple antecedents such as “if an animal flies and has fur, then it is a mammal.” This improves the algorithm’s ability to model complex data, but just like decision trees, it means that the rules can quickly become difficult to comprehend.</p>
    <div class="note">
      <p class="normal">The evolution of classification rule learners didn’t stop with RIPPER. New rule learning algorithms are being proposed rapidly. A survey of literature shows algorithms called IREP++, SLIPPER, TRIPPER, among many others.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-124">Rules from decision trees</h2>
    <p class="normal">Classification rules can also be obtained directly from decision trees. Beginning at a leaf node and following the branches back to<a id="_idIndexMarker592"/> the root, you obtain a series of decisions. These can be combined into a single rule. The following figure shows<a id="_idIndexMarker593"/> how rules could be constructed from the decision tree for predicting movie success:</p>
    <figure class="mediaobject"><img alt="Diagram, shape, polygon  Description automatically generated" src="../Images/B17290_05_13.png"/></figure>
    <p class="packt_figref">Figure 5.13: Rules can be generated from decision trees by following paths from the root node to each leaf node</p>
    <p class="normal">Following the paths from the<a id="_idIndexMarker594"/> root node down to each leaf, the rules would be:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">If the number of celebrities is low, then the movie will be a <em class="italic">Box Office Bust</em></li>
      <li class="numberedList">If the number of celebrities is high and the budget is high, then the movie will be a <em class="italic">Mainstream Hit</em></li>
      <li class="numberedList">If the number of celebrities is high and the budget is low, then the movie will be a <em class="italic">Critical Success</em></li>
    </ol>
    <p class="normal">For reasons that will be made clear in the following section, the chief downside to using a decision tree to generate rules is that the resulting rules are often more complex than those learned by a rule learning algorithm. The divide and conquer strategy employed by decision trees biases the results differently to that of a rule learner. On the other hand, it is sometimes more computationally efficient to generate rules from trees.</p>
    <div class="packt_tip">
      <p class="normal">The <code class="inlineCode">C5.0()</code> function in the <code class="inlineCode">C50</code> package will generate a model using classification rules if you specify <code class="inlineCode">rules = TRUE</code> when training the model.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-125">What makes trees and rules greedy?</h2>
    <p class="normal">Decision trees and rule<a id="_idIndexMarker595"/> learners are known as <strong class="keyWord">greedy learners</strong> because they use data on a <a id="_idIndexMarker596"/>first-come, first-served basis. Both the divide and conquer heuristic used by decision trees and the separate and conquer heuristic used by rule learners attempt to make partitions one at a time, finding the most homogeneous partition first, followed by the next best, and so on, until all examples have been classified.</p>
    <p class="normal">The downside to the greedy approach is that greedy algorithms are not guaranteed to generate the optimal, most accurate, or smallest number of rules for a particular dataset. By taking the low-hanging fruit early, a greedy learner may quickly find a single rule that is accurate for one subset of data; however, in doing so, the learner may miss the opportunity to develop a more nuanced set of rules with better overall accuracy on the entire set of data. However, without using the greedy approach to rule learning, it is likely that for all but the <a id="_idIndexMarker597"/>smallest of datasets, rule learning would be computationally infeasible.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_05_14.png"/></figure>
    <p class="packt_figref">Figure 5.14: Both decision trees and classification rule learners are greedy algorithms</p>
    <p class="normal">Though both trees and rules employ greedy learning heuristics, there are subtle differences in how they build rules. Perhaps the best way to distinguish them is to note that once divide and conquer splits on a feature, the partitions created by the split may not be re-conquered, only further subdivided. In this way, a tree is permanently limited by its history of past decisions. In contrast, once separate and conquer finds a rule, any examples not covered by all the rule’s conditions may be re-conquered.</p>
    <p class="normal">To illustrate this contrast, consider the previous case in which we built a rule learner to determine whether an animal was a mammal. The rule learner identified three rules that perfectly classify the example animals:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Animals that walk on land and have tails are mammals (bears, cats, dogs, elephants, pigs, rabbits, rats, and rhinos)</li>
      <li class="numberedList">If the animal does not have fur, it is not a mammal (birds, eels, fish, frogs, insects, and sharks)</li>
      <li class="numberedList">Otherwise, the animal is a mammal (bats)</li>
    </ol>
    <p class="normal">In contrast, a decision tree<a id="_idIndexMarker598"/> built on the same data might have come up with four rules to achieve the same perfect classification:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">If an animal walks on land and has a tail, then it is a mammal (bears, cats, dogs, elephants, pigs, rabbits, rats, and rhinos)</li>
      <li class="numberedList">If an animal walks on land and does not have a tail, then it is not a mammal (frogs)</li>
      <li class="numberedList">If the animal does not walk on land and has fur, then it is a mammal (bats)</li>
      <li class="numberedList">If the animal does not walk on land and does not have fur, then it is not a mammal (birds, insects, sharks, fish, and eels)</li>
    </ol>
    <p class="normal">The different result across these two approaches has to do with what happens to the frogs after they are separated by the “walk on land” decision. Where the rule learner allows frogs to be re-conquered by the “do not have fur” decision, the decision tree cannot modify the existing partitions and therefore must place the frog into its own rule.</p>
    <figure class="mediaobject"><img alt="Diagram  Description automatically generated" src="../Images/B17290_05_15.png"/></figure>
    <p class="packt_figref">Figure 5.15: The handling of frogs distinguishes the divide and conquer and separate and conquer heuristics. The latter approach allows the frogs to be re-conquered by later rules.</p>
    <p class="normal">On the one hand, because rule learners can reexamine cases that were considered but ultimately not covered as part of prior rules, rule learners often find a more parsimonious set of rules than those<a id="_idIndexMarker599"/> generated by decision trees. On the other hand, this reuse of data means that the computational cost of rule learners may be somewhat higher than for decision trees.</p>
    <h1 class="heading-1" id="_idParaDest-126">Example – identifying poisonous mushrooms with rule learners</h1>
    <p class="normal">Each year, many people fall ill and some<a id="_idIndexMarker600"/> even die from ingesting poisonous wild mushrooms. Since many mushrooms are very similar to each other in appearance, occasionally, even experienced mushroom gatherers are poisoned.</p>
    <p class="normal">Unlike the identification of harmful plants, such as poison oak or poison ivy, there are no clear rules like “leaves of three, let them be” for identifying whether a wild mushroom is poisonous or edible. </p>
    <p class="normal">Complicating matters, many traditional rules such as “poisonous mushrooms are brightly colored” provide dangerous or misleading information. If simple, clear, and consistent rules were available for identifying poisonous mushrooms, they could save the lives of foragers.</p>
    <p class="normal">As one of the strengths of rule learning algorithms is the fact that they generate easy-to-understand rules, they seem like an appropriate fit for this classification task. However, the rules will only be as useful as they are accurate.</p>
    <h2 class="heading-2" id="_idParaDest-127">Step 1 – collecting data</h2>
    <p class="normal">To identify rules for <a id="_idIndexMarker601"/>distinguishing poisonous mushrooms, we will use the Mushroom dataset by Jeff Schlimmer of Carnegie Mellon University. The raw dataset is available freely from the UCI Machine Learning Repository (<a href="http://archive.ics.uci.edu/ml"><span class="url">http://archive.ics.uci.edu/ml</span></a>).</p>
    <p class="normal">The dataset includes information on 8,124 mushroom samples from 23 species of gilled mushrooms listed in the <em class="italic">Audubon Society Field Guide to North American Mushrooms</em> (1981). In the field guide, each of the mushroom species is identified as “definitely edible,” “definitely poisonous,” or “likely poisonous, and not recommended to be eaten.” For the purposes of this dataset, the latter group was combined with the “definitely poisonous” group to make two classes: poisonous and non-poisonous. The data dictionary available on the UCI website describes the 22 features of the mushroom samples, including characteristics <a id="_idIndexMarker602"/>such as cap shape, cap color, odor, gill size and color, stalk shape, and habitat.</p>
    <div class="packt_tip">
      <p class="normal">This chapter uses a slightly modified version of the mushroom data. If you plan on following along with the example, download the <code class="inlineCode">mushrooms.csv</code> file from the Packt Publishing GitHub repository for this chapter and save it to your R working directory.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-128">Step 2 – exploring and preparing the data</h2>
    <p class="normal">We begin by using <code class="inlineCode">read.csv()</code> to import the data for our analysis. Since all 22 features and the target class are nominal, we<a id="_idIndexMarker603"/> will set <code class="inlineCode">stringsAsFactors = TRUE</code> to take advantage of the automatic factor conversion:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushrooms <span class="hljs-operator">&lt;-</span> read.csv<span class="hljs-punctuation">(</span><span class="hljs-string">"mushrooms.csv"</span><span class="hljs-punctuation">,</span> stringsAsFactors <span class="hljs-operator">=</span> <span class="hljs-literal">TRUE</span><span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">The output of the <code class="inlineCode">str(mushrooms)</code> command notes that the data contains 8,124 observations of 23 variables as the data <a id="_idIndexMarker604"/>dictionary had described. While most of the <code class="inlineCode">str()</code> output is unremarkable, one feature is worth mentioning. Do you notice anything peculiar about the <code class="inlineCode">veil_type</code> variable in the following line?</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">$</span> veil_type <span class="hljs-operator">:</span> Factor w<span class="hljs-operator">/</span> <span class="hljs-number">1</span> level <span class="hljs-string">"partial"</span><span class="hljs-operator">:</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> ...
</code></pre>
    <p class="normal">If you found it to be strange that a factor has only one level, you are correct. The data dictionary lists two levels for this feature: <code class="inlineCode">partial</code> and <code class="inlineCode">universal</code>; however, all examples in our data are classified as <code class="inlineCode">partial</code>. It is likely that this data element was somehow coded incorrectly. In any case, since the veil type does not vary across samples, it does not provide any useful information for prediction. We will drop this variable from our analysis using the following command:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushrooms<span class="hljs-operator">$</span>veil_type <span class="hljs-operator">&lt;-</span> <span class="hljs-literal">NULL</span>
</code></pre>
    <p class="normal">By assigning <code class="inlineCode">NULL</code> to the <code class="inlineCode">veil_type</code> vector, R eliminates the feature from the mushrooms data frame.</p>
    <p class="normal">Before going much further, we should take a quick look at the distribution of the mushroom <code class="inlineCode">type</code> variable in our dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>mushrooms<span class="hljs-operator">$</span>type<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">   edible poisonous
     4208      3916
</code></pre>
    <p class="normal">About 52 percent of the mushroom samples are edible, while 48 percent are poisonous.</p>
    <p class="normal">For the purposes of this <a id="_idIndexMarker605"/>experiment, we will consider the 8,214 samples in the mushroom data to be an exhaustive set of all the possible wild mushrooms. This is an important assumption<a id="_idIndexMarker606"/> because it means that we do not need to hold some samples out of the training data for testing purposes. We are not trying to develop rules that cover unforeseen types of mushrooms; we are merely trying to find rules that accurately depict the complete set of known mushroom types. Therefore, we can build and test the model on the same data.</p>
    <h2 class="heading-2" id="_idParaDest-129">Step 3 – training a model on the data</h2>
    <p class="normal">If we trained a hypothetical ZeroR classifier on this data, what would it predict? Since ZeroR ignores all of the features and simply predicts the target’s mode, in plain language, its rule would state that “all mushrooms are edible.” Obviously, this is not a very helpful classifier because it <a id="_idIndexMarker607"/>would leave a mushroom gatherer sick or dead for nearly half of the mushroom samples! Our rules will need to do much better than this benchmark in order to provide safe advice that can be published. At the same time, we need simple rules that are easy to remember.</p>
    <p class="normal">Since simple rules can still be useful, let’s see how a very simple rule learner performs on the mushroom data. Toward this end, we will apply the 1R classifier, which will identify the single feature that is the most predictive of the target class and use this feature to construct a rule.</p>
    <p class="normal">We will use the 1R implementation found in the <code class="inlineCode">OneR</code> package by Holger von Jouanne-Diedrich at the Aschaffenburg University of Applied Sciences. This is a relatively new package, which implements 1R in native R code for speed and ease of use. If you don’t already have this package, it can be installed using the <code class="inlineCode">install.packages("OneR")</code> command and loaded by typing <code class="inlineCode">library(OneR)</code>.</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_05_16.png"/></figure>
    <p class="packt_figref">Figure 5.16: 1R classification rule syntax</p>
    <p class="normal">Like C5.0, the <code class="inlineCode">OneR()</code> function <a id="_idIndexMarker608"/>uses the R formula syntax to specify the model to be trained. Using the formula <code class="inlineCode">type ~ .</code> with <code class="inlineCode">OneR()</code> allows our first rule learner to consider all possible features in the mushroom data when predicting the mushroom type:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushroom_1R <span class="hljs-operator">&lt;-</span> OneR<span class="hljs-punctuation">(</span>type <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> mushrooms<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">To examine the rules it created, we can type the name of the classifier object:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushroom_1R
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Call:
OneR.formula(formula = type ~ ., data = mushrooms)
Rules:
If odor = almond   then type = edible
If odor = anise    then type = edible
If odor = creosote then type = poisonous
If odor = fishy    then type = poisonous
If odor = foul     then type = poisonous
If odor = musty    then type = poisonous
If odor = none     then type = edible
If odor = pungent  then type = poisonous
If odor = spicy    then type = poisonous
Accuracy:
8004 of 8124 instances classified correctly (98.52%)
</code></pre>
    <p class="normal">Examining the output, we see that the <code class="inlineCode">odor</code> feature was selected for rule generation. The categories of <code class="inlineCode">odor</code>, such as <code class="inlineCode">almond</code>, <code class="inlineCode">anise</code>, and so on, specify rules for whether the mushroom is likely to be <code class="inlineCode">edible</code> or <code class="inlineCode">poisonous</code>. For instance, if the mushroom smells <code class="inlineCode">fishy</code>, <code class="inlineCode">foul</code>, <code class="inlineCode">musty</code>, <code class="inlineCode">pungent</code>, <code class="inlineCode">spicy</code>, or like <code class="inlineCode">creosote</code>, the mushroom is likely to be poisonous. On the other hand, mushrooms with more pleasant smells, like <code class="inlineCode">almond</code> and <code class="inlineCode">anise</code>, and those with no<a id="_idIndexMarker609"/> smell at all, are predicted to be <code class="inlineCode">edible</code>. For the purposes of a field guide for mushroom gathering, these rules could be summarized in a simple rule of thumb: “if the mushroom smells unappetizing, then it is likely to be poisonous.”</p>
    <h2 class="heading-2" id="_idParaDest-130">Step 4 – evaluating model performance</h2>
    <p class="normal">The last line of the output <a id="_idIndexMarker610"/>notes that the rules correctly predict the edibility for 8,004 of the 8,124 mushroom samples, or nearly 99 percent. Anything short of perfection, however, runs the risk of poisoning someone if the model were to classify a poisonous mushroom as edible.</p>
    <p class="normal">To determine whether this occurred, let’s examine a confusion matrix of the predicted versus actual values. This requires us to first generate the 1R model’s predictions, then compare the predictions to the actual values:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushroom_1R_pred <span class="hljs-operator">&lt;-</span> predict<span class="hljs-punctuation">(</span>mushroom_1R<span class="hljs-punctuation">,</span> mushrooms<span class="hljs-punctuation">)</span>
<span class="hljs-operator">&gt;</span> table<span class="hljs-punctuation">(</span>actual <span class="hljs-operator">=</span> mushrooms<span class="hljs-operator">$</span>type<span class="hljs-punctuation">,</span> predicted <span class="hljs-operator">=</span> mushroom_1R_pred<span class="hljs-punctuation">)</span>
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">           predicted
actual      edible poisonous
  edible      4208         0
  poisonous    120      3796
</code></pre>
    <p class="normal">Here, we can see where our rules went wrong. The table’s columns indicate the predicted edibility of the mushroom while the table’s rows divide the 4,208 actually edible mushrooms and the 3,916 actually poisonous mushrooms. Examining the table, we can see that although the 1R classifier did not classify any edible mushrooms as poisonous, it did classify 120 poisonous mushrooms as edible, which makes for an incredibly dangerous mistake!</p>
    <p class="normal">Considering that the learner<a id="_idIndexMarker611"/> utilized only a single feature, it did reasonably well; if you avoid unappetizing smells when foraging for mushrooms, you will almost always avoid a trip to the hospital. That said, close does not cut it when lives are involved, not to mention the field guide publisher might not be happy about the prospect of a lawsuit when its readers fall ill. Let’s see if we can add a few more rules and develop an even better classifier.</p>
    <h2 class="heading-2" id="_idParaDest-131">Step 5 – improving model performance</h2>
    <p class="normal">For a more <a id="_idIndexMarker612"/>sophisticated rule learner, we will use <code class="inlineCode">JRip()</code>, a Java-based implementation of the RIPPER algorithm. The <code class="inlineCode">JRip()</code> function is included in the <code class="inlineCode">RWeka</code> package, which gives R access to the machine learning algorithms in the Java-based Weka software application by Ian H. Witten and Eibe Frank.</p>
    <div class="note">
      <p class="normal">Weka is a popular open source and full-featured graphical application for performing data mining and machine learning tasks—one of the earliest such tools. For more information on Weka, see <a href="http://www.cs.waikato.ac.nz/~ml/weka/"><span class="url">http://www.cs.waikato.ac.nz/~ml/weka/</span></a>.</p>
    </div>
    <p class="normal">The <code class="inlineCode">RWeka</code> package depends on the <code class="inlineCode">rJava</code> package, which itself requires the <strong class="keyWord">Java development kit </strong>(<strong class="keyWord">JDK</strong>) to be installed on the<a id="_idIndexMarker613"/> host computer before installation. This can be downloaded from <code class="inlineCode">https://www.java.com/</code> and installed using the instructions specific to your platform. After installing Java, use the <code class="inlineCode">install.packages("RWeka")</code> command to install <code class="inlineCode">RWeka</code> and its dependencies, then load the <code class="inlineCode">RWeka</code> package using the <code class="inlineCode">library(RWeka)</code> command.</p>
    <div class="packt_tip">
      <p class="normal">Java is a set of programming tools available at no cost, which allows the development and use of cross-platform applications such as Weka. Although it was once included by default with many computers, this is no longer true. Unfortunately, it can be tricky to install, especially on Apple computers. If you are having trouble, be sure you have the latest Java version. Additionally, on Microsoft Windows, you may need to set your environment variables like <code class="inlineCode">JAVA_HOME</code> correctly, and check your <code class="inlineCode">PATH</code> settings (search the web for details). On macOS or Linux computers, you may also try executing <code class="inlineCode">R CMD javareconf</code> from a terminal window then install the R package <code class="inlineCode">rJava</code> from source using the R command <code class="inlineCode">install.packages("rJava", type = "source")</code>. If all else fails, you <a id="_idIndexMarker614"/>may try a free Posit Cloud account (<a href="https://posit.cloud/"><span class="url">https://posit.cloud/</span></a>), which offers an RStudio environment in which Java is already installed.</p>
    </div>
    <p class="normal">With <code class="inlineCode">rJava</code> and <code class="inlineCode">RWeka</code> installed, the <a id="_idIndexMarker615"/>process of training a <code class="inlineCode">JRip()</code> model is very similar to training a <code class="inlineCode">OneR()</code> model, as shown in the syntax box that follows. This is one of the pleasant benefits of the R formula interface: the syntax is consistent across algorithms, which makes it simple to compare a variety of models.</p>
    <figure class="mediaobject"><img alt="Text  Description automatically generated" src="../Images/B17290_05_17.png"/></figure>
    <p class="packt_figref">Figure 5.17: RIPPER classification rule syntax</p>
    <p class="normal">Let’s train the <code class="inlineCode">JRip()</code> rule learner as <a id="_idIndexMarker616"/>we did with <code class="inlineCode">OneR()</code>, allowing it to find rules among all of the available features:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushroom_JRip <span class="hljs-operator">&lt;-</span> JRip<span class="hljs-punctuation">(</span>type <span class="hljs-operator">~</span> .<span class="hljs-punctuation">,</span> data <span class="hljs-operator">=</span> mushrooms<span class="hljs-punctuation">)</span>
</code></pre>
    <p class="normal">To examine the rules, type the name of the classifier:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-operator">&gt;</span> mushroom_JRip
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">JRIP rules:
===========
(odor = foul) =&gt; type=poisonous (2160.0/0.0)
(gill_size = narrow) and (gill_color = buff)
  =&gt; type=poisonous (1152.0/0.0)
(gill_size = narrow) and (odor = pungent)
  =&gt; type=poisonous (256.0/0.0)
(odor = creosote) =&gt; type=poisonous (192.0/0.0)
(spore_print_color = green) =&gt; type=poisonous (72.0/0.0)
(stalk_surface_below_ring = scaly)
  and (stalk_surface_above_ring = silky)
    =&gt; type=poisonous (68.0/0.0)
(habitat = leaves) and (gill_attachment = free)
  and (population = clustered)
  =&gt; type=poisonous (16.0/0.0)
=&gt; type=edible (4208.0/0.0)
Number of Rules : 8
</code></pre>
    <p class="normal">The <code class="inlineCode">JRip()</code> classifier learned a total of eight rules from the mushroom data.</p>
    <p class="normal">An easy way to read these <a id="_idIndexMarker617"/>rules is to think of them as a list of if-else statements, similar to programming logic. The first three rules could be expressed as:</p>
    <ul>
      <li class="bulletList">If the odor is foul, then the mushroom type is poisonous</li>
      <li class="bulletList">If the gill size is narrow and the gill color is buff, then the mushroom type is poisonous</li>
      <li class="bulletList">If the gill size is narrow and the odor is pungent, then the mushroom type is poisonous</li>
    </ul>
    <p class="normal">Finally, the eighth rule implies that any mushroom sample that was not covered by the preceding seven rules is edible. Following the example of our programming logic, this can be read as:</p>
    <ul>
      <li class="bulletList">Else, the mushroom is edible</li>
    </ul>
    <p class="normal">The numbers next to each rule indicate the number of instances covered by the rule and a count of misclassified instances. Notably, there were no misclassified mushroom samples using these eight rules. As a result, the number of instances covered by the last rule is exactly equal to the number of edible mushrooms in the data (N = 4,208).</p>
    <p class="normal">The following figure provides a rough illustration of how the rules are applied to the mushroom data. If you imagine the large oval as containing all mushroom species, the rule learner identifies features, or sets of features, that separate homogeneous segments from the larger group. First, the algorithm found a large group of poisonous mushrooms uniquely distinguished by their foul odor. Next, it found smaller and more specific groups of poisonous mushrooms. By identifying covering rules for each of the varieties of poisonous mushrooms, all remaining mushrooms were edible. </p>
    <p class="normal">Thanks to Mother Nature, each variety of <a id="_idIndexMarker618"/>mushrooms was unique enough that the classifier was able to achieve 100 percent accuracy.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B17290_05_18.png"/></figure>
    <p class="packt_figref">Figure 5.18: A sophisticated rule learning algorithm identified rules to perfectly cover all types of poisonous mushrooms</p>
    <h1 class="heading-1" id="_idParaDest-132">Summary</h1>
    <p class="normal">This chapter covered two classification methods that use so-called “greedy” algorithms to partition the data according to feature values. Decision trees use a divide and conquer strategy to create flowchart-like structures, while rule learners separate and conquer data to identify logical if-else rules. Both methods produce models that can be interpreted without a statistical background.</p>
    <p class="normal">One popular and highly configurable decision tree algorithm is C5.0. We used the C5.0 algorithm to create a tree to predict whether a loan applicant will default. Using options for boosting and cost-sensitive errors, we were able to improve our accuracy and avoid risky loans that could cost the bank more money.</p>
    <p class="normal">We also used two rule learners, 1R and RIPPER, to develop rules for identifying poisonous mushrooms. The 1R algorithm used a single feature to achieve 99 percent accuracy in identifying potentially fatal mushroom samples. On the other hand, the set of eight rules generated by the more sophisticated RIPPER algorithm correctly identified the edibility of every mushroom.</p>
    <p class="normal">This merely scratches the surface of how trees and rules can be used. The next chapter, <em class="chapterRef">Chapter 6</em>, <em class="italic">Forecasting Numeric Data – Regression Methods</em>, describes techniques known as regression trees and model trees, which use decision trees for numeric prediction rather than classification. In <em class="chapterRef">Chapter 8</em>, <em class="italic">Finding Patterns – Market Basket Analysis Using Association Rules</em>, we will see how association rules—a close relative of classification rules—can be used to identify groups of items in transactional data. Lastly, in <em class="chapterRef">Chapter 14</em>, <em class="italic">Building Better Learners</em>, we will discover how the performance of decision trees can be improved by grouping them together in a model known as a random forest, in addition to other advanced modeling techniques that rely on decision trees.</p>
    <h1 class="heading-1" id="_idParaDest-133">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 4000 people at:</p>
    <p class="normal"><a href="https://packt.link/r"><span class="url">https://packt.link/r</span></a></p>
    <p class="normal"><img alt="" src="../Images/r.jpg"/></p>
  </div>
</body></html>