- en: Customer Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to learn about unsupervised learning models and
    how they can be used to extract insights from the data. Up until now, we have
    been focusing on supervised learning, where our **machine learning** (**ML**)
    models have known target variables that they try to predict. We have built classification
    models for spam email filtering and Twitter sentiment analysis. We have also built
    regression models for foreign exchange rate forecasting and predicting the fair
    value of house prices. All of these ML models that we have built so far are supervised
    learning algorithms, where the models learn to map the given input to expected
    outcomes. However, there are cases where we are more interested in finding hidden
    insights and drawing inferences from datasets, and we can use unsupervised learning
    algorithms for such tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to use an online retail dataset that contains
    information about the prices and quantities of items that customers bought. We
    will explore the data by looking at how the distributions of item prices and quantities
    for purchase orders differ from those of cancel orders. We will also look at how
    online store activities are spread across different countries. Then, we are going
    to take this transaction-level data and transform and aggregate it into customer-level
    data. As we transform this data to have a customer-centric view, we are going
    to discuss ways to build scale-independent features for unsupervised learning
    algorithms. With this feature set, we are going to use a k-means clustering algorithm
    to build customer segments and extract insights on the customer behaviors within
    each segment. We will introduce a new validation metric, Silhouette Coefficient,
    to evaluate the clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition for a customer segmentation project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for an online retail dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering and aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning using a k-means clustering algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering model validations using the Silhouette Coefficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's discuss in more detail what problems we are going to solve and build clustering
    models for. Whether you are trying to send marketing emails to your customers
    or you simply want to better understand your customers and their behaviors on
    your online store, you will want to analyze and identify different types and segments
    of your customers. Some customers might buy lots of items at once (bulk buyers),
    some might primarily buy expensive or luxury items (luxury product buyers), or
    some might have bought one or two items and never come back (unengaged customers).
    Depending on these behavioral patterns, your marketing campaigns should vary.
    For example, sending out emails with promotions on luxury items is likely to provoke
    luxury product buyers to log in to the online store and purchase certain items,
    but such an email campaign is not going to work well for bulk buyers. On the other
    hand, sending out emails with promotions on items that are frequently bought in
    bulk, such as pens and notepads for office supplies, is likely to make bulk buyers
    log in to the online store and place purchase orders, but it might not be attractive
    for luxury product buyers. By identifying customer segments based on their behavioral
    patterns and using customized marketing campaigns, you can optimize your marketing
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build models for customer segmentation, we are going to use an online
    retail dataset that contains all the transactions that occurred between Jan. 12th
    2010 and Sep. 12th 2011 for a UK-based online retail store. This dataset is available
    in the UCI Machine Learning Repository and can be downloaded from this link: [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#).
    With this data, we are going to build features that contain information about
    the net revenue, average item price, and average purchase quantity per customer.
    Using these features, we are going to build a clustering model using a **k-means
    clustering algorithm** that clusters the customer base into different segments.
    We will be using **Silhouette Coefficient** metrics to evaluate the quality of
    the clusters and deduce the optimal number of customer segments to build.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our problem definition for the customer segmentation project:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem? We need a clustering model that segments customers into
    different clusters, so that we can understand and draw insights about the behavioral
    patterns of the customers better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it a problem? There is no one-fits-all marketing campaign that works
    for all different types of customers. We will need to build custom-tailored marketing
    campaigns for bulk buyers and luxury product buyers separately. Also, we will
    have to target unengaged customers differently from the other customer types to
    have them re-engage with the products. The more customized the marketing messages
    are, the more likely customers will engage. It will be a big advantage if we have
    an ML model that clusters our customer base into different segments based on their
    behavioral patterns on the online store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some of the approaches to solving this problem?We are going to use
    the online retail dataset that contains all transactions from 2010 to mid-2011
    to aggregate the key features, such as net revenue, average unit price, and average
    purchase quantity for each customer. Then, we will use a k-means clustering algorithm
    to build a clustering model and use the Silhouette Coefficient to evaluate the
    quality of clusters and choose the optimal number of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the success criteria? We do not want too many clusters, as this would
    make it more difficult to explain and understand different patterns of customers.
    We will use the Silhouette Coefficient score to tell us the best number of clusters
    to use for customer segmentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for the online retail dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is now time to look into the dataset. You can follow this link: [http://archive.ics.uci.edu/ml/datasets/online+retail#](http://archive.ics.uci.edu/ml/datasets/online+retail#),
    click on the `Data Folder` link in the top left corner, and download the `Online
    Retail.xlsx` file. You can save the file as a CSV format and load it into a Deedle
    data frame.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since we will be aggregating the transaction data for each customer, we need
    to check whether there are any missing values in the `CustomerID` column. The
    following screenshot shows a few records with no `CustomerID`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'We are going to drop those records with missing values from the `CustomerID`,
    `Description`, `Quantity`, `UnitPrice`, and `Country` columns. The following code
    snippet shows how we can drop records with missing values for those columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We use the `DropSparseRows` method of the Deedle data frame to drop all the
    records with missing values in the columns of our interest. Then, we append the
    data frame with an additional column `Amount`, which is the total price for the
    given transaction. We can calculate this value by multiplying the unit price with
    the quantity.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the previous image, we had 541,909 records before we dropped
    the missing values. After dropping the records with missing values from the columns
    of our interest, the number of records in the data frame ends up being 406,829\.
    Now, we have a data frame that contains the information about `CustomerID`, `Description`,
    `Quantity`, `UnitPrice`, and `Country` for all the transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Variable distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start looking at the distributions in our dataset. First, we will take
    a look at the top five countries by the volume of transactions. The code we used
    to aggregate the records by the countries and count the number of transactions
    that occurred in each country is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The number of transactions for each of the top five countries looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.gif)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the largest number of transactions occurred in the United Kingdom.
    Germany and France come in as the countries with the second and third most transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start looking at the distributions of the features that we will be using
    for our clustering model—purchase quantity, unit price, and net amount. We will
    be looking at these distributions in three ways. First, we will get the overall
    distribution of each feature, regardless of whether the transaction was for purchase
    or cancellation. Second, we will take a look at the purchase orders only, excluding
    the cancel orders. Third, we will look at the distributions for cancel orders
    only.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to get distributions of transaction quantity is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As in the previous chapter, we are using the `Quantiles` method to compute
    `quartiles`—min, 25% percentile, median, 75% percentile, and max. Once we get
    the overall distribution of order quantities per transaction, we then look at
    the distribution for purchase orders and cancel orders. In our dataset, cancel
    orders are encoded with negative numbers in the `Quantity` column. In order to
    separate cancel orders from purchase orders, we can simply filter out positive
    and negative quantities from our data fame as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to get the `quartiles` of per-transaction unit prices, we use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can compute the `quartiles` of the per-transaction total amount
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the code, you will see the following output for the distributions
    of per-transaction order quantity, unit price, and total amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.gif)'
  prefs: []
  type: TYPE_IMG
- en: If you look at the distribution of the overall order quantities in this output,
    you will notice that from the first quartile (25% percentile), the quantities
    are positive. This suggests that there are far less cancel orders than purchase
    orders, which is actually a good thing for an online retail store. Let's look
    at how the purchase orders and cancel orders are divided in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following code, you can draw a bar chart to compare the number of
    purchase orders against cancel orders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following bar chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected and shown in the previous distribution output, the number of cancel
    orders is much less than the number of purchase orders. With these analysis results,
    we are going to start building features for our clustering model for customer
    segmentation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this data analysis step can be found by following this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/DataAnalyzer.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and data aggregation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The records in the dataset we have now represent individual transactions. However,
    we want to build a clustering model that clusters customers into different segments.
    In order to do that, we need to transform and aggregate our data by customer.
    In other words, we will need to group our data by `CustomerID` and `aggregate`
    all the transactions that belong to each customer by summing, counting, or taking
    averages of the values. Let''s look at an example first. The following code groups
    the transaction-level data by `CustomerID` and computes the net revenue, total
    number of transactions, total number of cancel orders, average unit price, and
    average order quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you may see from this code, we are using the `AggregateRowsBy` method in
    the Deedle data frame and passing a custom `aggFunc` for each aggregation. In
    the first example, where we compute the net revenue per customer, we sum all the
    purchase amounts for each customer. For the second feature, we count the number
    of transactions to compute the total number of orders for each customer. In order
    to compute the average order quantity for each customer, we sum up all the order
    quantities and divide it by the number of transactions. As you can see from this
    case, the `AggregateRowsBy` method comes in handy when you need to transform and
    aggregate a data frame with a custom `aggregation` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have computed all these features, we need to combine all the data into
    one place. We created a new empty data frame and added each of these aggregated
    features as separate columns to the new data frame. The following code shows how
    we created a features data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To take a closer look at the distributions of these features, we wrote a helper
    function that computes the `quartiles` of a given feature and prints out the results.
    The code for this helper function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look closely, there is one thing that is concerning. There is a small
    number of customers that have negative net revenue and negative average quantity.
    This suggests some customers have more cancel orders than purchase orders. However,
    this is odd. To cancel an order, there needs to be a purchase order first. This
    suggests that our dataset is not complete and there are some orphan cancel orders
    that do not have matching previous purchase orders. Since we cannot go back in
    time and pull out more data for those customers with orphan cancel orders, the
    simplest way to handle this problem is to drop those customers with orphan cancel
    orders. The following code shows some criteria we can use to drop such customers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code snippet, we drop any customer who has a negative
    net revenue, negative average quantity, and percentage of cancel orders more than
    50%. After dropping these customers, the resulting distributions look like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from these distributions, the scales for each feature are very
    different. `NetRevenue` rages from 0 to 279,489.02, while `PercentageCancelled`
    ranges from 0 to 0.45\. We are going to transform these features into percentiles,
    so that we can have all of our features on the same scale of 0 to 1\. The following
    code shows how to compute percentiles for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `StatsFunctions.PercentileRank` method, we can compute the percentile
    for each record. The following output shows the results for the `NetRevenue` and
    `NumTransactions` features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this output, instead of a wide range, the values for both
    features now range between 0 and 1\. We will use these percentile features when
    we build our clustering model in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this feature engineering step can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/FeatureEngineering.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning – k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is now time to start building our clustering models. In this project, we
    are going to try clustering customers into different segments based on the following
    three features: `NetRevenuePercentile`, `AvgUnitPricePercentile`, and `AvgQuantityPercentile`,
    so that we can analyze the item selections based on the spending habits of the
    customers. Before we start fitting a k-means clustering algorithm to our feature
    set, there is an important step we need to take. We need to normalize our features,
    so that our clustering model does not put more weight on certain features over
    the others. If variances of features are different, then a clustering algorithm
    can put more weight on those with small variances and can tend to cluster them
    together. The following code shows how you can normalize each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have normalized our variables, let''s start building clustering
    models. In order to build a k-means clustering model, we need to know the number
    of clusters we want in advance. Since we do not know what the best number of clusters
    is, we are going to try a few different numbers of clusters and rely on the validation
    metrics, the Silhouette Score, to tell us what the optimal number of clusters
    is. The following code shows how to build clustering models that use a k-means
    clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, it will output the centroids for each cluster. The
    following is an output of cluster centroids from a 4-cluster clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from this output, the cluster with label 3 is a cluster of customers
    who have high net revenue, middle-high average unit price, and middle-high average
    quantity. So, these customers are high value customers who bring in the most revenue
    and buy items with prices above average in above-average quantities. In contrast,
    the cluster labeled as 1 is a cluster of customers who have low net revenue, high
    average unit price and middle-low average quantity. So, these customers buy expensive
    items in average quantities and do not bring in that much revenue for the online
    store. As you may notice from this example, you can already see some patterns
    among different clusters. Let''s now look at which customers in each segment buy
    the most. The following is the top 10 items bought for each segment of the 4-cluster
    clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'This top 10 item list for each segment gives you a rough idea of what kinds
    of items the customers in each segment buy the most. This is out of scope for
    this chapter, but you can take a step further and analyze individual words in
    the item description and use word frequency analysis, such as we did in [Chapter
    2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam Email Filtering*
    and [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470), *Twitter
    Sentiment Analysis*. Another way to visualize the clustering results is to draw
    scatter plots for the segments. The following chart shows a scatter plot of `NetRevenuePercentile`
    versus `AvgQuantityPercentile` for the 4-cluster clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following chart shows a scatter plot of `AvgUnitPricePercentile ` versus 
    `AvgQuantityPercentile` for the 4-cluster clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following chart shows a scatter plot of `NetRevenuePercentile` versus `AvgUnitPricePercentile` for
    the 4-cluster clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from these plots, a scatter plot is a good way to visualize how
    each cluster is formed and what the boundaries look like for each cluster. For
    example, if you look at the scatter plot of `NetRevenuePercentile` versus `AvgUnitPricePercentile`,
    cluster 1 has high average unit price and low net revenue. This corresponds to
    the findings we have from looking at the cluster centroids. For higher dimensions
    and larger number of clusters, it gets more difficult to visualize using scatter
    plots. However, very often, visualizing in charts helps draw insights more easily
    from these clustering analyses. Let's start looking at how we can evaluate the
    cluster quality and choose the optimal number of clusters using the Silhouette
    Coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: The full code that was used in this k-means clustering step can be found at
    this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/Clustering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.6/Clustering.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Clustering model validations using the Silhouette Coefficient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The** Silhouette Coefficient** or **Silhouette Score** provides an easy way
    to evaluate the quality of clusters. The Silhouette Coefficient measures how closely
    related an object is to its own cluster against the other clusters. The way to
    compute the Silhouette Coefficient is as follows; for each record, `i`, calculate
    the average distance between the record and all the other records in the same
    cluster and call this number, `a[i]`. Then, calculate the average distances between
    the record and all the records in each other cluster for all the other clusters,
    take the lowest average distance, and call this number, `b[i]`. Once you have
    these two numbers, subtract `a[i]` from `b[i]` and divide it by the maximum number
    between `a[i]` and `b[i]`. You iterate this process for all the records in the
    dataset and calculate the average value to get the Silhouette Coefficient. The
    following is a formula to calculate the Silhouette Coefficient for a single data
    point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to get the final Silhouette value, you will need to iterate through
    the data points and take the average of Silhouette values. The Silhouette Coefficient
    ranges between -1 and 1\. The closer to 1, the better the cluster qualities are.
    The following code shows how we implemented this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'A helper function to calculate the average distance between a data point and
    all the points in a cluster is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the code, we iterate through each data point and start calculating
    the average distances between the given data point and all the other records in
    different clusters. Then, we take the difference between the lowest average distance
    to different clusters and the average distance within the same cluster and divide
    it by the maximum of those two numbers. Once we have iterated through all the
    data points, we take the average of this Silhouette value and return it as the
    Silhouette Coefficient for the clustering model.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run this code for the clustering models with different numbers of
    clusters, you will see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this output, the Silhouette Score increased as we increased
    the number of clusters to a certain point and then it dropped. In our case, a
    k-means clustering model with six clusters performed the best and six clusters
    seem to be the best choice for our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Oftentimes, just looking at the Silhouette Coefficient is not enough to make
    a decision on the best number of clusters. For example, a clustering model with
    a really large number of clusters can have a great Silhouette Score, but it would
    not help us draw any insights from such a clustering model. As clustering analysis
    is primarily used for explanatory analysis to draw insights and identify hidden
    patterns from the data, it is important that the clustering results can be explained.
    Pairing the Silhouette Score with two-dimensional or three-dimensional scatter
    plots will help you come up with the best number of clusters to choose and decide
    what makes the most sense to your dataset and project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored unsupervised learning and how it can be used to
    draw insights and identify hidden patterns in the data. Unlike other projects
    we have worked on so far, we did not have specific target variables that our ML
    models can learn from. We just had a raw online retail dataset, in which we had
    information about the items, quantities, and unit prices that customers bought
    on the online store. With this given dataset, we transformed transaction-level
    data into customer-level data and created numerous aggregate features. We learned
    how we can utilize the `AggregateRowsBy` method in Deedle's data frame to create
    aggregate features and transform the dataset to have a customer-centric view.
    We then briefly discussed a new library, `CenterSpace.NMath.Stats`, which we can
    use for various statistical computations. More specifically, we used the `StatsFunctions.PercentileRank`
    method to compute the percentiles of each record for a given feature.
  prefs: []
  type: TYPE_NORMAL
- en: We covered how we can fit a k-means clustering algorithm using the `Accord.NET`
    framework. Using the k-means clustering algorithm, we were able to build a few
    clustering models with different numbers of clusters. We discussed how we can
    draw insights using the 4-cluster clustering model as an example and how we can
    cluster customers into different customer segments, where one segment's customer
    characteristics were high net revenue, above-average unit price, and above-average
    quantity, the other segment's customer characteristics were low net revenue, high
    average unit price, and below-average quantity, and so forth. We then looked at
    the top 10 items each customer segment purchased the most frequently and created
    scatter plots of different segments on our feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we used the **S****ilhouette Coefficient** to evaluate the cluster qualities,
    and learned how we can use this as one of the criteria for choosing the optimal
    number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: From the next chapter, we are going to start building models for audio and image datasets.
    In the next chapter, we are going to discuss how to build a music genre recommendation
    model using a music audio dataset. We will learn how to build a ranking system
    where the output is the ranks of likelihood of individual categories. We will
    also learn what types of metrics to use to evaluate such a ranking model.
  prefs: []
  type: TYPE_NORMAL
