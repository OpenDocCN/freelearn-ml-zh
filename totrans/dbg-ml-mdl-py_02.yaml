- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Life Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning modeling in practice, either at the industrial level or in
    academic research, is beyond writing a couple of lines of Python code to train
    and evaluate a model on a public dataset. Learning to write a piece of Python
    program to train a machine learning model using Python and `scikit-learn` or a
    deep learning model using `PyTorch` is a starting point for becoming a machine
    learning developer and specialist. In this chapter, you will learn about the components
    of the machine learning life cycle and how, while considering this life cycle
    when planning for machine learning modeling, it helps you in designing a valuable
    and scalable model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the topics, including the main components of the machine learning
    life cycle, that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Before we start modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data wrangling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the code and the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model deployment and monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned how to design a machine learning
    life cycle for your projects and why modularizing your projects into the components
    of a life cycle helps you in your collaborative model developments. You will have
    also learned about some of the techniques and their Python implementations for
    different components of a machine learning life cycle, such as data wrangling
    and model training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` >= 1.4.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` >= 3.5.3'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02).
  prefs: []
  type: TYPE_NORMAL
- en: Before we start modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before collecting data as the starting point of a machine learning life cycle,
    you need to know your objectives. You need to know what problems you want to solve
    and then define smaller subproblems that would be machine learning solvable. For
    example, in the case of a problem such as, “*How could we reduce the number of
    fragile products returned to a manufacturing facility?,*” the subproblems could
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How could we detect the cracks* *before packaging?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How could we design better packaging to protect the products and reduce* *transportation-caused
    cracks?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Could we use better materials to reduce the risk* *of cracking?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Could we apply small design changes to our product that do not change its
    functionality but reduce the risk* *of cracking?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have identified your subproblems, you can find out how you can use
    machine learning for each and go through a machine learning life cycle for the
    defined subproblems. Each of the subproblems may need specific data processing
    and machine learning modeling, and some of them could be easier to solve compared
    to the rest.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.1* shows the major steps in machine learning life cycles. Some
    of these names are not universally defined. For example, data exploration sometimes
    gets included in data wrangling. But all these steps are required, even if they
    are named differently in different resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Machine learning life cycle](img/B16369_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Machine learning life cycle
  prefs: []
  type: TYPE_NORMAL
- en: When you rely on a dataset that’s already available in Python, through `scikit-learn`
    or `PyTorch`, for example, or a dataset that is ready for modeling in public repositories,
    you don’t need to worry about the early steps, such as data collection, selection,
    and wrangling. These steps have already been taken care of for you. Or if you
    are just doing modeling for practice and don’t want to provide your model in a
    production system, you don’t need to worry about model deployment and monitoring.
    But understanding the meaning, importance, and benefits of all these steps helps
    you develop or design a functional technology with continuous improvement to be
    provided for users. It also helps you better understand your role as a machine
    learning developer or find your first job or a better job in this field.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in the machine learning life cycle is data collection. It could
    be about collecting data from different public or commercial databases, storing
    user data back into your database or any data storage system you have, or even
    using commercial entities that take care of data collection and annotation for
    you. If you are relying on free resources, the main consideration for you could
    be the space the data will get in your local or cloud-based storage system and
    the time you need to spend to collect the data and analyze it in future steps.
    But for paid data, either provided in commercial resources or generated by data
    collection, generation, and annotation companies, you need to assess the value
    of the data for modeling before you decide to pay for it.
  prefs: []
  type: TYPE_NORMAL
- en: Data selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on the objectives of the corresponding projects, we need to select
    the required data for model training and testing. For example, you might have
    access to information about cancer patients in one or multiple hospitals, such
    as their age, gender, whether they smoke or not, their genetic information if
    available, their MRI or CT scans if available, history of their medication, their
    response to cancer drugs, whether they had surgery or not, their prescriptions,
    either handwritten or in PDF format, and much more. When you want to build a machine
    learning model to predict the response of patients to therapy using their CT scans,
    you need to select different data for each patient compared to when you want to
    build a model using their information, such as age, gender, and smoking status.
    You also need to select patients from whom you have the input and output data
    available if you are building a supervised learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to combine data points with and without outputs in a semi-supervised
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting relevant data for your models is not an easy task as the information
    that separates data as *relevant* and *irrelevant* from your model objective is
    not necessarily available in this binary way. Imagine you need to extract data
    from a chemical, biological, or physical database, which could be a collection
    of data from different smaller datasets, supplementary materials of papers, or
    even data coming from within scientific articles. Or perhaps you want to extract
    information from the medical records of patients or even from written answers
    to an economical or sociological survey. In all such examples, separation of data
    for your model, or querying relevant data from relevant databases, is not as simple
    as searching for one keyword. Each keyword could have synonyms, either in plain
    English or in technical terms, could be written in different ways, or even sometimes
    the relevant information could exist in different columns of a data file or a
    relational database. Proper data selection and query systems provide you with
    a huge opportunity to improve your models.
  prefs: []
  type: TYPE_NORMAL
- en: You can benefit from a literature review and asking experts, if needed, to extend
    the keywords you are using. You can benefit from known data selection methods
    for specific tasks you have or even license tools or pay for services to help
    you in extracting more relevant data for your objectives. There are also advanced
    natural language processing techniques to help you in your query system from text.
    We will discuss these in [*Chapter 13*](B16369_13.xhtml#_idTextAnchor342), *Advanced
    Deep Learning Techniques*, and [*Chapter 14*](B16369_14.xhtml#_idTextAnchor379),
    *Introduction to Recent Advancements in* *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this stage, your data is selected and you can explore the quantity, quality,
    sparsity, and format of the data. You can find the number of data points in each
    class if you have categorical output in supervised learning, distribution of features,
    confidence in output variables, if available, and other characteristics of the
    data you get out of the *data selection* stage. This process helps you identify
    issues with your data that need to be fixed in *data wrangling*, which is the
    next step in the life cycle, or opportunities for improving your data by revising
    your *data* *selection* process.
  prefs: []
  type: TYPE_NORMAL
- en: Data wrangling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your data needs to go through structuring and enriching processes and be transformed
    and cleaned up, if necessary. All these aspects are part of data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The raw data might come in different formats and sizes. You might have access
    to handwritten notes, Excel sheets, or even images of tables that contain information
    that needs to be extracted and put in the right format for further analysis and
    used for modeling. This process is not about transforming all data into a table-like
    format. In the process of data structuring, you need to be careful regarding information
    loss. For example, you could have features that are in a specific order, such
    as based on time, date, or the sequence of information coming through a device.
  prefs: []
  type: TYPE_NORMAL
- en: Enriching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After structuring and formatting your data, you need to assess whether you have
    the right data to build a machine learning model of that cycle. You might identify
    opportunities to add or generate new data before continuing the wrangling process.
    For example, you might find out that in the data for identifying cracks in images
    of products in a manufacturing pipeline, you only have 50 out of 10,000 images
    that are labeled as images of cracked products. You might be able to find other
    images of cracked products or you could generate new images using a process called
    **data augmentation**.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is a series of techniques for generating new data points,
    computationally, using the original dataset we have at hand. For example, if you
    rotate your portrait, or change the quality of an image by adding Gaussian noise
    to it, the new image will still show your face. But it could help your model to
    be more generalizable. We will talk about different data augmentation techniques
    in [*Chapter 5*](B16369_05.xhtml#_idTextAnchor183), *Improving the Performance
    of Machine* *Learning Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The features and the output of datasets could be different types of variables,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantitative** **or numerical**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discrete**: For example, the number of houses in a neighborhood'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous**: For example, the age or weight of patients'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Qualitative** **or categorical**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nominal (no order)**: For example, different colors of cars'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ordinal (qualitative variable with order)**: For example, grades of students,
    such as A, B, C, or D'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we train a machine learning model, the model needs to use numerical values
    to calculate the loss function in each iteration of the optimization process.
    Hence, we need to transform categorical variables into numerical alternatives.
    There are multiple feature encoding techniques, three of which are one-hot encoding,
    target encoding (Micci-Barreca, 2001), and label encoding. A one-hot, label, and
    target encoding calculation for an example matrix of four columns, including age,
    gender, group, and target, and seven rows, as seven example data points, is shown
    in *Figure 2**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.2 – Manual calculations for one-hot, target, and \uFEFFlabel encoding\
    \ using a simple example dataset with four features and seven data points](img/B16369_02_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Manual calculations for one-hot, target, and label encoding using
    a simple example dataset with four features and seven data points
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an imaginary dataset for predicting the response of patients to a drug,
    with the target column as the output. Variable categories are abbreviated as F:
    Female, M: Male, H1: Hospital 1, H2: Hospital 2, and H3: Hospital 3\. In reality,
    many more variables need to be considered and more data points are necessary to
    have a reliable model for drug response prediction and assess whether there are
    biases in the response of patients to drugs between male and female groups or
    in different hospitals.'
  prefs: []
  type: TYPE_NORMAL
- en: Each of these techniques has its benefits and caveats. For example, one-hot
    encoding increases the number of features (that is, the dimensionality of the
    dataset) and increases the chance of overfitting. Label encoding assigns integer
    values to each category, which do not necessarily have a meaning. For example,
    considering male as 1 and female as 0 is arbitrary and doesn’t have any real meaning.
    Target encoding is an alternative approach that considers the probabilities of
    each category concerning the target. You can read the mathematical details of
    this process in Micci-Barreca, 2001\. Python’s implementation of these approaches
    is provided in the following code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s define a synthetic DataFrame to use for feature encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we will use label encoding to encode the categorical features in the
    defined DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will try to perform one-hot encoding for categorical feature transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will implement target encoding in Python, after installing the `category_encoders`
    library, as the third encoding approach, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Ordinal variables can also be transformed using the `OrdinalEncoder` class as
    part of `sklearn.preprocessing`. The difference between ordinal and nominal transformation
    is the meaning behind the order of categories in ordinal variables. For example,
    if we are encoding grades of students, A, B, C, and D could be transformed into
    1, 2, 3, and 4, or 4, 3, 2, and 1, but transforming them into 1, 3, 4, and 2 will
    not be acceptable as it is changing the meaning behind the order of the grades.
  prefs: []
  type: TYPE_NORMAL
- en: Output variables can also be categorical. You can use label encoding to transform
    a nominal output into a numerical variable for classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After structuring the data, it needs to be cleaned. Cleaning data helps increase
    the quality of your data and makes it closer to being ready for modeling. An example
    of a cleaning process is filling in missing values in your data. For example,
    if you want to use patients’ living habits to predict their risk of getting diabetes
    using their responses to a survey, you might find out some of the participants
    didn’t respond to the questions about their smoking habits.
  prefs: []
  type: TYPE_NORMAL
- en: Feature imputation for filling in missing values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The features of a dataset we have at hand could contain missing values. The
    majority of machine learning models and their corresponding Python implementations
    cannot handle missing values. In these cases, we need to either remove data points
    with missing feature values or somehow fill in those missing values. There are
    feature imputation techniques we can use to calculate the values of features that
    are missing in our dataset. Examples of such methods are shown in *Figure 2**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Feature imputation techniques for calculating missing feature
    values](img/B16369_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Feature imputation techniques for calculating missing feature values
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, either we can use other values of the same features and replace
    the missing values with the mean or median of the available values, or we can
    use other features with low or no missing values that have a high correlation
    with the feature with missing values. In the second case, we can use the feature
    with the highest correlation, with the target feature with missing values, to
    build a linear model. The linear model considers the correlated feature as input
    and the feature with missing values as output and then uses the predictions of
    the linear model to calculate the missing values.
  prefs: []
  type: TYPE_NORMAL
- en: When we use a statistical summary of the values of the same feature, such as
    the mean or median, we are reducing the variance of the feature values as those
    summary values will be used for all the missing values of the same feature (*Figure
    2**.3*). On the other hand, when we use a linear model between the feature with
    missing values and a highly correlated feature with low or no missing values,
    we are assuming a linear relationship between them. Alternatively, we can build
    more complex models between features for missing value calculation. All these
    approaches have their benefits and limitations, and you need to choose the one
    that works best for your dataset, depending on the distribution of feature values,
    the fraction of data points with missing features values, the correlation range
    between features, the existence of features with low or no missing value, and
    other relevant factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used a very simple case of four features and five data points in *Figure
    2**.3* to showcase the discussed feature imputation techniques. But in reality,
    we need to build models with more than four features. We can use Python libraries
    such as `scikit-learn` for feature imputation by using the mean of the same feature
    values, as follows. First, we will import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must define the two-dimensional input list, where each internal list
    shows the feature values of a data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to fit a `SimpleImputer` function by specifying what needs
    to be considered as a missing value and what strategy to be used for imputation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use `scikit-learn` to make a linear regression model that calculates
    missing feature values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Outlier removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numerical variables in our datasets could have values that are far away from
    the rest of the data. They could be real values that are dissimilar to the rest
    of the data points or caused by errors in data generation, such as in experimental
    measurement processes. You can visually see and detect them using a boxplot (*Figure
    2**.4*). The circles of the plot are the outliers that get automatically detected
    by the plotting functions in Python, such as `matplotlib.pyplot.boxplot` (*Figure
    2**.4*). Although visualization is a good way of exploring our data and understanding
    the distribution of numerical variables, we need to have a quantitative way of
    detecting outliers without the need to plot the values of all the variables in
    our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way of detecting outliers is by using quantiles of the distribution
    of variable values. Data points that are beyond the upper and lower bounds are
    considered outliers (*Figure 2**.4*). Lower and upper bounds can be calculated
    as Q1 - a.IQR and Q3 - a.IQR, where can be a real value between 1.5 and 3\. The
    common value of a, which is also used by default in drawing boxplots, is 1.5,
    but having higher values makes the process of outlier identification less stringent
    and lets fewer data points be detected as outliers. For example, by changing the
    stringency of outlier detection from the default (that is, a = 1.5) to a = 3,
    none of the data points in *Figure 2**.4* would be detected as outliers. This
    approach for outlier identification is non-parametric, meaning it doesn’t have
    any assumptions regarding the distribution of data points. Hence, it can be applied
    to non-normal distributions, such as the data shown in *Figure 2**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Outliers in histograms and boxplots](img/B16369_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Outliers in histograms and boxplots
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, the plots were generated using the values of features
    in the diabetes dataset of the `scikit-learn` package, which was loaded via `sklearn.datasets.load_diabetes()`.
  prefs: []
  type: TYPE_NORMAL
- en: Data scaling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The values of features, either originally numerical or after transformation,
    could have different ranges. Many machine learning models perform better, or at
    least their optimization processes converge faster, if their feature values get
    scaled and normalized properly. For example, if you have a feature ranging from
    0.001 to 0.05 and another one from 1,000 to 5,000, bringing both of them to a
    reasonable range such as [0, 1] or [-1, 1] could help improve the speed of convergence
    or the performance of your model. You need to make sure the scaling and normalizations
    you implement don’t cause ties in your feature values, meaning data points don’t
    lose their difference based on features that went under transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of scaling is to change the range of values of a variable. In
    normalization, the shape of the distribution of values could also change. You
    can use examples of these methods and the corresponding classes available in `scikit-learn`
    in your projects to improve the scale and distribution of your features (*Table
    2.1*). The resulting scaled variables after using each of these classes have specific
    characteristics. For example, the values of a variable after using the `StandardScalar`
    class of `scikit-learn` will be centered around zero with a standard deviation
    of one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of these techniques, such as robust scaling, which can be done using the
    `RobustScaler` class of `scikit-learn`, are less likely to be affected by outliers
    (*Table 2.1*). In robust scaling, outliers, based on the definition we provided,
    don’t affect how the median and *IQR* are calculated and, therefore, do not affect
    the scaling process. Outliers themselves then be scaled using the calculated median
    and *IQR*. Outliers can be either kept or removed before or after scaling, depending
    on the machine learning method used and the task at hand. But the important point
    is to detect them and be aware of them when you’re trying to prepare data for
    modeling and, if required, scale or remove them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Python Class** | **Mathematical Definition** | **Value Limits** |'
  prefs: []
  type: TYPE_TB
- en: '| `sklearn.preprocessing.StandardScaler()` | Z = (X - u) / su: Means: Standard
    deviation | No limit>99% of data between -3 and 3 |'
  prefs: []
  type: TYPE_TB
- en: '| `sklearn.preprocessing.MinMaxScaler()` | X_scaled = (X-Xmin)/(Xmax-Xmin)
    | [0,1] |'
  prefs: []
  type: TYPE_TB
- en: '| `sklearn.preprocessing.MaxAbsScaler()` | X_scaled = X/&#124;X&#124;max |
    [-1,1] |'
  prefs: []
  type: TYPE_TB
- en: '| `sklearn.preprocessing.RobustScaler()` | Zrobust = (X - Q2) / IQRQ2: MedianIQR:
    Interquartile range | No limitMajority of data between -3 and 3 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Example of Python classes for scaling and normalizing feature values
  prefs: []
  type: TYPE_NORMAL
- en: Other forms of exploratory data analysis are conducted after data wrangling
    before machine learning modeling is started. Domain expertise could also help
    in identifying patterns whose interpretations need to be better understood regarding
    the subject domain for which the problem has been defined. To increase the likelihood
    of success for machine learning modeling, you may need feature engineering to
    build new features or learn new features through representation learning. These
    new features could be as simple as body mass index, defined as the ratio of someone’s
    weight in kilograms to the square of their height in meters. Or they could be
    new features and representations that are learned through complicated processes
    or extra machine learning modeling. We will talk about this later in [*Chapter
    14*](B16369_14.xhtml#_idTextAnchor379), *Introduction to Recent Advancements in*
    *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this stage of a machine learning life cycle, we need to finalize the features
    and data points we want to use for modeling, as well as our model evaluation and
    testing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection and extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original features that were normalized and scaled in previous steps can
    be now processed further to increase the likelihood of having a high-performance
    model. In general, features can either be sub-selected, meaning some of the features
    get thrown out, using a *feature selection* method, or be used to generate new
    features, which is traditionally called *feature extraction*.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of feature selection is to reduce the number of features, or the dimensionality
    of your data, and keep features that are information-rich. For example, if we
    have 20,000 features and 500 data points, there is a high chance that most of
    the original 20,000 features are not informative when used to build a supervised
    learning model. The following list explains some simple techniques for feature
    selection:'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping features with a high variance or MAD across the data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping features with the highest number of unique values across the data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping representative features from groups of highly correlated features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These processes can be conducted using all the data points or just training
    data to avoid potential information leakage between the training and test data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Combining original features linearly or nonlinearly could result in more informative
    features for building a predictive model. This process is called feature extraction
    and could be conducted based on domain knowledge or through different statistical
    or machine learning models. For example, you can use principal component analysis
    or isometric mapping to reduce the dimensionality of your data in a linear or
    non-linear way, respectively. Then, you can use these new features in your training
    and testing process. The Python implementation of these two approaches is provided
    in the following code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the required libraries and load the `scikit-learn` digit
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s use `isomap` and `pca`, both of which are available in `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The number of components you can select from each such method can be determined
    through different techniques. For example, the explained variance ratio is a commonly
    used approach to select the number of principal components. These are identified
    through principal component analysis and collectively explain more than a specific
    percentage, such as 70% of the total variance in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There are also more advanced techniques that are part of self-supervised pre-training
    and representation learning for identifying new features. In these techniques,
    large amounts of data are used to calculate new features, representations, or
    embeddings. For example, the English version of Wikipedia can be used to come
    up with better representations of English words rather than performing one-hot
    encoding for each word. We will talk about self-supervised learning models in
    [*Chapter 14*](B16369_14.xhtml#_idTextAnchor379), *Introduction to Recent Advancements
    in* *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an evaluation and testing strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to specify our testing strategy before we train our model to identify
    its parameters or optimal hyperparameters. Model testing could be done by another
    team on separate datasets if you are working in a big organization. Alternatively,
    you can dedicate one or multiple datasets, separate from your training set, or
    separate part of your data so that you can test it separately from the training
    set. You also need to list the ways you want to assess the performance of your
    model in the testing stage. For example, you may need to specify the performance
    plots or measures you want to use, such as the **receiver operating curve** (**ROC**)
    and **precision-recall** (**PR**) curve, or other criteria, to select a new classification
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Once your testing strategy has been defined, you can use the rest of the data
    to specify training and validation sets. Validation and training sets don’t need
    to be one series of fixed data points. We can use *k*-fold **cross-validation**
    (**CV**) to split a dataset into *k* chunks and use one chunk at a time as a validation
    set and the rest as the training set. Then, the average of the performance across
    all *k* chunks can be used as a validation set to calculate the validation’s performance.
    Training performance is important for finding optimal values for model parameters
    based on the objective of the model. You can also use validation performance to
    identify optimal hyperparameter values. If you specify one validation set or use
    *k*-fold CV, you can use the validation performance of different hyperparameter
    combinations to identify the best one. Then, the best hyperparameter set can be
    used to train the model on all data, excluding test data, so that you can come
    up with the final model to be tested in the testing stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some common practices for each application regarding the number of
    folds (that is, *k*) or fraction of data points to be separated as validation
    and test sets. For small datasets, 60%, 30%, and 10% are commonly used to specify
    the training, validation, and testing fraction of data points, respectively. But
    both the number of data points and their diversity are important factors in deciding
    on the number of data points within validation and test sets or specifying *k*
    in CV. You can also use available Python classes that perform training and validation
    using *k*-fold CV with your choice of *k*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Preferably, the data you prepared in each of these stages shouldn’t just get
    dumped in the cloud or a hard drive, or get added to a database after each of
    the previous steps in a life cycle. It is beneficial to have a report attached
    to the data to track historical efforts in each step and provide that information
    for other individuals or teams within your team or organization. Proper reporting,
    such as on data wrangling, could provide feedback-seeking opportunities to help
    you improve data provided for machine learning modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of training and validating or testing a model consists of the following
    three major steps if you use `scikit-learn` or `PyTorch` and TensorFlow for neural
    network modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initializing the model**: Initializing a model is about specifying the method,
    its hyperparameters, and the random state to be used for modeling.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training the model**: In model training, the initialized model in *Step 1*
    gets used on the training data to train a machine learning model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inference, assignment, and performance assessment**: In this step, the trained
    model can be used for inference (for example, predicting outputs) in supervised
    learning or, for example, assigning new data points to identified clusters in
    unsupervised learning. In supervised learning, you can use these predictions for
    model performance assessment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are similar for both supervised learning and unsupervised learning
    models. In *Steps 1* and *2*, both types of models can be trained. Python’s implementation
    of these three steps using `scikit-learn` is provided in the following code snippets
    for the random forest classifier and *k*-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the required libraries and load the `scikit-learn` breast
    cancer dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use a random forest to train and test a supervised learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This code prints out the following performance on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also build a *k*-means clustering model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t have enough experience in machine learning modeling, the methodologies
    and corresponding Python classes provided in *Table 2.2* could be a good starting
    point:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Method** | **Python Class** |'
  prefs: []
  type: TYPE_TB
- en: '| Classification | Logistic regression | `sklearn.linear_model.LogisticRegression()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| K-nearest neighbors | `sklearn.neighbors.KNeighborsClassifier()` |'
  prefs: []
  type: TYPE_TB
- en: '| Support vector machine classifier | `sklearn.svm.SVC()` |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest classifier | `sklearn.ensemble.RandomForestClassifier()` |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost classifier | `xgboost.XGBClassifier()` |'
  prefs: []
  type: TYPE_TB
- en: '| LightGBM classifier | `Lightgbm.LGBMClassifier()` |'
  prefs: []
  type: TYPE_TB
- en: '| Regression | Linear regression | `sklearn.linear_model.LinearRegression()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Support vector machine regressor | `sklearn.svm.SVR()` |'
  prefs: []
  type: TYPE_TB
- en: '| Random forest regressor | `sklearn.ensemble.RandomForestRegressor()` |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost regressor | `xgboost.XGBRegressor()` |'
  prefs: []
  type: TYPE_TB
- en: '| LightGBM regressor | `Lightgbm.LGBMRegressor()` |'
  prefs: []
  type: TYPE_TB
- en: '| Clustering | K-means clustering | `sklearn.cluster.KMeans()` |'
  prefs: []
  type: TYPE_TB
- en: '| Agglomerative clustering | `sklearn.cluster.AgglomerativeClustering()` |'
  prefs: []
  type: TYPE_TB
- en: '| DBSCAN clustering | `sklearn.cluster.DBSCAN()` |'
  prefs: []
  type: TYPE_TB
- en: '| UMAP | `umap.UMAP()` |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Starting methods and their Python classes for your supervised learning
    or clustering problems with tabular data
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: UMAP is a dimensionality reduction approach that provides lower dimensional
    visualization, such as a 2D plot of a series of data points. The resulting groups
    of data points in the lower dimensional space can also be used as reliable clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the code and the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the performance of a machine learning model that is selected and brought
    to this stage of the life cycle can be further tested using one or multiple datasets,
    there are a series of tests that need to be done in this stage to make sure of
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring the process of deployment and bringing the model into production goes
    smoothly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring the model will work as expected from a performance and computational
    cost perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring that using the model in production will not have legal and financial
    implications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some such tests that can be used in this stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unit tests**: These are fast tests that make sure our code runs correctly.
    These tests are not specific to machine learning modeling and not even to this
    stage. Throughout the life cycle, you need to design unit tests to make sure your
    data processing and modeling code runs as expected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A/B testing**: This type of testing helps you, your team, and your organization
    in deciding whether to select a model or reject it. The idea of this test is to
    assess two possible scenarios, such as two models, or two different designs of
    the frontend, and check which one is more favorable. But you need to quantitatively
    assess the result by deciding *what needs to be measured* and your *selection
    criteria*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`, `PyTorch`, or TensorFlow changes, this test makes sure your
    code runs and checks the effects of those changes on model performance and predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security tests**: Security testing is an important part of programming and
    modeling at an industrial level. You need to make sure your code and dependencies
    are not vulnerable. However, you need to design a test for advanced adversarial
    attacks. We will discuss this in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119),
    *Debugging toward* *Responsible AI*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsible AI test**: We need to design tests to assess the important factors
    of responsible AI, such as transparency, privacy, and fairness. We will go through
    some important aspects of responsible AI in the next chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these kinds of tests need to be designed for this stage, similar ones
    could be integrated as part of previous steps of the life cycle. For example,
    you can have security testing in all steps of the life cycle, especially if you
    are using different tools or code bases. There could be other tests such as checking
    the memory size and prediction runtime of a model or whether the format and structure
    of data in production and what is expected in the deployed model are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment and monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are new to deployment, you might think of it as how to develop a frontend,
    mobile application, or API for end users of your models. But that is not what
    we want to talk about in this book. There are two important aspects of deployment
    that we want to cover here and in future chapters: the actions needed to provide
    a model in production and integrating a model into a process that is supposed
    to benefit the users.'
  prefs: []
  type: TYPE_NORMAL
- en: When you deploy your model, your code should run properly in the designated
    environment and have access to the required hardware, such as the GPU, and users’
    data needs to be accessible in the right format for your model to work. Some of
    the tests that we talked about in the *testing* stage of the life cycle make sure
    that your model runs as expected in the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about providing a model in a production environment, it either
    gets used behind the scenes for the benefit of the user, such as when Netflix
    and Amazon Prime suggest movies to you using their machine learning models, or
    gets used directly by the user as a standalone process or as part of a bigger
    system, such as when machine learning models get used in hospitals to help clinicians
    in disease diagnosis. The considerations for these two different use cases are
    not the same. If you want to deploy a model in hospitals to be used directly by
    clinicians, you need to consider all the difficulties and planning needed to set
    up the proper production environment and all the software dependencies. You also
    need to make sure their local system has the necessary hardware requirements.
    Alternatively, you can provide your model through web applications. In this case,
    you need to ensure the security and privacy of the data that gets uploaded into
    your database.
  prefs: []
  type: TYPE_NORMAL
- en: Model mentoring is a critical part of the machine learning life cycle when it
    comes to collecting the necessary information and feedback. This feedback can
    then be used to improve or correct the data that’s used for modeling or improve
    the model’s training and testing. Monitoring machine learning models helps us
    ensure that the models in production provide predictions according to expectations.
    Three of the issues that could cause unreliable predictions by a machine learning
    model are data variance, data drift, and concept drift. Data drift and concept
    drift are considered two different types of model drift. Model drift is about
    different kinds of changes in the data, either features or output variables, that
    make predictions of a model irrelevant or ineffective on the new user data.
  prefs: []
  type: TYPE_NORMAL
- en: We will talk more about model deployment and monitoring and the engineering
    aspects of the machine learning life cycles in future chapters of this book, such
    as [*Chapter 10*](B16369_10.xhtml#_idTextAnchor286), *Versioning and Reproducible
    Machine* *Learning Modeling*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about different components of a machine learning
    life cycle, from data collection and selection to model training and evaluation
    and, finally, model deployment and monitoring. We also showed how modularizing
    the data processing, modeling, and deployment aspects of the machine learning
    life cycle helps in identifying opportunities for improving machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about concepts beyond improving the performance
    of machine learning models, such as impartial modeling and fairness, accountability,
    and transparency toward achieving responsible AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you provide two examples of data cleaning processes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you explain the difference between the one-hot and label encoding methods?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you use quantiles of a distribution to detect its outliers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What comes to your mind regarding the differences between the considerations
    of deploying a model locally for doctors versus deploying models behind chatbots
    in a banking system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Micci-Barreca, Daniele. *A preprocessing scheme for high-cardinality categorical
    attributes in classification and prediction problems*. ACM SIGKDD Explorations
    Newsletter 3.1 (2001): 27-32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basu, Anirban, *Software Quality Assurance, Testing and Metrics*, PRENTICE HALL,
    January 1, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
