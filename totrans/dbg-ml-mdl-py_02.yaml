- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Machine Learning Life Cycle
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习生命周期
- en: Machine learning modeling in practice, either at the industrial level or in
    academic research, is beyond writing a couple of lines of Python code to train
    and evaluate a model on a public dataset. Learning to write a piece of Python
    program to train a machine learning model using Python and `scikit-learn` or a
    deep learning model using `PyTorch` is a starting point for becoming a machine
    learning developer and specialist. In this chapter, you will learn about the components
    of the machine learning life cycle and how, while considering this life cycle
    when planning for machine learning modeling, it helps you in designing a valuable
    and scalable model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，机器学习建模，无论是在工业级别还是在学术研究中，都不仅仅是写几行Python代码来在公共数据集上训练和评估一个模型。学习编写Python程序来使用Python和`scikit-learn`或使用`PyTorch`的深度学习模型训练机器学习模型，是成为机器学习开发者和专家的起点。在本章中，你将了解机器学习生命周期的组件以及如何在规划机器学习建模时考虑这个生命周期，这有助于你设计一个有价值且可扩展的模型。
- en: 'Here are the topics, including the main components of the machine learning
    life cycle, that will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题，包括机器学习生命周期的核心组件：
- en: Before we start modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们开始建模之前
- en: Data collection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data selection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据选择
- en: Data exploration
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索
- en: Data wrangling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Modeling data preparation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据建模准备
- en: Model training and evaluation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练和评估
- en: Testing the code and the model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试代码和模型
- en: Model deployment and monitoring
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署和监控
- en: By the end of this chapter, you will have learned how to design a machine learning
    life cycle for your projects and why modularizing your projects into the components
    of a life cycle helps you in your collaborative model developments. You will have
    also learned about some of the techniques and their Python implementations for
    different components of a machine learning life cycle, such as data wrangling
    and model training and evaluation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学会如何为你的项目设计机器学习生命周期，以及为什么将你的项目模块化到生命周期的组件中有助于你在协作模型开发中。你还将了解机器学习生命周期不同组件的一些技术和它们的Python实现，例如数据清洗和模型训练与评估。
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要求应考虑在本章中，因为它们将帮助你更好地理解概念，在项目中使用它们，并使用提供的代码进行实践：
- en: 'Python library requirements:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`sklearn` >= 1.2.2'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` >= 1.2.2'
- en: '`numpy` >= 1.22.4'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` >= 1.22.4'
- en: '`pandas` >= 1.4.4'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` >= 1.4.4'
- en: '`matplotlib` >= 3.5.3'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` >= 3.5.3'
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，地址为[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter02)。
- en: Before we start modeling
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在我们开始建模之前
- en: 'Before collecting data as the starting point of a machine learning life cycle,
    you need to know your objectives. You need to know what problems you want to solve
    and then define smaller subproblems that would be machine learning solvable. For
    example, in the case of a problem such as, “*How could we reduce the number of
    fragile products returned to a manufacturing facility?,*” the subproblems could
    be as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据作为机器学习生命周期的起点之前，你需要了解你的目标。你需要知道你想要解决什么问题，然后定义一些较小的子问题，这些子问题可以通过机器学习来解决。例如，在像“*我们如何减少返回制造工厂的易碎产品数量？*”这样的问题中，子问题可能如下：
- en: '*How could we detect the cracks* *before packaging?*'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们如何在包装前检测裂缝？*'
- en: '*How could we design better packaging to protect the products and reduce* *transportation-caused
    cracks?*'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们如何设计更好的包装来保护产品并减少运输造成的裂缝？*'
- en: '*Could we use better materials to reduce the risk* *of cracking?*'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否使用更好的材料来降低**开裂**的风险？
- en: '*Could we apply small design changes to our product that do not change its
    functionality but reduce the risk* *of cracking?*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否对产品进行一些小的设计改动，这些改动不会改变其功能，但可以降低**开裂**的风险？
- en: Once you have identified your subproblems, you can find out how you can use
    machine learning for each and go through a machine learning life cycle for the
    defined subproblems. Each of the subproblems may need specific data processing
    and machine learning modeling, and some of them could be easier to solve compared
    to the rest.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您已经确定了您的子问题，您就可以找出如何使用机器学习来解决每个问题，并针对定义的子问题进行机器学习生命周期。每个子问题可能需要特定的数据处理和机器学习建模，其中一些可能比其他问题更容易解决。
- en: '*Figure 2**.1* shows the major steps in machine learning life cycles. Some
    of these names are not universally defined. For example, data exploration sometimes
    gets included in data wrangling. But all these steps are required, even if they
    are named differently in different resources:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.1* 展示了机器学习生命周期中的主要步骤。其中一些名称并非普遍定义。例如，数据探索有时会被包含在数据处理中。但所有这些步骤都是必需的，即使在不同资源中它们的名称可能不同：'
- en: '![Figure 2.1 – Machine learning life cycle](img/B16369_02_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.1 – 机器学习生命周期](img/B16369_02_01.jpg)'
- en: Figure 2.1 – Machine learning life cycle
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 机器学习生命周期
- en: When you rely on a dataset that’s already available in Python, through `scikit-learn`
    or `PyTorch`, for example, or a dataset that is ready for modeling in public repositories,
    you don’t need to worry about the early steps, such as data collection, selection,
    and wrangling. These steps have already been taken care of for you. Or if you
    are just doing modeling for practice and don’t want to provide your model in a
    production system, you don’t need to worry about model deployment and monitoring.
    But understanding the meaning, importance, and benefits of all these steps helps
    you develop or design a functional technology with continuous improvement to be
    provided for users. It also helps you better understand your role as a machine
    learning developer or find your first job or a better job in this field.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当您依赖已经在 Python 中可用的数据集，例如通过 `scikit-learn` 或 `PyTorch`，或者一个在公共存储库中准备好的用于建模的数据集时，您不需要担心早期步骤，如数据收集、选择和处理。这些步骤已经为您处理好了。或者如果您只是进行建模练习，不想在生产系统中提供您的模型，您也不需要担心模型部署和监控。但理解所有这些步骤的意义、重要性和好处，有助于您开发或设计一个具有持续改进功能的技术，为用户提供服务。这也有助于您更好地理解作为机器学习开发者的角色，或者找到这个领域的第一份工作或更好的工作。
- en: Data collection
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: The first step in the machine learning life cycle is data collection. It could
    be about collecting data from different public or commercial databases, storing
    user data back into your database or any data storage system you have, or even
    using commercial entities that take care of data collection and annotation for
    you. If you are relying on free resources, the main consideration for you could
    be the space the data will get in your local or cloud-based storage system and
    the time you need to spend to collect the data and analyze it in future steps.
    But for paid data, either provided in commercial resources or generated by data
    collection, generation, and annotation companies, you need to assess the value
    of the data for modeling before you decide to pay for it.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习生命周期的第一步是数据收集。这可能涉及从不同的公共或商业数据库收集数据，将用户数据存储回您的数据库或任何您拥有的数据存储系统，或者甚至使用那些为您处理数据收集和标注的商业实体。如果您依赖免费资源，您可能需要考虑数据在您本地或云存储系统中所占的空间以及您在后续步骤中收集和分析数据所需的时间。但对于付费数据，无论是商业资源中提供的还是数据收集、生成和标注公司生成的，在决定付费之前，您需要评估数据对建模的价值。
- en: Data selection
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据选择
- en: Depending on the objectives of the corresponding projects, we need to select
    the required data for model training and testing. For example, you might have
    access to information about cancer patients in one or multiple hospitals, such
    as their age, gender, whether they smoke or not, their genetic information if
    available, their MRI or CT scans if available, history of their medication, their
    response to cancer drugs, whether they had surgery or not, their prescriptions,
    either handwritten or in PDF format, and much more. When you want to build a machine
    learning model to predict the response of patients to therapy using their CT scans,
    you need to select different data for each patient compared to when you want to
    build a model using their information, such as age, gender, and smoking status.
    You also need to select patients from whom you have the input and output data
    available if you are building a supervised learning model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据相应项目的目标，我们需要选择用于模型训练和测试所需的数据。例如，你可能可以访问一个或多个医院中癌症患者的相关信息，例如他们的年龄、性别、是否吸烟，如果有的话，他们的遗传信息，如果有的话，他们的MRI或CT扫描，他们用药的历史，他们对癌症药物的反应，他们是否进行了手术，他们的处方，无论是手写还是PDF格式，以及更多。当你想要构建一个使用他们的CT扫描来预测患者对治疗反应的机器学习模型时，你需要为每位患者选择与你想使用他们的信息（如年龄、性别和吸烟状况）构建模型时不同的数据。如果你正在构建一个监督学习模型，你还需要选择那些你有输入和输出数据的患者。
- en: Note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is possible to combine data points with and without outputs in a semi-supervised
    learning model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习模型中，可以将带有和没有输出的数据点结合起来。
- en: Selecting relevant data for your models is not an easy task as the information
    that separates data as *relevant* and *irrelevant* from your model objective is
    not necessarily available in this binary way. Imagine you need to extract data
    from a chemical, biological, or physical database, which could be a collection
    of data from different smaller datasets, supplementary materials of papers, or
    even data coming from within scientific articles. Or perhaps you want to extract
    information from the medical records of patients or even from written answers
    to an economical or sociological survey. In all such examples, separation of data
    for your model, or querying relevant data from relevant databases, is not as simple
    as searching for one keyword. Each keyword could have synonyms, either in plain
    English or in technical terms, could be written in different ways, or even sometimes
    the relevant information could exist in different columns of a data file or a
    relational database. Proper data selection and query systems provide you with
    a huge opportunity to improve your models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的模型选择相关数据并不是一项容易的任务，因为将数据区分成对模型目标来说是*相关*和*不相关*的信息并不一定以这种二进制方式可用。想象一下，你需要从一个化学、生物或物理数据库中提取数据，这可能是一组来自不同较小数据集的数据，论文的补充材料，甚至是从科学文章中来的数据。或者，你可能想要从患者的病历或甚至从经济或社会学调查的书面答案中提取信息。在所有这些例子中，为你的模型分离数据，或从相关数据库中查询相关数据，并不像搜索一个关键词那么简单。每个关键词可能有同义词，无论是普通英语还是技术术语，可能以不同的方式书写，有时相关信息可能存在于数据文件或关系数据库的不同列中。适当的数据选择和查询系统为你提供了提高模型的机会。
- en: You can benefit from a literature review and asking experts, if needed, to extend
    the keywords you are using. You can benefit from known data selection methods
    for specific tasks you have or even license tools or pay for services to help
    you in extracting more relevant data for your objectives. There are also advanced
    natural language processing techniques to help you in your query system from text.
    We will discuss these in [*Chapter 13*](B16369_13.xhtml#_idTextAnchor342), *Advanced
    Deep Learning Techniques*, and [*Chapter 14*](B16369_14.xhtml#_idTextAnchor379),
    *Introduction to Recent Advancements in* *Machine Learning*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以受益于文献综述，并在需要时向专家咨询，以扩展你使用的关键词。你可以从已知的数据选择方法中受益，对于你拥有的特定任务，甚至可以许可工具或付费服务来帮助你提取更多与你的目标相关的数据。还有高级的自然语言处理技术可以帮助你在查询系统中从文本中提取信息。我们将在[*第13章*](B16369_13.xhtml#_idTextAnchor342)，*高级深度学习技术*，和[*第14章*](B16369_14.xhtml#_idTextAnchor379)，*机器学习最新进展介绍*中讨论这些内容。
- en: Data exploration
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: In this stage, your data is selected and you can explore the quantity, quality,
    sparsity, and format of the data. You can find the number of data points in each
    class if you have categorical output in supervised learning, distribution of features,
    confidence in output variables, if available, and other characteristics of the
    data you get out of the *data selection* stage. This process helps you identify
    issues with your data that need to be fixed in *data wrangling*, which is the
    next step in the life cycle, or opportunities for improving your data by revising
    your *data* *selection* process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，您可以选择数据并探索数据的数量、质量、稀疏性和格式。如果您在监督学习中具有分类输出，您可以找到每个类别的数据点数量，特征分布，输出变量的置信度（如果有的话），以及从*数据选择*阶段获取的数据的其他特征。这个过程有助于您识别需要修复的数据问题，这些问题将在生命周期中的下一个步骤*数据整理*中解决，或者通过修改您的*数据选择*过程来提高数据的机会。
- en: Data wrangling
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理
- en: Your data needs to go through structuring and enriching processes and be transformed
    and cleaned up, if necessary. All these aspects are part of data wrangling.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数据需要经过结构化和丰富过程，并在必要时进行转换和清理。所有这些方面都是数据整理的一部分。
- en: Structuring
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化
- en: The raw data might come in different formats and sizes. You might have access
    to handwritten notes, Excel sheets, or even images of tables that contain information
    that needs to be extracted and put in the right format for further analysis and
    used for modeling. This process is not about transforming all data into a table-like
    format. In the process of data structuring, you need to be careful regarding information
    loss. For example, you could have features that are in a specific order, such
    as based on time, date, or the sequence of information coming through a device.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据可能以不同的格式和大小出现。您可能可以访问手写笔记、Excel表格，甚至包含需要提取并放入正确格式以供进一步分析和用于建模的信息的表格图像。这个过程并不是将所有数据转换成类似表格的格式。在数据结构化的过程中，您需要小心信息丢失。例如，您可能有一些按特定顺序排列的特征，如基于时间、日期或通过设备传入的信息序列。
- en: Enriching
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 丰富
- en: After structuring and formatting your data, you need to assess whether you have
    the right data to build a machine learning model of that cycle. You might identify
    opportunities to add or generate new data before continuing the wrangling process.
    For example, you might find out that in the data for identifying cracks in images
    of products in a manufacturing pipeline, you only have 50 out of 10,000 images
    that are labeled as images of cracked products. You might be able to find other
    images of cracked products or you could generate new images using a process called
    **data augmentation**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据进行结构和格式化之后，您需要评估您是否拥有构建该周期机器学习模型所需的数据。在继续整理过程之前，您可能发现添加或生成新数据的机会。例如，您可能会发现，在用于识别制造管道中产品图像裂缝的数据中，只有50张标签为裂缝产品图像的图像，而总共有10,000张图像。您可能能够找到其他裂缝产品的图像，或者您可以使用称为**数据增强**的过程生成新的图像。
- en: Data augmentation
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强
- en: Data augmentation is a series of techniques for generating new data points,
    computationally, using the original dataset we have at hand. For example, if you
    rotate your portrait, or change the quality of an image by adding Gaussian noise
    to it, the new image will still show your face. But it could help your model to
    be more generalizable. We will talk about different data augmentation techniques
    in [*Chapter 5*](B16369_05.xhtml#_idTextAnchor183), *Improving the Performance
    of Machine* *Learning Models*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是一系列通过使用我们手头上的原始数据集，计算性地生成新数据点的技术。例如，如果您旋转您的肖像，或者通过向图像添加高斯噪声来改变图像的质量，新的图像仍然会显示您的脸。但这可能有助于使您的模型更具泛化能力。我们将在[*第五章*](B16369_05.xhtml#_idTextAnchor183)
    *提高机器学习模型性能*中讨论不同的数据增强技术。
- en: Data transformation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'The features and the output of datasets could be different types of variables,
    including the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的特征和输出可能是不同类型的变量，包括以下内容：
- en: '**Quantitative** **or numerical**:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定量**或**数值**：'
- en: '**Discrete**: For example, the number of houses in a neighborhood'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散**：例如，一个社区中的房屋数量'
- en: '**Continuous**: For example, the age or weight of patients'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续**：例如，患者的年龄或体重'
- en: '**Qualitative** **or categorical**:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定性**或**分类**：'
- en: '**Nominal (no order)**: For example, different colors of cars'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名义（无顺序）**：例如，不同颜色的汽车'
- en: '**Ordinal (qualitative variable with order)**: For example, grades of students,
    such as A, B, C, or D'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有序（有序的定性变量）**：例如，学生的成绩，如A、B、C或D'
- en: 'When we train a machine learning model, the model needs to use numerical values
    to calculate the loss function in each iteration of the optimization process.
    Hence, we need to transform categorical variables into numerical alternatives.
    There are multiple feature encoding techniques, three of which are one-hot encoding,
    target encoding (Micci-Barreca, 2001), and label encoding. A one-hot, label, and
    target encoding calculation for an example matrix of four columns, including age,
    gender, group, and target, and seven rows, as seven example data points, is shown
    in *Figure 2**.2*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个机器学习模型时，模型需要在优化过程的每次迭代中使用数值来计算损失函数。因此，我们需要将分类变量转换为数值替代品。有多种特征编码技术，其中三种是独热编码、目标编码（Micci-Barreca，2001）和标签编码。一个包含年龄、性别、组和目标等四个列和七个行（七个示例数据点）的示例矩阵的独热、标签和目标编码计算如图2.2所示。2*：
- en: "![Figure 2.2 – Manual calculations for one-hot, target, and \uFEFFlabel encoding\
    \ using a simple example dataset with four features and seven data points](img/B16369_02_02.jpg)"
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 使用具有四个特征和七个数据点的简单示例数据集进行独热、目标和标签编码的手动计算](img/B16369_02_02.jpg)'
- en: Figure 2.2 – Manual calculations for one-hot, target, and label encoding using
    a simple example dataset with four features and seven data points
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 使用具有四个特征和七个数据点的简单示例数据集进行独热、目标和标签编码的手动计算
- en: 'This is an imaginary dataset for predicting the response of patients to a drug,
    with the target column as the output. Variable categories are abbreviated as F:
    Female, M: Male, H1: Hospital 1, H2: Hospital 2, and H3: Hospital 3\. In reality,
    many more variables need to be considered and more data points are necessary to
    have a reliable model for drug response prediction and assess whether there are
    biases in the response of patients to drugs between male and female groups or
    in different hospitals.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于预测患者对药物反应的假设数据集，目标列作为输出。变量类别缩写为F：女性，M：男性，H1：医院1，H2：医院2，和H3：医院3。在现实中，需要考虑更多的变量，并且需要更多的数据点来有一个可靠的药物反应预测模型，并评估男性组和女性组之间或不同医院之间患者对药物反应是否存在偏差。
- en: Each of these techniques has its benefits and caveats. For example, one-hot
    encoding increases the number of features (that is, the dimensionality of the
    dataset) and increases the chance of overfitting. Label encoding assigns integer
    values to each category, which do not necessarily have a meaning. For example,
    considering male as 1 and female as 0 is arbitrary and doesn’t have any real meaning.
    Target encoding is an alternative approach that considers the probabilities of
    each category concerning the target. You can read the mathematical details of
    this process in Micci-Barreca, 2001\. Python’s implementation of these approaches
    is provided in the following code snippets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术各有其优点和缺点。例如，独热编码增加了特征的数量（即数据集的维度）并增加了过拟合的机会。标签编码将整数值分配给每个类别，这些值不一定有意义。例如，将男性视为1，女性视为0是任意的，并且没有任何实际意义。目标编码是一种考虑每个类别相对于目标的概率的替代方法。您可以在Micci-Barreca，2001中阅读此过程的数学细节。以下代码片段提供了这些方法的Python实现。
- en: 'Let’s define a synthetic DataFrame to use for feature encoding:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个用于特征编码的合成DataFrame：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we will use label encoding to encode the categorical features in the
    defined DataFrame:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用标签编码对定义的DataFrame中的分类特征进行编码：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we will try to perform one-hot encoding for categorical feature transformation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将尝试对分类特征进行独热编码：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we will implement target encoding in Python, after installing the `category_encoders`
    library, as the third encoding approach, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将安装`category_encoders`库后，在Python中实现目标编码，作为第三种编码方法，如下所示：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Ordinal variables can also be transformed using the `OrdinalEncoder` class as
    part of `sklearn.preprocessing`. The difference between ordinal and nominal transformation
    is the meaning behind the order of categories in ordinal variables. For example,
    if we are encoding grades of students, A, B, C, and D could be transformed into
    1, 2, 3, and 4, or 4, 3, 2, and 1, but transforming them into 1, 3, 4, and 2 will
    not be acceptable as it is changing the meaning behind the order of the grades.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 有序变量也可以通过`OrdinalEncoder`类作为`sklearn.preprocessing`的一部分进行转换。有序变量和名义变量转换之间的区别在于有序变量中类别顺序背后的含义。例如，如果我们正在编码学生的成绩，A、B、C和D可以转换为1、2、3和4，或者4、3、2和1，但将它们转换为1、3、4和2将不可接受，因为这改变了成绩顺序背后的含义。
- en: Output variables can also be categorical. You can use label encoding to transform
    a nominal output into a numerical variable for classification models.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出变量也可以是分类变量。您可以使用标签编码将名义输出转换为数值变量，以便用于分类模型。
- en: Cleaning
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清洗
- en: After structuring the data, it needs to be cleaned. Cleaning data helps increase
    the quality of your data and makes it closer to being ready for modeling. An example
    of a cleaning process is filling in missing values in your data. For example,
    if you want to use patients’ living habits to predict their risk of getting diabetes
    using their responses to a survey, you might find out some of the participants
    didn’t respond to the questions about their smoking habits.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据结构化后，需要对其进行清洗。清洗数据有助于提高数据质量，使其更接近建模准备状态。清洗过程的一个例子是在数据中填充缺失值。例如，如果您想使用患者的居住习惯来预测他们患糖尿病的风险，并使用他们对调查的回应，您可能会发现一些参与者没有回答有关他们吸烟习惯的问题。
- en: Feature imputation for filling in missing values
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征插补以填充缺失值
- en: 'The features of a dataset we have at hand could contain missing values. The
    majority of machine learning models and their corresponding Python implementations
    cannot handle missing values. In these cases, we need to either remove data points
    with missing feature values or somehow fill in those missing values. There are
    feature imputation techniques we can use to calculate the values of features that
    are missing in our dataset. Examples of such methods are shown in *Figure 2**.3*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手头的数据集的特征可能包含缺失值。大多数机器学习模型及其相应的Python实现都无法处理缺失值。在这些情况下，我们需要删除具有缺失特征值的数据点，或者以某种方式填充这些缺失值。我们可以使用特征插补技术来计算数据集中缺失的特征值。这些方法的示例如图
    *2.3* 所示：
- en: '![Figure 2.3 – Feature imputation techniques for calculating missing feature
    values](img/B16369_02_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 计算缺失特征值的特征插补技术](img/B16369_02_03.jpg)'
- en: Figure 2.3 – Feature imputation techniques for calculating missing feature values
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 计算缺失特征值的特征插补技术
- en: As you can see, either we can use other values of the same features and replace
    the missing values with the mean or median of the available values, or we can
    use other features with low or no missing values that have a high correlation
    with the feature with missing values. In the second case, we can use the feature
    with the highest correlation, with the target feature with missing values, to
    build a linear model. The linear model considers the correlated feature as input
    and the feature with missing values as output and then uses the predictions of
    the linear model to calculate the missing values.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们既可以使用相同特征的其它值，并用可用的值的均值或中位数来替换缺失值，也可以使用与缺失值特征高度相关的、低缺失值或无缺失值的其它特征。在后一种情况下，我们可以使用与目标缺失值特征相关性最高的特征来构建线性模型。线性模型将相关特征视为输入，将缺失值特征视为输出，然后使用线性模型的预测来计算缺失值。
- en: When we use a statistical summary of the values of the same feature, such as
    the mean or median, we are reducing the variance of the feature values as those
    summary values will be used for all the missing values of the same feature (*Figure
    2**.3*). On the other hand, when we use a linear model between the feature with
    missing values and a highly correlated feature with low or no missing values,
    we are assuming a linear relationship between them. Alternatively, we can build
    more complex models between features for missing value calculation. All these
    approaches have their benefits and limitations, and you need to choose the one
    that works best for your dataset, depending on the distribution of feature values,
    the fraction of data points with missing features values, the correlation range
    between features, the existence of features with low or no missing value, and
    other relevant factors.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用相同特征的值的统计摘要，如均值或中位数时，我们正在减少特征值的方差，因为这些摘要值将被用于相同特征的所有缺失值 (*图 2.3*)。另一方面，当我们使用具有缺失值特征和低缺失值或无缺失值的高相关特征之间的线性模型时，我们假设它们之间存在线性关系。或者，我们可以在特征之间构建更复杂的模型来进行缺失值计算。所有这些方法都有其优点和局限性，您需要根据特征值的分布、具有缺失特征值的数据点的比例、特征之间的相关范围、低缺失值或无缺失值特征的存在以及其他相关因素，选择最适合您数据集的方法。
- en: 'We used a very simple case of four features and five data points in *Figure
    2**.3* to showcase the discussed feature imputation techniques. But in reality,
    we need to build models with more than four features. We can use Python libraries
    such as `scikit-learn` for feature imputation by using the mean of the same feature
    values, as follows. First, we will import the required libraries:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图2.3*中使用了四个特征和五个数据点的非常简单的案例来展示所讨论的特征插补技术。但在现实中，我们需要构建具有超过四个特征的模型。我们可以使用Python库，如`scikit-learn`，通过使用相同特征值的平均值来进行特征插补，如下所示。首先，我们将导入所需的库：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we must define the two-dimensional input list, where each internal list
    shows the feature values of a data point:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须定义一个二维输入列表，其中每个内部列表显示数据点的特征值：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we are ready to fit a `SimpleImputer` function by specifying what needs
    to be considered as a missing value and what strategy to be used for imputation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好通过指定需要考虑哪些值作为缺失值以及使用哪种插补策略来拟合`SimpleImputer`函数：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also use `scikit-learn` to make a linear regression model that calculates
    missing feature values:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`scikit-learn`来创建一个线性回归模型，该模型计算缺失的特征值：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Outlier removal
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异常值移除
- en: Numerical variables in our datasets could have values that are far away from
    the rest of the data. They could be real values that are dissimilar to the rest
    of the data points or caused by errors in data generation, such as in experimental
    measurement processes. You can visually see and detect them using a boxplot (*Figure
    2**.4*). The circles of the plot are the outliers that get automatically detected
    by the plotting functions in Python, such as `matplotlib.pyplot.boxplot` (*Figure
    2**.4*). Although visualization is a good way of exploring our data and understanding
    the distribution of numerical variables, we need to have a quantitative way of
    detecting outliers without the need to plot the values of all the variables in
    our datasets.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的数值变量可能具有远离其他数据的值。它们可能是与数据点中的其他值不相似的真实值，或者是由数据生成过程中的错误引起的，例如在实验测量过程中。您可以使用箱线图（*图2.4*）直观地看到并检测到它们。图中的圆圈是Python中绘图函数（如`matplotlib.pyplot.boxplot`）自动检测到的异常值（*图2.4*）。尽管可视化是探索我们的数据和理解数值变量分布的好方法，但我们仍需要一个无需绘制数据集中所有变量值的定量方法来检测异常值。
- en: 'The simplest way of detecting outliers is by using quantiles of the distribution
    of variable values. Data points that are beyond the upper and lower bounds are
    considered outliers (*Figure 2**.4*). Lower and upper bounds can be calculated
    as Q1 - a.IQR and Q3 - a.IQR, where can be a real value between 1.5 and 3\. The
    common value of a, which is also used by default in drawing boxplots, is 1.5,
    but having higher values makes the process of outlier identification less stringent
    and lets fewer data points be detected as outliers. For example, by changing the
    stringency of outlier detection from the default (that is, a = 1.5) to a = 3,
    none of the data points in *Figure 2**.4* would be detected as outliers. This
    approach for outlier identification is non-parametric, meaning it doesn’t have
    any assumptions regarding the distribution of data points. Hence, it can be applied
    to non-normal distributions, such as the data shown in *Figure 2**.4*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检测异常值的最简单方法是使用变量值分布的分位数。超出上下界限的数据点被认为是异常值（*图2.4*）。下限和上限可以计算为Q1 - a.IQR和Q3 -
    a.IQR，其中a是一个介于1.5和3之间的实数值。a的常用值，也是绘制箱线图时的默认值，是1.5，但使用更高的值会使异常识别过程不那么严格，并让更少的数据点被检测为异常。例如，将异常检测的严格性从默认值（即a
    = 1.5）更改为a = 3，*图2.4*中的所有数据点都不会被检测为异常。这种异常识别方法是非参数的，这意味着它对数据点的分布没有任何假设。因此，它可以应用于非正态分布，例如*图2.4*中显示的数据：
- en: '![Figure 2.4 – Outliers in histograms and boxplots](img/B16369_02_04.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 直方图和箱线图中的异常值](img/B16369_02_04.jpg)'
- en: Figure 2.4 – Outliers in histograms and boxplots
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 直方图和箱线图中的异常值
- en: In the preceding figure, the plots were generated using the values of features
    in the diabetes dataset of the `scikit-learn` package, which was loaded via `sklearn.datasets.load_diabetes()`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，图表是使用`scikit-learn`包中糖尿病数据集的特征值生成的，该数据集是通过`sklearn.datasets.load_diabetes()`加载的。
- en: Data scaling
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据缩放
- en: The values of features, either originally numerical or after transformation,
    could have different ranges. Many machine learning models perform better, or at
    least their optimization processes converge faster, if their feature values get
    scaled and normalized properly. For example, if you have a feature ranging from
    0.001 to 0.05 and another one from 1,000 to 5,000, bringing both of them to a
    reasonable range such as [0, 1] or [-1, 1] could help improve the speed of convergence
    or the performance of your model. You need to make sure the scaling and normalizations
    you implement don’t cause ties in your feature values, meaning data points don’t
    lose their difference based on features that went under transformation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 特征的值，无论是原始数值还是经过转换后的，可能具有不同的范围。如果机器学习模型的特征值得到适当的缩放和归一化，许多模型的表现会更好，或者至少它们的优化过程会更快地收敛。例如，如果您有一个范围从0.001到0.05的特征，另一个范围从1,000到5,000的特征，将它们都调整到合理的范围，如[0,
    1]或[-1, 1]，可以帮助提高收敛速度或模型性能。您需要确保您实施的缩放和归一化不会导致特征值中的数据点失去差异，这意味着基于经过转换的特征的数据点不会失去它们之间的差异。
- en: The objective of scaling is to change the range of values of a variable. In
    normalization, the shape of the distribution of values could also change. You
    can use examples of these methods and the corresponding classes available in `scikit-learn`
    in your projects to improve the scale and distribution of your features (*Table
    2.1*). The resulting scaled variables after using each of these classes have specific
    characteristics. For example, the values of a variable after using the `StandardScalar`
    class of `scikit-learn` will be centered around zero with a standard deviation
    of one.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放的目标是改变变量的值域。在归一化中，值的分布形状也可能发生变化。您可以在项目中使用`scikit-learn`中提供的这些方法的示例和相应的类来改进您特征的缩放和分布（*表2.1*）。使用这些类中的每一个进行缩放后得到的缩放变量具有特定的特征。例如，使用`scikit-learn`的`StandardScaler`类后，变量的值将围绕零中心，标准差为1。
- en: 'Some of these techniques, such as robust scaling, which can be done using the
    `RobustScaler` class of `scikit-learn`, are less likely to be affected by outliers
    (*Table 2.1*). In robust scaling, outliers, based on the definition we provided,
    don’t affect how the median and *IQR* are calculated and, therefore, do not affect
    the scaling process. Outliers themselves then be scaled using the calculated median
    and *IQR*. Outliers can be either kept or removed before or after scaling, depending
    on the machine learning method used and the task at hand. But the important point
    is to detect them and be aware of them when you’re trying to prepare data for
    modeling and, if required, scale or remove them:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些技术，例如鲁棒缩放，可以使用`scikit-learn`的`RobustScaler`类实现，不太可能受到异常值的影响（*表2.1*）。在鲁棒缩放中，根据我们提供的定义，异常值不会影响中位数和*IQR*的计算，因此不会影响缩放过程。异常值本身可以使用计算出的中位数和*IQR*进行缩放。在缩放之前或之后，根据所使用的机器学习方法和任务，可以选择保留或删除异常值。但重要的是在尝试为建模准备数据时检测它们，并意识到它们，如果需要，可以对其进行缩放或删除：
- en: '| **Python Class** | **Mathematical Definition** | **Value Limits** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **Python类** | **数学定义** | **值限制** |'
- en: '| `sklearn.preprocessing.StandardScaler()` | Z = (X - u) / su: Means: Standard
    deviation | No limit>99% of data between -3 and 3 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `sklearn.preprocessing.StandardScaler()` | Z = (X - u) / su: 均值 | 无限制>99%的数据在-3和3之间
    |'
- en: '| `sklearn.preprocessing.MinMaxScaler()` | X_scaled = (X-Xmin)/(Xmax-Xmin)
    | [0,1] |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `sklearn.preprocessing.MinMaxScaler()` | X_scaled = (X-Xmin)/(Xmax-Xmin)
    | [0,1] |'
- en: '| `sklearn.preprocessing.MaxAbsScaler()` | X_scaled = X/&#124;X&#124;max |
    [-1,1] |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `sklearn.preprocessing.MaxAbsScaler()` | X_scaled = X/&#124;X&#124;max |
    [-1,1] |'
- en: '| `sklearn.preprocessing.RobustScaler()` | Zrobust = (X - Q2) / IQRQ2: MedianIQR:
    Interquartile range | No limitMajority of data between -3 and 3 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `sklearn.preprocessing.RobustScaler()` | Zrobust = (X - Q2) / IQRQ2: 中位数IQR:
    四分位距 | 无限制大多数数据在-3和3之间 |'
- en: Table 2.1 – Example of Python classes for scaling and normalizing feature values
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 – 缩放和归一化特征值的Python类示例
- en: Other forms of exploratory data analysis are conducted after data wrangling
    before machine learning modeling is started. Domain expertise could also help
    in identifying patterns whose interpretations need to be better understood regarding
    the subject domain for which the problem has been defined. To increase the likelihood
    of success for machine learning modeling, you may need feature engineering to
    build new features or learn new features through representation learning. These
    new features could be as simple as body mass index, defined as the ratio of someone’s
    weight in kilograms to the square of their height in meters. Or they could be
    new features and representations that are learned through complicated processes
    or extra machine learning modeling. We will talk about this later in [*Chapter
    14*](B16369_14.xhtml#_idTextAnchor379), *Introduction to Recent Advancements in*
    *Machine Learning*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Modeling data preparation
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this stage of a machine learning life cycle, we need to finalize the features
    and data points we want to use for modeling, as well as our model evaluation and
    testing strategies.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection and extraction
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original features that were normalized and scaled in previous steps can
    be now processed further to increase the likelihood of having a high-performance
    model. In general, features can either be sub-selected, meaning some of the features
    get thrown out, using a *feature selection* method, or be used to generate new
    features, which is traditionally called *feature extraction*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of feature selection is to reduce the number of features, or the dimensionality
    of your data, and keep features that are information-rich. For example, if we
    have 20,000 features and 500 data points, there is a high chance that most of
    the original 20,000 features are not informative when used to build a supervised
    learning model. The following list explains some simple techniques for feature
    selection:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Keeping features with a high variance or MAD across the data points
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping features with the highest number of unique values across the data points
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping representative features from groups of highly correlated features
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These processes can be conducted using all the data points or just training
    data to avoid potential information leakage between the training and test data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Combining original features linearly or nonlinearly could result in more informative
    features for building a predictive model. This process is called feature extraction
    and could be conducted based on domain knowledge or through different statistical
    or machine learning models. For example, you can use principal component analysis
    or isometric mapping to reduce the dimensionality of your data in a linear or
    non-linear way, respectively. Then, you can use these new features in your training
    and testing process. The Python implementation of these two approaches is provided
    in the following code snippets.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the required libraries and load the `scikit-learn` digit
    dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的库并加载`scikit-learn`数字数据集：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let’s use `isomap` and `pca`, both of which are available in `scikit-learn`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`isomap`和`pca`，它们都可在`scikit-learn`中找到：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The number of components you can select from each such method can be determined
    through different techniques. For example, the explained variance ratio is a commonly
    used approach to select the number of principal components. These are identified
    through principal component analysis and collectively explain more than a specific
    percentage, such as 70% of the total variance in a dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从每种方法中选择多少个组件可以通过不同的技术来确定。例如，解释方差比是选择主成分数量的常用方法。这些是通过主成分分析确定的，并且共同解释了超过特定百分比的总方差，例如在数据集中解释了70%的总方差。
- en: There are also more advanced techniques that are part of self-supervised pre-training
    and representation learning for identifying new features. In these techniques,
    large amounts of data are used to calculate new features, representations, or
    embeddings. For example, the English version of Wikipedia can be used to come
    up with better representations of English words rather than performing one-hot
    encoding for each word. We will talk about self-supervised learning models in
    [*Chapter 14*](B16369_14.xhtml#_idTextAnchor379), *Introduction to Recent Advancements
    in* *Machine Learning*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多高级技术，它们是自监督预训练和表示学习的一部分，用于识别新特征。在这些技术中，使用大量数据来计算新特征、表示或嵌入。例如，可以使用英文维基百科来提出更好的英文单词表示，而不是为每个单词执行独热编码。我们将在[*第14章*](B16369_14.xhtml#_idTextAnchor379)中讨论自监督学习模型，*机器学习最新进展介绍*。
- en: Designing an evaluation and testing strategy
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计评估和测试策略
- en: We need to specify our testing strategy before we train our model to identify
    its parameters or optimal hyperparameters. Model testing could be done by another
    team on separate datasets if you are working in a big organization. Alternatively,
    you can dedicate one or multiple datasets, separate from your training set, or
    separate part of your data so that you can test it separately from the training
    set. You also need to list the ways you want to assess the performance of your
    model in the testing stage. For example, you may need to specify the performance
    plots or measures you want to use, such as the **receiver operating curve** (**ROC**)
    and **precision-recall** (**PR**) curve, or other criteria, to select a new classification
    model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型以识别其参数或最佳超参数之前，我们需要指定我们的测试策略。如果你在一个大型组织中工作，模型测试可以由另一个团队在单独的数据集上完成。或者，你可以指定一个或多个数据集，这些数据集与你的训练集分开，或者将你的数据的一部分分开，以便你可以单独测试它。你还需要列出你希望在测试阶段评估模型性能的方法。例如，你可能需要指定你想要使用的性能图表或度量，如**接收者操作特征曲线**（**ROC**）和**精确率-召回率**（**PR**）曲线，或其他标准，以选择新的分类模型。
- en: Once your testing strategy has been defined, you can use the rest of the data
    to specify training and validation sets. Validation and training sets don’t need
    to be one series of fixed data points. We can use *k*-fold **cross-validation**
    (**CV**) to split a dataset into *k* chunks and use one chunk at a time as a validation
    set and the rest as the training set. Then, the average of the performance across
    all *k* chunks can be used as a validation set to calculate the validation’s performance.
    Training performance is important for finding optimal values for model parameters
    based on the objective of the model. You can also use validation performance to
    identify optimal hyperparameter values. If you specify one validation set or use
    *k*-fold CV, you can use the validation performance of different hyperparameter
    combinations to identify the best one. Then, the best hyperparameter set can be
    used to train the model on all data, excluding test data, so that you can come
    up with the final model to be tested in the testing stage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了测试策略，你就可以使用剩余的数据来指定训练集和验证集。验证集和训练集不需要是一系列固定的数据点。我们可以使用*k*-折**交叉验证**（**CV**）将数据集分成*k*个块，每次使用一个块作为验证集，其余的作为训练集。然后，可以使用所有*k*个块的平均性能作为验证集来计算验证性能。训练性能对于根据模型的目标找到模型参数的最佳值非常重要。你还可以使用验证性能来识别最佳超参数值。如果你指定了一个验证集或使用*k*-折CV，你可以使用不同超参数组合的验证性能来识别最佳组合。然后，可以使用最佳超参数集在所有数据上训练模型，排除测试数据，以便在测试阶段提出最终要测试的模型。
- en: 'There are some common practices for each application regarding the number of
    folds (that is, *k*) or fraction of data points to be separated as validation
    and test sets. For small datasets, 60%, 30%, and 10% are commonly used to specify
    the training, validation, and testing fraction of data points, respectively. But
    both the number of data points and their diversity are important factors in deciding
    on the number of data points within validation and test sets or specifying *k*
    in CV. You can also use available Python classes that perform training and validation
    using *k*-fold CV with your choice of *k*, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个应用程序，关于折叠数（即 *k*）或要分离为验证集和测试集的数据点分数的一些常见做法。对于小型数据集，通常使用 60%、30% 和 10% 分别指定数据点的训练、验证和测试分数。但是，数据点的数量及其多样性都是决定验证集和测试集中数据点数量或在
    CV 中指定 *k* 的重要因素。您还可以使用可用的 Python 类，这些类使用您选择的 *k* 进行训练和验证，如下所示：
- en: '[PRE10]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This returns the following output:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Preferably, the data you prepared in each of these stages shouldn’t just get
    dumped in the cloud or a hard drive, or get added to a database after each of
    the previous steps in a life cycle. It is beneficial to have a report attached
    to the data to track historical efforts in each step and provide that information
    for other individuals or teams within your team or organization. Proper reporting,
    such as on data wrangling, could provide feedback-seeking opportunities to help
    you improve data provided for machine learning modeling.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最好，在每个阶段准备的数据不应该只是简单地存放在云端或硬盘上，或者在每个生命周期的前一步之后添加到数据库中。将报告附加到数据上以跟踪每个步骤的历史努力，并为团队或组织内的其他个人或团队提供这些信息是有益的。适当的报告，如关于数据清洗的，可以提供寻求反馈的机会，以帮助您改进为机器学习建模提供的数据。
- en: Model training and evaluation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练和评估
- en: 'The process of training and validating or testing a model consists of the following
    three major steps if you use `scikit-learn` or `PyTorch` and TensorFlow for neural
    network modeling:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 `scikit-learn` 或 `PyTorch` 和 TensorFlow 进行神经网络建模，训练和验证或测试模型的过程包括以下三个主要步骤：
- en: '**Initializing the model**: Initializing a model is about specifying the method,
    its hyperparameters, and the random state to be used for modeling.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化模型**：初始化模型是关于指定方法、其超参数以及用于建模的随机状态。'
- en: '**Training the model**: In model training, the initialized model in *Step 1*
    gets used on the training data to train a machine learning model.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：在模型训练中，*步骤 1* 中初始化的模型用于训练数据以训练机器学习模型。'
- en: '**Inference, assignment, and performance assessment**: In this step, the trained
    model can be used for inference (for example, predicting outputs) in supervised
    learning or, for example, assigning new data points to identified clusters in
    unsupervised learning. In supervised learning, you can use these predictions for
    model performance assessment.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理、分配和性能评估**：在这个步骤中，训练好的模型可以用于监督学习中的推理（例如，预测输出）或，例如，将新数据点分配给无监督学习中已识别的聚类。在监督学习中，您可以使用这些预测来评估模型性能。'
- en: These steps are similar for both supervised learning and unsupervised learning
    models. In *Steps 1* and *2*, both types of models can be trained. Python’s implementation
    of these three steps using `scikit-learn` is provided in the following code snippets
    for the random forest classifier and *k*-means clustering.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤对于监督学习和无监督学习模型都是相似的。在 *步骤 1* 和 *步骤 2* 中，两种类型的模型都可以进行训练。以下代码片段提供了使用 `scikit-learn`
    实现这三个步骤的 Python 实现，用于随机森林分类器和 *k*-均值聚类。
- en: 'First, let’s import the required libraries and load the `scikit-learn` breast
    cancer dataset:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的库并加载 `scikit-learn` 乳腺癌数据集：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can use a random forest to train and test a supervised learning model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用随机森林来训练和测试一个监督学习模型：
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This code prints out the following performance on the test set:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码在测试集上打印出以下性能：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can also build a *k*-means clustering model, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以构建一个 *k*-均值聚类模型，如下所示：
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you don’t have enough experience in machine learning modeling, the methodologies
    and corresponding Python classes provided in *Table 2.2* could be a good starting
    point:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在机器学习建模方面没有足够的经验，*表 2.2* 中提供的方法和相应的 Python 类可能是一个好的起点：
- en: '| **Type** | **Method** | **Python Class** |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **类型** | **方法** | **Python 类** |'
- en: '| Classification | Logistic regression | `sklearn.linear_model.LogisticRegression()`
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 逻辑回归 | `sklearn.linear_model.LogisticRegression()` |'
- en: '| K-nearest neighbors | `sklearn.neighbors.KNeighborsClassifier()` |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| K-最近邻 | `sklearn.neighbors.KNeighborsClassifier()` |'
- en: '| Support vector machine classifier | `sklearn.svm.SVC()` |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机分类器 | `sklearn.svm.SVC()` |'
- en: '| Random forest classifier | `sklearn.ensemble.RandomForestClassifier()` |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林分类器 | `sklearn.ensemble.RandomForestClassifier()` |'
- en: '| XGBoost classifier | `xgboost.XGBClassifier()` |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost 分类器 | `xgboost.XGBClassifier()` |'
- en: '| LightGBM classifier | `Lightgbm.LGBMClassifier()` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| LightGBM 分类器 | `Lightgbm.LGBMClassifier()` |'
- en: '| Regression | Linear regression | `sklearn.linear_model.LinearRegression()`
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 回归 | 线性回归 | `sklearn.linear_model.LinearRegression()` |'
- en: '| Support vector machine regressor | `sklearn.svm.SVR()` |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 支持向量机回归器 | `sklearn.svm.SVR()` |'
- en: '| Random forest regressor | `sklearn.ensemble.RandomForestRegressor()` |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林回归器 | `sklearn.ensemble.RandomForestRegressor()` |'
- en: '| XGBoost regressor | `xgboost.XGBRegressor()` |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost 回归器 | `xgboost.XGBRegressor()` |'
- en: '| LightGBM regressor | `Lightgbm.LGBMRegressor()` |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| LightGBM 回归器 | `Lightgbm.LGBMRegressor()` |'
- en: '| Clustering | K-means clustering | `sklearn.cluster.KMeans()` |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | K-均值聚类 | `sklearn.cluster.KMeans()` |'
- en: '| Agglomerative clustering | `sklearn.cluster.AgglomerativeClustering()` |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 聚类层次 | `sklearn.cluster.AgglomerativeClustering()` |'
- en: '| DBSCAN clustering | `sklearn.cluster.DBSCAN()` |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| DBSCAN 聚类 | `sklearn.cluster.DBSCAN()` |'
- en: '| UMAP | `umap.UMAP()` |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| UMAP | `umap.UMAP()` |'
- en: Table 2.2 – Starting methods and their Python classes for your supervised learning
    or clustering problems with tabular data
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – 对于你的监督学习或聚类问题的表格数据，开始方法和它们的Python类
- en: Note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: UMAP is a dimensionality reduction approach that provides lower dimensional
    visualization, such as a 2D plot of a series of data points. The resulting groups
    of data points in the lower dimensional space can also be used as reliable clusters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: UMAP是一种降维方法，它提供了低维可视化，例如一系列数据点的2D图。在低维空间中形成的数据点组也可以用作可靠的聚类。
- en: Testing the code and the model
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试代码和模型
- en: 'Although the performance of a machine learning model that is selected and brought
    to this stage of the life cycle can be further tested using one or multiple datasets,
    there are a series of tests that need to be done in this stage to make sure of
    this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管选定的机器学习模型在生命周期这一阶段可以使用一个或多个数据集进行进一步测试，但在这个阶段还需要进行一系列测试以确保这一点：
- en: Ensuring the process of deployment and bringing the model into production goes
    smoothly
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保部署过程和将模型投入生产的流程顺利进行
- en: Ensuring the model will work as expected from a performance and computational
    cost perspective
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保模型在性能和计算成本方面按预期工作
- en: Ensuring that using the model in production will not have legal and financial
    implications
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保在生产中使用模型不会产生法律和财务影响
- en: 'Here are some such tests that can be used in this stage:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段可以使用的此类测试包括：
- en: '**Unit tests**: These are fast tests that make sure our code runs correctly.
    These tests are not specific to machine learning modeling and not even to this
    stage. Throughout the life cycle, you need to design unit tests to make sure your
    data processing and modeling code runs as expected.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元测试**：这些是快速测试，确保我们的代码运行正确。这些测试不仅针对机器学习建模，甚至不是针对这个阶段。在整个生命周期中，你需要设计单元测试以确保你的数据处理和建模代码按预期运行。'
- en: '**A/B testing**: This type of testing helps you, your team, and your organization
    in deciding whether to select a model or reject it. The idea of this test is to
    assess two possible scenarios, such as two models, or two different designs of
    the frontend, and check which one is more favorable. But you need to quantitatively
    assess the result by deciding *what needs to be measured* and your *selection
    criteria*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B 测试**：这种测试可以帮助你、你的团队以及你的组织决定是否选择或拒绝一个模型。这种测试的想法是评估两种可能的场景，例如两个模型，或者前端设计的两种不同版本，并检查哪一个更有利。但是，你需要通过决定*需要测量什么*和你的*选择标准*来定量评估结果。'
- en: '`scikit-learn`, `PyTorch`, or TensorFlow changes, this test makes sure your
    code runs and checks the effects of those changes on model performance and predictions.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`、`PyTorch`或TensorFlow的更改，这个测试确保你的代码运行，并检查这些更改对模型性能和预测的影响。'
- en: '**Security tests**: Security testing is an important part of programming and
    modeling at an industrial level. You need to make sure your code and dependencies
    are not vulnerable. However, you need to design a test for advanced adversarial
    attacks. We will discuss this in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119),
    *Debugging toward* *Responsible AI*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全测试**：安全测试是工业级编程和建模的重要部分。你需要确保你的代码和依赖项没有漏洞。然而，你需要设计一个测试来应对高级对抗性攻击。我们将在[*第3章*](B16369_03.xhtml#_idTextAnchor119)中讨论，*向负责任AI调试*。'
- en: '**Responsible AI test**: We need to design tests to assess the important factors
    of responsible AI, such as transparency, privacy, and fairness. We will go through
    some important aspects of responsible AI in the next chapter.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负责任的AI测试**：我们需要设计测试来评估负责任AI的重要因素，例如透明度、隐私和公平性。我们将在下一章中讨论负责任AI的一些重要方面。'
- en: Although these kinds of tests need to be designed for this stage, similar ones
    could be integrated as part of previous steps of the life cycle. For example,
    you can have security testing in all steps of the life cycle, especially if you
    are using different tools or code bases. There could be other tests such as checking
    the memory size and prediction runtime of a model or whether the format and structure
    of data in production and what is expected in the deployed model are the same.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些测试需要在这个阶段设计，但类似的测试可以作为生命周期之前步骤的一部分进行整合。例如，你可以在生命周期的所有步骤中进行安全测试，特别是如果你正在使用不同的工具或代码库。可能还有其他测试，例如检查模型的内存大小和预测运行时间，或者生产中的数据格式和结构以及部署的模型中预期的数据是否相同。
- en: Model deployment and monitoring
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署和监控
- en: 'If you are new to deployment, you might think of it as how to develop a frontend,
    mobile application, or API for end users of your models. But that is not what
    we want to talk about in this book. There are two important aspects of deployment
    that we want to cover here and in future chapters: the actions needed to provide
    a model in production and integrating a model into a process that is supposed
    to benefit the users.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触部署，你可能认为它是如何为你的模型最终用户开发前端、移动应用程序或API。但在这本书中，我们不想讨论这个话题。在这里和未来的章节中，我们想要涵盖部署的两个重要方面：提供生产环境中模型所需的操作以及将模型集成到应该为用户带来利益的过程中。
- en: When you deploy your model, your code should run properly in the designated
    environment and have access to the required hardware, such as the GPU, and users’
    data needs to be accessible in the right format for your model to work. Some of
    the tests that we talked about in the *testing* stage of the life cycle make sure
    that your model runs as expected in the production environment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署你的模型时，你的代码应该在指定的环境中正常运行，并且能够访问所需的硬件，例如GPU，并且用户的数据需要以正确的格式可用，以便你的模型能够工作。我们在生命周期测试阶段讨论的一些测试确保你的模型在生产环境中按预期运行。
- en: When we talk about providing a model in a production environment, it either
    gets used behind the scenes for the benefit of the user, such as when Netflix
    and Amazon Prime suggest movies to you using their machine learning models, or
    gets used directly by the user as a standalone process or as part of a bigger
    system, such as when machine learning models get used in hospitals to help clinicians
    in disease diagnosis. The considerations for these two different use cases are
    not the same. If you want to deploy a model in hospitals to be used directly by
    clinicians, you need to consider all the difficulties and planning needed to set
    up the proper production environment and all the software dependencies. You also
    need to make sure their local system has the necessary hardware requirements.
    Alternatively, you can provide your model through web applications. In this case,
    you need to ensure the security and privacy of the data that gets uploaded into
    your database.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论在生产环境中提供模型时，它要么在幕后被用于用户的利益，例如Netflix和Amazon Prime使用他们的机器学习模型为你推荐电影，要么被用户直接作为独立进程或作为更大系统的一部分使用，例如当机器学习模型在医院中用于帮助临床医生进行疾病诊断。这两种不同用例的考虑因素并不相同。如果你想将模型部署在医院中供临床医生直接使用，你需要考虑设置适当的生产环境所需的全部困难和规划，以及所有软件依赖项。你还需要确保他们的本地系统满足必要的硬件要求。或者，你可以通过Web应用程序提供你的模型。在这种情况下，你需要确保上传到你的数据库中的数据的保密性和安全性。
- en: Model mentoring is a critical part of the machine learning life cycle when it
    comes to collecting the necessary information and feedback. This feedback can
    then be used to improve or correct the data that’s used for modeling or improve
    the model’s training and testing. Monitoring machine learning models helps us
    ensure that the models in production provide predictions according to expectations.
    Three of the issues that could cause unreliable predictions by a machine learning
    model are data variance, data drift, and concept drift. Data drift and concept
    drift are considered two different types of model drift. Model drift is about
    different kinds of changes in the data, either features or output variables, that
    make predictions of a model irrelevant or ineffective on the new user data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到收集必要的信息和反馈时，模型辅导是机器学习生命周期中的一个关键部分。这些反馈可以用来改进或纠正用于建模的数据，或者改进模型的训练和测试。监控机器学习模型有助于我们确保生产中的模型能够根据预期提供预测。可能导致机器学习模型预测不可靠的三个问题是数据方差、数据漂移和概念漂移。数据漂移和概念漂移被认为是两种不同类型的模型漂移。模型漂移涉及数据中不同类型的改变，无论是特征还是输出变量，这些改变使得模型对新用户数据的预测变得无关或无效。
- en: We will talk more about model deployment and monitoring and the engineering
    aspects of the machine learning life cycles in future chapters of this book, such
    as [*Chapter 10*](B16369_10.xhtml#_idTextAnchor286), *Versioning and Reproducible
    Machine* *Learning Modeling*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的后续章节中更详细地讨论模型部署和监控，以及机器学习生命周期中的工程方面，例如[*第10章*](B16369_10.xhtml#_idTextAnchor286)，*版本控制和可重复的机器学习建模*。
- en: Summary
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about different components of a machine learning
    life cycle, from data collection and selection to model training and evaluation
    and, finally, model deployment and monitoring. We also showed how modularizing
    the data processing, modeling, and deployment aspects of the machine learning
    life cycle helps in identifying opportunities for improving machine learning models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了机器学习生命周期的不同组成部分，从数据收集和选择到模型训练和评估，最后到模型部署和监控。我们还展示了如何模块化机器学习生命周期的数据处理、建模和部署方面，有助于识别改进机器学习模型的机会。
- en: In the next chapter, you will learn about concepts beyond improving the performance
    of machine learning models, such as impartial modeling and fairness, accountability,
    and transparency toward achieving responsible AI systems.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解关于改进机器学习模型性能之外的概念，例如无偏建模和公平性，以及为了实现负责任的AI系统而进行的问责制和透明度。
- en: Questions
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can you provide two examples of data cleaning processes?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能提供两个数据清洗过程的例子吗？
- en: Can you explain the difference between the one-hot and label encoding methods?
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释一下one-hot编码和标签编码方法之间的区别吗？
- en: How can you use quantiles of a distribution to detect its outliers?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何使用分布的分位数来检测其异常值？
- en: What comes to your mind regarding the differences between the considerations
    of deploying a model locally for doctors versus deploying models behind chatbots
    in a banking system?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当考虑到在医生本地部署模型与在银行系统中部署聊天机器人背后的模型之间的差异时，你脑海中浮现的是什么？
- en: References
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Micci-Barreca, Daniele. *A preprocessing scheme for high-cardinality categorical
    attributes in classification and prediction problems*. ACM SIGKDD Explorations
    Newsletter 3.1 (2001): 27-32.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Micci-Barreca, Daniele. 《用于分类和预测问题中高基数分类属性的前处理方案》。ACM SIGKDD Explorations Newsletter
    3.1 (2001): 27-32。'
- en: Basu, Anirban, *Software Quality Assurance, Testing and Metrics*, PRENTICE HALL,
    January 1, 2015.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Basu, Anirban. 《软件质量保证、测试和度量》，PRENTICE HALL，2015年1月1日。
