["```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import KMeans \n```", "```py\ninput_file = ('data_multivar.txt')\n# Load data\nx = []\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        data = [float(i) for i in line.split(',')]\n        x.append(data)\n\ndata = np.array(x)\nnum_clusters = 4\n```", "```py\nplt.figure() \nplt.scatter(data[:,0], data[:,1], marker='o',  \n        facecolors='none', edgecolors='k', s=30) \nx_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 \ny_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 \nplt.title('Input data') \nplt.xlim(x_min, x_max) \nplt.ylim(y_min, y_max) \nplt.xticks(()) \nplt.yticks(()) \n```", "```py\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10) \nkmeans.fit(data)\n```", "```py\n# Step size of the mesh \nstep_size = 0.01 \n\n# Plot the boundaries \nx_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 \ny_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 \nx_values, y_values = np.meshgrid(np.arange(x_min, x_max, step_size), np.arange(y_min, y_max, step_size)) \n\n# Predict labels for all points in the mesh \npredicted_labels = kmeans.predict(np.c_[x_values.ravel(), y_values.ravel()]) \n```", "```py\n# Plot the results \npredicted_labels = predicted_labels.reshape(x_values.shape) \nplt.figure() \nplt.clf() \nplt.imshow(predicted_labels, interpolation='nearest', \n           extent=(x_values.min(), x_values.max(), y_values.min(), y_values.max()), \n           cmap=plt.cm.Paired, \n           aspect='auto', origin='lower') \n\nplt.scatter(data[:,0], data[:,1], marker='o',  \n        facecolors='none', edgecolors='k', s=30) \n```", "```py\ncentroids = kmeans.cluster_centers_ \nplt.scatter(centroids[:,0], centroids[:,1], marker='o', s=200, linewidths=3, \n        color='k', zorder=10, facecolors='black') \nx_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 \ny_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 \nplt.title('Centoids and boundaries obtained using KMeans') \nplt.xlim(x_min, x_max) \nplt.ylim(y_min, y_max) \nplt.xticks(()) \nplt.yticks(()) \nplt.show() \n```", "```py\nimport argparse \n\nimport numpy as np \nfrom scipy import misc  \nfrom sklearn import cluster \nimport matplotlib.pyplot as plt\n```", "```py\ndef build_arg_parser(): \n    parser = argparse.ArgumentParser(description='Compress the input image \\ \n            using clustering') \n    parser.add_argument(\"--input-file\", dest=\"input_file\", required=True, \n            help=\"Input image\") \n    parser.add_argument(\"--num-bits\", dest=\"num_bits\", required=False, \n            type=int, help=\"Number of bits used to represent each pixel\") \n    return parser \n```", "```py\ndef compress_image(img, num_clusters): \n    # Convert input image into (num_samples, num_features)  \n    # array to run kmeans clustering algorithm  \n   X = img.reshape((-1, 1))   \n\n    # Run kmeans on input data \n    kmeans = cluster.KMeans(n_clusters=num_clusters, n_init=4, random_state=5) \n    kmeans.fit(X) \n    centroids = kmeans.cluster_centers_.squeeze() \n    labels = kmeans.labels_ \n\n    # Assign each value to the nearest centroid and  \n    # reshape it to the original image shape \n    input_image_compressed = np.choose(labels, centroids).reshape(img.shape) \n\n    return input_image_compressed \n```", "```py\ndef plot_image(img, title): \n    vmin = img.min() \n    vmax = img.max() \n    plt.figure() \n    plt.title(title) \n    plt.imshow(img, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n```", "```py\nif __name__=='__main__': \n    args = build_arg_parser().parse_args() \n    input_file = args.input_file \n    num_bits = args.num_bits \n\n    if not 1 <= num_bits <= 8: \n        raise TypeError('Number of bits should be between 1 and 8') \n\n    num_clusters = np.power(2, num_bits) \n\n    # Print compression rate \n    compression_rate = round(100 * (8.0 - args.num_bits) / 8.0, 2) \n    print(\"The size of the image will be reduced by a factor of\", 8.0/args.num_bits) \n    print(\"Compression rate = \" + str(compression_rate) + \"%\") \n```", "```py\n    # Load input image \n    input_image = misc.imread(input_file, True).astype(np.uint8) \n\n    # original image  \n    plot_image(input_image, 'Original image') \n```", "```py\n    # compressed image  \n    input_image_compressed = compress_image(input_image, num_clusters) \n    plot_image(input_image_compressed, 'Compressed image; compression rate = '  \n            + str(compression_rate) + '%') \n\n    plt.show() \n```", "```py\n    $ python vector_quantization.py --input-file flower_image.jpg --num-bits 4\n```", "```py\nThe size of the image will be reduced by a factor of 2.0\nCompression rate = 50.0%\n```", "```py\n    $ python vector_quantization.py --input-file flower_image.jpg --num-bits 2  \n```", "```py\nThe size of the image will be reduced by a factor of 4.0\nCompression rate = 75.0%\n```", "```py\n    $ python vector_quantization.py --input-file flower_image.jpg --num-bits 1  \n```", "```py\nThe size of the image will be reduced by a factor of 8.0\nCompression rate = 87.5%\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.neighbors import kneighbors_graph \n```", "```py\ndef perform_clustering(X, connectivity, title, num_clusters=3, linkage='ward'): \n    plt.figure() \n    model = AgglomerativeClustering(linkage=linkage,  \n                    connectivity=connectivity, n_clusters=num_clusters) \n    model.fit(X) \n```", "```py\n    # extract labels \n    labels = model.labels_ \n` \n    # specify marker shapes for different clusters \n    markers = '.vx' \n```", "```py\n    for i, marker in zip(range(num_clusters), markers): \n        # plot the points belong to the current cluster \n        plt.scatter(X[labels==i, 0], X[labels==i, 1], s=50,  \n                    marker=marker, color='k', facecolors='none') \n\n    plt.title(title)\n```", "```py\ndef get_spiral(t, noise_amplitude=0.5): \n    r = t \n    x = r * np.cos(t) \n    y = r * np.sin(t) \n\n    return add_noise(x, y, noise_amplitude) \n```", "```py\ndef add_noise(x, y, amplitude): \n    X = np.concatenate((x, y)) \n    X += amplitude * np.random.randn(2, X.shape[1]) \n    return X.T \n```", "```py\ndef get_rose(t, noise_amplitude=0.02): \n    # Equation for \"rose\" (or rhodonea curve); if k is odd, then \n    # the curve will have k petals, else it will have 2k petals \n    k = 5        \n    r = np.cos(k*t) + 0.25  \n    x = r * np.cos(t) \n    y = r * np.sin(t) \n\n    return add_noise(x, y, noise_amplitude) \n```", "```py\ndef get_hypotrochoid(t, noise_amplitude=0): \n    a, b, h = 10.0, 2.0, 4.0 \n    x = (a - b) * np.cos(t) + h * np.cos((a - b) / b * t)  \n    y = (a - b) * np.sin(t) - h * np.sin((a - b) / b * t)  \n\n    return add_noise(x, y, 0)\n```", "```py\nif __name__=='__main__': \n    # Generate sample data \n    n_samples = 500  \n    np.random.seed(2) \n    t = 2.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples)) \n    X = get_spiral(t) \n\n    # No connectivity \n    connectivity = None  \n    perform_clustering(X, connectivity, 'No connectivity') \n\n    # Create K-Neighbors graph  \n    connectivity = kneighbors_graph(X, 10, include_self=False) \n    perform_clustering(X, connectivity, 'K-Neighbors connectivity') \n\n    plt.show() \n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom sklearn import metrics \nfrom sklearn.cluster import KMeans \n```", "```py\ninput_file = ('data_perf.txt')\n\nx = []\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        data = [float(i) for i in line.split(',')]\n        x.append(data)\n\ndata = np.array(x)\n```", "```py\nscores = [] \nrange_values = np.arange(2, 10) \n\nfor i in range_values: \n    # Train the model \n    kmeans = KMeans(init='k-means++', n_clusters=i, n_init=10) \n    kmeans.fit(data) \n    score = metrics.silhouette_score(data, kmeans.labels_,  \n                metric='euclidean', sample_size=len(data)) \n\n    print(\"Number of clusters =\", i)\n    print(\"Silhouette score =\", score)\n\n    scores.append(score) \n```", "```py\n# Plot scores \nplt.figure() \nplt.bar(range_values, scores, width=0.6, color='k', align='center') \nplt.title('Silhouette score vs number of clusters') \n\n# Plot data \nplt.figure() \nplt.scatter(data[:,0], data[:,1], color='k', s=30, marker='o', facecolors='none') \nx_min, x_max = min(data[:, 0]) - 1, max(data[:, 0]) + 1 \ny_min, y_max = min(data[:, 1]) - 1, max(data[:, 1]) + 1 \nplt.title('Input data') \nplt.xlim(x_min, x_max) \nplt.ylim(y_min, y_max) \nplt.xticks(()) \nplt.yticks(()) \n\nplt.show()\n```", "```py\nNumber of clusters = 2\nSilhouette score = 0.5290397175472954\nNumber of clusters = 3\nSilhouette score = 0.5572466391184153\nNumber of clusters = 4\nSilhouette score = 0.5832757517829593\nNumber of clusters = 5\nSilhouette score = 0.6582796909760834\nNumber of clusters = 6\nSilhouette score = 0.5991736976396735\nNumber of clusters = 7\nSilhouette score = 0.5194660249299737\nNumber of clusters = 8\nSilhouette score = 0.44937089046511863\nNumber of clusters = 9\nSilhouette score = 0.3998899991555578\n```", "```py\nfrom itertools import cycle \nimport numpy as np \nfrom sklearn.cluster import DBSCAN \nfrom sklearn import metrics \nimport matplotlib.pyplot as plt \n```", "```py\n# Load data\ninput_file = ('data_perf.txt')\n\nx = []\nwith open(input_file, 'r') as f:\n    for line in f.readlines():\n        data = [float(i) for i in line.split(',')]\n        x.append(data)\n\nX = np.array(x)\n```", "```py\n# Find the best epsilon \neps_grid = np.linspace(0.3, 1.2, num=10) \nsilhouette_scores = [] \neps_best = eps_grid[0] \nsilhouette_score_max = -1 \nmodel_best = None \nlabels_best = None\n```", "```py\nfor eps in eps_grid: \n    # Train DBSCAN clustering model \n    model = DBSCAN(eps=eps, min_samples=5).fit(X) \n\n    # Extract labels \n    labels = model.labels_\n```", "```py\n    # Extract performance metric  \n    silhouette_score = round(metrics.silhouette_score(X, labels), 4) \n    silhouette_scores.append(silhouette_score) \n\n    print(\"Epsilon:\", eps, \" --> silhouette score:\", silhouette_score) \n```", "```py\n    if silhouette_score > silhouette_score_max: \n        silhouette_score_max = silhouette_score \n        eps_best = eps \n        model_best = model \n        labels_best = labels \n```", "```py\n# Plot silhouette scores vs epsilon \nplt.figure() \nplt.bar(eps_grid, silhouette_scores, width=0.05, color='k', align='center') \nplt.title('Silhouette score vs epsilon') \n\n# Best params \nprint(\"Best epsilon =\", eps_best) \n```", "```py\n# Associated model and labels for best epsilon \nmodel = model_best  \nlabels = labels_best\n```", "```py\n# Check for unassigned datapoints in the labels \noffset = 0 \nif -1 in labels: \n    offset = 1 \n```", "```py\n# Number of clusters in the data  \nnum_clusters = len(set(labels)) - offset  \n\nprint(\"Estimated number of clusters =\", num_clusters)\n```", "```py\n# Extracts the core samples from the trained model \nmask_core = np.zeros(labels.shape, dtype=np.bool) \nmask_core[model.core_sample_indices_] = True \n```", "```py\n# Plot resultant clusters  \nplt.figure() \nlabels_uniq = set(labels) \nmarkers = cycle('vo^s<>') \n```", "```py\nfor cur_label, marker in zip(labels_uniq, markers): \n    # Use black dots for unassigned datapoints \n    if cur_label == -1: \n        marker = '.' \n\n    # Create mask for the current label \n    cur_mask = (labels == cur_label) \n\n    cur_data = X[cur_mask & mask_core] \n    plt.scatter(cur_data[:, 0], cur_data[:, 1], marker=marker, \n             edgecolors='black', s=96, facecolors='none') \n    cur_data = X[cur_mask & ~mask_core] \n    plt.scatter(cur_data[:, 0], cur_data[:, 1], marker=marker, \n             edgecolors='black', s=32) \nplt.title('Data separated into clusters') \nplt.show()\n```", "```py\nEpsilon: 0.3 --> silhouette score: 0.1287\nEpsilon: 0.39999999999999997 --> silhouette score: 0.3594\nEpsilon: 0.5 --> silhouette score: 0.5134\nEpsilon: 0.6 --> silhouette score: 0.6165\nEpsilon: 0.7 --> silhouette score: 0.6322\nEpsilon: 0.7999999999999999 --> silhouette score: 0.6366\nEpsilon: 0.8999999999999999 --> silhouette score: 0.5142\nEpsilon: 1.0 --> silhouette score: 0.5629\nEpsilon: 1.0999999999999999 --> silhouette score: 0.5629\nEpsilon: 1.2 --> silhouette score: 0.5629\nBest epsilon = 0.7999999999999999\nEstimated number of clusters = 5\n```", "```py\nimport json\nimport sys\nimport pandas as pd\n\nimport numpy as np\nfrom sklearn import covariance, cluster\n```", "```py\n# Input symbol file \nsymbol_file = 'symbol_map.json' \n```", "```py\n# Load the symbol map \nwith open(symbol_file, 'r') as f: \n    symbol_dict = json.loads(f.read()) \n\nsymbols, names = np.array(list(symbol_dict.items())).T \n```", "```py\nquotes = []\n\nexcel_file = 'stock_market_data.xlsx'\n\nfor symbol in symbols:\n    print('Quote history for %r' % symbol, file=sys.stderr)\n    quotes.append(pd.read_excel(excel_file, symbol))\n```", "```py\n# Extract opening and closing quotes \nopening_quotes = np.array([quote.open for quote in quotes]).astype(np.float) \nclosing_quotes = np.array([quote.close for quote in quotes]).astype(np.float) \n\n# The daily fluctuations of the quotes  \ndelta_quotes = closing_quotes - opening_quotes \n```", "```py\n# Build a graph model from the correlations \nedge_model = covariance.GraphicalLassoCV(cv=3) \n```", "```py\n# Standardize the data  \nX = delta_quotes.copy().T \nX /= X.std(axis=0) \n```", "```py\n# Train the model \nwith np.errstate(invalid='ignore'): \n    edge_model.fit(X) \n```", "```py\n# Build clustering model using affinity propagation \n_, labels = cluster.affinity_propagation(edge_model.covariance_) \nnum_labels = labels.max() \n\n# Print the results of clustering \nfor i in range(num_labels + 1): \n    print \"Cluster\", i+1, \"-->\", ', '.join(names[labels == i]) \n```", "```py\nCluster 1 --> Apple, Amazon, Yahoo\nCluster 2 --> AIG, American express, Bank of America, DuPont de Nemours, General Dynamics, General Electrics, Goldman Sachs, GlaxoSmithKline, Home Depot, Kellogg\nCluster 3 --> Boeing, Canon, Caterpillar, Ford, Honda\nCluster 4 --> Colgate-Palmolive, Kimberly-Clark\nCluster 5 --> Cisco, Dell, HP, IBM\nCluster 6 --> Comcast, Cablevision\nCluster 7 --> CVS\nCluster 8 --> ConocoPhillips, Chevron\n```", "```py\nimport csv \nimport numpy as np \nfrom sklearn.cluster import MeanShift, estimate_bandwidth \nimport matplotlib.pyplot as plt \n```", "```py\n# Load data from input file \ninput_file = 'wholesale.csv' \nfile_reader = csv.reader(open(input_file, 'rt'), delimiter=',') \nX = [] \nfor count, row in enumerate(file_reader): \n    if not count: \n        names = row[2:] \n        continue \n\n    X.append([float(x) for x in row[2:]]) \n\n# Input data as numpy array \nX = np.array(X) \n```", "```py\n# Estimating the bandwidth  \nbandwidth = estimate_bandwidth(X, quantile=0.8, n_samples=len(X)) \n\n# Compute clustering with MeanShift \nmeanshift_estimator = MeanShift(bandwidth=bandwidth, bin_seeding=True) \nmeanshift_estimator.fit(X) \nlabels = meanshift_estimator.labels_ \ncentroids = meanshift_estimator.cluster_centers_ \nnum_clusters = len(np.unique(labels)) \n\nprint(\"Number of clusters in input data =\", num_clusters) \n```", "```py\nprint(\"Centroids of clusters:\")\nprint('\\t'.join([name[:3] for name in names]))\nfor centroid in centroids:\n    print('\\t'.join([str(int(x)) for x in centroid]))\n```", "```py\n# Visualizing data \n\ncentroids_milk_groceries = centroids[:, 1:3] \n\n# Plot the nodes using the coordinates of our centroids_milk_groceries \nplt.figure() \nplt.scatter(centroids_milk_groceries[:,0], centroids_milk_groceries[:,1],  \n        s=100, edgecolors='k', facecolors='none') \n\noffset = 0.2 \nplt.xlim(centroids_milk_groceries[:,0].min() - offset * centroids_milk_groceries[:,0].ptp(), \n        centroids_milk_groceries[:,0].max() + offset * centroids_milk_groceries[:,0].ptp(),) \nplt.ylim(centroids_milk_groceries[:,1].min() - offset * centroids_milk_groceries[:,1].ptp(), \n        centroids_milk_groceries[:,1].max() + offset * centroids_milk_groceries[:,1].ptp()) \n\nplt.title('Centroids of clusters for milk and groceries') \nplt.show() \n```", "```py\nfrom keras.datasets import mnist\n```", "```py\n(XTrain, YTrain), (XTest, YTest) = mnist.load_data()\n\nprint('XTrain shape = ',XTrain.shape)\nprint('XTest shape = ',XTest.shape)\nprint('YTrain shape = ',YTrain.shape)\nprint('YTest shape = ',YTest.shape)\n```", "```py\nXTrain shape = (60000, 28, 28)\nXTest shape = (10000, 28, 28)\nYTrain shape = (60000,)\nYTest shape = (10000,)\n```", "```py\nimport numpy as np\nprint('YTrain values = ',np.unique(YTrain))\nprint('YTest values = ',np.unique(YTest))\n```", "```py\nYTrain values = [0 1 2 3 4 5 6 7 8 9]\nYTest values = [0 1 2 3 4 5 6 7 8 9]\n```", "```py\nunique, counts = np.unique(YTrain, return_counts=True)\nprint('YTrain distribution = ',dict(zip(unique, counts)))\nunique, counts = np.unique(YTest, return_counts=True)\nprint('YTrain distribution = ',dict(zip(unique, counts)))\n```", "```py\nYTrain distribution = {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\nYTrain distribution = {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n```", "```py\nimport matplotlib.pyplot as plt\nplt.figure(1)\nplt.subplot(121)\nplt.hist(YTrain, alpha=0.8, ec='black')\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Number of occurrences\")\nplt.title(\"YTrain data\")\n\nplt.subplot(122)\nplt.hist(YTest, alpha=0.8, ec='black')\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Number of occurrences\")\nplt.title(\"YTest data\")\nplt.show()\n```", "```py\nXTrain = XTrain.astype('float32') / 255\nXTest = XTest.astype('float32') / 255\n```", "```py\nXTrain = XTrain.reshape((len(XTrain), np.prod(XTrain.shape[1:])))\nXTest = XTest.reshape((len(XTest), np.prod(XTest.shape[1:])))\n```", "```py\nfrom keras.layers import Input \nfrom keras.layers import Dense\nfrom keras.models import Model\n```", "```py\nInputModel = Input(shape=(784,))\nEncodedLayer = Dense(32, activation='relu')(InputModel)\nDecodedLayer = Dense(784, activation='sigmoid')(EncodedLayer)\nAutoencoderModel = Model(InputModel, DecodedLayer)\nAutoencoderModel.summary()\n```", "```py\nAutoencoderModel.compile(optimizer='adadelta', loss='binary_crossentropy')\n```", "```py\nhistory = AutoencoderModel.fit(XTrain, XTrain,\nbatch_size=256,\nepochs=100,\nshuffle=True,\nvalidation_data=(XTest, XTest))\n```", "```py\nDecodedDigits = AutoencoderModel.predict(XTest)\n```", "```py\nn=5\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n ax = plt.subplot(2, n, i + 1)\n plt.imshow(XTest[i+10].reshape(28, 28))\n plt.gray()\n ax.get_xaxis().set_visible(False)\n ax.get_yaxis().set_visible(False)\n ax = plt.subplot(2, n, i + 1 + n)\n plt.imshow(DecodedDigits[i+10].reshape(28, 28))\n plt.gray()\n ax.get_xaxis().set_visible(False)\n ax.get_yaxis().set_visible(False)\nplt.show()\n```"]