- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s All About Data – Options to Store and Transform ML Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The real work on a machine learning project only starts once the required data
    is available in the project development environment. Sometimes, when the data
    changes very frequently or the use case requires real-time data, we may need to
    set up some data pipelines to ensure that the required data is always available
    for analysis and modeling purposes. The best way to transfer, store, or transform
    data also depends on the size, type, and nature of the underlying data. Raw data,
    as collected in the real world, is often massive in size and may belong to multiple
    types, such as text, audio, images, videos, and so on. Due to the varying nature,
    size, and type of real-world data, it becomes really important to set up the correct
    infrastructure for storing, transferring, transforming, and analyzing the data
    at scale.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about the different options for moving data to
    the Google Cloud environment, different data storage systems, and efficient ways
    to apply transformations to large-scale data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at the following topics about data:'
  prefs: []
  type: TYPE_NORMAL
- en: Moving data to Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where to store data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving data to Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we start a machine learning project on **Google Cloud Platform** (**GCP**),
    the very first step is to move all our project-related data to the Google Cloud
    environment. While transferring data to the cloud, the key things to focus on
    are reliability, security, scalability, and the ease of managing the transfer
    process. With these points in mind, Google Cloud provides four major data transfer
    utilities to meet customer requirements across a variety of use cases. In general,
    these utilities are useful for any kind of data transfer purposes, including data
    center migration, data backup, content storage, and machine learning. As our current
    focus is on making data available for machine learning use cases, we can utilize
    any of the following transfer solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Storage Transfer tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BigQuery Data Transfer Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage Transfer Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer Appliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s understand each of these transfer solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Storage Transfer tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This option is suitable when our dataset is not too big (up to a few TB is fine),
    and we wish to store it in **Google Cloud Storage** (**GCS**) buckets (GCS is
    an object-type storage system, very similar to the local filesystem in our computers;
    we will learn more about it in the next section). Google Cloud provides tools
    for uploading data into these GCS buckets directly from our computers. We can
    upload one or more files or even a folder containing files using one of the following
    methods and also track the progress of uploads using the **upload progress** window
    from the Google Cloud console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the three methods for uploading files or folders to a GCS bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud console UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the command line
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the REST API (JSON API)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at these methods in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud Console UI
  prefs: []
  type: TYPE_NORMAL
- en: 'It is quite easy to upload files or a folder containing files to GCS using
    the Cloud Console UI. When we upload a folder, the hierarchical structure inside
    the folder is also preserved. Follow these simple steps to upload data into a
    GCS bucket using the UI:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a browser and go to the **Google Cloud** **Console** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the left pane, click on **Cloud Storage** and open the **Buckets** page.
    It will list all the existing buckets in our project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the name of the relevant bucket if it already exists; otherwise, create
    a new bucket to store uploaded files or folders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we are inside the bucket, we will see bucket details and existing content.
    Now, we can directly upload the data using the **Upload Files** or **Upload Folder**
    button. The UI also provides options for creating a new folder and downloading
    or deleting files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Folder uploads are only supported using the Chrome browser. It may not work
    with other browsers.
  prefs: []
  type: TYPE_NORMAL
- en: Using the command line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud also provides an open source command-line utility called **GSUTIL**.
    We can utilize GSUTIL for scripted data transfers and also to manage our GCS buckets
    using simple commands. For large-scale streaming data, it supports multi-threaded/multi-processing
    data transfer for pushing script output. It can operate in *rsync* mode and transfer
    incremental copies of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'GSUTIL commands are quite similar to Unix commands. See the following example
    for copying a file into the GCS bucket we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can list the content of a bucket using the `ls` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: REST API (JSON API)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The JSON API interface lets us access or manipulate GCS data programmatically.
    This method is more suitable for software developers who are familiar with web
    programming and creating applications that consume web services using HTTP requests.
    For example, we can use the following HTTP request to list the objects of a particular
    bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To use the preceding methods to access, manipulate, or upload data, we must
    have the proper **Identity and Access Management** (**IAM**) permissions. The
    project owner can provide a list of relevant permissions to the project development
    team.
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery Data Transfer Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BigQuery Data Transfer Service currently supports loading data from Google **Software-as-a-Service**
    (**SaaS**) apps, external cloud providers, data warehouses such as **Teradata**
    or **Redshift**, and a few third-party sources. Once data is available, we can
    directly perform analytics or machine learning right inside **BigQuery** (**BQ**).
    It can also be used as a data warehousing solution; we will learn more about BQ
    in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Storage Transfer Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared to GSUTIL, Storage Transfer Service is a managed service that is suitable
    for transferring data quickly and securely between object and file storage systems
    across different clouds (AWS and Azure), on-premises, or within different buckets
    in Google Cloud. The data transfer process is really fast as it utilizes a high
    bandwidth network. It also handles retries and provides detailed transfer logging.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Appliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This option is suitable when we want to migrate a really large dataset and don’t
    have much bandwidth. Transfer Appliance is a physical device with a high memory
    capacity that can be utilized for transferring and securely shipping data to a
    Google upload facility, where data is uploaded to cloud storage. We can order
    the appliance from the Cloud Console, and once we receive the device, we can start
    copying our data. Finally, we can ship the appliance back to Google to transfer
    data into a specified GCS bucket.
  prefs: []
  type: TYPE_NORMAL
- en: For most machine learning-related use cases, the first two methods should be
    enough to transfer data fast, securely, and consistently. Next, let’s learn more
    about the GCS and BQ data storage systems on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Where to store data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCS and BQ are two recommended options for storing any machine learning use
    case-related datasets for high security and efficiency purposes. If the underlying
    data is structured or semi-structured, BQ is the recommended option due to its
    off-the-shelf features for manipulating or processing structured datasets. If
    the data contains images, videos, audio, and unstructured data, then GCS is the
    suitable option to store it. Let’s learn about these two data storage systems
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: GCS – object storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A significant amount of data that we collect from real-world applications is
    in unstructured form. Some examples are images, videos, emails, audio files, web
    pages, and sensor data. Managing and storing such huge amounts of unstructured
    data affordably and efficiently is quite challenging. Nowadays, object storage
    has become a preferable solution for storing such large amounts of static data
    and backups. Object storage is a computer data architecture that’s designed to
    handle large amounts of structured data efficiently. Each data object in an object-based
    storage system is considered a distinct unit bundled with metadata and a unique
    identifier that is useful in quickly retrieving and locating data.
  prefs: []
  type: TYPE_NORMAL
- en: GCS is an object-based storage system in Google Cloud. As it is cloud-based,
    GCS data can be accessed globally and provides massive scale. It is a suitable
    option for small to large enterprises to store their large amounts of data in
    a cost-effective and easily retrievable fashion. Object storage is more efficient
    for applications where you write the data once but have to read it very frequently.
    While it is extremely good for static data, it’s not a good solution for dynamic
    data. If data is constantly changing, we will have to write the entire data object
    again and again to modify it, which is inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'GCS is frequently used by machine learning practitioners on Google Cloud due
    to its variety of benefits. Here are some common benefits of storing data in an
    object storage system such as GCS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Massively scalable**: Object storage can be expanded infinitely by simply
    adding more servers or devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Less complex**: Unlike a file storage system, there is no hierarchy or folder
    structure in object storage, so retrieval is quite simple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Searchability**: It is easy to search for a specific object as metadata is
    also part of the object. We can use tags to make objects more filterable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resiliency**: There’s no fear of data loss as it can automatically replicate
    data and store it across multiple devices or geographical locations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low cost**: Object storage is cost-effective and thus ideal for storing large
    amounts of data. Secondly, we only pay for the capacity we use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With so many advantages, there are also some limitations of object-based storage
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Due to latency concerns, they cannot be used in place of traditional databases
    when designing web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They’re not suitable for situations when data is changing rapidly and lots of
    file writes are required very frequently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They’re not very compatible with operating system mounting and require additional
    clients or adapters to work with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a fair idea about the advantages and limitations of object
    storage systems, we will be able to utilize them based on relevant requirements
    in future projects. Now, let’s learn more about BQ.
  prefs: []
  type: TYPE_NORMAL
- en: BQ – data warehouse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BQ is a fully managed and serverless data warehouse available on Google Cloud.
    It is a petabyte-scale platform that enables scalable analysis on large datasets.
    BQ’s serverless architecture supports SQL queries for slicing and dicing large
    datasets. Its analysis engine is very scalable and supports distributed analysis
    such that we can query terabytes of data in seconds and petabytes in minutes.
    BQ also supports machine learning, meaning we can train and test common ML models
    within BQ using just a few SQL-like commands. We will learn about **BigQuery Machine
    Learning** (**BQML**) in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes, BQ stores data in a columnar storage format that is optimized
    for analytical queries. Data inside BQ is presented in a database via tables with
    rows and columns. It provides full support for **Atomicity, Consistency, Isolation,
    and Durability** (**ACID**) properties, similar to a transactional database management
    system. BQ provides high availability for data by automatically replicating it
    across multiple locations and regions in Google Cloud. In addition to the data
    that is present inside BQ storage, it also provides flexibility to query data
    from external sources, including GCS, Bigtable, Google Sheets, and Spanner.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a machine learning use case involves structured or semi-structured data,
    BQ can be the best place to store and analyze it. In subsequent chapters, we will
    learn more about how BQ is an extremely useful tool for data analysts and machine
    learning practitioners. Here are some common benefits of using BQ as a data warehousing
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Speed and scale**: With BQ, querying through massive datasets only takes
    seconds. BQ was designed to analyze and store very large amounts of datasets with
    ease. It can scale seamlessly from petabytes to exabytes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time analytics**: BQ supports streaming data ingestion and makes it
    immediately available for querying. Its integration with the BI tool Looker Studio
    allows it to provide real-time and interactive analytical capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning**: BQ provides capabilities to build and operationalize
    machine learning models on both structured and unstructured data using simple
    SQL – in a very short time. BQ models can be directly exported so that they can
    be served on the Vertex AI prediction service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: BQ provides strong security and fine-grained governance controls.
    Data is encrypted at rest and in transit by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-cloud support**: BQ allows for data analysis across multiple cloud
    platforms. BQ can run analysis on data where it is located without having to move
    it to a different location, which makes it more cost-effective and flexible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now have a fair idea of two popular data storage systems – GCS and BQ – and
    their benefits and limitations. Depending on the use case, we should now be capable
    of choosing the right place to store our machine learning datasets. Next, we’ll
    dig into data transformation options on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Raw data present in real-world applications is often unstructured and noisy.
    Thus, it cannot be fed directly to machine learning algorithms. We often need
    to apply several transformations on raw data and convert it into a format that
    is well supported by machine learning algorithms. In this section, we will learn
    about multiple options for transforming data in a scalable and efficient way on
    Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are three common options for data transformation in the GCP environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Ad hoc transformation within Jupyter Notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Data Fusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataflow pipelines for scalable data transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s learn about these three methods in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Ad hoc transformations within Jupyter Notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning algorithms are mathematical and can only understand numeric
    data. For example, in computer vision problems, images are converted into numerical
    pixel values before they’re fed into a model. Similarly, in the case of audio
    data, it is often converted into a time-frequency domain using different transformations,
    such as **Fast Fourier Transformation** (**FFT**). If data is in a tabular format
    and contains rows and columns, some of the columns might contain non-numeric or
    categorical types of data. These categorical columns are first converted into
    numeric form using a suitable transformation and then fed to a machine learning
    model or neural network. Some of these transformations can be directly applied
    in Jupyter notebooks using Python functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: After reading data into a Jupyter Notebook, we can apply any desired transformations
    to make the dataset ready for modeling. Once the dataset is ready, we can save
    it somewhere (for example, BQ or GCS) with a version number so that it can be
    read directly into multiple different model training experiments. Machine learning
    practitioners often create and save multiple versions of training datasets by
    applying different transformations or feature engineering. It makes it easier
    to compare the performance of experiments over different versions of data. Let’s
    learn about some common transformations that are very frequently applied in machine
    learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, data can be classified into the following two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numeric data**: As the name suggests, this data is numeric or quantifiable
    – for example, age, height, and weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical data**: This data is in string format or is qualitative data
    – for example, gender, language, and accent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s learn about some common transformations that can be applied to these
    two types of data columns.
  prefs: []
  type: TYPE_NORMAL
- en: Handling numeric data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numerical data can either be discrete or continuous. Discrete data is countable,
    for example, the number of players in a basketball team or the number of cities
    in a country. Discrete data can only take certain values, such as 10, 22, 35,
    41, and so on. On the other hand, any data that is measurable is called continuous
    data – for example, the height of a person or the distance between two racing
    cars. Continuous data can virtually take any value, such as 2.3, 11, 0.0001, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two common kinds of transformations are applied to numeric data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalizing**: This is done to bring different numeric columns to the same
    scale. It is recommended to normalize numerical data columns to the same scale
    as it helps in the convergence of gradient descent-based ML algorithms. If a data
    column has very large values, then normalizing it can prevent **NaN** errors while
    training models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bucketing**: In this type of transformation, numeric data is converted into
    categorical data. Bucketing is usually done on continuous numeric data when there
    is no linear relationship between the numeric column and the target column. For
    example, a car manufacturing company observes that the cars that have the lowest
    price or the highest price are less frequently sold, but the cars with mid-range
    prices are more frequently sold. In this case, if we want to predict the number
    of cars that are sold using car price as a feature, it will be beneficial to bucketize
    the car price into price ranges. In this way, the model will be able to identify
    the mid-range bucket in which most of the cars are sold and assign more weight
    to this feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling categorical data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Categorical data can be divided into two categories – **ordered** and **nominal**.
    Ordered categorical data has some order associated with it – for example, movie
    ratings (worst, bad, good, excellent) and feedback (negative, neutral, positive).
    Ordered data can always be marked on a scale. Nominal data, on the other hand,
    has no order. Some examples of nominal data include gender, country, and language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some algorithms, such as decision trees, can work well with categorical data,
    but most machine learning algorithms cannot handle categorical data directly.
    These algorithms require the categorical data to be converted into numerical form.
    While converting the categorical data into numerical form, some challenges are
    faced by machine learning practitioners:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High cardinality**: Cardinality means uniqueness in data. A high cardinality
    data column might have lots of different values – for example, ZIP codes in country-level
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rare or frequent occurrences**: A data column might have some values that
    rarely occur or some values that occur very frequently. In both cases, this column
    would not be significant enough to make an impact on the model due to very high
    or very low variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic values**: A data column that keeps changing some values from time
    to time – for example, in a city column, if new cities are added or removed very
    frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best way to overcome these challenges highly depends on the kind of problem
    or data we are dealing with. Now, let’s learn about some common methods of converting
    categorical data into numerical form:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Label encoding**: In this technique, we replace categorical data with integer
    values from 0 to *N*-1\. Here, each integer represents a value from a categorical
    data column with *N* unique values. For example, if there is a categorical data
    column that represents colors and 10 unique color values are possible, in this
    case, each color will be mapped and replaced with an integer from the range of
    0 to 9\. This method may not be ideal for all cases as the model might consider
    numeric values as weights assigned to the data. Thus, this method is more suitable
    for ordinal categorical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` if that color is present in a given row; otherwise, it will be `0`. One-hot
    encoding is often preferred for encoding categorical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings**: As one-hot encoding creates a new column for each unique value,
    this may create a very sparse representation of data when the number of unique
    values is large in number. For example, if we have a ZIP code column with 20k
    unique ZIP codes, the one-hot encoding method will create 20k new binary columns.
    Such sparse data takes a lot of memory to store and increases the complexity of
    machine learning training. To handle and represent such categorical data columns
    with a large number of unique values, dense embeddings can be used. These embeddings,
    however, are often generated using a neural network, so it’s an off-the-shelf
    encoding technique. These embeddings encode each value from a categorical column
    into a small dense vector of real numbers. One simple method to train and generate
    these embeddings is using the inbuilt Keras embedding layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a good understanding of common data transformations that are
    easy to apply in Jupyter notebooks using Python, let’s learn about some more scalable
    ways of data transformation on GCP, such as Cloud Data Fusion and Dataflow.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Data Fusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud Data Fusion is a fully managed service on GCP for quickly building and
    managing scalable data pipelines. Using the Data Fusion UI, we can build and deploy
    data pipelines without writing a single line of code (using a visual point-and-click
    interface). The Data Fusion UI lets us build scalable data integration solutions
    to clean, prepare, blend, transform, and transfer data in a fully managed way
    (which means we do not need to manage infrastructure). Cloud Data Fusion offers
    hundreds of prebuilt transformations for both batch and real-time data processing
    and quickly building ETL/ELT pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features of Cloud Data Fusion are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Portability**: Cloud Data Fusion is built using an open source project called
    **CDAP**, thus ensuring data pipeline portability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple integration**: Cloud Data Fusion’s easy integration with Google Cloud
    functionalities such as GCS, Dataproc, and BigQuery makes development faster and
    easier, ensuring security'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No-code pipelines**: Even non-technical users can build data pipelines quickly
    using Cloud Data Fusion’s web interface'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid**: Since Cloud Data Fusion is an open source project, it provides
    flexibility to build standardized data pipelines across hybrid and multi-cloud
    environments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Cloud Data Fusion provides enterprise-grade security and access
    management with Google Cloud for data protection and compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the web UI, we can also use command-line tools to create and
    manage Cloud Data Fusion instances and pipelines. Next, let’s learn about another
    data transformation tool on GCP – Dataflow
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow pipelines for scalable data transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dataflow is a managed service for executing data processing or transformation
    pipelines on Google Cloud that are developed using **Apache Beam SDK**. It supports
    unified batch and stream data processing that is fast, cost-effective, and serverless
    (which means we do not need to manage infrastructure). Because Dataflow is serverless,
    it lets us focus on expressing the business logic of our data pipeline (using
    SQL or code) without worrying about operational tasks and infrastructure management.
    Due to its streaming nature, Dataflow is ideal for building real-time pipelines
    for use cases such as anomaly detection, pattern recognition, and forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: By combining Dataflow with other managed Google Cloud services, we can simplify
    many aspects of productionizing data pipelines compared to self-managed solutions.
    For example, it can be combined with Google Cloud offerings such as Pub/Sub and
    BQ to develop a streaming solution that can ingest, process, and analyze fluctuating
    volumes of real-time data and generate invaluable real-time business insights.
    As it is managed, it provisions and scales the required resources automatically
    and thus reduces the time and complexity for data engineers or data analysts working
    on stream analytics solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features of Dataflow are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Smart autoscaling**: Dataflow supports both horizontal and vertical scaling
    of worker nodes. Scaling is performed automatically in such a way that the utilization
    of worker nodes and other pipeline scaling requirements are met in an efficient
    (cost-effective) or best-fit manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time pipelines**: Dataflow’s streaming nature is useful in building
    real-time stream analytics, machine learning forecasting, and anomaly detection
    pipelines. It is also useful for synchronizing or replicating data across multiple
    data sources (such as BQ, PostgreSQL, or Cloud Spanner) with minimal latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataflow SQL**: Dataflow streaming pipelines can be built using simple SQL
    commands right from BQ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible scheduling**: Batch processing jobs that need scheduling (such as
    overnight jobs) can easily be scheduled using Dataflow **Flexible Resource Scheduling**
    (**FlexRS**) in a cost-effective setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing data effectively is really important for saving time, cost, and complexity
    for every organization. A machine learning practitioner should be aware of the
    best options for transferring, storing, and transforming data to build machine
    learning solutions more efficiently. In this chapter, we learned about multiple
    ways of bringing data into the Google Cloud environment. We discussed the best
    options for storing it based on the characteristics of the data. Finally, we discussed
    multiple different tools and methods for transforming/processing data in a scalable
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you should feel confident about choosing the best
    option for moving or transferring data into your Google Cloud environment based
    on the requirements of the use case. Choosing the best place to store data and
    the best strategy to analyze and transform data should be easier as we now know
    the pros and cons of different options. In the next chapter, we will deep dive
    into **Vertex AI Workbench**, which is a managed notebook platform within Vertex
    AI.
  prefs: []
  type: TYPE_NORMAL
