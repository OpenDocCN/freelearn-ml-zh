- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: It’s All About Data – Options to Store and Transform ML Datasets
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一切都是关于数据——存储和转换ML数据集的选项
- en: The real work on a machine learning project only starts once the required data
    is available in the project development environment. Sometimes, when the data
    changes very frequently or the use case requires real-time data, we may need to
    set up some data pipelines to ensure that the required data is always available
    for analysis and modeling purposes. The best way to transfer, store, or transform
    data also depends on the size, type, and nature of the underlying data. Raw data,
    as collected in the real world, is often massive in size and may belong to multiple
    types, such as text, audio, images, videos, and so on. Due to the varying nature,
    size, and type of real-world data, it becomes really important to set up the correct
    infrastructure for storing, transferring, transforming, and analyzing the data
    at scale.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，真正的工作只有在项目开发环境中可用所需数据之后才开始。有时，当数据变化非常频繁或用例需要实时数据时，我们可能需要设置一些数据管道，以确保所需数据始终可用于分析和建模目的。最佳的数据传输、存储或转换方式也取决于底层数据的大小、类型和性质。在现实世界中收集的原始数据通常规模庞大，可能属于多种类型，如文本、音频、图像、视频等。由于现实世界数据的性质、大小和类型的多样性，设置正确的基础设施来大规模存储、传输、转换和分析数据变得非常重要。
- en: In this chapter, we will learn about the different options for moving data to
    the Google Cloud environment, different data storage systems, and efficient ways
    to apply transformations to large-scale data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习将数据移动到Google Cloud环境的不同选项、不同的数据存储系统以及高效地对大规模数据进行转换的方法。
- en: 'In this chapter, we will look at the following topics about data:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下关于数据的话题：
- en: Moving data to Google Cloud
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据移动到Google Cloud
- en: Where to store data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储位置
- en: Transforming data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Moving data to Google Cloud
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据移动到Google Cloud
- en: 'When we start a machine learning project on **Google Cloud Platform** (**GCP**),
    the very first step is to move all our project-related data to the Google Cloud
    environment. While transferring data to the cloud, the key things to focus on
    are reliability, security, scalability, and the ease of managing the transfer
    process. With these points in mind, Google Cloud provides four major data transfer
    utilities to meet customer requirements across a variety of use cases. In general,
    these utilities are useful for any kind of data transfer purposes, including data
    center migration, data backup, content storage, and machine learning. As our current
    focus is on making data available for machine learning use cases, we can utilize
    any of the following transfer solutions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在**Google Cloud Platform**（**GCP**）上启动机器学习项目时，第一步是将所有与项目相关的数据移动到Google Cloud环境中。在向云传输数据时，需要关注的关键问题是可靠性、安全性、可扩展性和管理传输过程的便捷性。考虑到这些因素，Google
    Cloud提供了四个主要的数据传输工具，以满足各种用例的客户需求。一般来说，这些工具对任何类型的数据传输目的都很有用，包括数据中心迁移、数据备份、内容存储和机器学习。鉴于我们目前的重点是使数据可用于机器学习用例，我们可以利用以下任何一种传输解决方案：
- en: Google Cloud Storage Transfer tools
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud Storage传输工具
- en: BigQuery Data Transfer Service
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BigQuery数据传输服务
- en: Storage Transfer Service
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储传输服务
- en: Transfer Appliance
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传输设备
- en: Let’s understand each of these transfer solutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解每种传输解决方案。
- en: Google Cloud Storage Transfer tools
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud Storage传输工具
- en: This option is suitable when our dataset is not too big (up to a few TB is fine),
    and we wish to store it in **Google Cloud Storage** (**GCS**) buckets (GCS is
    an object-type storage system, very similar to the local filesystem in our computers;
    we will learn more about it in the next section). Google Cloud provides tools
    for uploading data into these GCS buckets directly from our computers. We can
    upload one or more files or even a folder containing files using one of the following
    methods and also track the progress of uploads using the **upload progress** window
    from the Google Cloud console.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据集不是太大（几TB以内即可）且我们希望将其存储在**Google Cloud Storage**（**GCS**）桶中时（GCS是一个对象存储系统，与我们的计算机中的本地文件系统非常相似；我们将在下一节中了解更多关于它的信息），此选项是合适的。Google
    Cloud提供了从我们的计算机直接上传数据到这些GCS桶的工具。我们可以使用以下方法之一上传一个或多个文件，甚至是一个包含文件的文件夹，并使用Google
    Cloud控制台中的**上传进度**窗口跟踪上传进度。
- en: 'Here are the three methods for uploading files or folders to a GCS bucket:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是上传文件或文件夹到GCS桶的三种方法：
- en: Using the Google Cloud console UI
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Google Cloud控制台UI
- en: Using the command line
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命令行
- en: Using the REST API (JSON API)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用REST API（JSON API）
- en: Let’s look at these methods in more detail.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些方法。
- en: Using the Google Cloud Console UI
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Google Cloud控制台UI
- en: 'It is quite easy to upload files or a folder containing files to GCS using
    the Cloud Console UI. When we upload a folder, the hierarchical structure inside
    the folder is also preserved. Follow these simple steps to upload data into a
    GCS bucket using the UI:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云控制台用户界面上传文件或包含文件的文件夹到GCS非常简单。当我们上传一个文件夹时，文件夹内的层次结构也会被保留。按照以下简单步骤使用UI上传数据到GCS存储桶：
- en: Open a browser and go to the **Google Cloud** **Console** page.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器并访问**Google Cloud** **控制台**页面。
- en: From the left pane, click on **Cloud Storage** and open the **Buckets** page.
    It will list all the existing buckets in our project.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板中，点击**云存储**并打开**存储桶**页面。它将列出我们项目中所有现有的存储桶。
- en: Click on the name of the relevant bucket if it already exists; otherwise, create
    a new bucket to store uploaded files or folders.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果相关的存储桶已经存在，请点击其名称；否则，创建一个新的存储桶来存储上传的文件或文件夹。
- en: Once we are inside the bucket, we will see bucket details and existing content.
    Now, we can directly upload the data using the **Upload Files** or **Upload Folder**
    button. The UI also provides options for creating a new folder and downloading
    or deleting files.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们进入存储桶，我们将看到存储桶详情和现有内容。现在，我们可以直接使用**上传文件**或**上传文件夹**按钮上传数据。UI还提供了创建新文件夹和下载或删除文件选项。
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Folder uploads are only supported using the Chrome browser. It may not work
    with other browsers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹上传仅支持使用Chrome浏览器。它可能与其他浏览器不兼容。
- en: Using the command line
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用命令行
- en: Google Cloud also provides an open source command-line utility called **GSUTIL**.
    We can utilize GSUTIL for scripted data transfers and also to manage our GCS buckets
    using simple commands. For large-scale streaming data, it supports multi-threaded/multi-processing
    data transfer for pushing script output. It can operate in *rsync* mode and transfer
    incremental copies of data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud还提供了一个名为**GSUTIL**的开源命令行实用工具。我们可以利用GSUTIL进行脚本数据传输，也可以使用简单的命令来管理我们的GCS存储桶。对于大规模流式数据，它支持多线程/多进程数据传输以推送脚本输出。它可以在*rsync*模式下操作并传输数据的增量副本。
- en: 'GSUTIL commands are quite similar to Unix commands. See the following example
    for copying a file into the GCS bucket we created previously:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GSUTIL命令与Unix命令非常相似。以下示例展示了如何将文件复制到我们之前创建的GCS存储桶中：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Similarly, we can list the content of a bucket using the `ls` command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以使用`ls`命令列出存储桶的内容：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: REST API (JSON API)
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: REST API (JSON API)
- en: 'The JSON API interface lets us access or manipulate GCS data programmatically.
    This method is more suitable for software developers who are familiar with web
    programming and creating applications that consume web services using HTTP requests.
    For example, we can use the following HTTP request to list the objects of a particular
    bucket:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: JSON API接口让我们能够以编程方式访问或操作GCS数据。这种方法更适合熟悉网络编程和创建使用HTTP请求消费网络服务的应用程序的软件开发人员。例如，我们可以使用以下HTTP请求列出特定存储桶的对象：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Important note
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To use the preceding methods to access, manipulate, or upload data, we must
    have the proper **Identity and Access Management** (**IAM**) permissions. The
    project owner can provide a list of relevant permissions to the project development
    team.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用上述方法访问、操作或上传数据，我们必须拥有适当的**身份和访问管理**(**IAM**)权限。项目所有者可以向项目开发团队提供相关权限列表。
- en: BigQuery Data Transfer Service
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BigQuery数据传输服务
- en: BigQuery Data Transfer Service currently supports loading data from Google **Software-as-a-Service**
    (**SaaS**) apps, external cloud providers, data warehouses such as **Teradata**
    or **Redshift**, and a few third-party sources. Once data is available, we can
    directly perform analytics or machine learning right inside **BigQuery** (**BQ**).
    It can also be used as a data warehousing solution; we will learn more about BQ
    in the coming sections.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery数据传输服务目前支持从Google **软件即服务** (**SaaS**)应用、外部云提供商、数据仓库如**Teradata**或**Redshift**以及一些第三方源加载数据。一旦数据可用，我们就可以直接在**BigQuery**
    (**BQ**)中进行分析或机器学习。它也可以用作数据仓库解决方案；我们将在接下来的章节中了解更多关于BQ的信息。
- en: Storage Transfer Service
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储传输服务
- en: Compared to GSUTIL, Storage Transfer Service is a managed service that is suitable
    for transferring data quickly and securely between object and file storage systems
    across different clouds (AWS and Azure), on-premises, or within different buckets
    in Google Cloud. The data transfer process is really fast as it utilizes a high
    bandwidth network. It also handles retries and provides detailed transfer logging.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与GSUTIL相比，存储传输服务是一种托管服务，适用于在不同云（AWS和Azure）、本地或谷歌云中不同存储桶之间快速且安全地传输数据。数据传输过程非常快，因为它利用了高带宽网络。它还处理重试并提供详细的传输日志。
- en: Transfer Appliance
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移设备
- en: This option is suitable when we want to migrate a really large dataset and don’t
    have much bandwidth. Transfer Appliance is a physical device with a high memory
    capacity that can be utilized for transferring and securely shipping data to a
    Google upload facility, where data is uploaded to cloud storage. We can order
    the appliance from the Cloud Console, and once we receive the device, we can start
    copying our data. Finally, we can ship the appliance back to Google to transfer
    data into a specified GCS bucket.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要迁移一个非常大的数据集且带宽有限时，此选项是合适的。转移设备是一种具有高内存容量的物理设备，可用于将数据传输并安全地运送到谷歌上传设施，在那里数据被上传到云存储。我们可以从云控制台订购设备，一旦收到设备，我们就可以开始复制我们的数据。最后，我们可以将设备运回谷歌，以将数据传输到指定的GCS存储桶。
- en: For most machine learning-related use cases, the first two methods should be
    enough to transfer data fast, securely, and consistently. Next, let’s learn more
    about the GCS and BQ data storage systems on Google Cloud.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数与机器学习相关的用例，前两种方法应该足以快速、安全且一致地传输数据。接下来，让我们更多地了解谷歌云上的GCS和BQ数据存储系统。
- en: Where to store data
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据存储位置
- en: GCS and BQ are two recommended options for storing any machine learning use
    case-related datasets for high security and efficiency purposes. If the underlying
    data is structured or semi-structured, BQ is the recommended option due to its
    off-the-shelf features for manipulating or processing structured datasets. If
    the data contains images, videos, audio, and unstructured data, then GCS is the
    suitable option to store it. Let’s learn about these two data storage systems
    in more detail.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GCS和BQ是存储任何与机器学习用例相关的数据集的两个推荐选项，旨在实现高安全性和效率。如果底层数据是有结构的或半结构的，由于BQ具有现成的功能来操作或处理结构化数据集，因此BQ是推荐选项。如果数据包含图像、视频、音频和非结构化数据，那么GCS是存储它的合适选项。让我们更详细地了解这两个数据存储系统。
- en: GCS – object storage
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GCS – 对象存储
- en: A significant amount of data that we collect from real-world applications is
    in unstructured form. Some examples are images, videos, emails, audio files, web
    pages, and sensor data. Managing and storing such huge amounts of unstructured
    data affordably and efficiently is quite challenging. Nowadays, object storage
    has become a preferable solution for storing such large amounts of static data
    and backups. Object storage is a computer data architecture that’s designed to
    handle large amounts of structured data efficiently. Each data object in an object-based
    storage system is considered a distinct unit bundled with metadata and a unique
    identifier that is useful in quickly retrieving and locating data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从现实世界应用中收集的大量数据是无结构的。一些例子包括图像、视频、电子邮件、音频文件、网页和传感器数据。以经济高效的方式管理和存储如此大量的非结构化数据是一项相当具有挑战性的任务。如今，对象存储已成为存储如此大量静态数据和备份的首选解决方案。对象存储是一种计算机数据架构，旨在高效地处理大量结构化数据。在基于对象的存储系统中，每个数据对象都被视为一个独立的单元，包含元数据和唯一的标识符，这对于快速检索和定位数据非常有用。
- en: GCS is an object-based storage system in Google Cloud. As it is cloud-based,
    GCS data can be accessed globally and provides massive scale. It is a suitable
    option for small to large enterprises to store their large amounts of data in
    a cost-effective and easily retrievable fashion. Object storage is more efficient
    for applications where you write the data once but have to read it very frequently.
    While it is extremely good for static data, it’s not a good solution for dynamic
    data. If data is constantly changing, we will have to write the entire data object
    again and again to modify it, which is inefficient.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GCS 是 Google Cloud 中的一个基于对象的存储系统。由于它是基于云的，GCS 数据可以全球访问并提供大规模存储。它是一个适合从小型企业到大型企业以经济高效且易于检索的方式存储大量数据的理想选择。对于您只需写入一次数据但需要非常频繁地读取数据的应用程序，对象存储效率更高。虽然它非常适合静态数据，但不是动态数据的良好解决方案。如果数据不断变化，我们不得不反复写入整个数据对象来修改它，这是低效的。
- en: 'GCS is frequently used by machine learning practitioners on Google Cloud due
    to its variety of benefits. Here are some common benefits of storing data in an
    object storage system such as GCS:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其多样化的好处，GCS 经常被 Google Cloud 上的机器学习从业者使用。以下是存储数据在如 GCS 这样的对象存储系统中的常见好处：
- en: '**Massively scalable**: Object storage can be expanded infinitely by simply
    adding more servers or devices.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大规模可扩展性**：通过简单地添加更多服务器或设备，对象存储可以无限扩展。'
- en: '**Less complex**: Unlike a file storage system, there is no hierarchy or folder
    structure in object storage, so retrieval is quite simple.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更简单**：与文件存储系统不同，对象存储中没有层次结构或文件夹结构，因此检索相当简单。'
- en: '**Searchability**: It is easy to search for a specific object as metadata is
    also part of the object. We can use tags to make objects more filterable.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可搜索性**：由于元数据也是对象的一部分，因此很容易搜索特定对象。我们可以使用标签使对象更容易过滤。'
- en: '**Resiliency**: There’s no fear of data loss as it can automatically replicate
    data and store it across multiple devices or geographical locations.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：由于它可以自动复制数据并在多个设备或地理位置之间存储，因此无需担心数据丢失。'
- en: '**Low cost**: Object storage is cost-effective and thus ideal for storing large
    amounts of data. Secondly, we only pay for the capacity we use.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低成本**：对象存储经济高效，因此非常适合存储大量数据。其次，我们只为使用的容量付费。'
- en: 'With so many advantages, there are also some limitations of object-based storage
    systems:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有这么多优点，但基于对象的存储系统也有一些局限性：
- en: Due to latency concerns, they cannot be used in place of traditional databases
    when designing web applications
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于延迟问题，在设计 Web 应用程序时不能替代传统数据库
- en: They’re not suitable for situations when data is changing rapidly and lots of
    file writes are required very frequently
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不适合数据快速变化且需要非常频繁地大量写入文件的情况
- en: They’re not very compatible with operating system mounting and require additional
    clients or adapters to work with
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们与操作系统挂载不太兼容，需要额外的客户端或适配器才能工作
- en: Now that we have a fair idea about the advantages and limitations of object
    storage systems, we will be able to utilize them based on relevant requirements
    in future projects. Now, let’s learn more about BQ.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对对象存储系统的优点和局限性有了相当的了解，我们将在未来的项目中根据相关需求利用它们。现在，让我们更多地了解 BQ。
- en: BQ – data warehouse
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BQ – 数据仓库
- en: BQ is a fully managed and serverless data warehouse available on Google Cloud.
    It is a petabyte-scale platform that enables scalable analysis on large datasets.
    BQ’s serverless architecture supports SQL queries for slicing and dicing large
    datasets. Its analysis engine is very scalable and supports distributed analysis
    such that we can query terabytes of data in seconds and petabytes in minutes.
    BQ also supports machine learning, meaning we can train and test common ML models
    within BQ using just a few SQL-like commands. We will learn about **BigQuery Machine
    Learning** (**BQML**) in the coming chapters.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: BQ 是 Google Cloud 上提供的一个全托管和无服务器的数据仓库。它是一个可扩展分析大型数据集的皮字节级平台。BQ 的无服务器架构支持 SQL
    查询以切片和切块大型数据集。其分析引擎非常可扩展，支持分布式分析，这样我们可以在几秒钟内查询千兆字节的数据，在几分钟内查询皮字节的数据。BQ 还支持机器学习，这意味着我们只需使用几个类似
    SQL 的命令即可在 BQ 中训练和测试常见的 ML 模型。我们将在接下来的章节中学习**BigQuery 机器学习**（**BQML**）。
- en: Behind the scenes, BQ stores data in a columnar storage format that is optimized
    for analytical queries. Data inside BQ is presented in a database via tables with
    rows and columns. It provides full support for **Atomicity, Consistency, Isolation,
    and Durability** (**ACID**) properties, similar to a transactional database management
    system. BQ provides high availability for data by automatically replicating it
    across multiple locations and regions in Google Cloud. In addition to the data
    that is present inside BQ storage, it also provides flexibility to query data
    from external sources, including GCS, Bigtable, Google Sheets, and Spanner.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，BQ 以优化的列式存储格式存储数据，该格式针对分析查询进行了优化。BQ 内的数据通过具有行和列的表以数据库的形式呈现。它提供了类似于事务型数据库管理系统的**原子性、一致性、隔离性和持久性**（**ACID**）属性的全支持。BQ
    通过在 Google Cloud 的多个位置和区域自动复制数据，为数据提供高可用性。除了 BQ 存储内部的数据外，它还提供了从外部源查询数据的灵活性，包括
    GCS、Bigtable、Google Sheets 和 Spanner。
- en: 'If a machine learning use case involves structured or semi-structured data,
    BQ can be the best place to store and analyze it. In subsequent chapters, we will
    learn more about how BQ is an extremely useful tool for data analysts and machine
    learning practitioners. Here are some common benefits of using BQ as a data warehousing
    solution:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习用例涉及结构化或半结构化数据，BQ 可以是存储和分析的最佳选择。在随后的章节中，我们将了解 BQ 是数据分析师和机器学习实践者极其有用的工具。以下是使用
    BQ 作为数据仓库解决方案的一些常见好处：
- en: '**Speed and scale**: With BQ, querying through massive datasets only takes
    seconds. BQ was designed to analyze and store very large amounts of datasets with
    ease. It can scale seamlessly from petabytes to exabytes.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度和规模**：使用 BQ，对大规模数据集进行查询只需几秒钟。BQ 被设计成轻松分析和存储非常大的数据集。它可以从千兆字节无缝扩展到艾字节。'
- en: '**Real-time analytics**: BQ supports streaming data ingestion and makes it
    immediately available for querying. Its integration with the BI tool Looker Studio
    allows it to provide real-time and interactive analytical capabilities.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时分析**：BQ 支持流数据摄取，并使其立即可用于查询。它与 BI 工具 Looker Studio 的集成使其能够提供实时和交互式的分析能力。'
- en: '**Machine learning**: BQ provides capabilities to build and operationalize
    machine learning models on both structured and unstructured data using simple
    SQL – in a very short time. BQ models can be directly exported so that they can
    be served on the Vertex AI prediction service.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：BQ 提供了使用简单的 SQL 在结构化和非结构化数据上构建和运行机器学习模型的能力，时间非常短。BQ 模型可以直接导出，以便在 Vertex
    AI 预测服务上提供服务。'
- en: '**Security**: BQ provides strong security and fine-grained governance controls.
    Data is encrypted at rest and in transit by default.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：BQ 提供强大的安全性和细粒度的治理控制。数据默认在静态和传输过程中加密。'
- en: '**Multi-cloud support**: BQ allows for data analysis across multiple cloud
    platforms. BQ can run analysis on data where it is located without having to move
    it to a different location, which makes it more cost-effective and flexible.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多云支持**：BQ 允许在多个云平台之间进行数据分析。BQ 可以在不将数据移动到其他位置的情况下对数据进行分析，这使得它更加经济高效且灵活。'
- en: We now have a fair idea of two popular data storage systems – GCS and BQ – and
    their benefits and limitations. Depending on the use case, we should now be capable
    of choosing the right place to store our machine learning datasets. Next, we’ll
    dig into data transformation options on Google Cloud.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对两个流行的数据存储系统——GCS 和 BQ——及其优缺点有了相当的了解。根据具体的使用场景，我们现在应该能够选择合适的存储位置来存放我们的机器学习数据集。接下来，我们将深入了解
    Google Cloud 上的数据转换选项。
- en: Transforming data
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: Raw data present in real-world applications is often unstructured and noisy.
    Thus, it cannot be fed directly to machine learning algorithms. We often need
    to apply several transformations on raw data and convert it into a format that
    is well supported by machine learning algorithms. In this section, we will learn
    about multiple options for transforming data in a scalable and efficient way on
    Google Cloud.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界应用中存在的原始数据通常是未结构化和嘈杂的。因此，不能直接将其输入到机器学习算法中。我们通常需要对原始数据进行多次转换，并将其转换为机器学习算法广泛支持的格式。在本节中，我们将了解在
    Google Cloud 上以可扩展和高效的方式转换数据的多种选项。
- en: 'Here are three common options for data transformation in the GCP environment:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCP 环境中，这里有三种常见的数据转换选项：
- en: Ad hoc transformation within Jupyter Notebooks
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebooks 内进行临时转换
- en: Cloud Data Fusion
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Data Fusion
- en: Dataflow pipelines for scalable data transformations
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的数据转换数据流管道
- en: Let’s learn about these three methods in more detail.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解这三种方法。
- en: Ad hoc transformations within Jupyter Notebook
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中的临时转换
- en: Machine learning algorithms are mathematical and can only understand numeric
    data. For example, in computer vision problems, images are converted into numerical
    pixel values before they’re fed into a model. Similarly, in the case of audio
    data, it is often converted into a time-frequency domain using different transformations,
    such as **Fast Fourier Transformation** (**FFT**). If data is in a tabular format
    and contains rows and columns, some of the columns might contain non-numeric or
    categorical types of data. These categorical columns are first converted into
    numeric form using a suitable transformation and then fed to a machine learning
    model or neural network. Some of these transformations can be directly applied
    in Jupyter notebooks using Python functionalities.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法是数学的，只能理解数值数据。例如，在计算机视觉问题中，图像在输入模型之前被转换为数值像素值。同样，在音频数据的情况下，它通常使用不同的转换（如**快速傅里叶变换**（FFT））转换为时频域。如果数据以表格格式存在，并且包含行和列，那么一些列可能包含非数值或分类类型的数据。这些分类列首先使用合适的转换转换为数值形式，然后输入到机器学习模型或神经网络中。这些转换中的一些可以直接在Jupyter笔记本中使用Python功能应用。
- en: After reading data into a Jupyter Notebook, we can apply any desired transformations
    to make the dataset ready for modeling. Once the dataset is ready, we can save
    it somewhere (for example, BQ or GCS) with a version number so that it can be
    read directly into multiple different model training experiments. Machine learning
    practitioners often create and save multiple versions of training datasets by
    applying different transformations or feature engineering. It makes it easier
    to compare the performance of experiments over different versions of data. Let’s
    learn about some common transformations that are very frequently applied in machine
    learning projects.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据读入Jupyter Notebook后，我们可以应用任何所需的转换，使数据集准备好进行建模。一旦数据集准备就绪，我们可以将其保存到某个位置（例如，BQ或GCS），并附上版本号，以便可以直接将其读入多个不同的模型训练实验中。机器学习从业者经常通过应用不同的转换或特征工程来创建和保存多个版本的训练数据集。这使得比较不同版本数据的实验性能变得更容易。让我们了解一下在机器学习项目中非常频繁应用的一些常见转换。
- en: 'At a high level, data can be classified into the following two categories:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，数据可以分为以下两类：
- en: '**Numeric data**: As the name suggests, this data is numeric or quantifiable
    – for example, age, height, and weight'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值数据**：正如其名所示，这种数据是数值的或可量化的——例如，年龄、身高和体重'
- en: '**Categorical data**: This data is in string format or is qualitative data
    – for example, gender, language, and accent'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类数据**：这种数据是字符串格式或定性数据——例如，性别、语言和口音'
- en: Now, let’s learn about some common transformations that can be applied to these
    two types of data columns.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解可以应用于这两种类型数据列的一些常见转换。
- en: Handling numeric data
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理数值数据
- en: Numerical data can either be discrete or continuous. Discrete data is countable,
    for example, the number of players in a basketball team or the number of cities
    in a country. Discrete data can only take certain values, such as 10, 22, 35,
    41, and so on. On the other hand, any data that is measurable is called continuous
    data – for example, the height of a person or the distance between two racing
    cars. Continuous data can virtually take any value, such as 2.3, 11, 0.0001, and
    so on.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数值数据可以是离散的或连续的。离散数据是可以计数的，例如，一个篮球队中的球员数量或一个国家中的城市数量。离散数据只能取某些值，例如10、22、35、41等等。另一方面，任何可测量的数据都称为连续数据——例如，一个人的身高或两辆赛车之间的距离。连续数据几乎可以取任何值，例如2.3、11、0.0001等等。
- en: 'Two common kinds of transformations are applied to numeric data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对数值数据应用两种常见的转换：
- en: '**Normalizing**: This is done to bring different numeric columns to the same
    scale. It is recommended to normalize numerical data columns to the same scale
    as it helps in the convergence of gradient descent-based ML algorithms. If a data
    column has very large values, then normalizing it can prevent **NaN** errors while
    training models.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**：这是为了将不同的数值列调整到相同的尺度。建议将数值数据列归一化到相同的尺度，因为它有助于基于梯度下降的机器学习算法的收敛。如果一个数据列具有非常大的值，那么归一化它可以防止在训练模型时出现**NaN**错误。'
- en: '**Bucketing**: In this type of transformation, numeric data is converted into
    categorical data. Bucketing is usually done on continuous numeric data when there
    is no linear relationship between the numeric column and the target column. For
    example, a car manufacturing company observes that the cars that have the lowest
    price or the highest price are less frequently sold, but the cars with mid-range
    prices are more frequently sold. In this case, if we want to predict the number
    of cars that are sold using car price as a feature, it will be beneficial to bucketize
    the car price into price ranges. In this way, the model will be able to identify
    the mid-range bucket in which most of the cars are sold and assign more weight
    to this feature.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分桶**：在这种类型的转换中，数值数据被转换为分类数据。分桶通常在连续数值数据上执行，当数值列和目标列之间没有线性关系时。例如，一家汽车制造公司观察到价格最低或最高的汽车销售频率较低，而价格中等的汽车销售频率较高。在这种情况下，如果我们想使用汽车价格作为特征来预测销售的汽车数量，将汽车价格分桶到价格范围将是有益的。这样，模型将能够识别出销售汽车数量最多的中档价格桶，并为此特征分配更多的权重。'
- en: Handling categorical data
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理分类数据
- en: Categorical data can be divided into two categories – **ordered** and **nominal**.
    Ordered categorical data has some order associated with it – for example, movie
    ratings (worst, bad, good, excellent) and feedback (negative, neutral, positive).
    Ordered data can always be marked on a scale. Nominal data, on the other hand,
    has no order. Some examples of nominal data include gender, country, and language.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 分类数据可以分为两类——**有序**和**名义**。有序分类数据与其相关联的某些顺序有关——例如，电影评分（最差、差、好、优秀）和反馈（负面、中性、正面）。有序数据总是可以标记在量表上。另一方面，名义数据没有顺序。名义数据的例子包括性别、国家和语言。
- en: 'Some algorithms, such as decision trees, can work well with categorical data,
    but most machine learning algorithms cannot handle categorical data directly.
    These algorithms require the categorical data to be converted into numerical form.
    While converting the categorical data into numerical form, some challenges are
    faced by machine learning practitioners:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法，如决策树，可以很好地处理分类数据，但大多数机器学习算法不能直接处理分类数据。这些算法需要将分类数据转换为数值形式。在将分类数据转换为数值形式时，机器学习从业者会面临一些挑战：
- en: '**High cardinality**: Cardinality means uniqueness in data. A high cardinality
    data column might have lots of different values – for example, ZIP codes in country-level
    data.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高基数**：基数意味着数据中的唯一性。具有高基数的数据列可能有很多不同的值——例如，国家层面的ZIP代码。'
- en: '**Rare or frequent occurrences**: A data column might have some values that
    rarely occur or some values that occur very frequently. In both cases, this column
    would not be significant enough to make an impact on the model due to very high
    or very low variance.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏或频繁出现**：数据列可能有一些很少出现或一些非常频繁出现的值。在这两种情况下，由于非常高的或非常低的方差，这个列可能不足以对模型产生影响。'
- en: '**Dynamic values**: A data column that keeps changing some values from time
    to time – for example, in a city column, if new cities are added or removed very
    frequently.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态值**：数据列会不时地改变一些值——例如，在市列中，如果新城市被频繁添加或删除。'
- en: 'The best way to overcome these challenges highly depends on the kind of problem
    or data we are dealing with. Now, let’s learn about some common methods of converting
    categorical data into numerical form:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这些挑战的最佳方式高度依赖于我们处理的问题或数据类型。现在，让我们了解一些将分类数据转换为数值形式的常见方法：
- en: '**Label encoding**: In this technique, we replace categorical data with integer
    values from 0 to *N*-1\. Here, each integer represents a value from a categorical
    data column with *N* unique values. For example, if there is a categorical data
    column that represents colors and 10 unique color values are possible, in this
    case, each color will be mapped and replaced with an integer from the range of
    0 to 9\. This method may not be ideal for all cases as the model might consider
    numeric values as weights assigned to the data. Thus, this method is more suitable
    for ordinal categorical data.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签编码**：在这种技术中，我们将分类数据替换为从0到*N*-1的整数。在这里，每个整数代表一个具有*N*个唯一值的分类数据列中的值。例如，如果有一个表示颜色的分类数据列，并且可能有10种独特的颜色值，在这种情况下，每种颜色将被映射并替换为0到9范围内的一个整数。这种方法可能不是所有情况都理想，因为模型可能会将数值视为分配给数据的权重。因此，这种方法更适合有序分类数据。'
- en: '`1` if that color is present in a given row; otherwise, it will be `0`. One-hot
    encoding is often preferred for encoding categorical data.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` 如果该颜色在给定行中存在；否则，它将是 `0`。One-hot 编码通常用于对分类数据进行编码。'
- en: '**Embeddings**: As one-hot encoding creates a new column for each unique value,
    this may create a very sparse representation of data when the number of unique
    values is large in number. For example, if we have a ZIP code column with 20k
    unique ZIP codes, the one-hot encoding method will create 20k new binary columns.
    Such sparse data takes a lot of memory to store and increases the complexity of
    machine learning training. To handle and represent such categorical data columns
    with a large number of unique values, dense embeddings can be used. These embeddings,
    however, are often generated using a neural network, so it’s an off-the-shelf
    encoding technique. These embeddings encode each value from a categorical column
    into a small dense vector of real numbers. One simple method to train and generate
    these embeddings is using the inbuilt Keras embedding layer.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**：由于 One-hot 编码为每个唯一值创建一个新列，当唯一值的数量很大时，这可能会创建一个非常稀疏的数据表示。例如，如果我们有一个包含
    20k 个唯一 ZIP 代码的 ZIP 代码列，One-hot 编码方法将创建 20k 个新的二进制列。这种稀疏数据需要大量的内存来存储，并增加了机器学习训练的复杂性。为了处理和表示具有大量唯一值的此类分类数据列，可以使用密集嵌入。然而，这些嵌入通常使用神经网络生成，因此它是一种现成的编码技术。这些嵌入将分类列中的每个值编码为一个小型的密集实数向量。训练和生成这些嵌入的一个简单方法是用内置的
    Keras 嵌入层。'
- en: Now that we have a good understanding of common data transformations that are
    easy to apply in Jupyter notebooks using Python, let’s learn about some more scalable
    ways of data transformation on GCP, such as Cloud Data Fusion and Dataflow.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对在 Jupyter 笔记本中使用 Python 容易应用的一些常见数据转换有了很好的理解，让我们了解一些在 GCP 上进行数据转换的更多可扩展方法，例如
    Cloud Data Fusion 和 Dataflow。
- en: Cloud Data Fusion
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cloud Data Fusion
- en: Cloud Data Fusion is a fully managed service on GCP for quickly building and
    managing scalable data pipelines. Using the Data Fusion UI, we can build and deploy
    data pipelines without writing a single line of code (using a visual point-and-click
    interface). The Data Fusion UI lets us build scalable data integration solutions
    to clean, prepare, blend, transform, and transfer data in a fully managed way
    (which means we do not need to manage infrastructure). Cloud Data Fusion offers
    hundreds of prebuilt transformations for both batch and real-time data processing
    and quickly building ETL/ELT pipelines.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Data Fusion 是 GCP 上的一个完全托管服务，用于快速构建和管理可扩展的数据管道。使用 Data Fusion UI，我们可以构建和部署数据管道，而无需编写任何代码（使用可视化点选界面）。Data
    Fusion UI 允许我们以完全托管的方式构建可扩展的数据集成解决方案，以清理、准备、混合、转换和传输数据（这意味着我们不需要管理基础设施）。Cloud
    Data Fusion 为批处理和实时数据处理提供了数百个预构建的转换，并快速构建 ETL/ELT 管道。
- en: 'Some key features of Cloud Data Fusion are as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Data Fusion 的一些关键特性如下：
- en: '**Portability**: Cloud Data Fusion is built using an open source project called
    **CDAP**, thus ensuring data pipeline portability'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：Cloud Data Fusion 是使用名为 **CDAP** 的开源项目构建的，从而确保数据管道的可移植性'
- en: '**Simple integration**: Cloud Data Fusion’s easy integration with Google Cloud
    functionalities such as GCS, Dataproc, and BigQuery makes development faster and
    easier, ensuring security'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单集成**：Cloud Data Fusion 与 Google Cloud 的功能（如 GCS、Dataproc 和 BigQuery）的简单集成使开发更快、更简单，并确保安全性'
- en: '**No-code pipelines**: Even non-technical users can build data pipelines quickly
    using Cloud Data Fusion’s web interface'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无代码管道**：即使非技术用户也可以使用 Cloud Data Fusion 的 Web 界面快速构建数据管道'
- en: '**Hybrid**: Since Cloud Data Fusion is an open source project, it provides
    flexibility to build standardized data pipelines across hybrid and multi-cloud
    environments'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合**：由于 Cloud Data Fusion 是一个开源项目，它提供了在混合和多云环境中构建标准化数据管道的灵活性'
- en: '**Security**: Cloud Data Fusion provides enterprise-grade security and access
    management with Google Cloud for data protection and compliance.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：Cloud Data Fusion 提供了企业级的安全性和访问管理，结合 Google Cloud 的数据保护和合规性。'
- en: In addition to the web UI, we can also use command-line tools to create and
    manage Cloud Data Fusion instances and pipelines. Next, let’s learn about another
    data transformation tool on GCP – Dataflow
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Web UI 之外，我们还可以使用命令行工具创建和管理 Cloud Data Fusion 实例和管道。接下来，让我们了解 GCP 上的另一个数据转换工具——Dataflow。
- en: Dataflow pipelines for scalable data transformations
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展的数据转换的 Dataflow 管道
- en: Dataflow is a managed service for executing data processing or transformation
    pipelines on Google Cloud that are developed using **Apache Beam SDK**. It supports
    unified batch and stream data processing that is fast, cost-effective, and serverless
    (which means we do not need to manage infrastructure). Because Dataflow is serverless,
    it lets us focus on expressing the business logic of our data pipeline (using
    SQL or code) without worrying about operational tasks and infrastructure management.
    Due to its streaming nature, Dataflow is ideal for building real-time pipelines
    for use cases such as anomaly detection, pattern recognition, and forecasting.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Dataflow是一个托管服务，用于在Google Cloud上执行使用**Apache Beam SDK**开发的数据处理或转换管道。它支持快速、成本效益和无服务器（这意味着我们不需要管理基础设施）的统一批处理和流数据处理。由于Dataflow是无服务器的，它让我们能够专注于表达数据管道的业务逻辑（使用SQL或代码），而无需担心操作任务和基础设施管理。由于其流式特性，Dataflow非常适合构建用于异常检测、模式识别和预测等用例的实时管道。
- en: By combining Dataflow with other managed Google Cloud services, we can simplify
    many aspects of productionizing data pipelines compared to self-managed solutions.
    For example, it can be combined with Google Cloud offerings such as Pub/Sub and
    BQ to develop a streaming solution that can ingest, process, and analyze fluctuating
    volumes of real-time data and generate invaluable real-time business insights.
    As it is managed, it provisions and scales the required resources automatically
    and thus reduces the time and complexity for data engineers or data analysts working
    on stream analytics solutions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将Dataflow与其他管理的Google Cloud服务相结合，我们可以简化与自管理解决方案相比的生产化数据管道的许多方面。例如，它可以与Google
    Cloud提供的产品，如Pub/Sub和BQ结合，开发一个能够处理、分析和生成宝贵实时业务洞察的流式解决方案。由于它是管理的，它会自动配置和扩展所需资源，从而减少在流分析解决方案上工作的数据工程师或数据分析师的时间和复杂性。
- en: 'Some key features of Dataflow are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Dataflow的一些关键特性如下：
- en: '**Smart autoscaling**: Dataflow supports both horizontal and vertical scaling
    of worker nodes. Scaling is performed automatically in such a way that the utilization
    of worker nodes and other pipeline scaling requirements are met in an efficient
    (cost-effective) or best-fit manner.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**智能自动扩展**：Dataflow支持工作节点的水平和垂直扩展。扩展是自动执行的，以确保以高效（成本效益）或最佳匹配的方式满足工作节点和其他管道扩展需求。'
- en: '**Real-time pipelines**: Dataflow’s streaming nature is useful in building
    real-time stream analytics, machine learning forecasting, and anomaly detection
    pipelines. It is also useful for synchronizing or replicating data across multiple
    data sources (such as BQ, PostgreSQL, or Cloud Spanner) with minimal latency.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时管道**：Dataflow的流式特性在构建实时流分析、机器学习预测和异常检测管道时非常有用。它还适用于以最小延迟同步或复制跨多个数据源（如BQ、PostgreSQL或Cloud
    Spanner）的数据。'
- en: '**Dataflow SQL**: Dataflow streaming pipelines can be built using simple SQL
    commands right from BQ.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dataflow SQL**：可以使用简单的SQL命令直接从BQ构建Dataflow流式管道。'
- en: '**Flexible scheduling**: Batch processing jobs that need scheduling (such as
    overnight jobs) can easily be scheduled using Dataflow **Flexible Resource Scheduling**
    (**FlexRS**) in a cost-effective setting.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活调度**：需要调度的批处理作业（如夜间作业）可以轻松地使用Dataflow的**灵活资源调度**（**FlexRS**）在成本效益的环境中进行调度。'
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Managing data effectively is really important for saving time, cost, and complexity
    for every organization. A machine learning practitioner should be aware of the
    best options for transferring, storing, and transforming data to build machine
    learning solutions more efficiently. In this chapter, we learned about multiple
    ways of bringing data into the Google Cloud environment. We discussed the best
    options for storing it based on the characteristics of the data. Finally, we discussed
    multiple different tools and methods for transforming/processing data in a scalable
    manner.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有效地管理数据对于每个组织节省时间、成本和复杂性至关重要。机器学习从业者应该了解传输、存储和转换数据以更有效地构建机器学习解决方案的最佳选项。在本章中，我们学习了将数据带入Google
    Cloud环境的多种方式。我们讨论了根据数据特性存储的最佳选项。最后，我们讨论了多种不同的工具和方法，以可扩展的方式转换/处理数据。
- en: After reading this chapter, you should feel confident about choosing the best
    option for moving or transferring data into your Google Cloud environment based
    on the requirements of the use case. Choosing the best place to store data and
    the best strategy to analyze and transform data should be easier as we now know
    the pros and cons of different options. In the next chapter, we will deep dive
    into **Vertex AI Workbench**, which is a managed notebook platform within Vertex
    AI.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，你应该对根据用例需求选择最佳方案将数据移动或传输到你的Google Cloud环境中充满信心。选择最佳的数据存储位置以及最佳的数据分析和转换策略应该更容易，因为我们现在已经了解了不同选项的优缺点。在下一章中，我们将深入探讨**Vertex
    AI Workbench**，这是一个Vertex AI内部的托管笔记本平台。
