<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using Deep Learning to Classify Handwritten Digits</h1>
                </header>
            
            <article>
                
<p>Let's now return to supervised learning and discuss a family of algorithms known as <strong>artificial neural networks</strong>. Early studies of neural networks go back to the 1940s when Warren McCulloch and Walter Pitts first described how biological nerve cells (or neurons) in the brain might work. More recently, artificial neural networks have seen a revival under the buzzword deep learning, which powers state-of-the-art technologies such as Google's DeepMind and Facebook's DeepFace algorithms.</p>
<p>In this chapter, we want to wrap our heads around some simple versions of artificial neural <span><span>networks</span></span>, such as the McCulloch-Pitts neuron, the perceptron, and the multilayer perceptron. Once we have familiarized ourselves with the basics, we will be ready to implement a more sophisticated deep neural network to classify handwritten digits from the popular <strong>MNIST database</strong> (short for <strong>Mixed National Institute of Standards and Technology database</strong>). For this, we will be making use of Keras, a high-level neural network library, which is also frequently used by researchers and tech companies.</p>
<p><span>Along the way, we will address the following topics:</span></p>
<ul>
<li>Implementing perceptrons and multilayer perceptrons in OpenCV</li>
<li>Differentiating stochastic and batch gradient descent, and how they fit in with backpropagation</li>
<li>Finding the size of your neural network</li>
<li>Using Keras to build sophisticated deep neural networks</li>
</ul>
<p>Excited? Then let's go!</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can refer the code for this chapter at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09</a>.</p>
<p>Here is a short summary of the software and hardware requirements:</p>
<ul>
<li>OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>Python version 3.6 (any Python version 3.x will be fine).</li>
<li>Anaconda Python 3 for installing Python and the required modules.</li>
<li>You can use any operating system—macOS, Windows, or Linux-based—with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided with the book.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the McCulloch-Pitts neuron</h1>
                </header>
            
            <article>
                
<p>In 1943, Warren McCulloch and Walter Pitts published a mathematical description of neurons as they were believed to operate in the brain. A neuron receives input from other neurons through connections on its dendritic tree, which are integrated to produce an output at the cell body (or soma). The output is then communicated to other neurons via a long wire (or axon), which eventually branches out to make one or more connections (at axon terminals) on the dendritic tree of other neurons.</p>
<p>An example neuron is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-953 image-border" src="Images/b6e7d5f2-dde4-47cb-9ba1-dc48d0346dbd.png" style="width:23.17em;height:13.75em;" width="1581" height="938"/></p>
<p class="mce-root"/>
<p>McCulloch and Pitts described the inner workings of such a neuron as a simple logic gate that would be either on or off, depending on the input it received on its dendritic tree. Specifically, the neuron would sum up all of its inputs, and if the sum exceeded a certain threshold, an output signal would be generated and passed on by the axon.</p>
<div class="packt_infobox">However, today we know that real neurons are much more complicated than that. Biological neurons perform intricate nonlinear mathematical operations on thousands of inputs and can change their responsiveness dynamically depending on the context, importance, or novelty of the input signal. You can think of real neurons being as complex as computers and of the human brain being as complex as the internet.</div>
<p>Let's consider a simple artificial neuron that receives exactly two inputs, <em>x<sub>0</sub></em> and <em>x<sub>1</sub></em>. The job of the artificial neuron is to calculate a sum of the two inputs (usually in the form of a weighted sum), and if this sum exceeds a certain threshold (often zero), the neuron will be considered active and output a one; else it will be considered silent and output a minus one (or zero). In more mathematical terms, the output, <em>y</em>, of this McCulloch-Pitts neuron can be described as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/77e7cc8a-4619-4549-9be5-d20576ca9175.png" style="width:15.58em;height:2.92em;" width="2560" height="480"/></div>
<p>In the preceding equation, <em>w<sub>0</sub></em> and <em>w<sub>1</sub></em> are weight coefficients, which, together with <em>x<sub>0</sub></em> <span>and</span> <em>x<sub>1</sub></em>, make up the weighted sum. In textbooks, the two different scenarios where the output, <em>y</em>, is either <em>+1</em> and <em>-1</em> would often be masked by an activation function, <em>ϕ</em>, which could take on two different values:</p>
<p><img src="Images/d494b0cc-0876-44a1-abf7-6dd5029c2724.png" style="width:36.17em;height:3.58em;" width="6358" height="629"/></p>
<p>Here, we introduce a new variable, <em>z</em> (the so-called <strong>network input</strong>), which is equivalent to the weighted sum: <em>z = w<sub>0</sub>x<sub>0</sub> + w<sub>1</sub>x<sub>1</sub></em>. The weighted sum is then compared to a threshold, <span><em>θ</em>, to determine the value of <em>ϕ</em> and subsequently the value of <em>y</em>. Apart from that, these two equations say exactly the same thing as the preceding one.</span></p>
<p>If these equations look strangely familiar, you might be reminded of <a href="7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml" target="_blank">Chapter 1</a>, <em>A Taste of Machine Learning</em>, when we were talking about linear classifiers.</p>
<p>And you are right, a McCulloch-Pitts neuron is essentially a linear, binary classifier!</p>
<p>You can think of it this way: <em>x<sub>0</sub></em> and <em>x<sub>1</sub></em> are the input features, <em>w<sub>0</sub></em> and <em>w<sub>1</sub></em> are weights to be learned, and the classification is performed by the activation function, <em>ϕ.</em> If we do a good job of learning the weights, which we would do with the help of a suitable training set, we could classify data as positive or negative samples. In this scenario, <em>ϕ(z)=θ</em> would act as the decision boundary.</p>
<p>This might all make more sense with the help of the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-965 image-border" src="Images/052ff7aa-3830-49eb-8b74-a7c2b22b90fd.png" style="width:31.67em;height:16.50em;" width="1338" height="698"/></p>
<p>On the left, you can see the neuron's activation function, <em>ϕ</em>, plotted against <em>z</em>. Remember that <em>z</em> is nothing more than the weighted sum of the two inputs <em>x<sub>0</sub></em> <span>and</span> <em>x<sub>1</sub></em><sub>.</sub> The rule is that as long as the weighted sum is below some threshold, <em>θ</em>, the output of the neuron is -1; above <em>θ</em>, the output is +1.</p>
<p>On the right, you can see the decision boundary denoted by <em>ϕ(z)=θ</em>, which splits the data into two regimes, <em>ϕ(z)&lt;θ</em> (where all data points are predicted to be negative samples) and <em>ϕ(z)&gt;θ</em> (where all data points are predicted to be positive samples).</p>
<div class="packt_tip">The decision boundary does not need to be vertical or horizontal, it can be tilted as shown in the preceding diagram. But in the case of a single McCulloch-Pitts neuron, the decision boundary will always be a straight line.</div>
<p>Of course, the magic lies with learning the weight coefficients, <em>w<sub>0</sub></em> and <em>w<sub>1</sub></em>, such that the decision boundary comes to lie right between all positive and all negative data points.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To train a neural network, we generally need three things:</p>
<ul>
<li><strong>Training data</strong>: It is no surprise to learn that we need some data samples with which the effectiveness of our classifier can be verified.</li>
<li><strong>Cost function (also known as loss function)</strong>: A cost function provides a measure of how good the current weight coefficients are. There is a wide range of cost functions available, which we will talk about toward the end of this chapter. One solution is to count the number of misclassifications. Another one is to calculate the <strong>sum of squared errors</strong>.</li>
<li><strong>Learning rule</strong>: A learning rule specifies mathematically how we have to update the weight coefficients from one iteration to the next. This learning rule usually depends on the error (measured by the cost function) we observed on the training data.</li>
</ul>
<p>This is where the work of renowned researcher Frank Rosenblatt comes in.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the perceptron</h1>
                </header>
            
            <article>
                
<p>In the 1950s, American psychologist and artificial intelligence researcher Frank Rosenblatt invented an algorithm that would automatically learn the optimal weight coefficients <em>w<sub>0</sub></em> <span>and</span> <em>w<sub>1</sub></em> needed to perform an accurate binary classification: the perceptron learning rule.</p>
<p>Rosenblatt's original perceptron algorithm can be summed up as follows:</p>
<ol>
<li>Initialize the weights to zero or some small random numbers.</li>
<li>For each training sample, <em>s<sub>i</sub></em>, perform the following steps:
<ol>
<li>Compute the predicted target value, <em>ŷ</em><em><sub>i</sub>.</em></li>
<li>Compare <em>ŷ</em><em><sub>i</sub></em> to the ground truth, <em>y</em><em><sub>i</sub></em>, and update the weights accordingly:
<ul>
<li>If the two are the same (correct prediction), skip ahead.</li>
<li>If the two are different (wrong prediction), push the weight coefficients, <em>w<sub>0</sub></em> <span>and</span> </li></ul></li></ol></li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing your first perceptron</h1>
                </header>
            
            <article>
                
<p>Perceptrons are easy enough to be implemented from scratch. We can mimic the typical OpenCV or scikit-learn implementation of a classifier by creating a perceptron object. This will allow us to initialize new perceptron objects that can learn from data via a <kbd>fit</kbd> method and make predictions via a separate <kbd>predict</kbd> method.</p>
<p>When we initialize a new perceptron object, we want to pass a learning rate (<kbd>lr</kbd>, or <em>η</em> in the previous section) and the number of iterations after which the algorithm should terminate (<kbd>n_iter</kbd>):</p>
<pre>In [1]: import numpy as np<br/>In [2]: class Perceptron(object):<br/>...     def __init__(self, lr=0.01, n_iter=10):<br/>...     self.lr = lr<br/>...     self.n_iter = n_iter<br/>... </pre>
<p>The <kbd>fit</kbd> method is where most of the work is done. This method should take as input some data samples (<kbd>X</kbd>) and their associated target labels (<kbd>y</kbd>). We will then create an array of weights (<kbd>self.weights</kbd>), one for each feature (<kbd>X.shape[1]</kbd>), initialized to zero. For convenience, we will keep the bias term (<kbd>self.bias</kbd>) separate from the weight vector and initialize it to zero as well. One of the reasons for initializing the bias to zero is because the small random numbers in the weights provide asymmetry breaking in the network:</p>
<pre>...         def fit(self, X, y):<br/>...             self.weights = np.zeros(X.shape[1])<br/>...             self.bias = 0.0</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>predict</kbd> method should take in a number of data samples (<kbd>X</kbd>) and, for each of them, return a target label, either +1 or -1. In order to perform this classification, we need to implement <em>ϕ(z)&gt;θ</em>. Here we will choose <em>θ = 0</em>, and the weighted sum can be computed with NumPy's dot product:</p>
<pre>...         def predict(self, X):<br/>...             return np.where(np.dot(X, self.weights) + self.bias &gt;= 0.0,<br/>...                             1, -1)</pre>
<p>Then we will calculate the <span>Δ</span><em>w</em> terms for every data sample (<kbd>xi</kbd>, <kbd>yi</kbd>) in the dataset and repeat this step for a number of iterations (<kbd>self.n_iter</kbd>). For this, we need to compare the ground-truth label (<kbd>yi</kbd>) to the predicted label (aforementioned <kbd>self.predict(xi)</kbd>). The resulting delta term will be used to update both the weights and the bias term:</p>
<pre>...             for _ in range(self.n_iter):<br/>...                 for xi, yi in zip(X, y):<br/>...                     delta = self.lr * (yi - self.predict(xi))<br/>...                     self.weights += delta * xi<br/>...                     self.bias += delta</pre>
<p>That's it!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generating a toy dataset</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will learn how to create and plot a toy dataset:</p>
<ol>
<li>To test our perceptron classifier, we need to create some mock data. Let's keep things simple for now and generate 100 data samples (<kbd>n_samples</kbd>) belonging to one of two blobs (<kbd>centers</kbd>), again relying on scikit-learn's <kbd>make_blobs</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In [3]: from sklearn.datasets.samples_generator import make_blobs...     X, y = make_blobs(n_samples=100, centers=2,...                       cluster_std=2.2, random_state=42)</pre>
<ol start="2">
<li>One thing to keep in mind is that our perceptron classifier expects target labels to be either +1 or -1, whereas <kbd>make_blobs</kbd> returns <kbd>0</kbd> and <kbd>1</kbd>. An easy way to adjust the labels is with the following equation:</li>
</ol>
<pre style="padding-left: 60px">In [4]: y = 2 * y - 1</pre>
<ol start="4">
<li>In the following code, we ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fitting the perceptron to data</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will learn to fit a perceptron algorithm on the given data:</p>
<ol>
<li>We can instantiate our perceptron object similar to other classifiers we encountered with OpenCV:</li>
</ol>
<pre style="padding-left: 60px">In [6]: p = Perceptron(lr=0.1, n_iter=10)</pre>
<p style="padding-left: 60px">Here, we chose a learning rate of 0.1 and told the perceptron to terminate after 10 iterations. These values are chosen rather arbitrarily at this point, although we will come back to them in a little while.</p>
<div class="mce-root packt_tip"><span>Choosing an appropriate learning rate is critical, but it's not always clear what the most appropriate choice is. The learning rate determines how quickly or slowly we move toward the optimal weight coefficients. If the learning rate is too large, we might accidentally skip the optimal solution. If it is too small, we will need a large number of iterations to converge to the best values.</span></div>
<ol start="2">
<li>Once the perceptron is set up, we can call the <kbd>fit</kbd> method to optimize the weight coefficients:</li>
</ol>
<pre style="padding-left: 60px">In [7]: p.fit(X, y)</pre>
<ol start="3">
<li>Did it work? Let's have a look at the learned weight values:</li>
</ol>
<pre style="padding-left: 60px">In [8]: p.weights<br/>Out[8]: array([ 2.20091094, -0.4798926 ])</pre>
<ol start="4">
<li>And don't forget to have a peek at the bias term:</li>
</ol>
<pre style="padding-left: 60px">In [9]: p.bias<br/>Out[9]: 0.20000000000000001</pre>
<p>If we plug these values into our equation for <em>ϕ</em>, it becomes clear that the perceptron learned a decision boundary of the form <em>2.2 x<sub>1</sub> - 0.48 x<sub>2</sub> + 0.2 &gt;= 0</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Evaluating the perceptron classifier</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will be evaluating the trained perceptron on the test data:</p>
<ol>
<li>In order to find out how good our perceptron performs, we can calculate the accuracy score on all data samples:</li>
</ol>
<pre style="padding-left: 60px">In [10]: from sklearn.metrics import accuracy_score...      accuracy_score(p.predict(X), y)Out[10]: 1.0</pre>
<p style="padding-left: 60px">Perfect score!</p>
<ol start="2">
<li>Let's have a look at the decision landscape by bringing back our <kbd>plot_decision_boundary</kbd> from the earlier chapters:</li>
</ol>

<pre style="padding-left: 60px">In [10]: def plot_decision_boundary(classifier, X_test, y_test):...          # create a mesh to plot in...          h = 0.02 # step size in mesh...          x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1...          y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1... xx, yy = np.meshgrid(np.arange(x_min, ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applying the perceptron to data that is not linearly separable</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will learn to build a perceptron to separate a nonlinear data:</p>
<ol>
<li>Since the perceptron is a linear classifier, you can imagine that it would have trouble trying to classify data that is not linearly separable. We can test this by increasing the spread (<kbd>cluster_std</kbd>) of the two blobs in our toy dataset so that the two blobs start overlapping:</li>
</ol>
<pre style="padding-left: 60px">In [12]: X, y = make_blobs(n_samples=100, centers=2,<br/>...      cluster_std=5.2, random_state=42)<br/>...      y = 2 * y - 1</pre>
<ol start="2">
<li>We can plot the dataset again using matplotlib's <kbd>scatter</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In [13]: plt.scatter(X[:, 0], X[:, 1], s=100, c=y);<br/>...      plt.xlabel('x1')<br/>...      plt.ylabel('x2')</pre>
<p style="padding-left: 60px">As is evident in the following screenshot, this data is no longer linearly separable because there is no straight line that perfectly separates the two blobs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-973 image-border" src="Images/6a9a9a8d-1f6a-4fc8-992f-bb38f1192b49.png" style="width:37.75em;height:23.08em;" width="907" height="554"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The preceding s<span>creenshot</span> shows an example of data that is not linearly separable. So what would happen if we applied the perceptron classifier to this dataset?</p>
<ol start="3">
<li>We can find an answer to this question by repeating the preceding steps:</li>
</ol>
<pre style="padding-left: 60px">In [14]: p = Perceptron(lr=0.1, n_iter=10)<br/>...      p.fit(X, y)</pre>
<ol start="4">
<li>Then we find an accuracy score of 81%:</li>
</ol>
<pre style="padding-left: 60px">In [15]: accuracy_score(p.predict(X), y)<br/>Out[15]: 0.81000000000000005</pre>
<ol start="5">
<li>In order to find out which data points were misclassified, we can again visualize the decision landscape using our helper function:</li>
</ol>
<pre>In [16]: plot_decision_boundary(p, X, y)<br/>...      plt.xlabel('x1')<br/>...      plt.ylabel('x2')</pre>
<p>The following graph makes the limitations of the perceptron classifier evident. Being a linear classifier, it tried to separate the data using a straight line but ultimately failed. The main reason it failed is because the data was not linearly separable even though we achieved 81% accuracy. However, from the following plot, it is clear that many of the red dots lie in the blue region and vice versa. So, unlike a perceptron, we need a nonlinear algorithm that can create not a straight but a nonlinear (circular) decision boundary:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-974 image-border" src="Images/21aa1c5d-691d-412d-af9a-14d86495a613.png" style="width:35.25em;height:22.08em;" width="875" height="547"/></p>
<p class="mce-root"/>
<p>Fortunately, there are ways to make the perceptron more powerful and ultimately create nonlinear decision boundaries.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding multilayer perceptrons</h1>
                </header>
            
            <article>
                
<p>In order to create nonlinear decision boundaries, we can combine multiple perceptrons to form a larger network. This is also known as a <strong>multilayer perceptron</strong> (<strong>MLP</strong>). MLPs usually consist of at least three layers, where the first layer has a node (or neuron) for every input feature of the dataset, and the last layer has a node for every class label. The layer in between is called the <strong>hidden layer</strong>.</p>
<p>An example of this feedforward neural network architecture is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-978 image-border" src="Images/d824c0c2-9d1f-4ccd-a5ba-ac2010eee788.png" style="width:29.92em;height:22.17em;" width="1470" height="1089"/></p>
<p><span>In this network, every circle is an artificial neuron (or, essentially, a perceptron), and the output of one artificial ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding gradient descent</h1>
                </header>
            
            <article>
                
<p>When we talked about the perceptron earlier in this chapter, we identified three of the essential ingredients needed for training: training data, a cost function, and a learning rule. While the learning rule worked great for a single perceptron, unfortunately, it did not generalize to MLPs, so people had to come up with a more general rule.</p>
<p>If you think about how we measure the success of a classifier, we usually do so with the help of a cost function. A typical example is the number of misclassifications of the network or the mean squared error. This function (also known as a <strong>loss function</strong>) usually depends on the parameters we are trying to tweak. In neural networks, these parameters are the weight coefficients.</p>
<p>Let's assume a simple neural network has a single weight to tweak, <em>w</em>. Then we can visualize the cost as a function of the weight:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-982 image-border" src="Images/9f8fb5ba-35a2-48e9-8bed-606c50106a8a.png" style="width:20.92em;height:18.58em;" width="824" height="732"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At the beginning of training, at time zero, we may start out way on the left of this graph (<em>w<sub>t=0</sub></em>). But from the graph, we know that there would be a better value for <em>w</em>, namely <em>w<sub>optimal</sub></em>, which would minimize the cost function. The smallest cost means the lowest error, so it should be our highest goal to reach <em>w<sub>optimal</sub></em> through learning.</p>
<p>This is exactly what gradient descent does. You can think of the gradient as a vector that points up the hill. In gradient descent, we are trying to walk opposite of the gradient, effectively walking down the hill, from the peaks down to the valley:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-983 image-border" src="Images/ccc693b1-b01c-46f0-869f-565c6304fe32.png" style="width:19.58em;height:17.50em;" width="818" height="732"/></p>
<p>Once you reach the valley, the gradient goes to zero, and that completes the training.</p>
<p>There are several ways to reach the valley—we could approach from the left, or we could approach from the right. The starting point of our descent is determined by the initial weight values. Furthermore, we have to be careful not to take too large a step, otherwise we might miss the valley:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-984 image-border" src="Images/dc30f57b-3402-41f5-ba64-e7404f9072fc.png" style="width:18.83em;height:16.83em;" width="818" height="732"/></p>
<p>Hence, in stochastic gradient descent (sometimes also called iterative or on-line gradient descent), the goal is to take small steps but to take them as often as possible. The effective step size is determined by the learning rate of the algorithm.</p>
<p>Specifically, we would perform the following procedure over and over:</p>
<ol>
<li>Present a small number of training samples to the network (called the <strong>batch size</strong>).</li>
<li>On this small batch of data, calculate the gradient of the cost function.</li>
<li>Update the weight coefficients by taking a small step in the opposite direction of the gradient, toward the valley.</li>
<li><span>Repeat steps 1-3 until the weight cost no longer goes down. This is an indication that we have reached the valley.</span></li>
</ol>
<p>Some other ways to improve SGD are using the learning rate finder in the Keras framework, decreasing the step size (learning rate) in epochs, and, as discussed in the preceding point, using a batch size (or mini-batch), which will compute the weight update faster.</p>
<p>Can you think of an example where this procedure might fail?</p>
<p>One scenario that comes to mind is where the cost function has multiple valleys, some deeper than others, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-985 image-border" src="Images/60373859-3362-45bc-b11b-031aeeb28fda.png" style="width:35.25em;height:16.75em;" width="1505" height="716"/></p>
<p>If we start on the left, we should arrive at the same valley as before—no problem. But, if our starting point is all the way to the right, we might encounter another valley on the way. Gradient descent will lead us straight down to the valley, but it will not have any means to climb out of it.</p>
<p class="mce-root"/>
<div class="packt_infobox">This is also known as <strong>getting stuck in a local minimum</strong>. Researchers have come up with different ways to try and avoid this issue, one of them being to add noise to the process.</div>
<p>There is one piece left in the puzzle. Given our current weight coefficients, how do we know the slope of the cost function?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training MLPs with backpropagation</h1>
                </header>
            
            <article>
                
<p>This is where backpropagation comes in, which is an algorithm for estimating the gradient of the cost function in neural networks. Some might say that it is basically a fancy word for the chain rule, which is a means to calculate the partial derivative of functions that depend on more than one variable. Nonetheless, it is a method that helped bring the field of artificial neural networks back to life, so we should be thankful for that.</p>
<p>Understanding backpropagation involves quite a bit of calculus, so I will only give you a brief introduction here.</p>
<p>Let's remind ourselves that the cost function, and therefore its gradient, depends on the difference between the true output (<em>y<sub>i</sub></em>) and the current output (<span><em>ŷ<sub>i</sub></em></span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing a MLP in OpenCV</h1>
                </header>
            
            <article>
                
<p>Implementing an MLP in OpenCV uses the same syntax that we have seen at least a dozen times before. In order to see how an MLP compares to a single perceptron, we will operate on the same toy data as before:</p>
<pre>In [1]: from sklearn.datasets.samples_generator import make_blobs<br/>...     X_raw, y_raw = make_blobs(n_samples=100, centers=2,<br/>...                               cluster_std=5.2, random_state=42)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing the data</h1>
                </header>
            
            <article>
                
<p>However, since we are working with OpenCV, this time we want to make sure the input matrix is made up of 32-bit floating point numbers, otherwise the code will break:</p>
<pre>In [2]: import numpy as np... X = X_raw.astype(np.float32)</pre>
<p>Furthermore, we need to think back to <a href="142fec63-a847-4cde-9de9-c34805d2bb84.xhtml" target="_blank">Chapter 4</a>, <em>Representing Data and Engineering Features</em>, and remember how to represent categorical variables. We need to find a way to represent target labels, not as integers but with a one-hot encoding. The easiest way to achieve this is by using scikit-learn's <kbd>preprocessing</kbd> module:</p>
<pre>In [3]: from sklearn.preprocessing import OneHotEncoder...     enc = OneHotEncoder(sparse=False, dtype=np.float32)...     y = enc.fit_transform(y_raw.reshape(-1, 1))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating an MLP classifier in OpenCV</h1>
                </header>
            
            <article>
                
<p>The syntax to create an MLP in OpenCV is the same as for all the other classifiers:</p>
<pre>In [4]: import cv2<br/>...     mlp = cv2.ml.ANN_MLP_create()</pre>
<p>However, now we need to specify how many layers we want in the network and how many neurons there are per layer. We do this with a list of integers, which specify the number of neurons in each layer. Since the data matrix <kbd>X</kbd> has two features, the first layer should also have two neurons in it <span>(</span><kbd>n_input</kbd><span>)</span><span>. Since the output has two different values, the last layer should also have two neurons in it (<kbd>n_output</kbd>).</span></p>
<p><span>In between these two layers, we can put as many hidden layers with as many neurons as we want. Let's choose a single hidden layer with an arbitrary number of 10 neurons in it (<kbd>n_hidden</kbd>):</span></p>
<pre>In [5]: n_input = 2<br/>...     n_hidden = 10<br/>...     n_output = 2<br/>...     mlp.setLayerSizes(np.array([n_input, n_hidden, n_output]))</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Customizing the MLP classifier</h1>
                </header>
            
            <article>
                
<p>Before we move on to training the classifier, we can customize the MLP classifier via a number of optional settings:</p>
<ul>
<li><kbd>mlp.setActivationFunction</kbd><span>: This defines the</span> <span>activation function</span> <span>to be used for every neuron in the network.</span></li>
<li><kbd>mlp.setTrainMethod</kbd>: This defines a suitable <span>training method.</span></li>
<li><kbd>mlp.setTermCriteria</kbd>: This sets the <span>termination criteria</span> of the training phase.</li>
</ul>
<p>Whereas our home-brewed perceptron classifier used a linear activation function, OpenCV provides two additional options:</p>
<ul>
<li><kbd>cv2.ml.ANN_MLP_IDENTITY</kbd>: This is the linear activation function, <em>f(x) = x</em>.</li>
<li><kbd>cv2.ml.ANN_MLP_SIGMOID_SYM</kbd>: This is the symmetrical sigmoid function (also known as <strong>hyperbolic tangent</strong>), <em>f(x) = β (1 -</em> exp<em>(-α x)) / (1 +</em> exp<em>(-α x))</em>. Whereas ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training and testing the MLP classifier</h1>
                </header>
            
            <article>
                
<p>This is the easy part. Training the MLP classifier is the same as with all other classifiers:</p>
<pre>In [11]: mlp.train(X, cv2.ml.ROW_SAMPLE, y)<br/>Out[11]: True</pre>
<p>The same goes for predicting target labels:</p>
<pre>In [12]: _, y_hat = mlp.predict(X)</pre>
<p>The easiest way to measure accuracy is by using scikit-learn's helper function:</p>
<pre>In [13]: from sklearn.metrics import accuracy_score<br/>...      accuracy_score(y_hat.round(), y)<br/>Out[13]: 0.88</pre>
<p>It looks like we were able to increase our performance from 81% with a single perceptron to 88% with an MLP consisting of 10 hidden-layer neurons and 2 output neurons. In order to see what changed, we can look at the decision boundary one more time:</p>
<pre>In [14]: def plot_decision_boundary(classifier, X_test, y_test):<br/>... # create a mesh to plot in<br/>... h = 0.02 # step size in mesh<br/>... x_min, x_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1<br/>... y_min, y_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1<br/>... xx, yy = np.meshgrid(np.arange(x_min, x_max, h),<br/>... np.arange(y_min, y_max, h))<br/>... <br/>... X_hypo = np.c_[xx.ravel().astype(np.float32),<br/>... yy.ravel().astype(np.float32)]<br/>... _, zz = classifier.predict(X_hypo)</pre>
<p>However, there is a problem right here, in that <kbd>zz</kbd> is now a one-hot encoded matrix. In order to transform the one-hot encoding into a number that corresponds to the class label (zero or one), we can use NumPy's <kbd>argmax</kbd> function:</p>
<pre>...          zz = np.argmax(zz, axis=1)</pre>
<p>Then the rest stays the same:</p>
<pre>...          zz = zz.reshape(xx.shape)<br/>...          plt.contourf(xx, yy, zz, cmap=plt.cm.coolwarm, alpha=0.8)<br/>...          plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=200)</pre>
<p>Then we can call the function like this:</p>
<pre>In [15]: plot_decision_boundary(mlp, X, y_raw)</pre>
<p>The output looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-987 image-border" src="Images/7430fc52-df88-490d-972d-4d56c4ee0eb3.png" style="width:35.50em;height:21.33em;" width="1204" height="722"/></p>
<p>The preceding output shows a decision boundary of an MLP with one hidden layer.</p>
<p>And voila! The decision boundary is no longer a straight line. That being said, you got a great performance increase and might have expected a more drastic performance increase. But nobody said we have to stop here!</p>
<p>There are at least two different things we can try from here on out:</p>
<ul>
<li>We can add more neurons to the hidden layer. You can do this by replacing <kbd>n_hidden</kbd> on line 6 with a larger value and running the code again. Generally speaking, the more neurons you put in the network, the more powerful the MLP will be.</li>
<li>We can add more hidden layers. It turns out that this is where neural networks really get their power from.</li>
</ul>
<p>Hence, this is where I should tell you about deep learning.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting acquainted with deep learning</h1>
                </header>
            
            <article>
                
<p>Back when deep learning didn't have a fancy name yet, it was called artificial neural networks. So you already know a great deal about it! </p>


<p>Eventually, interest in neural networks was rekindled in 1986 when David Rumelhart, Geoffrey Hinton, and Ronald Williams were involved in the (re)discovery and popularization of the aforementioned backpropagation algorithm. However, it was not until recently that computers became powerful enough so they could actually execute the backpropagation algorithm on large-scale networks, leading to a surge in deep learning research.</p>
<div class="packt_infobox">You can find more information on the history and origin of deep learning in the following scientific article: Wang and Raj (2017), <em>On the Origin ...</em></div></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting acquainted with Keras</h1>
                </header>
            
            <article>
                
<p>The core data structure of Keras is a model, which is similar to OpenCV's classifier object, except it focuses on neural networks only. <span>The simplest type of model is the</span> sequential <span>model, which arranges the different layers of the neural network in a linear stack, just like we did for the MLP in OpenCV:</span></p>
<pre>In [1]: from keras.models import Sequential<br/>...     model = Sequential()<br/>Out[1]: Using TensorFlow backend.</pre>
<p>Then, different layers can be added to the model one by one. In Keras, layers do not just contain neurons, they also perform a function. Some core layer types include the following:</p>
<ul>
<li><strong>Dense</strong>: This is a densely connected layer. This is exactly what we used when we designed our MLP: a layer of neurons that is connected to every neuron in the previous layer.</li>
<li><strong>Activation</strong>: This applies an activation function to an output. Keras provides a whole range of activation functions, including OpenCV's identify function (<kbd>linear</kbd>), the hyperbolic tangent (<kbd>tanh</kbd>), a sigmoidal squashing function (<kbd>sigmoid</kbd>), a softmax function (<kbd>softmax</kbd>), and many more.</li>
<li><strong>Reshape</strong>: This reshapes an output to a certain shape.</li>
</ul>
<p>There are other layers that calculate arithmetic or geometric operations on their inputs:</p>
<ul>
<li><strong>Convolutional layers</strong>: These layers allow you to specify a kernel with which the input layer is convolved. This allows you to perform operations such as a Sobel filter or apply a Gaussian kernel in 1D, 2D, or even 3D.</li>
<li><strong>Pooling layers</strong>: These layers perform a max pooling operation on their input, where the output neuron's activity is given by the maximally active input neuron.</li>
</ul>
<p>Some other layers that are popular in deep learning are as follows:</p>
<ul>
<li><strong>Dropout</strong>: This layer randomly sets a fraction of input units to zero at each update. This is a way to inject noise into the training process, making it more robust.</li>
<li><strong>Embeddin</strong><strong>g</strong>: This layer encodes categorical data, similar to some functions from scikit-learn's <kbd>preprocessing</kbd> module.</li>
<li><strong>Gaussian noise</strong>: This layer applies additive zero-centered Gaussian noise. This is another way of injecting noise into the training process, making it more robust.</li>
</ul>
<p>A perceptron similar to the preceding one could thus be implemented using a dense layer that has two inputs and one output. Staying true to our earlier example, we will initialize the weights to zero and use the hyperbolic tangent as an activation function:</p>
<pre style="font-size: 16px">In [2]: from keras.layers import Dense<br/>...     model.add(Dense(1, activation='tanh', input_dim=2,<br/>...                     kernel_initializer='zeros'))</pre>
<p>Finally, we want to specify the training method. Keras provides a number of optimizers, including the following:</p>
<ul>
<li><strong>Stochastic gradient descent (SGD)</strong>: This is what we discussed earlier.</li>
<li><strong>Root mean square propagation (RMSprop)</strong>: This is a method in which the learning rate is adapted for each of the parameters.</li>
<li><strong>Adaptive moment estimation (Adam)</strong>: This is an update to the root mean square propagation.</li>
</ul>
<p>In addition, Keras also provides a number of different loss functions:</p>
<ul>
<li><strong>Mean squared error (mean_squared_error)</strong>: This is what was discussed earlier.</li>
<li><strong>Hinge loss (hinge)</strong>: This is a maximum-margin classifier often used with SVM, as discussed in <a href="419719a8-3340-483a-86be-1d9b94f4a682.xhtml" target="_blank">Chapter 6</a>, <em>Detecting Pedestrians with Support Vector Machines</em>.</li>
</ul>
<p class="mce-root"/>
<p>You can see that there's a plethora of parameters to be specified and methods to choose from. To stay true to our aforementioned perceptron implementation, we will choose SGD as an optimizer, the mean squared error as a cost function, and accuracy as a scoring function:</p>
<pre>In [3]: model.compile(optimizer='sgd',<br/>...                   loss='mean_squared_error',<br/>...                   metrics=['accuracy'])</pre>
<p>In order to compare the performance of the Keras implementation to our home-brewed version, we will apply the classifier to the same dataset:</p>
<pre>In [4]: from sklearn.datasets.samples_generator import make_blobs<br/>...     X, y = make_blobs(n_samples=100, centers=2,<br/>...     cluster_std=2.2, random_state=42)</pre>
<p>Finally, a Keras model is fit to the data with a very familiar syntax. Here, we can also choose how many iterations to train for (<kbd>epochs</kbd>), how many samples to present before we calculate the error gradient (<kbd>batch_size</kbd>), whether to shuffle the dataset (<kbd>shuffle</kbd>), and whether to output progress updates (<kbd>verbose</kbd>):</p>
<pre>In [5]: model.fit(X, y, epochs=400, batch_size=100, shuffle=False,<br/>...               verbose=0)</pre>
<p>After the training completes, we can evaluate the classifier as follows:</p>
<pre>In [6]: model.evaluate(X, y)<br/>Out[6]: 32/100 [========&gt;.....................] - ETA: 0s<br/>        [0.040941802412271501, 1.0]</pre>
<p>Here, the first reported value is the mean squared error, whereas the second value denotes accuracy. This means that the final mean squared error was 0.04, and we had 100% accuracy. Way better than our own implementation!</p>
<div class="packt_infobox">You can find more information on Keras, source code documentation, and a number of tutorials at <a href="http://keras.io">http://keras.io</a>.</div>
<p>With these tools in hand, we are now ready to approach a real-world dataset!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying handwritten digits</h1>
                </header>
            
            <article>
                
<p>In the previous section, we covered a lot of the theory around neural networks, which can be a little overwhelming if you are new to this topic. <span><span>In this section, we will use the famous MNIST dataset, which contains 60,000 samples of handwritten digits along with their labels.</span></span></p>
<p>We will train two different networks on it:</p>
<ul>
<li>An MLP using OpenCV</li>
<li>A deep neural network using Keras</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the MNIST dataset</h1>
                </header>
            
            <article>
                
<p>The easiest way to obtain the MNIST dataset is by using Keras:</p>
<pre>In [1]: from keras.datasets import mnist<br/>...     (X_train, y_train), (X_test, y_test) = mnist.load_data()<br/>Out[1]: Using TensorFlow backend.<br/>        Downloading data from<br/>        https://s3.amazonaws.com/img-datasets/mnist.npz</pre>
<p>This will download the data from Amazon Cloud (might take a while depending on your internet connection) and automatically split the data into training and test sets.</p>
<div class="packt_infobox">MNIST provides its own predefined train-test split. This way, it is easier to compare the performance of different classifiers because they will all use the same data for training and the same data for testing.</div>
<p>This data comes in a format that we are already familiar with:</p>
<pre>In [2]: X_train.shape, y_train.shape<br/>Out[2]: ((60000, 28, 28), (60000,))</pre>
<p>We should take note that the labels come as integer values between zero and nine (corresponding to the digits 0-9):</p>
<pre>In [3]: import numpy as np<br/>...     np.unique(y_train)<br/>Out[3]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)</pre>
<p class="mce-root"/>
<p>We can have a look at some example digits:</p>
<pre>In [4]: import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>In [5]: for i in range(10):<br/>...         plt.subplot(2, 5, i + 1)<br/>...         plt.imshow(X_train[i, :, :], cmap='gray')<br/>...         plt.axis('off')</pre>
<p>The digits look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-988 image-border" src="Images/cbf6668b-2d3f-4188-99c7-6363e034ca9f.png" style="width:23.00em;height:9.33em;" width="1282" height="520"/></p>
<p>In fact, the MNIST dataset is the successor to the NIST digits dataset provided by scikit-learn that we used before (<kbd>sklearn.datasets.load_digits</kbd>; refer to <a href="8b9a6f9b-32b4-4af5-9a8c-4e121341f292.xhtml" target="_blank">Chapter 2</a>, <em>Working with Data in OpenCV</em>). Some notable differences are as follows:</p>
<ul>
<li>MNIST images are significantly larger (28 x 28 pixels) than NIST images (8 x 8 pixels), thus paying more attention to fine details such as distortions and individual differences between images of the same digit.</li>
<li>The MNIST dataset is much larger than the NIST dataset, providing 60,000 training and 10,000 test samples (as compared to a total of 5,620 NIST images).</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing the MNIST dataset</h1>
                </header>
            
            <article>
                
<p>As we learned in <a href="142fec63-a847-4cde-9de9-c34805d2bb84.xhtml" target="_blank">Chapter 4</a>, <em>Representing Data and Engineering Features</em>, there are a number of preprocessing steps we might like to apply here:</p>
<ul>
<li><strong>Centering</strong>: It is important that all the digits are centered in the image. For example, take a look at all the example images of the digit 1 in the preceding diagram, which are all made of an almost-vertical strike. If the images were misaligned, the strike could lie anywhere in the image, making it hard for the neural network to find commonalities in the training samples. Fortunately, images in MNIST are already centered.</li>
<li><strong>Scaling</strong>: The same is true for scaling the digits so that they all have the same size. This way, the location of strikes, curves, and loops are important. ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training an MLP using OpenCV</h1>
                </header>
            
            <article>
                
<p>We can set up and train an MLP in OpenCV with the following recipe:</p>
<ol>
<li>Instantiate a new MLP object:</li>
</ol>
<pre style="padding-left: 60px">In [9]: import cv2<br/>...     mlp = cv2.ml.ANN_MLP_create()</pre>
<p class="mce-root"/>
<ol start="2">
<li>Specify the size of every layer in the network. We are free to add as many layers as we want, but we need to make sure that the first layer has the same number of neurons as input features (<kbd>784</kbd> in our case), and that the last layer has the same number of neurons as class labels (<kbd>10</kbd> in our case), while there are two hidden layers each having <kbd>512</kbd> nodes:</li>
</ol>
<pre style="padding-left: 60px">In [10]: mlp.setLayerSizes(np.array([784, 512, 512, 10]))</pre>
<ol start="3">
<li>Specify an activation function. Here we use the sigmoidal activation function from before:</li>
</ol>
<pre style="padding-left: 60px">In [11]: mlp.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM,<br/>      ...                                2.5, 1.0)</pre>
<ol start="4">
<li>Specify the training method. Here, we use the backpropagation algorithm described earlier. We also need to make sure that we choose a small enough learning rate. Since we have on the order of 10<sup>5</sup> training samples, it is a good idea to set the learning rate to at most 10<sup>-5</sup>:</li>
</ol>
<pre style="padding-left: 60px">In [12]: mlp.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP)<br/>...      mlp.setBackpropWeightScale(0.00001)</pre>
<ol start="5">
<li>Specify the termination criteria. Here, we use the same criteria as before: to run training for 10 iterations (<kbd>term_max_iter</kbd>) or until the error does no longer increase significantly (<kbd>term_eps</kbd>):</li>
</ol>
<pre style="padding-left: 60px">In [13]: term_mode = (cv2.TERM_CRITERIA_MAX_ITER + <br/>...                   cv2.TERM_CRITERIA_EPS)<br/>...      term_max_iter = 10<br/>...      term_eps = 0.01<br/>...      mlp.setTermCriteria((term_mode, term_max_iter,<br/>...                           term_eps))</pre>
<ol start="6">
<li>Train the network on the training set (<kbd>X_train_pre</kbd>):</li>
</ol>
<pre style="padding-left: 60px">In [14]: mlp.train(X_train_pre, cv2.ml.ROW_SAMPLE, y_train_pre)<br/>Out[14]: True</pre>
<div class="packt_tip">Before you call <kbd>mlp.train</kbd>, here is a word of caution: this might take several hours to run, depending on your computer setup! For comparison, it took just under an hour on my own laptop. We are now dealing with a real-world dataset of 60,000 samples: if we run 100 training epochs, we have to compute 6 million gradients! So beware.</div>
<p>When the training completes, we can calculate the accuracy score on the training set to see how far we got:</p>
<pre>In [15]: _, y_hat_train = mlp.predict(X_train_pre)<br/>In [16]: from sklearn.metrics import accuracy_score<br/>...      accuracy_score(y_hat_train.round(), y_train_pre)<br/>Out[16]: 0.92976666666666663</pre>
<p>But, of course, what really counts is the accuracy score we get on the held-out test data which was not taken into account in training process:</p>
<pre>In [17]: _, y_hat_test = mlp.predict(X_test_pre)<br/>...      accuracy_score(y_hat_test.round(), y_test_pre)<br/>Out[17]: 0.91690000000000005</pre>
<p>91.7% accuracy is not bad at all if you ask me! The first thing you should try is to change the layer sizes in the preceding <kbd>In [10]</kbd> and see how the test score changes. As you add more neurons to the network, you should see the training score increase—and with it, hopefully, the test score. However, having <em>N</em> neurons in a single layer is not the same as having them spread out over several layers! Can you confirm this observation?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training a deep neural network using Keras</h1>
                </header>
            
            <article>
                
<p>Although we achieved a formidable score with the preceding MLP, our result does not hold up to state-of-the-art results. Currently, the best result has close to 99.8% accuracy—better than human performance! This is why, nowadays, the task of classifying handwritten digits is largely regarded as solved.</p>
<p>To get closer to the state-of-the-art results, we need to use state-of-the-art techniques. Thus, we return to Keras.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Preprocessing the MNIST dataset</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will learn to preprocess the data before it is fed to the neural network:</p>
<ol>
<li>To make sure we get the same result every time we run the experiment, we will pick a random seed for NumPy's random number generator. This way, shuffling the training samples from the MNIST dataset will always result in the same order:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import numpy as np<br/>...     np.random.seed(1337)</pre>
<ol start="2">
<li>Keras provides a loading function similar to <kbd>train_test_split</kbd> from scikit-learn's <kbd>model_selection</kbd> module. Its syntax might look strangely familiar to you:</li>
</ol>
<pre style="padding-left: 60px">In [2]: from keras.datasets import mnist<br/>...     (X_train, y_train), (X_test, y_test) = mnist.load_data()</pre>
<div class="packt_infobox">In contrast to other datasets we have encountered so far, MNIST comes with a predefined train-test split. This allows the dataset to be used as a benchmark, as the test score reported by different algorithms will always apply to the same test samples.</div>
<ol start="3">
<li>The neural networks in Keras act on the feature matrix slightly differently than the standard OpenCV and scikit-learn estimators. Whereas the rows of a feature matrix in Keras still correspond to the number of samples (<kbd>X_train.shape[0]</kbd> in the following code), we can preserve the two-dimensional nature of the input images by adding more dimensions to the feature matrix:</li>
</ol>
<pre style="padding-left: 60px">In [3]: img_rows, img_cols = 28, 28<br/>...     X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)<br/>...     X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)<br/>...     input_shape = (img_rows, img_cols, 1)</pre>
<ol start="4">
<li>Here, we have reshaped the feature matrix into a four-dimensional matrix with dimensions <kbd>n_features</kbd> x 28 x 28 x 1. We also need to make sure we operate on 32-bit floating point numbers between [0, 1], rather than unsigned integers in [0, 255]:</li>
</ol>
<pre style="padding-left: 60px">...     X_train = X_train.astype('float32') / 255.0<br/>...     X_test = X_test.astype('float32') / 255.0</pre>
<ol start="5">
<li>Then, we can one-hot encode the training labels as we did before. This will make sure each category of target labels can be assigned to a neuron in the output layer. We could do this with scikit-learn's <kbd>preprocessing</kbd>, but in this case, it is easier to use Keras' own utility function:</li>
</ol>
<pre style="padding-left: 60px">In [4]: from keras.utils import np_utils<br/>...     n_classes = 10<br/>...     Y_train = np_utils.to_categorical(y_train, n_classes)<br/>...     Y_test = np_utils.to_categorical(y_test, n_classes)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating a convolutional neural network</h1>
                </header>
            
            <article>
                
<p>In the following steps, you will create a neural network and train on the data you preprocessed earlier:</p>
<ol>
<li>Once we have preprocessed the data, it is time to define the actual model. Here, we will once again rely on the <kbd>Sequential</kbd> model to define a feedforward neural network:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">In [5]: from keras.model import Sequential... model = Sequential()</pre>
<ol start="2">
<li>However, this time, we will be smarter about the individual layers. We will design our neural network around a convolutional layer, where the kernel is a 3 x 3-pixel two-dimensional convolution:</li>
</ol>
<pre style="padding-left: 60px">In [6]: from keras.layers import Convolution2D...     n_filters = 32...     kernel_size = (3, 3)...     model.add(Convolution2D(n_filters, kernel_size[0], kernel_size[1],... border_mode='valid', ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Model summary</h1>
                </header>
            
            <article>
                
<p>You can also visualize the model's summary, which will list all the layers along with their respective dimensions and the number of weights each layer consists. It will also provide you with information about the total number of parameters (weights and biases) in your network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-989 image-border" src="Images/d014b7f0-d32b-4887-a660-1828cfc3e1cc.png" style="width:33.67em;height:31.92em;" width="1135" height="1076"/></p>
<p>We can see that there are in total 600,810 parameters that will be trained and will require a good amount of computation power! Please note that how we calculate the number of parameters in each layer is out of the scope of this book.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fitting the model</h1>
                </header>
            
            <article>
                
<p>We fit the model as we do with all other classifiers (caution, this might take a while):</p>
<pre style="padding-left: 60px">In [12]: model.fit(X_train, Y_train, batch_size=128, nb_epoch=12,...                verbose=1, validation_data=(X_test, Y_test))</pre>






<p>After training completes, we can evaluate the classifier:</p>
<pre style="padding-left: 60px" class="mce-root">In [13]: model.evaluate(X_test, Y_test, verbose=0)Out[13]: 0.99</pre>
<p>And we achieved 99% accuracy! This is worlds apart from the MLP classifier we implemented before. And this is just one way to do things. As you can see, neural networks provide a plethora of tuning parameters, and it is not at all clear which ones will lead to the best performance.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we added a whole bunch of skills to our list as a machine learning practitioner. Not only did we cover the basics of artificial neural networks, including perceptrons and MLPs, we also got our hands on some advanced deep learning software. We learned how to build a simple perceptron from scratch and how to build state-of-the-art networks using Keras. Furthermore, we learned about all the details of neural nets: activation functions, loss functions, layer types, and training methods. All in all, this was probably the most intensive chapter yet.</p>
<p>Now that you know about most of the essential supervised learners, it is time to talk about how to combine different algorithms into a more powerful one. Thus, in the next chapter, we will talk about how to build ensemble classifiers.</p>


            </article>

            
        </section>
    </div>



  </body></html>