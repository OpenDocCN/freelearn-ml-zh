- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Local Model-Agnostic Interpretation Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 局部模型无关解释方法
- en: In the previous two chapters, we dealt exclusively with global interpretation
    methods. This chapter will foray into local interpretation methods, which are
    there to explain why a single prediction or a group of predictions was made. It
    will cover how to leverage **SHapley Additive exPlanations** (**SHAP’s**) `KernelExplainer`
    and also another method called **Local Interpretable Model-agnostic Explanations**
    (**LIME**) for local interpretations. We will also explore how to use these methods
    with both tabular and text data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们专门讨论了全局解释方法。本章将探讨局部解释方法，这些方法旨在解释为什么做出了单个预测或一系列预测。它将涵盖如何利用**SHapley Additive
    exPlanations**（**SHAP**）的`KernelExplainer`以及另一种称为**Local Interpretable Model-agnostic
    Explanations**（**LIME**）的局部解释方法。我们还将探讨如何使用这些方法处理表格和文本数据。
- en: 'These are the main topics we are going to cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将在本章中讨论的主要主题：
- en: Leveraging SHAP’s `KernelExplainer` for local interpretations with SHAP values
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用SHAP的`KernelExplainer`进行局部解释的SHAP值
- en: Employing LIME
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LIME
- en: Using LIME for **Natural Language Processing** (**NLP**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LIME进行**自然语言处理**（**NLP**）
- en: Trying SHAP for NLP
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试SHAP进行NLP
- en: Comparing SHAP with LIME
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较SHAP与LIME
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `pandas`, `numpy`, `sklearn`,
    `nltk`, `lightgbm`, `rulefit`, `matplotlib`, `seaborn`, `shap`, and `lime` libraries.
    Instructions on how to install all of these libraries are in the *Preface* of
    the book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`pandas`、`numpy`、`sklearn`、`nltk`、`lightgbm`、`rulefit`、`matplotlib`、`seaborn`、`shap`和`lime`库。如何安装所有这些库的说明在书的**前言**中。
- en: 'The code for this chapter is located here: [https://packt.link/SRqJp](https://packt.link/SRqJp).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/SRqJp](https://packt.link/SRqJp).
- en: The mission
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: Who doesn’t love chocolate?! It’s a global favorite, with around nine out of
    ten people loving it and about a billion people eating it every day. One popular
    form in which it is consumed is a chocolate bar. However, even universally beloved
    ingredients can be used in ways that aren’t universally appealing—so, chocolate
    bars can range from the sublime to the mediocre to the downright unpleasant.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 谁不喜欢巧克力？！它是全球最受欢迎的食品，大约有九成的人喜欢它，每天大约有十亿人食用它。它的一种流行形式是巧克力棒。然而，即使是普遍受欢迎的成分也可以以不普遍吸引人的方式使用——因此，巧克力棒的质量可以从极好到平庸，甚至到令人不快的程度。
- en: Often, this is solely determined by the quality of the cocoa or additional ingredients,
    and sometimes it becomes an acquired taste once it’s combined with exotic flavors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这完全取决于可可豆或额外成分的质量，有时一旦与异国风味结合，它就会变成一种习得的口味。
- en: A French chocolate manufacturer obsessed with excellence has reached out to
    you. They have a problem. All of their bars have been highly rated by critics,
    yet critics have very particular taste buds. And some bars they love have inexplicably
    mediocre sales, but non-critics seem to like them in focus groups and tastings,
    so they are puzzled why sales don’t coincide with their market research. They
    have found a dataset of chocolate bars rated by knowledgeable lovers of chocolate,
    and these ratings happen to coincide with their sales. To get an unbiased opinion,
    they have sought your expertise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一家法国巧克力制造商对卓越的追求，向你伸出了援手。他们遇到了一个问题。他们的所有巧克力棒都得到了评论家的好评，但评论家的口味非常独特。有些他们喜欢的巧克力棒销售却出奇地平庸，但非评论家在焦点小组和品鉴中似乎都喜欢它们，所以他们困惑为什么销售没有与他们的市场调研相符。他们找到了一组由巧克力爱好者评级的巧克力棒数据集，这些评级恰好与他们的销售情况相符。为了获得无偏见的意见，他们寻求了你的专业知识。
- en: 'As for the dataset, members of the *Manhattan Chocolate Society* have been
    meeting since 2007 for the sole purpose of tasting and judging fine chocolate
    to educate consumers and inspire chocolate makers to produce higher-quality chocolate.
    Since then, they have compiled a dataset of over 2,200 chocolate bars, rated by
    their members with the following scale:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集，*曼哈顿巧克力协会*的成员自2007年以来一直在聚会，唯一的目的就是品尝和评判优质巧克力，以教育消费者并激励巧克力制造商生产更高品质的巧克力。从那时起，他们已经汇编了一个包含2200多块巧克力棒的数据集，其成员按照以下标准进行评级：
- en: 4.0–5.00 = *Outstanding*
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4.0–5.00 = *杰出*
- en: 3.5–3.99 = *Highly Recommended*
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3.5–3.99 = *强烈推荐*
- en: 3.0–3.49 = *Recommended*
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3.0–3.49 = *推荐*
- en: 2.0–2.99 = *Disappointing*
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2.0–2.99 = *令人失望*
- en: 1.0–1.90 = *Unpleasant*
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1.0–1.90 = *不愉快*
- en: These ratings are derived from a rubric that factors in aroma, appearance, texture,
    flavor, aftertaste, and overall opinion, and the bars rated are mostly darker
    chocolate bars since the aim is to appreciate the flavors of cacao. In addition
    to the ratings, the *Manhattan Chocolate Society* dataset includes many characteristics,
    such as the country where the cocoa bean was farmed, how many ingredients the
    bar has, whether it includes salt, and the words used to describe it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评分是根据一个考虑了香气、外观、质地、风味、回味和整体意见的评分标准得出的，而且被评为的巧克力条大多是较深的巧克力条，因为目的是欣赏可可的风味。除了评分外，*曼哈顿巧克力协会*数据集还包括许多特征，例如可可豆种植的国家、巧克力条有多少种成分、是否包含盐以及描述它的单词。
- en: The goal is to understand why one of the chocolate manufacturers’ bars is rated
    *Outstanding* yet sells poorly, while another one, whose sales are impressive,
    is rated as *Disappointing*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是理解为什么某个巧克力制造商的巧克力条被评为*杰出*但销量不佳，而另一个销量令人印象深刻，但被评为*令人失望*的巧克力条。
- en: The approach
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'You have decided to use local model interpretation to explain why each bar
    is rated as it is. To that end, you will prepare the dataset and then train classification
    models to predict if chocolate bar ratings are above or equal to *Highly Recommended*,
    because the client would like all their bars to fall above this threshold. You
    will need to train two models: one for tabular data, and another NLP one for the
    words used to describe the chocolate bars. We will employ **Support Vector Machines**
    (**SVMs**) and **Light Gradient Boosting Machine** (**LightGBM**), respectively,
    for these tasks. If you haven’t used these black-box models, no worries—we will
    briefly explain them. Once you have trained the models, then comes the fun part:
    leveraging two local model-agnostic interpretation methods to understand what
    makes a specific chocolate bar *Highly Recommended* or not.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经决定使用本地模型解释来解释为什么每块巧克力条被评为这样。为此，您将准备数据集，然后训练分类模型来预测巧克力条评分是否高于或等于*强烈推荐*，因为客户希望所有巧克力条都高于这个阈值。您需要训练两个模型：一个用于表格数据，另一个用于描述巧克力条的单词的NLP模型。我们将分别使用**支持向量机**（**SVMs**）和**轻梯度提升机**（**LightGBM**）来完成这些任务。如果您还没有使用这些黑盒模型，请不要担心——我们将简要解释它们。一旦您训练了模型，接下来就是有趣的部分：利用两种本地模型无关的解释方法来了解是什么让特定的巧克力条被评为*强烈推荐*或不是。
- en: These explanation methods are SHAP and LIME, which when combined will provide
    a richer explanation to convey back to your client. Then, we will compare both
    methods to understand their strengths and limitations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些解释方法包括SHAP和LIME，当它们结合使用时，将为您的客户提供更丰富的解释。然后，我们将比较这两种方法，以了解它们的优点和局限性。
- en: The preparations
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/05/ChocoRatings.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/05/ChocoRatings.ipynb)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在此处找到此示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/05/ChocoRatings.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/05/ChocoRatings.ipynb)
- en: Loading the libraries
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mldatasets`来加载数据集'
- en: '`pandas`, `numpy`, and `nltk` to manipulate it'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`、`numpy`和`nltk`来操作它'
- en: '`sklearn` (scikit-learn) and `lightgbm` to split the data and fit the models'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`（scikit-learn）和`lightgbm`来分割数据和拟合模型'
- en: '`matplotlib`, `seaborn`, `shap`, and `lime` to visualize the interpretations'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`、`seaborn`、`shap`和`lime`来可视化解释'
- en: 'You should load all of them first, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该首先加载所有这些库，如下所示：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Understanding and preparing the data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We load the data into a DataFrame we call `chocolateratings_df`, like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据加载到我们称之为`chocolateratings_df`的DataFrame中，如下所示：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There should be over 2,200 records and 18 columns. We can verify this was the
    case simply by inspecting the contents of the DataFrame, like this:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 应该有超过2,200条记录和18列。我们可以简单地通过检查DataFrame的内容来验证这一点，如下所示：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output shown here in *Figure 5.1* corresponds to what we were expecting:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里在*图5.1*中显示的输出与我们预期的相符：
- en: '![Table  Description automatically generated](img/B18406_05_01.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_05_01.png)'
- en: 'Figure 5.1: Contents of the chocolate bar dataset'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：巧克力条数据集的内容
- en: The data dictionary
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据字典
- en: 'The data dictionary comprises the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据字典包括以下内容：
- en: '`company`: Categorical; the manufacturer of the chocolate bar (out of over
    500 different ones)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`company`: 分类型；巧克力条的制造商（超过500种不同类型）'
- en: '`company_location`: Categorical; the country of the manufacturer (66 different
    countries)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`company_location`: 分类型；制造商所在的国家（66个不同的国家）'
- en: '`review_date`: Continuous; the year in which the bar was reviewed (from 2006
    to 2020)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`review_date`: 连续型；评价巧克力条的那一年（从2006年到2020年）'
- en: '`country_of_bean_origin`: Categorical; the country where the cocoa beans were
    harvested (62 different countries)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`country_of_bean_origin`: 分类型；可可豆收获的国家（62个不同的国家）'
- en: '`cocoa_percent`: Categorical; what percentage of the bar is cocoa'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cocoa_percent`: 分类型；巧克力条中可可的百分比'
- en: '`rating`: Continuous; the rating given by the *Manhattan Chocolate Society*
    (possible values: 1–5)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rating`: 连续型；由曼哈顿巧克力协会（*Manhattan Chocolate Society*）给出的评分（可能值为1–5）'
- en: '`counts_of_ingredients`: Continuous; the amount of ingredients in the bar'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`counts_of_ingredients`: 连续型；巧克力条中成分的量'
- en: '`cocoa_butter`: Binary; was it made with cocoa butter?'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cocoa_butter`: 二元型；是否使用了可可脂？'
- en: '`vanilla`: Binary; was it made with vanilla?'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vanilla`: 二元型；是否使用了香草？'
- en: '`lecithin`: Binary; was it made with lecithin?'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lecithin`: 二元型；是否使用了卵磷脂？'
- en: '`salt`: Binary; was it made with salt?'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`salt`: 二元型；是否使用了盐？'
- en: '`sugar`: Binary; was it made with sugar?'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sugar`: 二元型；是否使用了糖？'
- en: '`sweetener_without_sugar`: Binary; was it made with sweetener without sugar?'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sweetener_without_sugar`: 二元型；是否使用了无糖甜味剂？'
- en: '`first_taste`: Text; word(s) used to describe the first taste'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`first_taste`: 文本；用于描述第一次品尝的词语'
- en: '`second_taste`: Text; word(s) used to describe the second taste'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`second_taste`: 文本；用于描述第二次品尝的词语'
- en: '`third_taste`: Text; word(s) used to describe the third taste'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`third_taste`: 文本；用于描述第三次品尝的词语'
- en: '`fourth_taste`: Text; word(s) used to describe the fourth taste'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fourth_taste`: 文本；用于描述第四次品尝的词语'
- en: Now that we have taken a peek at the data, we can quickly prepare it and then
    work on the modeling and interpretation!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经浏览了数据，我们可以快速准备它，然后进行建模和解释！
- en: Data preparation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'The first thing we ought to do is set aside the text features so that we can
    process them separately. We can start by creating a DataFrame named `tastes_df`
    containing text features and then drop them from `chocolateratings_df`. We can
    then explore `tastes_df` using `head` and `tail`, as illustrated in the following
    code snippet:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该做的是将文本特征留出，这样我们就可以单独处理它们。我们可以通过创建一个名为`tastes_df`的数据框来包含文本特征，然后从`chocolateratings_df`中删除它们。然后，我们可以使用`head`和`tail`来探索`tastes_df`，如下面的代码片段所示：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code produces the DataFrame shown here in *Figure 5.2*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了这里在*图5.2*中显示的数据框：
- en: '![Table  Description automatically generated](img/B18406_05_02.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_05_02.png)'
- en: 'Figure 5.2: The taste columns have quite a few null values'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：品尝列中有许多空值
- en: 'Now, let’s categorically encode the categorical features. There are too many
    countries in `company_location` and `country_of_bean_origin`, so let’s establish
    a threshold. If, say, there are fewer than 3.333% (or 74 rows) for any country,
    let’s bucket them into an `Other` category and then encode the categories. We
    can easily do this with the `make_dummies_with_limits` function and the process
    is shown again in the following code snippet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对分类型特征进行分类编码。`company_location`和`country_of_bean_origin`中有太多的国家，因此我们设定一个阈值。例如，如果任何国家的数量少于3.333%（或74行），我们就将它们归入一个`Other`类别，然后对类别进行编码。我们可以使用`make_dummies_with_limits`函数轻松完成此操作，以下代码片段展示了该过程：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, to process the content of `tastes_df`, the following code replaces all
    the null values with empty strings, then joins all the columns in `tastes_df`
    together, forming a single series. Then, it strips leading and trailing whitespace.
    The code is illustrated in the following snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了处理`tastes_df`的内容，以下代码将所有空值替换为空字符串，然后将`tastes_df`中的所有列连接起来，形成一个单一的序列。然后，它删除了前导和尾随空格。以下代码片段展示了该过程：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And voilà! You can verify that the result is a `pandas` series (`tastes_s`)
    with (mostly) taste-related adjectives by printing it. As expected, this series
    is the same length as the `chocolateratings_df` DataFrame, as illustrated in the
    following output:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！你可以验证结果是一个`pandas`系列（`tastes_s`），其中包含（主要是）与品尝相关的形容词，通过打印它来验证。正如预期的那样，这个系列与`chocolateratings_df`数据框的长度相同，如下面的输出所示：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: But let’s find out how many of its phrases are unique, with `print(np.unique(tastes_s).shape)`.
    Since the output is (`2178`,), that means fewer than 50 phrases are duplicated,
    so tokenizing by phrases would be a bad idea since so few of them are repeated.
    After all, when tokenizing, we want the elements to repeat enough times to make
    it worthwhile.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们先找出它的短语中有多少是唯一的，使用`print(np.unique(tastes_s).shape)`。由于输出是（2178，），这意味着少于50个短语是重复的，所以按短语进行分词会是一个糟糕的主意，因为其中很少重复。毕竟，在分词时，我们希望元素重复足够多次，这样才值得。
- en: There are many approaches you could take here, such as tokenizing by bi-grams
    (sequences of two words) or even subwords (dividing words into logical parts).
    However, even though order matters slightly (because the first words are related
    to the first taste, and so on), our dataset is too small and had too many nulls
    (especially in `third taste` and `fourth taste`) to derive meaning from the order.
    This is why it was a good choice to concatenate all the “tastes” together, thus
    removing their discernible division.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以采取许多方法，例如按二元组（两个词的序列）或甚至子词（将词分成逻辑部分）进行分词。然而，尽管顺序略微重要（因为第一个词与第一个口味有关，依此类推），但我们的数据集太小，有太多的空值（特别是在`第三口味`和`第四口味`中），无法从顺序中提取意义。这就是为什么将所有“口味”连接起来是一个好的选择，从而消除了它们可辨别的分隔。
- en: Another thing to note is that our words are (mostly) adjectives such as “fruity”
    and “nutty”. We made a small effort to remove adverbs such as “sweetly”, but there
    are still some nouns present, such as “fruit” and “nuts”, versus adjectives such
    as “fruity” and “nutty”. We can’t be sure if the chocolate connoisseurs who judged
    the bars meant something different by using “fruit” rather than “fruity”. However,
    if we were sure of this, we could have performed **stemming** or **lemmatization**
    to turn all instances of “fruit”, “fruity”, and “fruitiness” into a consistent
    “fru” (*stem*) or “fruiti” (*lemma*). We won’t concern ourselves with this because
    many of our adjectives’ variations are not as common in the phrases anyway.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点需要注意的是，我们的词（大多是）形容词，如“fruity”和“nutty”。我们做了一些努力来移除副词，如“sweetly”，但仍然有一些名词存在，如“fruit”和“nuts”，与形容词“fruity”和“nutty”相对。我们无法确定评鉴巧克力条的品酒家使用“fruit”而不是“fruity”是否意味着不同的东西。然而，如果我们确定这一点，我们可以执行**词干提取**或**词形还原**，将“fruit”、“fruity”和“fruitiness”的所有实例转换为一致的“fru”（*词干*）或“fruiti”（*词形*）。我们不会关注这一点，因为我们的许多形容词的变化在短语中并不常见。
- en: 'Let’s find out the most common words by first tokenizing them with `word_tokenize`
    and using `FreqDist` to count their frequency. We can then place the resulting
    `tastewords_fdist` dictionary into a DataFrame (`tastewords_df`). We can save
    only those words with more than 74 instances as a list (`commontastes_l`). The
    code is illustrated in the following snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先通过`word_tokenize`对它们进行分词，并使用`FreqDist`来计算它们的频率，找出最常见的词。然后，我们可以将结果`tastewords_fdist`字典放入DataFrame（`tastewords_df`）。我们可以将出现次数超过74次的单词保存为列表（`commontastes_l`）。代码如下所示：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can tell from the following output for `commontastes_l`, the most common
    words are mostly different (except for `spice` and `spicy`):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从以下`commontastes_l`的输出中可以看出，最常见的词大多不同（除了`spice`和`spicy`）：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Something we can do with this list to enhance our tabular dataset is turn these
    common words into binary features. In other words, there will be a column for
    each one of these “common tastes” (`commontastes_l`), and if the “tastes” for
    the chocolate bar include it, the column will contain a 1, otherwise a 0\. Fortunately,
    we can easily do this with two lines of code. First, we create a new column with
    our text-tastes series (`tastes_s`). Then, we use the `make_dummies_from_dict`
    function we used in the last chapter to generate the dummy features by looking
    for each “common taste” in the contents of our new column, as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用这个列表来增强我们的表格数据集，方法是将这些常用词转换为二元特征。换句话说，将会有一个列代表这些“常见口味”中的每一个（`commontastes_l`），如果巧克力条的“口味”包括它，则该列将包含一个1，否则为0。幸运的是，我们可以用两行代码轻松完成这个操作。首先，我们创建一个新的列，包含我们的文本口味序列（`tastes_s`）。然后，我们使用上一章中使用的`make_dummies_from_dict`函数，通过在新列的内容中查找每个“常见口味”来生成虚拟特征，如下所示：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that we are done with our feature engineering, we can use `info()` to examine
    our DataFrame. The output has all numeric non-null features except for `company`.
    There are over 500 companies, so **categorical encoding** of this feature would
    be complicated and, because it would be advisable to bucket most companies as
    `Other`, it would likely introduce bias toward the few companies that are most
    represented. Therefore, it’s better to remove this column altogether. The output
    is shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了特征工程，我们可以使用`info()`来检查我们的DataFrame。输出包含所有数值非空特征，除了`company`。有超过500家公司，所以对这个特征的**分类编码**将会很复杂，并且由于建议将大多数公司归入`Other`类别，这可能会引入对最常出现的少数公司的偏差。因此，最好完全删除这个列。输出如下所示：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Our last step to prepare the data for modeling starts with initializing `rand`,
    a constant to serve as our “random state” throughout this exercise. Then, we define
    `y` as the `rating` column converted to 1 if greater than or equal to `3.5`, and
    `0` otherwise. `X` is everything else (excluding `company`). Then, we split `X`
    and `y` into train and test datasets with `train_test_split`, as illustrated in
    the following code snippet:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备数据以进行建模的最后一步是从初始化`rand`，一个在整个练习中作为我们的“随机状态”的常量。然后，我们将`y`定义为如果大于或等于`3.5`则转换为`1`的`rating`列，否则为`0`。`X`是其他所有内容（不包括`company`）。然后，我们使用`train_test_split`将`X`和`y`分割成训练集和测试集，如下代码片段所示：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In addition to the tabular test and train datasets, for our NLP models, we
    will need text-only feature datasets that are consistent with our `train_test_split`
    so that we can use the same `y` labels. To this end, we can do this by subsetting
    our tastes series (`tastes_s`) using the `index` of our `X_train` and `X_test`
    sets to yield NLP-specific versions of the series, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表格形式的测试集和训练集之外，对于我们的NLP模型，我们还需要与我们的`train_test_split`一致的纯文本特征数据集，以便我们可以使用相同的`y`标签。为此，我们可以通过使用`X_train`和`X_test`集合的`index`来子集化我们的`tastes_s`系列，从而得到NLP特定的系列版本，如下所示：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: OK! We are all set now. Let’s start modeling and interpreting our models!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！我们现在已经准备好了。让我们开始建模并解释我们的模型！
- en: Leveraging SHAP’s KernelExplainer for local interpretations with SHAP values
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用SHAP的KernelExplainer进行局部解释，并使用SHAP值
- en: For this section, and for subsequent use, we will train a **Support Vector Classifier**
    (**SVC**) model first.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节以及随后的使用，我们将首先训练一个**支持向量分类器**（**SVC**）模型。
- en: Training a C-SVC model
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个C-SVC模型
- en: SVM is a family of model classes that operate in high-dimensional space to find
    an optimal hyperplane, where they attempt to separate the classes with the maximum
    margin between them. Support vectors are the points closest to the decision boundary
    (the dividing hyperplane) that would change it if were removed. To find the best
    hyperplane, they use a cost function called **hinge loss** and a computationally
    cheap method to operate in high-dimensional space, called the **kernel trick**,
    and even though a hyperplane suggests linear separability, it’s not always limited
    to a linear kernel.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是一系列在多维空间中操作的模型类，它们试图找到一个最优的超平面，其中它们试图通过它们之间的最大间隔来分离类别。支持向量是距离决策边界（分割超平面）最近的点，如果移除它们，将改变该边界。为了找到最佳超平面，它们使用一个称为**hinge
    loss**的代价函数，以及一个在多维空间中操作的计算上便宜的方法，称为**核技巧**。尽管超平面暗示了线性可分性，但它并不总是限于线性核。
- en: The scikit-learn implementation we will use is called C-SVC. SVC uses an L2
    regularization parameter called *C* and, by default, uses a kernel called the
    **Radial Basis Function** (**RBF**), which is decidedly nonlinear. For an RBF,
    a **gamma** hyperparameter defines the radius of influence of each training example
    in the kernel, but in an inversely proportional fashion. Hence, a low value increases
    the radius, while a high value decreases it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的scikit-learn实现称为C-SVC。SVC使用一个名为*C*的L2正则化参数，并且默认使用一个称为**径向基函数**（**RBF**）的核，这是一个决定性的非线性核。对于RBF，一个**gamma**超参数定义了核中每个训练示例的影响半径，但以相反的比例。因此，低值增加半径，而高值减小半径。
- en: The SVM family includes several variations for classification and even regression
    classes through **Support Vector Regression** (**SVR**). The most significant
    advantage of SVM models is that they tend to work effectively and efficiently
    when there are many features compared to the observations, and even when the features
    exceed the observations! It also tends to find latent nonlinear relationships
    in the data, without overfitting or becoming unstable. However, SVMs are not as
    scalable to larger datasets, and it’s hard to tune their hyperparameters.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SVM家族包括用于分类和甚至回归类的几种变体，通过**支持向量回归**（**SVR**）。SVM模型最显著的优势是，与观察值相比，当有大量特征时，它们往往能够有效地工作，甚至在特征超过观察值的情况下！它还倾向于在数据中找到潜在的非线性关系，而不会过拟合或变得不稳定。然而，SVM模型并不容易扩展到更大的数据集，并且很难调整它们的超参数。
- en: 'Since we will use `seaborn` plot styling, which is activated with `set()`,
    for some of this chapter’s plots, we will first save the original `matplotlib`
    settings (`rcParams`) so that we can restore them later. One thing to note about
    `SVC` is that it doesn’t natively produce probabilities since it’s linear algebra.
    However, if `probability=True`, the scikit-learn implementation uses cross-validation
    and then fits a logistic regression model to the SVC’s scores to produce the probabilities.
    We are also using `gamma=auto`, which means it is set to 1/# features—so, 1/44\.
    As always, it is recommended to set your `random_state` parameter for reproducibility.
    Once we fit the model to the training data, we can use `evaluate_class_mdl` to
    evaluate our model’s predictive performance, as illustrated in the following code
    snippet:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用`seaborn`绘图样式，该样式通过`set()`激活，用于本章的一些图表，我们首先保存原始的`matplotlib`设置（`rcParams`），以便我们可以在以后恢复它们。关于`SVC`的一个需要注意的事项是，它本身不产生概率，因为它涉及线性代数。然而，如果`probability=True`，scikit-learn实现使用交叉验证，然后拟合一个逻辑回归模型到SVC的分数以产生概率。我们还使用`gamma=auto`，这意味着它设置为1/#特征——所以，1/44。始终建议设置你的`random_state`参数以实现可重复性。一旦我们将模型拟合到训练数据，我们就可以使用`evaluate_class_mdl`来评估我们的模型预测性能，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code produces the output shown here in *Figure 5.3*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了这里显示的*图5.3*中的输出：
- en: '![Chart, line chart  Description automatically generated](img/B18406_05_03.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  描述自动生成](img/B18406_05_03.png)'
- en: 'Figure 5.3: Predictive performance of our SVC model'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：我们的SVC模型的预测性能
- en: '*Figure 5.3* shows the performance achieved is not bad, considering this is
    a small imbalanced dataset in an already challenging domain for machine learning
    models’ user ratings. In any case, the **Area Under the Curve** (**AUC**) curve
    is above the dotted coin toss line, and the **Matthews correlation coefficient**
    (**MCC**) is safely above 0\. More importantly, precision is substantially higher
    than recall, and this is very good given the hypothetical cost of misclassifying
    a lousy chocolate bar as *Highly Recommended*. We favor precision over recall
    because we would prefer to have fewer false positives than false negatives.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.3*显示，考虑到这是一个在小不平衡数据集上，对于机器学习模型用户评分已经是一个具有挑战性的领域，所取得的性能还不错。无论如何，**曲线下面积**（**AUC**）曲线高于虚线抛硬币线，**马修斯相关系数**（**MCC**）安全地高于0。更重要的是，精确度远高于召回率，考虑到将一块糟糕的巧克力误分类为**强烈推荐**的假设成本，这是非常好的。我们更倾向于精确度而不是召回率，因为我们更愿意有较少的误报而不是误判。'
- en: Computing SHAP values using KernelExplainer
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用KernelExplainer计算SHAP值
- en: Given how computationally intensive calculating SHAP values by brute force can
    be, the SHAP library takes many statistically valid shortcuts. As we learned in
    *Chapter 4*, *Global Model-Agnostic Interpretation Methods*, these shortcuts range
    from leveraging a decision tree’s structure (`TreeExplainer`) to the difference
    in a neural network’s activations, a baseline (`DeepExplainer`) to a neural network’s
    gradient (`GradientExplainer`). These shortcuts make the explainers model-specific
    since they are limited to a family of model classes. However, there is a model-agnostic
    explainer in SHAP, called the `KernelExplainer`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通过暴力计算SHAP值可能非常计算密集，SHAP库采用了许多统计上有效的捷径。正如我们在*第4章*中学到的，*全局模型无关解释方法*，这些捷径从利用决策树的结构（`TreeExplainer`）到神经网络激活的差异，一个基线（`DeepExplainer`）到一个神经网络的梯度（`GradientExplainer`）。这些捷径使得解释器模型特定，因为它们局限于一系列模型类。然而，SHAP中有一个模型无关的解释器，称为`KernelExplainer`。
- en: '`KernelExplainer` has two shortcuts; it samples a subset of all feature permutations
    for coalitions and uses a weighting scheme according to the size of the coalition
    to compute SHAP values. The first shortcut is a recommended technique to reduce
    computation time. The second one is drawn from LIME’s weighting scheme, which
    we will cover next in this chapter, and the authors of SHAP did this so that it
    remains compliant with Shapley. However, for “missing” features in the coalition,
    it randomly samples from the features’ values in a background training dataset,
    which violates the **dummy** property of Shapley values. More importantly, as
    with **permutation feature importance**, if there’s multicollinearity, it puts
    too much weight on unlikely instances. Despite this near-fatal flaw, `KernelExplainer`
    has all the other benefits of Shapley values and at least one of LIME’s main advantages.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`KernelExplainer` 有两个快捷方式；它为联盟采样所有特征排列的子集，并使用根据联盟大小计算的加权方案来计算 SHAP 值。第一个快捷方式是减少计算时间的推荐技术。第二个快捷方式来自
    LIME 的加权方案，我们将在本章的下一部分介绍，SHAP 的作者这样做是为了保持与 Shapley 的一致性。然而，对于联盟中的“缺失”特征，它从背景训练数据集中随机采样特征值，这违反了
    Shapley 值的 **虚拟** 属性。更重要的是，与 **排列特征重要性** 一样，如果存在多重共线性，它会过分重视不太可能的情况。尽管存在这个几乎致命的缺陷，`KernelExplainer`
    仍然具有 Shapley 值的所有其他优点，以及 LIME 至少一个主要优点。'
- en: Before we engage with the `KernelExplainer`, it’s important to note that for
    classification models, it yields a list of multiple SHAP values. We can access
    the list of values for each class with an index. Confusion may arise if this index
    is not in the order we expect because it’s in the order provided by the model.
    So, it is essential to make sure of the order of the classes in our model by running
    `print(svm_mdl.classes_)`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们与 `KernelExplainer` 交互之前，需要注意的是，对于分类模型，它会产生多个 SHAP 值的列表。我们可以通过索引访问每个类的值列表。如果这个索引不是我们预期的顺序，可能会引起混淆，因为它是按照模型提供的顺序排列的。因此，确保我们模型中类的顺序非常重要，可以通过运行
    `print(svm_mdl.classes_)` 来实现。
- en: The output `array([0, 1])` tells us that *Not Highly Recommended* has an index
    of 0, as we would expect, and *Highly Recommended* has an index of 1\. We are
    interested in the SHAP values for the latter because this is what we are trying
    to predict.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 `array([0, 1])` 告诉我们，*不推荐* 的索引为 0，正如我们所预期，而 *强烈推荐* 的索引为 1。我们对后者的 SHAP 值感兴趣，因为这正是我们试图预测的内容。
- en: '`KernelExplainer` takes a `predict` function for a model (`fitted_svm_mdl.predict_proba`)
    and some background training data (`X_train_summary`). `KernelExplainer` leverages
    additional measures to minimize computation. One of these is using **k-means**
    to summarize the background training data instead of using it whole. Another method
    could be using a sample of the training data. In this case, we opted for k-means
    clustering into 10 centroids. Once we have initialized our explainer, we can use
    samples of our test dataset (`nsamples=200`) to come up with the SHAP values.
    It uses L1 regularization (`l1_reg`) during the fitting process. What we are telling
    it here is to regularize to a point where it only has 20 relevant features. Lastly,
    we can use a `summary_plot` to plot our SHAP values for class 1\. The code is
    illustrated in the following snippet:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`KernelExplainer` 接受一个模型的 `predict` 函数（`fitted_svm_mdl.predict_proba`）和一些背景训练数据（`X_train_summary`）。`KernelExplainer`
    利用额外的措施来最小化计算。其中之一是使用 **k-means** 对背景训练数据进行总结，而不是使用整个数据。另一种方法可能是使用训练数据的一个样本。在这种情况下，我们选择了将数据聚类到
    10 个质心。一旦我们初始化了我们的解释器，我们就可以使用测试数据集的样本（`nsamples=200`）来得出 SHAP 值。它在拟合过程中使用 L1 正则化（`l1_reg`）。我们在这里告诉它的是正则化到一个点，它只有
    20 个相关特征。最后，我们可以使用 `summary_plot` 来绘制类别 1 的 SHAP 值。代码在下面的片段中展示：'
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code produces the output shown in *Figure 5.4*. Even though the
    point of this chapter is local model interpretation, it’s important to start with
    the global form of this to make sure outcomes are intuitive. If they aren’t, perhaps
    something is amiss:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了 *图 5.4* 中所示的输出。尽管本章的重点是局部模型解释，但重要的是要从全局形式开始，以确保结果直观。如果结果不直观，可能存在问题：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18406_05_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![包含图形用户界面的图片 描述自动生成](img/B18406_05_04.png)'
- en: 'Figure 5.4: Global model interpretation with SHAP using a summary plot'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：使用 SHAP 的全局模型解释与总结图
- en: In *Figure 5.4*, we can tell that the highest (red) cocoa percentages (`cocoa_percent`)
    tend to correlate with a decrease in the likelihood of *Highly Recommended*, but
    the middle values (purple) tend to increase it. This finding makes intuitive sense
    because the darkest chocolates are more of an acquired taste than less-dark chocolates.
    The low values (blue) are scattered throughout so they show no trend, but this
    could be because there aren’t many.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.4*中，我们可以看出最高的（红色）可可百分比(`cocoa_percent`)往往与**高度推荐**可能性的降低相关，而中间值（紫色）往往增加它。这个发现从直观上是有意义的，因为最深的巧克力比不太深的巧克力更是一种习得的味道。低值（蓝色）散布在整个图表中，因此没有显示出趋势，但这可能是因为数量不多。
- en: On the other hand, `review_date` suggests that it was likely to be *Highly Recommended*
    in earlier years. There are significant shades of red and purple on both sides
    of 0, so it’s hard to identify a trend here. A dependence plot, such as those
    used in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*, would be
    better for this purpose. However, it’s very easy for binary features to visualize
    how high and low values, ones and zeros, impact the model. For instance, we can
    tell that the presence of cocoa, creamy, rich, and berry tastes increases the
    likelihood of the chocolate being recommended, while sweet, earthy, sour, and
    fatty tastes do the opposite. Likewise, the odds for *Highly* *Recommended* decrease
    if the chocolate was manufactured in the US! Sorry, the US.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`review_date`表明，在早期年份很可能被**高度推荐**。在0的两侧都有显著的红色和紫色阴影，因此很难在这里识别出趋势。像*第4章*中使用的依赖图将更适合这个目的。然而，对于二元特征来说，可视化高值和低值、一和零如何影响模型是非常容易的。例如，我们可以知道可可、奶油、丰富和浆果味道的存在增加了巧克力被推荐的可能性，而甜、土质、酸和油腻的味道则相反。同样，如果巧克力是在美国制造的，那么**高度****推荐**的几率就会降低！抱歉，是美国。
- en: Local interpretation for a group of predictions using decision plots
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用决策图对一组预测进行局部解释
- en: 'For local interpretation, you don’t have to visualize one point at a time—you
    can instead interpret several at a time. The key is providing some context to
    compare the points adequately, and there can’t be so many that you can’t distinguish
    them. Usually, you would find outliers or only those that meet specific criteria.
    For this exercise, we will select only those bars that were produced by your client,
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于局部解释，你不必一次可视化一个点——你可以同时解释几个点。关键是提供一些上下文来充分比较这些点，而且不能有太多以至于你无法区分它们。通常，你会找到异常值或者只满足特定标准的那些点。对于这个练习，我们将只选择那些由你的客户生产的条形，如下所示：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'One great thing about Shapley is its additivity property, which can be easily
    demonstrated. If you add all the SHAP values to the expected value used to compute
    them, you get a prediction. Of course, this is a classification problem, so the
    prediction is a probability; so, to get a Boolean array instead, we have to check
    if the probability is greater than 0.5\. We can check if this Boolean array matches
    our model’s test dataset predictions `(y_test_svc_pred`) by running the following
    code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley的一个优点是其可加性属性，这一点很容易证明。如果你将所有SHAP值加到计算它们所用的期望值上，你就能得到一个预测。当然，这是一个分类问题，所以预测是一个概率；因此，为了得到一个布尔数组，我们必须检查这个概率是否大于0.5。我们可以通过运行以下代码来检查这个布尔数组是否与我们的模型测试数据集预测`(y_test_svc_pred)`相匹配：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It should, and it does! You can see it confirmed with a `True` value.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该，并且确实如此！你可以通过一个`True`值看到它得到了证实。
- en: 'SHAP’s decision plot comes with a highlight feature that we can use to make
    false negatives (`FN`) stand out. Now, let’s figure out which of our sample observations
    are `FN`, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP的决策图自带一个高亮功能，我们可以使用它来使**假阴性**(`FN`)突出。现在，让我们找出我们的样本观测值中哪些是`FN`，如下所示：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now quickly reset and plot a `decision_plot`. It takes the `expected_value`,
    the SHAP values, and the actual values of those items we wish to plot. Optionally,
    we can provide a Boolean array of the items we want to highlight, with dotted
    lines—in this case, the false negatives (`FN`), as illustrated in the following
    code snippet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以快速重置并绘制一个`decision_plot`。它需要`expected_value`、SHAP值以及我们希望绘制的那些项目的实际值。可选地，我们可以提供一个布尔数组，表示我们想要高亮的项，用虚线表示——在这个例子中，是**假阴性**(`FN`)，如下面的代码片段所示：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The plot produced in *Figure 5.5* has a single color-coded line for each observation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.5*中产生的图表为每个观测值提供了一个单色编码的线条。'
- en: '![Chart, radar chart  Description automatically generated](img/B18406_05_05.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图表，雷达图  自动生成的描述](img/B18406_05_05.png)'
- en: 'Figure 5.5: Local model interpretation with SHAP for a sample of predictions,
    highlighting false negatives'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：使用SHAP对预测样本的局部模型解释，突出显示假阴性
- en: The color of each line represents not the value of any feature, but the model
    output. Since we used `predict_proba` in `KernelExplainer`, this is a probability,
    but otherwise, it would have displayed SHAP values, and the value they have when
    they strike the top *x*-axis is the predicted value. The features are sorted in
    terms of importance but only among the observations plotted, and you can tell
    that the lines increase and decrease horizontally depending on each feature. How
    much they vary and in which direction depends on the feature’s contribution to
    the outcome. The gray line represents the class’s expected value, which is like
    the intercept in a linear model. In fact, similarly, all lines start at this value,
    making it best to read the plot from bottom to top.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 每条线的颜色代表的不是任何特征的值，而是模型输出。由于我们在`KernelExplainer`中使用了`predict_proba`，这是一个概率，但否则它将显示SHAP值，并且当它们击中顶部的*x*轴时，它们的值是预测值。特征是根据重要性排序的，但仅限于绘制的观察值，你可以看出线条根据每个特征水平增加和减少。它们的变异程度和方向取决于特征对结果的影响。灰色线代表该类别的期望值，类似于线性模型中的截距。事实上，所有线条都是从这个值开始的，因此最好从下往上阅读这个图。
- en: You can tell that there are three false negatives plotted in *Figure 5.5* because
    they have dotted lines. Using this plot, we can easily visualize which features
    made them veer toward the left the most because this is what made them negative
    predictions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以知道*图5.5*中有三个假阴性，因为它们有虚线。使用这个图，我们可以轻松地可视化哪些特征使它们向左偏转最多，因为这是使它们成为负预测的原因。
- en: For instance, we know that the leftmost false negative was to the right of the
    expected value line until `lecithin` and then continued decreasing until `company_location_France`,
    and `review_date` increased its likelihood of *Highly Recommended*, but it wasn’t
    enough. You can tell that `county_of_bean_origin_Other` decreased the likelihood
    of two of the misclassifications. This decision could be unfair because the country
    could be one of over 50 countries that didn’t get their own feature. Quite possibly,
    there’s a lot of variation between the beans of these countries grouped together.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们知道最左侧的假阴性出现在期望值线右侧，直到`lecithin`，然后继续下降直到`company_location_France`，而`review_date`增加了其成为*高度推荐*的可能性，但这还不够。你可以看出`county_of_bean_origin_Other`降低了两种误分类的可能性。这个决定可能是不公平的，因为国家可能是超过50个没有自己特征的国家之一。很可能，这些国家聚集在一起的豆子之间存在很多差异。
- en: 'Decision plots can also isolate a single observation. When it does this, it
    prints the value of each feature next to the dotted line. Let’s plot one for a
    decision plot of the same company (true-positive observation #696), as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 决策图也可以隔离单个观察值。当它这样做时，它会将每个特征值打印在虚线旁边。让我们为同一公司的决策图（真阳性观察值#696）绘制一个，如下所示：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 5.6* here was outputted by the preceding code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.6*在这里是由前面的代码生成的：'
- en: '![Graphical user interface, table  Description automatically generated](img/B18406_05_06.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，表格  自动生成的描述](img/B18406_05_06.png)'
- en: 'Figure 5.6: The SHAP decision plot for a single true positive in the sample
    of predictions'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：预测样本中单个真阳性的SHAP决策图
- en: In *Figure 5.6*, you can see that `lecithin` and `counts_of_ingredients` decreased
    the *Highly Recommended* likelihood to a point where they could have jeopardized
    it. Fortunately, all features above those veered the line decidedly rightward
    because `company_location_France=1`, `cocoa_percent=70`, and `tastes_berry=1`
    are all favorable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.6*中，你可以看到`lecithin`和`counts_of_ingredients`将*高度推荐*的可能性降低到可能危及它的程度。幸运的是，所有高于这些特征的值都使线条明显向右偏转，因为`company_location_France=1`、`cocoa_percent=70`和`tastes_berry=1`都是有利因素。
- en: Local interpretation for a single prediction at a time using a force plot
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用力图对单个预测进行局部解释
- en: 'Your client, the chocolate manufacturer, has two bars they want you to compare.
    Bar #5 is *Outstanding* and #24 is *Disappointing*. They are both in your test
    dataset. One way of comparing them is to place their values side by side in a
    DataFrame to understand how exactly they differ. We will concatenate the rating,
    the actual label `y`, and the `y_pred` predicted label to these observations’
    values, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你的客户，巧克力制造商，有两块巧克力希望让你比较。第5块是*杰出*，第24块是*令人失望*。它们都在你的测试数据集中。比较它们的一种方法是将它们的值并排放置在DataFrame中，以了解它们究竟有何不同。我们将以下列的评分、实际标签`y`和预测标签`y_pred`合并到这些观察值的旁边，如下所示：
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code produces the DataFrame shown in *Figure 5.7*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图5.7*中显示的DataFrame：
- en: '![Table  Description automatically generated](img/B18406_05_07.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18406_05_07.png)'
- en: 'Figure 5.7: Observations #5 and #24 side by side, with feature differences
    highlighted in yellow'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：观察#5和#24并排，特征差异以黄色突出显示
- en: With this DataFrame, you can confirm that they aren’t misclassifications because
    `y=y_pred`. A misclassification could make model interpretations unreliable for
    understanding why people tend to like one chocolate bar more than another. Then,
    you can examine the features to spot the differences—for instance, you can tell
    that the `review_date` is 2 years apart. Also, the beans for the *Outstanding*
    bar were from Venezuela, and the *Disappointing* beans came from another, lesser-represented
    country. The *Outstanding* one had a berry taste, and the *Disappointing* one
    was earthy.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个DataFrame，你可以确认它们不是误分类，因为`y=y_pred`。误分类可能会使模型解释不可靠，难以理解为什么人们倾向于喜欢一块巧克力而不是另一块。然后，你可以检查特征以发现差异——例如，你可以知道`review_date`相差2年。此外，*杰出*巧克力中的豆子来自委内瑞拉，而*令人失望*的豆子来自另一个代表性较小的国家。*杰出*的巧克力有浆果的味道，而*令人失望*的则是土质的。
- en: 'The force plot can tell us a complete story of what weighed in the model’s
    decisions (and, presumably, the reviewers’), and gives us clues as to what consumers
    might prefer. Plotting a `force_plot` requires the expected value for the class
    of your interest (`expected_value`), the SHAP values for the observation of your
    interest, and this observation’s actual values. We will start with observation
    #5, as illustrated in the following code snippet:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 力图可以告诉我们模型决策（以及，推测，审评者）中考虑了哪些因素，并为我们提供了关于消费者可能偏好的线索。绘制`force_plot`需要你感兴趣类别的期望值（`expected_value`），你感兴趣观察的SHAP值，以及这个观察的实际值。我们将从以下代码片段中的观察#5开始：
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding code produces the plot shown in *Figure 5.8*. This force plot
    depicts how much `review_date`, `cocoa_percent`, and `tastes_berry` weigh in the
    prediction, while the only feature that seems to be weighing in the opposite direction
    is `counts_of_ingredients`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图5.8*中显示的图表。这个力图描绘了`review_date`、`cocoa_percent`和`tastes_berry`在预测中的权重，而唯一似乎在相反方向起作用的特征是`counts_of_ingredients`：
- en: '![Timeline  Description automatically generated](img/B18406_05_08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![时间线 描述自动生成](img/B18406_05_08.png)'
- en: 'Figure 5.8: The force plot for observation #5 (Outstanding)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8：观察#5的力图（杰出）
- en: 'Let’s compare it with a force plot of observation #24, as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与观察#24的力图进行比较，如下所示：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding code produces the plot shown in *Figure 5.9*. We can easily tell
    that `tastes_earthy` and `country_of_bean_origin_Other` are considered highly
    negative attributes by our model. The outcome can be mostly explained by the difference
    in the chocolate tasting of “berry” versus “earthy.” Despite our findings, the
    beans’ origin country needs further investigation. After all, it is possible that
    the actual country of origin doesn’t correlate with poor ratings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图5.9*中显示的图表。我们可以很容易地看出，`tastes_earthy`和`country_of_bean_origin_Other`在我们的模型中被认为是高度负面的属性。结果可以主要由巧克力品尝中“浆果”与“土质”的差异来解释。尽管我们有这些发现，但豆子的原产国需要进一步调查。毕竟，实际的原产国可能与低评分不相关。
- en: '![Timeline  Description automatically generated](img/B18406_05_09.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![时间线 描述自动生成](img/B18406_05_09.png)'
- en: 'Figure 5.9: The force plot for observation #24 (Disappointing)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：观察#24的力图（令人失望）
- en: In this section, we covered the `KernelExplainer`, which uses some tricks it
    learned from LIME. But what is LIME? We will find that out next!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了`KernelExplainer`，它从LIME学到了一些技巧。但LIME是什么？我们将在下一节找到答案！
- en: Employing LIME
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LIME
- en: Until now, the model-agnostic interpretation methods we’ve covered attempt to
    reconcile the totality of outputs of a model with its inputs. For these methods
    to get a good idea of how and why `X` becomes `y_pred`, we need some data first.
    Then, we perform simulations with this data, pushing variations of it into a model
    and evaluating what comes out of the model. Sometimes, they even leverage a global
    surrogate to connect the dots. By using what we learned in this process, we yield
    feature importance values that quantify a feature’s impact, interactions, or decisions
    on a global level. For many methods such as SHAP, these can be observed locally
    too. However, even when they can be observed locally, what was quantified globally
    may not apply locally. For this reason, there should be another approach that
    quantifies the local effects of features solely for local interpretation—one such
    as LIME!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一直以来，我们介绍的模型无关解释方法试图将模型的输出整体与其输入相协调。为了使这些方法能够很好地理解`X`如何变成`y_pred`以及原因，我们首先需要一些数据。然后，我们使用这些数据进行模拟，将数据的变体推入模型并评估模型输出的结果。有时，它们甚至利用全局代理来连接这些点。通过使用在这个过程中学到的知识，我们得到特征重要性值，这些值量化了特征对全局预测的影响、交互或决策。对于SHAP等许多方法，这些值也可以在局部观察到。然而，即使它们可以在局部观察到，全局量化的结果可能并不适用于局部。因此，应该有另一种方法，仅针对局部解释量化特征对局部的影响——LIME就是这样一种方法！
- en: What is LIME?
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是LIME？
- en: '**LIME** trains local surrogates to explain a single prediction. To this end,
    it starts by asking us which *data point* we want to interpret. You also provide
    it with your black-box model and a sample dataset. It then makes predictions on
    a *perturbed* version of the dataset with the model, creating a scheme whereby
    it samples and *weighs* points higher if they are *closer* to your chosen data
    point. This area around your point is called a neighborhood. Then, using the sampled
    points and black-box predictions in this neighborhood, it trains a weighted *intrinsically
    interpretable surrogate model*. Lastly, it interprets the surrogate model.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**LIME**通过训练局部代理来解释单个预测。为此，它首先询问我们想要解释哪个*数据点*。你还提供你的黑盒模型和样本数据集。然后，它使用模型对数据集的*扰动*版本进行预测，创建一个方案，其中它根据点与所选数据点的接近程度来采样和*加权*点。这个点周围的区域被称为邻域。然后，使用这个邻域中的采样点和黑盒预测，它训练一个带权的*内在可解释的代理模型*。最后，它解释这个代理模型。'
- en: 'There are lots of keywords to unpack here so let’s define them, as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多关键词需要解释，让我们如下定义它们：
- en: '**Chosen data point**: LIME calls the data point, row, or observation you want
    to interpret an *instance*. It’s just another word for this concept.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chosen data point**: LIME将你想要解释的数据点、行或观察称为一个*实例*。这只是这个概念的另一种说法。'
- en: '**Perturbation**: LIME simulates new samples by adding noise to each sample.
    In other words, it creates random samples close to each instance.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Perturbation**: LIME通过向每个样本添加噪声来模拟新的样本。换句话说，它创建了接近每个实例的随机样本。'
- en: '**Weighting scheme**: LIME uses an exponential smoothing kernel to both define
    the local instance neighborhood radius and determine how to weigh the points farthest
    versus those closest to the instance.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weighting scheme**: LIME使用指数平滑核来定义局部实例邻域半径，并确定如何权衡远离实例的点与靠近实例的点。'
- en: '**Closer**: LIME uses Euclidean distance for tabular and image data, and cosine
    similarity for text. This is hard to imagine in high-dimensional feature spaces,
    but you can calculate the distance between points for any number of dimensions
    and find which points are closest to the one of interest.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Closer**: LIME使用欧几里得距离来处理表格和图像数据，而对于文本数据则使用余弦相似度。这在高维特征空间中难以想象，但你仍然可以计算任意维度的点之间的距离，并找出与目标点最近的点。'
- en: '**Intrinsically interpretable surrogate model**: LIME uses a sparse linear
    model with weighted ridge regularization. However, it could use any intrinsically
    interpretable model as long as the data points can be weighted. The idea behind
    this is twofold. It needs a model that can yield reliable intrinsically interpretable
    parameters, such as coefficients that indicate the influence of the feature on
    the prediction. It also needs to consider data points closest to the chosen point
    more because these are more relevant.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Intrinsically interpretable surrogate model**: LIME使用带权重的岭回归的正则化稀疏线性模型。然而，只要数据点可以被加权，它就可以使用任何内在可解释的模型。这个想法有两方面。它需要一个可以产生可靠内在可解释参数的模型，例如指示特征对预测影响的系数。它还需要更多地考虑与所选点最接近的数据点，因为这些点更相关。'
- en: Much like with **k-Nearest Neighbors** (**k-NN**), the intuition behind LIME
    is that points in a neighborhood have commonality because we expect points close
    to each other to have similar, if not the same, labels. There are decision boundaries
    for classifiers, so this could be a very naive assumption to make when close points
    are divided by one.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与**k-邻近邻居**（**k-NN**）类似，LIME背后的直觉是，邻域中的点具有共性，因为我们期望彼此靠近的点具有相似，如果不是相同的标签。对于分类器有决策边界，所以当接近的点被一个决策边界分开时，这可能会是一个非常天真的假设。
- en: Similar to another model class in the Nearest Neighbors family, **Radius Nearest
    Neighbors**, LIME factors in the distance along a radius and weighs points accordingly,
    although it does this exponentially. However, LIME is not a model class but an
    interpretation method, so the similarities stop there. Instead of “voting” for
    predictions among neighbors, it fits a weighted surrogate sparse linear model
    because it assumes that every complex model is linear locally, and because it’s
    not a model class, the predictions the surrogate model makes don’t matter. In
    fact, the surrogate model doesn’t even have to fit the data like a glove because
    all you need from it is the coefficients. Of course, that being said, it is best
    if it fits well so that there is higher fidelity in the interpretation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与邻近邻居家族中的另一个模型类**半径邻近邻居**相似，LIME考虑了沿着半径的距离并相应地权衡点，尽管它是以指数方式进行的。然而，LIME不是一个模型类，而是一种解释方法，所以相似之处到此为止。它不是通过在邻居之间“投票”预测，而是通过拟合一个加权的代理稀疏线性模型，因为它假设每个复杂模型在局部都是线性的，而且因为它不是一个模型类，所以代理模型所做的预测并不重要。实际上，代理模型甚至不需要像手套一样拟合数据，因为你从它那里需要的只是系数。当然，话虽如此，如果它能很好地拟合，那么在解释上就会有更高的保真度。
- en: LIME works for tabular, image, and text data and generally has high local fidelity,
    meaning that it can approximate the model predictions quite well on a local level.
    However, this is contingent on the neighborhood being defined correctly, which
    stems from choosing the right kernel width and the assumption of local linearity
    holding true.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: LIME适用于表格、图像和文本数据，并且通常具有高局部保真度，这意味着它可以在局部水平上很好地近似模型预测。然而，这取决于邻域是否被正确定义，这源于选择合适的核宽度和局部线性假设的成立。
- en: Local interpretation for a single prediction at a time using LimeTabularExplainer
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LimeTabularExplainer对单个预测进行局部解释
- en: 'To explain a single prediction, you first instantiate a `LimeTabularExplainer`
    by providing it with your sample dataset in a `numpy` 2D array (`X_test.values`),
    a list with the names of the features (`X_test.columns`), a list with the indices
    of the categorical features (only the first three features aren’t categorical),
    and the class names. Even though only the sample dataset is required, it is recommended
    that you provide names for your features and classes so that the interpretation
    makes sense. For tabular data, telling LIME which features are categorical (`categorical_features`)
    is important because it treats categorical features differently from continuous
    ones, and not specifying this could potentially make for a poor-fitting local
    surrogate. Another parameter that can greatly impact the local surrogate is `kernel_width`.
    This defines the diameter of the neighborhood, thus answering the question of
    what is considered local. It has a default value, which may or may not yield interpretations
    that make sense for your instance. You could tune this parameter on an instance-by-instance
    basis to ensure that each time you generate an explanation, it is consistent.
    Please note that the random nature of perturbation, when applied to a large neighborhood,
    might lead to inconsistent results. So as you make this neighborhood smaller,
    you will reduce the variability between runs. The code can be seen in the following
    snippet:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释单个预测，你首先需要通过提供你的样本数据集（以 `numpy` 2D 数组的形式 `X_test.values`）、一个包含特征名称的列表（`X_test.columns`）、一个包含分类特征索引的列表（只有前三个特征不是分类特征），以及类名称来实例化一个
    `LimeTabularExplainer`。尽管只需要样本数据集，但建议你为你的特征和类提供名称，以便解释有意义。对于表格数据，告诉 LIME 哪些特征是分类的（`categorical_features`）很重要，因为它将分类特征与连续特征区分对待，不指定这一点可能会使局部代理拟合不良。另一个可以极大地影响局部代理的参数是
    `kernel_width`。它定义了邻域的直径，从而回答了什么是局部的问题。它有一个默认值，这个值可能或可能不会产生对你实例有意义的解释。你可以根据实例调整此参数，以确保每次生成解释时都是一致的。请注意，当应用于大邻域时，扰动的随机性质可能会导致结果不一致。因此，当你使这个邻域更小的时候，你会减少运行之间的变异性。以下代码片段展示了代码：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With the instantiated explainer, you can now use `explain_instance` to fit
    a local surrogate model to observation #5\. We will also use our model’s classifier
    function (`predict_proba`) and limit our number of features to eight (`num_features=8`).
    We can take the “explanation” returned and immediately visualize it with `show_in_notebook`.
    At the same time, the `predict_proba` parameter makes sure it also includes a
    plot to show which class is the most probable, according to the local surrogate
    model. The code is illustrated in the following snippet:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '使用实例化的解释器，你现在可以使用 `explain_instance` 为观察 #5 拟合一个局部代理模型。我们还将使用我们模型的分类函数（`predict_proba`）并将特征数量限制为八个（`num_features=8`）。我们可以获取“解释”并立即使用
    `show_in_notebook` 进行可视化。同时，`predict_proba` 参数确保它还包括一个图表，显示根据局部代理模型，哪个类是最可能的。以下代码片段展示了代码：'
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code provides the output shown in *Figure 5.10*. This figure
    can be read as other feature importance plots with the most influential features
    having the highest coefficients – and vice versa. However, it has features that
    weigh in each direction. According to the local surrogate, a `cocoa_percent` value
    smaller or equal to 70 is a favorable attribute, as is the berry taste. A lack
    of sour, sweet, and molasses tastes also weighs favorably in this model. However,
    a lack of rich, creamy, and cocoa tastes does the opposite, but not enough to
    push the scales toward *Not Highly Recommended*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码提供了 *图 5.10* 中所示的输出。这个图可以像其他特征重要性图一样阅读，最有影响力的特征具有最高的系数——反之亦然。然而，它具有在每个方向上都有权重的特征。根据局部代理，`cocoa_percent`
    值小于或等于 70 是一个有利属性，同样，浆果味道也是。缺乏酸味、甜味和糖浆味也有利于这个模型。然而，缺乏丰富、奶油和可可味的缺乏则相反，但不足以将天平推向
    *不推荐*：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_05_10.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_05_10.png)'
- en: 'Figure 5.10: LIME’s tabular explanation for observation #5 (Outstanding)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.10：LIME 对观察 #5（杰出）的表格解释'
- en: 'With a small adjustment to the code that produced *Figure 5.10*, we can produce
    the same plot but for observation #24, as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '通过对生成 *图 5.10* 的代码进行微小调整，我们可以为观察 #24 生成相同的图表，如下所示：'
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here, in *Figure 5.11*, we can clearly see why the local surrogate believes
    that observation #24 is *Not Highly Recommended*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在*图5.11*中，我们可以清楚地看到为什么局部代理认为观察#24是*不强烈推荐*：
- en: '![Graphical user interface, application  Description automatically generated](img/B18406_05_11.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18406_05_11.png)'
- en: 'Figure 5.11: LIME’s tabular explanation for observation #24 (Disappointing)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11：LIME对观察#24（令人失望）的表格解释
- en: 'Once you compare the explanation of #24 (*Figure 5.11*) with that of #5 (*Figure
    5.10*), the problems become evident. A single feature, `tastes_berry`, is what
    differentiates both explanations. Of course, we have limited it to the top eight
    features, so there’s probably much more to it. However, you would expect the top
    eight features to include the ones that make the most difference.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你比较了#24（*图5.11*）和#5（*图5.10*）的解释，问题就变得明显了。一个单一的特征，`tastes_berry`，区分了这两个解释。当然，我们将其限制在前八个特征中，所以可能还有更多。然而，你可能会期望前八个特征包括那些造成最大差异的特征。
- en: 'According to SHAP, knowing that `tastes_earthy=1` is what globally explains
    the disappointing nature of the #24 chocolate bar, but this appears to be counterintuitive.
    So, what happened? It turns out that observations #5 and #24 are relatively similar
    and, thus, in the same neighborhood. This neighborhood also includes many chocolate
    bars with berry tastes and very few with earthy ones. However, there are not enough
    earthy ones to consider it a salient feature, so it attributes the difference
    between *Highly Recommended* and *Not Highly Recommended* to other features that
    seem to differentiate more often, at least locally. The reason for this is twofold:
    the local neighborhood could be too small, and linear models, given their simplicity,
    are on the bias end of a *bias-variance trade-off*. This bias is only exacerbated
    by the fact that some features such as `tastes_berry` can appear relatively more
    often than `tastes_earthy`. There’s an approach we can use to fix this, and we’ll
    cover this in the next section.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SHAP（SHapley Additive exPlanations），知道`tastes_earthy=1`是解释#24号巧克力棒令人失望性质的全局原因，但这看起来似乎与直觉相反。那么，发生了什么？结果发现，观察#5和#24相对相似，因此它们位于同一个邻域。这个邻域还包括许多带有浆果口味的巧克力棒，而带有泥土口味的则非常少。然而，泥土口味的数量不足以将其视为一个显著特征，因此它将“强烈推荐”和“不强烈推荐”之间的差异归因于其他似乎更频繁区分的特征。这种原因有两方面：局部邻域可能太小，而且由于线性模型的简单性，它们在**偏差-方差权衡**的偏差端。这种偏差由于一些特征（如`tastes_berry`）相对于`tastes_earthy`出现频率较高而加剧。我们可以使用一种方法来解决这个问题，我们将在下一节中介绍。
- en: Using LIME for NLP
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LIME进行NLP
- en: 'At the beginning of the chapter, we set aside training and test datasets with
    the cleaned-up contents of all the “tastes” columns for NLP. We can take a peek
    at the test dataset for NLP, as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们为NLP（自然语言处理）设置了清洗后的所有“口味”列的内容的训练集和测试集。我们可以先看看NLP的测试集，如下所示：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This outputs the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下内容：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: No machine learning model can ingest the data as text, so we need to turn it
    into a numerical format—in other words, vectorize it. There are many techniques
    we can use to do this. In our case, we are not interested in the position of words
    in each phrase, nor the semantics. However, we are interested in their relative
    occurrence—after all, that was an issue for us in the last section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 没有机器学习模型能够直接处理文本数据，因此我们需要将其转换为数值格式——换句话说，就是进行向量化。我们可以使用许多技术来完成这项工作。在我们的案例中，我们并不关心每个短语中单词的位置，也不关心语义。然而，我们对其相对出现频率感兴趣——毕竟，这是我们上一节中遇到的问题。
- en: 'For these reasons, **Term Frequency-Inverse Document Frequency** (**TF-IDF**)
    is the ideal method because it’s meant to evaluate how often a term (each word)
    appears in a document (each phrase). However, it’s weighted according to its frequency
    in the entire corpus (all phrases). We can easily vectorize our datasets using
    the TF-IDF method with `TfidfVectorizer` from scikit-learn. However, when you
    have to make TF-IDF scores, these are fitted to the training dataset only because
    that way, the transformed train and test datasets have consistent scoring for
    each term. Have a look at the following code snippet:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，**词频-逆文档频率**（**TF-IDF**）是理想的方法，因为它旨在评估一个术语（每个单词）在文档（每个短语）中出现的频率。然而，它的权重是根据其在整个语料库（所有短语）中的频率来确定的。我们可以使用scikit-learn中的`TfidfVectorizer`方法轻松地将我们的数据集向量化。然而，当你需要制作TF-IDF分数时，这些分数仅适用于训练集，因为这样，转换后的训练集和测试集对每个术语的评分是一致的。请看以下代码片段：
- en: '[PRE28]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To get an idea of what the TF-IDF score looks like, we can place all the feature
    names in one column of a DataFrame, and their respective scores for a single observation
    in another. Note that since the vectorizer produces a `scipy` sparse matrix, we
    have to convert it into a `numpy` matrix with `todense()` and then a `numpy` array
    with `asarray()`. We can sort this DataFrame in descending order by TD-IDF scores.
    The code is shown in the following snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解TF-IDF分数的样子，我们可以将所有特征名称放在DataFrame的一列中，并将单个观察到的相应分数放在另一列中。请注意，由于向量器生成的是`scipy`稀疏矩阵，我们必须使用`todense()`将其转换为`numpy`矩阵，然后使用`asarray()`转换为`numpy`数组。我们可以按TD-IDF分数降序排序这个DataFrame。代码如下所示：
- en: '[PRE29]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code produces the output shown here in *Figure 5.12*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下*图5.12*中所示的结果：
- en: '![Table  Description automatically generated](img/B18406_05_12.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_05_12.png)'
- en: 'Figure 5.12: The TF-IDF scores for words present in observation #5'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：观察#5中出现的单词的TF-IDF分数
- en: 'And as you can tell from *Figure 5.12*, the TD-IDF scores are normalized values
    between 0 and 1, and those most common in the corpus have a lower value. Interestingly
    enough, we realize that observation #5 in our tabular dataset had `berry=1` because
    of **raspberry**. The categorical encoding method we used searched occurrences
    of `berry` regardless of whether they matched an entire word or not. This isn’t
    a problem because raspberry is a kind of berry, and raspberry wasn’t one of our
    common tastes with its own binary column.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从*图5.12*中可以看出，TD-IDF分数是介于0和1之间的归一化值，而在语料库中最常见的分数较低。有趣的是，我们意识到我们表格数据集中的观察#5中`berry=1`是因为**覆盆子**。我们使用的分类编码方法搜索了`berry`的出现，无论它们是否匹配整个单词。这不是问题，因为覆盆子是一种浆果，覆盆子并不是我们具有自己二进制列的常见口味之一。
- en: Now that we have vectorized our NLP datasets, we can proceed with the modeling.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将NLP数据集向量化，我们可以继续进行建模。
- en: Training a LightGBM model
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练LightGBM模型
- en: '**LightGBM**, like **XGBoost**, is another very popular and performant gradient-boosting
    framework that leverages boosted-tree ensembles and histogram-based split finding.
    The main differences lie in the split method’s algorithms, which for LightGBM
    uses sampling with **Gradient-Based One-Side Sampling** (**GOSS**) and bundling
    sparse features with **Exclusive Feature Bundling** (**EFB**) versus XGBoost’s
    more rigorous **Weighted Quantile Sketch** and **Sparsity-aware Split Finding**.
    Another difference lies in how the trees are built, which is **depth-first** (top-down)
    for XGBoost and **breadth-first** (across a tree’s leaves) for LightGBM. We won’t
    get into the details of how these algorithms work because that would derail the
    topic at hand. However, it’s important to note that thanks to GOSS, LightGBM is
    usually even faster than XGBoost, and though it can lose predictive performance
    due to GOSS split approximations, it gains some of it back with its best-first
    approach. On the other hand, **Explainable Boosting Machine** (**EBM**) makes
    LightGBM ideal for training on sparse features efficiently and effectively, such
    as those in our `X_train_nlp_fit` sparse matrix! That pretty much sums up why
    we are using LightGBM for this exercise.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**LightGBM**，就像**XGBoost**一样，是另一个非常流行且性能出色的梯度提升框架，它利用了提升树集成和基于直方图的分割查找。主要区别在于分割方法的算法，LightGBM使用**基于梯度的单侧采样**（**GOSS**）和将稀疏特征捆绑在一起使用**独家特征捆绑**（**EFB**），而XGBoost则使用更严格的**加权分位数草图**和**稀疏感知分割查找**。另一个区别在于构建树的方式，XGBoost是**深度优先**（自上而下），而LightGBM是**广度优先**（跨树叶）。我们不会深入探讨这些算法的工作原理，因为这会偏离当前的主题。然而，重要的是要注意，由于GOSS，LightGBM通常比XGBoost还要快，尽管它可能会因为GOSS分割近似而损失一些预测性能，但它通过最佳优先的方法弥补了一些。另一方面，**可解释提升机**（**EBM**）使LightGBM在高效且有效地训练稀疏特征方面变得理想，例如我们`X_train_nlp_fit`稀疏矩阵中的那些特征！这基本上概括了我们为什么在这个练习中使用LightGBM的原因。'
- en: 'To train the LightGBM model, we first initialize the model by setting the maximum
    tree depth (`max_depth`), the learning rate (`learning_rate`), the number of boosted
    trees to fit (`n_estimators`), the `objective`, which is binary classification,
    and—last but not least—the `random_state` for reproducibility. With `fit`, we
    train the model using our vectorized NLP training dataset (`X_train_nlp_fit`)
    and the same labels used for the SVM model (`y_train`). Once trained, we can evaluate
    using the `evaluate_class_mdl` we used with the SVM. The code is illustrated in
    the following snippet:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练LightGBM模型，我们首先通过设置最大树深度（`max_depth`）、学习率（`learning_rate`）、要拟合的增强树数量（`n_estimators`）、`objective`（二分类目标）以及最后但同样重要的是`random_state`（用于可重复性）来初始化模型。使用`fit`，我们使用我们的向量化NLP训练数据集（`X_train_nlp_fit`）和与SVM模型相同的标签（`y_train`）来训练模型。一旦训练完成，我们可以使用与SVM相同的`evaluate_class_mdl`来评估。代码如下所示：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code produces *Figure 5.13*, shown here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图5.13*，如下所示：
- en: '![Chart, line chart  Description automatically generated](img/B18406_05_13.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_05_13.png)'
- en: 'Figure 5.13: The predictive performance of our LightGBM model'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：我们LightGBM模型的预测性能
- en: '*Figure 5.13* shows the performance achieved by LightGBM is slightly lower
    than for the SVM (*Figure 5.3*) but it’s still pretty good, safely above the coin-toss
    line. The comments for the SVM about favoring precision over recall for this model
    also apply here.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.13*显示LightGBM达到的性能略低于SVM（*图5.3*），但仍然相当不错，安全地高于抛硬币的线。关于SVM的注释，即在此模型中优先考虑精确度而非召回率，也适用于此处。'
- en: Local interpretation for a single prediction at a time using LimeTextExplainer
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LimeTextExplainer对单个预测进行局部解释
- en: 'To interpret any black-box model prediction with LIME, we need to specify a
    classifier function such as `predict_proba` for your model, and it will use this
    function to make predictions with perturbed data in the neighborhood of your instance
    and then train a linear model with it. The instance must be in its numerical form—in
    other words, vectorized. However, it would be easier if you could provide any
    arbitrary text, and it could then vectorize it on the fly. This is precisely what
    a pipeline can do for us. With the `make_pipeline` function from scikit-learn,
    you can define a sequence of estimators that transform the data, followed by one
    that can fit it. In this case, we just need `vectorizer` to transform our data,
    followed by our LightGBM model (`lgb_mdl`) that takes the transformed data, as
    illustrated in the following code snippet:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用LIME解释任何黑盒模型预测，我们需要指定一个分类器函数，例如模型的`predict_proba`，它将使用此函数在实例邻域内使用扰动数据做出预测，然后使用它训练一个线性模型。实例必须是数值形式——换句话说，是向量化的。然而，如果你能提供任何任意文本，并且它可以在飞行中将其向量化，那就更容易了。这正是管道为我们所做的事情。使用scikit-learn的`make_pipeline`函数，你可以定义一系列转换数据的估计器，然后是一个可以拟合数据的估计器。在这种情况下，我们只需要`vectorizer`转换我们的数据，然后是我们的LightGBM模型（`lgb_mdl`），它接受转换后的数据，如下面的代码片段所示：
- en: '[PRE31]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Initializing a `LimeTextExplainer` is pretty simple. All parameters are optional,
    but it’s recommended to specify names for your classes. Just as with `LimeTabularExplainer`,
    a `kernel_width` optional parameter can be critical because it defines the neighborhood’s
    size, and there’s a default that may not be optimal but can be tuned on an instance-by-instance
    basis. The code is illustrated here:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个`LimeTextExplainer`相当简单。所有参数都是可选的，但建议指定类名。就像`LimeTabularExplainer`一样，一个可选的`kernel_width`参数可能非常关键，因为它定义了邻域的大小，默认值可能不是最优的，但可以根据实例进行调整。代码如下所示：
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Explaining an instance with `LimeTextExplainer` is similar to doing it for
    `LimeTabularExplainer`. The difference is that we are using a pipeline (`lgb_pipeline`),
    and the data we are providing (first parameter) is text since the pipeline can
    transform it for us. The code is illustrated in the following snippet:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`LimeTextExplainer`解释实例与对`LimeTabularExplainer`进行操作类似。区别在于我们使用了一个管道（`lgb_pipeline`），而我们提供的数据（第一个参数）是文本，因为管道可以为我们转换它。代码如下所示：
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'According to the LIME text explainer (see *Figure 5.14*), the model predicts
    *Highly Recommended* for observation #5 because of the word **caramel**. At least
    according to the local neighborhood, **raspberry** is not a factor. In this case,
    the local surrogate is making a different prediction from the LightGBM model.
    In some cases, the LIME prediction disagrees with the model prediction. If the
    disagreement rate is too high, we would refer to this as “low fidelity.”'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 根据LIME文本解释器（见图5.14），模型因为**焦糖**这个词预测观察#5为**强烈推荐**。至少根据局部邻域，**覆盆子**不是一个因素。在这种情况下，局部代理模型做出了与LightGBM模型不同的预测。在某些情况下，LIME的预测与模型预测不一致。如果不一致率太高，我们将称之为“低保真度”。
- en: '![Chart, waterfall chart  Description automatically generated](img/B18406_05_14.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图 描述自动生成](img/B18406_05_14.png)'
- en: 'Figure 5.14: LIME’s text explanation for observation #5 (Outstanding)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14：LIME对观察#5（杰出）的文本解释
- en: 'Now, let’s contrast the interpretation for observation #5 with that of #24,
    as we’ve done before. We can use the same code but simply replace `5` with `24`,
    as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将观察#5的解释与之前所做的观察#24的解释进行对比。我们可以使用相同的代码，但只需将`5`替换为`24`，如下所示：
- en: '[PRE34]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'According to *Figure 5.15*, you can tell that observation #24, described as
    tasting like **burnt wood earthy choco**, is *Not Highly Recommended* because
    of the words **earthy** and **burnt**:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图5.15，你可以看出观察#24，描述为尝起来像**烧焦的木头土质巧克力**，因为**土质**和**烧焦**这两个词而被标记为**不推荐**：
- en: '![Chart, waterfall chart  Description automatically generated](img/B18406_05_15.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图表，瀑布图 描述自动生成](img/B18406_05_15.png)'
- en: 'Figure 5.15: LIME’s tabular explanation for observation #24 (Disappointing)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：LIME对观察#24（令人失望）的表格解释
- en: 'Given that we are using a pipeline that can vectorize any arbitrary text, let’s
    have some fun with that! We will first try a phrase made out of adjectives we
    suspect that our model favors, then try one with unfavorable adjectives, and lastly,
    try using words that our model shouldn’t be familiar with, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们使用的是一个可以将任何任意文本矢量化的小管道，那么让我们来点乐趣吧！我们首先尝试一个由我们怀疑模型偏好的形容词组成的短语，然后尝试一个由不受欢迎的形容词组成的短语，最后尝试使用模型不应该熟悉的词，如下所示：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In *Figure 5.16*, the explanations are spot-on for **creamy rich complex fruity**
    and **sour bitter roasty molasses** since the model knows these words to be either
    very favorable or unfavorable. These words are also common enough to be appreciated
    on a local level.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.16中，解释对于**奶油丰富复杂果味**和**酸苦烤焦糖浆**非常准确，因为模型知道这些词要么非常受欢迎，要么不受欢迎。这些词也足够常见，可以在局部层面上得到欣赏。
- en: 'You can see the output here:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到输出：
- en: '![Graphical user interface  Description automatically generated](img/B18406_05_16.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 描述自动生成](img/B18406_05_16.png)'
- en: 'Figure 5.16: Arbitrary phrases not in the training or test dataset can be effortlessly
    explained with LIME, as long as words are in the corpus'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16：LIME可以轻松解释不在训练或测试数据集中的任意短语，只要这些词在语料库中
- en: However, you’d be mistaken to think that the prediction of *Not Highly Recommended*
    for **nasty disgusting gross stuff** has anything to do with the words. The LightGBM
    model hasn’t seen these words before, so the prediction has more to do with *Not
    Highly Recommended* being the majority class, which is a good guess, and the sparse
    matrix for this phrase is all zeros. Therefore, LIME likely found few distant
    points—if any at all—in its neighborhood, so the zero coefficients of LIME’s local
    surrogate model reflect this.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，认为对**令人厌恶的恶心恶心的东西**预测为**不推荐**与这些词有任何关系的想法是错误的。LightGBM模型之前从未见过这些词，因此预测更多与**不推荐**是多数类有关，这是一个不错的猜测，并且这个短语的稀疏矩阵全是零。因此，LIME可能在其邻域中找到了很少的远点——如果有的话，所以LIME的局部代理模型的零系数反映了这一点。
- en: Trying SHAP for NLP
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试SHAP用于NLP
- en: 'Most of SHAP’s explainers will work with tabular data. `DeepExplainer` can
    do text but is restricted to deep learning models, and, as we will cover in *Chapter
    7*, *Visualizing Convolutional Neural Networks*, three of them do images, including
    `KernelExplainer`. In fact, SHAP’s `KernelExplainer` was designed to be a general-purpose,
    truly model-agnostic method, but it’s not promoted as an option for NLP. It is
    easy to understand why: it’s slow, and NLP models tend to be very complex and
    with hundreds—if not thousands—of features to boot. In cases such as this one,
    where word order is not a factor and you have a few hundred features, but the
    top 100 are present in most of your observations, `KernelExplainer` could work.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to overcoming the high computation cost, there are a couple of
    technical hurdles you would need to overcome. One of them is that `KernelExplainer`
    is compatible with a pipeline, but it expects a single set of predictions back.
    But LightGBM returns two sets, one for each class: *Not Highly Recommended* and
    *Highly Recommended*. To overcome this problem, we can create a `lambda` function
    (`predict_fn`) that includes a `predict_proba` function, which returns only those
    predictions for *Highly Recommended*. This is illustrated in the following code
    snippet:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The second technical hurdle is to do with SHAP’s incompatibility with SciPy’s
    sparse matrices, and for our explainer, we will need sample vectorized test data,
    which is in this format. To overcome this issue, we can convert our data in the
    SciPy sparse-matrix format in to a `numpy` matrix and then in to a `pandas` DataFrame
    (`X_test_nlp_samp_df`). To overcome any slowness, we can use the same `kmeans`
    trick we used last time. Other than the adjustments made to overcome obstacles,
    the following code is exactly the same as SHAP performed with the SVM model:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: By using SHAP’s summary plot in *Figure 5.17*, you can tell that globally the
    words **creamy**, **rich**, **cocoa**, **fruit**, **spicy**, **nutty**, and **berry**
    have a positive impact on the model toward predicting *Highly Recommended*. On
    the other hand, **sweet**, **sour**, **earthy**, **hammy**, **sandy**, and **fatty**
    have the opposite effect. These results shouldn’t be entirely unexpected given
    what we learned with our prior SVM model, with the tabular data and local LIME
    interpretations. That being said, the SHAP values were derived from samples of
    a sparse matrix, and they could be missing details and perhaps even be partially
    incorrect, especially for underrepresented features. Therefore, we should take
    the conclusions with a grain of salt, especially toward the bottom half of the
    plot. To increase the interpretation fidelity, it’s best to increase sample size,
    but given the slowness of `KernelExplainer`, there’s a trade-off to consider.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'You can view the output here:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B18406_05_17.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: The SHAP summary plot for the LightGBM NLP model'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have validated our SHAP values globally, we can use them for local
    interpretation with a force plot. Unlike LIME, we cannot use arbitrary data for
    this. With SHAP, we are limited to those data points we have previously generated
    SHAP values for. For instance, let’s take the 18th observation from our test dataset
    sample, as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code outputs this phrase:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It’s important to note which words are represented in the 18th observation
    because the `X_test_nlp_samp_df` DataFrame contains the vectorized representation.
    The 18th observation’s row in this DataFrame is what you use to generate the force
    plot, along with the SHAP values for this observation and the expected value for
    the class, as illustrated in the following code snippet:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*Figure 5.18* is the force plot for **woody earthy medicinal**. As you can
    tell, **earthy** and **woody** weigh heavily in a prediction against *Highly Recommended*.
    The word **medicinal** is not featured in the force plot and instead you get a
    lack of **creamy** and **cocoa** as negative factors. As you can imagine, *medicinal*
    is not a word used often to describe chocolate bars, so there was only one observation
    in the sampled dataset that included it. Therefore, its average marginal contribution
    across possible coalitions would be greatly diminished:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, timeline  Description automatically generated](img/B18406_05_18.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: The SHAP force plot for the 18th observation of the sampled test
    dataset'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try another one, as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The 9th observation is the following phrase:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Generating a `force_plot` for this observation is the same as before, except
    you replace `18` with `9`. If you run this code, you produce the output shown
    here in *Figure 5.19*:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Timeline  Description automatically generated](img/B18406_05_19.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19: The SHAP force plot for the 9th observation of the sampled test
    dataset'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can appreciate in *Figure 5.19*, all the words in the phrase are featured
    in the force plot: **floral** and **spicy** pushing toward *Highly Recommended*,
    and **intense** toward *Not Highly Recommended*. So, now you know how to perform
    both tabular and NLP interpretations with SHAP, how does it compare with LIME?'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Comparing SHAP with LIME
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you will have noticed by now, both SHAP and LIME have limitations, but they
    also have strengths. SHAP is grounded in game theory and approximate Shapley values,
    so its SHAP values are supported by theory. These have great properties such as
    additivity, efficiency, and substitutability that make them consistent but violate
    the dummy property. It always adds up and doesn’t need parameter tuning to accomplish
    this. However, it’s more suited for global interpretations, and one of its most
    model-agnostic explainers, `KernelExplainer`, is painfully slow. `KernelExplainer`
    also deals with missing values by using random ones, which can put too much weight
    on unlikely observations.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: LIME is speedy, very model-agnostic, and adaptable to all kinds of data. However,
    it’s not grounded on strict and consistent principles but has the intuition that
    neighbors are alike. Because of this, it can require tricky parameter tuning to
    define the neighborhood size optimally, and even then, it’s only suitable for
    local interpretations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Mission accomplished
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mission was to understand why one of your client’s bars is *Outstanding*
    while another one is *Disappointing*. Your approach employed the interpretation
    of machine learning models to arrive at the following conclusions:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: According to SHAP on the tabular model, the *Outstanding* bar owes that rating
    to its berry taste and its cocoa percentage of 70%. On the other hand, the unfavorable
    rating for the *Disappointing* bar is due mostly to its earthy flavor and bean
    country of origin (`Other`). Review date plays a smaller role, but it seems that
    chocolate bars reviewed in that period (2013–15) were at an advantage.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME confirms that `cocoa_percent<=70` is a desirable property, and that, in
    addition to **berry**, **creamy**, **cocoa**, and **rich** are favorable tastes,
    while **sweet**, **sour**, and **molasses** are unfavorable.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The commonality between both methods using the tabular model is that despite
    the many non-taste-related attributes, taste features are among the most salient.
    Therefore, it’s only fitting to interpret the words used to describe each chocolate
    bar via an NLP model.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Outstanding* bar was represented by the phrase **oily nut caramel raspberry**,
    of which, according to `LIMETextExplainer`, **caramel** is positive and **oily**
    is negative. The other two words are neutral. On the other hand, the *Disappointing*
    bar was represented by **burnt wood earthy choco**, of which **burnt** and **earthy**
    are unfavorable and the other two are favorable.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inconsistencies between the tastes in tabular and NLP interpretations are
    due to the presence of lesser-represented tastes, including **raspberry**, which
    is not as common as **berry**.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to SHAP’s global explanation of the NLP model, **creamy**, **rich**,
    **cocoa**, **fruit**, **spicy**, **nutty**, and **berry** have a positive impact
    on the model toward predicting *Highly Recommended*. On the other hand, **sweet**,
    **sour**, **earthy**, **hammy**, **sandy**, and **fatty** have the opposite effect.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these notions of which chocolate bar characteristics and tastes are considered
    less attractive by *Manhattan Chocolate Society* members, a client can apply changes
    to their chocolate bar formulas to appeal to a broader audience—that is, if the
    assumption is correct about that group being representative of their target audience.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: It could be argued that it is pretty apparent that words such as **earthy**
    and **burnt** are not favorable words to associate with chocolate bars, while
    **caramel** is. Therefore, we could have reached this conclusion without machine
    learning! But first of all, a conclusion not informed by data would have been
    an opinion, and, secondly, context is everything. Furthermore, humans can’t always
    be relied upon to place one point objectively in its context—especially considering
    it’s among thousands of records!
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Also, local model interpretation is *not only about the explanation for one
    prediction* because it’s connected to how a model makes all predictions but, more
    importantly, how it makes predictions for similar points—in other words, in the
    local neighborhood! In the next chapter, we will expand on what it means to be
    in the local neighborhood by looking at the commonalities (*anchors*) and inconsistencies
    (*counterfactuals*) we can find there.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use SHAP’s `KernelExplainer`, as well as
    its decision and force plot to conduct local interpretations. We carried out a
    similar analysis using LIME’s instance explainer for both tabular and text data.
    Lastly, we looked at the strengths and weaknesses of SHAP’s `KernelExplainer`
    and LIME. In the next chapter, we will learn how to create even more human-interpretable
    explanations of a model’s decisions, such as *if X conditions are met, then Y
    is the outcome*.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Dataset sources
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Brelinski, Brady (2020). *Manhattan Chocolate Society*: [http://flavorsofcacao.com/mcs_index.html](http://flavorsofcacao.com/mcs_index.html)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Platt, J. C., 1999, *Probabilistic Outputs for Support Vector Machines and
    Comparisons to Regularized Likelihood Methods*. Advances in Large Margin Classifiers,
    MIT Press: [https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lundberg, S. and Lee, S., 2017, *A Unified Approach to Interpreting Model Predictions*.
    [https://arxiv.org/abs/1705.07874](https://arxiv.org/abs/1705.07874) (documentation
    for SHAP: [https://github.com/slundberg/shap](https://github.com/slundberg/shap))'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro, M. T., Singh, S., and Guestrin, C., 2016, *“Why Should I Trust You?”:
    Explaining the Predictions of Any Classifier*. Proceedings of the 22^(nd) ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining: [http://arxiv.org/abs/1602.04938](http://arxiv.org/abs/1602.04938)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribeiro, M. T., Singh, S., and Guestrin, C., 2016, *“Why Should I Trust You?”:
    Explaining the Predictions of Any Classifier*. Proceedings of the 22^(nd) ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining: [http://arxiv.org/abs/1602.04938](http://arxiv.org/abs/1602.04938)'
- en: 'Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu,
    T., 2017, *LightGBM: A Highly Efficient Gradient Boosting Decision Tree*. Advances
    in Neural Information Processing Systems vol. 30, pp. 3149–3157: [https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu,
    T., 2017, *LightGBM: A Highly Efficient Gradient Boosting Decision Tree*. Advances
    in Neural Information Processing Systems vol. 30, pp. 3149–3157: [https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)'
- en: Learn more on Discord
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的 Discord 社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_5.xhtml)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_5.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code107161072033138125.png)'
