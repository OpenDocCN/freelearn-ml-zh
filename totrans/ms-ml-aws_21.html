<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deploying Models Built in AWS</h1>
                </header>
            
            <article>
                
<p>At this point, we have our models built in AWS and would like to ship them to production. We know that there is a variety of different contexts in which models should be deployed. In some cases, it's as easy as generating a CSV of actions that would be fed to some system. Often we just need to deploy a web service capable of making predictions. However, there are special circumstances in which we need to deploy these models to complex, low-latency, or edge systems. In this chapter, we will look at the different ways to deploy machine learning models to production.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>SageMaker model deployment</li>
<li>Apache Spark model deployment</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SageMaker model deployment</h1>
                </header>
            
            <article>
                
<p>In <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a><span>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Classifying Twitter Feeds with Naive Bayes</span></span></em>, we deployed our first model with SageMaker. At that point, we had trained our classifier using <strong>BlazingText</strong> and stored it in a variable called <kbd>bt_model</kbd>. To deploy the model, we just need to call the <kbd>deploy</kbd> method stating the number and kinds of machines to use:</p>
<pre>bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')</pre>
<div class="packt_infobox">
<p>SageMaker can balance the requests made to the endpoint across the number of instances and automatically scale up or down the depending on the service load. Details can be found at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html">https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html</a>.</p>
</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Once we invoke the <kbd>deploy</kbd> method, an endpoint should appear in the AWS SageMaker console at</span> <a href="https://console.aws.amazon.com/sagemaker">https://console.aws.amazon.com/sagemaker</a><span>. The following screenshot shows the endpoint for our BlazingText example:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-678 image-border" src="assets/312a1b7d-c26a-4093-adc1-bf6501be796d.png" style="width:162.58em;height:32.67em;"/></p>
<p>By clicking on the endpoint in the console, we can find further details:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-679 image-border" src="assets/3624986e-15d0-4dbe-8000-eb3eb6fe5240.png" style="width:166.08em;height:61.67em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">In particular, we can see that the endpoint has a specific URL in which the service is hosted. If we attempt to call this URL directly via HTTP tools, such as <kbd>curl</kbd>, we would get the following result:</p>
<pre>curl -X POST \<br/>&gt; https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/blazingtext-endpoint-2019-01-04-01/invocations \<br/>&gt; -H 'cache-control: no-cache' \<br/>&gt; -H 'content-type: application/json' \<br/>&gt; -H 'postman-token: 7hsjkse-f24f-221e-efc9-af4c654d677a' \<br/>&gt; -d '{"instances": ["This new deal will be the most modern, up-to-date, and balanced trade agreement in the history of our country, with the most advanced protections for workers ever developed"]}'</pre>
<div class="packt_quote">{"message":"Missing Authentication Token"}</div>
<p>This is because every request made to SageMaker endpoints must be properly signed to ensure authentication. Only users with role permissions to call the Amazon SageMaker InvokeEndpoint API will be allowed to make calls to SageMaker endpoints. In order for the HTTP service behind SageMaker to be able to identify and authenticate the caller, the <kbd>http</kbd> request needs to be properly signed. More information about signing requests can be found at <a href="https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html">https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html</a>. An alternative to signing the requests—if we want to expose our model endpoint publicly—would be to create a lambda function in AWS and expose it behind an API Gateway. More information about how to do that can be found here: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html">https://docs.aws.amazon.com/sagemaker/latest/dg/getting-started-client-app.html</a>.</p>
<p>Fortunately, if we are calling the endpoint from within an AWS instance, we can avoid manually signing the requests by using the <kbd>sagemaker</kbd> library. Let's recap how such calls can be made.</p>
<p>As usual, we first import the necessary Python libraries:</p>
<pre>import sagemaker<br/>from sagemaker import get_execution_role<br/><br/>sess = sagemaker.Session()<br/>role = get_execution_role()</pre>
<p>Next, if we know the name of the endpoint, we can create a <kbd>RealTimePredictor</kbd> instance in order to make real-time predictions:</p>
<pre>from sagemaker.predictor import json_serializer, RealTimePredictor<br/><br/>predictor = RealTimePredictor(endpoint='blazingtext-endpoint-2019-01-04-01', serializer=json_serializer)</pre>
<p><span>In this case, we are using <kbd>json_serializer</kbd>, which is a convenient and human-readable format for our example. To invoke the endpoint, we just need to call the <kbd>predict()</kbd> method:</span></p>
<pre>predictor.predict({"instances": ["This new deal will be the most modern, up-to-date, and balanced trade agreement in the history of our country, with the most advanced protections for workers ever developed"]})</pre>
<p><span>Here is the output:</span></p>
<pre>b'[{"prob": [0.5000401735305786], "label": ["__label__1"]}]'</pre>
<p>You can go back to <a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a><span>,</span><span> </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Classifying Twitter Feeds with Naive Bayes</span></span></em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">,</span></span> for an interpretation of this output, but the important point here is that the <kbd>RealTimePredictor</kbd> instance did all the proper authentication, request signing, and endpoint invocation on our behalf.</p>
<p>In addition to the URL and basic information about the endpoint, the AWS console also shows the endpoint configuration:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-680 image-border" src="assets/87d39a59-79e7-4d24-a8b5-49e2182db1cd.png" style="width:162.92em;height:63.42em;"/></p>
<p>Through the configuration, we can follow the model and training job that originated from this endpoint. Let's follow the link to inspect the originating model. We then get the following screen:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-681 image-border" src="assets/8bfb8d93-53da-47e1-bd60-5b3e3263f15b.png" style="width:165.58em;height:96.08em;"/></p>
<p>In the model description, we can find details such as the S3 location of the model. This model serialization is specific to each kind of model. In <a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml">Chapter 4</a><span>, </span><em>Predicting User Behavior with Tree-Based Methods</em>, we saw that the format of such a model was conveniently in an <kbd>xgboost</kbd> pickle-serialized-compatible format.</p>
<p>You may also have noticed that there is an image associated to this model. SageMaker creates an image of the machine that hosts this model in the Amazon <strong>Elastic Container Registry</strong> (<strong>ECR</strong>). Typically these are Docker images under the hood.</p>
<div class="packt_infobox">The following link is a great resource on the inner workings of deployment and how containerization works within SageMaker: <a href="https://sagemaker-workshop.com/custom/containers.html">https://sagemaker-workshop.com/custom/containers.html</a>.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark model deployment</h1>
                </header>
            
            <article>
                
<p>Apache Spark does not come with an out-of-the-box method for exposing models as endpoints, like SageMaker does. However, there are easy ways to load Spark models on standard web services using the serialization and deserialization capabilities of Spark's ML package. In this section, we will show how to deploy the model we created in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a><span>, </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Predicting House Value with Regression Algorithms</span></span></em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">,</span></span> to serve predictions through a simple endpoint. To do this, we will save a trained model to disk so that we can ship that model to the machine that is serving the model through an endpoint.</p>
<p>We'll start by training our model. In <span class="cdp-organizer-chapter-number"><a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, </span><em><span class="cdp-organizer-chapter-title"><span class="cdp-organize-title-label">Predicting House Value with Regression Algorithms</span></span></em>, we loaded the housing data into a dataframe:</p>
<pre>housing_df = sql.read.csv(SRC_PATH + 'train.csv', <br/>                          header=True, inferSchema=True)</pre>
<p><span>To simplify this example, we're going to use a reduced set of features to build a model that will be exposed as an endpoint. Of all the features, we are going to select just three training features (<kbd>crim</kbd>, <kbd>zn</kbd>, and <kbd>indus</kbd>):</span></p>
<pre>reduced_housing_df = housing_df.select(['crim', 'zn', 'indus', 'medv'])</pre>
<p><span>You might recall that <kbd>medv</kbd> was the actual house value (which is the value we're trying to predict). Now that we have our dataframe, we can create a <kbd>pipeline</kbd> just like we did before:</span></p>
<pre class="mce-root">from pyspark.ml import Pipeline<br/>from pyspark.ml.regression import LinearRegression<br/>from pyspark.ml.feature import VectorAssembler<br/><br/>training_features = ['crim', 'zn', 'indus']<br/>vector_assembler = VectorAssembler(inputCols=training_features,           <br/>               outputCol="features")<br/>linear = LinearRegression(featuresCol="features", labelCol="medv")<br/>pipeline = Pipeline(stages=[vector_assembler, linear])<br/>model = pipeline.fit(reduced_housing_df)</pre>
<p>With the model instance, we can save it to disk by calling the <kbd>save()</kbd> method:</p>
<pre>model.save("file:///tmp/linear-model")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span>This serialized model representation can then be shipped to the location in which we want to serve predictions (for example, a web server). In such a context, we can load back the model by invoking the <kbd>PipelineModel.load()</kbd> static method, as follows:</span></p>
<pre>from pyspark.ml import PipelineModel<br/>loaded_model = PipelineModel.load('/tmp/linear-model')</pre>
<p><span>Let's use this model to obtain predictions for the first few rows of our reduced dataset:</span></p>
<pre>loaded_model.transform(reduced_housing_df.limit(3)).show()</pre>
<p>The output of the preceding command is as follows:</p>
<div class="mce-root">
<pre>+-------+----+-----+----+-------------------+------------------+<br/>| crim  | zn |indus|medv| features          | prediction       |<br/>+-------+----+-----+----+-------------------+------------------+<br/>|0.00632|18.0| 2.31|24.0|[0.00632,18.0,2.31]|27.714445239256854|<br/>|0.02731| 0.0| 7.07|21.6| [0.02731,0.0,7.07]|24.859566163416336|<br/>|0.03237| 0.0| 2.18|33.4| [0.03237,0.0,2.18]| 26.74953947801712|<br/>+-------+----+-----+----+-------------------+------------------+</pre></div>
<p>Look at how the <kbd>pipeline</kbd> model started from the raw CSV and applied all the transformation steps in the pipeline to finish with a prediction. Of course, it's not as interesting to obtain predictions from our training dataset. Realistically, on an endpoint serving predictions, we want to receive arbitrary values of our three features and obtain a prediction. At the time of this writing, Apache Spark can only obtain predictions given a dataframe. So, each time we want to obtain predictions for a few values, we need to construct a dataframe, even if we just need to find the prediction for a single row.</p>
<p>Suppose we want to find the prediction for this combination of features: <kbd>crim=0.00632</kbd>, <kbd>zn=18.0</kbd>, <kbd>indus=2.31</kbd>. The first step is to define the schema of our features as Spark will expect the dataframe to be in the exact format that was used for training.</p>
<p>We define the schema as follows:</p>
<pre>from pyspark.sql.types import *<br/><br/>schema = StructType([StructField('crim', DoubleType(), True),<br/>                    StructField('zn', DoubleType(), True),<br/>                    StructField('indus', DoubleType(), True)])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the preceding schema definition, we place the names and types of each field. With the schema in place, we can construct a one-row dataframe with the feature values we're interested in:</p>
<pre>from pyspark.sql import Row<br/><br/>predict_df = <br/>sql.createDataFrame([Row<br/>(crim=0.00632, zn=18.0,<br/>indus=2.31)],<br/>schema=schema)</pre>
<p>This is how the dataframe looks:</p>
<pre>+-------+----+-----+<br/>| crim  | zn |indus|<br/>+-------+----+-----+<br/>|0.00632|18.0| 2.31|<br/>+-------+----+-----+</pre>
<p>With this short dataframe and the loaded model, we can obtain predictions for our arbitrary features:</p>
<pre>loaded_model.transform(predict_df).show()</pre>
<p>Here is the output of the preceding command:</p>
<pre>+-------+----+-----+-------------------+------------------+<br/>| crim  | zn |indus| features          |        prediction|<br/>+-------+----+-----+-------------------+------------------+<br/>|0.00632|18.0| 2.31|[0.00632,18.0,2.31]|27.714445239256854|<br/>+-------+----+-----+-------------------+------------------+</pre>
<p>So, with the preceding ideas in mind, how can we construct an endpoint capable of serving this model? The simplest way is to use packages, such as Flask, that allow us to easily expose an endpoint on any machine of our choice. Details about flask can be found at <a href="http://flask.pocoo.org/">http://flask.pocoo.org</a>. To run a flask web service, we just need to write a Python file that knows how to respond to different endpoint requests. In our case, we will just create one endpoint to respond with a prediction given the values of our three features. We will implement a simple <kbd>GET</kbd> endpoint in which the three features will be passed as URL params.</p>
<p>The call to the service when running on our local host <span>will be as follows</span>:</p>
<pre>curl 'http://127.0.0.1:5000/predict?crim=0.00632&amp;zn=18.0&amp;indus=2.31'</pre>
<p>Here is the output of the service:</p>
<pre><strong>27.71</strong></pre>
<p>To start the flask service on the machine, perform these three steps:</p>
<ol>
<li style="font-weight: 400">Create a python file that specifies how to respond to the endpoint. We will name this file <kbd>deploy_flask.py</kbd>.</li>
<li style="font-weight: 400">Set the <kbd>FLASK_APP</kbd> environment variable to point to the python file we just created.</li>
<li style="font-weight: 400">Run the <kbd>flask run</kbd> command.</li>
</ol>
<p>In <kbd>deploy_flask.py</kbd>, we put together the preceding ideas regarding how to load the model and construct the dataframe for prediction:</p>
<pre>from flask import Flask<br/>from flask import request<br/>from pyspark.ml import PipelineModel<br/>from pyspark.sql import Row<br/>from pyspark.sql.types import *<br/>from pyspark.sql import SQLContext<br/>from pyspark.context import SparkContext<br/><br/>sc = SparkContext('local', 'test')<br/>sql = SQLContext(sc)<br/>app = Flask(__name__)<br/>loaded_model = PipelineModel.load('/tmp/linear-model')<br/><br/>schema = StructType([StructField('crim', DoubleType(), True),<br/>                    StructField('zn', DoubleType(), True),<br/>                    StructField('indus', DoubleType(), True)])<br/><br/>@app.route('/predict', methods=['GET'])<br/>def predict():<br/>   crim = float(request.args.get('crim'))<br/>   zn = float(request.args.get('zn'))<br/>   indus = float(request.args.get('indus'))<br/>   predict_df = sql.createDataFrame([Row(crim=crim, zn=zn, indus=indus)],schema=schema)<br/>   prediction = loaded_model.transform(predict_df).collect()[0].prediction<br/>   return str(prediction)</pre>
<p>The only new parts in the <kbd>deploy_flask.py</kbd> file are the initialization of the flask app and the definition of the <kbd>predict</kbd> method, in which we extract the three features granted as URL params. Next, we set the mentioned environmental variable and run the service:</p>
<pre>export FLASK_APP=deploy_flask.py<br/>flask run</pre>
<p>In the logs, you can see how the service and Spark are initialized, as well as calls made to the service:</p>
<div>
<pre>* Serving Flask app "deploy_flask.py"<br/>* Environment: production<br/>  WARNING: Do not use the development server in a production environment.<br/>  Use a production WSGI server instead.<br/>* Debug mode: off<br/>Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties<br/>Setting default log level to "WARN".<br/>* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)<br/>127.0.0.1 - - [13/Apr/2019 19:13:03] "GET /predict?crim=0.00632&amp;zn=18.0&amp;indus=2.31 HTTP/1.1" 200 -</pre></div>
<p>As the flask logs mention, if you are thinking about serious production load, consider running flask behind a WSGI server. More information about this can be found in the flask documentation.</p>
<p>SageMaker is also able to host any arbitrary model. To do so, we need to create a Docker image that responds to two endpoints: <kbd>/ping</kbd> and <kbd>/invocations</kbd>. It's that simple. In our case, the <kbd>/invocations</kbd> endpoint would use the loaded Spark model to respond with the predictions. Once the Docker image is created, we need to upload it to AWS ECR. As soon as it's loaded on ECR, we can create a SageMaker model just by providing the ECR image identifier.</p>
<p>In the AWS Console (or through the API), choose to create a model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-842 image-border" src="assets/214256d3-0872-4212-9a24-73e1d2fcc383.png" style="width:43.00em;height:33.75em;"/></p>
<p>Once you provide the basic model details, input the ECR location of your custom inference endpoint:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-686 image-border" src="assets/6512da6a-4c59-4b7c-8033-f0e0ee290158.png" style="width:40.25em;height:46.25em;"/></p>
<p>Like any SageMaker model, you can deploy it to an endpoint with the usual means. We won't go through the process of the Docker image creation in this chapter, but notebooks are available at our GitHub repository (<a href="https://github.com/mg-um/mastering-ml-on-aws">https://github.com/mg-um/mastering-ml-on-aws</a>) under <a href="6bc1a319-1195-4c30-8de8-09c795076f10.xhtml">Chapter 16</a>, <em>Deploying Models Built in AWS</em>, that explain how to do so.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Even if your production environment is outside of AWS, SageMaker and Spark in EMR can be of great use, as models can be trained in AWS offline and shipped to a different environment. Also, the artifacts created by AWS as models can usually be obtained and used offline (this was the case for the <kbd>xgboost</kbd> model). If you need to port the Spark ML models to an environment in which you can't instantiate a local Spark session or need a very low-latency predictor, consider using the following tool: <a href="https://github.com/TrueCar/mleap">https://github.com/TrueCar/mleap</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at how models are deployed through SageMaker and covered how the endpoints are defined and invoked. Through the use of Spark's model serialization and deserialization, we illustrated how models can be shipped to other environments, such as a custom web service implementation in flask. Finally, we outlined how your Spark model (or any other arbitrary model) can be served through SageMaker by registering a custom Docker image in AWS ECR.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">Why do SageMaker endpoints respond with a missing authentication token message when you attempt to access the service directly?</li>
<li style="font-weight: 400">Name two alternatives to solve the preceding problem.</li>
<li>Provide two means to deploy a model built on Apache Spark onto an endpoint.</li>
<li>Using our flask example as a basis, construct a Docker image that servers the <kbd>/invocations</kbd> and <kbd>/ping</kbd> endpoint and then deploys a model through SageMaker.</li>
</ol>


            </article>

            
        </section>
    </body></html>