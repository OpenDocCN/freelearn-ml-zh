- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-Class Conformal Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the last chapter of this book, where we delve into the fascinating
    world of multi-class **Conformal Prediction**. This chapter introduces you to
    various conformal prediction methods that can be effectively applied to multi-class
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore the concept of multi-class classification, a common scenario
    in **machine learning** (**ML**), where an instance can belong to one of many
    classes. Understanding this problem is the first step toward applying conformal
    prediction techniques effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will investigate the metrics used to evaluate multi-class classification
    problems. These metrics provide a quantitative measure of the performance of our
    models, and understanding them is crucial for effective model evaluation and selection.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will learn how to apply conformal prediction to multi-class classification
    problems. This section will provide practical insights and techniques to apply
    directly to your industrial applications.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained valuable skills and knowledge
    in multi-class classification and learned how conformal prediction can be effectively
    applied to these problems. So, let’s dive in and start our journey into multi-class
    conformal prediction!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics for multi-class classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How conformal prediction can be applied to multi-class classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-class classification problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In ML, classification problems are ubiquitous. They involve predicting a discrete
    class label output for an instance. While binary classification – predicting one
    of two possible outcomes – is a common scenario, many real-world problems require
    predicting more than two classes. This is where multi-class classification comes
    into play.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification is a problem where an instance can belong to one
    of many classes. For example, consider an ML model designed to categorize news
    articles into topics. The articles could be classified into categories such as
    *Sports*, *Politics*, *Technology*, *Health*, and so on. Each of these categories
    represents a class, and since there are more than two classes, this is a multi-class
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that each instance belongs to exactly one class in multi-class
    classification. If each instance could belong to multiple classes, it would be
    a multi-label classification problem, which is a different kind of problem.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification problems are a staple in ML, and they require a slightly
    different approach than binary classification problems. Let’s dive deeper into
    the intricacies of multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for multi-class classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several ML algorithms can handle multi-class classification problems directly.
    These include, but are not limited to, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision trees**: Decision tree algorithms such as **Classification and Regression
    Trees** (**CARTs**) can naturally handle multi-class classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naive Bayes**: Naive Bayes treats each class as a separate one-versus-all
    binary classification problem and picks the outcome with the highest probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`softmax` activation function can then be used to calculate the probability
    distribution of each class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many ML algorithms are inherently designed for binary classification between
    two classes. Special strategies must be employed to extend these models to multi-class
    problems with more than two categories. Two common approaches are one-vs-all and
    one-vs-one.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore these one-vs-all and one-vs-one strategies in more detail,
    including how binary classification outcomes get aggregated to make final multi-class
    predictions. We will also discuss evaluating multi-class classifiers using specialized
    performance metrics suited for problems with more than two categories.
  prefs: []
  type: TYPE_NORMAL
- en: One-vs-all and one-vs-one strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For algorithms that do not natively support multi-class classification, strategies
    such as one-vs-all (also known as one-vs-rest) and one-vs-one are used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-vs-all strategy**: For a problem with *n* classes, *n* separate binary
    classification models are trained. Each model is trained to distinguish instances
    of one class from instances of all other classes. All *n* models are applied for
    a new instance, and the model that gives the highest confidence score determines
    the instance’s class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-vs-one strategy**: A binary classification model is trained for every
    pair of classes in this strategy. For *n* classes, this results in *n(n-1)/2*
    models. Each model’s decision contributes to a voting scheme, and the class with
    the most votes is chosen as the final class of the instance. For example, for
    a 4-class problem, 4*3/2 = 6 binary models would be built, one for each pair of
    classes. Each model casts a vote for its predicted class, and the class with the
    most votes across all models is chosen as the final prediction. So, with four
    classes, if three models predicted class A, two predicted class B, and one predicted
    class C, class A would be selected since it received the most votes. In this way,
    each model’s decision contributes to a voting scheme to determine the overall
    predicted class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following section will discuss the metrics used for evaluating multi-class
    classification problems. Understanding these metrics is crucial for assessing
    the performance of our models and making informed decisions about model selection
    and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for multi-class classification problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the multi-class classification field, evaluating models’ performance is as
    crucial as developing them. Effective evaluation hinges upon utilizing the right
    metrics that can accurately measure the performance of the multi-class classification
    models and provide insights for improvement. This section demystifies the various
    metrics essential for assessing the performance of multi-class classification
    models, providing a solid foundation for selecting and employing the right metric
    for your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the fundamental metrics for evaluating multi-class classification models
    is the **confusion matrix.** It provides a visualization of the performance of
    an algorithm, typically a **supervised learning** (**SL**) one. Each row of the
    confusion matrix represents the instances of an actual class, while each column
    represents the instances of a predicted class. It’s an essential tool for understanding
    the model’s performance beyond overall accuracy, offering insight into classification
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Precision** (or **positive predictive value**; **PPV** in short) is a measure
    that examines the number of true positive predictions among the total positive
    predictions made by the model. High precision indicates that the false positive
    rate is low. For multi-class classification problems, precision is calculated
    for each class separately and can be averaged to understand the overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Recall** (or **sensitivity** or **true positive rate**; **TPR** in short)
    gauges the number of true positive predictions among the actual positives. It
    is a crucial metric for problems where identifying all actual positives is essential.
    As with precision, recall is calculated for each class and can be averaged for
    overall performance assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: F1 score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**F1 score** is the harmonic mean of precision and recall, balancing the two
    metrics. It is particularly useful when dealing with imbalanced datasets, offering
    a more holistic view of the model’s performance beyond accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Macro- and micro-averaged metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In multi-class classification problems, averaging metrics such as precision,
    recall, and F1 score, commonly known as macro- and micro-averaging, can be done
    in multiple ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Macro-averaging** computes the metric independently for each class and then
    takes the average, treating all classes equally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Micro-averaging** aggregates the contributions of all classes to compute
    the average metric'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area Under Curve (AUC-ROC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important metric is the **Area Under the Receiver Operating Characteristic
    Curve** (**AUC-ROC**). While it is primarily used for binary classification problems,
    it can be extended to multi-class classification by considering each class against
    the rest.
  prefs: []
  type: TYPE_NORMAL
- en: Log loss and its application in measuring calibration of multi-class models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Log loss, also known as **logistic loss** or **cross-entropy loss**, is a commonly
    used loss function for classification problems, including multi-class classification.
    It quantifies the performance of a classification model by measuring the uncertainty
    in the predictions. Log loss assigns a penalty for incorrect classifications;
    the penalty is higher for confidently wrong predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Mathematically, log loss for multi-class classification can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: −  1 _ N  ∑ i=1 N ∑ j=1 M y ij log(p ij)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following apply:'
  prefs: []
  type: TYPE_NORMAL
- en: '*N* is the number of observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*M* is the number of classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y ij is a binary indicator for whether class *j* is the correct classification
    for observation i.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p ij is the predicted probability that observation *i* is of class *j*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using log loss to measure calibration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A calibrated model is one whose predicted probabilities reliably reflect the
    true likelihood of the predicted outcomes. Calibration in multi-class classification
    means that if a model predicts a class with a probability *p*, then that class
    should occur about *p* percent of the time among all instances where that class
    is predicted with probability *p*.
  prefs: []
  type: TYPE_NORMAL
- en: Log loss and calibration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Log loss is an appropriate metric to assess the calibration of a multi-class
    classification model because it directly compares the predicted probabilities
    (the confidence of the predictions) with the actual classes. A well-calibrated
    model will have a lower log loss as the predicted probabilities for the actual
    classes will be higher.
  prefs: []
  type: TYPE_NORMAL
- en: How to evaluate calibration using Log loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Predict the probabilities**: Use your multi-class classification model to
    predict the probabilities for each class for each observation in a validation
    dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute log loss**: Calculate the log loss using the formula previously shown'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpret the result**: A lower log loss value indicates better calibration
    as it shows that the predicted probabilities are closer to the actual classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the log loss of your multi-class classification model gives you insights
    into the calibration of the model. A model with a lower log loss is more calibrated,
    providing more reliable prediction probability estimates. Understanding and using
    log loss as a metric to measure the calibration is vital to ensure that your multi-class
    classification model performs optimally in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Brier score and its application in measuring the calibration of multi-class
    models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Brier score, or quadratic loss, is another popular metric used to evaluate
    the performance of classification models, including multi-class classification
    problems. It quantifies the difference between the predicted probabilities and
    the actual classes, assigning a lower score to better-calibrated models.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For multi-class classification, the Brier score is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 _ N  ∑ i=1 N ∑ j=1 M (p ij − o ij) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following apply:'
  prefs: []
  type: TYPE_NORMAL
- en: '*N* is the number of observations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*M* is the number of classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y ij is a binary indicator for whether class j is the correct classification
    for observation i.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p ij is the predicted probability that observation i is of class j.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Brier score to measure calibration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Brier score is an effective metric for assessing the calibration of multi-class
    models as it penalizes the model more when there is a larger difference between
    the predicted probabilities and the actual outcomes. A well-calibrated model will
    have a lower Brier score as its predicted probabilities will be closer to the
    actual outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: How to evaluate calibration using Brier Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Brier score provides a way to quantitatively assess how well calibrated
    a multi-class classifier’s predicted probabilities are. Evaluating calibration
    is key for ensuring reliability in real-world deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To utilize the Brier score, there are three main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predict the probabilities**: Use your multi-class classification model to
    estimate the probabilities for each class for every observation in a validation
    dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the Brier score**: Calculate the Brier score using the provided formula.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interpret the result**: A lower Brier score indicates better calibration.
    It signifies that the model’s predicted probabilities are more aligned with the
    actual outcomes, thus making the model more reliable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In essence, employing the Brier score to evaluate the calibration of your multi-class
    model helps ensure the reliability of your model’s probability estimates. A lower
    Brier score, reflecting smaller differences between predicted and actual probabilities,
    indicates a well-calibrated model, enhancing the model’s trustworthiness in real-world
    applications. Understanding and utilizing the Brier score as a calibration metric
    is essential for optimizing the performance of your multi-class classification
    model in practical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and employing the appropriate metrics is pivotal for evaluating
    and improving multi-class classification models. A thorough grasp of these metrics
    allows for a more nuanced analysis, paving the way for developing robust and efficient
    multi-class classification models and ensuring their successful deployment in
    real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: How conformal prediction can be applied to multi-class classification problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: conformal prediction is a powerful framework that can be applied to multi-class
    classification problems. It provides a way to make predictions with a measure
    of certainty, which is particularly useful when dealing with multiple classes.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we have already looked at how conformal prediction
    assigns a *p*-value to each class for a given instance in the context of multi-class
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: The p-value represents the confidence level of the prediction for that class.
    The higher the p-value, the more confident the model is that the instance belongs
    to that class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure for applying conformal prediction to multi-class classification
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calibration**: A portion of the training data, known as the calibration set,
    is set aside. The model is trained on the remaining data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction**: For each class, the model predicts class scores. The conformity
    score, which measures how well the prediction conforms to the actual outcomes
    in the calibration set, is calculated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**P-value calculation**: For a new instance, the model calculates a nonconformity
    score for each class. The p-value for each class is then calculated as the proportion
    of instances in the calibration set with a higher nonconformity score.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Output**: The model outputs the predicted class labels along with their p-values.
    The classes are ranked by their p-values, providing a measure of confidence for
    each prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Applying conformal prediction to multi-class classification problems offers
    several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidence measures**: conformal prediction provides a measure of confidence
    (the p-value) for each prediction, which can be very useful in decision-making
    processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validity**: conformal prediction offers a theoretical guarantee of validity,
    meaning that the error rate of the predictions will be close to the significance
    level set by the user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiency**: conformal prediction is computationally efficient and can be
    applied to large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versatility**: conformal prediction can be used with any ML algorithm, making
    it a versatile tool for multi-class classification problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section will examine how Venn-ABERS predictors can be applied to multi-class
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class probabilistic classification using inductive and cross-Venn-ABERS
    predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Venn-ABERS is a conformal prediction method developed by Vladimir Vovk, Ivan
    Petej, and Valentina Fedorova (*Large-scale probabilistic predictors with and
    without guarantees of validity*, [https://papers.nips.cc/paper/2015/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html](https://papers.nips.cc/paper/2015/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html))
    to address the limitations of classical calibrators such as Platt scaling and
    isotonic regression. It guarantees mathematical validity, regardless of the data
    distribution, dataset size, or underlying classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Venn-ABERS predictors work by fitting isotonic regression twice, assuming that
    each test object can have both the label `0` and the label `1`. This results in
    two probabilities, *p0* and *p1*, for each test object, representing probabilities
    of the object belonging to class 1\. These probabilities create a prediction interval
    for the probability of class 1, with mathematical guarantees that the actual probability
    falls within this interval.
  prefs: []
  type: TYPE_NORMAL
- en: The Venn-ABERS prediction is a multi-predictor, and the width of the interval
    (p0, p1) contains valuable information about the classification confidence. In
    critical situations, the Venn-ABERS predictor outputs accurate and well-calibrated
    probabilities and issues an “alert” by widening the (p0, p1) interval. This alert
    indicates that the decision-making process should consider the increased uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities can be combined into a single value using *p = p1 / (1 - p0
    + p1)* for practical decision-making purposes. This combined probability of class
    1, *p*, can be used for decision-making tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The research paper by Valery Manokhin, *Multi-class probabilistic classification
    using inductive and cross Venn–Abers predictors*, introduces a method for adapting
    Venn-ABERS predictors for multi-class classification. You can access the paper
    here: http://proceedings.mlr.press/v60/manokhin17a/manokhin17a.pdf.'
  prefs: []
  type: TYPE_NORMAL
- en: Experimental results demonstrate that the proposed multi-class predictors outperform
    uncalibrated and existing classical calibration methods in terms of accuracy,
    indicating potential substantial advancements in multi-class probabilistic classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'For practitioners and researchers eager to employ this technique, a Python
    implementation of the fast Venn-ABERS predictor for binary classification is accessible
    on GitHub (*Multi-class probabilistic classification using inductive and cross
    Venn–Abers predictors*: [https://github.com/valeman/Multi-class-probabilistic-classification](https://github.com/valeman/Multi-class-probabilistic-classification)).
    This educational resource offers a hands-on opportunity to explore the details
    of implementation and practical advantages of utilizing Venn-ABERS predictors
    in real-world ML scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: The proposed approach to multi-class probability estimation using **inductive**
    and **cross-Venn-ABERS predictors** (**IVAPs** and **CVAPs**) is based on transforming
    multi-class classifiers into binary classifiers. In this approach, the binary
    classifiers are trained to distinguish between each class and all other classes
    combined.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in a three-class problem with classes A, B, and C, three binary
    classifiers are trained: one to distinguish between A and (B or C), one to distinguish
    between B and (A or C), and one to distinguish between C and (A or B). The IVAP
    is then used to estimate the probability of each class for a given test instance.
    The IVAP computes the probability of a class as the fraction of binary classifiers
    that classify the instance as belonging to that class.'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for converting pairwise classification scores and pairwise class
    probabilities uses a method introduced in the paper *Pairwise Neural Network Classifiers
    with Probabilistic* *Outputs* ([https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, if *rij* is the class score *i* over class *j* from the respective
    binary model, the estimated probability for class *i* is calculated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea is to first compute pairwise probabilities between each pair of
    classes using the dedicated binary classifiers. Then, these pairwise probabilities
    can be combined to estimate a normalized probability distribution over all classes:'
  prefs: []
  type: TYPE_NORMAL
- en: p i PKPD =  1 ___________ ∑ j:j≠i n   1 _ r ij − (k − 2)
  prefs: []
  type: TYPE_NORMAL
- en: This provides a principled approach to converting pairwise binary classification
    outcomes into class probability estimates suitable for multi-class evaluation
    and calibration.
  prefs: []
  type: TYPE_NORMAL
- en: After calculating the probabilities (this technique is dubbed the **PKPD** method),
    normalizing these values is crucial to ensure their total sum is 1\. These pairwise
    probabilities can be obtained by applying IVAPs and CVAPs to the pairwise classification
    scores/probabilities, which are used to calibrate classification scores produced
    by underlying classification models.
  prefs: []
  type: TYPE_NORMAL
- en: In simpler terms, the PKPD method helps convert probabilities from binary comparisons
    (pairwise probabilities) into multi-class probabilities. The calculated multi-class
    probabilities are then used to classify test objects into one of the *k* possible
    classes. This classification enables the computation of metrics, which can be
    compared across various calibration algorithms to evaluate their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore a real-world example to understand how to apply conformal
    prediction to multi-class classification problems. The following is a code walk-through
    that demonstrates this application.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s analyze the code from the `Chapter_12.ipynb` notebook (the code can be
    found in the book’s GitHub repo at [https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_12.ipynb](https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_12.ipynb)).
    The notebook demonstrates the application of conformal prediction to a multi-class
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general steps of multi-class classification are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data preparation**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the dataset.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the dataset into features (*X*) and target (*y*).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and test sets.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a classification model on the training data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the trained model to make predictions on the test data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Applying** **conformal prediction**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply conformal prediction to the trained model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain confidence and credibility measures for the predictions.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluation**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model using appropriate metrics.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the performance of the calibrated model with the original model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`evaluate_model_performance` function is used to evaluate the performance of
    different models and calibration methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It trains the model, makes predictions, and evaluates the performance using
    various metrics such as accuracy, log loss, and Brier loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calibration**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different calibration methods, such as Platt, isotonic, and others, are applied
    to the model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The predictions of the calibrated models are evaluated to analyze the performance
    improvement.*   `evaluate_model_performance` function to each model.*   The results
    for each model and calibration method are stored and can be analyzed to determine
    the best-performing model and calibration method.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s summarize the chapter next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book’s final chapter, we explored the intriguing domain of multi-class
    conformal prediction. We began by understanding the concept of multi-class classification,
    a prevalent scenario in ML where an instance can belong to one of many classes.
    This understanding is crucial for effectively applying conformal prediction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We then delved into the metrics used for evaluating multi-class classification
    problems. These metrics quantitatively measure our model’s performance and are
    vital for effective model evaluation and selection.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to apply conformal prediction to multi-class classification
    problems. This section provided practical insights and techniques to apply to
    your industrial applications directly.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you should have gained valuable skills and knowledge
    in multi-class classification and how conformal prediction can be effectively
    applied to these problems. This knowledge will prove invaluable in your journey
    as a data scientist, ML engineer, or researcher.
  prefs: []
  type: TYPE_NORMAL
- en: We covered different main topics, including multi-class classification problems,
    where we explored multi-class classification and its importance in ML. We also
    discussed the difference between multi-class and multi-label classification problems.
    We then investigated the metrics used to evaluate multi-class classification problems.
    Understanding these metrics is crucial for assessing the performance of our models
    and making informed decisions about model selection and optimization. Finally,
    we learned how to apply conformal prediction to multi-class classification problems.
    This section provided practical insights and techniques to apply directly to your
    industrial applications.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter marks the end of our journey into conformal prediction. We hope
    that the knowledge and skills you’ve gained will serve you well in your future
    endeavors in ML. Happy learning!
  prefs: []
  type: TYPE_NORMAL
