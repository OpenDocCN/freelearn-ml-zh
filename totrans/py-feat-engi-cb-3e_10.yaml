- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating Features from a Time Series with tsfresh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we’ve discussed feature engineering methods and tools
    tailored for tabular and relational datasets. In this chapter, we will shift our
    focus to time-series data. A time series is a sequence of observations taken sequentially
    over time. Examples include energy generation and demand, temperature, air pollutant
    concentration, stock prices, and sales revenue. Each of these examples represents
    a variable and their values change over time.
  prefs: []
  type: TYPE_NORMAL
- en: The widespread availability of affordable sensors capable of measuring motion,
    movement, humidity, glucose, and other parameters has significantly increased
    the amount of temporally annotated data. These time series can be utilized in
    various classification tasks. For instance, by analyzing the electricity usage
    pattern of a household at a given time interval, we can infer whether a particular
    appliance was being used. Similarly, the signal of an ultrasound sensor can help
    determine the probability of a (gas) pipeline failure, and the characteristics
    of a sound wavelength can help predict whether a listener will like a song. Time-series
    data is also valuable for regression tasks. For example, signals from machinery
    sensors can be used to predict the remaining useful life of the device.
  prefs: []
  type: TYPE_NORMAL
- en: To use time series with traditional supervised machine learning models, such
    as linear and logistic regression, or decision-tree-based algorithms, we need
    to map each time series into a well-defined feature vector that captures its characteristics.
    Time-series patterns, including trends, seasonality, and periodicity, among other
    things, can be captured by a combination of simple and complex mathematical operations.
    Simple calculations include, for instance, taking the mean and the standard deviation
    of the time series. More complex methods include determining correlation or entropy,
    for example. In addition, we can apply non-linear time-series analysis functions
    to decompose the time-series signal, for example, Fourier or wavelet transformations,
    and use the parameters of these functions as features of the supervised models.
  prefs: []
  type: TYPE_NORMAL
- en: Creating features from time series can be very time-consuming; we need to apply
    various signal processing and time-series analysis algorithms to identify and
    extract meaningful features. The `tsfresh` Python package, which stands for `tsfresh`
    includes a feature selection algorithm that identifies the most predictive features
    for a given time series. By automating the application of complex time-series
    methods, `tsfresh` bridges the gap between signal-processing experts and machine
    learning practitioners, making it easier to extract valuable features from time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to automatically create hundreds of features
    from time-series data by utilizing `tsfresh`. Following that, we will discuss
    how to fine-tune this feature creation process by selecting the most relevant
    features, extracting different features from different time series, and integrating
    the feature creation process into a scikit-learn pipeline to classify time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting hundreds of features automatically from a time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically creating and selecting predictive features from time-series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting different features from different time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a subset of features identified through feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding feature creation into a scikit-learn pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the open source `tsfresh` Python library. You can
    install `tsfresh` with `pip` by executing `pip` `install tsfresh`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have an old Microsoft operating system, you may need to update the Microsoft
    C++ build tools to proceed with the `tsfresh` package’s installation. Follow the
    steps in this thread to do so: [https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst](https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work with the **Occupancy Detection** dataset from the UCI Machine
    Learning Repository, available at [http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection](http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection)
    and licensed under a Creative Commons Attribution 4.0 International (CC BY 4.0)
    license: [https://creativecommons.org/licenses/by/4.0/legalcode](https://creativecommons.org/licenses/by/4.0/legalcode).
    The corresponding citation for this data is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Candanedo, Luis. (2016). Occupancy Detection. UCI Machine Learning Repository.
    [https://doi.org/10.24432/C5X01N](https://doi.org/10.24432/C5X01N).
  prefs: []
  type: TYPE_NORMAL
- en: 'I downloaded and modified the data as shown in this notebook: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a copy of the modified dataset and a target variable, check out the files
    called `occupancy.csv` and `occupancy_target.csv`, available at the following
    link: [https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh)'
  prefs: []
  type: TYPE_NORMAL
- en: The Occupancy Detection dataset contains time-series data taken over 135 hours
    at one-minute intervals. The variables measure the temperature, humidity, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/38.png)
    level, and light consumption in an office. Camera footage was used to determine
    whether someone was in the office. The target variable shows whether the office
    was occupied at any one hour. If the target takes the value `1`, it means that
    the office was occupied during that hour; otherwise, it takes the value `0`.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset with the time series and that with the target variable have different
    numbers of rows. The time-series dataset contains 135 hours of records at one-minute
    intervals – that is, 8,100 rows. The target has only 135 rows, with a label indicating
    whether the office was occupied at each of the 135 hours.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the notebook in this book’s GitHub repository for plots of the different
    time series to become familiar with the dataset: [https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting hundreds of features automatically from a time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series are data points indexed in time order. Analyzing time-series sequences
    allows us to make various predictions. For example, sensor data can be used to
    predict pipeline failures, sound data can help identify music genres, health history
    or personal measurements such as glucose levels can indicate whether a person
    is sick, and, as we will show in this recipe, patterns of light usage, humidity,
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/39.png)
    levels can determine whether an office is occupied.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train regression and classification models using traditional machine learning
    algorithms, such as linear regression or random forests, we require a dataset
    of size *M x N*, where M is the number of rows and N is the number of features
    or columns. However, with time-series data, what we have is a collection of *M*
    time series, and each time series has multiple rows indexed chronologically. To
    use time series in supervised learning models, each time series needs to be mapped
    into a well-defined feature vector, *N*, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Diagram showing the process of feature creation from a time
    series for classification or regression](img/B22396_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Diagram showing the process of feature creation from a time series
    for classification or regression
  prefs: []
  type: TYPE_NORMAL
- en: These feature vectors, which are represented as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/40.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/41.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/42.png)
    in *Figure 10**.1*, should capture the characteristics of the time series. For
    example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/43.png)could
    be the mean value of the time series and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/44.png)its
    variance. We can create many features to characterize the time series concerning
    the distribution of data points, correlation properties, stationarity, or entropy,
    among others. Therefore, the feature vector, N, can be constructed by applying
    a series of **characterization methods** that take a time series as input and
    return one or more scalars as output. The mean, or the sum, takes the time-series
    sequence as input and returns a single scalar as output, with the mean value of
    the time series or the sum of its values. We can also fit a linear trend to the
    time-series sequence, which will return two scalars – one with the slope and one
    with the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` applies 63 characterization methods to a time series, each of which
    returns one or more scalars, therefore resulting in more than 750 features for
    any given time series. In this recipe, we will use `tsfresh` to transform time-series
    data into an M x N feature table, which we will then use to predict office occupancy.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will use the Occupancy Detection dataset described in the
    *Technical requirements* section. This dataset contains measurements of temperature,
    humidity, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/45.png)
    level, and light consumption in an office taken at one-minute intervals. There
    are 135 hours of measurements, and each hour is flagged with a unique identifier.
    There is also a dataset with a target variable that indicates in which of these
    135 hours the office was occupied. Let’s load the data and make some plots to
    understand its patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s load `pandas` and `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and display the first five rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see the dataset containing a unique identifier,
    followed by the date and time of the measurements and values for five time series
    capturing temperature, humidity, lights, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/39.png)
    levels in the office:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – DataFrame with the time-series data](img/B22396_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – DataFrame with the time-series data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a function to plot the time series from *step 2* at a given hour
    (the `id` column is a unique identifier for each of the 135 hours of records):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the time series corresponding to an hour when the office was not
    occupied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see the time-series values during the second
    hour of records, when the office was empty:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 10.3 – Time-series values during the second hour of data collection\
    \ when the office was empt\uFEFFy](img/B22396_10_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Time-series values during the second hour of data collection when
    the office was empty
  prefs: []
  type: TYPE_NORMAL
- en: Note that the lights were off, and that is why we see the flat line at 0 in
    the plot of `light` consumption in the top-right corner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s plot the time-series data corresponding to an hour when the office
    was occupied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see the time-series values during the fifteenth
    hour of records, when the office was occupied:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Time-series values during the fifteenth hour of data collection,
    when the office was occupied](img/B22396_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Time-series values during the fifteenth hour of data collection,
    when the office was occupied
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the lights were on this time (top-right panel).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will extract features from each of these one-hour windows
    of time-series data, capturing various aspects of their characteristics. From
    each of these 60-minute time-series segments, we will automatically generate more
    than 750 features using `tsfresh`, ensuring a comprehensive representation of
    the data’s properties.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will begin by automatically creating hundreds of features from one time
    series, `lights`, and then use those features to predict whether the office was
    occupied at any given hour:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset described in the *Technical* *requirements* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the target variable into a `pandas` Series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create hundreds of features automatically for each hour of light records
    using `tsfresh`. To create features from the `light` variable, we pass the DataFrame
    containing this variable and the unique identifier for each time series to the
    `extract_features` function from `tsfresh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we execute `features.shape`, we’ll obtain `(135, 789)` corresponding to the
    size of the resulting DataFrame, where each row represents an hour of records
    and each column one of the features created by `tsfresh`. There are 789 features
    that characterize light consumption at any given hour. Go ahead and execute `features.head()`
    to get a view of the resulting DataFrame. For space reasons, we can’t display
    the entire DataFrame in the book. So, instead, we will explore some of the features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s capture the names of five of the created features in an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `feats`, we’ll see the names of five features corresponding to
    the mean, length, standard deviation, coefficient of variation, and variance of
    the light consumption per hour:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s display the values of the features from *step 5* for the first five
    hours:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following DataFrame, we see the features extracted from the time series
    for the first five hours of light consumption:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Features created for each hour of light consumption](img/B22396_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Features created for each hour of light consumption
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the mean value of light consumption in *Figure 10**.4*, we can see
    that the lights were on during the first hour, and then off in the following four
    hours. The length of the time series is 60 because we have 60 minutes of records
    per hour.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` applies 63 feature creation methods to a time series. Based on the
    characteristics of the time series, such as its length or its variability, some
    of the methods will return missing values or infinite values. For example, in
    *Figure 10**.4*, we see that the variation coefficient could not be calculated
    for those hours where the light consumption is constant. And the variance is also
    `0` in those cases. In fact, for our dataset, many of the resulting features contain
    only `NaN` values, or are constant, like the length, and are therefore not useful
    for training machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` includes an imputation function to impute features that contain `NaN`
    values. Let’s go ahead and impute our features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `impute` function from `tsfresh` replaces `NaN`, `-Inf`, and `Inf` values
    with the variable’s median, minimum, or maximum values, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let’s use these features to train a logistic regression model and predict whether
    the office was occupied.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s begin by separating the dataset into training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s set up and train a logistic regression model, and then evaluate
    its performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we see the values of evaluation metrics that are commonly
    used for classification analysis, which suggests that the created features are
    useful for predicting office occupancy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To keep the recipe simple, I have not optimized the model hyperparameters or
    tuned the probability threshold – things that we normally do to ensure our models
    are accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To finish off, let’s extract features from every time series, that is, `light`,
    `temperature`, `humidity`, and `co2`, and this time, we will impute the features
    right after the extraction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In *step 10*, we indicated that we want to sort our time series based on the
    timestamp containing the time and date of the measurement, by passing the `date`
    variable to the `column_sort` parameter. This is useful when our time series are
    not equidistant or not ordered chronologically. If we leave this parameter set
    to `None`, `tsfresh` assumes that the time series are ordered and equidistant.
  prefs: []
  type: TYPE_NORMAL
- en: The output of *step 10* consists of a DataFrame with 135 rows, containing 3,945
    features (execute `features.shape` to check that out) that characterize the five
    original time series – temperature, light, humidity and its ratio, and ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/47.png)
    in the office. These features were imputed in *step 10*, so you can go ahead and
    use this DataFrame to train another logistic regression model to predict office
    occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used `tsfresh` to automatically create hundreds of features
    from five time series, and then used those features to train a logistic regression
    model to predict whether the office was occupied.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To create features with `tsfresh`, the time-series interval from which we want
    to extract features must be marked with a `id` variable.
  prefs: []
  type: TYPE_NORMAL
- en: To create features from time series, we used the `extract_features` function
    from `tsfresh`. This function takes the DataFrame containing the time series and
    the unique identifier as input and returns a DataFrame containing the extracted
    features as output.
  prefs: []
  type: TYPE_NORMAL
- en: '`extract_features` has three key parameters: `column_id`, `column_sort`, and
    `impute_function`. `column_id` receives the name of the column with the unique
    identifier for each sequence that’ll be used to extract features. `column_sort`
    is used to reorder the time series before extracting features. When `column_sort`
    is left to `None`, `tsfresh` assumes that the data is ordered chronologically
    and that the timestamps are equidistant. In *step 10*, we passed the `date` variable
    as the sorting variable, which informs `tsfresh` how to sort the data before extracting
    the features.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, leaving `column_sort` set to `None` or passing the `date` variable
    made no difference, because our time series were already ordered chronologically
    and the timestamps were equidistant. If this is not the case in your time series,
    use this parameter to create features correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `extract_features` also accepts the `impute` function through the `impute_function`
    parameter, to automatically remove infinite and `NaN` values from the created
    features. Will discuss additional parameters of `extract_features` in the coming
    recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the `extract_features` function, visit [https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.extraction](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.extraction).
  prefs: []
  type: TYPE_NORMAL
- en: The `impute` function, which can be used independently, as we did in *step 7*,
    or within the `extract_features` function, as we did in *step 10*, replaced `NAN`,
    `-Inf`, and `Inf` values with the variable’s median, minimum, or maximum values,
    respectively. If the feature contains only `NaN` values, they are replaced by
    zeroes. The imputation occurs in place – that is, in the same DataFrame that is
    being imputed.
  prefs: []
  type: TYPE_NORMAL
- en: The `extract_features` function returns a DataFrame containing as many rows
    as unique identifiers in the data. In our case, it returned a DataFrame with 135
    rows. The columns of the resulting DataFrame correspond to the 789 values that
    were returned by 63 characterization methods applied to each of the 135 60-minute
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 5*, we explored some of the resulting features, which captured the
    time series mean, variance, and coefficient of variation, as well as their length.
    Let’s explore a few more of the resulting features.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the created variables are self-explanatory. For example, the `'light__skewness'`
    and `'light__kurtosis'` variables contain the skewness and kurtosis coefficients,
    which characterize the data distribution. The `'light__has_duplicate_max'`, `'light__has_duplicate_min'`,
    and `'light__has_duplicate'` variables indicate whether the time series has duplicated
    values or duplicated minimum or maximum values within the time interval. The `'light__quantile__q_0.1'`,
    `'light__quantile__q_0.2'`, and `'light__quantile__q_0.3'` variables display the
    different quantile values of the time series. Finally, the `'light__autocorrelation__lag_0'`,
    `'light__autocorrelation__lag_1'`, and `'light__autocorrelation__lag_2'` variables
    show the autocorrelation of the time series with its past values, lagged by 0,
    1, or 2 steps – information that is generally useful in forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Other characterization methods return features obtained from signal processing
    algorithms, such as the continuous wavelet transform for the Ricker wavelet, which
    returns the `'light__cwt_coefficients__coeff_0__w_2__widths_(2, 5, 10, 20)'`,
    `'light__cwt_coefficients__coeff_0__w_5__widths_(2, 5, 10, 20)'`, `'light__cwt_coefficients__coeff_0__w_10__widths_(2,
    5, 10, 20)'`, and `'light__cwt_coefficients__coeff_0__w_20__widths_(2, 5, 10,
    20)'` features, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We can’t discuss each of these feature characterization methods or their outputs
    in detail in this book because there are too many. You can find more details about
    the transformations supported by `tsfresh` and their formulation at [https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html).
  prefs: []
  type: TYPE_NORMAL
- en: Some of the features that are automatically created by `tsfresh` may not make
    sense or even be possible to calculate for some time series because they require
    a certain length or data variability, or the time series must meet certain distribution
    assumptions. Therefore, the suitability of the features will depend on the nature
    of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can decide which features to extract from your time series based on domain
    knowledge, or by creating all possible features and then applying feature selection
    algorithms or following up with data analysis. In fact, from our dataset, many
    of the resulting features were either constant or contained only missing data.
    Hence, we can reduce the feature space to informative features by taking those
    features out of the data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more details about `tsfresh`, check out the article Christ M., Braun N.,
    , Neuffer J., and Kempa-Liehr A., (2018). *Time Series FeatuRe Extraction on basis
    of Scalable Hypothesis tests (tsfresh – A Python package). Neurocomputing 307
    (2018). Pages* *72-77.* [https://dl.acm.org/doi/10.1016/j.neucom.2018.03.067](https://dl.acm.org/doi/10.1016/j.neucom.2018.03.067).
  prefs: []
  type: TYPE_NORMAL
- en: Automatically creating and selecting predictive features from time-series data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we automatically extracted several hundred features
    from time-series variables using `tsfresh`. If we have more than one time-series
    variable, we can easily end up with a dataset containing thousands of features.
    In addition, many of the resulting features had only missing data or were constant
    and were therefore not useful for training machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: When we create classiﬁcation and regression models to solve real-life problems,
    we often want our models to take a small number of relevant features as input
    to produce interpretable machine learning outputs. Simpler models have many advantages.
    First, their output is easier to interpret. Second, simpler models are cheaper
    to store and faster to train. They also return their outputs faster.
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` includes a highly parallelizable feature selection algorithm based
    on non-parametric statistical hypothesis tests, which can be executed at the back
    of the feature creation procedure to quickly remove irrelevant features. The feature
    selection procedure utilizes different tests for different features.'
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` uses the following tests to select features:'
  prefs: []
  type: TYPE_NORMAL
- en: Fisher’s exact test of independence, if both the feature and the target are
    binary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kolmogorov-Smirnov test, if either the feature or the target is binary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall rank test, if neither the feature nor the target is binary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantage of these tests is that they are non-parametric, and thus make
    no assumptions on the underlying distribution of the variables being tested.
  prefs: []
  type: TYPE_NORMAL
- en: The result of these tests is a vector of p-values that measures the significance
    of the association between each feature and the target. These p-values are then
    evaluated based on the Benjamini-Yekutieli procedure to decide which features
    to keep.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more details about `tsfresh`’s feature selection procedure, check out the
    article Christ, Kempa-Liehr, and Feindt, *Distributed and parallel time series
    feature extraction for industrial big data applications*. Asian Machine Learning
    Conference (ACML) 2016, Workshop on Learning on Big Data (WLBD), Hamilton (New
    Zealand), arXiv, [https://arxiv.org/abs/1610.07717v1](https://arxiv.org/abs/1610.07717v1).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will automatically create hundreds of features from various
    time series, and then select the most relevant features by utilizing `tsfresh`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will begin by automatically creating and selecting features from one time
    series, `lights`, and then we will automate the procedure for multiple time series:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and the target variable described in the *Technical* *requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create hundreds of features automatically for each hour of `light` use
    records and impute the resulting features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the previous step is a DataFrame with 135 rows and 789 columns,
    corresponding to the features created from each hour of light consumption.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more details about *step 3*, or the Occupancy Detection dataset, check out
    the *Extracting hundreds of features automatically from a time* *series* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s select the features based on the non-parametric tests that we mentioned
    in the introduction of this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we execute `len(features)`, we’ll see the value `135`, which means that from
    the 789 features created in *step 3*, only 135 are statistically significant.
    Go ahead and execute `features.head()` to display the first five rows of the resulting
    DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For space reasons, we will only display the first five features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following DataFrame, we see the values of the first five features for
    the first five hours of light consumption:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.6 – DataFrame with five of the selected features created from each
    hour of light consumption](img/B22396_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – DataFrame with five of the selected features created from each
    hour of light consumption
  prefs: []
  type: TYPE_NORMAL
- en: Check the discussion in the *How it works…* section for a more detailed analysis
    of the DataFrame resulting from *step 4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the features from *step 4* to train a logistic regression
    model and predict whether the office was occupied. Let’s begin by separating the
    dataset into training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up and train a logistic regression model and then evaluate its performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we see the values of commonly used evaluation metrics
    for classification analysis. These suggest that the selected features are useful
    for predicting office occupancy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: features = extract_relevant_features(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: y,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: column_id="id",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: column_sort="date",
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The parameters of `extract_relevant_features` are very similar to those of `extract_features`.
    Note, however, that the former will automatically perform imputation to be able
    to proceed with the feature selection. We discussed the parameters of `extract_features`
    in the *Extracting hundreds of features automatically from time* *series* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The output of *step 8* consists of a DataFrame with 135 rows and 968 features,
    from the original 3,945 that are returned by default by `tsfresh` (you can check
    that out by executing `features.shape`). Go ahead and use this DataFrame to train
    another logistic regression model to predict office occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created hundreds of features from a time series and then
    selected the most relevant features based on non-parametric statistical tests.
    The feature creation and selection procedures were carried out automatically by
    `tsfresh`.
  prefs: []
  type: TYPE_NORMAL
- en: To create the features, we used `tsfresh`’s `extract_features` function, which
    we described in detail in the *Extracting hundreds of features automatically from
    a time* *series* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: To select features, we used the `select_features` function, also from `tsfresh`.
    This function applies different statistical tests, depending on the nature of
    the feature and the target. Briefly, if the feature and target are binary, it
    tests their relationship with Fisher’s exact test. If either the feature or the
    target is binary, and the other variable is continuous, it tests their relationship
    by using the Kolmogorov-Smirnov test. If neither the features nor the target is
    binary, it uses the Kendall rank test.
  prefs: []
  type: TYPE_NORMAL
- en: The result of these tests is a vector with one p-value per feature. Next, `tsfresh`
    applies the Benjamini-Yekutieli procedure, which aims to reduce the false discovery
    rate, to select which features to keep based on the p-values. This feature selection
    procedure has some advantages, the main one being that statistical tests are fast
    to compute, and therefore the selection algorithm is scalable and can be parallelized.
    Another advantage is that the tests are non-parametric and hence suitable for
    linear and non-linear models.
  prefs: []
  type: TYPE_NORMAL
- en: However, feature selection methods that evaluate each feature individually are
    unable to remove redundant features. In fact, many of the features automatically
    created by `tsfresh` will be highly correlated, like those capturing the different
    quantiles of light consumption. Hence, they will show similar p-values and be
    retained. But in practice, we only need one or a few of them to capture the information
    of the time series. I’d recommend following up the `tsfresh` selection procedure
    with alternative feature selection methods that are able to pick up feature interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in *step 8*, we combined the feature creation step (*step 3*) with
    the feature selection step (*step 4*) by using the `extract_relevant_features`
    function. `extract_relevant_features` applies the `extract_features` function
    to create the features from each time series and imputes them. Next, it applies
    the `select_features` function to return a DataFrame containing one row per unique
    identifier, and the features that were selected for each time series. Note that
    different features can be selected for different time series.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The selection algorithm from `tsfresh` offers a quick method to remove irrelevant
    features. However, it does not find the best feature subset for the classification
    or regression task. Other feature selection methods can be applied at the back
    of `tsfresh`’s algorithm to reduce the feature space further.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on feature selection algorithms, check out the book *Feature
    Selection in Machine Learning with Python* by Soledad Galli on Leanpub: [https://leanpub.com/feature-selection-in-machine-learning/](https://leanpub.com/feature-selection-in-machine-learning/).'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting different features from different time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`tsfresh` extracts many features based on the time-series characteristics and
    distribution, such as their correlation properties, stationarity, and entropy.
    It also applies non-linear time-series analysis functions, which decompose the
    time-series signal through, for example, Fourier or wavelet transformations. Depending
    on the nature of the time series, some of these transformations make more sense
    than others. For example, wavelength decomposition methods can make sense for
    time series resulting from signals or sensors but are not always useful for time
    series representing sales or stock prices.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will discuss how to optimize the feature extraction procedure
    to extract specific features from each time series, and then use these features
    to predict office occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`tsfresh` accesses the methods that will be used to create features through
    a dictionary that contains the method names as keys and, if they need a parameter,
    it has the parameter as a value. `tsfresh` includes some predefined dictionaries
    as well. We’ll explore these predefined dictionaries first, which can be accessed
    through the `settings` module:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries and functions and the `settings`
    module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset and the target variable described in the *Technical* *requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`tsfresh` includes three main dictionaries that control the feature creation
    output: `settings.ComprehensiveFCParameters`, `settings.EfficientFCParameters`,
    and `settings.MinimalFCParameters`. Here, we’ll explore the dictionary that returns
    the fewest features. You can repeat the steps to explore the additional dictionaries.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the feature creation methods that will be applied when using the dictionary
    that returns the fewest features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the output of *step 3*, we see a dictionary with the feature extraction
    method names as keys, and the parameters used by those methods, if any, as values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and explore the other two predefined dictionaries, `settings.ComprehensiveFCParameters`
    and `settings.EfficientFCParameters`, by adapting the code from *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the dictionary from *step 3* to extract only those features
    from the `light` time series and then display the shape of the resulting DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of *step 4* is `(135, 10)`, which means that only 10 features were
    created for each of the 135 hours of light consumption data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s display the resulting DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We see the values of the resulting features for the first five hours of light
    consumption in the following DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 10.7 – DataFrame with the features created \uFEFFfor each hour of\
    \ light consumption](img/B22396_10_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – DataFrame with the features created for each hour of light consumption
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will use these features to train a logistic regression model to predict
    whether the office was occupied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by separating the dataset into training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s set up and train a logistic regression model, and then evaluate
    its performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we see the evaluation metrics that are commonly used
    for classification analysis. These suggest that the selected features are useful
    for predicting office occupancy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Because light consumption is a very good indicator of office occupancy, with
    very simple features, we can obtain a predictive logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn how to specify the creation of different features for different
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a dictionary with the names of the methods that we want to use
    to create features from the `light` time series. We enter the method’s names as
    keys, and if the methods take a parameter, we pass it as an additional dictionary
    to the corresponding key; otherwise, we pass `None` as the values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create a dictionary with the features that we want to create from
    the `co2` time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s combine these dictionaries into a new dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s use the dictionary from *step 10* to create the features from
    both time series:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of *step 11* consists of a DataFrame with 135 rows and 8 features.
    If we execute `features.columns`, we will see the names of the created features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the output from *step 11*, different variables have been created
    from each of the `light` and `co2` time series.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we extracted specific features from our time-series data. First,
    we created features based on a predefined dictionary that comes with `tsfresh`.
    Next, we created our own dictionary, specifying the creation of different features
    for different time series.
  prefs: []
  type: TYPE_NORMAL
- en: The `tsfresh` package comes with some predefined dictionaries that can be accessed
    through the `settings` module. The `MinimalFCParameters` dictionary is used to
    create 10 simple features based on basic statistical parameters of the time-series
    distribution, such as the mean, median, standard deviation, variance, sum of its
    values, count (or length), and minimum and maximum values. In *step 3*, we displayed
    the dictionary, with the method names as keys, and, as these methods do not require
    additional parameters, each key had `None` as the value.
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` has two additional predefined dictionaries. `EfficientFCParameters`
    is used to apply methods that are fast to compute, whereas `ComprehensiveFCParameters`
    returns all possible features and is the one used by default by the `extract_features`
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details about the predefined dictionaries, check out `tsfresh`’s documentation:
    [https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html](https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html)'
  prefs: []
  type: TYPE_NORMAL
- en: By using these predefined dictionaries in the `default_fc_parameters` parameter
    of `tsfresh`’s `extract_features` function, we can create specific features from
    one or more time series, as we did in *step 4*. Note that `default_fc_parameters`
    instructs `extract_features` to create the same features from *all* the time series.
    What if we want to extract different features from different time series?
  prefs: []
  type: TYPE_NORMAL
- en: To create different features for different time series, we can use the `kind_to_fc_parameters`
    parameter of `tsfresh`’s `extract_features` function. This parameter takes a dictionary
    of dictionaries, specifying the methods to apply to each time series.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 8*, we created a dictionary to specify the creation of specific features
    from the `light` time series. Note that the `"sum_values"` and `"mean"` methods
    take `None` as values, but the `quantile` method needs additional parameters corresponding
    to the quantiles that should be returned from the time series. In *step 9*, we
    created a dictionary to specify the creation of features from the `co2` time series.
    In *step 10*, we combined both dictionaries into one that takes the name of the
    time series as the key and the feature creation dictionaries as values. Then,
    we passed this dictionary to the `kind_to_fc_parameters` parameter of `tsfresh`’s
    `extract_features` function. This way of specifying features is suitable if we
    use domain knowledge to create the features, or if we only create a small number
    of features.
  prefs: []
  type: TYPE_NORMAL
- en: Do we need to type each method by hand into a dictionary if we want to create
    multiple features for various time series? Not really. In the following recipe,
    we will learn how to specify which features to create based on features selected
    by Lasso.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a subset of features identified through feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Automatically creating and selecting predictive features from time-series
    data* recipe, we learned how to select relevant features using `tsfresh`. We also
    discussed the limitations of `tsfresh`’s selection procedures and suggested following
    up with alternative feature selection methods to identify predictive features
    while avoiding redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create and select features using `tsfresh`. Following
    that, we will reduce the feature space further by utilizing Lasso regularization.
    Then, we will learn how to create a dictionary from the selected feature names
    to trigger the creation of those features *only* from future time series.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Occupancy Detection dataset described in the *Technical* *requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create and select features from our five time series and then display the shape
    of the resulting DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of *step 3* is `(135, 968)`, indicating that 968 features were returned
    from the five original time series, for each hour of records.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the function from *step 3* in the *Automatically creating and selecting
    predictive features from time-series* *data* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s reduce the feature space further by selecting features with Lasso regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up logistic regression with Lasso regularization, which is the `"l1"` penalty.
    I also set some additional parameters arbitrarily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up a transformer to retain those features whose logistic regression
    coefficients are different from 0:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the logistic regression model and select the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, capture the selected features in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `features`, we’ll see the names of the selected features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To extract just the features from *step 6* from the time series, we need to
    capture the feature creation method name and corresponding parameters in a dictionary.
    We can do this automatically from the feature names with `tsfresh`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `kind_to_fc_parameters`, we’ll see the dictionary that was created
    from the names of the features from *step 6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use the dictionary from *step 8* together with the `extract_features`
    function to create only those features from our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The new DataFrame, which can be displayed by executing `features.head()`, only
    contains the 12 features that were selected by Lasso. Go ahead and corroborate
    the result on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created 968 features from 5 time series. Next, we reduced
    the feature space to 12 features by using Lasso regularization. Finally, we captured
    the specifications of the selected features in a dictionary so that, looking forward,
    we only created those features from our time series.
  prefs: []
  type: TYPE_NORMAL
- en: To automatically create and select features with `tsfresh`, we used the `extract_relevant_features`
    function, which we described in detail in the *Automatically creating and selecting
    predictive features from time-series* *data* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization has the intrinsic ability to reduce some of the coefficients
    of the logistic regression model to 0\. The contribution of the features whose
    coefficient is 0 to the prediction of office occupancy is null and can therefore
    be removed. The `SelectFromModel()` class can identify and remove those features.
    We set up an instance of `SelectFromModel()` with a logistic regression model
    that used Lasso regularization to find the model coefficients. With `fit()`, `SelectFromModel()`
    trained the logistic regression model using the 968 features created from our
    time series and identified those whose coefficients were different from 0\. Then,
    with the `get_feature_names_out()` method, we captured the names of the selected
    features in a new variable.
  prefs: []
  type: TYPE_NORMAL
- en: To create only the 12 features selected by Lasso regularization, we created
    a dictionary from the variable names by using the `from_columns()` function from
    `tsfresh`. This function returned a dictionary with the variables from which features
    were selected as keys. The values were additional dictionaries, containing the
    methods used to create features as keys, and the parameters used, if any, as values.
    To create the new features, we used this dictionary together with the `extract_features`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In *step 9*, we passed the entire dataset to the `extract_features` function.
    The resulting features only contained features extracted from three of the five
    time series. The additional two time series were ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding feature creation into a scikit-learn pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve discussed how to automatically create and select
    features from time-series data by utilizing `tsfresh`. Then, we used these features
    to train a classification model to predict whether an office was occupied at any
    given hour.
  prefs: []
  type: TYPE_NORMAL
- en: '`tsfresh` includes *wrapper* classes around its main functions, `extract_features`
    and `extract_relevant_features`, to make the creation and selection of features
    compatible with the scikit-learn pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will set up a scikit-learn pipeline that extracts features
    from time series using `tsfresh` and then trains a logistic regression model with
    those features to predict office occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the necessary libraries and getting the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Occupancy Detection dataset described in the *Technical* *requirements*
    section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an empty DataFrame that contains the index of the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s split the DataFrame from *step 3* and the target from *step 2* into
    training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`X_train` and `X_test` will be used as containers to store the features created
    by `tsfresh`. They are needed for the functionality of `RelevantFeatureAugmenter()`
    that we will discuss in the coming steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a dictionary specifying the features to extract from each time
    series (I defined the following features arbitrarily):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We discussed the parameters of this dictionary in the *Extracting different
    features from different time* *series* recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s set up `RelevantFeatureAugmenter()`, which is a wrapper around the `extract_relevant_features`
    function, to create the features specified in *step 5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To create all possible features, use the `FeatureAugmenter()` class instead
    in *step 6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s combine the feature creation instance from *step 6* with a logistic regression
    model in a scikit-learn pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s tell `RelevantFeatureAugmenter()` which dataset it needs to use
    to create the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the pipeline, which will trigger the feature creation process, followed
    by the training of the logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s obtain predictions using the time series in the test set and evaluate
    the model’s performance through a classification report:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the output of *step* *10* here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The values of the classification report suggest that the extracted features
    are suitable for predicting whether the office is occupied at any given hour.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we combined creating features from a time series with `tsfresh`
    with training a machine learning algorithm from the scikit-learn library in a
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The `tsfresh` library includes two wrapper classes around its main functions
    to make the feature creation process compatible with the scikit-learn pipeline.
    In this recipe, we used the `RelevantFeatureAugmenter()` class, which wraps the
    `extract_relevant_features` function to create and then select features from a
    time series.
  prefs: []
  type: TYPE_NORMAL
- en: '`RelevantFeatureAugmenter()` works as follows; with `fit()`, it creates and
    selects features by using `extract_relevant_features`. The names of the selected
    features are then stored internally in the transformer. With `transform()`, `RelevantFeatureAugmenter()`
    creates the selected features from the time series.'
  prefs: []
  type: TYPE_NORMAL
- en: We overrode the default functionality of `RelevantFeatureAugmenter()` by passing
    a dictionary with the features we wanted to create to its `kind_to_fc_parameters`
    parameter. Therefore, with `transform()`, `RelevantFeatureAugmenter()` created
    the indicated features from the time series.
  prefs: []
  type: TYPE_NORMAL
- en: To create all features from the time series, `tsfresh` includes the `FeatureAugmenter()`
    class, which has the same functionality as `RelevantFeatureAugmenter()`, but without
    the feature selection step.
  prefs: []
  type: TYPE_NORMAL
- en: Both `RelevantFeatureAugmenter()` and `FeatureAugmenter()` need two DataFrames
    to work. The first DataFrame contains the time-series data and the unique identifiers
    (we loaded this DataFrame in *step 2*). The second DataFrame should be empty and
    contain the unique identifiers *in its index* (we created this DataFrame in *step
    3*). The features are extracted from the first DataFrame with the time series
    (when applying `transform()`) and subsequently added to the second DataFrame,
    which is then used to train the logistic regression or obtain its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The index of the empty DataFrame is used by `RelevantFeatureAugmenter()` and
    `FeatureAugmenter()` to identify the time series from which to extract the features.
    Hence, when applying `fit()` while passing `X_train`, features were extracted
    from time series whose `id` value was in the training set. After that, the model
    was evaluated by observing predictions made using the test set, which triggered
    the creation of features from time series whose `id` value was in `X_test`.
  prefs: []
  type: TYPE_NORMAL
- en: When we used `fit()` on the pipeline, we created features from our raw time
    series and trained a logistic regression model with the resulting features. With
    the `predict()` method, we created features from the test set and obtained the
    predictions of the logistic regression based on those features.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more details about the classes and procedures used in this recipe, visit
    the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tsfresh` documentation: [https://tsfresh.readthedocs.io/en/latest/api/tsfresh.transformers.html#tsfresh.transformers.relevant_feature_augmenter.RelevantFeatureAugmenter](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.transformers.html#tsfresh.transformers.relevant_feature_augmenter.RelevantFeatureAugmenter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Jupyter notebook with a demo: [https://github.com/blue-yonder/tsfresh/blob/main/notebooks/02%20sklearn%20Pipeline.ipynb](https://github.com/blue-yonder/tsfresh/blob/main/notebooks/02%20sklearn%20Pipeline.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
