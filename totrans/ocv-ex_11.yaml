- en: Chapter 11. Text Recognition with Tesseract
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the very basic OCR processing functions.
    Although they are quite useful for scanned or photographed documents, they are
    almost useless when dealing with text that casually appears in a picture.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll explore the OpenCV 3.0 text module, which deals specifically
    with scene text detection. Using this API, it is possible to detect text that
    appears in a webcam video, or to analyze photographed images (like the ones in
    Street View or taken by a surveillance camera) to extract text information in
    real time. This allows a wide range of applications to be created, from accessibility
    to marketing and even robotics fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand what is scene text recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how the text API works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the OpenCV 3.0 text API to detect text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the detected text to an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the text API and Tesseract integration to identify letters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the text API works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The text API implements the algorithm proposed by Lukás Neumann and Jiri Matas
    in the article called *Real-Time Scene Text Localization and Recognition* during
    the **CVPR** (**Computer Vision and Pattern Recognition)** Conference in 2012\.
    This algorithm represented a significant increase in scene text detection, performing
    the state-of-the art detection both in the CVPR database as well as in the Google
    Street View database.
  prefs: []
  type: TYPE_NORMAL
- en: Before we use the API, let's take a look at how this algorithm works under the
    hood, and how it addresses the scene text detection problem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Remember** that the OpenCV 3.0 text API does not come with the standard OpenCV
    modules. It''s an additional module present in the OpenCV contribute package.
    If you need to install OpenCV using the Windows Installer, refer to [Chapter 1](ch01.html
    "Chapter 1. Getting Started with OpenCV"), *Getting Started with OpenCV*, which
    will help you install these modules.'
  prefs: []
  type: TYPE_NORMAL
- en: The scene detection problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Detecting text that randomly appears in a scene is a problem harder than it
    looks. There are several new variables when we compare them to identify scanned
    text, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tri-dimensionality**: The text can be in any scale, orientation, or perspective.
    Also, the text can be partially occluded or interrupted. There are literally thousands
    of possible regions where it can appear in the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: Text can be in several different fonts and colors. The font can
    have outline borders or not. The background can be a dark, light, or a complex
    image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Illumination and shadows**: The sunlight position and apparent color changes
    over the time. Different weather conditions such as fog or rain can generate noise.
    Illumination can be a problem even in closed spaces, since light reflects over
    colored objects and hits the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blurring**: Text can appear in a region that is not prioritized by the auto
    focus lenses. Blurring is also common in moving cameras, in perspective text,
    or in the presence of fog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image, taken from Google Street View, illustrates these problems.
    Notice how several of these situations occur simultaneously in just a single image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The scene detection problem](img/B04283_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Performing a text detection to deal with such situations may prove computationally
    expensive, since there are *2n* subsets of pixels where the text can be, *n* being
    the number of pixels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to reduce the complexity, two strategies are commonly applied, which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a sliding window to search a subset of image rectangles. This strategy just
    reduces the number of subsets to a smaller amount. The amount of regions varies
    according to the complexity of text being considered. Algorithms that deal just
    with text rotation can use small values, as compared to the ones that also deal
    with rotation, skewing, perspective, and so on. The advantage of this approach
    is its simplicity, but it is usually limited to a narrow range of fonts, and often,
    to a lexicon of specific words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use of the connected component analysis. This approach assumes that pixels can
    be grouped into regions where pixels have similar properties. These regions are
    supposed to have higher chances of being identified as characters. The advantage
    of this approach is that it does not depend on several text properties (orientation,
    scale, and fonts), and they also provide a segmentation region that can be used
    to crop text to the OCR. This was the approach used in the previous chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV 3.0 algorithm uses the second one by performing the connected component
    analysis and searching for extremal regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremal regions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Extremal regions are connected areas that are characterized by uniform intensity
    and surrounded by a contrast background. The stability of a region can be measured
    by calculating how resistant the region is to the thresholding variance. This
    variance can be measured with a simple algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the threshold generates an image A. Detect its connected pixel regions
    (extremal regions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increasing the threshold by a delta amount generates an image B. Detect its
    connected pixel regions (extremal regions).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare image B with A. If a region in image A is similar to the same region
    in image B, then add it to the same branch in the tree. The criteria of similarity
    may vary from implementation to implementation, but it's usually related to the
    image area or general shape. If a region in image A appears to be split in image
    B, create two new branches in the tree for the new regions, and associate them
    with the previous branch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *A = B* and go back to step 2, until a maximum threshold is applied.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This will assemble a tree of regions, as shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Extremal regions](img/B04283_11_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Image source: [http://docs.opencv.org/master/da/d56/group__text__detect.html#gsc.tab=0](http://docs.opencv.org/master/da/d56/group__text__detect.html#gsc.tab=0)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The resistance to variance is determined by counting the number of nodes that
    are in the same level.
  prefs: []
  type: TYPE_NORMAL
- en: By analyzing this tree, it's also possible to determine the **MSERs** (**Maximally
    Stable Extremal Regions**), that is, the regions where the area remains stable
    in a wide variety of thresholds. In the previous image, it is clear that these
    areas will contain the letters *O*, *N*, and *Y*. The main disadvantage of MSERs
    is that they are weak in the presence of blur. OpenCV provides a MSER feature
    detector in the `feature2d` module.
  prefs: []
  type: TYPE_NORMAL
- en: Extremal regions are interesting because they are strongly invariant to illumination,
    scale, and orientation. They are also good candidates for text because they are
    also invariant of the type of font used, even when the font is styled. Each region
    can also be analyzed in order to determine its boundary ellipsis and have properties,
    such as affine transformation and area that can be numerically determined. Finally,
    it's worth mentioning that this entire process is fast, which makes it a very
    good candidate for real-time applications.
  prefs: []
  type: TYPE_NORMAL
- en: Extremal region filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although MSERs are a common approach to define which extremal regions are worth
    working with, the Neumann and Matas algorithm uses a different approach by submitting
    all extremal regions to a sequential classifier that is trained for character
    detection. This classifier works in the following two different stages:'
  prefs: []
  type: TYPE_NORMAL
- en: The first stage incrementally computes descriptors (the bounding box, perimeter,
    area, and Euler number) for each region. These descriptors are submitted to a
    classifier that estimates how probable the region is for it to be a character
    in the alphabet. Then, only the regions of high probability are selected to stage
    2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this stage, the features of the whole area ratio, convex hull ratio, and
    the number of outer boundary inflexion points are calculated. This provides a
    more detailed information that allows the classifier to discard `nontext` characters,
    but they are also much slower to calculate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In OpenCV, this process is implemented in an `ERFilter` class. It is also possible
    to use different image single-channel projections such as R, G, B, luminance,
    or grayscale conversion to increase the character recognition rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, all the characters must be grouped in text blocks (such as words or
    paragraphs). OpenCV 3.0 provides two algorithms for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prune Exhaustive Search**: This was also proposed by Mattas in 2011\. This
    algorithm does not need any previous training or classification, but is limited
    to a horizontally aligned text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical Method for Oriented Text**: This deals with texts in any orientation,
    but needs a trained classifier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since these operations require classifiers, it is also necessary to provide
    a trained set as an input. OpenCV3.0 provides some of these trained sets in the
    `sample` package. This also means that this algorithm is sensitive to the fonts
    used in classifier training.
  prefs: []
  type: TYPE_NORMAL
- en: A demonstration of this algorithm can be seen in the video provided by Neumann
    at [https://youtu.be/ejd5gGea2Fo](https://youtu.be/ejd5gGea2Fo).
  prefs: []
  type: TYPE_NORMAL
- en: Once the text is segmented, it just needs to be sent to an OCR, such as Tesseract,
    similar to what we did in the previous chapter. The only difference is that now
    we will use OpenCV text module classes to interact with Tesseract, since they
    provide a way to encapsulate the specific OCR engine we are using.
  prefs: []
  type: TYPE_NORMAL
- en: Using the text API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enough of theory. It's time to see how the text module works in practice. Let's
    study how to use it to perform text detection, extraction, and identification.
  prefs: []
  type: TYPE_NORMAL
- en: Text detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start with creating a simple program to perform text segmentation using
    `ERFilters`. In this program, we will use the trained classifiers from text API
    samples. You can download them from the OpenCV repository, but they are also available
    in the book's companion code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we start with including all the necessary `libs` and using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall from our previous section that the `ERFilter` works separately in each
    image channel. So, we must provide a way to separate each desired channel in a
    different single `cv::Mat` channel. This is done by the `separateChannels` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: First, we verify that the image is a single channel image (for example, a grayscale
    image). If that's the case, we just add this image and its negative to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, we check whether it''s an RGB image. For colored images, we call
    the `computeNMChannels` to split the image in its several channels. The function
    is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Its parameters are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`src`: This is the source input array. It should be a colored image of type
    8UC3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`channels`: This is a vector of mats that will be filled with the resulting
    channels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mode`: This defines the channels that will be computed. There are two possible
    values that can be used, which are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERFILTER_NM_RGBLGrad`: This indicates that the algorithm uses an RGB color,
    lightness, and gradient magnitude as channels (default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERFILTER_NM_IHSGrad`: This indicates that the image will be split by its intensity,
    hue, saturation, and gradient magnitude.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also append the negatives of all color components in the vector. Finally,
    if another kind of image is provided, the function will terminate the program
    with an error message.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Negatives are appended so the algorithms will cover both bright text on a dark
    background and dark text on a bright background. There is no point in adding a
    negative to the gradient magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed to the `main` method. We''ll use the program to segment the
    `easel.png` image, which is provided with the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text detection](img/B04283_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This image was taken by a mobile phone camera, while I was walking on the street.
    Let''s code so that you can also use a different image easily by providing its
    name in the first program argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll convert the image to grayscale and separate its channels by calling
    the `separateChannels` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to work with all the channels in a colored image, just replace
    the first two lines of the preceding code with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to analyze six channels (RGB + inverted) instead of two (gray
    + inverted). Actually the processing time will increase much more than the improvements
    that we can `get.With` the channels in hand, we need to create `ERFilters` for
    both the stages of the algorithm. Luckily, the `opencv text` contribution module
    provides functions for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For the first stage, we call the `loadClassifierNM1` function to load a previously
    trained classification model. The XML containing the training data is its only
    argument. Then, we call `createERFilterNM1` to create an instance of the `ERFilter`
    class that will perform the classification. The function has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cb`: This is the classification model. This is the same model that we loaded
    with the `loadCassifierNM1` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thresholdDelta`: This is the amount to be added to the threshold in each algorithm
    iteration. The default value is *1*, but we''ll use *15* in our example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minArea`: This is the minimum area of the ER where text can be found. This
    is measured in *%* of the image size. ERs with areas smaller than this are immediately
    discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxArea`: This is the maximum area of the ER where text can be found. This
    is also measured in *%* of the image size. ERs with areas greater than this are
    immediately discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minProbability`: This is the minimum probability that a region must have to
    be a character in order to remain for the next stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nonMaxSupression`: This indicates that non-maximum suppression will be done
    in each branch probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minProbabilityDiff`: This is the minimum probability difference between the
    minimum and maximum extreme region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process for the second stage is similar. We call `loadClassifierNM2` to
    load the classifier model for the second stage and `createERFilterNM2` to create
    the second stage classifier. This function only takes the loaded classification
    model and a minimum probability that a region must achieve to be considered a
    character as input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s call these algorithms in each channel to identify all possible text
    regions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we used the `run` function of the `ERFilter` class. This
    function takes the following two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The input channel**: This is the image to be processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The regions**: In the first stage algorithm, this argument will be filled
    with the detected regions. In the second stage (performed by `filter2`), this
    argument must contain the regions selected in stage 1, which will be processed
    and filtered by stage 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we release both the filters, since they will not be needed anymore
    in the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final segmentation step is to group all `ERRegions` into possible words
    and define their bounding boxes. This is done by calling the `erGrouping` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This function has the following signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the definition of each parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`img`: This is the original input image. You can refer to the following observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regions`: This is a vector of single-channel images where regions are extracted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groups`: This is an output vector of indexes of grouped regions. Each group
    region contains all extremal regions of a single word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupRects`: This is a list of rectangles with the detected text regions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`method`: This is a method of grouping. It can be as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERGROUPING_ORIENTATION_HORIZ`: This is the default value. This only generates
    groups with horizontally oriented text by performing an exhaustive search, as
    proposed originally by Neumann and Matas.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ERGROUPING_ORIENTATION_ANY`: This generates groups with text in any orientation,
    using **Single Linkage Clustering** and **classifiers**. If you use this method,
    the filename of the classifier model must be provided in the next parameter.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Filename`: This is the name of the classifier model. It is only needed if
    `ERGROUPING_ORIENTATION_ANY` is selected.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minProbability`: This is the minimum detected probability of accepting a group.
    Also, it is only needed if the `ERGROUPING_ORIENTATION_ANY` is used.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code also provides a call to the second method, but it''s commented. You
    can switch between the two to test it. Just comment the previous call and uncomment
    this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: For this call, we also used the default trained classifier provided in the text
    module sample package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we draw the region boxes and show the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the program is shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text detection](img/B04283_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can check the complete source code in the `detection.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While most OpenCV text module functions are written to support both grayscale
    and colored images as their input parameters, by the time this book was written,
    there were bugs that prevented using grayscale images in functions, such as erGrouping;
    for instance. Refer to [https://github.com/Itseez/opencv_contrib/issues/309](https://github.com/Itseez/opencv_contrib/issues/309).
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that the OpenCV contribute modules package is not as stable
    as the default `opencv` packages.
  prefs: []
  type: TYPE_NORMAL
- en: Text extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we detected the regions, we must crop the text before we submit it
    to the OCR. We can simply use a function such as `getRectSubpix` or `Mat::copy`
    using each region rectangle as a **ROI** (**region of interest**), but since the
    letters are skewed, some undesired text may be cropped as well; for instance,
    this is what one of the regions will look like if we just extract the ROI based
    in its given rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text extraction](img/B04283_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Fortunately, the `ERFilter` provides us with an object called `ERStat`, which
    contains pixels inside each extremal region. With these pixels, we can use the
    OpenCV `floodFill` function to reconstruct each letter. This function is capable
    of painting similar colored pixels based in a seed point, just like the `bucket`
    tool of most drawing applications. This is what the function signature looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand these parameters and see how they can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`image`: This is the input image. We''ll use the channel image where the extremal
    region was taken. This is where the function normally does the flood fill, unless
    the `FLOODFILL_MASK_ONLY` is supplied. In this case, the image remains untouched
    and the drawing occurs in the mask. That''s exactly what we will do.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mask`: The mask must be an image two rows and columns greater than the input
    image. When flood fill draws a pixel, it verifies that the corresponding pixel
    in the mask is zero. In that case, it will draw and mark this pixel as one (or
    the other value passed in the flags). If the pixel is not zero, flood fill does
    not paint the pixel. In our case, we''ll provide a blank mask, so every letter
    will get painted in the mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seedPoint`: This is the starting point. It''s similar to the place where you
    click when you want to use the "bucket" tool of a graphic application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`newVal`: This is the new value of the repainted pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loDiff and upDiff`: These parameters represent the lower and upper difference
    between the pixels being processed and their neighbors. The neighbor will be painted
    if it falls in this range. If the `FLOODFILL_FIXED_RANGE` flag is used, the difference
    between the seed point and the pixels being processed will be used instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rect`: This is the optional parameter that limits the region where the flood
    fill will be applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`: This value is represented by a bit mask.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The least significant eight bits of the flag contain a connectivity value. A
    value of 4 indicates that all the four edge pixels will be used, and a value of
    8 will indicates that diagonal pixels must also be taken into account. We'll use
    four for this parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The next 8 to 16 bits contain a value from 1 to 255 and are used to fill the
    mask. Since we want to fill the mask with white, we'll use 255 << 8 for this value.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two more bits that can be set by adding the `FLOODFILL_FIXED_RANGE`
    and `FLOODFILL_MASK_ONLY` flags, as described earlier.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll create a function called `drawER`. This function will receive four parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A vector with all processed channels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ERStat` regions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The group that must be drawn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The group rectangle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This function will return an image with the word represented by this group.
    Let''s start with this function by creating the mask image and defining the flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll loop though each group. It''s necessary to find the region index
    and its stats. There''s a chance that this extreme region will be the root, which
    does not contain any points. In this case, we''ll just ignore it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can read the pixel coordinate from the `ERStat` object. It''s represented
    by the pixel number, counting from top to bottom, left to right. This linear index
    must be converted to a *row (y)* and *column (z)* notation, using a formula similar
    to the one that we discussed in [Chapter 2](ch02.html "Chapter 2. An Introduction
    to the Basics of OpenCV"), *An Introduction to the Basics of OpenCV*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can call the `floodFill` function. The `ERStat` object gives us the
    value that we need to use in the `loDiff` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After we do this for all regions in the group, we''ll end it with an image
    a little bigger than the original one, with a black background and the word in
    white letters. Now, let''s crop just the area of the letters. Since the region
    rectangle was given, we start with defining it as our region of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll find all nonzero pixels. This is the value that we''ll use in
    the `minAreaRect` function to get the rotated rectangle around the letters. Finally,
    we borrow the previous chapter''s `deskewAndCrop` function to crop and rotate
    the image for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result of the process for the easel image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text extraction](img/B04283_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Text recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.html "Chapter 10. Developing Segmentation Algorithms for
    Text Recognition"), *Developing Segmentation Algorithms for Text Recognition*,
    we used the Tesseract API directly to recognize the text regions. This time, we'll
    use OpenCV classes to accomplish the same goal.
  prefs: []
  type: TYPE_NORMAL
- en: In OpenCV, all OCR-specific classes are derived from the `BaseOCR` virtual class.
    This class provides a common interface for the OCR execution method itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specific implementations must inherit from this class. By default, the text
    module provides three different implementations: `OCRTesseract`, `OCRHMMDecoder`,
    and `OCRBeamSearchDecoder`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This hierarchy is depicted in the following class diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text recognition](img/B04283_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: With this approach, we can separate the part of the code where the OCR mechanism
    is created from the execution itself. This makes it easier to change the OCR implementation
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start with creating a method that decides which implementation we''ll
    use based on a string. We will currently support Tesseract. However, you can take
    a look at the chapter code where a demonstration with `HMMDecoder` is also provided.
    We are also accepting the OCR engine name in a string parameter, but we can improve
    our application flexibility by reading it from an external `JSON` or `XML` configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You may notice that the function returns a `Ptr<BaseOCR>`. Now, take a look
    at the highlighted code. It calls the create method to initialize a Tesseract
    OCR instance. Let''s take a look at its official signature, since it allows several
    specific parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s dissect each one of these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`datapath`: This is the path to the `tessdata` files of the root directory.
    The path must end with a backslash `/` character. The `tessdata` directory contains
    the language files you installed. Passing `nullptr` to this parameter will make
    Tesseract search in its installation directory, which is the location where this
    folder is normally present. It''s common to change this value to `args[0]` when
    deploying an application and include the `tessdata` folder in your application
    path.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`language`: This is a three letter word with the language code (for example,
    eng for English, por for Portuguese, or hin for Hindi). Tesseract supports loading
    of multiple language codes using the + sign. So, passing *eng+por* will load both
    English and Portuguese languages. Of course, you can only use languages that you
    previously installed; otherwise, the loading will fail. A language configuration
    file can specify that two or more languages must be loaded together. To prevent
    this, you can use a tilde ~. For example, you can use *hin+~eng* to guarantee
    that English is not loaded with Hindi, even if it is configured to do so.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`whitelist`: This is the character set to be considered for recognition. If
    `nullptr` is passed, the characters will be `0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oem`: These are OCR algorithms that will be used. They can have one of the
    following values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_TESSERACT_ONLY`: This uses just Tesseract. It''s the fastest method, but
    it also has less precision.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_CUBE_ONLY`: This uses the cube engine. It''s slower, but more precise.
    This will only work if your language was trained to support this engine mode.
    To check whether that''s the case, look for .`cube` files for your language in
    the `tessdata` folder. The support for English language is guaranteed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_TESSERACT_CUBE_COMBINED`: This combines both Tesseract and cube to achieve
    the best possible OCR classification. This engine has the best accuracy and the
    slowest execution time.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OEM_DEFAULT`: This tries to infer the strategy based in the language `config`
    file, command line `config` file, or in the absence of both, uses `OEM_TESSERACT_ONLY`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`psmode`: This is the segmentation mode. The modes are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_OSD_ONLY`: Using this mode, Tesseract will just run its preprocessing
    algorithms to detect orientation and script detection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_AUTO_OSD`: This tells Tesseract to do automatic page segmentation with
    orientation and script detection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_AUTO_ONLY`: This does page segmentation, but avoids doing orientation,
    script detection, or OCR. This is the default value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_AUTO`: This does page segmentation and OCR, but avoids doing orientation
    or script detection.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_COLUMN`: This assumes that the text of variable sizes is displayed
    in a single column.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_BLOCK_VERT_TEXT`: This treats the image as a single uniform block
    of a vertically aligned text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_BLOCK`: This is a single block of text. This is the default configuration.
    We will use this flag since our preprocessing phase guarantees this condition.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_LINE`: This indicates that the image contains only one line of
    text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_WORD`: This indicates that the image contains just one word.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_WORD_CIRCLE`: This informs that the image is a just one word disposed
    in a circle.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PSM_SINGLE_CHAR`: This indicates that the image contains a single character.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For the last two parameters, the `#include tesseract` directory recommends you
    to use the constant names instead of directly inserting their values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to add text detection to our `main` function. To do this,
    just add the following code to the end of the `main` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we started by calling our `initOCR` method to create a Tesseract
    instance. Notice that the remaining code will not change if we choose a different
    OCR engine, since the run method signature is guaranteed by the `BaseOCR` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we iterate over each detected `ERFilter` group. Since each group represents
    a different word, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Call the previously created `drawER` function to create an image with the word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a text string called `word`, and call the `run` function to recognize
    the word image. The recognized word will be stored in the string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Print the text string on the screen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at the run method signature. This method is defined in the
    `BaseOCR` class and will be equal for all specific OCR implementations, even the
    ones that might be implemented in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, this is a pure virtual function that must be implemented by each
    specific class (such as the `OCRTesseract` class that we just used):'
  prefs: []
  type: TYPE_NORMAL
- en: '`image`: This is the input image. It must be an RGB or a grayscale image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_rects`: We can provide a vector to be filled with the bounding box
    of each component (words or text lines) detected by the OCR engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_texts`: If given, this vector will be filled with the text strings
    of each component detected by the OCR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_confidences`: If given, the vector will be filled with floats and
    the confidence values of each component'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`component_level`: This defines what a component is. It may have the `OCR_LEVEL_WORD`
    (by default) or `OCR_LEVEL_TEXT_LINE` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If necessary, we prefer changing the component level to a word or line in the
    `run()` method instead of doing the same thing in the `psmode` parameter of the
    `create()` function. This is preferable since the `run` method will be supported
    by any OCR engine that decides to implement the `BaseOCR` class. Always remember
    that the `create()` method is where vendor specific configurations are set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the program''s final output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text recognition](img/B04283_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Despite a minor confusion with the `&` symbol, every word was perfectly recognized!
    You can check the complete source code in the `ocr.cpp` file, in the chapter code.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw that scene text recognition is a far more difficult
    OCR situation than working with scanned texts. We studied how the text module
    addresses this problem with extremal region identification using the **Newmann
    and Matas** algorithm. We also saw how to use this API with the `floodfill` function
    to extract the text to an image and submit it to Tesseract OCR. Finally, we studied
    how the OpenCV text module integrates with Tesseract and other OCR engines, and
    how we can use its classes to identify what's written in the image.
  prefs: []
  type: TYPE_NORMAL
- en: This ends our journey with OpenCV. From the beginning to the end of this book,
    we expected you to have a glance about the Computer Vision area and have a better
    understanding of how several applications work. We also sought to show you that,
    although OpenCV is quite an impressive library, the field is already full of opportunities
    for improvement and research.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! No matter whether you use OpenCV for creating impressive
    commercial programs based on Computer Vision, or if you use it in a research that
    will change the world, we hope you will find this content useful. Just keep working
    with your skills—this was just the beginning!
  prefs: []
  type: TYPE_NORMAL
