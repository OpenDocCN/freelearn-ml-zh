["```py\nimport pandas as pd\ndf = pd.read_csv(\"./engage-data.csv\")\ndf[\"Engaged\"] = df[\"Response\"].apply(lambda x: 1 if x == \"Yes\" else 0) \n```", "```py\ndf.describe() \n```", "```py\nimport statsmodels.api as sm\ncontinuous_vars = [\n    x for x in df.dtypes[(df.dtypes == float) | (df.dtypes == int)].index if x != \"Engaged\"\n]\nlogit = sm.Logit(\n    df['Engaged'],\n    df[continuous_vars]\n).fit() \n```", "```py\nlogit.summary() \n```", "```py\n labels, levels = df['Education'].factorize() \n```", "```py\nlabels, levels = df['Education'].factorize()\nlabels \n```", "```py\narray([0, 0, 0, ..., 0, 1, 1]) \n```", "```py\nlevels \n```", "```py\nIndex(['Bachelor', 'College', 'Master', 'High School or Below', 'Doctor'], dtype='object') \nfactorize function for the Education variable and get the two variables, labels and levels. The newly created labels variable contains numerical values for each record and the levels variable contains information about what each numerical value of the labels variable means. In this example, Bachelor is encoded as 0, College as 1, Master as 2, High School or Below as 3, and Doctor as 4.\n```", "```py\ncategories = pd.Categorical(\n    df['Education'], \n    categories=['High School or Below', 'Bachelor', 'College', 'Master', 'Doctor'],\ncategories.categories \n```", "```py\nIndex(['High School or Below', 'Bachelor', 'College', 'Master', 'Doctor'], dtype='object') \n```", "```py\ncategories.codes \n```", "```py\narray([1, 1, 1, ..., 1, 2, 2], dtype=int8) \n```", "```py\npd.get_dummies(df['Education']).head(10) \n```", "```py\nlogit = sm.Logit(\n    df['Engaged'],\n    df[[\n        'GenderFactorized',\n        'EducationFactorized'\n    ]]\n).fit() \n```", "```py\nlogit = sm.Logit.from_formula(\n  data=df,\n  formula=\"Engaged ~ Income + EducationFactorized + Income:EducationFactorized\"\n).fit()\nlogit.summary() \n```", "```py\ndf = pd.read_csv(\"./convert-data.csv\", sep=\";\")\ndf[\"conversion\"] = df[\"y\"].apply(lambda x: 1 if x == \"yes\" else 0) \n```", "```py\ncontinuous_vars = [\n    \"age\", \"balance\", \"duration\", \"campaign\", \"previous\"\n]\ndf[continuous_vars].describe() \n```", "```py\nmonths = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\ndf['month'] = df['month'].apply(\n    lambda x: months.index(x)+1\n) \n```", "```py\njobs_encoded_df = pd.get_dummies(df['job']\njobs_encoded_df.columns = ['job_%s' % x for x in jobs_encoded_df.columns] \n```", "```py\njobs_encoded_df.head() \n```", "```py\ndf = pd.concat([df, jobs_encoded_df], axis=1) \n```", "```py\nmarital_encoded_df = pd.get_dummies(df['marital'])\nmarital_encoded_df.columns = ['marital_%s' % x for x in marital_encoded_df.columns]\ndf = pd.concat([df, marital_encoded_df], axis=1)\ndf['housing'] = df['housing'].apply(lambda x: 1 if x == 'yes' else 0) \n```", "```py\nfeatures = (\n    continuous_vars\n    + [\"housing\", \"month\"]\n    + list(jobs_encoded_df.columns)\n    + list(marital_encoded_df.columns)\n)\nresponse_var = 'conversion' \n```", "```py\nfeatures \n```", "```py\nfrom sklearn import tree\ndt_model = tree.DecisionTreeClassifier(\n    max_depth=3\n)\ndt_model.fit(df[features], df[response_var]) \n```", "```py\nconda install python-graphviz \n```", "```py\nimport graphviz\ndot_data = tree.export_graphviz(\n    dt_model,\n    out_file=None,\n    feature_names=features, \n    class_names=['0', '1'], \n    filled=True,\n    rounded=True, \n    special_characters=True\n)\ngraph = graphviz.Source(dot_data, format=\"png\")\ngraph.render(\"conversion-dt-depth-3\") \n```", "```py\npip install dowhy \n```", "```py\ndf.info() \n```", "```py\ndf[\"MultipleProduct\"] = df[\"NumOfProducts\"].apply(lambda x: 1 if x > 1 else 0) \n```", "```py\nfrom dowhy import CausalModel\nmodel = CausalModel(\n    data=df,\n    treatment=\"MultipleProduct\",\n    outcome=\"Exited\",\n    common_causes=[\"Balance\", \"EstimatedSalary\", \"Tenure\"],\n    instruments=[\"Age\", \"CreditScore\"]\n) \n```", "```py\nmodel.view_model() \n```", "```py\nestimands = model.identify_effect()\nprint(estimands) \n```", "```py\nestimate = model.estimate_effect(\n    estimands,\n    method_name = \"backdoor.propensity_score_weighting\"\n)\nprint(estimate) \n```", "```py\nrefute_results = model.refute_estimate(\n    estimands,\n    estimate,\n    \"random_common_cause\"\n)\nprint(refute_results) \n```", "```py\nRefute: Add a random common cause\nEstimated effect:-0.13904650583124545\nNew effect:-0.13904650583124542 \n```", "```py\nrefute_results = model.refute_estimate(\n    estimands,\n    estimate,\n    \"data_subset_refuter\"\n)\nprint(refute_results) \n```", "```py\nRefute: Use a subset of data\nEstimated effect:-0.13904650583124545\nNew effect:-0.13842175180225635 \n```", "```py\nimport networkx as nx\ncausal_graph = nx.DiGraph([\n    ('CreditScore', 'MultipleProduct'),\n    ('Age', 'MultipleProduct'),\n    ('MultipleProduct', 'Exited'),\n    ('Balance', 'MultipleProduct'),\n    ('EstimatedSalary', 'MultipleProduct'),\n    ('Tenure', 'MultipleProduct'),\n    ('Balance', 'Exited'),\n    ('EstimatedSalary', 'Exited'),\n    ('Tenure', 'Exited'),\n]) \n```", "```py\npip install networkx \n```", "```py\n nx.draw_networkx(causal_graph, arrows=True) \n```", "```py\nfrom dowhy import gcm\nscm = gcm.StructuralCausalModel(causal_graph)\ngcm.auto.assign_causal_mechanisms(scm, df)\ngcm.fit(scm, df) \n```", "```py\narrow_strengths = gcm.arrow_strength(scm, target_node='Exited') \n```", "```py\ntotal = sum(arrow_strengths.values())\narrow_strengths_perc = {key: val/total*100 for key, val in arrow_strengths.items()}\ngcm.util.plot(\n    causal_graph,\n    causal_strengths=arrow_strengths_perc,\n    figure_size=[8, 5]\n) \n```", "```py\ninfluence = gcm.intrinsic_causal_influence(\n    scm,\n    target_node='Exited',\n    num_samples_randomization=100\n) \n```", "```py\ntotal = sum([val for key, val in influence.items() if key != \"Exited\"])\ninfluence_perc = {key: val/total*100 for key, val in influence.items() if key != \"Exited\"}\nxlabels = sorted(influence_perc.keys())\nyvals = [influence_perc[x] for x in xlabels]\nplt.bar(xlabels, yvals)\nplt.xticks(rotation=45)\nplt.grid()\nplt.ylabel(\"Variance attribution in %\")\nplt.show() \n```", "```py\nattributions = gcm.attribute_anomalies(\n    scm,\n    target_node='Exited',\n    anomaly_samples=df.loc[df[\"Exited\"] == 1].iloc[0:1]\n) \n```", "```py\ntotal = sum([abs(val[0]) for key, val in attributions.items() if key != \"Exited\"])\nattributions_perc = {\n    key: val[0]/total*100 for key, val in attributions.items() if key != \"Exited\"\n}\nxlabels = sorted(attributions_perc.keys())\nyvals = [attributions_perc[x] for x in xlabels]\nplt.bar(xlabels, yvals)\nplt.xticks(rotation=45)\nplt.grid()\nplt.ylabel(\"Anomaly Attribution (%)\")\nplt.show() \n```", "```py\n df.loc[df[\"Exited\"] == 1].iloc[0][list(attributions_perc.keys())] \n```", "```py\n df.loc[df[\"Exited\"] == 1].iloc[1][list(attributions_perc.keys())] \n```"]