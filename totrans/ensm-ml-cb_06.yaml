- en: When in Doubt, Use Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a random forest for predicting credit card defaults using scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a random forest for predicting credit card defaults using H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A random forest is a supervised machine learning algorithm based on ensemble
    learning. It is used for both regression and classification problems. The general
    idea behind random forests is to build multiple decision trees and aggregate them
    to get an accurate result. A decision tree is a deterministic algorithm, which
    means if the same data is given to it, the same tree will be produced each time.
    They have a tendency to overfit, because they build the best tree possible with
    the given data, but may fail to generalize when unseen data is provided. All the
    decision trees that make up a random forest are different because we build each
    tree on a different random subset of our data. A random forest tends to be more
    accurate than a single decision tree because it minimizes overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates bootstrap sampling being done from the source
    sample. Models are built on each of the samples and then the predictions are combined
    to arrive at a final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7a368ae-ce16-47db-96c7-5ab98ab0b289.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each tree in a random forest is built using the following steps where A represents
    the entire forest, a represents a single tree, for *a = 1* to *A*:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a bootstrap sample with replacement, *D* training from *x*, *y* label
    these *X[a]*[, ]*Y[a]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the tree *f[a]* on *X[a]*, *Y[a]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average the predictions or take the majority vote to arrive at a final prediction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a regression problem, predictions for the test instances are made by taking
    the mean of the predictions made by all trees. This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b058aa6d-f242-48e0-ab2c-e20bca7feca3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* is the total number of trees in the random forest. *a=1* represents
    the first tree in a forest, while the last tree in the forest is *A*. ![](img/b6820836-5f23-4996-b72c-86e17bcfbed3.png)(![](img/d7f322ac-15d3-4190-bea7-6b3f7250eba4.png))
    represents the prediction from a single tree.
  prefs: []
  type: TYPE_NORMAL
- en: If we have a classification problem, majority voting or the most common answer
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a random forest for predicting credit card defaults using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scikit-learn library implements random forests by providing two estimators:
    `RandomForestClassifier`and `RandomForestRegressor`. They take various parameters,
    some of which are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '` n_estimators`:This parameter is the number of trees the algorithm builds
    before taking a maximum vote or the average prediction. In general, the higher
    the number of trees the better the performance and the accuracy of the predictions,
    but it also costs more in terms of computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This parameter is the maximum number of features that the random
    forest is allowed to try in an individual tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_sample_leaf`:This parameter determines the minimum number of leaves that
    are required to split an internal node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_jobs`: This hyperparameter tells the engine how many jobs to run in parallel
    for both fitting the model and predicting new instances. If it has a value of
    `None` or `1`, it runs only one job. A value of `-1` means it will use all the
    processors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state`:This parameter will always produce the same results when it
    has a definite value of `random_state` and if it has been given the same hyperparameters
    and the same training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oob_score`: This parameter is also known as **out-of-the-bag ****sampling**,
    and is a random forest cross-validation method. In this sampling method, about
    one-third of the data is not used to train the model and can be used to evaluate
    its performance. These samples are called the **out-of-the-bag** **samples**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we use a dataset from the UCI ML repository on credit card
    defaults. This dataset contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Default payments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demographic factors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Credit data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: History of payments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bill statements of credit card clients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data and the data descriptions are provided in the GitHub folder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading the required libraries and reading our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working folder as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now read our data. We will prefix the DataFrame name with `df_` so that
    we can understand it easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the shape of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the datatypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We drop the `ID` column, as this is not required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can explore our data in various ways. Let''s take a look at a couple of
    different methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that we have used a semicolon in the last line in the preceding code block.
    The semicolon helps to hide the verbose information produced by Matplotlib. `xlabelsize`
    and `ylabelsize` are used to adjust the font size in the x-axis and the y-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the distribution of the numeric variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ea0dcfd-9bd1-47c3-9207-8d476ce78ee0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now explore the payment defaults by age group. We bucket the `age`
    variable and store the binned values in a new variable, `age_group`, in `df_creditcarddata`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then use our new `age_group` variable to plot the number of defaults per
    age group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the amount of defaults per age:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a01f25a0-5db7-469d-b4b7-e1638ac2ed03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can drop the `age_group` variable from `df_creditcarddata` since we do not
    need it anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now look at the payment defaults according to the credit limits of
    the account holders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ca04a04-c2dc-4bac-bbf1-55319d9dee18.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also assign labels to some of our variables to make the interpretations
    better. We assign labels for the `Gender`, `Marriage`, and `Education` variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also change the datatype of the `pay` variables to the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There are more explorations available in the code bundle provided with this
    book. We now move on to training our random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look at how to use a random forest to train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by splitting our target and feature variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate the numerical and non-numerical variables in our feature set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We dummy code the categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We concatenate the dummy code variables to our DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We split our dataset into training and testing subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We scale the features with `StandardScaler()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We might notice that the column names have been changed to numbers. We assign
    the columns names and index values back to the scaled DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We import `RandomForestClassifier()` from `sklearn.ensemble`. We will then
    build our random forest classifier model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we calculate the accuracy of our training model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the **f****alse positive rate** (**FPR**) and **t****rue positive rate** (**TPR**)
    by passing `y_test` and `y_pred_proba` to `roc_curve()`. We also get the `auc `value
    using `roc_auc_score()`. Using the FPR, TPR, and the AUC value, we plot the ROC
    curve with the AUC value annotated on the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the ROC curve with the AUC value annotated on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43c5f4c5-d916-4f76-a3b4-8bf301a6926f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also evaluate other scores, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the following evaluation scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dac2941b-1804-4d74-977b-f9878a69d72a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also evaluate a few statistics based on the class of the target variable,
    which in this case is `0` or `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`classification_report` from `sklearn.metrics` gives us the following scores
    based on each class of the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb489a75-2234-4f49-86ce-191e94fd501c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can plot the top 10 variables by feature importance to see which variables
    are important for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the top 10 variables with their relative importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/152d22d8-51d0-4861-9e34-66ac936652a2.png)'
  prefs: []
  type: TYPE_IMG
- en: We can change the hyperparameters to see how the model can perform better. We
    can also perform a grid search over combinations of hyperparameter values to fine-tune
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we split our target and feature variables. In *Step 2*, in our
    feature set, we separated the numeric and non-numeric variables. In *Step 3* and
    *Step 4*, we converted the non-numeric variables to dummy coded variables and
    added them back to the DataFrame. In *Step 5*, we split our dataset into training
    and testing subsets, and in *Step 6*, we imported `StandardScaler()` from `sklearn.preprocessing`
    and applied the same scale to our features.
  prefs: []
  type: TYPE_NORMAL
- en: After executing the commands in *Step 6*, we noticed that the column names had
    changed to sequential numbers. For this reason, in *Step 7*, we assigned the column
    names and the index values back to the scaled DataFrame. In *Step 8*, we imported `RandomForestClassifier()`
    from `sklearn.ensemble` and built our first random forest classifier model. After
    that, in *Step 9* and *Step 10*, we used our model to calculate the accuracy of
    our training model and plotted the ROC curve respectively. We also annotated the
    ROC Curve with the AUC value.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 11*, we evaluated other scores, including the kappa value, the precision,
    the recall, and the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we also evaluated these scores based on each class of the target
    variable, which in this case is `0` or `1`, using `classification_report` from
    `sklearn.metrics`. There, `classification_report()` provides us with metrics such
    as precision, recall, and f1-score by each class, as well as the average of each
    of the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '`classification_report()` reports averages, including averaging the total true
    positives, false negatives and false positives, averaging the unweighted mean
    per label, and averaging the support-weighted mean per label. It also reports
    sample averages for multi-label classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in *Step 13*, we looked at the relative variable importance of the
    top 10 features. This can help in feature selection to build the models with the
    right features.
  prefs: []
  type: TYPE_NORMAL
- en: There are various feature selection methods available, such as averaged variable,
    importance, Boruta, recursive feature selection, and variable selection using
    RF.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Isolation forest is another algorithm that is built on the basis of decision
    trees, and it's used for anomaly and outlier detection. This algorithm is based
    on the assumption that the outlier data points are rare.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works a bit differently to the random forest. It creates a bunch
    of decision trees, then it calculates the path length necessary to isolate an
    observation in the tree. The idea is that isolated observations, or anomalies,
    are easier to separate because there are fewer conditions necessary to distinguish
    them from normal cases. Thus, the anomalies will have shorter paths than normal
    observations and will, therefore, reside closer to the root of the tree. When
    several decision trees are created, the scores are averaged, which gives us a
    good idea about which observations are truly anomalies. As a result, isolation
    forests are used for outliers and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Also, an isolation forest does not utilize any distance or density measures
    to detect an anomaly. This reduces the computational cost significantly compared
    to the distance-based and density-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, `sklearn.ensemble.IsolationForest` provides an implementation
    of the isolation forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn implementation of the isolation forest algorithm can be found
    here: [https://bit.ly/2DCjGGF](https://bit.ly/2DCjGGF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing random forest for predicting credit card defaults using H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O is an open source and distributed machine learning platform that allows
    you to build machine learning models on large datasets. H2O supports both supervised
    and unsupervised algorithms and is extremely fast, scalable, and easy to implement. H2O's
    REST API allows us to access all its functionalities from external programs such
    as R and Python. H2O in Python is designed to be very similar to scikit-learn. At
    the time of writing this book, the latest version of H2O is H2O v3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why H2O brought lightning-fast machine learning to enterprises is
    given by the following explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '"H2O''s core code is written in Java. Inside H2O, a distributed key/value store
    is used to access and reference data, models, objects, and so on, across all nodes
    and machines. The algorithms are implemented on top of H2O''s distributed Map/Reduce
    framework and utilize the Java fork/join framework for multi-threading. The data
    is read in parallel and is distributed across the cluster and stored in memory
    in a columnar format in a compressed way. H2O''s data parser has built-in intelligence
    to guess the schema of the incoming dataset and supports data ingest from multiple
    sources in various formats"'
  prefs: []
  type: TYPE_NORMAL
- en: '- from h2o.ai'
  prefs: []
  type: TYPE_NORMAL
- en: H2O provides us with distributed random forests, which are a powerful tool used
    for classification and regression tasks. This generates multiple trees, rather
    than single trees. In a distributed random forest, we use the average predictions
    of both the classification and regression models to reach a final result.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Java is an absolute must for H2O to run. Make sure you have Java installed
    with the following command in Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You will now need to install H2O. To install this from Jupyter, use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To use H2O, we need to initialize an instance and connect to it. We can do
    that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the preceding command tries to connect to an instance. If it fails
    to do so, it will attempt to start an instance and then connect to it. Once connected
    to an instance, we will see the details of that instance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/845d5994-fe6f-4d56-abfe-db1c235d5142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We read our data into a `pandas` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We change our `pandas` DataFrame to an H2O DataFrame using `h2o.H2OFrame()`.
    We name the `df_creditcarddata` H2O DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Check whether the data in the H2O DataFrame is properly loaded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the summary statistics with the `describe()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We drop the ID column, as this will not be required for our model building
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We will now move on to explore our data and build our model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have performed various explorations on our data in the previous section.
    There is no limit to the ways in which we can explore our data. In this section,
    we are going to look at a few more techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We check the correlation of each of our feature variables with the target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot shows how each of the features is correlated with the target
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fac4a15a-c60d-4fca-9ae4-01edb2638f15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We check the datatypes in the H2O DataFrame. Note that for the `pandas` DataFrame, we
    used `dtypes`. For the H2O DataFrame, we use types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that they are all of the integer datatype. We will convert them to
    factor type, which is categorical in nature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We can check the datatypes with `hf_creditcarddata.types` to see that the datatype
    conversion has taken place.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will encode the binary target variable as a factor type variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We select the features and the `target` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We now split the H2O DataFrame into training and testing subsets. We use 70%
    of our data for training the model and the remaining 30% for validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We build our random forest model with the default settings. You can check the
    model performance on the test data with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following performance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b237e8e-a899-4125-8590-a66ef4baa7a0.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Getting ready* section, we installed JRE and H2O. We initialized and
    connected to an H2O instance with `h2o.init()`. We then read our data using `pandas`
    and converted it to an H2O DataFrame. We used the `head()` and `describe()` methods
    on the H2O DataFrame, just like we used them on a `pandas` DataFrame. We then
    dropped the `ID` column from the H2O DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: After we did these data explorations in the *Getting ready* section, we moved
    on to the next steps. In *Step 1,* we checked the correlation of each of the features
    with the `target` variable. In *Step 2*, we used the `h2o` DataFrame and checked
    the datatypes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for the `pandas` DataFrame we used `dtypes`, whereas we used `types`
    with the `h2o `DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we used `asfactor()` to convert the numeric variables to the categorical
    type. We performed this on variables that were supposed to be of a categorical
    type but were appearing as numeric.
  prefs: []
  type: TYPE_NORMAL
- en: In previous examples, we used the `astype()` method on a `pandas` DataFrame.
    With an H2O DataFrame, we used the `asfactor()` method.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we used `asfactor()` on our `target` variable to convert it to
    a categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we separated our features and the `target` variable. In *Step*
    6, we split the H2O DataFrame into training and testing subsets using `split_frame()`
    on our H2O DataFrame. We used the `ratios` parameter and set it to `ratios=[0.7]`
    for `split_frame()` to allocate 70% of the data to the training set and 30% of
    the data to the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we imported `H2ORandomForestEstimator` from `h2o.estimators.random_forest`.
    We passed `model_id` and then referred to it to call the `train()` function and
    pass the predictor and the `target` variables. We then looked at the performance
    metrics by passing the test subset to `model_performance()`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our preceding example, we have an AUC of `0.76` and a log loss of `0.44`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply cross-validation by passing `nfolds` as a parameter to `H2ORandomForestEstimator()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that the AUC has slightly improved to `0.77` and that the log loss
    has dropped to `0.43`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee005306-9054-40e0-b54f-eb0931b04b2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also apply a grid search to extract the best model from the given options.
    We set our options as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We build the model with the preceding search parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We now sort all models by AUC in a descending manner and then pick the first
    model, which has the highest AUC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the best model for our test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the variable importance from the best model that we have achieved
    so far:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1014ef94-8e46-46df-af56-802e3ba9589f.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may want to look into extremely randomized trees, which have a slightly
    different implementation but can sometimes perform better than random forests.
  prefs: []
  type: TYPE_NORMAL
- en: In ensemble methods, each model learns differently in terms of the subset of
    the dataset and the subset of the feature vector used for training. These subsets
    are taken randomly. Extremely randomized trees possess a high randomness factor
    in the way they compute the splits and the subset of the features selected. Unlike
    random forests, in which the splitting threshold is chosen randomly, in extremely
    randomized trees, a discriminative threshold is used as the splitting rule. Due
    to this, the overall variance of the ensemble decreases and the overall performance
    may be better.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn implementation of extremely randomized trees can be found at
    the following link: [https://bit.ly/2zWsNNS](https://bit.ly/2zWsNNS)[. H2O also
    supports extremely randomized trees. ](https://bit.ly/2zWsNNS)
  prefs: []
  type: TYPE_NORMAL
