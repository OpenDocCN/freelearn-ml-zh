<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>What is a Feature?</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">What is a Feature?</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous chapter, our&#160;main focus was filtering an image and applying different transformations on it. These are good techniques to analyze images but are not sufficient for the majority of computer vision tasks. For example, if we were to make a product detector for a shopping store, computing only edges may not be enough to say whether the image is of an orange or an apple. On the other hand, if a person is given the same task, it is very intuitive to differentiate between an orange and an apple. This is because of the fact that human perception combines several features,&#160;<span>such as texture, color, surface, shape, reflections, and so on,&#160;</span>&#160;to distinguish between one object with another. This motivates to look for more details that relates to complex features of objects. These complex features can then be used in high level image vision tasks like image recognition, search, and so on.&#160;<span>There are, however, cases where someone just walks straight into a glass wall, which is due not being able to find enough features to say whether it is free space or glass.</span></p>
<p>In this chapter, we will first begin with an explanation features and its importance in computer vision. Later in the chapter, we will different types of features extractors like Harris Corner Detector, FAST keypoint detectors, ORB features detectors. The visualization of the keypoints using each of them are also described using OpenCV. Lastly, the effectiveness of ORB features is shown with two similar applications. We will also see a brief discussion on black box features.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Features use cases&#160;</h1>
                </header>
            
            <article>
                
<p><span class="md-line md-end-block md-focus"><span class="md-expand">Following are some of the generic applications that are popular in computer vision:</span></span></p>
<ul>
<li><span>We have two images and we would like to quantify whether these images match each other. Assuming a comparison metric, we say that the image matches when our comparison metric value is greater than a threshold.</span></li>
<li>In another example, we have a large database of images, and for a new image, we want to perform an operation similar to matching. Instead of recomputing everything for every image, we can store a smaller, easier to search and robust enough to match, representation of images. This is often referred to as a feature vector of the image. Once a new image comes, we extract similar representation for the new image and search for the nearest match among the previously generated database. This representation is usually formulated in terms of features.</li>
<li>Also, in the case of finding an object, we have a small image of an object or a region called a&#160;<strong>template</strong><span>. The goal is to check whether an image has this template. This would require matching key points from the template against the given sample image. If the match value is greater than a threshold, we can say the sample image has a region similar to the&#160;</span>given template<span>. To further enhance our finding, we can also show where in the sample image lies our template image.</span></li>
</ul>
<p class="mce-root">Similarly, a computer vision system needs to learn several features that describes an object such that it is quite easy to distinguish from other objects.&#160;</p>
<p class="mce-root">When we design software to do image matching or object detection in images, the basic pipeline for detection is formulated from a machine learning perspective. This means that we take a set of images, extract significant information, learn our model and use the learned model on new images to detect similar objects. In this section, we will explore more on this. &#160;</p>
<p>&#160;In general, an image matching procedure looks as follows:&#160;</p>
<ul>
<li>The first step is to extract robust features from a given image. This involves searching through the whole image for possible features and then thresholding them. There are several techniques for the selection of features such as SIFT[3], SURF[4], FAST[5], BRIEF[6], ORB detectors[2], and so on. The feature extracted, in some cases, needs to be converted into a more descriptive form such that it is learnt by the model or can be stored for re-reading. &#160;</li>
<li>In the case of feature matching, we are given a sample image and we would like to see whether this matches a reference image. After feature detection and extraction, as shown previously, a distance metric is formed to compute the distance between features of a sample with respect to the features of reference. If this distance is less than the threshold, we can say the two images are similar. &#160;</li>
<li>For feature tracking, we omit previously explained feature matching steps. Instead of globally matching features, the focus is more on neighborhood matching. This is used in cases such as image stabilization, object tracking, or motion detection.&#160;</li>
</ul>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets and libraries</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will use <kbd>OpenCV</kbd> library for performing feature detection and matching. The plots are generated using <kbd>matplotlib</kbd>. We will be using custom images to show the results of various algorithms. However, the code provided here should work on webcam or other custom images too.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Why are features important?</h1>
                </header>
            
            <article>
                
<p>Features play a major role in creating good quality computer vision systems. One of the first features we can think of is&#160;<strong>pixels</strong>. In order to create a comparison tool, we use an average of squared distance between the pixel values of two images. These, however, are not robust because rarely will you see two images that are exactly the same. There is always some camera movement and illumination changes between images, and computing a difference between pixel values will be giving out large values even when the images are quite similar.</p>
<p>There are, however, other kinds of features that take into account local and global properties of an image. The local properties are referred to as image statistics around the neighborhood of the image, while global refers to considering overall image statistics. Since both local, and global properties of an image provide significant information about an image, computing features that can capture these will make them more robust and accurate in applications.&#160; &#160;</p>
<p>The most basic form of feature detector is point features. In applications such as panorama creation on our smartphones, each image is stitched with the corresponding previous image. This stitching of image requires correct orientation of an image overlapped at pixel level accuracy. Computing corresponding pixels between two images requires pixel matching.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Harris Corner Detection</h1>
                </header>
            
            <article>
                
<p class="mce-root">We start feature point detection using the Harris Corner Detection[1] technique. In this, we begin with choosing a matrix, termed a <strong>window</strong>, which is small in size as compared to the image size.&#160;</p>
<p class="mce-root">The basic idea is to first overlay chosen window on the input image and observe only the overlayed region from the input image. This window is later shifted over the image and the new overlayed region is observed. In this process, there arise three different cases:</p>
<ul>
<li class="mce-root">If there is a flat surface, then we won't be able to see any change in the window region irrespective of the direction of movement of the window. This is because there is no edge or corner in the window region.</li>
<li class="mce-root">In our second case, the window is overlayed on edge in the image and shifted. If the window moves along the direction of the edge, we will not be able to see any changes in the window. While, if the window is moved in any other direction, we can easily observe changes in the window region.</li>
<li class="mce-root">Lastly, if the window is overlayed on a corner in the image and is shifted, where the corner is an intersection of two edges, in most of the cases, we will be able to observe the changes in the window region.</li>
</ul>
<p>Harris Corner Detection uses this property in terms of a score function. Mathematically, it is given as:</p>
<div style="padding-left: 60px" class="mce-root CDPAlignLeft CDPAlign"><img height="40" width="454" src="images/57db2e62-ffbc-41f7-8bc3-c5c706c99c02.png"/></div>
<p>Where <em>w</em> is a window, <em>u</em> and <em>v</em> are the shift and <em>I</em> is image pixel value. The output <em>E</em> is the objective function and maximizing this with respect to <em>u</em> and <em>v</em> results in corner pixels in the image <em>I</em>.</p>
<p class="mce-root">The Harris Corner Detection score value will show whether there is an edge, corner, or flat surface. An example of Harris Corners of different kinds of images is shown in the following figure:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="309" width="562" class="alignnone size-full wp-image-518 image-border" src="images/26e4734c-afa1-48e7-b2b6-afa9e56c4236.png"/></div>
<p><span>In the previous figure, the upper row has input images, while the bottom row has detected corners. These corners are shown with small gray pixels values corresponding to the location in the input image.&#160; In order to generate an image of corners for a given colored image, u</span>se the following code:</p>
<pre><strong># load image and convert to grayscale</strong><br/><strong>img = cv2.imread('../figures/flower.png')</strong><br/><strong>gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</strong><br/><br/><strong># harris corner parameters</strong><br/><strong>block_size = 4 # Covariance matrix size</strong><br/><strong>kernel_size = 3 # neighbourhood kernel</strong><br/><strong>k = 0.01 # parameter for harris corner score</strong><br/><br/><strong># compute harris corner</strong><br/><strong>corners = cv2.cornerHarris(gray, block_size, kernel_size, k)</strong><br/><br/><strong># create corner image</strong><br/><strong>display_corner = np.ones(gray.shape[:2])</strong><br/><strong>display_corner = 255*display_corner</strong><br/><strong># apply thresholding to the corner score</strong><br/><strong>thres = 0.01 # more than 1% of max value</strong><br/><strong>display_corner[corners&gt;thres*corners.max()] = 10 #display pixel value</strong><br/><br/><strong># set up display</strong><br/><strong>plt.figure(figsize=(12,8))</strong><br/><strong>plt.imshow(display_corner, cmap='gray')</strong><br/><strong>plt.axis('off')</strong></pre>
<p>We can generate different number of corners for an image by changing the parameters such as covariance matrix block size, neighbourhood kernel size and Harris score parameter. In the next section, we will see more robust feature detectors.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">FAST features</h1>
                </header>
            
            <article>
                
<div>
<p>Many features detectors are not useful for real-time applications such as&#160; a robot with a camera is moving on the streets. Any delay caused may decrease the functionality of the robot or complete system failure. Features detection is not the only part of the robot system but if this effects the runtime, it can cause significant overhead on other tasks to make it work real time.&#160;</p>
<p><strong>FAST</strong>&#160;(<strong>Features from Accelerated Segment Test</strong>)[5], was introduced by Edward Rosten and Tom Drummond in 2006.&#160;The algorithm uses pixel neighborhood to compute key points in an image.&#160;<span>The algorithm for FAST feature detection is as follows:</span></p>
</div>
<ol>
<li>An interesting <span>point candidate pixel</span> <strong>(i,j)</strong> <span>is selected with an intensity <em>I (i,j)</em>:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img style="font-size: 1em;text-align: center" height="216" width="229" src="images/9d6d678d-ac93-4f05-a4ab-41e561fa4109.png"/></div>
<ol start="2">
<li>In a circle of 16 pixels, given a threshold <em>t,</em> estimate <em>n</em> adjoining points which are brighter than pixel <em>(i,j)</em> intensity by a threshold <em>t</em> or darker than <em>(i,j)</em> pixel intensity by a threshold <em>t</em>. This will become <em>n</em> pixels which are either less than <em>(I(i,j) + t)</em> or greater than <em>(I(i,j) - t)</em>. This <em>n</em> was chosen as 12.</li>
<li>In a high-speed test, only four pixels (as shown in the figure) at 1, 9, 5, and 13 are looked at. The intensity value of at least three pixels of these decides whether the center pixel <em>p</em> is a corner. If these values are either greater than the <em>(I(i,j) + t)</em> or less than <span><em>(I(i,j) - t)</em>&#160;</span>then the center pixel is considered a corner.&#160;</li>
</ol>
<p>In OpenCV , the steps to compute FAST features are as follows:</p>
<ol>
<li>&#160;Initialize detector using <kbd>cv2.FastFeatureDetector_create()</kbd></li>
<li>Setup threshold parameters for filtering detections&#160;</li>
<li>Setup flag if non-maximal suppression to be used for clearing neighbourhood regions of repeated detections</li>
<li>Detect keypoints and plot them on the input image</li>
</ol>
<p>In the following figure, there are plots of FAST corners (in small circles) on the input image with varying threshold values. Depending on the image, a different choice of thresholds produce different&#160; number of key feature points:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="291" width="1500" class="alignnone size-full wp-image-437 image-border" src="images/a69b4bbf-cc5e-4cfd-b77f-18597c86bb15.png"/></div>
<p>To generate each image in the previous figure, use the following code by changing the threshold values:&#160;</p>
<pre><strong>def compute_fast_det(filename, is_nms=True, thresh = 10):</strong><br/><strong>    """</strong><br/><strong>    Reads image from filename and computes FAST keypoints.</strong><br/><strong>    Returns image with keypoints</strong><br/><strong>    filename: input filename</strong><br/><strong>    is_nms: flag to use Non-maximal suppression</strong><br/><strong>    thresh: Thresholding value</strong><br/><strong>    """</strong><br/><strong>    img = cv2.imread(filename)</strong><br/>   <br/><strong>    # Initiate FAST object with default values</strong><br/><strong>    fast = cv2.FastFeatureDetector_create() </strong><br/><br/><strong>    # find and draw the keypoints</strong><br/><strong>    if not is_nms:</strong><br/><strong>        fast.setNonmaxSuppression(0)</strong><br/><br/><strong>    fast.setThreshold(thresh)</strong><br/><br/><strong>    kp = fast.detect(img,None)</strong><br/><strong>    cv2.drawKeypoints(img, kp, img, color=(255,0,0))</strong><br/>    <br/><strong>    return img</strong></pre>
<p>The following figure shows variations of the same detector across different images with varying thresholds:</p>
<div class="CDPAlignCenter CDPAlign"><img height="499" width="1500" class="alignnone size-full wp-image-438 image-border" src="images/56b8c648-f7fa-42ef-81b9-1966529dc528.png"/></div>
<p>This shows that choice of parameters is quite crucial for different images. Though a common threshold value may not work for all image, a good approximation can be used depending on the similarity of images.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">ORB features</h1>
                </header>
            
            <article>
                
<p><span>Using previously described corner detectors are fast to compute, however in matching two images, it is difficult to select which two image corners are matched for corresponding pixels. An additional information that describes properties a corner is required. A combination of detected keypoints, such as corners, and corresponding descriptors makes comparing images more efficient and robust.&#160; &#160;</span></p>
<p>ORB features detection[2] features were described by&#160;<span>Ethan Rublee et al. in 2011 and have since been one of the popular features in various applications. This combines two algorithms: FAST feature detector with an orientation component and BRIEF Descriptors, hence the name <strong>Oriented FAST and Rotated BRIEF</strong></span> (<span><strong>ORB</strong></span>).<span>&#160;The major advantage of using ORB features is the speed of detections while maintaining robust detections. This makes them useful for several real-time applications like robotics vision system, smartphone apps, and so on.&#160;&#160;</span></p>
<p>In this chapter we have already seen FAST feature detectors, we will further continue describing BRIEF descriptor and finally build on ORB detector.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">FAST feature limitations</h1>
                </header>
            
            <article>
                
<p>FAST Features as described in the previous section computes corner in the image using neighborhood pixels. By creating a comparison test along the circular region around a pixel, features are computed rapidly.&#160;<span>FAST features are quite efficient for real-time applications; these do not produce rotation information of the features. This causes a limitation if we are looking for orientation invariant features.&#160;</span></p>
<p>In ORB,&#160; FAST features are used with orientation information as well. Using a circular radius of 9 pixels, a vector between computed intensity centroid and center of the corner is used to describe orientation at the given corner. This intensity centroid for a given patch is computed as follows :</p>
<ul>
<li>For an image I and a patch window, compute moments using:&#160; &#160;&#160;</li>
</ul>
<p style="padding-left: 150px" class="mce-root"><img height="45" width="178" class="alignnone size-full wp-image-519 image-border" src="images/14ad6e32-f91b-4e8d-8d22-676b7c221fe5.png"/></p>
<ul>
<li>Using previous moments, intensity centroid of given patch is given as:</li>
</ul>
<p style="padding-left: 150px" class="mce-root"><img height="37" width="148" class="alignnone size-full wp-image-520 image-border" src="images/ff053273-7b12-4d6f-ac57-9f88240e707d.png"/></p>
<p>Since, we already know the center <em>O</em> of the patch, a vector joining&#160;<img height="28" width="20" class="fm-editor-equation" src="images/29e62981-0826-45d3-8e36-d9b721011edb.png"/>&#160;is the orientation of the patch.&#160; In further sections, we will see an overall implementation of ORB feature detectors which uses this method.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">BRIEF Descriptors and their limitations</h1>
                </header>
            
            <article>
                
<p>The popular feature descriptors like SIFT or SURF outputs large vectors of dimensions 128 and 64 respectively. In applications such as image search, it is quite likely that the features are stored and searched for features rather than the original image. This becomes computationally complex and memory may be inefficient if the number of images reaches a few hundred thousand. In such cases, simple dimensionality reduction is an added step and may reduce overall efficiency. The descriptor proposed by&#160;<span>Michael Calonder and their co-authors. in <em>BRIEF: Binary Robust Independent Elementary Features</em>[6]&#160;resolves issues by consuming less memory.&#160; &#160;</span></p>
<p>BRIEF computes differences of intensities in a small patch of an image and represents it as a binary string. This not only makes it faster but also the descriptor preserves good accuracy. However, there is no feature detector in BRIEF but combining it with FAST detectors makes it efficient.</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">ORB features using OpenCV</h1>
                </header>
            
            <article>
                
<p>The following code uses ORB features implementation in <kbd>OpenCV</kbd>.</p>
<p>It is a three-step process, which is described as follows:</p>
<ul>
<li>First create an ORB object and update parameter values:&#160;</li>
</ul>
<pre style="padding-left: 60px"><strong>orb = cv2.ORB_create()<br/># set parameters<br/></strong><strong>orb.setScoreType(cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)</strong></pre>
<ul>
<li>Detect keypoints from previously created ORB object:</li>
</ul>
<pre style="padding-left: 60px"><strong># detect keypoints</strong><br/><strong>kp = orb.detect(img,None)</strong></pre>
<ul>
<li>Lastly, compute descriptors for each keypoints detected:</li>
</ul>
<pre style="padding-left: 60px"><strong># for detected keypoints compute descriptors. </strong><br/><strong>kp, des = orb.compute(img, kp)</strong></pre>
<p>The overall code for &#160;ORB keypoints detections and descriptor extractor is given as:&#160;</p>
<pre><strong>import numpy as np </strong><br/><strong>import matplotlib.pyplot as plt </strong><br/><strong>import cv2 </strong><br/><strong># With jupyter notebook uncomment below line </strong><br/><strong># %matplotlib inline </strong><br/><strong># This plots figures inside the notebook</strong><br/><br/><br/><strong>def compute_orb_keypoints(filename):</strong><br/><strong>    """</strong><br/><strong>    Reads image from filename and computes ORB keypoints</strong><br/><strong>    Returns image, keypoints and descriptors. </strong><br/><strong>    """</strong><br/><strong>    # load image</strong><br/><strong>    img = cv2.imread(filename)</strong><br/>    <br/><strong>    # create orb object</strong><br/><strong>    orb = cv2.ORB_create()</strong><br/>    <br/><strong>    # set parameters </strong><br/><strong>    # FAST feature type</strong><br/><strong>    orb.setScoreType(cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)</strong><br/>    <br/><strong>    # detect keypoints</strong><br/><strong>    kp = orb.detect(img,None)</strong><br/><br/><strong>    # for detected keypoints compute descriptors. </strong><br/><strong>    kp, des = orb.compute(img, kp)</strong><br/><strong>    return img, kp, des</strong></pre>
<p>An example of generated keypoints is as shown in the following figure (in circles):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="219" width="319" src="images/2acb8135-7ca7-4144-88d6-94cd87f58a27.png"/></div>
<p>As you can see in the following&#160;figure, different images produce different feature points&#160;for various shapes of objects:</p>
<div class="CDPAlignCenter CDPAlign"><img height="241" width="573" class="alignnone size-full wp-image-440 image-border" src="images/24e634de-267e-4f7f-b768-c79e2b5c4de7.png"/></div>
<p>In order to plot previous shown figures with different keypoints, we can use both <kbd>OpenCV</kbd> and <kbd>Matplotlib</kbd> as:&#160;</p>
<pre><strong>def draw_keyp(img, kp):</strong><br/><strong>    """</strong><br/><strong>    Takes image and keypoints and plots on the same images</strong><br/><strong>    Does not display it. </strong><br/><strong>    """</strong><br/><strong>    cv2.drawKeypoints(img,kp,img, color=(255,0,0), flags=2) </strong><br/><strong>    return img</strong><br/><br/><br/><strong>def plot_img(img, figsize=(12,8)):</strong><br/><strong>    """</strong><br/><strong>    Plots image using matplotlib for the given figsize</strong><br/><strong>    """</strong><br/><strong>    fig = plt.figure(figsize=figsize)</strong><br/><strong>    ax = fig.add_subplot(1,1,1)</strong><br/><br/><strong>    # image need to be converted to RGB format for plotting</strong><br/><strong>    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))</strong><br/><strong>    plt.axis('off')</strong><br/><strong>    plt.show()</strong><br/><br/><br/><br/><strong>def main():</strong><br/><strong>    # read an image </strong><br/><strong>    filename = '../figures/flower.png'</strong><br/><strong>    # compute ORB keypoints</strong><br/><strong>    img1,kp1, des1 = compute_orb_keypoints(filename)</strong><br/><strong>    # draw keypoints on image </strong><br/><strong>    img1 = draw_keyp(img1, kp1)</strong><br/><strong>    # plot image with keypoints</strong><br/><strong>    plot_img(img1)</strong><br/>    <br/><br/><strong>if __name__ == '__main__':</strong><br/><strong>    main()</strong></pre>
<p>In this section, we saw formulation of ORB features that not only combines robust features, but also provides descriptors for easier comparison to other features. This is a strong formulation of feature detector, however explicitly designing a feature detector for different task will require efficient choice of parameters such as patch size for FAST detector, BRIEF descriptor parameters etc. For a non-expert, setting these parameters may be quite cumbersome task. In following section, we will begin with discussion on black box features and its importance in creating computer vision systems.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The black box feature</h1>
                </header>
            
            <article>
                
<p>The features we discussed previously are highly dependent on an image to image basis. Some of the challenges observed in detecting features are:</p>
<ul>
<li>In case of illumination changes, such as nighttime image or daylight images there would be a significant difference in pixel intensity values as well as neighborhood regions</li>
<li>As object orientation changes, keypoint descriptor changes significantly. In order to match corresponding features, a proper choice of descriptor parameters is required</li>
</ul>
<p>Due to these challenges, several parameters used here need to be tuned by experts.</p>
<p>In recent years, a lot has been happening with neural networks in the field of computer vision. The popularity of them has risen due to higher accuracy and less hand-tuned parameters. We can call them black box features—though the term black refers only to the way they are designed. In a majority of these model deployments, the parameters are learned through training and require the least supervision of parameters setting. The black box modeling feature detection helps in getting better features by learning over a dataset of images. This dataset consists of possible different variations&#160; of images, as a result the learnt detector can extract better features even in wide variation of image types. We will study these feature detectors in the next chapter as CNNs.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application – find your object in an image&#160;</h1>
                </header>
            
            <article>
                
<p>The most common application for using features is given an object, find the best possible match for it in the image. This is often referred to as <strong>template matching</strong>, where the object at hand is a usually small window called a&#160;<strong>template</strong> and goal is to compute the best-matched features from this template to a target image. There exist several solutions to this, but for the sake of understanding, we will use ORB features. &#160;</p>
<p>Using ORB features, we can do feature matching in a brute force way as follows:</p>
<ul>
<li>Compute features in each image (template and target).</li>
<li>For each feature in a template, compare all the features in the target detected previously. The criterion is set using a matching score.</li>
<li>If the feature pair passes the criterion, then they are considered a match.</li>
<li>Draw matches to visualize.</li>
</ul>
<p>As a pre-requisite, we will follow previously shown codes for extracting features as:</p>
<pre><strong>def compute_orb_keypoints(filename):</strong><br/><strong>    """</strong><br/><strong>    Takes in filename to read and computes ORB keypoints</strong><br/><strong>    Returns image, keypoints and descriptors </strong><br/><strong>    """</strong><br/><br/><strong>    img = cv2.imread(filename)</strong><br/><strong>    # create orb object</strong><br/><strong>    orb = cv2.ORB_create()</strong><br/>    <br/><strong>    # set parameters </strong><br/><strong>    orb.setScoreType(cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)</strong><br/>    <br/><strong>    # detect keypoints</strong><br/><strong>    kp = orb.detect(img,None)</strong><br/><br/><strong>    # using keypoints, compute descriptor</strong><br/><strong>    kp, des = orb.compute(img, kp)</strong><br/><strong>    return img, kp, des</strong></pre>
<p>Once we have keypoints and descriptors from each of the images, we can use them to compare and match.&#160;</p>
<p>Matching keypoints between two images is a two-step process:</p>
<ul>
<li>Create desired kind of matcher specifying the distance metric to be used. Here we will use Brute-Force Matching with Hamming distance:</li>
</ul>
<pre style="padding-left: 60px"><strong>bf = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=True)</strong></pre>
<ul>
<li>Using descriptors for keypoints from each image, perform matching as:</li>
</ul>
<pre style="padding-left: 60px"><strong>matches = bf.match(des1,des2)</strong></pre>
<p>In the following code, we will show overall Brute-Force method of matching keypoints from one image to another using corresponding descriptors only:</p>
<pre><strong>def brute_force_matcher(des1, des2):</strong><br/><strong>    """</strong><br/><strong>    Brute force matcher to match ORB feature descriptors</strong><br/><strong>    des1, des2: descriptors computed using ORB method for 2 images</strong><br/><strong>    returns matches </strong><br/><strong>    """</strong><br/><strong>    # create BFMatcher object</strong><br/><strong>    bf = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=True)</strong><br/><strong>    # Match descriptors.</strong><br/><strong>    matches = bf.match(des1,des2)</strong><br/><br/><strong>    # Sort them in the order of their distance.</strong><br/><strong>    matches = sorted(matches, key = lambda x:x.distance)</strong><br/><br/><strong>    return matches</strong></pre>
<p>In the following figure, the features from the template are matched to the original image. To show the effectiveness of matching, only the best matches are shown:</p>
<div class="CDPAlignCenter CDPAlign"><img height="765" width="1500" class="alignnone size-full wp-image-442 image-border" src="images/aeb9b00d-4645-4f3f-9f2c-0ebed4b2656a.png"/></div>
<p>The previous feature matching image is created using the following code, where we use a sample template image to match to a large image of the same object:</p>
<pre><br/><strong>def compute_img_matches(filename1, filename2, thres=10):</strong><br/><strong>    """</strong><br/><strong>    Extracts ORB features from given filenames</strong><br/><strong>    Computes ORB matches and plot them side by side </strong><br/><strong>    """</strong><br/><strong>    img1, kp1, des1 = compute_orb_keypoints(filename1)</strong><br/><strong>    img2, kp2, des2 = compute_orb_keypoints(filename2)</strong><br/>    <br/><strong>    matches = brute_force_matcher(des1, des2)</strong><br/><strong>    draw_matches(img1, img2, kp1, kp2, matches, thres)</strong><br/>    <br/><br/><strong>def draw_matches(img1, img2, kp1, kp2, matches, thres=10):</strong><br/><strong>    """</strong><br/><strong>    Utility function to draw lines connecting matches between two images.</strong><br/><strong>    """</strong><br/><strong>    draw_params = dict(matchColor = (0,255,0),</strong><br/><strong>                       singlePointColor = (255,0,0),</strong><br/><strong>                       flags = 0)</strong><br/><br/><strong>    # Draw first thres matches.</strong><br/><strong>    img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:thres],None, **draw_params)</strong><br/><strong>    plot_img(img3)</strong><br/><br/><br/><br/><strong>def main():</strong><br/><strong>    # read an image </strong><br/><strong>    filename1 = '../figures/building_crop.jpg'</strong><br/><strong>    filename2 = '../figures/building.jpg'</strong><br/><br/><strong>    compute_img_matches(filename1, filename2)</strong><br/>   <br/><br/><br/><strong>if __name__ == '__main__':</strong><br/><strong>    main()</strong></pre>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applications&#160;– is it similar?</h1>
                </header>
            
            <article>
                
<p>In this application, we would like to see if they are similar using previously described feature detectors. For that, we use a similar approach as previously mentioned. The first step is computing feature keypoints and descriptors for each image. Using these performs matching between one image and another. If there are a sufficient number of matches, we can comfortably say that the two images are similar.</p>
<p>For the prerequisites,&#160; we use the same ORB keypoint and descriptor extractor but added downsampling of the image:&#160;</p>
<pre><strong>def compute_orb_keypoints(filename):</strong><br/><strong>    """</strong><br/><strong>    Takes in filename to read and computes ORB keypoints</strong><br/><strong>    Returns image, keypoints and descriptors </strong><br/><strong>    """</strong><br/><br/><strong>    img = cv2.imread(filename)</strong><br/>    <br/><strong>    # downsample image 4x</strong><br/><strong>    img = cv2.pyrDown(img) # downsample 2x</strong><br/><strong>    img = cv2.pyrDown(img) # downsample 4x</strong><br/>  <br/><strong>    # create orb object</strong><br/><strong>    orb = cv2.ORB_create()</strong><br/>    <br/><strong>    # set parameters </strong><br/><strong>    orb.setScoreType(cv2.FAST_FEATURE_DETECTOR_TYPE_9_16)</strong><br/>    <br/><strong>    # detect keypoints</strong><br/><strong>    kp = orb.detect(img,None)</strong><br/><br/><strong>    kp, des = orb.compute(img, kp)</strong><br/><strong>    return img, kp,  des</strong></pre>
<p>Using the previously computed keypoints and descriptors, the matching is done as:&#160;</p>
<pre><strong>def compute_img_matches(filename1, filename2, thres=10):</strong><br/><strong>    """</strong><br/><strong>    Extracts ORB features from given filenames</strong><br/><strong>    Computes ORB matches and plot them side by side </strong><br/><strong>    """</strong><br/><strong>    img1, kp1, des1 = compute_orb_keypoints(filename1)</strong><br/><strong>    img2, kp2, des2 = compute_orb_keypoints(filename2)</strong><br/>    <br/><strong>    matches = brute_force_matcher(des1, des2)</strong><br/><strong>    draw_matches(img1, img2, kp1, kp2, matches, thres)</strong><br/>    <br/><strong>def brute_force_matcher(des1, des2):</strong><br/><strong>    """</strong><br/><strong>    Brute force matcher to match ORB feature descriptors</strong><br/><strong>    """</strong><br/><strong>    # create BFMatcher object</strong><br/><strong>    bf = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=True)</strong><br/><strong>    # Match descriptors.</strong><br/><strong>    matches = bf.match(des1,des2)</strong><br/><br/><strong>    # Sort them in the order of their distance.</strong><br/><strong>    matches = sorted(matches, key = lambda x:x.distance)</strong><br/><br/><strong>    return matches</strong><br/><br/><strong>def draw_matches(img1, img2, kp1, kp2, matches, thres=10):</strong><br/><strong>    """</strong><br/><strong>    Utility function to draw lines connecting matches between two images.</strong><br/><strong>    """</strong><br/><strong>    draw_params = dict(matchColor = (0,255,0),</strong><br/><strong>                       singlePointColor = (255,0,0),</strong><br/><strong>                       flags = 0)</strong><br/><br/><strong>    # Draw first thres matches.</strong><br/><strong>    img3 = cv2.drawMatches(img1,kp1,img2,kp2,matches[:thres],None, **draw_params)</strong><br/><strong>    plot_img(img3)</strong><br/><br/><br/><br/><strong>def main():</strong><br/><strong>    # read an image </strong><br/><strong>    filename2 = '../figures/building_7.JPG'</strong><br/><strong>    filename1 = '../figures/building_crop.jpg'</strong><br/><strong>    compute_img_matches(filename1, filename2, thres=20)</strong><br/>   <br/><br/><strong>if __name__ == '__main__':</strong><br/><strong>    main()</strong></pre>
<p>The results of an example are as shown in the following figure, where inputs are same objects with different viewpoints. The correct matches are shown as with connecting lines:&#160;</p>
<div class="CDPAlignCenter CDPAlign"><img height="335" width="586" class="alignnone size-full wp-image-443 image-border" src="images/3983685d-067f-473f-bf48-634dd55bc183.png"/></div>
<p>In this section, we saw two similar approaches for image matching using ORB keypoints and a Brute-Force matcher. The matching can be further enhanced by using more faster algorithms like approximate neighborhood matches. The effect of faster matching is mostly seen in the cases where a large number of features keypoints are extracted.&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we began discussion features and its importance in computer vision applications. Harris Corner Detector is used to detect corners where runtime is of utmost importance. These can run on embedded devices with high speeds. Extending over to more complex detectors, we saw FAST features and in combination with BRIEF descriptors, ORB features can be formed. These are robust for different scales as well as rotations. Finally, we saw the application of feature matching using ORB features and a use of pyramid downsampling.</p>
<p class="mce-root">The discussion on black box features will continue in the next chapter with the introduction of neural networks and especially CNNs.&#160;&#160;</p>


            </article>

            
        </section>
    </div>


  <div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Harris Chris, and Mike Stephens. <em>A combined corner and edge detector</em>. In&#160;</span>Alvey vision conference<span>, vol. 15, no. 50, pp. 10-5244. 1988.</span></li>
<li><span>Rublee Ethan, Vincent Rabaud, Kurt Konolige, and Gary Bradski. <em>ORB: An efficient alternative to SIFT or SURF</em>. In&#160;</span>Computer Vision (ICCV), 2011 IEEE international conference on<span>, pp. 2564-2571. IEEE, 2011.</span></li>
<li><span>Lowe David G. <em>Object recognition from local scale-invariant features</em>. In&#160;</span>Computer vision, 1999. The proceedings of the seventh IEEE international conference on<span>, vol. 2, pp. 1150-1157. IEEE, 1999.</span></li>
<li><span>Bay Herbert, Tinne Tuytelaars, and Luc Van Gool. <em>Surf: Speeded up robust features</em>.&#160;</span>Computer vision–ECCV 2006<span>(2006): 404-417.</span></li>
<li><span>Rosten Edward, and Tom Drummond. <em>Machine learning for high-speed corner detection</em>.&#160;</span>Computer Vision–ECCV 2006<span>(2006): 430-443.</span></li>
<li>Calonder Michael, Vincent Lepetit, Christoph Strecha, and Pascal Fua. <em>Brief: Binary robust independent elementary features</em>. Computer Vision–ECCV 2010 (2010): 778-792.</li>
</ul>


            </article>

            
        </section>
    </div>
</body>
</html>