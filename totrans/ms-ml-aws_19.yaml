- en: Optimizing Models in Spark and SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The models that are trained on AWS can be further optimized by modifying the
    training directives or hyperparameters. In this chapter, we will discuss various
    techniques that our readers can use to improve the performance of their algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of model optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning in Apache Spark and SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of model optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Very few algorithms produce optimized models on a first attempt. This is because
    the algorithm might need some parameter tuning from the data scientist to improve
    their accuracy or performance. For example, the learning rate we mentioned in
    [Chapter 7](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml), *Implementing Deep Learning
    Algorithms*, for deep neural networks needs to be manually tuned. A low learning
    rate may lead the algorithm to take longer (and hence be more expensive if we're
    running on a cloud), whereas a high learning rate might miss the optimal set of
    weights. Likewise, a tree with more levels may take more time to train, but could
    create a model with better predictive capabilities (though it could also cause
    the tree to overfit). These parameters that direct the learning of the algorithms
    are called **hyperparameters**, and contrary to the model parameters (for example, the
    weights of a network), these are not learned throughout the training process.
    Some hyperparameters are not just used to optimize or tune the model, but also
    to define or constrain the problem. For example, the number of clusters is also
    considered a hyperparameter, though it's not really about optimization here, but
    rather is used to define the problem being solved.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is not trivial to adjust these hyperparameters for best performance, and
    in many cases it requires understanding the data at hand, as well as how the underlying
    algorithm works. So why not learn these hyperparameters? Many data scientists
    use algorithms that tweak the values of these hyperparameters to see whether they
    produce more accurate results. The problem with this approach is that we could
    be finding the hyperparameters that are optimal on the testing dataset, and we
    might think our model has a better accuracy when we''re just overfitting the testing
    dataset. For this reason, we typically split the dataset into three partitions:
    the training dataset, which is used for training the model, the validation dataset,
    which is used to perform parameter tuning, and the testing dataset, which is just
    used to assess the final accuracy of the model once the parameter tuning is complete.'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest way to perform hyperparameter tuning is called grid search. We
    define different values we would like to try for each hyperparameter. For example,
    if we are training trees, we may want to try depths of 5, 10, and 15\. At the
    same time, we'd like to see whether the best impurity measure is information gain
    or gini. This creates a total of six combinations that have to be tested for accuracy.
    As you might be anticipating, the number of combinations will grow exponentially
    with the number of hyperparameters to consider. For this reason, other techniques
    are used to avoid testing all possible combinations. A simple approach is to randomize
    the combinations be tried. Some combinations will be missed, but some variations
    will be tested without an inductive bias.
  prefs: []
  type: TYPE_NORMAL
- en: AWS SageMaker provides a service for hyperparameter tuning that is smart in
    choosing the hyperparameters to test. In both grid search and randomization, each
    training run doesn't use information about the accuracy obtained in previous runs.
    SageMaker uses a technique called **Bayesian optimization** that is able to select
    the next set of hyperparameter combinations to test based on the accuracy values
    of previously tested combinations. The main idea behind this algorithm is to construct
    a probability distribution over the hyperparameter space. Each time we obtain
    the accuracy of a given combination, the probability distribution is adjusted
    to reflect the new information. A successful optimization will exploit information
    of known combinations that yielded good accuracy, as well as sufficient exploration
    of new combinations that could lead to potential improvements. You will appreciate
    that this is an extremely hard problem to solve, as each training run is slow
    and probably expensive. We usually can't afford to test too many combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall our regression problem from [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms*, in which we constructed a
    linear regression to estimate the value of houses. At that point, we used a few
    arbitrary values for our hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we will show how Apache Spark can test 18 different
    hyperparameter combinations for `elasticNetParam`, `regParam`, and `solver`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will start by constructing our classifier as usual, without providing any
    hyperparameters. We store the regressor in the `linear` variable. Next, we define
    the different values to test for each hyperparameter by defining a parameter grid.
    The functional reference to the methods that set the values is passed to a `ParamGridBuilder`
    which is responsible for keeping the combinations to test out.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we can define our pipeline with any preprocessing stages (in this
    case, we use a vector assembler). `CrossValidator` takes the pipeline, parameter
    grid, and evaluator. Recall that the evaluator was used to obtain a specific score
    using a test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we will be using the R2 metric as we did in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms*. `CrossValidator`, upon the
    call to `fit()`, will run all combinations and find the hyperparameter that achieves
    the highest R2 value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it completes, we can inspect the underlying best model by accessing it
    through the `optimized_model.bestModel` reference. Through it, we can show the
    actual set of hyperparameters used in the best model found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the above statement is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'However, more interesting than the actual parameters used is to see the accuracy
    changes across the different combinations tested. The `optimized_model.avgMetrics`
    values will show the accuracy values for all 18 combinations of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0.60228046689935, 0.6022857524897973, ... 0.6034106428627964, 0.6034118340373834]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `optimized_model`, returned by `CrossValidator`, to obtain predictions
    using the best model, as it is also a transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we obtain an R2 of 0.72, which is slightly better than what we
    got with our arbitrary set of hyperparameters in [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml), *Predicting
    House Value with Regression Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning in SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the previous section,  *Automatic hyperparameter tuning*, 
    SageMaker has a library for smart parameter tuning using Bayesian Optimization.
    In this section, we will show how we can further tune the model we created in
    [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User Behavior
    with Tree-based Methods*. Recall from that chapter that we posed a binary classification
    problem for trying to predict whether a user would click on an advertisement.
    We had used an `xgboost` model, but at that point we hadn't performed any parameter
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by creating the SageMaker session and choosing the `xgboost:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the estimator just as we did in [Chapter 4](https://cdp.packtpub.com/mastering_machine_learning_on_aws/wp-admin/post.php?post=39&action=edit#post_27), *Predicting
    User Behavior with Tree-Based Methods*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As we always do with SageMaker service calls, we define the location and format
    of the input data for training and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With the base estimator defined and the input data determined, we can now construct
    a training job that will take this estimator, and run a series of training jobs
    varying the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'SageMaker: Creating hyperparameter tuning job with name: `xgboost-190407-1532`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create an instance of `HyperparameterTuner` in which we
    set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The base estimator upon which the hyperparameters will be varied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective metric, which will be used to find the best possible combination
    of hyperparameters. Since we're dealing with a binary classification problem,
    using the area under the curve metric on the validation data is a good choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different ranges we'd like to test for each hyperparameter. These ranges
    can be specified for parameters that vary continuously using `ContinuousParameter`,
    or discretely using `IntegerParameter` or `CategoricalParameter`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of jobs to run, as well as the maximum amount of jobs to run in parallel.
    There is a trade off here between accuracy and speed. The more parallel jobs you
    run, the less data about prior job metrics will be used to inform the next set
    of hyperparameters to try. This leads to a sub-optimal range search. However,
    it will complete the tuning faster. In this example, we just run 10 jobs. We typically
    want to run more than that to obtain significant improvements. Here we just present
    a low value so that the reader can get fast results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fitting can be monitored through the AWS console ([https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/hyper-tuning-jobs](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/hyper-tuning-jobs))
    or through methods in the python SDK, we can see the status of the jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it''s complete, the AWS Console should look like the following screenshot;
    in it, you can see the different jobs that ran and the different performance metrics
    that were obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a2b965a-7b43-4a80-9ab7-852c603113da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us inspect which training job yields the best performance using the SDK.
    The first thing is to find the name of the best job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '''xgboost-190407-1342-001-5c7e2a26'''
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the methods in the session object, we can show the values of the hyperparameters
    for the optimal training job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the previous describe command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `describe_hyper_parameter_tuning_job()` method, we can also get the
    final value of the optimal AUC metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You should explore the full API and Python SDK for a complete set of features
    and options regarding the automatic tuning. Please check out:[ https://github.com/aws/sagemaker-python-sdk ](https://github.com/aws/sagemaker-python-sdk)
    We hope this introduction can help to get started on how to fine-tune the models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the importance of model tuning through hyperparameter
    optimization. We provided examples of doing grid search in Apache Spark, as well
    as how to use SageMaker's advanced parameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will focus on optimizing the hardware and cluster set
    up upon which we train and apply models. Both model optimization and hardware
    optimization are important for successful and cost-effective AI processes.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regarding ways to find the best hyperparameters, compare the advantages and
    disadvantages of grid search, random search, and Bayesian optimization as they
    apply to hyperparameter tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we typically need three splits of data when we do hyperparameter tuning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Which metric do you think would be best for our `xgboost` example: `validation:auc`
    or `training:auc`?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
