["```py\n> # load the dataset into data frame\n> credit.df <- read.csv(\"credit_dataset_final.csv\", header = TRUE, sep = \",\")\n\n```", "```py\n## data type transformations - factoring\nto.factors <- function(df, variables){\n for (variable in variables){\n df[[variable]] <- as.factor(df[[variable]])\n }\n return(df)\n}\n\n## normalizing - scaling\nscale.features <- function(df, variables){\n for (variable in variables){\n df[[variable]] <- scale(df[[variable]], center=T, scale=T)\n }\n return(df)\n}\n\n```", "```py\n> # normalize variables\n> numeric.vars <- c(\"credit.duration.months\", \"age\", \n \"credit.amount\")\n> credit.df <- scale.features(credit.df, numeric.vars)\n> # factor variables\n> categorical.vars <- c('credit.rating', 'account.balance', \n+                       'previous.credit.payment.status',\n+                       'credit.purpose', 'savings', \n+                       'employment.duration', 'installment.rate',\n+                       'marital.status', 'guarantor', \n+                       'residence.duration', 'current.assets',\n+                       'other.credits', 'apartment.type', \n+                       'bank.credits', 'occupation', \n+                       'dependents', 'telephone', \n+                       'foreign.worker')\n> credit.df <- to.factors(df=credit.df, \n variables=categorical.vars)\n\n```", "```py\n> # split data into training and test datasets in 60:40 ratio\n> indexes <- sample(1:nrow(credit.df), size=0.6*nrow(credit.df))\n> train.data <- credit.df[indexes,]\n> test.data <- credit.df[-indexes,]\n\n```", "```py\n> library(caret)  # feature selection algorithm\n> library(randomForest) # random forest algorithm\n\n```", "```py\nthe R console to load into memory for using it later:\n```", "```py\nrun.feature.selection <- function(num.iters=20, feature.vars, class.var){\n set.seed(10)\n variable.sizes <- 1:10\n control <- rfeControl(functions = rfFuncs, method = \"cv\", \n verbose = FALSE, returnResamp = \"all\", \n number = num.iters)\n results.rfe <- rfe(x = feature.vars, y = class.var, \n sizes = variable.sizes, \n rfeControl = control)\n return(results.rfe)\n}\n\n```", "```py\nrfe.results <- run.feature.selection(feature.vars=train.data[,-1], \n class.var=train.data[,1])\n# view results\nrfe.results\n\n```", "```py\nlibrary(caret) # model training and evaluation\nlibrary(ROCR) # model evaluation\nsource(\"performance_plot_utils.R\") # plotting metric results\n## separate feature and class variables\ntest.feature.vars <- test.data[,-1]\ntest.class.var <- test.data[,1]\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> lr.model <- glm(formula=formula.init, data=train.data, family=\"binomial\")\n\n```", "```py\n> lr.predictions <- predict(lr.model, test.data, type=\"response\")\n> lr.predictions <- round(lr.predictions)\n> confusionMatrix(data=lr.predictions, reference=test.class.var, positive='1')\n\n```", "```py\nformula <- \"credit.rating ~ .\"\nformula <- as.formula(formula)\ncontrol <- trainControl(method=\"repeatedcv\", number=10, repeats=2)\nmodel <- train(formula, data=train.data, method=\"glm\", \n trControl=control)\nimportance <- varImp(model, scale=FALSE)\nplot(importance)\n\n```", "```py\n> formula.new <- \"credit.rating ~ account.balance + credit.purpose \n + previous.credit.payment.status + savings \n + credit.duration.months\"\n> formula.new <- as.formula(formula.new)\n> lr.model.new <- glm(formula=formula.new, data=train.data, family=\"binomial\")\n> lr.predictions.new <- predict(lr.model.new, test.data, type=\"response\") \n> lr.predictions.new <- round(lr.predictions.new)\n> confusionMatrix(data=lr.predictions.new, reference=test.class.var, positive='1')\n\n```", "```py\n> lr.model.best <- lr.model\n> lr.prediction.values <- predict(lr.model.best, test.feature.vars, type=\"response\")\n> predictions <- prediction(lr.prediction.values, test.class.var)\n> par(mfrow=c(1,2))\n> plot.roc.curve(predictions, title.text=\"LR ROC Curve\")\n> plot.pr.curve(predictions, title.text=\"LR Precision/Recall Curve\")\n\n```", "```py\nlibrary(e1071) # svm model\nlibrary(caret) # model training\\optimizations\nlibrary(kernlab) # svm model for hyperparameters\nlibrary(ROCR) # model evaluation\nsource(\"performance_plot_utils.R\") # plot model metrics\n## separate feature and class variables\ntest.feature.vars <- test.data[,-1]\ntest.class.var <- test.data[,1]\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> svm.model <- svm(formula=formula.init, data=train.data, \n+                  kernel=\"radial\", cost=100, gamma=1)\n> summary(svm.model)\n\n```", "```py\n> svm.predictions <- predict(svm.model, test.feature.vars)\n> confusionMatrix(data=svm.predictions, reference=test.class.var, positive=\"1\")\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> control <- trainControl(method=\"repeatedcv\", number=10, repeats=2)\n> model <- train(formula.init, data=train.data, method=\"svmRadial\", \n+                trControl=control)\n> importance <- varImp(model, scale=FALSE)\n> plot(importance, cex.lab=0.5)\n\n```", "```py\n> formula.new <- \"credit.rating ~ account.balance + \n credit.duration.months + savings + \n previous.credit.payment.status + credit.amount\"\n> formula.new <- as.formula(formula.new)\n> svm.model.new <- svm(formula=formula.new, data=train.data, \n+                  kernel=\"radial\", cost=100, gamma=1)\n> svm.predictions.new <- predict(svm.model.new, test.feature.vars)\n> confusionMatrix(data=svm.predictions.new, \n reference=test.class.var, positive=\"1\")\n\n```", "```py\n1% to 66.5%. However, the most interesting part is that now our model is able to predict more bad ratings from bad, which can be seen from the confusion matrix. The specificity is now 38% compared to 0% earlier and, correspondingly, the sensitivity has gone down to 80% from 100%, which is still good because now this model is actually useful and profitable! You can see from this that feature selection can indeed be extremely powerful. The confusion matrix for the preceding observations is depicted in the following snapshot:\n```", "```py\ncost.weights <- c(0.1, 10, 100)\ngamma.weights <- c(0.01, 0.25, 0.5, 1)\ntuning.results <- tune(svm, formula.new,\n data = train.data, kernel=\"Radial\", \n ranges=list(cost=cost.weights, gamma=gamma.weights))\nprint(tuning.results)\n\n```", "```py\n> plot(tuning.results, cex.main=0.6, cex.lab=0.8,xaxs=\"i\", yaxs=\"i\")\n\n```", "```py\n> svm.model.best = tuning.results$best.model\n> svm.predictions.best <- predict(svm.model.best,\n test.feature.vars)\n> confusionMatrix(data=svm.predictions.best, \n reference=test.class.var, positive=\"1\")\n\n```", "```py\n> svm.predictions.best <- predict(svm.model.best, test.feature.vars, decision.values = T)\n> svm.prediction.values <- attributes(svm.predictions.best)$decision.values\n> predictions <- prediction(svm.prediction.values, test.class.var)\n> par(mfrow=c(1,2))\n> plot.roc.curve(predictions, title.text=\"SVM ROC Curve\")\n> plot.pr.curve(predictions, title.text=\"SVM Precision/Recall Curve\")\n\n```", "```py\n> transformed.train <- train.data\n> transformed.test <- test.data\n> for (variable in categorical.vars){\n+   new.train.var <- make.names(train.data[[variable]])\n+   transformed.train[[variable]] <- new.train.var\n+   new.test.var <- make.names(test.data[[variable]])\n+   transformed.test[[variable]] <- new.test.var\n+ }\n> transformed.train <- to.factors(df=transformed.train, variables=categorical.vars)\n> transformed.test <- to.factors(df=transformed.test, variables=categorical.vars)\n> transformed.test.feature.vars <- transformed.test[,-1]\n> transformed.test.class.var <- transformed.test[,1]\n\n```", "```py\n> grid <- expand.grid(C=c(1,10,100), sigma=c(0.01, 0.05, 0.1, 0.5, \n 1))\n> ctr <- trainControl(method='cv', number=10, classProbs=TRUE,\n summaryFunction=twoClassSummary)\n> svm.roc.model <- train(formula.init, transformed.train,\n+                        method='svmRadial', trControl=ctr, \n+                        tuneGrid=grid, metric=\"ROC\")\n\n```", "```py\n> predictions <- predict(svm.roc.model, \n transformed.test.feature.vars)\n> confusionMatrix(predictions, transformed.test.class.var, \n positive = \"X1\")\n\n```", "```py\n> svm.predictions <- predict(svm.roc.model, transformed.test.feature.vars, type=\"prob\")\n> svm.prediction.values <- svm.predictions[,2]\n> predictions <- prediction(svm.prediction.values, test.class.var)\n> par(mfrow=c(1,2))\n> plot.roc.curve(predictions, title.text=\"SVM ROC Curve\")\n> plot.pr.curve(predictions, title.text=\"SVM Precision/Recall Curve\")\n\n```", "```py\n> library(rpart)# tree models \n> library(caret) # feature selection\n> library(rpart.plot) # plot dtree\n> library(ROCR) # model evaluation\n> library(e1071) # tuning model\n> source(\"performance_plot_utils.R\") # plotting curves\n> ## separate feature and class variables\n> test.feature.vars <- test.data[,-1]\n> test.class.var <- test.data[,1]\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> dt.model <- rpart(formula=formula.init, \n method=\"class\",data=train.data,control = \n rpart.control(minsplit=20, cp=0.05))\n\n```", "```py\n> dt.predictions <- predict(dt.model, test.feature.vars, \n type=\"class\")\n> confusionMatrix(data=dt.predictions, reference=test.class.var, \n positive=\"1\")\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> control <- trainControl(method=\"repeatedcv\", number=10, repeats=2)\n> model <- train(formula.init, data=train.data, method=\"rpart\", \n+                trControl=control)\n> importance <- varImp(model, scale=FALSE)\n> plot(importance)\n\n```", "```py\n> formula.new <- \"credit.rating ~ account.balance + savings +\n credit.amount + \n credit.duration.months + \n previous.credit.payment.status\"\n> formula.new <- as.formula(formula.new)\n> dt.model.new <- rpart(formula=formula.new, method=\"class\",data=train.data, \n+                   control = rpart.control(minsplit=20, cp=0.05),\n+                   parms = list(prior = c(0.7, 0.3)))\n\n```", "```py\n> dt.predictions.new <- predict(dt.model.new, test.feature.vars, \n type=\"class\")\n> confusionMatrix(data=dt.predictions.new, \n reference=test.class.var, positive=\"1\")\n\n```", "```py\n> dt.model.best <- dt.model.new\n> print(dt.model.best)\n\n```", "```py\n> par(mfrow=c(1,1))\n> prp(dt.model.best, type=1, extra=3, varlen=0, faclen=0)\n\n```", "```py\n> dt.predictions.best <- predict(dt.model.best, test.feature.vars, \n type=\"prob\")\n> dt.prediction.values <- dt.predictions.best[,2]\n> predictions <- prediction(dt.prediction.values, test.class.var)\n> par(mfrow=c(1,2))\n> plot.roc.curve(predictions, title.text=\"DT ROC Curve\")\n> plot.pr.curve(predictions, title.text=\"DT Precision/Recall \n Curve\")\n\n```", "```py\n> library(randomForest) #rf model \n> library(caret) # feature selection\n> library(e1071) # model tuning\n> library(ROCR) # model evaluation\n> source(\"performance_plot_utils.R\") # plot curves\n> ## separate feature and class variables\n> test.feature.vars <- test.data[,-1]\n> test.class.var <- test.data[,1]\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> rf.model <- randomForest(formula.init, data = train.data, \n importance=T, proximity=T)\n\n```", "```py\n> print(rf.model)\n\n```", "```py\n> rf.predictions <- predict(rf.model, test.feature.vars, \n type=\"class\")\n> confusionMatrix(data=rf.predictions, reference=test.class.var, \n positive=\"1\")\n\n```", "```py\nnew model:\n```", "```py\nformula.new <- \"credit.rating ~ account.balance + savings +\n credit.amount + \n credit.duration.months + \n previous.credit.payment.status\"\nformula.new <- as.formula(formula.new)\nrf.model.new <- randomForest(formula.new, data = train.data, \n importance=T, proximity=T)\n\n```", "```py\n> rf.predictions.new <- predict(rf.model.new, test.feature.vars, \n type=\"class\")\n> confusionMatrix(data=rf.predictions.new,   reference=test.class.var, positive=\"1\")\n\n```", "```py\nnodesize.vals <- c(2, 3, 4, 5)\nntree.vals <- c(200, 500, 1000, 2000)\ntuning.results <- tune.randomForest(formula.new, \n data = train.data,\n mtry=3, \n nodesize=nodesize.vals,\n ntree=ntree.vals)\nprint(tuning.results)\n\n```", "```py\n> rf.model.best <- tuning.results$best.model\n> rf.predictions.best <- predict(rf.model.best, test.feature.vars, \n type=\"class\")\n> confusionMatrix(data=rf.predictions.best,\n reference=test.class.var, positive=\"1\")\n\n```", "```py\n> rf.predictions.best <- predict(rf.model.best, test.feature.vars, type=\"prob\")\n> rf.prediction.values <- rf.predictions.best[,2]\n> predictions <- prediction(rf.prediction.values, test.class.var)\n> par(mfrow=c(1,2))\n> plot.roc.curve(predictions, title.text=\"RF ROC Curve\")\n> plot.pr.curve(predictions, title.text=\"RF Precision/Recall Curve\")\n\n```", "```py\n> library(caret) # nn models\n> library(ROCR) # evaluate models\n> source(\"performance_plot_utils.R\") # plot curves\n> # data transformation\n> test.feature.vars <- test.data[,-1]\n> test.class.var <- test.data[,1]\n\n```", "```py\n> transformed.train <- train.data\n> transformed.test <- test.data\n> for (variable in categorical.vars){\n+   new.train.var <- make.names(train.data[[variable]])\n+   transformed.train[[variable]] <- new.train.var\n+   new.test.var <- make.names(test.data[[variable]])\n+   transformed.test[[variable]] <- new.test.var\n+ }\n> transformed.train <- to.factors(df=transformed.train, variables=categorical.vars)\n> transformed.test <- to.factors(df=transformed.test, variables=categorical.vars)\n> transformed.test.feature.vars <- transformed.test[,-1]\n> transformed.test.class.var <- transformed.test[,1]\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> nn.model <- train(formula.init, data = transformed.train, method=\"nnet\")\n\n```", "```py\nnnet package if you do not have it installed, so just select the option when it asks you and it will install it automatically and build the model. If it fails, you can install it separately and run the code again. Remember, it is an iterative process so the model building might take some time. Once the model converges, you can view the model details using the print(nn.model) command which will show several iteration results with different size and decay options, and you will see that it does hyperparameter tuning internally itself to try and get the best model!\n```", "```py\n> nn.predictions <- predict(nn.model, \n transformed.test.feature.vars, type=\"raw\")\n> confusionMatrix(data=nn.predictions, \n reference=transformed.test.class.var, \n positive=\"X1\")\n\n```", "```py\n> formula.init <- \"credit.rating ~ .\"\n> formula.init <- as.formula(formula.init)\n> control <- trainControl(method=\"repeatedcv\", number=10, repeats=2)\n> model <- train(formula.init, data=transformed.train, method=\"nnet\", \n trControl=control)\n> importance <- varImp(model, scale=FALSE)\n> plot(importance)\n\n```", "```py\n> formula.new <- \"credit.rating ~ account.balance + credit.purpose + savings + current.assets +\nforeign.worker + previous.credit.payment.status\"\n> formula.new <- as.formula(formula.new)\n> nn.model.new <- train(formula.new, data=transformed.train, method=\"nnet\")\n\n```", "```py\n> nn.predictions.new <- predict(nn.model.new, \n transformed.test.feature.vars, \n type=\"raw\")\n> confusionMatrix(data=nn.predictions.new, \n reference=transformed.test.class.var, \n positive=\"X1\")\n\n```", "```py\n> plot(nn.model.new, cex.lab=0.5)\n\n```", "```py\n> nn.model.best <- nn.model\n> nn.predictions.best <- predict(nn.model.best, transformed.test.feature.vars, type=\"prob\")\n> nn.prediction.values <- nn.predictions.best[,2]\n> predictions <- prediction(nn.prediction.values, test.class.var)\n> par(mfrow=c(1,2))\n> plot.roc.curve(predictions, title.text=\"NN ROC Curve\")\n> plot.pr.curve(predictions, title.text=\"NN Precision/Recall Curve\")\n\n```"]