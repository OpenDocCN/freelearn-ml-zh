<html><head></head><body>
		<div id="_idContainer128">
			<h1 class="chapter-number"><a id="_idTextAnchor159"/>13</h1>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor160"/>Designing and Implementing Large-Scale, Robust  ML Software</h1>
			<p>So far, we have learned how to develop ML models, how to work with data, and how to create and test the entire ML pipeline. What remains is to learn how we can integrate these elements into a <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) and how to deploy it so that they can be used without the need to program. To do so, we’ll learn how to deploy the model complete with a UI and the data storage for <span class="No-Break">the model.</span></p>
			<p>In this chapter, we’ll learn how to integrate the ML model with a graphical UI programmed in Gradio and storage in a database. We’ll use two examples of ML pipelines – an example of the model for predicting defects from our previous chapters and a generative AI model to create pictures from a natural <span class="No-Break">language prompt.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>ML is not alone – elements of a deployed <span class="No-Break">ML-based system</span></li>
				<li>The UI of an <span class="No-Break">ML model</span></li>
				<li><span class="No-Break">Data storage</span></li>
				<li>Deploying an ML model for <span class="No-Break">numerical data</span></li>
				<li>Deploying a generative ML model <span class="No-Break">for images</span></li>
				<li>Deploying a code completion model as an extension to Visual <span class="No-Break">Studio Code</span></li>
			</ul>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor161"/>ML is not alone</h1>
			<p><a href="B19548_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> introduced several elements of an ML system – storage, data collection, monitoring, and infrastructure, just to name a few of them. We need all of them to deploy a model for the users, but not all of them are important for the users directly. We need to remember that the users are interested in the results, but we need to pay attention to all details related to the development of such systems. These activities are often called<a id="_idIndexMarker517"/> <span class="No-Break">AI engineering.</span></p>
			<p>The UI is important as it provides the ability to access our models. Depending on the use of our software, the interface can be different. So far, we’ve focused on the models themselves and on the data that is used to train the models. We have not focused on the usability of models and how to integrate them into <span class="No-Break">the tools.</span></p>
			<p>By extension, as for the UI, we also need to talk about storing data in ML. We can use <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) files, but <a id="_idIndexMarker518"/>they quickly become difficult to handle. They are either too large to read into memory or too cumbersome for version control and <span class="No-Break">exchanging data.</span></p>
			<p>Therefore, in this chapter, we’ll focus on making the ML system usable. We’ll learn how to develop a UI, how to link the system to the database, and how to design a Visual Studio Code extension that can complete code <span class="No-Break">in Python.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor162"/>The UI of an ML model</h1>
			<p>A UI <a id="_idIndexMarker519"/>serves as the bridge between the intricate complexities of ML algorithms and the end users who interact with the system. It is the interactive canvas that allows users to input data, visualize results, control parameters, and gain insights from the ML model’s outputs. A well-designed UI empowers users, regardless of their technical expertise, to harness the potential of ML for solving <span class="No-Break">real-world problems.</span></p>
			<p>Effective UIs for ML applications prioritize clarity, accessibility, and interactivity. Whether the application is aimed at business analysts, healthcare professionals, or researchers, the interface should be adaptable to the user’s domain knowledge and objectives. Clear communication of the model’s capabilities and limitations is vital, fostering trust in the technology and enabling users to make informed decisions based on its outputs. Hence my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #66</p>
			<p class="callout">Focus on the user task when designing the UI of the <span class="No-Break">ML model.</span></p>
			<p>We can use <a id="_idIndexMarker520"/>different types of UIs, but the majority of modern tools gravitate around two – web-based interfaces (which require thin clients) and extensions (which provide in-situ improvements). ChatGPT is an example of the web-based interface to the GPT-4 model, while GitHub CoPilot is an example of the extension interface to the <span class="No-Break">same model.</span></p>
			<p>In the first example, let’s look at how easy it is to deploy an ML app using the Gradio framework. Once we have prepared a pipeline for our model, we just need a handful of lines of code to make the app. Here are the lines, based on the example of a model that exists at Hugging Face, for <span class="No-Break">text classification:</span></p>
			<pre class="source-code">
import gradio as gr
from transformers import pipeline
pipe = pipeline("text-classification")
gr.Interface.from_pipeline(pipe).launch()</pre>			<p>The first two lines import the necessary libraries – one for the UI (Gradio) and one for the pipeline. The second line imports the default text classification pipeline from Hugging Face and the last line creates the UI for the pipeline. The UI is in the form of a website with input and output buttons, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer120">
					<img alt="Figure 13.1 – UI for the default text classification pipeline" src="image/B19548_13_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – UI for the default text classification pipeline</p>
			<p>We can<a id="_idIndexMarker521"/> test it by inputting some example text. Normally, we would input this in a script and provide some sort of analysis, but this is done by the Gradio framework for us. We do not even need to link the parameters of the pipeline with the elements of <span class="No-Break">the UI.</span></p>
			<p>What happens behind the scenes can be explained by observing the output of the script in the console (edited <span class="No-Break">for brevity):</span></p>
			<pre class="console">
No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b
Using a pipeline without specifying a model name and revision in production is not recommended.
Downloading (…)lve/main/config.json: 100%|██████████████████| 629/629 [00:00&lt;00:00, 64.6kB/s]
Downloading model.safetensors:
100%|████████████████| 268M/268M [00:04&lt;00:00, 58.3MB/s]
Downloading (…)okenizer_config.json: 100%|████████████████| 48.0/48.0 [00:00&lt;00:00, 20.7kB/s]
Downloading (…)solve/main/vocab.txt: 100%|████████████████| 232k/232k [00:00&lt;00:00, 6.09MB/s]
Running on local URL:  http://127.0.0.1:7860</pre>			<p>The framework has downloaded the default model, its tokenizers, and the vocabulary file and then created the application on the <span class="No-Break">local machine.</span></p>
			<p>The result <a id="_idIndexMarker522"/>of using this app is presented in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.2</em>. We input some simple text and almost instantly get <span class="No-Break">its classification:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer121">
					<img alt="Figure 13.2 – Data analyzed using the default text classification pipeline" src="image/B19548_13_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – Data analyzed using the default text classification pipeline</p>
			<p>This kind of integration is a great way to deploy models first and to make sure that they can be used without the need to open a Python environment or similar. With this, we’ve come to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #67</p>
			<p class="callout">Prepare your models for <span class="No-Break">web deployment.</span></p>
			<p>Regardless of <a id="_idIndexMarker523"/>what kind of models you develop, try to prepare them for web deployment. Our models can be then packaged as Docker containers and provided as part of a larger system of microservices. Using Gradio is a great example of how such a web deployment can <span class="No-Break">be achieved.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor163"/>Data storage</h1>
			<p>So far, we’ve <a id="_idIndexMarker524"/>used CSV files and Excel files to store our data. It’s an easy way to work with ML, but it is also a local one. However, when we want to scale our application and use it outside of just our machine, it is often much more convenient to use a real database engine. The database plays a crucial role in an ML pipeline by providing a structured and organized repository for storing, managing, and retrieving data. As ML applications increasingly rely on large volumes of data, integrating a database into the pipeline becomes essential for a <span class="No-Break">few reasons.</span></p>
			<p>Databases offer a systematic way to store vast amounts of data, making it easily accessible and retrievable. Raw data, cleaned datasets, feature vectors, and other relevant information can be efficiently stored in the database, enabling seamless access by various components of the <span class="No-Break">ML pipeline.</span></p>
			<p>In many ML projects, data preprocessing is a critical step that involves cleaning, transforming, and aggregating data before feeding it to the model. Databases allow you to store intermediate preprocessed data, reducing the need to repeat resource-intensive preprocessing steps each time the model is trained. This speeds up the overall pipeline and maintains <span class="No-Break">data consistency.</span></p>
			<p>ML pipelines often involve data from diverse sources such as sensors, APIs, files, and databases. Having a centralized database simplifies the process of integrating different data streams, ensuring that all relevant information is readily available for training <span class="No-Break">and inference.</span></p>
			<p>Even maintaining a record of different dataset versions is important for reproducibility and tracking changes. Databases can be used to store different versions of datasets, making it easier to roll back to previous versions if needed and facilitating collaboration among <span class="No-Break">team members.</span></p>
			<p>Finally, ML applications that handle large-scale data require efficient data management to scale effectively. Databases provide mechanisms for indexing, partitioning, and optimizing queries, which enhance performance and allow the pipeline to handle increasing <span class="No-Break">data volumes.</span></p>
			<p>So, let’s<a id="_idIndexMarker525"/> create a database in SQLite that would contain the same numerical data that we used in our <span class="No-Break">previous work:</span></p>
			<pre class="source-code">
# create the database
import sqlite3
conn = sqlite3.connect('ant13.db')
c = conn.cursor()</pre>			<p>In the preceding code fragment, we use the <strong class="source-inline">sqlite3</strong> engine to create a database and connect to it (<strong class="source-inline">sqlite3.connect</strong>). Once we connect to a database, we need a cursor to move around in the database and execute our queries. The next step is to import our existing data into <span class="No-Break">the database.</span></p>
			<p>Now, we can open the Excel file and transfer the data to <span class="No-Break">the database:</span></p>
			<pre class="source-code">
# read the excel file with the data
# and save the data to the database
import pandas as pd
# read the excel file
df = pd.read_excel('chapter_12.xlsx', sheet_name='ant_1_3')
# print the first 5 rows
print(df.head())
# create the engine that we use to connect to the database to
# save the data
engine = create_engine('sqlite:///ant13.db')
# save the dataframe to the database
df.to_sql('ant_1_3', engine, index=False, if_exists='replace')</pre>			<p>The<a id="_idIndexMarker526"/> preceding code reads data from an Excel file, processes it using the pandas library, and then saves the processed data into an SQLite database. First, the code reads an Excel file called <strong class="source-inline">'chapter_12.xlsx'</strong> and extracts data from the <strong class="source-inline">'ant_1_3'</strong> sheet. The data is loaded into a pandas DataFrame, <strong class="source-inline">df</strong>. Then, the code uses the <strong class="source-inline">create_engine</strong> function from the <strong class="source-inline">sqlalchemy</strong> library to establish a connection to an SQLite database. It then creates a connection to a database file <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">'ant13.db'</strong></span><span class="No-Break">.</span></p>
			<p>Then, it uses the built-in <strong class="source-inline">to_sql</strong> function to create a database table based on the DataFrame. In this example, the function has the <span class="No-Break">following parameters:</span></p>
			<ul>
				<li><strong class="source-inline">'ant_1_3'</strong> is the name of the table in the database where the data will <span class="No-Break">be stored.</span></li>
				<li><strong class="source-inline">engine</strong> is the connection to the SQLite database that was <span class="No-Break">created earlier.</span></li>
				<li><strong class="source-inline">index=False</strong> specifies that the DataFrame index should not be saved as a separate column in <span class="No-Break">the database.</span></li>
				<li><strong class="source-inline">if_exists='replace'</strong> indicates that if a table named <strong class="source-inline">'ant_1_3'</strong> already exists in the database, it should be replaced with the new data. Other options for <strong class="source-inline">if_exists</strong> include <strong class="source-inline">append</strong> (add data to the table if it exists) and <strong class="source-inline">fail</strong> (raise an error if the table <span class="No-Break">already exists).</span></li>
			</ul>
			<p>After this, we have our data in a database and can easily share the data across multiple ML pipelines. However, in our case, we’ll only demonstrate how to extract such data into a DataFrame so that we can use it in a simple <span class="No-Break">ML application:</span></p>
			<pre class="source-code">
# select all rows from that database
data = engine.execute('SELECT * FROM ant_1_3').fetchall()
# and now, let's create a dataframe from that data
df = pd.DataFrame(data)
# get the names of the columns from the SQL database
# and use them as the column names for the dataframe
df.columns = [x[0] for x in engine.description]
# print the head of the dataframe
df.head()</pre>			<p>The <strong class="source-inline">'SELECT * FROM ant_1_3'</strong> query selects all columns from the <strong class="source-inline">'ant_1_3'</strong> table in the database. The <strong class="source-inline">fetchall()</strong> method retrieves all the rows returned by the query and stores them in the data variable. The data variable will be a list of tuples, where each tuple represents a row <span class="No-Break">of data.</span></p>
			<p>Then, it <a id="_idIndexMarker527"/>creates a pandas DataFrame, <strong class="source-inline">df</strong>, from the data list. Each tuple in the list corresponds to a row in the DataFrame, and the columns of the DataFrame will be numbered automatically. Finally, the code retrieves the names of the columns in the original database table. The <strong class="source-inline">engine.description</strong> attribute holds metadata about the result of the executed SQL query. Specifically, it provides information about the columns returned by the query. The code then extracts the first element of each tuple in <strong class="source-inline">engine.description</strong>, which is the column name, and assigns these names to the columns of the <span class="No-Break">DataFrame, </span><span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break">.</span></p>
			<p>From there, the workflow with the data is just as we know it – it uses a <span class="No-Break">pandas DataFrame.</span></p>
			<p>In this example, the entire DataFrame fits in the database and the entire database can fit into one frame. However, this is not the case for most ML datasets. The pandas library has limitations in terms of its size, so when training models such as GPT models, we need more data than a DataFrame can hold. For that, we can use either the Dataset library from Hugging Face, or we can use databases. We can only fetch a limited amount of data, train a neural network on it, validate on another data, then fetch a new set of rows, train the neural network a bit more, and <span class="No-Break">so on.</span></p>
			<p>In addition <a id="_idIndexMarker528"/>to making the database on files, which can be a bit slow, the SQLite library allows us to create databases in memory, which is much faster, but they do not get serialized to our permanent storage – we need to take care of <span class="No-Break">that ourselves.</span></p>
			<p>To create an in-memory database, we can simply change the name of the database to <strong class="source-inline">:memory:</strong> in the first script, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
conn = sqlite3.connect(':memory:')
c = conn.cursor()</pre>			<p>We can use it later on in a similar way, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
# create the enginve that we use to connect to the database to
# save the data
engine = create_engine('sqlite:///:memory:')
# save the dataframe to the database
df.to_sql('ant_1_3', engine, index=False, if_exists='replace')</pre>			<p>In the end, we need to remember to serialize the database to a file; otherwise, it will disappear the moment our <span class="No-Break">system closes:</span></p>
			<pre class="source-code">
# serialize to disk
c.execute("vacuum main into 'saved.db'")</pre>			<p>Using databases together with ML is quite simple if we know how to work with DataFrames. The added value, however, is quite large. We can serialize data to files, read them into memory, manipulate them, and serialize them again. We can also scale up our applications beyond one system and use these systems online. However, for that, we need <span class="No-Break">a UI.</span></p>
			<p>With that, we’ve come to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #68</p>
			<p class="callout">Try to work with in-memory databases and dump them to <span class="No-Break">disk often.</span></p>
			<p>Libraries such<a id="_idIndexMarker529"/> as pandas have limitations on how much data they can contain. Databases do not. Using an in-memory database provides a combination of the benefits of both without these limitations. Storing the data in memory enables fast access, and using the database engine does not limit the size of the data. We just need to remember to save (dump) the database from the memory to the disk once in a while to prevent the loss of data in case of exceptions, errors, defects, or <span class="No-Break">equipment failures.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor164"/>Deploying an ML model for numerical data</h1>
			<p>Before<a id="_idIndexMarker530"/> we create the UI, we need to define a function that will take care of making predictions using a model that we trained in the previous chapter. This function takes the parameters as a user would see them and then makes a prediction. The following code fragment contains <span class="No-Break">this function:</span></p>
			<pre class="source-code">
import gradio as gr
import pandas as pd
import joblib
def predict_defects(cbo,
                    dcc,
                    exportCoupling,
                    importCoupling,
                    nom,
                    wmc):
    # we need to convert the input parameters to floats to use them in the prediction
    cbo = float(cbo)
    dcc = float(dcc)
    exportCoupling = float(exportCoupling)
    importCoupling = float(importCoupling)
    nom = float(nom)
    wmc = float(wmc)
    # now, we need to make a data frame out of the input parameters
    # this is necessary because the model expects a data frame
    # we create a dictionary with the column names as keys
    # and the input parameters as values
    # please note that the names of the features must be the same as in the model
    data = {
        'CBO': [cbo],
        'DCC': [dcc],
        'ExportCoupling': [exportCoupling],
        'ImportCoupling': [importCoupling],
        'NOM': [nom],
        'WMC': [wmc]
    }
    # we create a data frame from the dictionary
    df = pd.DataFrame(data)
    # load the model
    model = joblib.load('./chapter_12_decision_tree_model.joblib')
    # predict the number of <a id="_idTextAnchor165"/>defects
    result = model.predict(df)[0]
    # return the number of defects
    return result</pre>			<p>This <a id="_idIndexMarker531"/>fragment starts by importing three libraries that are important for the UI and the modeling. We already know about the pandas library, but the other two are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">gradio</strong>: This library is used to create simple UIs for interactive ML model testing. The library makes it very easy to create the UI and connect it to <span class="No-Break">the model.</span></li>
				<li><strong class="source-inline">joblib</strong>: This library is used for saving and loading Python objects, particularly ML models. Thanks to this library, we do not need to train the model every time the user wants to open the <span class="No-Break">software (UI).</span></li>
			</ul>
			<p>The <strong class="source-inline">predict_defects</strong> function is where we use the model. It is important to note that the naming of the parameters is used automatically by the UI to name the input boxes (as we’ll see a bit later). It takes six input parameters: <strong class="source-inline">cbo</strong>, <strong class="source-inline">dcc</strong>, <strong class="source-inline">exportCoupling</strong>, <strong class="source-inline">importCoupling</strong>, <strong class="source-inline">nom</strong>, and <strong class="source-inline">wmc</strong>. These parameters are the same software metrics that we used to train the model. As these parameters are input as text or numbers, it is important to convert them into floats, as this was the input value of our model. Once they have been converted, we need to turn these loose parameters into a single DataFrame that we can use as input to the model. First, we must convert it into a dictionary and then use that dictionary to create <span class="No-Break">a DataFrame.</span></p>
			<p>Once the data is ready, we can load the model using the <strong class="source-inline">model = joblib.load('./chapter_12_decision_tree_model.joblib')</strong> command. The last thing we must do is make a prediction using that model. We can do this by writing <strong class="source-inline">result = model.predict(df)[0]</strong>. The function ends by returning the result of <span class="No-Break">the predictions.</span></p>
			<p>There are a <a id="_idIndexMarker532"/>few items that are important to note. First, we need a separate function to handle the entire workflow since the UI is based on that. This function must have the same number of parameters as the number of input elements we have on our UI. Second, it is important to note that the names of the columns in the DataFrame should be the same as the names of the columns in the training data (the names <span class="No-Break">are case-sensitive).</span></p>
			<p>So, the actual UI is handled completely by the Gradio library. This is exemplified in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# This is where we integrate the function above with the user interface
# for this, we need to create an input box for each of the following parameters:
# CBO, DCC, ExportCoupling,  ImportCoupling,  NOM,  WMC
demo = gr.Interface(fn=predict_defects,
                    inputs = ['number', 'number', 'number', 'number', 'number', 'number'],
                    outputs = gr.Textbox(label='Will contain defects?',
                                         value= 'N/A'))
# and here we start the actual user interface
# in a browser window
demo.launch()</pre>			<p>This code fragment demonstrates the integration of the previously defined <strong class="source-inline">predict_defects</strong> function with a UI. Gradio is used to create a simple UI that takes input from the user, processes it using the provided function, and displays the result. The code consists of<a id="_idIndexMarker533"/> <span class="No-Break">two statements:</span></p>
			<ol>
				<li>Creating the interface using the <strong class="source-inline">gr.Interface</strong> function with the <span class="No-Break">following parameters:</span><ul><li><strong class="source-inline">fn=predict_defects</strong>: This argument specifies the function that will be used to process the user input and produce the output. In this case, it’s the <strong class="source-inline">predict_defects</strong> function that was defined previously. Please note that the arguments of the function are not provided, and the library takes care of extracting them (and their <span class="No-Break">names) automatically.</span></li><li><strong class="source-inline">inputs</strong>: This argument specifies the types of inputs the interface should expect. In this case, it lists six input parameters, each of the <strong class="source-inline">'number'</strong> type. These correspond to the <strong class="source-inline">cbo</strong>, <strong class="source-inline">dcc</strong>, <strong class="source-inline">exportCoupling</strong>, <strong class="source-inline">importCoupling</strong>, <strong class="source-inline">nom</strong>, and <strong class="source-inline">wmc</strong> parameters in the <span class="No-Break"><strong class="source-inline">predict_defects</strong></span><span class="No-Break"> function.</span></li><li><strong class="source-inline">outputs</strong>: This argument specifies the output format that the interface should display to the user. In this case, it’s a text box labeled <strong class="bold">'Will contain defects?'</strong> with an initial value of <strong class="source-inline">'N/A'</strong>. Since our model is binary, we only use 1 and 0 as the output. To mark the fact that the model has not been used yet, we start with the <strong class="source-inline">'</strong><span class="No-Break"><strong class="source-inline">N/A'</strong></span><span class="No-Break"> label.</span></li></ul></li>
				<li>Launching the interface (<strong class="source-inline">demo.launch()</strong>): This line of code starts the UI in a web browser window, allowing users to interact <span class="No-Break">with it.</span></li>
			</ol>
			<p>The UI that was created using Gradio has input fields where the user can provide values for the software metrics (<strong class="source-inline">cbo</strong>, <strong class="source-inline">dcc</strong>, <strong class="source-inline">exportCoupling</strong>, <strong class="source-inline">importCoupling</strong>, <strong class="source-inline">nom</strong>, <strong class="source-inline">wmc</strong>). Once the user provides these values and submits the form, the <strong class="source-inline">predict_defects</strong> function will be called with the provided input values. The predicted result (whether defects will be present or not) will be displayed in the text box labeled 'Will <span class="No-Break">contain defects?'</span></p>
			<p>We can start this application by typing the following in the <span class="No-Break">command prompt:</span></p>
			<pre class="console">
&gt;python app.py</pre>			<p>This starts a local web server and provides us with the address of it. Once we open the page with the app, we’ll see the <span class="No-Break">following UI:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer122">
					<img alt="Figure 13.3 – UI for the defect prediction model created using Gradio" src="image/B19548_13_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – UI for the defect prediction model created using Gradio</p>
			<p>The <a id="_idIndexMarker534"/>UI is structured into two columns – the right-hand column with the result and the left-hand column with the input data. At the moment, the input data is the default, and therefore the prediction value is N/A, as per <span class="No-Break">our design.</span></p>
			<p>We can fill in the data and press the <strong class="bold">Submit</strong> button to obtain the values of the prediction. This is shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer123">
					<img alt="Figure 13.4 – UI with the prediction outcome" src="image/B19548_13_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – UI with the prediction outcome</p>
			<p>Once we<a id="_idIndexMarker535"/> fill in data to make predictions, we can submit it; at this point, our outcome shows that the module with these characteristics would contain defects. It’s also quite logical – for any module that has 345 inputs, we could almost guarantee that there would be some defects. It’s just <span class="No-Break">too complex.</span></p>
			<p>This model, and the UI, are only available locally on our computer. We can, however, share it with others and even embed it in websites, if we change just one line. Instead of <strong class="source-inline">demo.launch()</strong> without parameters, we can supply one parameter – <span class="No-Break"><strong class="source-inline">demo.launch(share=True)</strong></span><span class="No-Break">.</span></p>
			<p>Although we’ve used Gradio as an example of the UI, it illustrates that it is rather easy to link an existing model to a UI. We can input the data manually and get a prediction from the model. Whether the UI is programmed in Gradio or any other framework becomes less important. The difficulty may differ – for example, we may need to program the link between the input text boxes and model parameters manually – but the essence is <span class="No-Break">the same.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor166"/>Deploying a generative ML model for images</h1>
			<p>The<a id="_idIndexMarker536"/> Gradio framework is very flexible and allows for quickly deploying models such as generative AI stable diffusion models – image generators that work similarly to the DALL-E model. The deployment of such a model is very similar to the deployment of the numerical model we <span class="No-Break">covered previously.</span></p>
			<p>First, we need to create a function that will generate images based on one of the models from Hugging Face. The following code fragment shows <span class="No-Break">this function:</span></p>
			<pre class="source-code">
import gradio as gr
import pandas as pd
from diffusers import StableDiffusionPipeline
import torch
def generate_images(prompt):
    '''
    This function uses the prompt to generate an image
    using the anything 4.0 model from Hugging Face
    '''
    # importing the model from Hugging Face
    model_id = "xyn-ai/anything-v4.0"
    pipe = StableDiffusionPipeline.from_pretrained(model_id,
                                                   torch_dtype=torch.float16,
                                                   safety_checker=None)
    # send the pipeline to the GPU for faster processing
    pipe = pipe.to("cuda")
    # create the image here
    image = pipe(prompt).images[0]
    # return the number of defects
    return image</pre>			<p>This <a id="_idIndexMarker537"/>code fragment starts by importing the necessary libraries. Here, we’ll notice that there is another library – <strong class="source-inline">diffusers</strong> – which is an interface to image generation networks. The function imports a pre-trained model from the Hugging Face hub. The model is <strong class="source-inline">"xyn-ai/anything-v4.0"</strong>. It is a variant of the Anything 4.0 model, cloned by one of the users. The <strong class="source-inline">StableDiffusionPipeline.from_pretrained()</strong> function is used to load the model as a pipeline for image generation. The <strong class="source-inline">torch_dtype</strong> parameter is set to <strong class="source-inline">torch.float16</strong>, which indicates the data type to be used for computations (lower precision for <span class="No-Break">faster processing).</span></p>
			<p>The image is generated using the pipeline bypassing the prompt as an argument to the <strong class="source-inline">pipe()</strong> function. The generated images are accessed using the <strong class="source-inline">images[0]</strong> attribute. The <strong class="source-inline">prompt</strong> parameter is provided through the parameter of the function, which is supplied by <span class="No-Break">the UI.</span></p>
			<p>The function returns the image, which is then captured by the UI <span class="No-Break">and displayed.</span></p>
			<p>The code for the UI is also quite straightforward once we know the code from the <span class="No-Break">previous example:</span></p>
			<pre class="source-code">
demo = gr.Interface(fn=generate_images,
                    inputs = 'text',
                    outputs = 'image')
# and here we start the actual user interface
# in a browser window
demo.launch()</pre>			<p>Compared to <a id="_idIndexMarker538"/>the previous example, this code contains only one input parameter, which is the prompt that’s used to generate the image. It also has one output, which is the image itself. We use the <strong class="source-inline">'image'</strong> class to indicate that it is an image and should be displayed as such. The output of this model is presented in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer124">
					<img alt="Figure 13.5 – Image generated from the Anything 4.0 model using the “car and sun” prompt" src="image/B19548_13_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Image generated from the Anything 4.0 model using the “car and sun” prompt</p>
			<p>Please note <a id="_idIndexMarker539"/>that the model is not perfect as the generated car has distortion artifacts – for example, the right-hand taillight has not been <span class="No-Break">generated perfectly.</span></p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor167"/>Deploying a code completion model as an extension</h1>
			<p>So far, we’ve <a id="_idIndexMarker540"/>learned how to deploy models online and on the Hugging Face hub. These are good methods and provide us with the ability to create a UI for our models. However, these are standalone tools that require manual input and provide an output that we need to use manually – for example, paste into another tool or save <span class="No-Break">to disk.</span></p>
			<p>In software engineering, many tasks are automated and many modern tools provide an ecosystem of extensions and add-ins. GitHub Copilot is such an add-in to Visual Studio 2022 and an extension to Visual Studio Code – among other tools. ChatGPT is both a standalone web tool and an add-in to Microsoft’s Bing <span class="No-Break">search engine.</span></p>
			<p>Therefore, in the last part of this chapter, we’ll package our models as an extension to a programming environment. In this section, we learn how to create an extension to complete code, just like GitHub CoPilot. Naturally, we won’t use the CodeX model from CoPilot, but Codeparrot’s model for the Python programming language. We’ve seen this model before, so let’s dive deeper into the <span class="No-Break">actual extension.</span></p>
			<p>We need<a id="_idIndexMarker541"/> a few tools to develop the extension. Naturally, we need Visual Studio Code itself and the Python programming environment. We also need the Node.js toolkit to create the extension. We installed it from nodejs.org. Once we have installed it, we can use Node.js’s package manager to install Yeoman and the framework to develop the extension. We can do that by using the following command in the <span class="No-Break">command prompt:</span></p>
			<pre class="console">
npm install -g yo generator-code</pre>			<p>Once the packages have been installed, we need to create the skeleton code for our extension by typing <span class="No-Break">the following:</span></p>
			<pre class="console">
yo code</pre>			<p>This will bring up the menu that we need to <span class="No-Break">fill in:</span></p>
			<pre class="console">
     _-----_     ╭──────────────────────────╮
    |       |    │   Welcome to the Visual  │
    |--(o)--|    │   Studio Code Extension  │
   `---------´   │        generator!        │
    ( _´U`_ )    ╰──────────────────────────╯
    /___A___\   /
     |  ~  |
   __'.___.'__
 ´   `  |° ´ Y `
? What type of extension do you want to create? (Use arrow keys)
&gt; New Extension (TypeScript)
  New Extension (JavaScript)
  New Color Theme
  New Language Support
  New Code Snippets
  New Keymap
  New Extension Pack
  New Language Pack (Localization)
  New Web Extension (TypeScript)
  New Notebook Renderer (TypeScript)</pre>			<p>We need <a id="_idIndexMarker542"/>to choose the first option, which is a new extension that uses Typescript. It is the easiest way to start writing the extension. We could develop a very powerful extension using the language pack and language protocol, but for this first extension, simplicity <span class="No-Break">beats power.</span></p>
			<p>We need to make a few decisions about the setup of our extension, so let’s do <span class="No-Break">that now:</span></p>
			<pre class="console">
? What type of extension do you want to create? New Extension (TypeScript)
? What's the name of your extension? mscopilot
? What's the identifier of your extension? mscopilot
? What's the description of your extension? Code generation using Parrot
? Initialize a git repository? No
? Bundle the source code with webpack? No
? Which package manager to use? (Use arrow keys)
&gt; npm
  yarn
  pnpm</pre>			<p>We call our extension <strong class="source-inline">mscopilot</strong> and do not create much additional code – no Git repository and no webpack. Again, simplicity is the key for this example. Once the folder has been created, we need one more package from Node.js to interact <span class="No-Break">with Python:</span></p>
			<pre class="console">
npm install python-shell</pre>			<p>After <a id="_idIndexMarker543"/>we click on the last entry, we get a new folder named <strong class="source-inline">mscopilot</strong>; we can enter it with the <strong class="source-inline">code .</strong> command. It opens Visual Studio Code, where we can fill the template with the code for our new extension. Once the environment opens, we need to navigate to the <strong class="source-inline">package.json</strong> file and change a few things. In that file, we need to find the <strong class="source-inline">contributes</strong> section and make a few changes, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
"contributes": {
    "commands": [
      {
        "command": "mscopilot.logSelectedText",
        "title": "MS Suggest code"
      }
    ],
    "keybindings": [
      {
        "command": "mscopilot.logSelectedText",
        "key": "ctrl+shift+l",
        "mac": "cmd+shift+l"
      }
    ]
  },</pre>			<p>In the preceding code fragment, we added some information stating that our extension has one new function – <strong class="source-inline">logSelectedText</strong> – and that it will be available via the <em class="italic">Ctrl</em> + <em class="italic">Shift</em> + <em class="italic">L</em> key combination on Windows (and a similar one on Mac). We need to remember that the command name includes the name of our extension so that the extension <a id="_idIndexMarker544"/>manager knows that this command belongs to our extension. Now, we need to go to the <strong class="source-inline">extension.ts</strong> file and add the code for our command. The code following fragment contains the first part of the code – the setup for the extension and <span class="No-Break">its activation:</span></p>
			<pre class="source-code">
import * as vscode from 'vscode';
// This method is called when your extension is activated
export function activate(context: vscode.ExtensionContext) {
  // Use the console to output diagnostic information (console.log) and errors (console.error)
  // This line of code will only be executed once when your extension is activated
  console.log('Congratulations, your extension "mscopilot" is now active!');</pre>			<p>This function just logs that our extension has been activated. Since the extension is rather invisible to the user (and it should be), it is a good practice to use the log file to store the information that has <span class="No-Break">been instantiated.</span></p>
			<p>Now, we add the code that will get the selected text, instantiate the Parrot model, and add the suggestion to <span class="No-Break">the editor:</span></p>
			<pre class="source-code">
// Define a command to check which code is selected.
vscode.commands.registerCommand('mscopilot.logSelectedText', () =&gt; {
  // libraries needed to execute python scripts
  const python = require('python-shell');
  const path = require('path');
  // set up the path to the right python interpreter
  // in case we have a virtual environment
  python.PythonShell.defaultOptions = { pythonPath: 'C:/Python311/python.exe' };
  // Get the active text editor
  const editor = vscode.window.activeTextEditor;
  // Get the selected text
     const selectedText = editor.document.getText(editor.selection);
  // prompt is the same as the selected text
  let prompt:string = selectedText;
  // this is the script in Python that we execute to
  // get the code generated by the Parrot model
  //
  // please note the strange formatting,
  // which is necessary as python is sensitive to indentation
  let scriptText = `
from transformers import pipeline
pipe = pipeline("text-generation", model="codeparrot/codeparrot-small")
outputs = pipe("${prompt}", max_new_tokens=30, do_sample=False)
print(outputs[0]['generated_text'])`;
  // Let the user know what we start the code generation
  vscode.window.showInformationMessage(`Starting code generation for prompt: ${prompt}`);
  // run the script and get the message back
  python.PythonShell.runString(scriptText, null).then(messages=&gt;{
  console.log(messages);
  // get the active editor to paste the code there
  let activeEditor = vscode.window.activeTextEditor;
  // paste the generated code snippet
  activeEditor.edit((selectedText) =&gt; {
  // when we get the response, we need to format it
  // as one string, not an array of strings
  let snippet = messages.join('\n');
  // and replace the selected text with the output
  selectedText.replace(activeEditor.selection, snippet)  });
  }).then(()=&gt;{
   vscode.window.showInformationMessage(`Code generation finished!`);});
  });
context.subscriptions.push(disposable);
}</pre>			<p>This<a id="_idIndexMarker545"/> code registers our <strong class="source-inline">'mscopilot.logSelectedText'</strong> command. We made this visible to the extension manager in the previous file – <strong class="source-inline">package.json</strong>. When this command is executed, it performs the following steps. The important part is the interaction between the code in TypeScript and the code in Python. Since we’re using the Hugging Face model, the easiest way is to use the same scripts that we’ve used so far in this book. However, since the extensions are written in TypeScript (or JavaScript), we need to embed the Python code in TypeScript, add a variable to it, and capture <span class="No-Break">the outcome:</span></p>
			<ol>
				<li>First, imports the required libraries – <strong class="source-inline">python-shell</strong> and <strong class="source-inline">path</strong> – which are needed to execute Python scripts from within a <span class="No-Break">Node.js environment.</span></li>
				<li>Next, it sets up the Python interpreter via <strong class="source-inline">C:/Python311/python.exe</strong>, which will be used to run Python scripts. This is important for ensuring that the correct Python environment is used, even when using a virtual environment. If we do not specify it, we need to find it in the script, which is a bit tricky in the <span class="No-Break">user environment.</span></li>
				<li>After, it sets the active text editor and the selected text. We need this selection so that we can send the prompt to the model. In our case, we’ll simply send the selection to the model and get the <span class="No-Break">suggested code.</span></li>
				<li>Then, it prepares the prompt, which means that it creates a string variable that we use in the code of the <span class="No-Break">Python script.</span></li>
				<li>Next, it defines the Python script, where the connection to the ML model is established. Our Python script is defined as a multi-line string (<strong class="source-inline">scriptText</strong>) using a template literal. This script utilizes the Hugging Face Transformers library’s <strong class="source-inline">pipeline</strong> function to perform text generation using the <strong class="source-inline">codeparrot-small</strong> model. The Python code is in boldface, and we can see that the string is complemented with the prompt, which is the selected text in the <span class="No-Break">active editor.</span></li>
				<li>Then, it <a id="_idIndexMarker546"/>displays short information to the user since the model requires some time to load and make an inference. It may take up to a minute (for the first execution) to get the inference as the model needs to be downloaded and set up. Therefore, it is important to display a message that we’re starting the inference. A message is displayed to the user using <strong class="source-inline">vscode.window.showInformationMessage</strong>, indicating that the code generation process is about <span class="No-Break">to start.</span></li>
				<li>After, it runs the Python script (<strong class="source-inline">scriptText</strong>) using <strong class="source-inline">python.PythonShell.runString</strong>. The script’s output is captured in the <strong class="source-inline">messages</strong> array. We lost control over the execution for a while since we waited for the Python script to finish; it provided us with a suggestion for <span class="No-Break">code completion.</span></li>
				<li>Next, it pastes the generated code from the response (<strong class="source-inline">messages</strong>) array into a single string (<strong class="source-inline">snippet</strong>). The snippet is then pasted into the active text editor at the position of the selected text, effectively replacing the selected text with the generated code. Since the first element of the response from the model is the prompt, we can simply just replace the selection with <span class="No-Break">the snippet.</span></li>
				<li>Finally, it displays the completion message after the code <span class="No-Break">generation process.</span></li>
			</ol>
			<p>Now, we have an extension that we can test. We can execute it by pressing the <em class="italic">F5</em> key. This brings up a new instance of Visual Studio, where we can type a piece of code to be completed, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer125">
					<img alt="Figure 13.6 – A test instance of some Visual Studio code with our extension activated. The selected text is used as the prompt for the model" src="image/B19548_13_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – A test instance of some Visual Studio code with our extension activated. The selected text is used as the prompt for the model</p>
			<p>Once <a id="_idIndexMarker547"/>we press <em class="italic">Ctrl</em> + <em class="italic">Shift</em> + <em class="italic">L</em>, as we defined in the <strong class="source-inline">package.json</strong> file, our command is activated. This is indicated by the messages in the lower right-hand corner of the environment in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer126">
					<img alt="Figure 13.7 – Starting to generate the code. The message box and the log information indicate that our command is working" src="image/B19548_13_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – Starting to generate the code. The message box and the log information indicate that our command is working</p>
			<p>After a few seconds, we get a suggestion from the Parrot model, which we must then paste into the editor, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer127">
					<img alt="Figure 13.8 – Code suggestion from the Parrot model pasted into the editor" src="image/B19548_13_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Code suggestion from the Parrot model pasted into the editor</p>
			<p>Here, we <a id="_idIndexMarker548"/>can see that our extension is rather simple and gets all the suggestions from the model. So, in addition to the proper code (<strong class="source-inline">return "Hello World!"</strong>), it included more than we needed. We could write more interesting logic, parse the code, and clean it – the sky is the limit. I leave it to you to continue this work and to make it better. My job was to illustrate that writing a GitHub CoPilot-like tool is not as difficult as it <span class="No-Break">may seem.</span></p>
			<p>With this, we’ve come to my final best practice for <span class="No-Break">this chapter.</span></p>
			<p class="callout-heading">Best practice #69</p>
			<p class="callout">If your model/software aims to help in the daily tasks of your users, make sure that you develop it as <span class="No-Break">an add-in.</span></p>
			<p>Although we could use the Codeparrot model from the Gradio interface, it would not be appreciated. Programmers would have to copy their code to a web browser, click a button, wait for the suggestion, and paste it back into their environment. By providing an extension to Visual Studio Code, we can tap into the workflow of software developers. The only extra task is to select the text to complete and press <em class="italic">Ctrl</em> + <em class="italic">Shift</em> + <em class="italic">L</em>; I’m sure that this could be simplified even more, just like GitHub <span class="No-Break">Copilot does.</span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor168"/>Summary</h1>
			<p>This chapter concludes the third part of this book. It also concludes the most technical part of our journey through the best practices. We’ve learned how to develop ML systems and how to deploy them. These activities are often called AI engineering, which is the term that places the focus on the development of software systems rather than the models themselves. This term also indicates that testing, deploying, and using ML is much more than training, validating, and testing <span class="No-Break">the models.</span></p>
			<p>Naturally, there is even more to this. Just developing and deploying AI software is not enough. We, as software engineers or AI engineers, need to consider the implications of our actions. Therefore, in the next part of this book, we’ll explore the concepts of bias, ethics, and the sustainable use of the fruits of our work – AI <span class="No-Break">software systems.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor169"/>References</h1>
			<ul>
				<li><em class="italic">Rana, R., et al. A framework for adoption of machine learning in industry for software defect prediction. In 2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA). </em><span class="No-Break"><em class="italic">2014. IEEE.</em></span></li>
				<li><em class="italic">Bosch, J., H.H. Olsson, and I. Crnkovic, Engineering ai systems: A research agenda. Artificial Intelligence Paradigms for Smart Cyber-Physical Systems, 2021: </em><span class="No-Break"><em class="italic">p. 1-19.</em></span></li>
				<li><em class="italic">Giray, G., A software engineering perspective on engineering machine learning systems: State of the art and challenges. Journal of Systems and Software, 2021. 180: </em><span class="No-Break"><em class="italic">p. 111031.</em></span></li>
			</ul>
		</div>
	</body></html>