<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Face Recognition Using Eigenfaces or Fisherfaces</h1>
            </header>

            <article>
                
<p>In this chapter, we cover the following:</p>
<ul>
<li>Face detection</li>
<li>Face preprocessing</li>
<li>Training a machine-learning algorithm from collected faces</li>
<li>Face recognition</li>
<li>Finishing touches</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introduction to face recognition and face detection</h1>
            </header>

            <article>
                
<p>Face recognition is the process of putting a label to a known face. Just like humans learn to recognize their family, friends, and celebrities just by seeing their face, there are many techniques for a computer to learn to recognize a known face. These generally involve four main steps:</p>
<ol>
<li><strong>Face detection</strong>: This is the process of locating a face region in an image (a large rectangle near the center of the following screenshot). This step does not care who the person is, just that it is a human face.</li>
<li><strong>Face preprocessing</strong>: This is the process of adjusting the face image to look more clear and similar to other faces (a small grayscale face in the top-center of the following screenshot).</li>
<li><strong>Collecting and learning faces</strong>: This is the process of saving many preprocessed faces (for each person that should be recognized), and then learning how to recognize them.</li>
<li><strong>Face recognition</strong>: This is the process that checks which of the collected people are most similar to the face in the camera (a small rectangle on the top-right of the following screenshot).</li>
</ol>
<div class="packt_infobox">Note that the phrase <strong>face recognition</strong> is often used by the general public for finding positions of faces (that is, face detection, as described in step 1), but this book will use the formal definition of face recognition referring to step 4 and face detection referring to <em>step 1</em>.</div>
<p>The following screenshot shows the final <kbd>WebcamFaceRec</kbd> project, including a small rectangle at the top-right corner highlighting the recognized person. Also notice the confidence bar that is next to the preprocessed face (a small face at the top-center of the rectangle marking the face), which in this case shows roughly 70 percent confidence that it has recognized the correct person:</p>
<div class="CDPAlignCenter CDPAlign"><img height="220" width="293" class="image-border" src="assets/image_07_001.jpg"/></div>
<p>The current face detection techniques are quite reliable in real-world conditions, whereas current face recognition techniques are much less reliable when used in real-world conditions. For example, it is easy to find research papers showing face recognition accuracy rates above 95 percent, but when testing those same algorithms yourself, you may often find that accuracy is lower than 50 percent. This comes from the fact that current face recognition techniques are very sensitive to exact conditions in the images, such as the type of lighting, direction of lighting and shadows, exact orientation of the face, expression of the face, and the current mood of the person. If they are all kept constant when training (collecting images) as well as when testing (from the camera image), then face recognition should work well, but if the person was standing to the left-hand side of the lights in a room when training, and then stood to the right-hand side while testing with the camera, it may give quite bad results. So the dataset used for training is very important.</p>
<p>Face preprocessing (<em>step 2</em>) aims to reduce these problems, such as by making sure the face always appears to have similar brightness and contrast, and perhaps making sure the features of the face will always be in the same position (such as aligning the eyes and/or nose to certain positions). A good face preprocessing stage will help improve the reliability of the whole face recognition system, so this chapter will place some emphasis on face preprocessing methods.</p>
<p>Despite the big claims about face recognition for security in the media, it is unlikely that the current face recognition methods alone are reliable enough for any true security system, but they can be used for purposes that don't need high reliability, such as playing personalized music for different people entering a room or a robot that says your name when it sees you. There are also various practical extensions to face recognition, such as gender recognition, age recognition, and emotion recognition.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 1 - face detection</h1>
            </header>

            <article>
                
<p>Until the year 2000, there were many different techniques used for finding faces, but all of them were either very slow, very unreliable, or both. A major change came in 2001 when Viola and Jones invented the Haar-based cascade classifier for object detection, and in 2002 when it was improved by Lienhart and Maydt. The result is an object detector that is both fast (it can detect faces in real time on a typical desktop with a VGA webcam) and reliable (it detects approximately 95 percent of frontal faces correctly). This object detector revolutionized the field of face recognition (as well as that of robotics and computer vision in general), as it finally allowed real-time face detection and face recognition, especially as Lienhart himself wrote the object detector that comes free with OpenCV! It works not only for frontal faces but also side-view faces (referred to as profile faces), eyes, mouths, noses, company logos, and many other objects.</p>
<p>This object detector was extended in OpenCV v2.0 to also use LBP features for detection based on work by Ahonen, Hadid, and Pietikäinen in 2006, as LBP-based detectors are potentially several times faster than Haar-based detectors, and don't have the licensing issues that many Haar detectors have.</p>
<p>The basic idea of the Haar-based face detector is that if you look at most frontal faces, the region with the eyes should be darker than the forehead and cheeks, and the region with the mouth should be darker than cheeks, and so on. It typically performs about 20 stages of comparisons like this to decide if it is a face or not, but it must do this at each possible position in the image and for each possible size of the face, so in fact it often does thousands of checks per image. The basic idea of the LBP-based face detector is similar to the Haar-based one, but it uses histograms of pixel intensity comparisons, such as edges, corners, and flat regions.</p>
<p>Rather than have a person decide which comparisons would best define a face, both Haar- and LBP-based face detectors can be automatically trained to find faces from a large set of images, with the information stored as XML files to be used later. These cascade classifier detectors are typically trained using at least 1,000 unique face images and 10,000 non-face images (for example, photos of trees, cars, and text), and the training process can take a long time even on a multi-core desktop (typically a few hours for LBP but 1week for Haar!). Luckily, OpenCV comes with some pretrained Haar and LBP detectors for you to use! In fact you can detect frontal faces, profile (side-view) faces, eyes, or noses just by loading different cascade classifier XML files to the object detector, and choose between the Haar or LBP detector, based on which XML file you choose.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Implementing face detection using OpenCV</h1>
            </header>

            <article>
                
<p>As mentioned previously, OpenCV v2.4 comes with various, pretrained XML detectors that you can use for different purposes. The following table lists some of the most popular XML files:</p>
<table class="table">
<tbody>
<tr>
<td><strong>Type of cascade classifier</strong></td>
<td><strong>XML filename</strong></td>
</tr>
<tr>
<td>Face detector (default)</td>
<td><kbd>haarcascade_frontalface_default.xml</kbd></td>
</tr>
<tr>
<td>Face detector (fast Haar)</td>
<td><kbd>haarcascade_frontalface_alt2.xml</kbd></td>
</tr>
<tr>
<td>Face detector (fast LBP)</td>
<td><kbd>lbpcascade_frontalface.xml</kbd></td>
</tr>
<tr>
<td>Profile (side-looking) face detector</td>
<td><kbd>haarcascade_profileface.xml</kbd></td>
</tr>
<tr>
<td>Eye detector (separate for left and right)</td>
<td><kbd>haarcascade_lefteye_2splits.xml</kbd></td>
</tr>
<tr>
<td>Mouth detector</td>
<td><kbd>haarcascade_mcs_mouth.xml</kbd></td>
</tr>
<tr>
<td>Nose detector</td>
<td><kbd>haarcascade_mcs_nose.xml</kbd></td>
</tr>
<tr>
<td>Whole person detector</td>
<td><kbd>haarcascade_fullbody.xml</kbd></td>
</tr>
</tbody>
</table>
<p>Haar-based detectors are stored in the <kbd>datahaarcascades</kbd> folder and LBP-based detectors are stored in the <kbd>datalbpcascades</kbd> folder of the OpenCV root folder, such as <kbd>C:opencvdatalbpcascades</kbd>.</p>
<p>For our face recognition project, we want to detect frontal faces, so let's use the LBP face detector because it is the fastest and doesn't have patent licensing issues. Note that this pretrained LBP face detector that comes with OpenCV v2.x is not tuned as well as the pretrained Haar face detectors, so if you want more reliable face detection then you may want to train your own LBP face detector or use a Haar face detector.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading a Haar or LBP detector for object or face detection</h1>
            </header>

            <article>
                
<p>To perform object or face detection, first you must load the pretrained XML file using OpenCV's <kbd>CascadeClassifier</kbd> class as follows:</p>
<pre>
    CascadeClassifier faceDetector; 
    faceDetector.load(faceCascadeFilename);
</pre>
<p>This can load Haar or LBP detectors just by giving a different filename. A very common mistake when using this is to provide the wrong folder or filename, but depending on your build environment, the <kbd>load()</kbd> method will either return <kbd>false</kbd> or generate a C++ exception (and exit your program with an assert error). So it is best to surround the <kbd>load()</kbd> method with a <kbd>try... catch</kbd> block and display a nice error message to the user if something went wrong. Many beginners skip checking for errors, but it is crucial to show a help message to the user when something did not load correctly, otherwise you may spend a very long time debugging other parts of your code before eventually realizing something did not load. A simple error message can be displayed as follows:</p>
<pre>
    CascadeClassifier faceDetector; 
    try { 
      faceDetector.load(faceCascadeFilename); 
    } catch (cv::Exception e) {} 
    if ( faceDetector.empty() ) { 
      cerr &lt;&lt; "ERROR: Couldn't load Face Detector ("; 
      cerr &lt;&lt; faceCascadeFilename &lt;&lt; ")!" &lt;&lt; endl; 
      exit(1); 
    }
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Accessing the webcam</h1>
            </header>

            <article>
                
<p>To grab frames from a computer's webcam or even from a video file, you can simply call the <kbd>VideoCapture::open()</kbd> function with the camera number or video filename, then grab the frames using the C++ stream operator, as mentioned in the section,<em>Accessing the webcam</em> in <a href="03913e76-ec18-4a31-875e-dfceca32d26f.xhtml"><span class="ChapterrefPACKT">Chapter 1</span></a>, <em>Cartoonifier and Skin Changer for Raspberry Pi</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Detecting an object using the Haar or LBP Classifier</h1>
            </header>

            <article>
                
<p>Now that we have loaded the classifier (just once during initialization), we can use it to detect faces in each new camera frame. But first, we should do some initial processing of the camera image just for face detection, by performing the following steps:</p>
<ol>
<li><strong>Grayscale color conversion</strong>: Face detection only works on grayscale images. So we should convert the color camera frame to grayscale.</li>
<li><strong>Shrinking the camera image</strong>: The speed of face detection depends on the size of the input image (it is very slow for large images but fast for small images), and yet detection is still fairly reliable even at low resolutions. So we should shrink the camera image to a more reasonable size (or use a large value for <kbd>minFeatureSize</kbd> in the detector, as explained shortly).</li>
<li><strong>Histogram equalization</strong>: Face detection is not as reliable in low-light conditions. So we should perform histogram equalization to improve the contrast and brightness.</li>
</ol>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Grayscale color conversion</h1>
            </header>

            <article>
                
<p>We can easily convert an RGB color image to grayscale using the <kbd>cvtColor()</kbd> function. But we should do this only if we know we have a color image (that is, it is not a grayscale camera), and we must specify the format of our input image (usually 3-channel BGR on desktop or 4-channel BGRA on mobile). So we should allow three different input color formats, as shown in the following code:</p>
<pre>
    Mat gray; 
    if (img.channels() == 3) { 
      cvtColor(img, gray, CV_BGR2GRAY); 
    } 
    else if (img.channels() == 4) { 
      cvtColor(img, gray, CV_BGRA2GRAY); 
    } 
    else { 
      // Access the grayscale input image directly. 
      gray = img; 
    }
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Shrinking the camera image</h1>
            </header>

            <article>
                
<p>We can use the <kbd>resize()</kbd> function to shrink an image to a certain size or scale factor. Face detection usually works quite well for any image size greater than 240x240 pixels (unless you need to detect faces that are far away from the camera), because it will look for any faces larger than the <kbd>minFeatureSize</kbd> (typically 20x20 pixels). So let's shrink the camera image to be 320 pixels wide; it doesn't matter if the input is a VGA webcam or a five mega pixel HD camera. It is also important to remember and enlarge the detection results, because if you detect faces in a shrunk image then the results will also be shrunk. Note that instead of shrinking the input image, you could use a large value for the <kbd>minFeatureSize</kbd> variable in the detector instead. We must also ensure the image does not become fatter or thinner. For example, a widescreen 800x400 image when shrunk to 300x200 would make a person look thin. So we must keep the aspect ratio (the ratio of width to height) of the output the same as the input. Let's calculate how much to shrink the image width by, then apply the same scale factor to the height as well, as follows:</p>
<pre>
    const int DETECTION_WIDTH = 320; 
    // Possibly shrink the image, to run much faster. 
    Mat smallImg; 
    float scale = img.cols / (float) DETECTION_WIDTH; 
    if (img.cols &gt; DETECTION_WIDTH) { 
      // Shrink the image while keeping the same aspect ratio. 
      int scaledHeight = cvRound(img.rows / scale); 
      resize(img, smallImg, Size(DETECTION_WIDTH, scaledHeight)); 
    } 
    else { 
      // Access the input directly since it is already small. 
      smallImg = img; 
    }
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Histogram equalization</h1>
            </header>

            <article>
                
<p>We can easily perform histogram equalization to improve the contrast and brightness of an image, using the <kbd>equalizeHist()</kbd> function. Sometimes this will make the image look strange, but in general it should improve the brightness and contrast and help face detection. The <kbd>equalizeHist()</kbd> function is used as follows:</p>
<pre>
    // Standardize the brightness &amp; contrast, such as 
    // to improve dark images. 
    Mat equalizedImg; 
    equalizeHist(inputImg, equalizedImg);
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Detecting the face</h1>
            </header>

            <article>
                
<p>Now that we have converted the image to grayscale, shrunk the image, and equalized the histogram, we are ready to detect the faces using the <kbd>CascadeClassifier::detectMultiScale()</kbd> function! There are many parameters that we pass to this function:</p>
<ul>
<li><kbd>minFeatureSize</kbd>: This parameter determines the minimum face size that we care about, typically 20x20 or 30x30 pixels but this depends on your use case and image size. If you are performing face detection on a webcam or smartphone where the face will always be very close to the camera, you could enlarge this to 80 x 80 to have much faster detections, or if you want to detect far away faces, such as on a beach with friends, then leave this as 20x20.</li>
<li><kbd>searchScaleFactor</kbd>: This parameter determines how many different sizes of faces to look for; typically it would be <kbd>1.1</kbd>, for good detection, or <kbd>1.2</kbd> for faster detection that does not find the face as often.</li>
<li><kbd>minNeighbors</kbd>: This parameter determines how sure the detector should be that it has detected a face, typically a value of <kbd>3</kbd> but you can set it higher if you want more reliable faces, even if many faces are not detected.</li>
<li><kbd>flags</kbd>: This parameter allows you to specify whether to look for all faces (default) or only look for the largest face (<kbd>CASCADE_FIND_BIGGEST_OBJECT</kbd>). If you only look for the largest face, it should run faster. There are several other parameters you can add to make the detection about 1% or 2% faster, such as <kbd>CASCADE_DO_ROUGH_SEARCH</kbd> or <kbd>CASCADE_SCALE_IMAGE</kbd>.</li>
</ul>
<p>The output of the <kbd>detectMultiScale()</kbd> function will be a <kbd>std::vector</kbd> of the <kbd>cv::Rect</kbd> type object. For example, if it detects two faces then it will store an array of two rectangles in the output. The <kbd>detectMultiScale()</kbd> function is used as follows:</p>
<pre>
    int flags = CASCADE_SCALE_IMAGE; // Search for many faces. 
    Size minFeatureSize(20, 20);     // Smallest face size. 
    float searchScaleFactor = 1.1f;  // How many sizes to search. 
    int minNeighbors = 4;            // Reliability vs many faces. 

// Detect objects in the small grayscale image. 
std::vector&lt;Rect&gt; faces; 
faceDetector.detectMultiScale(img, faces, searchScaleFactor,  
                minNeighbors, flags, minFeatureSize);
</pre>
<p>We can see if any faces were detected by looking at the number of elements stored in our vector of rectangles; that is, by using the <kbd>objects.size()</kbd> function.</p>
<p>As mentioned earlier, if we gave a shrunken image to the face detector, the results will also be shrunk, so we need to enlarge them if we want to know the face regions for the original image. We also need to make sure faces on the border of the image stay completely within the image, as OpenCV will now raise an exception if this happens, as shown by the following code:</p>
<pre>
    // Enlarge the results if the image was temporarily shrunk. 
    if (img.cols &gt; scaledWidth) { 
      for (int i = 0; i &lt; (int)objects.size(); i++ ) { 
        objects[i].x = cvRound(objects[i].x * scale); 
        objects[i].y = cvRound(objects[i].y * scale); 
        objects[i].width = cvRound(objects[i].width * scale); 
        objects[i].height = cvRound(objects[i].height * scale); 
      } 
    } 
    // If the object is on a border, keep it in the image. 
    for (int i = 0; i &lt; (int)objects.size(); i++ ) { 
      if (objects[i].x &lt; 0) 
        objects[i].x = 0; 
      if (objects[i].y &lt; 0) 
        objects[i].y = 0; 
      if (objects[i].x + objects[i].width &gt; img.cols) 
        objects[i].x = img.cols - objects[i].width; 
      if (objects[i].y + objects[i].height &gt; img.rows) 
        objects[i].y = img.rows - objects[i].height; 
    }
</pre>
<p>Note that the preceding code will look for all faces in the image, but if you only care about one face, then you could change the flag variable as follows:</p>
<pre>
    int flags = CASCADE_FIND_BIGGEST_OBJECT |  
                CASCADE_DO_ROUGH_SEARCH;
</pre>
<p>The <kbd>WebcamFaceRec</kbd> project includes a wrapper around OpenCV's Haar or LBP detector, to make it easier to find a face or eye within an image. For example:</p>
<pre>
    Rect faceRect;    // Stores the result of the detection, or -1. 
    int scaledWidth = 320;     // Shrink the image before detection. 
    detectLargestObject(cameraImg, faceDetector, faceRect, scaledWidth); 
    if (faceRect.width &gt; 0) 
      cout &lt;&lt; "We detected a face!" &lt;&lt; endl;
</pre>
<p>Now that we have a face rectangle, we can use it in many ways, such as to extract or crop the face image from the original image. The following code allows us to access the face:</p>
<pre>
    // Access just the face within the camera image. 
    Mat faceImg = cameraImg(faceRect);
</pre>
<p>The following image shows the typical rectangular region given by the face detector:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="215" width="246" class="image-border" src="assets/image_07_002.jpg"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 2 - face preprocessing</h1>
            </header>

            <article>
                
<p>As mentioned earlier, face recognition is extremely vulnerable to changes in lighting conditions, face orientation, face expression, and so on, so it is very important to reduce these differences as much as possible. Otherwise the face recognition algorithm will often think there is more similarity between faces of two different people in the same conditions than between two faces of the same person.</p>
<p>The easiest form of face preprocessing is just to apply histogram equalization using the <kbd>equalizeHist()</kbd> function, like we just did for face detection. This may be sufficient for some projects where the lighting and positional conditions won't change by much. But for reliability in real-world conditions, we need many sophisticated techniques, including facial feature detection (for example, detecting eyes, nose, mouth, and eyebrows). For simplicity, this chapter will just use eye detection and ignore other facial features such as the mouth and nose, which are less useful. The following image shows an enlarged view of a typical preprocessed face, using the techniques that will be covered in this section.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Eye detection</h1>
            </header>

            <article>
                
<p>Eye detection can be very useful for face preprocessing, because for frontal faces you can always assume a person's eyes should be horizontal and on opposite locations of the face and should have a fairly standard position and size within a face, despite changes in facial expressions, lighting conditions, camera properties, distance to camera, and so on. It is also useful to discard false positives when the face detector says it has detected a face and it is actually something else. It is rare that the face detector and two eye detectors will all be fooled at the same time, so if you only process images with a detected face and two detected eyes then it will not have many false positives (but will also give fewer faces for processing, as the eye detector will not work as often as the face detector).</p>
<p>Some of the pretrained eye detectors that come with OpenCV v2.4 can detect an eye whether it is open or closed, whereas some of them can only detect open eyes.</p>
<p>Eye detectors that detect open or closed eyes are as follows:</p>
<ul>
<li><kbd>haarcascade_mcs_lefteye.xml</kbd> (and <kbd>haarcascade_mcs_righteye.xml</kbd>)</li>
<li><kbd>haarcascade_lefteye_2splits.xml</kbd> (and <kbd>haarcascade_righteye_2splits.xml</kbd>)</li>
</ul>
<p>Eye detectors that detect open eyes only are as follows:</p>
<ul>
<li><kbd>haarcascade_eye.xml</kbd></li>
<li><kbd>haarcascade_eye_tree_eyeglasses.xml</kbd></li>
</ul>
<div class="packt_infobox">As the open or closed eye detectors specify which eye they are trained on, you need to use a different detector for the left and the right eye, whereas the detectors for just open eyes can use the same detector for left or right eyes.<br/>
The detector <kbd>haarcascade_eye_tree_eyeglasses.xml</kbd> can detect the eyes if the person is wearing glasses, but is not reliable if they don't wear glasses.<br/>
If the XML filename says <em>left eye</em>, it means the actual left eye of the person, so in the camera image it would normally appear on the right-hand side of the face, not on the left-hand side!<br/>
The list of four eye detectors mentioned is ranked in approximate order from most reliable to least reliable, so if you know you don't need to find people with glasses then the first detector is probably the best choice.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Eye search regions</h1>
            </header>

            <article>
                
<p>For eye detection, it is important to crop the input image to just show the approximate eye region, just like doing face detection and then cropping to just a small rectangle where the left eye should be (if you are using the left eye detector) and the same for the right rectangle for the right eye detector. If you just do eye detection on a whole face or whole photo then it will be much slower and less reliable. Different eye detectors are better suited to different regions of the face; for example, the <kbd>haarcascade_eye.xml</kbd> detector works best if it only searches in a very tight region around the actual eye, whereas the <kbd>haarcascade_mcs_lefteye.xml</kbd> and <kbd>haarcascade_lefteye_2splits.xml</kbd> detectors work best when there is a large region around the eye.</p>
<p>The following table lists some good search regions of the face for different eye detectors (when using the LBP face detector), using relative coordinates within the detected face rectangle:</p>
<table class="table">
<tbody>
<tr>
<td><strong>Cascade classifier</strong></td>
<td><strong>EYE_SX</strong></td>
<td><strong>EYE_SY</strong></td>
<td><strong>EYE_SW</strong></td>
<td><strong>EYE_SH</strong></td>
</tr>
<tr>
<td><kbd>haarcascade_eye.xml</kbd></td>
<td>0.16</td>
<td>0.26</td>
<td>0.30</td>
<td>0.28</td>
</tr>
<tr>
<td><kbd>haarcascade_mcs_lefteye.xml</kbd></td>
<td>0.10</td>
<td>0.19</td>
<td>0.40</td>
<td>0.36</td>
</tr>
<tr>
<td><kbd>haarcascade_lefteye_2splits.xml</kbd></td>
<td>0.12</td>
<td>0.17</td>
<td>0.37</td>
<td>0.36</td>
</tr>
</tbody>
</table>
<p>Here is the source code to extract the left-eye and right-eye regions from a<br/>
detected face:</p>
<pre>
    int leftX = cvRound(face.cols * EYE_SX); 
    int topY = cvRound(face.rows * EYE_SY); 
    int widthX = cvRound(face.cols * EYE_SW); 
    int heightY = cvRound(face.rows * EYE_SH); 
    int rightX = cvRound(face.cols * (1.0-EYE_SX-EYE_SW)); 

    Mat topLeftOfFace = faceImg(Rect(leftX, topY, widthX, heightY)); 
    Mat topRightOfFace = faceImg(Rect(rightX, topY, widthX, heightY));
</pre>
<p>The following image shows the ideal search regions for the different eye detectors, where the <kbd>haarcascade_eye.xml</kbd> and <kbd>haarcascade_eye_tree_eyeglasses.xml</kbd> files are best with the small search region, while the <kbd>haarcascade_mcs_*eye.xml</kbd> and <kbd>haarcascade_*eye_2splits.xml</kbd> files are best with larger search regions. Note that the detected face rectangle is also shown, to give an idea of how large the eye search regions are compared to the detected face rectangle:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="224" width="256" class="image-border" src="assets/image_07_003.jpg"/></div>
<p>When using the eye search regions given in the preceding table, here are the approximate detection properties of the different eye detectors:</p>
<table class="table">
<tbody>
<tr>
<td><strong>Cascade classifier</strong></td>
<td><strong>Reliability*</strong></td>
<td><strong>Speed**</strong></td>
<td><strong>Eyes found</strong></td>
<td><strong>Glasses</strong></td>
</tr>
<tr>
<td><kbd>haarcascade_mcs_lefteye.xml</kbd></td>
<td>80%</td>
<td>18 msec</td>
<td>Open or closed</td>
<td>no</td>
</tr>
<tr>
<td><kbd>haarcascade_lefteye_2splits.xml</kbd></td>
<td>60%</td>
<td>7 msec</td>
<td>Open or closed</td>
<td>no</td>
</tr>
<tr>
<td><kbd>haarcascade_eye.xml</kbd></td>
<td>40%</td>
<td>5 msec</td>
<td>Open only</td>
<td>no</td>
</tr>
<tr>
<td><kbd>haarcascade_eye_tree_eyeglasses.xml</kbd></td>
<td>15%</td>
<td>10 msec</td>
<td>Open only</td>
<td>yes</td>
</tr>
</tbody>
</table>
<p class="packt_figure"><strong>* Reliability</strong> values show how often both eyes will be detected after LBP frontal face detection when no eyeglasses are worn and both eyes are open. If eyes are closed then the reliability may drop, or if eyeglasses are worn then both reliability and speed will drop.</p>
<p><strong>** Speed</strong> values are in milliseconds for images scaled to the size of 320x240 pixels on an Intel Core i7 2.2 GHz (averaged across 1,000 photos). Speed is typically much faster when eyes are found than when eyes are not found, as it must scan the entire image, but the <kbd>haarcascade_mcs_lefteye.xml</kbd> is still much slower than the other eye detectors.</p>
<p>For example, if you shrink a photo to 320x240 pixels, perform a histogram equalization on it, use the LBP frontal face detector to get a face, then extract the<br/>
<em>left-eye-region</em> and <em>right-eye-region</em> from the face using the <kbd>haarcascade_mcs_lefteye.xml</kbd> values, then perform a histogram equalization on each eye region. Then if you the <kbd>haarcascade_mcs_lefteye.xml</kbd> detector on the left eye (which is actually on the top-right side of your image) and use the <kbd>haarcascade_mcs_righteye.xml</kbd> detector on the right eye (the top-left part of your image), each eye detector should work in roughly 90 percent of photos with LBP-detected frontal faces. So if you want both eyes detected then it should work in roughly 80 percent<br/>
of photos with LBP-detected frontal faces.</p>
<p>Note that while it is recommended to shrink the camera image before detecting faces, you should detect eyes at the full camera resolution because eyes will obviously be much smaller than faces, so you need as much resolution as you can get.</p>
<div class="packt_infobox">Based on the table, it seems that when choosing an eye detector to use, you should decide whether you want to detect closed eyes or only open eyes. And remember that you can even use a one eye detector, and if it does not detect an eye then you can try with another one.<br/>
For many tasks, it is useful to detect eyes whether they are opened or closed, so if speed is not crucial, it is best to search with the <kbd>mcs_*eye</kbd> detector first, and if it fails then search with the <kbd>eye_2splits</kbd> detector.<br/>
But for face recognition, a person will appear quite different if their eyes are closed, so it is best to search with the plain <kbd>haarcascade_eye</kbd> detector first, and if it fails then search with the <kbd>haarcascade_eye_tree_eyeglasses</kbd> detector.</div>
<p>We can use the same <kbd>detectLargestObject()</kbd> function we used for face detection to search for eyes, but instead of asking to shrink the images before eye detection, we specify the full eye region width to get a better eye detection. It is easy to search for the left eye using one detector, and if it fails then try another detector (same for right eye). The eye detection is done as follows:</p>
<pre>
    CascadeClassifier eyeDetector1("haarcascade_eye.xml"); 
    CascadeClassifier eyeDetector2("haarcascade_eye_tree_eyeglasses.xml"); 
    ... 
    Rect leftEyeRect;    // Stores the detected eye. 
    // Search the left region using the 1st eye detector. 
    detectLargestObject(topLeftOfFace, eyeDetector1, leftEyeRect, 
    topLeftOfFace.cols); 
    // If it failed, search the left region using the 2nd eye  
    // detector. 
    if (leftEyeRect.width &lt;= 0) 
      detectLargestObject(topLeftOfFace, eyeDetector2,  
                leftEyeRect, topLeftOfFace.cols); 
    // Get the left eye center if one of the eye detectors worked. 
    Point leftEye = Point(-1,-1); 
    if (leftEyeRect.width &lt;= 0) { 
      leftEye.x = leftEyeRect.x + leftEyeRect.width/2 + leftX; 
      leftEye.y = leftEyeRect.y + leftEyeRect.height/2 + topY; 
    } 

    // Do the same for the right-eye 
    ... 

    // Check if both eyes were detected. 
    if (leftEye.x &gt;= 0 &amp;&amp; rightEye.x &gt;= 0) { 
      ... 
    }
</pre>
<p>With the face and both eyes detected, we'll perform face preprocessing by combining:</p>
<ul>
<li><strong>Geometrical transformation and cropping</strong>: This process would include scaling, rotating, and translating the images so that the eyes are aligned, followed by the removal of the forehead, chin, ears, and background from the face image.</li>
<li><strong>Separate histogram equalization for left and right sides</strong>: This process standardizes the brightness and contrast on both the left- and right-hand sides of the face independently.</li>
<li><strong>Smoothing</strong>: This process reduces the image noise using a bilateral filter.</li>
<li><strong>Elliptical mask</strong>: The elliptical mask removes some remaining hair and background from the face image.</li>
</ul>
<p>The following image shows the face preprocessing steps 1 to 4 applied to a detected face. Notice how the final image has good brightness and contrast on both sides of the face, whereas the original does not:</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Geometrical transformation</h1>
            </header>

            <article>
                
<p>It is important that the faces are all aligned together, otherwise the face-recognition algorithm might be comparing part of a nose with part of an eye, and so on. The output of face detection just seen will give aligned faces to some extent, but it is not very accurate (that is, the face rectangle will not always be starting from the same point on the forehead).</p>
<p>To have better alignment, we will use eye detection to align the face so the positions of the two detected eyes line up perfectly in the desired positions. We will do the geometrical transformation using the <kbd>warpAffine()</kbd> function, which is a single operation that will do four things:</p>
<ul>
<li>Rotate the face so that the two eyes are horizontal</li>
<li>Scale the face so that the distance between the two eyes is always the same</li>
<li>Translate the face so that the eyes are always centered horizontally and at a desired height</li>
<li>Crop the outer parts of the face, since we want to crop away the image background, hair, forehead, ears, and chin</li>
</ul>
<p>Affine Warping takes an affine matrix that transforms the two detected eye locations to the two desired eye locations, and then crops to a desired size and position. To generate this affine matrix, we will get the center between the eyes, calculate the angle at which the two detected eyes appear, and look at their distance apart as follows:</p>
<pre>
    // Get the center between the 2 eyes. 
    Point2f eyesCenter; 
    eyesCenter.x = (leftEye.x + rightEye.x) * 0.5f; 
    eyesCenter.y = (leftEye.y + rightEye.y) * 0.5f; 

    // Get the angle between the 2 eyes. 
    double dy = (rightEye.y - leftEye.y); 
    double dx = (rightEye.x - leftEye.x); 
    double len = sqrt(dx*dx + dy*dy); 

    // Convert Radians to Degrees. 
    double angle = atan2(dy, dx) * 180.0/CV_PI; 

    // Hand measurements shown that the left eye center should  
    // ideally be roughly at (0.16, 0.14) of a scaled face image. 
    const double DESIRED_LEFT_EYE_X = 0.16; 
    const double DESIRED_RIGHT_EYE_X = (1.0f - 0.16); 

    // Get the amount we need to scale the image to be the desired 
    // fixed size we want. 
    const int DESIRED_FACE_WIDTH = 70; 
    const int DESIRED_FACE_HEIGHT = 70; 
    double desiredLen = (DESIRED_RIGHT_EYE_X - 0.16); 
    double scale = desiredLen * DESIRED_FACE_WIDTH / len;
</pre>
<p>Now we can transform the face (rotate, scale, and translate) to get the two detected eyes to be in the desired eye positions in an ideal face as follows:</p>
<pre>
    // Get the transformation matrix for the desired angle &amp; size. 
    Mat rot_mat = getRotationMatrix2D(eyesCenter, angle, scale); 
    // Shift the center of the eyes to be the desired center. 
    double ex = DESIRED_FACE_WIDTH * 0.5f - eyesCenter.x; 
    double ey = DESIRED_FACE_HEIGHT * DESIRED_LEFT_EYE_Y -  
      eyesCenter.y; 
    rot_mat.at&lt;double&gt;(0, 2) += ex; 
    rot_mat.at&lt;double&gt;(1, 2) += ey; 
    // Transform the face image to the desired angle &amp; size &amp; 
    // position! Also clear the transformed image background to a  
    // default grey. 
    Mat warped = Mat(DESIRED_FACE_HEIGHT, DESIRED_FACE_WIDTH, 
      CV_8U, Scalar(128)); 
    warpAffine(gray, warped, rot_mat, warped.size());
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Separate histogram equalization for left and right sides</h1>
            </header>

            <article>
                
<p>In real-world conditions, it is common to have strong lighting on one half of the face and weak lighting on the other. This has an enormous effect on the face-recognition algorithm, as the left- and right-hand sides of the same face will seem like very different people. So we will perform histogram equalization separately on the left and right halves of the face, to have standardized brightness and contrast on each side of the face.</p>
<p>If we simply applied histogram equalization on the left half and then again on the right half, we would see a very distinct edge in the middle because the average brightness is likely to be different on the left and the right side, so to remove this edge, we will apply the two histogram equalizations gradually from the left-or right-hand side towards the center and mix it with a whole-face histogram equalization, so that the far left-hand side will use the left histogram equalization, the far right-hand side will use the right histogram equalization, and the center will use a smooth mix of the left or right value and the whole-face equalized value.</p>
<p>The following image shows how the left-equalized, whole-equalized, and right-equalized images are blended together:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="300" width="338" class="image-border" src="assets/a7829_7_6.png"/></div>
<p>To perform this, we need copies of the whole face equalized as well as the left half equalized and the right half equalized, which is done as follows:</p>
<pre>
    int w = faceImg.cols; 
    int h = faceImg.rows; 
    Mat wholeFace; 
    equalizeHist(faceImg, wholeFace); 
    int midX = w/2; 
    Mat leftSide = faceImg(Rect(0,0, midX,h)); 
    Mat rightSide = faceImg(Rect(midX,0, w-midX,h)); 
    equalizeHist(leftSide, leftSide); 
    equalizeHist(rightSide, rightSide);
</pre>
<p>Now we combine the three images together. As the images are small, we can easily access pixels directly using the <kbd>image.at&lt;uchar&gt;(y,x)</kbd> function even if it is slow; so let's merge the three images by directly accessing pixels in the three input images and output images, as follows:</p>
<pre>
    for (int y=0; y&lt;h; y++) { 
      for (int x=0; x&lt;w; x++) { 
        int v; 
        if (x &lt; w/4) { 
          // Left 25%: just use the left face. 
          v = leftSide.at&lt;uchar&gt;(y,x); 
        } 
        else if (x &lt; w*2/4) { 
          // Mid-left 25%: blend the left face &amp; whole face. 
          int lv = leftSide.at&lt;uchar&gt;(y,x); 
          int wv = wholeFace.at&lt;uchar&gt;(y,x); 
          // Blend more of the whole face as it moves 
          // further right along the face. 
          float f = (x - w*1/4) / (float)(w/4); 
          v = cvRound((1.0f - f) * lv + (f) * wv); 
        } 
        else if (x &lt; w*3/4) { 
          // Mid-right 25%: blend right face &amp; whole face. 
          int rv = rightSide.at&lt;uchar&gt;(y,x-midX); 
          int wv = wholeFace.at&lt;uchar&gt;(y,x); 
          // Blend more of the right-side face as it moves 
          // further right along the face. 
          float f = (x - w*2/4) / (float)(w/4); 
          v = cvRound((1.0f - f) * wv + (f) * rv); 
        } 
        else { 
          // Right 25%: just use the right face. 
          v = rightSide.at&lt;uchar&gt;(y,x-midX); 
        } 
        faceImg.at&lt;uchar&gt;(y,x) = v; 
      } // end x loop 
    } //end y loop
</pre>
<p>This separated histogram equalization should significantly help reduce the effect of different lighting on the left- and right-hand sides of the face, but we must understand that it won't completely remove the effect of one-sided lighting, since the face is a complex 3D shape with many shadows.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Smoothing</h1>
            </header>

            <article>
                
<p>To reduce the effect of pixel noise, we will use a bilateral filter on the face, as a bilateral filter is very good at smoothing, most of an image while keeping edges sharp. Histogram equalization can significantly increase the pixel noise, so we will make the filter strength <kbd>20</kbd> to cover heavy pixel noise, but use a neighborhood of just two pixels as we want to heavily smooth the tiny pixel noise but not the large image regions, as follows:</p>
<pre>
    Mat filtered = Mat(warped.size(), CV_8U); 
    bilateralFilter(warped, filtered, 0, 20.0, 2.0);
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Elliptical mask</h1>
            </header>

            <article>
                
<p>Although we have already removed most of the image background and forehead and hair when we did the geometrical transformation, we can apply an elliptical mask to remove some of the corner region such as the neck, which might be in shadow from the face, particularly if the face is not looking perfectly straight towards the camera. To create the mask, we will draw a black-filled ellipse onto a white image. One ellipse to perform this has a horizontal radius of 0.5 (that is, it covers the face width perfectly), a vertical radius of 0.8 (as faces are usually taller than they are wide), and centered at the coordinates 0.5, 0.4, as shown in the following image, where the elliptical mask has removed some unwanted corners from the face:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="image-border" src="assets/a7829_7_7.png"/></div>
<p>We can apply the mask when calling the <kbd>cv::setTo()</kbd> function, which would normally set a whole image to a certain pixel value, but as we will give a mask image, it will only set some parts to the given pixel value. We will fill the image in gray so that it should have less contrast to the rest of the face:</p>
<pre>
    // Draw a black-filled ellipse in the middle of the image. 
    // First we initialize the mask image to white (255). 
    Mat mask = Mat(warped.size(), CV_8UC1, Scalar(255)); 
    double dw = DESIRED_FACE_WIDTH; 
    double dh = DESIRED_FACE_HEIGHT; 
    Point faceCenter = Point( cvRound(dw * 0.5), 
      cvRound(dh * 0.4) ); 
    Size size = Size( cvRound(dw * 0.5), cvRound(dh * 0.8) ); 
    ellipse(mask, faceCenter, size, 0, 0, 360, Scalar(0),  
      CV_FILLED); 

    // Apply the elliptical mask on the face, to remove corners. 
    // Sets corners to gray, without touching the inner face. 
    filtered.setTo(Scalar(128), mask);
</pre>
<p>The following enlarged image shows a sample result from all the face preprocessing stages. Notice it is much more consistent for face recognition at a different brightness, face rotations, angle from camera, backgrounds, positions of lights, and so on. This preprocessed face will be used as input to the face-recognition stages, both when collecting faces for training, and when trying to recognize input faces:</p>
<div class="CDPAlignCenter CDPAlign"><img height="197" width="197" class="image-border" src="assets/a7829_7_8.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 3 - Collecting faces and learning from them</h1>
            </header>

            <article>
                
<p>Collecting faces can be just as simple as putting each newly preprocessed face into an array of preprocessed faces from the camera, as well as putting a label into an array (to specify which person the face was taken from). For example, you could use 10 preprocessed faces of the first person and 10 preprocessed faces of a second person, so the input to the face-recognition algorithm will be an array of 20 preprocessed faces and an array of 20 integers (where the first 10 numbers are 0 and the next 10 numbers are 1).</p>
<p>The face-recognition algorithm will then learn how to distinguish between the faces of the different people. This is referred to as the training phase and the collected faces are referred to as the training set. After the face-recognition algorithm has finished training, you could then save the generated knowledge to a file or memory and later use it to recognize which person is seen in front of the camera. This is referred to as the testing phase. If you used it directly from a camera input then the preprocessed face would be referred to as the test image, and if you tested with many images (such as from a folder of image files), it would be referred to as the testing set.</p>
<p>It is important that you provide a good training set that covers the types of variations you expect to occur in your testing set. For example, if you will only test with faces that are looking perfectly straight ahead (such as ID photos), then you only need to provide training images with faces that are looking perfectly straight ahead. But if the person might be looking to the left or up, then you should make sure the training set will also include faces of that person doing this, otherwise the face-recognition algorithm will have trouble recognizing them, as their face will appear quite different. This also applies to other factors such as facial expression (for example, if the person is always smiling in the training set but not smiling in the testing set) or lighting direction (for example, a strong light is to the left-hand side in the training set but to the right-hand side in the testing set), then the face recognition algorithm will have difficulty recognizing them. The face preprocessing steps that we just saw will help reduce these issues, but it certainly won't remove these factors, particularly the direction in which the face is looking, as it has a large effect on the position of all elements in the face.</p>
<div class="packt_infobox">One way to obtain a good training set that will cover many different real-world conditions is for each person to rotate their head from looking left, to up, to right, to down, then looking directly straight. Then the person tilts their head sideways and then up and down, while also changing their facial expression, such as alternating between smiling, looking angry, and having a neutral face. If each person follows a routine such as this while collecting faces, then there is a much better chance of recognizing everyone in the real-world conditions.<br/>
For even better results, it should be performed again with one or two more locations or directions, such as by turning the camera around by 180 degrees and walking in the opposite direction of the camera and then repeating the whole routine, so that the training set would include many different lighting conditions.</div>
<p>So in general, having 100 training faces for each person is likely to give better results than having just 10 training faces for each person, but if all 100 faces look almost identical then it will still perform badly because it is more important that the training set has enough variety to cover the testing set, rather than to just have a large number of faces. So to make sure the faces in the training set are not all too similar, we should add a noticeable delay between each collected face. For example, if the camera is running at 30 frames per second, then it might collect 100 faces in just several seconds when the person has not had time to move around, so it is better to collect just one face per second, while the person moves their face around. Another simple method to improve the variation in the training set is to only collect a face if it is noticeably different from the previously collected face.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Collecting preprocessed faces for training</h1>
            </header>

            <article>
                
<p>To make sure there is at least a 1 second gap between collecting new faces, we need to measure how much time has passed. This is done as follows:</p>
<pre>
    // Check how long since the previous face was added. 
    double current_time = (double)getTickCount(); 
    double timeDiff_seconds = (current_time - 
      old_time) / getTickFrequency();
</pre>
<p>To compare the similarity of two images, pixel by pixel, you can find the relative L2 error, which just involves subtracting one image from the other, summing the squared value of it, and then getting the square root of it. So if the person had not moved at all, subtracting the current face with the previous face should give a very low number at each pixel, but if they had just moved slightly in any direction, subtracting the pixels would give a large number and so the L2 error will be high. As the result is summed over all pixels, the value will depend on the image resolution. So to get the mean error, we should divide this value by the total number of pixels in the image. Let's put this in a handy function, <kbd>getSimilarity()</kbd>, as follows:</p>
<pre>
    double getSimilarity(const Mat A, const Mat B) { 
      // Calculate the L2 relative error between the 2 images. 
      double errorL2 = norm(A, B, CV_L2); 
      // Scale the value since L2 is summed across all pixels. 
      double similarity = errorL2 / (double)(A.rows * A.cols); 
      return similarity; 
    } 

    ... 

    // Check if this face looks different from the previous face. 
    double imageDiff = MAX_DBL; 
    if (old_prepreprocessedFaceprepreprocessedFace.data) { 
      imageDiff = getSimilarity(preprocessedFace, 
        old_prepreprocessedFace); 
    }
</pre>
<p>This similarity will often be less than 0.2 if the image did not move much, and<br/>
higher than 0.4 if the image did move, so let's use 0.3 as our threshold for collecting<br/>
a new face.</p>
<p>There are many tricks we can play to obtain more training data, such as using mirrored faces, adding random noise, shifting the face by a few pixels, scaling the face by a percentage, or rotating the face by a few degrees (even though we specifically tried to remove these effects when preprocessing the face!). Let's add mirrored faces to the training set, so that we have both a larger training set as well as a reduction in the problems of asymmetrical faces or if a user is always oriented slightly to the left or right during training but not testing. This is done as follows:</p>
<pre>
    // Only process the face if it's noticeably different from the 
    // previous frame and there has been a noticeable time gap. 
    if ((imageDiff &gt; 0.3) &amp;&amp; (timeDiff_seconds &gt; 1.0)) { 
      // Also add the mirror image to the training set. 
      Mat mirroredFace; 
      flip(preprocessedFace, mirroredFace, 1); 

      // Add the face &amp; mirrored face to the detected face lists. 
      preprocessedFaces.push_back(preprocessedFace); 
      preprocessedFaces.push_back(mirroredFace); 
      faceLabels.push_back(m_selectedPerson); 
      faceLabels.push_back(m_selectedPerson); 

      // Keep a copy of the processed face, 
      // to compare on next iteration. 
      old_prepreprocessedFace = preprocessedFace; 
      old_time = current_time; 
    }
</pre>
<p>This will collect the <kbd>std::vector</kbd> arrays <kbd>preprocessedFaces</kbd> and <kbd>faceLabels</kbd> for a preprocessed face as well as the label or ID number of that person (assuming it is in the integer <kbd>m_selectedPerson</kbd> variable).</p>
<p>To make it more obvious to the user that we have added their current face to the collection, you could provide a visual notification by either displaying a large white rectangle over the whole image or just displaying their face for just a fraction of a second so they realize a photo was taken. With OpenCV's C++ interface, you can use the <kbd>+</kbd> overloaded <kbd>cv::Mat</kbd> operator to add a value to every pixel in the image and have it clipped to 255 (using <kbd>saturate_cast</kbd>, so it doesn't overflow from white back to black!) Assuming <kbd>displayedFrame</kbd> will be a copy of the color camera frame that should be shown, insert this after the preceding code for face collection:</p>
<pre>
    // Get access to the face region-of-interest. 
    Mat displayedFaceRegion = displayedFrame(faceRect); 
    // Add some brightness to each pixel of the face region. 
    displayedFaceRegion += CV_RGB(90,90,90);
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training the face recognition system from collected faces</h1>
            </header>

            <article>
                
<p>After you have collected enough faces for each person to recognize, you must train the system to learn the data using a machine-learning algorithm suited for face recognition. There are many different face-recognition algorithms in the literature, the simplest of which are Eigenfaces and Artificial Neural Networks. Eigenfaces tends to work better than ANNs, and despite its simplicity, it tends to work almost as well as many more complex face-recognition algorithms, so it has become very popular as the basic face-recognition algorithm for beginners as well as for new algorithms to be compared to.</p>
<p>Any reader who wishes to work further on face recognition is recommended to read the theory behind:</p>
<ul>
<li>Eigenfaces (also referred to as <strong>Principal Component Analysis</strong> (<strong>PCA</strong>)</li>
<li>Fisherfaces (also referred to as <strong>Linear Discriminant Analysis</strong> (<strong>LDA</strong>)</li>
<li>Other classic face recognition algorithms (many are available at <a href="http://www.face-rec.org/algorithms/" target="_blank">h t t p ://w w w . f a c e - r e c . o r g /a l g o r i t h m s /</a>)</li>
<li>Newer face recognition algorithms in recent Computer Vision research papers (such as CVPR and ICCV at <a href="http://www.cvpapers.com/" target="_blank"><span class="URLPACKT">http://www.cvpapers.c</span>om/</a>), as there are hundreds of face recognition papers published each year</li>
</ul>
<p>However, you don't need to understand the theory of these algorithms in order to use them as shown in this book. Thanks to the OpenCV team and Philipp Wagner's <kbd>libfacerec</kbd> contribution, OpenCV v2.4.1 provided <kbd>cv::Algo</kbd><kbd>rithm</kbd> as a simple and generic method to perform face recognition using one of several different algorithms (even selectable at runtime) without necessarily understanding how they are implemented. You can find the available algorithms in your version of OpenCV by using the <kbd>Algorithm::getList()</kbd> function, such as with this code:</p>
<pre>
    vector&lt;string&gt; algorithms; 
    Algorithm::getList(algorithms); 
    cout &lt;&lt; "Algorithms: " &lt;&lt; algorithms.size() &lt;&lt; endl; 
    for (int i=0; i&lt;algorithms.size(); i++) { 
      cout &lt;&lt; algorithms[i] &lt;&lt; endl; 
    }
</pre>
<p>Here are the three face-recognition algorithms available in OpenCV v2.4.1:</p>
<ul>
<li><kbd>FaceRecognizer.Eigenfaces</kbd>: Eigenfaces, also referred to as PCA, first used by Turk and Pentland in 1991.</li>
<li><kbd>FaceRecognizer.Fisherfaces</kbd>: Fisherfaces, also referred to as LDA, invented by Belhumeur, Hespanha, and Kriegman in 1997.</li>
<li><kbd>FaceRecognizer.LBPH</kbd>: Local Binary Pattern Histograms, invented by Ahonen, Hadid, and Pietikäinen in 2004.</li>
</ul>
<div class="packt_infobox">More information on these face-recognition algorithm implementations can be found with documentation, samples, and Python equivalents for each of them on Philipp Wagner's websites (<a href="http://bytefish.de/blog" target="_blank"><span class="URLPACKT">http://bytefish.d</span>e/blog</a> and <a href="http://bytefish.de/dev/libfacerec/" target="_blank"><span class="URLPACKT">http://bytefish.de/dev/libfa</span>cerec/</a>).</div>
<p>These face recognition-algorithms are available through the <kbd>FaceRecognizer</kbd> class in OpenCV's <kbd>contrib</kbd> module. Due to dynamic linking, it is possible that your program is linked to the <kbd>contrib</kbd> module but it is not actually loaded at runtime (if it was deemed as not required). So it is recommended to call the <kbd>cv::initModule_contrib()</kbd> function before trying to access the <kbd>FaceRecognizer</kbd> algorithms. This function is only available from OpenCV v2.4.1, so it also ensures that the face-recognition algorithms are at least available to you at compile time:</p>
<pre>
    // Load the "contrib" module is dynamically at runtime. 
    bool haveContribModule = initModule_contrib(); 
    if (!haveContribModule) { 
      cerr &lt;&lt; "ERROR: The 'contrib' module is needed for "; 
      cerr &lt;&lt; "FaceRecognizer but hasn't been loaded to OpenCV!"; 
      cerr &lt;&lt; endl; 
      exit(1); 
    }
</pre>
<p>To use one of the face-recognition algorithms, we must create a <kbd>FaceRecognizer</kbd> object using the <kbd>cv::Algorithm::create&lt;FaceRecognizer&gt;()</kbd> function. We pass the name of the face-recognition algorithm we want to use, as a string to this create function. This will give us access to that algorithm, if it is available in the OpenCV version. So it may be used as a runtime error check to ensure the user has OpenCV v2.4.1 or newer. For example:</p>
<pre>
    string facerecAlgorithm = "FaceRecognizer.Fisherfaces"; 
    Ptr&lt;FaceRecognizer&gt; model; 
    // Use OpenCV's new FaceRecognizer in the "contrib" module: 
    model = Algorithm::create&lt;FaceRecognizer&gt;(facerecAlgorithm); 
    if (model.empty()) { 
      cerr &lt;&lt; "ERROR: The FaceRecognizer [" &lt;&lt; facerecAlgorithm; 
      cerr &lt;&lt; "] is not available in your version of OpenCV. "; 
      cerr &lt;&lt; "Please update to OpenCV v2.4.1 or newer." &lt;&lt; endl; 
      exit(1); 
    }
</pre>
<p>Once we have loaded the <kbd>FaceRecognizer</kbd> algorithm, we simply call the <kbd>FaceRecognizer::train()</kbd> function with our collected face data as follows:</p>
<pre>
    // Do the actual training from the collected faces. 
    model-&gt;train(preprocessedFaces, faceLabels);
</pre>
<p>This one line of code will run the whole face recognition training algorithm that you selected (for example, Eigenfaces, Fisherfaces, or potentially other algorithms). If you have just a few people with less than 20 faces, then this training should return very quickly, but if you have many people with many faces, it is possible that <kbd>train()</kbd> function will take several seconds or even minutes to process all the data. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Viewing the learned knowledge</h1>
            </header>

            <article>
                
<p>While it is not necessary, it is quite useful to view the internal data structures that the face-recognition algorithm generated when learning your training data, particularly if you understand the theory behind the algorithm you selected and want to verify if it worked or find out why it is not working as you hoped. The internal data structures can be different for different algorithms, but luckily they are the same for eigenfaces and fisherfaces, so let's just look at those two. They are both based on 1D eigenvector matrices that appear somewhat like faces when viewed as 2D images, therefore it is common to refer as eigenvectors as eigenfaces when using the <strong>Eigenface</strong> algorithm or as fisherfaces when using the <strong>Fisherface</strong> algorithm.</p>
<p>In simple terms, the basic principle of Eigenfaces is that it will calculate a set of special images (eigenfaces) and blending ratios (eigenvalues), which when combined in different ways can generate each of the images in the training set but can also be used to differentiate the many face images in the training set from each other. For example, if some of the faces in the training set had a moustache and some did not, then there would be at least one eigenface that shows a moustache, and so the training faces with a moustache would have a high blending ratio for that eigenface to show that it has a moustache, and the faces without a moustache would have a low blending ratio for that eigenvector. If the training set had five people with 20 faces for each person, then there would be 100 eigenfaces and eigenvalues to differentiate the 100 total faces in the training set, and in fact these would be sorted so the first few eigenfaces and eigenvalues would be the most critical differentiators, and the last few eigenfaces and eigenvalues would just be random pixel noises that don't actually help to differentiate the data. So it is common practice to discard some of the last eigenfaces and just keep the first 50 or so eigenfaces.</p>
<p>In comparison, the basic principle of Fisherfaces is that instead of calculating a special eigenvector and eigenvalue for each image in the training set, it only calculates one special eigenvector and eigenvalue for each person. So in the preceding example that has fivepeople with 20 faces for each person, the Eigenfaces algorithm would use 100 eigenfaces and eigenvalues whereas the Fisherfaces algorithm would use just five fisherfaces and eigenvalues.</p>
<p>To access the internal data structures of the Eigenfaces and Fisherfaces algorithms, we must use the <kbd>cv::Algorithm::get()</kbd> function to obtain them at runtime, as there is no access to them at compile time. The data structures are used internally as part of mathematical calculations rather than for image processing, so they are usually stored as floating-point numbers typically ranging between 0.0 and 1.0, rather than 8-bit <kbd>uchar</kbd> pixels ranging from 0 to 255, similar to pixels in regular images. Also, they are often either a 1D row or column matrix or they make up one of the many 1D rows or columns of a larger matrix. So before you can display many of these internal data structures, you must reshape them to be the correct rectangular shape, and convert them to 8-bit <kbd>uchar</kbd> pixels between 0 and 255. As the matrix data might range from 0.0 to 1.0 or -1.0 to 1.0 or anything else, you can use the <kbd>cv::normalize()</kbd> function with the <kbd>cv::NORM_MINMAX</kbd> option to make sure it outputs data ranging between 0 and 255 no matter what the input range may be. Let's create a function to perform this reshaping to a rectangle and conversion to 8-bit pixels for us as follows:</p>
<pre>
    // Convert the matrix row or column (float matrix) to a 
    // rectangular 8-bit image that can be displayed or saved. 
    // Scales the values to be between 0 to 255. 
    Mat getImageFrom1DFloatMat(const Mat matrixRow, int height) 
    { 
      // Make a rectangular shaped image instead of a single row. 
      Mat rectangularMat = matrixRow.reshape(1, height); 
      // Scale the values to be between 0 to 255 and store them  
      // as a regular 8-bit uchar image. 
      Mat dst; 
      normalize(rectangularMat, dst, 0, 255, NORM_MINMAX,  
        CV_8UC1); 
      return dst; 
    }
</pre>
<p>To make it easier to debug OpenCV code and even more so, when internally debugging the <kbd>cv::Algorithm</kbd> data structure, we can use the <kbd>ImageUtils.cpp</kbd> and <kbd>ImageUtils.h</kbd> files to display information about a <kbd>cv::Mat</kbd> structure easily as follows:</p>
<pre>
    Mat img = ...; 
    printMatInfo(img, "My Image");
</pre>
<p>You will see something similar to the following printed to your console:</p>
<pre>
<strong>My Image: 640w480h 3ch 8bpp, range[79,253][20,58][18,87]</strong>
</pre>
<p>This tells you that it is 640 elements wide and 480 high (that is, a 640 x 480 image or a 480 x 640 matrix, depending on how you view it), with three channels per pixel that are 8-bits each (that is, a regular BGR image), and it shows the min and max value in the image for each of the color channels.</p>
<div class="packt_infobox">It is also possible to print the actual contents of an image or matrix by using the <kbd>printMat()</kbd> function instead of the <kbd>printMatInfo()</kbd> function. This is quite handy for viewing matrices and multichannel-float matrices as these can be quite tricky to view for beginners.<br/>
The <kbd>ImageUtils</kbd> code is mostly for OpenCV's C interface, but is gradually including more of the C++ interface over time. The most recent version can be found at <a href="http://shervinemami.info/openCV.html" target="_blank"><span class="URLPACKT">http://shervinemami.</span>info/openCV.html</a>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Average face</h1>
            </header>

            <article>
                
<p>Both the Eigenfaces and Fisherfaces algorithms first calculate the average face that is the mathematical average of all the training images, so they can subtract the average image from each facial image to have better face recognition results. So let's view the average face from our training set. The average face is named <kbd>mean</kbd> in the Eigenfaces and Fisherfaces implementations, shown as follows:</p>
<pre>
    Mat averageFace = model-&gt;get&lt;Mat&gt;("mean"); 
    printMatInfo(averageFace, "averageFace (row)"); 
    // Convert a 1D float row matrix to a regular 8-bit image. 
    averageFace = getImageFrom1DFloatMat(averageFace, faceHeight); 
    printMatInfo(averageFace, "averageFace"); 
    imshow("averageFace", averageFace);
</pre>
<p>You should now see an average face image on your screen similar to the following (enlarged) image that is a combination of a man, a woman, and a baby. You should also see similar text to this shown on your console:</p>
<pre>
<strong>    averageFace (row): 4900w1h 1ch 64bpp, range[5.21,251.47]</strong>
<strong>    averageFace: 70w70h 1ch 8bpp, range[0,255]</strong>
</pre>
<p>The image would appear as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="206" width="206" class="image-border" src="assets/a7829_7_9.png"/></div>
<p>Notice that <kbd>averageFace (row)</kbd> was a single row matrix of 64-bit floats, whereas <kbd>averageFace</kbd> is a rectangular image with 8-bit pixels covering the full range<br/>
from 0 to 255.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Eigenvalues, Eigenfaces, and Fisherfaces</h1>
            </header>

            <article>
                
<p>Let's view the actual component values in the eigenvalues (as text):</p>
<pre>
    Mat eigenvalues = model-&gt;get&lt;Mat&gt;("eigenvalues"); 
    printMat(eigenvalues, "eigenvalues");
</pre>
<p>For Eigenfaces, there is one eigenvalue for each face, so if we have three people with four faces each, we get a column vector with 12 eigenvalues sorted from best to worst as follows:</p>
<pre>
    eigenvalues: 1w18h 1ch 64bpp, range[4.52e+04,2.02836e+06] 
    2.03e+06  
    1.09e+06 
    5.23e+05 
    4.04e+05 
    2.66e+05 
    2.31e+05 
    1.85e+05 
    1.23e+05 
    9.18e+04 
    7.61e+04  
    6.91e+04 
    4.52e+04
</pre>
<p>For Fisherfaces, there is just one eigenvalue for each extra person, so if there are three people with four faces each, we just get a row vector with two eigenvalues as follows:</p>
<pre>
    eigenvalues: 2w1h 1ch 64bpp, range[152.4,316.6] 
    317, 152
</pre>
<p>To view the eigenvectors (as Eigenface or Fisherface images), we must extract them as columns from the big eigenvectors matrix. As data in OpenCV and C/C++ is normally stored in matrices using row-major order, it means that to extract a column, we should use the <kbd>Mat::clone()</kbd> function to ensure the data will be continuous, otherwise we can't reshape the data to a rectangle. Once we have a continuous column <kbd>Mat</kbd>, we can display the eigenvectors using the <kbd>getImageFrom1DFloatMat()</kbd> function just like we did for the average face:</p>
<pre>
    // Get the eigenvectors 
    Mat eigenvectors = model-&gt;get&lt;Mat&gt;("eigenvectors"); 
    printMatInfo(eigenvectors, "eigenvectors"); 

    // Show the best 20 eigenfaces 
    for (int i = 0; i &lt; min(20, eigenvectors.cols); i++) { 
      // Create a continuous column vector from eigenvector #i. 
      Mat eigenvector = eigenvectors.col(i).clone(); 

      Mat eigenface = getImageFrom1DFloatMat(eigenvector, 
        faceHeight); 
      imshow(format("Eigenface%d", i), eigenface); 
    }
</pre>
<p>The following figure displays eigenvectors as images. You can see that for three people with four faces, there are 12 Eigenfaces (left-hand side of the figure) or two Fisherfaces (right-hand side):</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="333" width="525" class="image-border" src="assets/a7829_7_10.png"/></div>
<p>Notice that both Eigenfaces and Fisherfaces seem to have the resemblance of some facial features but they don't really look like faces. This is simply because the average face was subtracted from them, so they just show the differences for each Eigenface from the average face. The numbering shows which Eigenface it is, because they are always ordered from the most significant Eigenface to the least significant Eigenface, and if you have 50 or more Eigenfaces then the later Eigenfaces will often just show random image noise and therefore should be discarded.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Step 4 - face recognition</h1>
            </header>

            <article>
                
<p>Now that we have trained the Eigenfaces or Fisherfaces machine-learning algorithm with our set of training images and face labels, we are finally ready to figure out who a person is, just from a facial image! This last step is referred to as face recognition or face identification.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Face identification - recognizing people from their face</h1>
            </header>

            <article>
                
<p>Thanks to OpenCV's <kbd>FaceRecognizer</kbd> class, we can identify the person in a photo simply by calling the <kbd>FaceRecognizer::predict()</kbd> function on a facial image<br/>
as follows:</p>
<pre>
    int identity = model-&gt;predict(preprocessedFace);
</pre>
<p>This <kbd>identity</kbd> value will be the label number that we originally used when collecting faces for training. For example, 0 for the first person, 1 for the second person, and so on.</p>
<p>The problem with this identification is that it will always predict one of the given people, even if the input photo is of an unknown person or of a car. It would still tell you which person is the most likely person in that photo, so it can be difficult to trust the result! The solution is to obtain a confidence metric so we can judge how reliable the result is, and if it seems that the confidence is too low then we assume it is an unknown person.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Face verification - validating that it is the claimed person</h1>
            </header>

            <article>
                
<p>To confirm if the result of the prediction is reliable or whether it should be taken as an unknown person, we perform <strong>face verification</strong> (also referred to as <strong>face authentication</strong>), to obtain a confidence metric showing whether the single face image is similar to the claimed person (as opposed to face identification, which we just performed, comparing the single face image with many people).</p>
<p>OpenCV's <kbd>FaceRecognizer</kbd> class can return a confidence metric when you call the <kbd>predict()</kbd> function but unfortunately the confidence metric is simply based on the distance in eigen-subspace, so it is not very reliable. The method we will use is to reconstruct the facial image using the <em>eigenvectors</em> and <em>eigenvalues</em>, and compare this reconstructed image with the input image. If the person had many of their faces included in the training set, then the reconstruction should work quite well from the learned eigenvectors and eigenvalues, but if the person did not have any faces in the training set (or did not have any that have similar lighting and facial expressions as the test image), then the reconstructed face will look very different from the input face, signaling that it is probably an unknown face.</p>
<p>Remember we said earlier that the Eigenfaces and Fisherfaces algorithms are based on the notion that an image can be roughly represented as a set of eigenvectors (special face images) and eigenvalues (blending ratios). So if we combine all the eigenvectors with the eigenvalues from one of the faces in the training set then we should obtain a fairly close replica of that original training image. The same applies with other images that are similar to the training set--if we combine the trained eigenvectors with the eigenvalues from a similar test image, we should be able to reconstruct an image that is somewhat a replica to the test image.</p>
<p>Once again, OpenCV's <kbd>FaceRecognizer</kbd> class makes it quite easy to generate a reconstructed face from any input image, by using the <kbd>subspaceProject()</kbd> function to project onto the eigenspace and the <kbd>subspaceReconstruct()</kbd> function to go back from eigenspace to image space. The trick is that we need to convert it from a floating-point row matrix to a rectangular 8-bit image (like we did when displaying the average face and eigenfaces), but we don't want to normalize the data, as it is already in the ideal scale to compare with the original image. If we normalized the data, it would have a different brightness and contrast from the input image, and it would become difficult to compare the image similarity just by using the L2 relative error. This is done as follows:</p>
<pre>
    // Get some required data from the FaceRecognizer model. 
    Mat eigenvectors = model-&gt;get&lt;Mat&gt;("eigenvectors"); 
    Mat averageFaceRow = model-&gt;get&lt;Mat&gt;("mean"); 

    // Project the input image onto the eigenspace. 
    Mat projection = subspaceProject(eigenvectors, averageFaceRow, 
      preprocessedFace.reshape(1,1)); 

    // Generate the reconstructed face back from the eigenspace. 
    Mat reconstructionRow = subspaceReconstruct(eigenvectors, 
      averageFaceRow, projection); 

    // Make it a rectangular shaped image instead of a single row. 
    Mat reconstructionMat = reconstructionRow.reshape(1,  
      faceHeight); 

    // Convert the floating-point pixels to regular 8-bit uchar. 
    Mat reconstructedFace = Mat(reconstructionMat.size(), CV_8U); 
    reconstructionMat.convertTo(reconstructedFace, CV_8U, 1, 0);
</pre>
<p>The following image shows two typical reconstructed faces. The face on the left-hand side was reconstructed well because it was from a known person, whereas the face on the right-hand side was reconstructed badly because it was from an unknown person or a known person but with unknown lighting conditions/facial expression/face direction:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="189" width="499" class="image-border" src="assets/a7829_7_11.png"/></div>
<p>We can now calculate how similar this reconstructed face is to the input face by using the same <kbd>getSimilarity()</kbd> function we created previously for comparing two images, where a value less than 0.3 implies that the two images are very similar. For Eigenfaces, there is one eigenvector for each face, so reconstruction tends to work well and therefore we can typically use a threshold of 0.5, but Fisherfaces has just one eigenvector for each person, so reconstruction will not work as well and therefore it needs a higher threshold, say 0.7. This is done as follows:</p>
<pre>
    similarity = getSimilarity(preprocessedFace, reconstructedFace); 
    if (similarity &gt; UNKNOWN_PERSON_THRESHOLD) { 
      identity = -1;    // Unknown person. 
    }
</pre>
<p>Now you can just print the identity to the console, or use it for wherever your imagination takes you! Remember that this face-recognition method and this face-verification method are only reliable in the certain conditions that you train them for. So to obtain good recognition accuracy, you will need to ensure that the training set of each person covers the full range of lighting conditions, facial expressions, and angles that you expect to test with. The face preprocessing stage helped reduce some differences with lighting conditions and in-plane rotation (if the person tilts their head towards their left or right shoulder), but for other differences, such as out-of-plane rotation (if the person turns their head towards the left-hand side or right-hand side), it will only work if it is covered well in your training set.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Finishing touches - saving and loading files</h1>
            </header>

            <article>
                
<p>You could potentially add a command-line based method that processes input files and saves them to the disk, or even perform face detection, face preprocessing and/or face recognition as a web service, and so on. For these types of projects, it is quite easy to add the desired functionality by using the <kbd>save</kbd> and <kbd>load</kbd> functions of the <kbd>FaceRecognizer</kbd> class. You may also want to save the trained data and then load it on the program's start up.</p>
<p>Saving the trained model to an XML or YML file is very easy:</p>
<pre>
model-&gt;save("trainedModel.yml");
</pre>
<p>You may also want to save the array of preprocessed faces and labels, if you want to add more data to the training set later.</p>
<p>For example, here is some sample code for loading the trained model from a file. Note that you must specify the face-recognition algorithm (for example, <kbd>FaceRecognizer.Eigenfaces</kbd> or <kbd>FaceRecognizer.Fisherfaces</kbd>) that was originally used to create the trained model:</p>
<pre>
    string facerecAlgorithm = "FaceRecognizer.Fisherfaces"; 
    model = Algorithm::create&lt;FaceRecognizer&gt;(facerecAlgorithm); 
    Mat labels; 
    try { 
      model-&gt;load("trainedModel.yml"); 
      labels = model-&gt;get&lt;Mat&gt;("labels"); 
    } catch (cv::Exception &amp;e) {} 
    if (labels.rows &lt;= 0) { 
      cerr &lt;&lt; "ERROR: Couldn't load trained data from " 
              "[trainedModel.yml]!" &lt;&lt; endl; 
      exit(1); 
    }
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Finishing touches - making a nice and interactive GUI</h1>
            </header>

            <article>
                
<p>While the code given so far in this chapter is sufficient for a whole face recognition system, there still needs to be a way to put the data into the system and a way to use it. Many face recognition systems for research will choose the ideal input to be text files listing where the static image files are stored on the computer, as well as other important data such as the true name or identity of the person and perhaps true pixel coordinates of regions of the face (such as ground truth of where the face and eye centers actually are). This would either be collected manually or by another face recognition system.</p>
<p>The ideal output would then be a text file comparing the recognition results with the ground truth, so that statistics may be obtained for comparing the face recognition system with other face recognition systems.</p>
<p>However, as the face recognition system in this chapter is designed for learning as well as practical fun purposes, rather than competing with the latest research methods, it is useful to have an easy-to-use GUI that allows face collection, training, and testing, interactively from the webcam in real time. So this section will provide an interactive GUI providing these features. The reader is expected to either use this provided GUI that comes with this book, or to modify the GUI for their own purposes, or to ignore this GUI and design their own GUI to perform the face recognition techniques discussed so far.</p>
<p>As we need the GUI to perform multiple tasks, let's create a set of modes or states that the GUI will have, with buttons or mouse clicks for the user to change modes:</p>
<ul>
<li><strong>Startup</strong>: This state loads and initializes the data and webcam.</li>
<li><strong>Detection</strong>: This state detects faces and shows them with preprocessing, until the user clicks on the <span class="packt_screen">Add Person</span> button.</li>
<li><strong>Collection</strong>: This state collects faces for the current person, until the user clicks anywhere in the window. This also shows the most recent face of each person. The user clicks either one of the existing people or the <span class="packt_screen">Add Person</span> button, to collect faces for different people.</li>
<li><strong>Training</strong>: In this state, the system is trained with the help of all the collected faces of all the collected people.</li>
<li><strong>Recognition</strong>: This consists of highlighting the recognized person and showing a confidence meter. The user clicks either one of the people or the <span class="packt_screen">Add Person</span> button, to return to mode 2 (<em>Collection</em>).</li>
</ul>
<p>To quit, the user can hit the <em>Esc</em> key in the window at any time. Let's also add a <span class="packt_screen">Delete All</span> mode that restarts a new face recognition system, and a <span class="packt_screen"><span class="packt_screen">Debug</span></span> button that toggles the display of extra debug info. We can create an enumerated <kbd>mode</kbd> variable to show the current mode.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Drawing the GUI elements</h1>
            </header>

            <article>
                
<p>To display the current mode on the screen, let's create a function to draw text easily. OpenCV comes with a <kbd>cv::putText()</kbd> function with several fonts and anti-aliasing, but it can be tricky to place the text in the correct location that you want. Luckily, there is also a <kbd>cv::getTextSize()</kbd> function to calculate the bounding box around the text, so we can create a wrapper function to make it easier to place text. We want to be able to place text along any edge of the window and make sure it is completely visible and also to allow placing multiple lines or words of text next to each other without overwriting each other. So here is a wrapper function to allow you to specify either left-justified or right-justified, as well as to specify top-justified or bottom-justified, and return the bounding box, so we can easily draw multiple lines of text on any corner or edge of the window:</p>
<pre>
    // Draw text into an image. Defaults to top-left-justified  
    // text, so give negative x coords for right-justified text, 
    // and/or negative y coords for bottom-justified text. 
    // Returns the bounding rect around the drawn text. 
    Rect drawString(Mat img, string text, Point coord, Scalar  
      color, float fontScale = 0.6f, int thickness = 1, 
      int fontFace = FONT_HERSHEY_COMPLEX);
</pre>
<p>Now to display the current mode on the GUI, as the background of the window will be the camera feed, it is quite possible that if we simply draw text over the camera feed; it might be the same color as the camera background! So let's just draw a black shadow of text that is just 1 pixel apart from the foreground text we want to draw. Let's also draw a line of helpful text below it, so the user knows the steps to follow. Here is an example of how to draw some text using the <kbd>drawString()</kbd> function:</p>
<pre>
    string msg = "Click [Add Person] when ready to collect faces."; 
    // Draw it as black shadow &amp; again as white text. 
    float txtSize = 0.4; 
    int BORDER = 10; 
    drawString (displayedFrame, msg, Point(BORDER, -BORDER-2), 
      CV_RGB(0,0,0), txtSize); 
    Rect rcHelp = drawString(displayedFrame, msg, Point(BORDER+1, 
      -BORDER-1), CV_RGB(255,255,255), txtSize);
</pre>
<p>The following partial screenshot shows the mode and info at the bottom of the GUI window, overlaid on top of the camera image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="55" width="476" src="assets/a7829_07_12.png"/></div>
<p>We mentioned that we want a few GUI buttons, so let's create a function to draw a GUI button easily as follows:</p>
<pre>
    // Draw a GUI button into the image, using drawString(). 
    // Can give a minWidth to have several buttons of same width. 
    // Returns the bounding rect around the drawn button. 
    Rect drawButton(Mat img, string text, Point coord, 
      int minWidth = 0) 
    { 
      const int B = 10; 
      Point textCoord = Point(coord.x + B, coord.y + B); 
      // Get the bounding box around the text. 
      Rect rcText = drawString(img, text, textCoord,  
        CV_RGB(0,0,0)); 
      // Draw a filled rectangle around the text. 
      Rect rcButton = Rect(rcText.x - B, rcText.y - B, 
        rcText.width + 2*B, rcText.height + 2*B); 
      // Set a minimum button width. 
      if (rcButton.width &lt; minWidth) 
        rcButton.width = minWidth; 
      // Make a semi-transparent white rectangle. 
      Mat matButton = img(rcButton); 
      matButton += CV_RGB(90, 90, 90); 
      // Draw a non-transparent white border. 
      rectangle(img, rcButton, CV_RGB(200,200,200), 1, CV_AA); 

      // Draw the actual text that will be displayed. 
      drawString(img, text, textCoord, CV_RGB(10,55,20)); 

      return rcButton; 
    }
</pre>
<p>Now we create several clickable GUI buttons using the <kbd>drawButton()</kbd> function,<br/>
which will always be shown at the top-left of the GUI, as shown in the following<br/>
partial screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="85" width="172" class="image-border" src="assets/image_07_006.jpg"/></div>
<p>As we mentioned, the GUI program has some modes that it switches between (as a finite state machine), beginning with the Startup mode. We will store the current mode as the <kbd>m_mode</kbd> variable.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Startup mode</h1>
            </header>

            <article>
                
<p>In the Startup mode, we just need to load the XML detector files to detect the face and eyes and initialize the webcam, which we've already covered. Let's also create a main GUI window with a mouse callback function that OpenCV will call whenever the user moves or clicks their mouse in our window. It may also be desirable to set the camera resolution to something reasonable; for example, 640x480, if the camera supports it. This is done as follows:</p>
<pre>
    // Create a GUI window for display on the screen. 
    namedWindow(windowName); 

    // Call "onMouse()" when the user clicks in the window. 
    setMouseCallback(windowName, onMouse, 0); 

    // Set the camera resolution. Only works for some systems. 
    videoCapture.set(CV_CAP_PROP_FRAME_WIDTH, 640); 
    videoCapture.set(CV_CAP_PROP_FRAME_HEIGHT, 480); 

    // We're already initialized, so let's start in Detection mode. 
    m_mode = MODE_DETECTION;
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Detection mode</h1>
            </header>

            <article>
                
<p>In the Detection mode, we want to continuously detect faces and eyes, draw rectangles or circles around them to show the detection result, and show the current preprocessed face. In fact, we will want these to be displayed no matter which mode we are in. The only thing special about the Detection mode is that it will change to the next mode (<em>Collection</em>) when the user clicks the <span class="packt_screen">Add Person</span> button.</p>
<p>If you remember from the detection step previously in this chapter, the output of our detection stage will be:</p>
<ul>
<li><kbd>Mat preprocessedFace</kbd>: The preprocessed face (if face and eyes<br/>
were detected).</li>
<li><kbd>Rect faceRect</kbd>: The detected face region coordinates.</li>
<li><kbd>Point leftEye</kbd>, <kbd>rightEye</kbd>: The detected left and right eye center coordinates.</li>
</ul>
<p>So we should check if a preprocessed face was returned and draw a rectangle and circles around the face and eyes if they were detected as follows:</p>
<pre>
    bool gotFaceAndEyes = false; 
    if (preprocessedFace.data) 
      gotFaceAndEyes = true; 

    if (faceRect.width &gt; 0) { 
      // Draw an anti-aliased rectangle around the detected face. 
      rectangle(displayedFrame, faceRect, CV_RGB(255, 255, 0), 2, 
        CV_AA); 

      // Draw light-blue anti-aliased circles for the 2 eyes. 
      Scalar eyeColor = CV_RGB(0,255,255); 
      if (leftEye.x &gt;= 0) {   // Check if the eye was detected 
        circle(displayedFrame, Point(faceRect.x + leftEye.x, 
          faceRect.y + leftEye.y), 6, eyeColor, 1, CV_AA); 
      } 
      if (rightEye.x &gt;= 0) {   // Check if the eye was detected 
        circle(displayedFrame, Point(faceRect.x + rightEye.x,  
          faceRect.y + rightEye.y), 6, eyeColor, 1, CV_AA); 
      } 
    }
</pre>
<p>We will overlay the current preprocessed face at the top-center of the window<br/>
as follows:</p>
<pre>
    int cx = (displayedFrame.cols - faceWidth) / 2; 
    if (preprocessedFace.data) { 
      // Get a BGR version of the face, since the output is BGR. 
      Mat srcBGR = Mat(preprocessedFace.size(), CV_8UC3); 
      cvtColor(preprocessedFace, srcBGR, CV_GRAY2BGR); 

      // Get the destination ROI. 
      Rect dstRC = Rect(cx, BORDER, faceWidth, faceHeight); 
      Mat dstROI = displayedFrame(dstRC); 

      // Copy the pixels from src to dst. 
      srcBGR.copyTo(dstROI); 
    } 
    // Draw an anti-aliased border around the face. 
    rectangle(displayedFrame, Rect(cx-1, BORDER-1, faceWidth+2, 
      faceHeight+2), CV_RGB(200,200,200), 1, CV_AA);
</pre>
<p>The following screenshot shows the displayed GUI when in the Detection mode.<br/>
The preprocessed face is shown at the top-center, and the detected face and<br/>
eyes are marked:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="262" width="350" class="image-border" src="assets/image_07_007.jpg"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Collection mode</h1>
            </header>

            <article>
                
<p>We enter the Collection mode when the user clicks on the <span class="packt_screen">Add Person</span> button to signal that they want to begin collecting faces for a new person. As mentioned previously, we have limited the face collection to one face per second and then only if it has changed noticeably from the previously collected face. And remember, we decided to collect not only the preprocessed face but also the mirror image of the preprocessed face.</p>
<p>In the Collection mode, we want to show the most recent face of each known person and let the user click on one of those people to add more faces to them or click the <span class="packt_screen">Add Person</span> button to add a new person to the collection. The user must click somewhere in the middle of the window to continue to the next (<em>Training mode</em>) mode.</p>
<p>So first we need to keep a reference to the latest face that was collected for each person. We'll do this by updating the <kbd>m_latestFaces</kbd> array of integers, which just stores the array index of each person, from the big <kbd>preprocessedFaces</kbd> array (that is, the collection of all faces of all the people). As we also store the mirrored face in that array, we want to reference the second last face, not the last face. This code should be appended to the code that adds a new face (and mirrored face) to the <kbd>preprocessedFaces</kbd> array as follows:</p>
<pre>
    // Keep a reference to the latest face of each person. 
    m_latestFaces[m_selectedPerson] = preprocessedFaces.size() - 2;
</pre>
<p>We just have to remember to always grow or shrink the <kbd>m_latestFaces</kbd> array whenever a new person is added or deleted (for example, due to the user clicking on the <span class="packt_screen">Add Person</span> button). Now let's display the most recent face for each of the collected people, on the right-hand side of the window (both in the Collection mode and Recognition mode later) as follows:</p>
<pre>
    m_gui_faces_left = displayedFrame.cols - BORDER - faceWidth; 
    m_gui_faces_top = BORDER; 
    for (int i=0; i&lt;m_numPersons; i++) { 
      int index = m_latestFaces[i]; 
      if (index &gt;= 0 &amp;&amp; index &lt; (int)preprocessedFaces.size()) { 
        Mat srcGray = preprocessedFaces[index]; 
        if (srcGray.data) { 
          // Get a BGR face, since the output is BGR. 
          Mat srcBGR = Mat(srcGray.size(), CV_8UC3); 
          cvtColor(srcGray, srcBGR, CV_GRAY2BGR); 

          // Get the destination ROI 
          int y = min(m_gui_faces_top + i * faceHeight, 
          displayedFrame.rows - faceHeight); 
          Rect dstRC = Rect(m_gui_faces_left, y, faceWidth, 
          faceHeight); 
          Mat dstROI = displayedFrame(dstRC); 

          // Copy the pixels from src to dst. 
          srcBGR.copyTo(dstROI); 
        } 
      } 
    }
</pre>
<p>We also want to highlight the current person being collected, using a thick red border around their face. This is done as follows:</p>
<pre>
    if (m_mode == MODE_COLLECT_FACES) { 
      if (m_selectedPerson &gt;= 0 &amp;&amp; 
        m_selectedPerson &lt; m_numPersons) { 
        int y = min(m_gui_faces_top + m_selectedPerson *  
        faceHeight, displayedFrame.rows - faceHeight); 
        Rect rc = Rect(m_gui_faces_left, y, faceWidth, faceHeight); 
        rectangle(displayedFrame, rc, CV_RGB(255,0,0), 3, CV_AA); 
      } 
    }
</pre>
<p>The following partial screenshot shows the typical display when faces for several people have been collected. The user can click any of the people at the top-right to collect more faces for that person.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="164" width="389" src="assets/a7829_07_15.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Training mode</h1>
            </header>

            <article>
                
<p>When the user finally clicks in the middle of the window, the face-recognition algorithm will begin training on all the collected faces. But it is important to make sure there have been enough faces or people collected, otherwise the program may crash. In general, this just requires making sure there is at least one face in the training set (which implies there is at least one person). But the Fisherfaces algorithm looks for comparisons between people, so if there are less than two people in the training set, it will also crash. So we must check whether the selected face-recognition algorithm is Fisherfaces. If it is, then we require at least two people with faces, otherwise we require at least one person with a face. If there isn't enough data, then the program goes back to the Collection mode so the user can add more faces before training.</p>
<p>To check if there are at least two people with collected faces, we can make sure that when a user clicks on the <span class="packt_screen">Add Person</span> button, a new person is only added if there isn't any empty person (that is, a person that was added but does not have any collected faces yet). If there are just two people and we are using the Fisherfaces algorithm, then we must make sure an <kbd>m_latestFaces</kbd> reference was set for the last person during the Collection mode. <kbd>m_latestFaces[i]</kbd> is initialized to -1 when there still haven't been any faces added to that person, and then it becomes <kbd>0</kbd> or higher once faces for that person have been added. This is done as follows:</p>
<pre>
    // Check if there is enough data to train from. 
    bool haveEnoughData = true; 
    if (!strcmp(facerecAlgorithm, "FaceRecognizer.Fisherfaces")) { 
      if ((m_numPersons &lt; 2) || 
      (m_numPersons == 2 &amp;&amp; m_latestFaces[1] &lt; 0) ) { 
        cout &lt;&lt; "Fisherfaces needs &gt;= 2 people!" &lt;&lt; endl; 
        haveEnoughData = false; 
      } 
    } 
    if (m_numPersons &lt; 1 || preprocessedFaces.size() &lt;= 0 || 
      preprocessedFaces.size() != faceLabels.size()) { 
      cout &lt;&lt; "Need data before it can be learnt!" &lt;&lt; endl; 
      haveEnoughData = false; 
    } 

    if (haveEnoughData) { 
      // Train collected faces using Eigenfaces or Fisherfaces. 
      model = learnCollectedFaces(preprocessedFaces, faceLabels, 
              facerecAlgorithm); 

      // Now that training is over, we can start recognizing! 
      m_mode = MODE_RECOGNITION; 
    } 
    else { 
      // Not enough training data, go back to Collection mode! 
      m_mode = MODE_COLLECT_FACES; 
    }
</pre>
<p>The training may take a fraction of a second or it may take several seconds or even minutes, depending on how much data is collected. Once the training of collected faces is complete, the face recognition system will automatically enter the <em>Recognition mode</em>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Recognition mode</h1>
            </header>

            <article>
                
<p>In the Recognition mode, a confidence meter is shown next to the preprocessed face, so the user knows how reliable the recognition is. If the confidence level is higher than the unknown threshold, it will draw a green rectangle around the recognized person to show the result easily. The user can add more faces for further training if they click on the <span class="packt_screen">Add Person</span> button or one of the existing people, which causes the program to return to the Collection mode.</p>
<p>Now we have obtained the recognized identity and the similarity with the reconstructed face as mentioned earlier. To display the confidence meter, we know that the L2 similarity value is generally between 0 to 0.5 for high confidence and between 0.5 to 1.0 for low confidence, so we can just subtract it from 1.0 to get the confidence level between 0.0 to 1.0. Then we just draw a filled rectangle using the confidence level as the ratio shown as follows:</p>
<pre>
    int cx = (displayedFrame.cols - faceWidth) / 2; 
    Point ptBottomRight = Point(cx - 5, BORDER + faceHeight); 
    Point ptTopLeft = Point(cx - 15, BORDER); 

    // Draw a gray line showing the threshold for "unknown" people. 
    Point ptThreshold = Point(ptTopLeft.x, ptBottomRight.y - 
      (1.0 - UNKNOWN_PERSON_THRESHOLD) * faceHeight); 
    rectangle(displayedFrame, ptThreshold, Point(ptBottomRight.x, 
    ptThreshold.y), CV_RGB(200,200,200), 1, CV_AA); 

    // Crop the confidence rating between 0 to 1 to fit in the bar. 
    double confidenceRatio = 1.0 - min(max(similarity, 0.0), 1.0); 
    Point ptConfidence = Point(ptTopLeft.x, ptBottomRight.y - 
      confidenceRatio * faceHeight); 

    // Show the light-blue confidence bar. 
    rectangle(displayedFrame, ptConfidence, ptBottomRight, 
      CV_RGB(0,255,255), CV_FILLED, CV_AA); 

    // Show the gray border of the bar. 
    rectangle(displayedFrame, ptTopLeft, ptBottomRight, 
      CV_RGB(200,200,200), 1, CV_AA);
</pre>
<p>To highlight the recognized person, we draw a green rectangle around their face<br/>
as follows:</p>
<pre>
    if (identity &gt;= 0 &amp;&amp; identity &lt; 1000) { 
      int y = min(m_gui_faces_top + identity * faceHeight, 
        displayedFrame.rows - faceHeight); 
      Rect rc = Rect(m_gui_faces_left, y, faceWidth, faceHeight); 
      rectangle(displayedFrame, rc, CV_RGB(0,255,0), 3, CV_AA); 
    }
</pre>
<p>The following partial screenshot shows a typical display when running in Recognition mode, showing the confidence meter next to the preprocessed face<br/>
at the top-center, and highlighting the recognized person in the top-right corner.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Checking and handling mouse clicks</h1>
            </header>

            <article>
                
<p>Now that we have all our GUI elements drawn, we just need to process mouse events. When we initialized the display window, we told OpenCV that we want a mouse event callback to our <kbd>onMouse</kbd> function. We don't care about mouse movement, only the mouse clicks, so first we skip the mouse events that aren't for the left-mouse-button click as follows:</p>
<pre>
    void onMouse(int event, int x, int y, int, void*) 
    { 
      if (event != CV_EVENT_LBUTTONDOWN) 
        return; 

      Point pt = Point(x,y); 

      ... (handle mouse clicks) 
      ... 
    }
</pre>
<p>As we obtained the drawn rectangle bounds of the buttons when drawing them, we just check if the mouse click location is in any of our button regions by calling OpenCV's <kbd>inside()</kbd> function. Now we can check for each button we have created.</p>
<p>When the user clicks on the <span class="packt_screen">Add Person</span> button, we just add 1 to the <kbd>m_numPersons</kbd> variable, allocate more space in the <kbd>m_latestFaces</kbd> variable, select the new person for collection, and begin the Collection mode (no matter which mode we were previously in).</p>
<p>But there is one complication; to ensure that we have at least one face for each<br/>
person when training, we will only allocate space for a new person if there isn't already a person with zero faces. This will ensure that we can always check the value of <kbd>m_latestFaces[m_numPersons-1]</kbd> to see if a face has been collected for every person. This is done as follows:</p>
<pre>
    if (pt.inside(m_btnAddPerson)) { 
      // Ensure there isn't a person without collected faces. 
      if ((m_numPersons==0) || 
         (m_latestFaces[m_numPersons-1] &gt;= 0)) { 
          // Add a new person. 
          m_numPersons++; 
          m_latestFaces.push_back(-1); 
      } 
      m_selectedPerson = m_numPersons - 1; 
      m_mode = MODE_COLLECT_FACES; 
    }
</pre>
<p>This method can be used to test for other button clicks, such as toggling the debug flag as follows:</p>
<pre>
    else if (pt.inside(m_btnDebug)) { 
      m_debug = !m_debug; 
    }
</pre>
<p>To handle the <span class="packt_screen">Delete All</span> button, we need to empty various data structures that are local to our main loop (that is, not accessible from the mouse event callback function), so we change to the <span class="packt_screen">Delete All</span> mode and then we can delete everything from inside the main loop. We also must deal with the user clicking the main window (that is, not a button). If they clicked on one of the people on the right-hand side, then we want to select that person and change to the Collection mode. Or if they clicked in the main window while in the Collection mode, then we want to change to the Training mode. This is done as follows:</p>
<pre>
    else { 
      // Check if the user clicked on a face from the list. 
      int clickedPerson = -1; 
      for (int i=0; i&lt;m_numPersons; i++) { 
        if (m_gui_faces_top &gt;= 0) { 
          Rect rcFace = Rect(m_gui_faces_left,  
          m_gui_faces_top + i * faceHeight, faceWidth, faceHeight); 
          if (pt.inside(rcFace)) { 
            clickedPerson = i; 
            break; 
          } 
        } 
      } 
      // Change the selected person, if the user clicked a face. 
      if (clickedPerson &gt;= 0) { 
        // Change the current person &amp; collect more photos. 
        m_selectedPerson = clickedPerson; 
        m_mode = MODE_COLLECT_FACES; 
      } 
      // Otherwise they clicked in the center. 
      else { 
        // Change to training mode if it was collecting faces. 
        if (m_mode == MODE_COLLECT_FACES) { 
            m_mode = MODE_TRAINING; 
        } 
      } 
    }
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>This chapter has shown you all the steps required to create a real-time face recognition app, with enough preprocessing to allow some differences between the training set conditions and the testing set conditions, just using basic algorithms. We used face detection to find the location of a face within the camera image, followed by several forms of face preprocessing to reduce the effects of different lighting conditions, camera and face orientations, and facial expressions. We then trained an Eigenfaces or Fisherfaces machine-learning system with the preprocessed faces we collected, and finally we performed face recognition to see who the person is with face verification providing a confidence metric in case it is an unknown person.</p>
<p>Rather than providing a command-line tool that processes image files in an offline manner, we combined all the preceding steps into a self-contained real-time GUI program to allow immediate use of the face recognition system. You should be able to modify the behavior of the system for your own purposes, such as to allow an automatic login of your computer, or if you are interested in improving the recognition reliability then you can read conference papers about recent advances in face recognition to potentially improve each step of the program until it is reliable enough for your specific needs. For example, you could improve the face preprocessing stages, or use a more advanced machine-learning algorithm, or an even better face verification algorithm, based on methods at <a href="http://www.face-rec.org/algorithms/" target="_blank">h t t p ://w w w . f a c e - r e c . o r g /a l g o r i t h m s /</a> and <a href="http://www.cvpapers.com" target="_blank">h t t p ://w w w . c v p a p e r s . c o m</a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">References</h1>
            </header>

            <article>
                
<ul>
<li><em>Rapid Object Detection using a Boosted Cascade of Simple Features</em>, <em>P. Viola<br/>
and M.J. Jones</em>, <em>Proceedings of the IEEE Transactions on CVPR 2001</em>, <em>Vol. 1</em>,<br/>
<em>pp. 511-518</em></li>
<li><em>An Extended Set of Haar-like Features for Rapid Object Detection</em>, <em>R. Lienhart and J. Maydt</em>, <em>Proceedings of the IEEE Transactions on ICIP 2002</em>, <em>Vol. 1</em>, <em>pp. 900-903</em></li>
<li><em>Face Description with Local Binary Patterns: Application to Face Recognition</em>, <em>T. Ahonen, A. Hadid and M. Pietikäinen</em>, <em>Proceedings of the IEEE Transactions on PAMI 2006</em>, <em>Vol. 28</em>, <em>Issue 12</em>, <em>pp. 2037-2041</em></li>
<li><em>Learning OpenCV: Computer Vision with the OpenCV Library</em>, <em>G. Bradski and A. Kaehler</em>, <em>pp. 186-190</em>, <em>O'Reilly Media</em>.</li>
<li><em>Eigenfaces for recognition</em>, <em>M. Turk and A. Pentland</em>, <em>Journal of Cognitive Neuroscience 3</em>, <em>pp. 71-86</em></li>
<li><em>Eigenfaces vs. Fisherfaces: Recognition using class specific linear projection</em>, <em>P.N. Belhumeur, J. Hespanha and D. Kriegman</em>, <em>Proceedings of the IEEE Transactions on PAMI 1997</em>, <em>Vol. 19</em>, <em>Issue 7</em>, <em>pp. 711-720</em></li>
<li><em>Face Recognition with Local Binary Patterns</em>, <em>T. Ahonen, A. Hadid and M. Pietikäinen</em>, <em>Computer Vision - ECCV 2004</em>, <em>pp. 469-48</em></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>