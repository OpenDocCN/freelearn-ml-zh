<html><head></head><body>
		<div id="_idContainer292" class="Content">
			<h1 id="_idParaDest-335"><em class="italics"><a id="_idTextAnchor339"/>Chapter 9:</em></h1>
		</div>
		<div id="_idContainer293" class="Content">
			<h1 id="_idParaDest-336"><a id="_idTextAnchor340"/>Capstone Project - Based on Research Papers</h1>
		</div>
		<div id="_idContainer294" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Apply end-to-end machine learning workflow on a problem using mlr and OpenML, which involves identifying research articles.</li>
				<li class="bullets">Train machine learning model and, subsequently, predict and evaluate using the model on a test dataset.</li>
				<li class="bullets">Perform resampling on the dataset.</li>
				<li class="bullets">Design experiments for building various models.</li>
				<li class="bullets">Build benchmarks for choosing the best model.</li>
			</ul>
			<p>In this chapter, we will take up the latest research paper based on a real-world problem and will reproduce the result.</p>
		</div>
		<div id="_idContainer312" class="Content">
			<h2 id="_idParaDest-337"><a id="_idTextAnchor341"/>Introduction</h2>
			<p>In this final chapter, we will focus on working on a research-based capstone project. The ideas from all the previous chapters such as designing the problem using the SCQ framework, identifying the source of data, preprocessing the dataset, training a machine learning model, evaluating a model, applying resampling techniques, and many other concepts will be used. Additionally, this chapter will also focus on benchmarking models, designing experiments in machine learning, collaborating in open source platforms, and making a research work reproducible for the benefits of the larger community.</p>
			<p>The abundance of online resources, computation power, and out-of-the-box toolkit solutions has made the entry barrier in becoming a machine learning professional minimum. Today, we have plenty of quickstart algorithms provided as a function in a package in programming languages such as R and Python, or even as a drag and drop in platforms such as Google Cloud AutoML or Microsoft Azure Machine Learning Studio. However, what is often missing in many such quick start Hello World models is the keen focus on problem solving and the ability to go beyond the available tools.</p>
			<p>Apart from the extravagant toolkits, there exists a world of research-oriented work produced by many leading practitioners from academia and industry. The importance of such research work is immense when it comes to producing breakthrough and high-quality outcomes. However, the accessibility of such research work is limited and hence prevents the widespread adoption among machine learning practitioners. Another reason why one does not pursue research-based work is because of the lack of reproducibility (mostly because of code not being available in public domain, unclear research finding, or poor quality of the research work) and jargon-filled theory (many written in mathematical language) found in research articles or papers.</p>
			<p>This chapter is dedicated to such research work, which often goes unnoticed by many learners who endeavor into machine learning but limit themselves to using only specific tools and packages advocated in blogs or online books. We will focus on two significant research works, which, fortunately, also found a place in R packages. The next section will introduce the work and set the flow of the rest of this chapter.</p>
			<h2 id="_idParaDest-338"><a id="_idTextAnchor342"/>Exploring Research Work</h2>
			<p>In this chapter, we will explore the two most significant research works that eventually also became an open source offering. The emphasis in this chapter is given onto a top-down approach, where we will start from the origin of excellent research work and see how it became a mainstream toolkit for everyone to use. While emphasizing on research work, we would like to highlight that a lot of research work does not find its place in the standard toolkit available in the market, but some gems could be found if one works slightly harder.</p>
			<p>We recommend following the fantastic effort put by the creators of https://paperswithcode.com. The <strong class="bold">Papers With Code</strong> team has created a free and open resource platform with machine learning papers, code, and evaluation tables with the help from the community and powered by automation. They have already automated the linking of code to papers, and they are now working on automating the extraction of evaluation metrics from papers. The work is commendable because it will bring the best research work to stand out amid the noise and abundance.</p>
			<p>The following table will highlight five cases of research work, which we found through the Papers With Code website. Throughout this book, you would have seen a lot of R code using various packages for each stage of the machine learning workflow. The work done by the researchers of mlr and OpenML is now packaged in R, and in particular, OpenML is a complete platform. We will learn how to leverage the mlr and OpenML platforms to produce the best machine learning model, beyond just the quickstart <strong class="bold">Hello World</strong> examples. For reference, review the following table:</p>
			<div>
				<div id="_idContainer295" class="IMG---Figure">
					<img src="image/C12624_09_01.jpg" alt="Figure 9.1: Research papers used in this lesson for demonstration (Part 1) &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.1: Research papers used in this chapter for demonstration (Part 1) </h6>
			<div>
				<div id="_idContainer296" class="IMG---Figure">
					<img src="image/C12624_09_02.jpg" alt="Figure 9.2: Research papers used in this lesson for demonstration (Part 2)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.2: Research papers used in this chapter for demonstration (Part 2)</h6>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor343"/>The mlr Package</h2>
			<p>Now, we shall go into learning how the mlr package offers a complete framework to work with many machine learning models and problem. Often, in many ML projects, one has to manage an overwhelming amount of detailing around numerous experiments (also called <strong class="bold">trial-and-error iterations</strong>). Each experiment consists of many pieces of training using different machine learning algorithms, performance measures, hyperparameters, resampling techniques and predictions. Unless we do not systematically analyze the information obtained in each experiment, we will not be able to come out with the best combination of parameter values.</p>
			<p>Another advantage of using the mlr package comes from its rich collection of machine learning algorithms from various packages. We do not have to install multiple packages for different implementation of the machine learning algorithm anymore. Instead, mlr offers everything in one place. To understand this better, refer to the following table:</p>
			<div>
				<div id="_idContainer297" class="IMG---Figure">
					<img src="image/C12624_09_03.jpg" alt="Figure 9.3: The mlr Package (Part 1)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.3: The mlr Package (Part 1)</h6>
			<p><strong class="bold">Multilabel Classification Algorithms</strong></p>
			<p>The following models are available in the mlr package for multilabel classification, where one observation could be assigned to more than one class. These models are useful in solving many useful problems, such as, in Netflix, you will see that each movie could be tagged as Action, Adventure and Fantasy. Alternatively, in YouTube, where millions of videos are posted every day, an automatic algorithm could tag the videos into multiple class and hence help in content filtering and better search.</p>
			<p>We will use these algorithms along with the classifiers defined in the previous table:</p>
			<div>
				<div id="_idContainer298" class="IMG---Figure">
					<img src="image/C12624_09_04.jpg" alt="Figure 9.4: Multilabel classification with the mlr package&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.4: Multilabel classification with the mlr package</h6>
			<h3 id="_idParaDest-340"><a id="_idTextAnchor344"/>OpenML Package</h3>
			<p>OpenML, a collaboration platform through which researchers from academia and industry can automatically share, organize, and deliberate machine learning experiments, data, algorithms, and flows. The platform brings efficient collaboration and results in reproducibility.</p>
			<p>The OpenML package in R comes with various features that allow users to search, upload, and download the datasets and perform Machine Learning related operations. A user can upload the output of ML experiments, share them with other users, and download the output results. This enhances the reproducibility of work, speeds up the research work, and brings people from different domains together.</p>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor345"/>Problem Design from the Research Paper</h2>
			<p>In this chapter, we will understand, analyze, and reproduce the results from the <em class="italics">Learning multi-label scene classification</em> paper. We will effectively use the mlr and OpenML packages to reproduce the result. Before that, let’s write the <strong class="bold">Situation-Complication-Question</strong> from the paper using the <strong class="bold">SCQ framework</strong>:</p>
			<div>
				<div id="_idContainer299" class="IMG---Figure">
					<img src="image/C12624_09_05.jpg" alt="Figure 9.5: SCQ from the paper Learning multi-label scene classification&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.5: SCQ from the paper Learning multi-label scene classification</h6>
			<h2 id="_idParaDest-342"><a id="_idTextAnchor346"/>Features in Scene Dataset</h2>
			<p>The paper uses the <strong class="inline">scene</strong> dataset for semantic scene classification task. The dataset is a collection of images of natural scenes, where a natural scene may contain multiple objects, such that multiple class labels can describe the scene. For example, a field scene with a mountain in the background. From the paper, we have taken the first figure, which shows two images that are multilabel images depicting two different scenes in a single image. <em class="italics">Figure 9.6</em> is a beach and urban scene, whereas <em class="italics">Figure 9.7</em> shows mountains:</p>
			<div>
				<div id="_idContainer300" class="IMG---Figure">
					<img src="image/C12624_09_06.jpg" alt="Figure 9.6: A beach and urban scene.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.6: A beach and urban scene.</h6>
			<div>
				<div id="_idContainer301" class="IMG---Figure">
					<img src="image/C12624_09_07.jpg" alt="Figure 9.7: A mountains scene.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.7: A mountains scene.</h6>
			<p>From the given images, we could use the following:</p>
			<ul>
				<li><strong class="bold">Color information</strong>: This information is useful when differentiating between certain types of outdoor scenes.</li>
				<li><strong class="bold">Spatial information</strong>: This information is useful in various cases. For example, light, warm colors at the top of the image may correspond to sunrise.</li>
			</ul>
			<p>The paper uses <em class="italics">CIE L*U*V*</em>, such as space, denoted as Luv. Luv space proposes the anticipated perceptual uniformity. After adaptation (a mathematical transformation from the XYZ space to the <em class="italics">L*U*V</em> space) to Luv spaces, the image is divided into 49 blocks using a 7 x 7 grid. Then, the authors calculate the first and second moment (mean and variance) of each band (RGB), which corresponds to a low-resolution image and to computationally low-cost quality features, respectively. In total, we obtain <em class="italics">49 x 2 x 3 = 294</em> features vector per image.</p>
			<p>The remaining six columns in the dataset correspond to the six labels represented in a true/false encoded value. If an image belongs to two classes, the respective column will have true value.</p>
			<h4>Note</h4>
			<p class="callout">In colorimetry, the CIE 1976 L*, u*, v* color space, was adopted by the <strong class="bold">International Commission on Illumination</strong> (<strong class="bold">CIE</strong>) in 1976, as a simple-to-compute transformation of the 1931 CIE XYZ color space, but which attempted perceptual uniformity, which is the difference or distance between two colors.</p>
			<h2 id="_idParaDest-343"><a id="_idTextAnchor347"/>Implementing Multilabel Classifier Using the mlr and OpenML Packages</h2>
			<p>We will now see how to train a multilabel classifier using the mlr and OpenML packages. First, we will download the scene dataset from OpenML.</p>
			<h3 id="_idParaDest-344"><a id="_idTextAnchor348"/>Exercise 102: Downloading the Scene Dataset from OpenML</h3>
			<p>In this exercise, we will download the scene dataset and set it up for further analysis.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>In order to download the scene dataset through OpenML API, first create an account in the OpenML website at <a href="https://www.openml.org/register">https://www.openml.org/register</a>. The registration process involves verifying your email address, post which you will get the access to the API keys.<div id="_idContainer302" class="IMG---Figure"><img src="image/C12624_09_08.jpg" alt="Figure 9.8: The OpenML registration page.&#13;&#10;"/></div><h6>Figure 9.8: The OpenML registration page.</h6></li>
				<li>After logging in to your account, navigate to your account and select the API AUTHENTICATION option.</li>
				<li>On the API Authentication page, select and copy-paste the API key from the API key section. The authors of the Multilabel Classification with R Package mlr paper uploaded a bunch of datasets with a 2016_multilabel_r_benchmark_paper tag, which we can now download from OpenML and start reproducing their results. We will specifically use the scene dataset (with ID 40595).</li>
				<li>Open RStudio and install all the required packages before proceeding.</li>
				<li>Import the required packages and libraries:<p class="snippet">library(mlr)</p><p class="snippet">library(BBmisc)</p><p class="snippet">library(OpenML)</p><p class="snippet">library(batchtools)</p><p class="snippet">library(parallelMap)</p></li>
				<li>Use the API key from the OpenML and register the API key using the following command:<p class="snippet">setOMLConfig(apikey = “627394a14f562f0fa8bcc9ec443d879f”)</p><p>The output is as follows:</p><p class="snippet">## OpenML configuration:</p><p class="snippet">##   server           : http://www.openml.org/api/v1</p><p class="snippet">##   cachedir         : C:\Users\Karthik\AppData\Local\Temp\Rtmp6bSgE4/cache</p><p class="snippet">##   verbosity        : 1</p><p class="snippet">##   arff.reader      : farff</p><p class="snippet">##   confirm.upload   : TRUE</p><p class="snippet">##   apikey           : ***************************d879f</p></li>
				<li>Use the following command to download the dataset from the source:<p class="snippet">ds.list = listOMLDataSets(tag = “2016_multilabel_r_benchmark_paper”)</p><p>The output is as follows:</p><p class="snippet">## Downloading from ‘http://www.openml.org/api/v1/json/data/list/tag/2016_multilabel_r_benchmark_paper/limit/5000/status/active’ to ‘&lt;mem&gt;’.</p></li>
				<li>Next, we will use the ID 40595 to get the scene dataset:<p class="snippet">oml.data = lapply(40595, getOMLDataSet)</p><p class="snippet">df.oml.data.scene &lt;- data.frame(oml.data)</p><h4>Note</h4><p class="callout">Ensure that you install the farff package before proceeding with the previous two commands.</p></li>
				<li>Create the DataFrame using the following command:<p class="snippet">df_scene = df.oml.data.scene</p><p class="snippet">labels = colnames(df_scene)[295:300]</p><p class="snippet">scene.task = makeMultilabelTask(id = “multi”, data = df_scene, target = labels)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/C12624_09_09.jpg" alt="Figure 9.9: Environment setting of the scene.task variable.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.9: Environment setting of the scene.task variable.</h6>
			<p>In this exercise, we registered an account in OpenML and obtained an API key. Using the API key, we were able to download the scene dataset, which has a 2016_multilabel_r_benchmark_paper tag in OpenML. Finally, we converted the dataset into data frame. OpenML provides many such features to collaborate. One can share their code, experiments, and flow with a larger community by assigning a tag.</p>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor349"/>Constructing a Learner</h2>
			<p>A <strong class="bold">learner</strong> is a machine learning algorithm implementation in the mlr package. As highlighted in the previous section on the mlr package, there is a rich collection of such learner functions in mlr.</p>
			<p>For our scene classification problem, the mlr package offers building a multilabel classification model in two possible ways:</p>
			<ul>
				<li><strong class="bold">Adaptation method</strong>: In this, we adapt an explicit algorithm on the entire problem.</li>
				<li><strong class="bold">Transformation method</strong>: We transform the problem into a simple binary classification problem and then apply the available algorithm for the binary classification.</li>
			</ul>
			<h3 id="_idParaDest-346"><a id="_idTextAnchor350"/>Adaptation Methods</h3>
			<p>The mlr package in R offers two algorithm adaption methods. First, the multivariate random forest algorithm that comes from the <strong class="inline">randomForestSRC</strong> package, and second, the random ferns multilabel algorithm built in the <strong class="inline">rFerns</strong> package.</p>
			<p>The <strong class="inline">makeLearner()</strong> function in mlr creates the model object for the <strong class="inline">rFerns</strong> and <strong class="inline">randomForestSRC</strong> algorithms, as shown in the following code:</p>
			<p class="snippet">multilabel.lrn3 = makeLearner(“multilabel.rFerns”)</p>
			<p class="snippet">multilabel.lrn4 = makeLearner(“multilabel.randomForestSRC”)</p>
			<p class="snippet">multilabel.lrn3</p>
			<p>The output is as follows, which shows the information such as name and predict-type about the multilable.rFerns model:</p>
			<p class="snippet">## Learner multilabel.rFerns from package rFernsd</p>
			<p class="snippet">## Type: multilabel</p>
			<p class="snippet">## Name: Random ferns; Short name: rFerns</p>
			<p class="snippet">## Class: multilabel.rFerns</p>
			<p class="snippet">## Properties: numerics,factors,ordered</p>
			<p class="snippet">## Predict-Type: response</p>
			<p class="snippet">## Hyperparameters:</p>
			<h4>Note</h4>
			<p class="callout">Ensure that you install the <em class="italics">rFerns</em> and <em class="italics">randomForestSRC</em> packages before proceeding with the previous two commands.</p>
			<h3 id="_idParaDest-347"><a id="_idTextAnchor351"/>Transformation Methods</h3>
			<p>The second method for constructing a learner is to use the <strong class="bold">problem transformation</strong> methods. The mlr package comes with a wrapped multilabel learner, which creates a multilabel or binary classification learner using the <strong class="inline">makeLearner()</strong> function, and then any one of the five wrapper functions (described in the following section) could be utilized for problem transformation.</p>
			<h3 id="_idParaDest-348"><a id="_idTextAnchor352"/>Binary Relevance Method</h3>
			<p>In multilabel classification problems, each label could be transformed as a binary classification problem. In the process, any one observation could have multiple labels assigned to it. In the mlr package, the <strong class="inline">makeMultilabelBinaryRelevanceWrapper()</strong> method converts the binary learner method to a wrapped Binary Relevance multilabel learner.</p>
			<h3 id="_idParaDest-349"><a id="_idTextAnchor353"/>Classifier Chains Method</h3>
			<p>The classifier chain wrapper method implements a multilabel model, where the binary classifiers are arranged into a chain. Each model comes out with a prediction in the order specified by the chain. The model uses all the features in the given dataset, along with all the predictions of the model that are before in the chain. The <strong class="inline">makeMultilabelClassifierChainsWrapper()</strong> method in mlr is used to create the classifier chain wrappers.</p>
			<h3 id="_idParaDest-350"><a id="_idTextAnchor354"/>Nested Stacking</h3>
			<p>Like classifier chain, however, the class (or label) of the observation are not the actual class but are based on estimations of the class obtained by the trained model (learner) from the previous model in the chain. The <strong class="inline">makeMultilabelNestedStackingWrapper()</strong> method in the mlr package is used to create the nested stacking wrappers.</p>
			<h3 id="_idParaDest-351"><a id="_idTextAnchor355"/>Dependent Binary Relevance Method</h3>
			<p>The <strong class="bold">Dependent Binary Relevance</strong> (<strong class="bold">DBR</strong>) method combines both the learning strategies of chaining and stacking, in which each label goes through training with the actual values of all other labels. During the prediction phase for a label, the other required labels are obtained in a previous step by a base learner (like Binary Relevance method). The <strong class="inline">makeMultilabelDBRWrapper</strong> method in the mlr package is used to create the dependent Binary Relevance wrappers.</p>
			<h3 id="_idParaDest-352"><a id="_idTextAnchor356"/>Stacking</h3>
			<p>Like the dependent Binary Relevance method, however, in the training phase, the labels used as input for each label are obtained by the Binary Relevance method instead of using the actual labels. The <strong class="inline">makeMultilabelStackingWrapper</strong> method in the mlr package is used to create the stacking wrappers.</p>
			<p>In the following exercise, we will see how to generate decision tree model using the <strong class="inline">classif.rpart</strong> method.</p>
			<h3 id="_idParaDest-353"><a id="_idTextAnchor357"/>Exercise 103: Generating Decision Tree Model Using the classif.rpart Method</h3>
			<p>In this exercise, we will generate the decision tree model using the <strong class="inline">classif.rpart</strong> method and then transform it using <em class="italics">Binary Relevance</em> and nested <em class="italics">Stacking</em> wrappers.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">First, use the <strong class="inline">makeLearner</strong> method to create the object for <strong class="inline">classif.rpart</strong>:<p class="snippet">lrn = makeLearner(“classif.rpart”, predict.type = “prob”)</p></li>
				<li>Next, create the stacking wrappers using the <strong class="inline">makeMultilabelBinaryRelevanceWrapper</strong> method:<p class="snippet">multilabel.lrn1 = makeMultilabelBinaryRelevanceWrapper(lrn)</p><p class="snippet">multilabel.lrn2 = makeMultilabelNestedStackingWrapper(lrn)</p></li>
				<li>Next, print the model:<p class="snippet">lrn</p><p>The output is as follows:</p><p class="snippet">Learner classif.rpart from package rpart</p><p class="snippet">Type: classif</p><p class="snippet">Name: Decision Tree; Short name: rpart</p><p class="snippet">Class: classif.rpart</p><p class="snippet">Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp</p><p class="snippet">Predict-Type: prob</p><p class="snippet">Hyperparameters: xval=0</p></li>
				<li>Print the stacking wrappers, as illustrated here:<p class="snippet">multilabel.lrn1</p><p>The output of the previous command is as follows, which shows the information such as the type of model, properties available as part of the model output, and the predict-type:</p><p class="snippet">Learner multilabel.binaryRelevance.classif.rpart from package rpart</p><p class="snippet">Type: multilabel</p><p class="snippet">Name: ; Short name: </p><p class="snippet">Class: MultilabelBinaryRelevanceWrapper</p><p class="snippet">Properties: numerics,factors,ordered,missings,weights,prob,twoclass,multiclass</p><p class="snippet">Predict-Type: prob</p><p class="snippet">Hyperparameters: xval=0</p></li>
			</ol>
			<h3 id="_idParaDest-354"><a id="_idTextAnchor358"/>Train the Model</h3>
			<p>We can train a model as usual with a multilabel learner and a multilabel task as input; use the <strong class="inline">multilabel.lrn1</strong> object.</p>
			<h3 id="_idParaDest-355"><a id="_idTextAnchor359"/>Exercise 104: Train the Model</h3>
			<p>In this exercise, we will first randomly split the data into train and test datasets and then train the model using the <strong class="inline">tain()</strong> function from mlr package and the <strong class="inline">multilabel.lrn1</strong> object defined in the previous section.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Use the following command to train, predict, and evaluate the dataset:<p class="snippet">df_nrow &lt;- nrow(df_scene)</p><p class="snippet">df_all_index &lt;- c(1:df_nrow)</p></li>
				<li>Create the <strong class="bold">train_index</strong> and <strong class="inline">test_index variable</strong> using the following command:<p class="snippet">train_index &lt;- sample(1:df_nrow, 0.7*df_nrow)</p><p class="snippet">test_index &lt;- setdiff(df_all_index,train_index)</p></li>
				<li>Use the <strong class="inline">train</strong> function, which takes model object <strong class="inline">multilabel.lrn1</strong> (<strong class="bold">BinaryRelevanceWrapper</strong>), dataset <strong class="inline">scene.task</strong> with only randomly selected <strong class="inline">train_index</strong> to train the model:<p class="snippet">scene_classi_mod = train(multilabel.lrn1, scene.task, subset = train_index)</p><p class="snippet">scene_classi_mod</p><p>The output is as follows:</p><p class="snippet">Model for learner.id=multilabel.binaryRelevance.classif.rpart; learner.class=MultilabelBinaryRelevanceWrapper</p><p class="snippet">Trained on: task.id = multi; obs = 1684; features = 294</p><p class="snippet">Hyperparameters: xval=0</p></li>
			</ol>
			<p>The <strong class="inline">scene_classi_mod</strong> model using the <strong class="inline">1684</strong> randomly chosen observations from <strong class="inline">scene</strong> dataset is trained using the <strong class="inline">rpart</strong> package in R, which is an implementation of the <strong class="bold">Classification and Regression Tree</strong> (<strong class="bold">CART</strong>) algorithm in machine learning wrapped with the Binary Relevance method for multilabel classification.</p>
			<h3 id="_idParaDest-356"><a id="_idTextAnchor360"/>Predicting the Output</h3>
			<p>Prediction can be done as usual in mlr with the <strong class="inline">predict</strong> function. The input arguments are trained models; the scene.task dataset is assigned to the <strong class="inline">task</strong> and <strong class="inline">test_index</strong> arguments, which correspond to the <strong class="inline">test</strong> dataset is assigned to the <strong class="inline">subset</strong> argument:</p>
			<p class="snippet">pred = predict(scene_classi_mod, task = scene.task, subset = test_index)</p>
			<p class="snippet">names(as.data.frame(pred))</p>
			<p class="snippet">[1] “id”                   “truth.Beach”          “truth.Sunset”         “truth.FallFoliage”   </p>
			<p class="snippet"> [5] “truth.Field”          “truth.Mountain”       “truth.Urban”          “prob.Beach”          </p>
			<p class="snippet"> [9] “prob.Sunset”          “prob.FallFoliage”     “prob.Field”           “prob.Mountain”       </p>
			<p class="snippet">[13] “prob.Urban”           “response.Beach”       “response.Sunset”      “response.FallFoliage”</p>
			<p class="snippet">[17] “response.Field”       “response.Mountain”    “response.Urban”</p>
			<h3 id="_idParaDest-357"><a id="_idTextAnchor361"/>Performance of the Model</h3>
			<p>In order assess the performance of the prediction, the mlr package offers the <strong class="inline">performance()</strong> function, which takes as an input the prediction of the model along with all the measures we would like to compute. All available measures for multilabel classification can be listed by <strong class="inline">listMeasures()</strong>. As per the paper, we use measures such as <strong class="inline">hamloss</strong>, <strong class="inline">f1</strong>, <strong class="inline">subset01</strong>, <strong class="inline">acc</strong>, <strong class="inline">tpr</strong>, and <strong class="inline">ppv</strong> on our predictions:</p>
			<p class="snippet">MEASURES = list(multilabel.hamloss, multilabel.f1, multilabel.subset01, multilabel.acc, multilabel.tpr, multilabel.ppv)</p>
			<p class="snippet">performance(pred, measures = MEASURES)</p>
			<p>The output of the previous command is as follows:</p>
			<p class="snippet">multilabel.hamloss       multilabel.f1 multilabel.subset01      multilabel.acc      multilabel.tpr </p>
			<p class="snippet">          0.1260950           0.5135085           0.5878285           0.4880129           0.5477178 </p>
			<p class="snippet">     multilabel.ppv </p>
			<p class="snippet">          0.7216733</p>
			<p>The following command will list down all the measures available for multilabel classification problem:</p>
			<p class="snippet">listMeasures(“multilabel”)</p>
			<p>The output is as follows:</p>
			<p class="snippet">[1] “featperc”            “multilabel.tpr”      “multilabel.hamloss”  “multilabel.subset01” “timeboth”           </p>
			<p class="snippet"> [6] “timetrain”           “timepredict”         “multilabel.ppv”      “multilabel.f1”       “multilabel.acc”</p>
			<h3 id="_idParaDest-358"><a id="_idTextAnchor362"/>Resampling the Data</h3>
			<p>For evaluating the complete performance of the learning algorithm, we can do some resampling. To define a resampling strategy, either use <strong class="inline">makeResampleDesc()</strong> or <strong class="inline">makeResampleInstance()</strong>. After that, run the <strong class="inline">resample()</strong> function. Use the following default measure to calculate the hamming loss:</p>
			<p class="snippet">rdesc = makeResampleDesc(method = “CV”, stratify = FALSE, iters = 3)</p>
			<p class="snippet">r = resample(learner = multilabel.lrn1, task = scene.task, resampling = rdesc,measures = list(multilabel.hamloss), show.info = FALSE)</p>
			<p class="snippet">r</p>
			<p>The output is as follows:</p>
			<p class="snippet">Resample Result</p>
			<p class="snippet">Task: multi</p>
			<p class="snippet">Learner: multilabel.binaryRelevance.classif.rpart</p>
			<p class="snippet">Aggr perf: multilabel.hamloss.test.mean=0.1244979</p>
			<p class="snippet">Runtime: 4.28345</p>
			<h3 id="_idParaDest-359"><a id="_idTextAnchor363"/>Binary Performance for Each Label</h3>
			<p>We can calculate a binary performance measure, for example, the accuracy, or the <strong class="inline">auc</strong> for each label, the <strong class="inline">getMultilabelBinaryPerformances()</strong> function is useful. We can apply this function to any multilabel prediction, for example, also on the resampled multilabel prediction. For calculating <strong class="inline">auc</strong>, we need predicted probabilities:</p>
			<p class="snippet">getMultilabelBinaryPerformances(r$pred, measures = list(acc, mmce, auc))</p>
			<p>The output is as follows:</p>
			<p class="snippet">##             acc.test.mean mmce.test.mean auc.test.mean</p>
			<p class="snippet">## Beach           0.8728708     0.12712921     0.7763484</p>
			<p class="snippet">## Sunset          0.9335272     0.06647279     0.9066371</p>
			<p class="snippet">## FallFoliage     0.9148317     0.08516826     0.8699105</p>
			<p class="snippet">## Field           0.9077690     0.09223099     0.8895795</p>
			<p class="snippet">## Mountain        0.7922725     0.20772746     0.7670873</p>
			<p class="snippet">## Urban           0.8213544     0.17864562     0.7336219</p>
			<h3 id="_idParaDest-360"><a id="_idTextAnchor364"/>Benchmarking Model</h3>
			<p>In a benchmark experiment, different learning methods are applied to one or more than a few datasets with the purpose of comparing and ranking the algorithms concerning one or more performance measures. The <strong class="inline">mlr()</strong> method offers a very robust framework to conduct such experiments and helps in keeping track of all the results of the experiment to compare.</p>
			<h3 id="_idParaDest-361"><a id="_idTextAnchor365"/>Conducting Benchmark Experiments</h3>
			<p>In our first experiment, we use the multilabel <strong class="inline">randomForestSRC</strong> and <strong class="inline">rFerns</strong> learners of the <strong class="inline">mlr()</strong> package and various measures to get our first benchmark.</p>
			<p>In the following exercise, we will explore how to conduct a benchmarking on various learners.</p>
			<h3 id="_idParaDest-362"><a id="_idTextAnchor366"/>Exercise 105: Exploring How to Conduct a Benchmarking on Various Learners</h3>
			<p>In this exercise, we will see how to conduct a benchmarking on various learners we created so far and compare the results to select the best learner (model) for the multilabel scene classification problem. This helps us organize all the results in a structured format to select the best performing model.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">First, list all learners using the following command:<p class="snippet">lrns = list(makeLearner(“multilabel.randomForestSRC”),</p><p class="snippet">            makeLearner(“multilabel.rFerns”)</p><p class="snippet">            )</p><p class="snippet">MEASURES = list(multilabel.hamloss, multilabel.f1, multilabel.subset01, multilabel.acc, multilabel.tpr, multilabel.ppv)</p></li>
				<li>Conduct the benchmark experiment:<p class="snippet">bmr = benchmark(lrns, scene.task, measures = MEASURES)</p><p>The output is as follows:</p><p class="snippet">## Exporting objects to slaves for mode socket: .mlr.slave.options</p><p class="snippet">## Mapping in parallel: mode = socket; cpus = 2; elements = 2.</p></li>
				<li>Now, execute the <strong class="inline">bmr</strong> object:<p class="snippet">bmr</p><p>Iterations of the model train will look something like the following for each learner:</p><p class="snippet">Task: multi, Learner: multilabel.rFerns</p><p class="snippet">[Resample] cross-validation iter 9: multilabel.hamloss.test.mean=0.183,multilabel.f1.test.mean=0.653,multilabel.subset01.test.mean=0.768,multilabel.acc.test.mean=0.54,multilabel.tpr.test.mean= 0.9,multilabel.ppv.test.mean=0.564</p><p class="snippet">...</p><p class="snippet">[Resample] Aggr. Result: multilabel.hamloss.test.mean=0.183,multilabel.f1.test.mean=0.663,multilabel.subset01.test.mean=0.756,multilabel.acc.test.mean=0.549,multilabel.tpr.test.mean=0.916,multilabel.ppv.test.mean=0.566</p><p>The following table demonstrates the mean of various measures on the test data:</p></li>
			</ol>
			<div>
				<div id="_idContainer304" class="IMG---Figure">
					<img src="image/C12624_09_10.jpg" alt="Figure 9.10: Mean of various measures on the test data.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.10: Mean of various measures on the test data.</h6>
			<p>This table shows that the <strong class="inline">randomForestSRC</strong> model does a slightly better job than <strong class="inline">rFerns</strong> on all the measures, primarily in the <strong class="bold">hamloss mean measure</strong>.</p>
			<h3 id="_idParaDest-363"><a id="_idTextAnchor367"/>Accessing Benchmark Results</h3>
			<p>The <strong class="inline">mlr()</strong> method provides many <strong class="inline">getBMR</strong> functions to extract useful information such as performance, predictions, leaners, and many more from the benchmark experiment object.</p>
			<h3 id="_idParaDest-364"><a id="_idTextAnchor368"/>Learner Performances</h3>
			<p>The <strong class="inline">getBMRPerformances</strong> function gives all values of all the measures defined in the benchmark in each iteration of the training. The following table lists the values for each measure using the <strong class="inline">randomForestSRC</strong> and <strong class="inline">rFerns</strong> learners.</p>
			<p class="snippet">getBMRPerformances(bmr, as.df = TRUE)</p>
			<div>
				<div id="_idContainer305" class="IMG---Figure">
					<img src="image/C12624_09_11.jpg" alt="Figure 9.11: Learner performances randomForestSRC&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.11: Learner performances randomForestSRC</h6>
			<div>
				<div id="_idContainer306" class="IMG---Figure">
					<img src="image/C12624_09_12.jpg" alt="Figure 9.12: Learner performances rFerns&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.12: Learner performances rFerns</h6>
			<h2 id="_idParaDest-365"><a id="_idTextAnchor369"/>Predictions</h2>
			<p>We could also get the predictions on the test dataset using the <strong class="inline">getBMRPredictions</strong> function. The two tables in this section show the actual and the predicted labels of a few images represented by the ID column. Observe that the predictions are not perfect, just as we would expect from the relatively low overall accuracy.</p>
			<p><strong class="bold">Predictions using randomForestSRC</strong>:</p>
			<p class="snippet">head(getBMRPredictions(bmr, as.df = TRUE))</p>
			<div>
				<div id="_idContainer307" class="IMG---Figure">
					<img src="image/C12624_09_13.jpg" alt="Figure 9.13: The actual labels.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.13: The actual labels.</h6>
			<div>
				<div id="_idContainer308" class="IMG---Figure">
					<img src="image/C12624_09_14.jpg" alt="Figure 9.14: The predicted labels.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.14: The predicted labels.</h6>
			<h3 id="_idParaDest-366"><a id="_idTextAnchor370"/>Learners and measures</h3>
			<p>The <strong class="inline">getBMRLearners</strong> function gives details about the learners used in the benchmark. Information such as hyperparameter and predict-type could be obtained using this function. Similarly, the <strong class="inline">getBMRMeasures</strong> function provides details such as best about the performance measures. The following table shows the details about the measures we used in our benchmark experiment:</p>
			<p class="snippet">getBMRLearners(bmr)</p>
			<p>The output is as follows:</p>
			<p class="snippet">## $multilabel.randomForestSRC</p>
			<p class="snippet">## Learner multilabel.randomForestSRC from package randomForestSRC</p>
			<p class="snippet">## Type: multilabel</p>
			<p class="snippet">## Name: Random Forest; Short name: rfsrc</p>
			<p class="snippet">## Class: multilabel.randomForestSRC</p>
			<p class="snippet">## Properties: missings,numerics,factors,prob,weights</p>
			<p class="snippet">## Predict-Type: response</p>
			<p class="snippet">## Hyperparameters: na.action=na.impute</p>
			<p class="snippet">## </p>
			<p class="snippet">## </p>
			<p class="snippet">## $multilabel.rFerns</p>
			<p class="snippet">## Learner multilabel.rFerns from package rFerns</p>
			<p class="snippet">## Type: multilabel</p>
			<p class="snippet">## Name: Random ferns; Short name: rFerns</p>
			<p class="snippet">## Class: multilabel.rFerns</p>
			<p class="snippet">## Properties: numerics,factors,ordered</p>
			<p class="snippet">## Predict-Type: response</p>
			<p class="snippet">## Hyperparameters:</p>
			<p>Run the <strong class="inline">getBMRMeasures(bmr)</strong> function:</p>
			<p class="snippet">getBMRMeasures(bmr)</p>
			<div>
				<div id="_idContainer309" class="IMG---Figure">
					<img src="image/C12624_09_15.jpg" alt="Figure 9.15: Learners and measures (part 1).&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.15: Learners and measures (part 1).</h6>
			<p>Figures 15 and 16 summarize the result of the <strong class="inline">getBMRMeasures(bmr)</strong> command:</p>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<img src="image/C12624_09_16.jpg" alt="Figure 9.16: Learners and measures (part 2).&#13;&#10;"/>
				</div>
			</div>
			<p class="figure">Figure 9.16: Learners and measures (part 2).</p>
			<p>Merging Benchmark Results</p>
			<p>Often, we run multiple experiments and would like to see all the benchmarks coming from the experiments into one consolidated list of values to compare the results. The <strong class="inline">mergeBenchmarkResults</strong> function helps in combining the results.</p>
			<p>Here’s the benchmark:</p>
			<p class="snippet">lrns = list(makeLearner(“multilabel.randomForestSRC”),</p>
			<p class="snippet">            makeLearner(“multilabel.rFerns”)</p>
			<p class="snippet">            )</p>
			<p class="snippet">bmr = benchmark(lrns, scene.task, measures = MEASURES)</p>
			<p class="snippet">lrn.classif.randomForest = makeLearner(“classif.randomForest”)</p>
			<p class="snippet">bmr.BR.rf = benchmark(lrn.classif.randomForest, scene.task, measures = MEASURES)</p>
			<p class="snippet">mergeBenchmarkResults(list(bmr, bmr.BR.rf))</p>
			<div>
				<div id="_idContainer311" class="IMG---Figure">
					<img src="image/C12624_09_17.jpg" alt="Figure 9.17: Merging benchmark results.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 9.17: Merging benchmark results.</h6>
			<p>Clearly, using <strong class="inline">classif.randomForest</strong> with the classifier chain wrapper produces the highest accuracy and performs well in all the other measures as well.</p>
			<h3 id="_idParaDest-367"><a id="_idTextAnchor371"/>Activity 14: Getting the Binary Performance Step with classif.C50 Learner Instead of classif.rpart</h3>
			<p>In this activity, we will revisit the entire process flow of building a model, in which we will use the <strong class="inline">makeLearner</strong> function to specify the <strong class="inline">rpart</strong> model, replacing <strong class="inline">C50</strong>. Specifically, we will rerun the entire machine learning flow, starting from the problem transformation step to getting the binary performance step with the classif.C50 learner instead of <strong class="inline">classif.rpart</strong>.</p>
			<p>Perform the following steps to complete the activity:</p>
			<ol>
				<li value="1">Define the algorithm adaptation methods.</li>
				<li>Use the problem transformation method, and change the <strong class="inline">classif.rpart</strong> learner to <strong class="inline">classif.C50</strong>.<h4>Note</h4><p class="callout">You need to install the <strong class="inline">C50</strong> package for this code to work.</p></li>
				<li>Print the learner details.</li>
				<li>Print the multilabel learner details.</li>
				<li>Train the model using the same dataset with training dataset.</li>
				<li>Print the model details.</li>
				<li>Predict the output using the C50 model we created earlier on the test dataset.</li>
				<li>Print the performance measures.</li>
				<li>Print the performance measures for the <strong class="inline">listMeasures</strong> variable.</li>
				<li>Run the resampling with the cross validation method.</li>
				<li>Print the binary performance.</li>
			</ol>
			<p>Once you complete the activity, you should see the following output:</p>
			<p class="snippet">##             acc.test.mean mmce.test.mean auc.test.mean</p>
			<p class="snippet">## Beach           0.8608226     0.13917740     0.8372448</p>
			<p class="snippet">## Sunset          0.9401745     0.05982551     0.9420085</p>
			<p class="snippet">## FallFoliage     0.9081845     0.09181554     0.9008202</p>
			<p class="snippet">## Field           0.8998754     0.10012464     0.9134458</p>
			<p class="snippet">## Mountain        0.7710843     0.22891566     0.7622767</p>
			<p class="snippet">## Urban           0.8184462     0.18155380     0.7837401</p>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 466.</p>
			<h3 id="_idParaDest-368"><a id="_idTextAnchor372"/>Working with OpenML Upload Functions</h3>
			<p>In order to improve collaboration and version control the experiments, we can upload the flows we create into OpenML using the <strong class="inline">uploadOMLFlow</strong> function:</p>
			<p class="snippet">flow.id = uploadOMLFlow(makeLearner(“multilabel.randomForestSRC”))</p>
			<p>The output is as follows:</p>
			<p class="snippet">Downloading from <a id="_idTextAnchor373"/>‘http://www.openml.org/api/v1/flow/exists/mlr.multilabel.randomForestSRC/R_3.2.2-v2.b955a5ec’ to ‘&lt;mem&gt;’.</p>
			<p class="snippet">Do you want to upload the flow? (yes|no)</p>
			<p class="snippet">Uploading flow to the server.</p>
			<p class="snippet">Downloading response to: C:\Users\Karthik\AppData\Local\Temp\Rtmpe4W4BW\file3f044abf30f2.xml</p>
			<p class="snippet">Uploading to ‘http://www.openml.org/api/v1/flow’.</p>
			<p class="snippet">Flow successfully uploaded. Flow ID: 9708</p>
			<p>We encourage students to explore the OpenML platform to find many more such functionalities, as the platform helps researchers all around the world to collaborate and share their work, making good work spread fast and help build the best model with the collective wisdom of researchers.</p>
			<h2 id="_idParaDest-369"><a id="_idTextAnchor374"/>Summary</h2>
			<p>In this chapter, we used the mlr and OpenML packages from R to build an entire machine learning workflow for solving a multilabel semantic scene classification problem. The mlr package offered a rich collection of machine learning algorithms and evaluation measures that helped us in quick implementation and facilitated a faster experimentation process to get the best model for the problem. The package also offered many wrapper functions to handle the multilabel problem. Building real-world machine learning models using a robust framework such as the one in mlr helps in speeding the implementation and provides a structure to the complete project. Further, using OpenML, we could reproduce a research work using the already available dataset and code, and then modify it according to our need. Such a platform offers the ability to collaborate at scale with researchers all over the world. At the end, we could also upload our own machine learning flows with others for them to pick it up from where we left.</p>
			<p>In this book, our focus was to teach supervised learning in R programming language. Supervised learning is a class of algorithms where we are provided with labeled observation of data. Exploratory Data Analysis (EDA) methods help in understanding the dataset well, and the SCQ framework is used to design the problem precisely. The features are chosen on the basis of the problem design and an appropriate supervised learning model is selected after many rounds of experiments and evaluation. We then learned how to deploy machine learning models in production environment, which could be used by an application team in a business. Also, in cases where the dataset has hundreds of features, we used feature reduction and selection techniques.</p>
			<p>We would like to <a id="_idTextAnchor375"/>emphasize that in any machine learning project, beyond a certain point (could be defined such as 3 months of effort from the start of project or 100 trial runs with different combinations), we should stop and ask whether or not what we have done so far could be deployed in a production environment. If the answer is yes, deploy it and keep monitoring the response for any abnormality and improvements. If it’s a no, go back to the drawing boards and start over (obviously if such a luxury is given). Machine learning in stages such as hyperparameter fine-tuning and model selection is an art. A lot of trial-and-error experiments are required to come out with the best model.</p>
		</div>
	</body></html>