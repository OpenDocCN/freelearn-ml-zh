<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer014">
<h1 id="_idParaDest-15"><em class="italic"><a id="_idTextAnchor014"/>Chapter 1</em>: Examining the Distribution of Features and Targets</h1>
<p>Machine learning writing and instruction are often algorithm-focused. Sometimes, this gives the impression that all we have to do is choose the right model and that organization-changing insights will follow. But the best place to begin a machine learning project is with an understanding of how the features and targets we will use are distributed. </p>
<p>It is important to make room for the same kind of learning from data that has been central to our work as analysts for decades – studying the distribution of variables, identifying anomalies, and examining bivariate relationships –  even as we focus more and more on the accuracy of our predictions.</p>
<p>We will explore tools for doing so in the first three chapters of this book, while also considering implications for model building.</p>
<p>In this chapter, we will use common NumPy and pandas techniques to get a better sense of the attributes of our data. We want to know how key features are distributed before we do any predictive analyses. We also want to know the central tendency, shape, and spread of the distribution of each continuous feature and have a count for each value for categorical features. We will take advantage of very handy NumPy and pandas tools for generating summary statistics, such as the mean, min, and max, as well as standard deviation.</p>
<p>After that, we will create visualizations of key features, including histograms and boxplots, to give us a better sense of the distribution of each feature than we can get by just looking at summary statistics. We will hint at the implications of feature distribution for data transformation, encoding and scaling, and the modeling that we will be doing in subsequent chapters with the same data.</p>
<p>Specifically, in this chapter, we are going to cover the following topics:</p>
<ul>
<li>Subsetting data</li>
<li>Generating frequencies for categorical features</li>
<li>Generating summary statistics for continuous features</li>
<li>Identifying extreme values and outliers in univariate analysis</li>
<li>Using histograms, boxplots, and violin plots to examine the distribution of continuous features</li>
</ul>
<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Technical requirements</h1>
<p>This chapter will rely heavily on the pandas, NumPy, and Matplotlib libraries, but you don't require any prior knowledge of these. If you have installed Python from a scientific distribution, such as Anaconda or WinPython, then these libraries are probably already installed. If you need to install one of them to run the code in this chapter, you can run <strong class="source-inline">pip install [package name]</strong> from a terminal.</p>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Subsetting data</h1>
<p>Almost<a id="_idIndexMarker000"/> every statistical modeling project I have worked on has required removing some data from the analysis. Often, this is because of missing values or outliers. Sometimes, there are theoretical reasons for limiting our analysis to a subset of the data. For example, we have weather data going back to 1600, but our analysis goals only involve changes in weather since 1900. Fortunately, the subsetting tools in pandas are quite powerful and flexible. We will work with data from the United States <strong class="bold">National Longitudinal Survey</strong> (<strong class="bold">NLS</strong>) of Youth in this section.</p>
<p class="callout-heading">Note</p>
<p class="callout">The NLS of Youth<a id="_idIndexMarker001"/> is conducted by the United States Bureau of Labor Statistics. This survey started with a cohort of individuals in 1997 who were born between 1980 and 1985, with annual follow-ups each year through 2017. For this recipe, I pulled 89 variables on grades, employment, income, and attitudes toward government from the hundreds of data items on the survey. Separate files for SPSS, Stata, and SAS can be downloaded from the repository. The NLS data is available for public use at <a href="https://www.nlsinfo.org/investigator/pages/search">https://www.nlsinfo.org/investigator/pages/search</a>.</p>
<p>Let's start <a id="_idIndexMarker002"/>subsetting the <a id="_idIndexMarker003"/>data using pandas:</p>
<ol>
<li>We will start by loading the NLS data. We also set an index:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">nls97 = pd.read_csv("data/nls97.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p></li>
<li>Let's select a few columns from the NLS data. The following code creates a new DataFrame that contains some demographic and employment data. A useful feature of pandas is that the new DataFrame retains the index of the old DataFrame, as shown here:<p class="source-code">democols = ['gender','birthyear','maritalstatus',</p><p class="source-code"> 'weeksworked16','wageincome','highestdegree']</p><p class="source-code">nls97demo = nls97[democols]</p><p class="source-code">nls97demo.index.name</p><p class="source-code"><strong class="bold">'personid'</strong></p></li>
<li>We can use slicing to select rows by position. <strong class="source-inline">nls97demo[1000:1004]</strong> selects every row, starting from the row indicated by the integer to the left of the colon (<strong class="source-inline">1000</strong>, in this case) up to, but not including, the row indicated by the integer to the right of the colon (<strong class="source-inline">1004</strong>). The row at <strong class="source-inline">1000</strong> is the 1,001st row because of zero-based indexing. Each row appears as a column in the output since we have transposed the resulting DataFrame:<p class="source-code">nls97demo[1000:1004].T</p><p class="source-code"><strong class="bold">personid      195884       195891        195970\</strong></p><p class="source-code"><strong class="bold">gender        Male         Male          Female</strong></p><p class="source-code"><strong class="bold">birthyear     1981         1980          1982</strong></p><p class="source-code"><strong class="bold">maritalstatus NaN          Never-married Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16 NaN          53            53</strong></p><p class="source-code"><strong class="bold">wageincome    NaN          14,000        52,000   </strong></p><p class="source-code"><strong class="bold">highestdegree 4.Bachelors  2.High School 4.Bachelors</strong></p><p class="source-code"><strong class="bold">personid       195996  </strong></p><p class="source-code"><strong class="bold">gender         Female  </strong></p><p class="source-code"><strong class="bold">birthyear      1980  </strong></p><p class="source-code"><strong class="bold">maritalstatus  NaN  </strong></p><p class="source-code"><strong class="bold">weeksworked16  NaN  </strong></p><p class="source-code"><strong class="bold">wageincome     NaN</strong></p><p class="source-code"><strong class="bold">highestdegree  3.Associates</strong>    </p></li>
<li>We can also<a id="_idIndexMarker004"/> skip rows over the interval by setting a value for<a id="_idIndexMarker005"/> the step after the second colon. The default value for the step is 1. The value for the following step is 2, which means that every other row between <strong class="source-inline">1000</strong> and <strong class="source-inline">1004</strong> will be selected:<p class="source-code">nls97demo[1000:1004:2].T</p><p class="source-code"><strong class="bold">personid        195884       195970</strong></p><p class="source-code"><strong class="bold">gender          Male         Female</strong></p><p class="source-code"><strong class="bold">birthyear       1981         1982</strong></p><p class="source-code"><strong class="bold">maritalstatus   NaN          Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16   NaN          53</strong></p><p class="source-code"><strong class="bold">wageincome      NaN          52,000</strong></p><p class="source-code"><strong class="bold">highestdegree   4.Bachelors  4. Bachelors</strong></p></li>
<li>If we do <a id="_idIndexMarker006"/>not include a value to the left of the colon, row <a id="_idIndexMarker007"/>selection will start with the first row. Notice that this returns the same DataFrame as the <strong class="source-inline">head</strong> method does:<p class="source-code">nls97demo[:3].T</p><p class="source-code"><strong class="bold">personid       100061         100139          100284</strong></p><p class="source-code"><strong class="bold">gender         Female         Male            Male</strong></p><p class="source-code"><strong class="bold">birthyear      1980           1983            1984</strong></p><p class="source-code"><strong class="bold">maritalstatus  Married        Married         Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16  48             53              47</strong></p><p class="source-code"><strong class="bold">wageincome     12,500         120,000         58,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2. High School  0.None</strong></p><p class="source-code">nls97demo.head(3).T</p><p class="source-code"><strong class="bold">personid       100061         100139         100284</strong></p><p class="source-code"><strong class="bold">gender         Female         Male           Male</strong></p><p class="source-code"><strong class="bold">birthyear      1980           1983           1984</strong></p><p class="source-code"><strong class="bold">maritalstatus  Married        Married        Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16  48             53             47</strong></p><p class="source-code"><strong class="bold">wageincome     12,500         120,000        58,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2.High School  0. None</strong></p></li>
<li>If we use a <a id="_idIndexMarker008"/>negative number, -n, to the left of the colon, the <a id="_idIndexMarker009"/>last n rows of the DataFrame will be returned. This returns the same DataFrame as the <strong class="source-inline">tail</strong> method does:<p class="source-code"> nls97demo[-3:].T</p><p class="source-code"><strong class="bold">personid       999543          999698        999963</strong></p><p class="source-code"><strong class="bold">gender         Female         Female         Female</strong></p><p class="source-code"><strong class="bold">birthyear      1984           1983           1982</strong></p><p class="source-code"><strong class="bold">maritalstatus  Divorced       Never-married  Married</strong></p><p class="source-code"><strong class="bold">weeksworked16  0              0              53</strong></p><p class="source-code"><strong class="bold">wageincome     NaN            NaN            50,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2.High School  4. Bachelors</strong></p><p class="source-code"> nls97demo.tail(3).T</p><p class="source-code"><strong class="bold">personid       999543         999698         999963</strong></p><p class="source-code"><strong class="bold">gender         Female         Female         Female</strong></p><p class="source-code"><strong class="bold">birthyear      1984           1983           1982</strong></p><p class="source-code"><strong class="bold">maritalstatus  Divorced       Never-married  Married</strong></p><p class="source-code"><strong class="bold">weeksworked16  0              0              53</strong></p><p class="source-code"><strong class="bold">wageincome     NaN            NaN            50,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2.High School  4. Bachelors</strong></p></li>
<li>We can select rows by index value using the <strong class="source-inline">loc</strong> accessor. Recall that for the <strong class="source-inline">nls97demo</strong> DataFrame, the index is <strong class="source-inline">personid</strong>. We can pass a list of the index labels to the <strong class="source-inline">loc</strong> accessor, such as <strong class="source-inline">loc[[195884,195891,195970]]</strong>, to get the rows associated with those labels. We can also pass a lower and upper bound of index labels, such as <strong class="source-inline">loc[195884:195970]</strong>, to retrieve the indicated rows:<p class="source-code"> nls97demo.loc[[195884,195891,195970]].T</p><p class="source-code"><strong class="bold">personid       195884       195891         195970</strong></p><p class="source-code"><strong class="bold">gender         Male         Male           Female</strong></p><p class="source-code"><strong class="bold">birthyear      1981         1980           1982</strong></p><p class="source-code"><strong class="bold">maritalstatus  NaN          Never-married  Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16  NaN          53             53</strong></p><p class="source-code"><strong class="bold">wageincome     NaN          14,000         52,000</strong></p><p class="source-code"><strong class="bold">highestdegree  4.Bachelors  2.High School  4.Bachelors</strong></p><p class="source-code"> nls97demo.loc[195884:195970].T</p><p class="source-code"><strong class="bold">personid       195884       195891         195970</strong></p><p class="source-code"><strong class="bold">gender         Male         Male           Female</strong></p><p class="source-code"><strong class="bold">birthyear      1981         1980           1982</strong></p><p class="source-code"><strong class="bold">maritalstatus  NaN          Never-married  Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16  NaN          53             53</strong></p><p class="source-code"><strong class="bold">wageincome     NaN          14,000         52,000</strong></p><p class="source-code"><strong class="bold">highestdegree  4.Bachelors  2.High School  4.Bachelors</strong></p></li>
<li>To select rows<a id="_idIndexMarker010"/> by position, rather than by index label, we <a id="_idIndexMarker011"/>can use the <strong class="source-inline">iloc</strong> accessor. We can pass a list of position numbers, such as <strong class="source-inline">iloc[[0,1,2]]</strong>, to the accessor to get the rows at those positions. We can pass a range, such as <strong class="source-inline">iloc[0:3]</strong>, to get rows between the lower and upper bound, not including the row at the upper bound. We <a id="_idIndexMarker012"/>can also use the <strong class="source-inline">iloc</strong> accessor to select <a id="_idIndexMarker013"/>the last n rows. <strong class="source-inline">iloc[-3:]</strong> selects the last three rows:<p class="source-code"> nls97demo.iloc[[0,1,2]].T</p><p class="source-code"><strong class="bold">personid       100061         100139         100284</strong></p><p class="source-code"><strong class="bold">gender         Female         Male           Male</strong></p><p class="source-code"><strong class="bold">birthyear      1980           1983           1984</strong></p><p class="source-code"><strong class="bold">maritalstatus  Married        Married        Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16  48             53             47</strong></p><p class="source-code"><strong class="bold">wageincome     12,500         120,000        58,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2.High School  0. None</strong></p><p class="source-code"> nls97demo.iloc[0:3].T</p><p class="source-code"><strong class="bold">personid       100061         100139         100284</strong></p><p class="source-code"><strong class="bold">gender         Female         Male           Male</strong></p><p class="source-code"><strong class="bold">birthyear      1980           1983           1984</strong></p><p class="source-code"><strong class="bold">maritalstatus  Married        Married        Never-married</strong></p><p class="source-code"><strong class="bold">weeksworked16  48             53             47</strong></p><p class="source-code"><strong class="bold">wageincome     12,500         120,000        58,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2.High School  0. None</strong></p><p class="source-code"> nls97demo.iloc[-3:].T</p><p class="source-code"><strong class="bold">personid       999543         999698         999963</strong></p><p class="source-code"><strong class="bold">gender         Female         Female         Female</strong></p><p class="source-code"><strong class="bold">birthyear      1984           1983           1982</strong></p><p class="source-code"><strong class="bold">maritalstatus  Divorced       Never-married  Married</strong></p><p class="source-code"><strong class="bold">weeksworked16  0              0              53</strong></p><p class="source-code"><strong class="bold">wageincome     NaN            NaN            50,000</strong></p><p class="source-code"><strong class="bold">highestdegree  2.High School  2.High School  4. Bachelors</strong></p></li>
</ol>
<p>Often, we need to<a id="_idIndexMarker014"/> select rows based on a column value or the <a id="_idIndexMarker015"/>values of several columns. We can do this in pandas by using Boolean indexing. Here, we pass a vector of Boolean values (which can be a Series) to the <strong class="source-inline">loc</strong> accessor or the bracket operator. The Boolean vector needs to have the same index as the DataFrame.</p>
<ol>
<li value="9">Let's try this using the <strong class="source-inline">nightlyhrssleep</strong> column on the NLS DataFrame. We want a Boolean Series that is <strong class="source-inline">True</strong> for people who sleep 6 or fewer hours a night (the 33rd percentile) and <strong class="source-inline">False</strong> if <strong class="source-inline">nightlyhrssleep</strong> is greater than 6 or is missing. <strong class="source-inline">sleepcheckbool = nls97.nightlyhrssleep&lt;=lowsleepthreshold</strong> creates the boolean Series. If we display the first few values of <strong class="source-inline">sleepcheckbool</strong>, we will see that we are getting the expected values. We can also confirm that the <strong class="source-inline">sleepcheckbool</strong> index is equal to the <strong class="source-inline">nls97</strong> index:<p class="source-code">nls97.nightlyhrssleep.head()</p><p class="source-code"><strong class="bold">personid</strong></p><p class="source-code"><strong class="bold">100061     6</strong></p><p class="source-code"><strong class="bold">100139     8</strong></p><p class="source-code"><strong class="bold">100284     7</strong></p><p class="source-code"><strong class="bold">100292     nan</strong></p><p class="source-code"><strong class="bold">100583     6</strong></p><p class="source-code"><strong class="bold">Name: nightlyhrssleep, dtype: float64</strong></p><p class="source-code">lowsleepthreshold = nls97.nightlyhrssleep.quantile(0.33)</p><p class="source-code">lowsleepthreshold</p><p class="source-code"><strong class="bold">6.0</strong></p><p class="source-code">sleepcheckbool = nls97.nightlyhrssleep&lt;=lowsleepthreshold</p><p class="source-code">sleepcheckbool.head()</p><p class="source-code"><strong class="bold">personid</strong></p><p class="source-code"><strong class="bold">100061    True</strong></p><p class="source-code"><strong class="bold">100139    False</strong></p><p class="source-code"><strong class="bold">100284    False</strong></p><p class="source-code"><strong class="bold">100292    False</strong></p><p class="source-code"><strong class="bold">100583    True</strong></p><p class="source-code"><strong class="bold">Name: nightlyhrssleep, dtype: bool</strong></p><p class="source-code">sleepcheckbool.index.equals(nls97.index)</p><p class="source-code"><strong class="bold">True</strong></p></li>
</ol>
<p>Since the <strong class="source-inline">sleepcheckbool</strong> Series has the same index as <strong class="source-inline">nls97</strong>, we can just pass it to<a id="_idIndexMarker016"/> the <strong class="source-inline">loc</strong> accessor to create a DataFrame<a id="_idIndexMarker017"/> containing people who sleep 6 hours or less a night. This is a little pandas magic here. It handles the index alignment for us:</p>
<p class="source-code">lowsleep = nls97.loc[sleepcheckbool]</p>
<p class="source-code">lowsleep.shape</p>
<p class="source-code"><strong class="bold">(3067, 88)</strong></p>
<ol>
<li value="10">We could have created the <strong class="source-inline">lowsleep</strong> subset of our data in one step, which is what we would typically do unless we need the Boolean Series for some other purpose:<p class="source-code">lowsleep = nls97.loc[nls97.nightlyhrssleep&lt;=lowsleepthreshold]</p><p class="source-code">lowsleep.shape</p><p class="source-code"><strong class="bold">(3067, 88)</strong></p></li>
<li>We can pass more <a id="_idIndexMarker018"/>complex conditions to the <strong class="source-inline">loc</strong> accessor<a id="_idIndexMarker019"/> and evaluate the values of multiple columns. For example, we can select rows where <strong class="source-inline">nightlyhrssleep</strong> is less than or equal to the threshold and <strong class="source-inline">childathome</strong> (number of children living at home) is greater than or equal to <strong class="source-inline">3</strong>:<p class="source-code">lowsleep3pluschildren = \</p><p class="source-code">  nls97.loc[(nls97.nightlyhrssleep&lt;=lowsleepthreshold)</p><p class="source-code">    &amp; (nls97.childathome&gt;=3)]</p><p class="source-code">lowsleep3pluschildren.shape</p><p class="source-code"><strong class="bold">(623, 88)</strong></p></li>
</ol>
<p>Each condition in <strong class="source-inline">nls97.loc[(nls97.nightlyhrssleep&lt;=lowsleepthreshold) &amp; (nls97.childathome&gt;3)]</strong> is placed in parentheses. An error will be generated if the parentheses are excluded. The <strong class="source-inline">&amp;</strong> operator is the equivalent of <strong class="source-inline">and</strong> in standard Python, meaning that <em class="italic">both</em> conditions have to be <strong class="source-inline">True</strong> for the row to be selected. We could have used <strong class="source-inline">|</strong> for <strong class="source-inline">or</strong> if we wanted to select the row if <em class="italic">either</em> condition was <strong class="source-inline">True</strong>.</p>
<ol>
<li value="12">Finally, we can select rows and columns at the same time. The expression to the left of the comma selects rows, while the list to the right of the comma selects columns:<p class="source-code">lowsleep3pluschildren = \</p><p class="source-code">  nls97.loc[(nls97.nightlyhrssleep&lt;=lowsleepthreshold)</p><p class="source-code">    &amp; (nls97.childathome&gt;=3),</p><p class="source-code">    ['nightlyhrssleep','childathome']]</p><p class="source-code">lowsleep3pluschildren.shape</p><p class="source-code"><strong class="bold">(623, 2)</strong></p></li>
</ol>
<p>We used three different tools to select columns and rows from a pandas DataFrame in the last two sections: the <strong class="source-inline">[]</strong> bracket operator and two pandas-specific accessors, <strong class="source-inline">loc</strong> and <strong class="source-inline">iloc</strong>. This will be a little confusing if you are new to pandas, but it becomes clear which tool to use in which situation after just a few months. If you came to pandas with a fair bit of Python and NumPy experience, you will likely find the <strong class="source-inline">[]</strong> operator most familiar. However, the pandas documentation recommends against using the <strong class="source-inline">[]</strong> operator for production code. The <strong class="source-inline">loc</strong> accessor<a id="_idIndexMarker020"/> is used for selecting rows by Boolean indexing or by index label, while the <strong class="source-inline">iloc</strong> accessor is used for selecting rows by row <a id="_idIndexMarker021"/>number. </p>
<p>This section was a brief primer on selecting columns and rows with pandas. Although we did not go into too much detail on this, most of what you need to know to subset data was covered, as well as everything you need to know to understand the pandas-specific material in the rest of this book. We will start putting some of that to work in the next two sections by creating frequencies and summary statistics for our features.</p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Generating frequencies for categorical features</h1>
<p>Categorical features <a id="_idIndexMarker022"/>can be either nominal or<a id="_idIndexMarker023"/> ordinal. <strong class="bold">Nominal</strong> features, such as gender, species name, or country, have a limited<a id="_idIndexMarker024"/> number of possible values, and are either strings or are numerical without having any intrinsic numerical meaning. For example, if country is represented by 1 for Afghanistan, 2 for Albania, and so on, the data is numerical but it does not make sense to perform arithmetic operations on those values.</p>
<p><strong class="bold">Ordinal</strong> features<a id="_idIndexMarker025"/> also have a limited number of possible values but are different from nominal features in that the order of the values matters. A <strong class="bold">Likert scale</strong> rating (ranging from 1 for very unlikely to 5 for very likely) is an example of an ordinal<a id="_idIndexMarker026"/> feature. Nonetheless, arithmetic operations would not typically make sense because there is no uniform and meaningful distance between values.</p>
<p>Before we begin <a id="_idIndexMarker027"/>modeling, we want to have counts of <a id="_idIndexMarker028"/>all the possible values for the categorical features we may use. This is typically referred to as a one-way frequency distribution. Fortunately, pandas makes this very easy to do. We can quickly select columns from a pandas DataFrame and use the <strong class="source-inline">value_counts</strong> method to generate counts for each categorical value:</p>
<ol>
<li value="1">Let's load the NLS data, create a DataFrame that contains just the first 20 columns of the data, and look at the data types:<p class="source-code">nls97 = pd.read_csv("data/nls97.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p><p class="source-code">nls97abb = nls97.iloc[:,:20]</p><p class="source-code">nls97abb.dtypes</p><p class="source-code"><strong class="bold">gender                   object</strong></p><p class="source-code"><strong class="bold">birthmonth               int64</strong></p><p class="source-code"><strong class="bold">birthyear                int64</strong></p><p class="source-code"><strong class="bold">highestgradecompleted    float64</strong></p><p class="source-code"><strong class="bold">maritalstatus            object</strong></p><p class="source-code"><strong class="bold">childathome              float64</strong></p><p class="source-code"><strong class="bold">childnotathome           float64</strong></p><p class="source-code"><strong class="bold">wageincome               float64</strong></p><p class="source-code"><strong class="bold">weeklyhrscomputer        object</strong></p><p class="source-code"><strong class="bold">weeklyhrstv              object</strong></p><p class="source-code"><strong class="bold">nightlyhrssleep          float64</strong></p><p class="source-code"><strong class="bold">satverbal                float64</strong></p><p class="source-code"><strong class="bold">satmath                  float64</strong></p><p class="source-code"><strong class="bold">gpaoverall               float64</strong></p><p class="source-code"><strong class="bold">gpaenglish               float64</strong></p><p class="source-code"><strong class="bold">gpamath                  float64</strong></p><p class="source-code"><strong class="bold">gpascience               float64</strong></p><p class="source-code"><strong class="bold">highestdegree            object</strong></p><p class="source-code"><strong class="bold">govprovidejobs           object</strong></p><p class="source-code"><strong class="bold">govpricecontrols         object</strong></p><p class="source-code"><strong class="bold">dtype: object</strong></p><p class="callout-heading">Note</p><p class="callout">Recall from the previous section how column and row selection works with the <strong class="source-inline">loc</strong> and <strong class="source-inline">iloc</strong> accessors. The colon to the left of the comma indicates that we want all the rows, while <strong class="source-inline">:20</strong> to the right of the comma gets us the first 20 columns.</p></li>
<li>All of the <a id="_idIndexMarker029"/>object type columns in the <a id="_idIndexMarker030"/>preceding code are categorical. We can use <strong class="source-inline">value_counts</strong> to see the counts for each value for <strong class="source-inline">maritalstatus</strong>. We can also use <strong class="source-inline">dropna=False</strong> to get <strong class="source-inline">value_counts</strong> to show the missing values (<strong class="source-inline">NaN</strong>):<p class="source-code">nls97abb.maritalstatus.value_counts(dropna=False)</p><p class="source-code"><strong class="bold">Married          3066</strong></p><p class="source-code"><strong class="bold">Never-married    2766</strong></p><p class="source-code"><strong class="bold">NaN              2312</strong></p><p class="source-code"><strong class="bold">Divorced         663</strong></p><p class="source-code"><strong class="bold">Separated        154</strong></p><p class="source-code"><strong class="bold">Widowed          23</strong></p><p class="source-code"><strong class="bold">Name: maritalstatus, dtype: int64</strong></p></li>
<li>If we just want the number of missing values, we can chain the <strong class="source-inline">isnull</strong> and <strong class="source-inline">sum</strong> methods. <strong class="source-inline">isnull</strong> returns a Boolean Series containing <strong class="source-inline">True</strong> values when <strong class="source-inline">maritalstatus</strong> is missing and <strong class="source-inline">False</strong> otherwise. <strong class="source-inline">sum</strong> then counts the number of <strong class="source-inline">True</strong> values, since it will interpret <strong class="source-inline">True</strong> values as 1 and <strong class="source-inline">False</strong> values as 0:<p class="source-code">nls97abb.maritalstatus.isnull().sum()</p><p class="source-code"><strong class="bold">2312</strong></p></li>
<li>You have <a id="_idIndexMarker031"/>probably noticed that <a id="_idIndexMarker032"/>the <strong class="source-inline">maritalstatus</strong> values were sorted by frequency by default. You can sort them alphabetically by values by sorting the index. We can do this by taking advantage of the fact that <strong class="source-inline">value_counts</strong> returns a Series with the values as the index:<p class="source-code">marstatcnt = nls97abb.maritalstatus.value_counts(dropna=False)</p><p class="source-code">type(marstatcnt)</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.series.Series'&gt;</strong></p><p class="source-code"><strong class="bold">marstatcnt.index</strong></p><p class="source-code"><strong class="bold">Index(['Married', 'Never-married', nan, 'Divorced', 'Separated', 'Widowed'], dtype='object')</strong></p></li>
<li>To sort the index, we just need to call <strong class="source-inline">sort_index</strong>:<p class="source-code">marstatcnt.sort_index()</p><p class="source-code"><strong class="bold">Divorced         663</strong></p><p class="source-code"><strong class="bold">Married          3066</strong></p><p class="source-code"><strong class="bold">Never-married    2766</strong></p><p class="source-code"><strong class="bold">Separated        154</strong></p><p class="source-code"><strong class="bold">Widowed          23</strong></p><p class="source-code"><strong class="bold">NaN              2312</strong></p><p class="source-code"><strong class="bold">Name: maritalstatus, dtype: int64</strong></p></li>
<li>Of course, we could have gotten the same results in one step with <strong class="source-inline">nls97.maritalstatus.value_counts(dropna=False).sort_index()</strong>. We can also show ratios instead of counts by setting <strong class="source-inline">normalize</strong> to <strong class="source-inline">True</strong>. In the following code, we can see that 34% of the responses were <strong class="source-inline">Married</strong> (notice that we did not set <strong class="source-inline">dropna</strong> to <strong class="source-inline">True</strong>, so missing values have been excluded):<p class="source-code">nls97.maritalstatus.\</p><p class="source-code">  value_counts(normalize=True, dropna=False).\</p><p class="source-code">     sort_index()</p><p class="source-code"> </p><p class="source-code"><strong class="bold">Divorced             0.07</strong></p><p class="source-code"><strong class="bold">Married              0.34</strong></p><p class="source-code"><strong class="bold">Never-married        0.31</strong></p><p class="source-code"><strong class="bold">Separated            0.02</strong></p><p class="source-code"><strong class="bold">Widowed              0.00</strong></p><p class="source-code"><strong class="bold">NaN                  0.26</strong></p><p class="source-code"><strong class="bold">Name: maritalstatus, dtype: float64</strong></p></li>
<li>pandas has a<a id="_idIndexMarker033"/> category data type that can<a id="_idIndexMarker034"/> store data much more efficiently than the object data type when a column has a limited number of values. Since we already know that all of our object columns contain categorical data, we should convert those columns into the category data type. In the following code, we're creating a list that contains the column names for the object columns, <strong class="source-inline">catcols</strong>. Then, we're looping through those columns and using <strong class="source-inline">astype</strong> to change the data type to <strong class="source-inline">category</strong>:<p class="source-code">catcols = nls97abb.select_dtypes(include=["object"]).columns</p><p class="source-code">for col in nls97abb[catcols].columns:</p><p class="source-code">...      nls97abb[col] = nls97abb[col].astype('category')</p><p class="source-code">... </p><p class="source-code">nls97abb[catcols].dtypes</p><p class="source-code"><strong class="bold">gender                   category</strong></p><p class="source-code"><strong class="bold">maritalstatus            category</strong></p><p class="source-code"><strong class="bold">weeklyhrscomputer        category</strong></p><p class="source-code"><strong class="bold">weeklyhrstv              category</strong></p><p class="source-code"><strong class="bold">highestdegree            category</strong></p><p class="source-code"><strong class="bold">govprovidejobs           category</strong></p><p class="source-code"><strong class="bold">govpricecontrols         category</strong></p><p class="source-code"><strong class="bold">dtype: object</strong></p></li>
<li>Let's check <a id="_idIndexMarker035"/>our category features for missing <a id="_idIndexMarker036"/>values. There are no missing values for <strong class="source-inline">gender</strong> and very few for <strong class="source-inline">highestdegree</strong>. But the overwhelming majority of values for <strong class="source-inline">govprovidejobs</strong> (the government should provide jobs) and <strong class="source-inline">govpricecontrols</strong> (the government should control prices) are missing. This means that those features probably won't be useful for most modeling:<p class="source-code">nls97abb[catcols].isnull().sum()</p><p class="source-code"><strong class="bold">gender               0</strong></p><p class="source-code"><strong class="bold">maritalstatus        2312</strong></p><p class="source-code"><strong class="bold">weeklyhrscomputer    2274</strong></p><p class="source-code"><strong class="bold">weeklyhrstv          2273</strong></p><p class="source-code"><strong class="bold">highestdegree        31</strong></p><p class="source-code"><strong class="bold">govprovidejobs       7151</strong></p><p class="source-code"><strong class="bold">govpricecontrols     7125</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>We can generate <a id="_idIndexMarker037"/>frequencies for<a id="_idIndexMarker038"/> multiple features at once by passing a <strong class="source-inline">value_counts</strong> call to <strong class="source-inline">apply</strong>. We can use <strong class="source-inline">filter</strong> to select the columns that we want – in this case, all the columns with <em class="italic">gov</em> in their name. Note that the missing values for each feature have been omitted since we did not set <strong class="source-inline">dropna</strong> to <strong class="source-inline">False</strong>:<p class="source-code"> nls97abb.filter(like="gov").apply(pd.value_counts, normalize=True)</p><p class="source-code">                 <strong class="bold">govprovidejobs    govpricecontrols</strong></p><p class="source-code"><strong class="bold">1. Definitely              0.25                0.54</strong></p><p class="source-code"><strong class="bold">2. Probably                0.34                0.33</strong></p><p class="source-code"><strong class="bold">3. Probably not            0.25                0.09</strong></p><p class="source-code"><strong class="bold">4. Definitely not          0.16                0.04</strong></p></li>
<li>We can use the same frequencies on a subset of our data. If, for example, we want to see the responses of only married people to the government role questions, we can do that subsetting by placing <strong class="source-inline">nls97abb[nls97abb.maritalstatus=="Married"]</strong> before <strong class="source-inline">filter</strong>:<p class="source-code"> nls97abb.loc[nls97abb.maritalstatus=="Married"].\</p><p class="source-code"> filter(like="gov").\</p><p class="source-code">   apply(pd.value_counts, normalize=True)</p><p class="source-code">                 <strong class="bold">govprovidejobs    govpricecontrols</strong></p><p class="source-code"><strong class="bold">1. Definitely              0.17                0.46</strong></p><p class="source-code"><strong class="bold">2. Probably                0.33                0.38</strong></p><p class="source-code"><strong class="bold">3. Probably not            0.31                0.11</strong></p><p class="source-code"><strong class="bold">4. Definitely not          0.18                0.05</strong></p></li>
<li>Since, in this case, there were only two <em class="italic">gov</em> columns, it may have been easier to do the following:<p class="source-code"> nls97abb.loc[nls97abb.maritalstatus=="Married",</p><p class="source-code">   ['govprovidejobs','govpricecontrols']].\</p><p class="source-code">   apply(pd.value_counts, normalize=True)</p><p class="source-code">                  <strong class="bold">govprovidejobs     govpricecontrols</strong></p><p class="source-code"><strong class="bold">1. Definitely               0.17                 0.46</strong></p><p class="source-code"><strong class="bold">2. Probably                 0.33                 0.38</strong></p><p class="source-code"><strong class="bold">3. Probably not             0.31                 0.11</strong></p><p class="source-code"><strong class="bold">4. Definitely not           0.18                 0.05</strong></p></li>
</ol>
<p>Nonetheless, it <a id="_idIndexMarker039"/>will often be easier to <a id="_idIndexMarker040"/>use <strong class="source-inline">filter</strong> since it is not unusual to have to do the same cleaning or exploration task on groups of features with similar names.</p>
<p>There are times when we may want to model a continuous or discrete feature as categorical. The NLS DataFrame contains <strong class="source-inline">highestgradecompleted</strong>. A year increase from 5 to 6 may not be as important as that from 11 to 12 in terms of its impact on a target. Let's create a dichotomous feature instead – that is, 1 when the person has completed 12 or more grades, 0 if they have completed less than that, and missing when <strong class="source-inline">highestgradecompleted</strong> is missing.</p>
<ol>
<li value="12">We need to do a little bit of cleaning up first, though. <strong class="source-inline">highestgradecompleted</strong> has two logical missing values – an actual NaN value that pandas recognizes as missing and a 95 value that the survey designers intend for us to also treat as missing for most use cases. Let's use <strong class="source-inline">replace</strong> to fix that before moving on:<p class="source-code">nls97abb.highestgradecompleted.\</p><p class="source-code">  replace(95, np.nan, inplace=True)</p></li>
<li>We can use <a id="_idIndexMarker041"/>NumPy's <strong class="source-inline">where</strong> function<a id="_idIndexMarker042"/> to assign values to <strong class="source-inline">highschoolgrad</strong> based on the values of <strong class="source-inline">highestgradecompleted</strong>. If <strong class="source-inline">highestgradecompleted</strong> is null (<strong class="source-inline">NaN</strong>), we assign <strong class="source-inline">NaN</strong> to our new column, <strong class="source-inline">highschoolgrad</strong>. If the value for <strong class="source-inline">highestgradecompleted</strong> is not null, the next clause tests for a value less than 12, setting <strong class="source-inline">highschoolgrad</strong> to 0 if that is true, and to 1 otherwise. We can confirm that the new column, <strong class="source-inline">highschoolgrad</strong>, contains the values we want by using <strong class="source-inline">groupby</strong> to get the min and max values of <strong class="source-inline">highestgradecompleted</strong> at each level of <strong class="source-inline">highschoolgrad</strong>:<p class="source-code">nls97abb['highschoolgrad'] = \</p><p class="source-code">  np.where(nls97abb.highestgradecompleted.isnull(),np.nan, \</p><p class="source-code">  np.where(nls97abb.highestgradecompleted&lt;12,0,1))</p><p class="source-code"> </p><p class="source-code">nls97abb.groupby(['highschoolgrad'], dropna=False) \</p><p class="source-code">  ['highestgradecompleted'].agg(['min','max','size'])</p><p class="source-code">                  <strong class="bold">min       max       size</strong></p><p class="source-code"><strong class="bold">highschoolgrad                </strong></p><p class="source-code"><strong class="bold">0                   5        11       1231</strong></p><p class="source-code"><strong class="bold">1                  12        20       5421</strong></p><p class="source-code"><strong class="bold">nan               nan       nan       2332</strong></p><p class="source-code"> nls97abb['highschoolgrad'] = \</p><p class="source-code">...  nls97abb['highschoolgrad'].astype('category')</p></li>
</ol>
<p>While 12 makes conceptual sense as the threshold for classifying our new feature, <strong class="source-inline">highschoolgrad</strong>, this would present some modeling challenges if we intended to use <strong class="source-inline">highschoolgrad</strong> as a target. There is a pretty substantial class imbalance, with <strong class="source-inline">highschoolgrad</strong> equal to 1 class being more than 4 times the size of the 0 group. We should explore using more groups to represent <strong class="source-inline">highestgradecompleted</strong>.</p>
<ol>
<li value="14">One way to do<a id="_idIndexMarker043"/> this with pandas is with <a id="_idIndexMarker044"/>the <strong class="source-inline">qcut</strong> function. We can set the <strong class="source-inline">q</strong> parameter of <strong class="source-inline">qcut</strong> to <strong class="source-inline">6</strong> to create six groups that are as evenly distributed as possible. These groups are now closer to being balanced:<p class="source-code">nls97abb['highgradegroup'] = \</p><p class="source-code">  pd.qcut(nls97abb['highestgradecompleted'], </p><p class="source-code">   q=6, labels=[1,2,3,4,5,6])</p><p class="source-code"> </p><p class="source-code">nls97abb.groupby(['highgradegroup'])['highestgradecompleted'].\</p><p class="source-code">    agg(['min','max','size'])</p><p class="source-code">                  <strong class="bold">min         max      size</strong></p><p class="source-code"><strong class="bold">highgradegroup                </strong></p><p class="source-code"><strong class="bold">1                   5          11       1231</strong></p><p class="source-code"><strong class="bold">2                  12          12       1389</strong></p><p class="source-code"><strong class="bold">3                  13          14       1288</strong></p><p class="source-code"><strong class="bold">4                  15          16       1413</strong></p><p class="source-code"><strong class="bold">5                  17          17        388</strong></p><p class="source-code"><strong class="bold">6                  18          20        943</strong></p><p class="source-code">nls97abb['highgradegroup'] = \</p><p class="source-code">    nls97abb['highgradegroup'].astype('category')</p></li>
<li>Finally, I typically find it helpful to generate frequencies for all the categorical features and save that output so that I can refer to it later. I rerun that code whenever I make some change to the data that may change these frequencies. The following code <a id="_idIndexMarker045"/>iterates over all the columns that are of the category data type and<a id="_idIndexMarker046"/> runs <strong class="source-inline">value_counts</strong>:<p class="source-code"> freqout = open('views/frequencies.txt', 'w') </p><p class="source-code"> for col in nls97abb.select_dtypes(include=["category"]):</p><p class="source-code">      print(col, "----------------------",</p><p class="source-code">        "frequencies",</p><p class="source-code">      nls97abb[col].value_counts(dropna=False).sort_index(),</p><p class="source-code">        "percentages",</p><p class="source-code">      nls97abb[col].value_counts(normalize=True).\</p><p class="source-code">        sort_index(),</p><p class="source-code">      sep="\n\n", end="\n\n\n", file=freqout)</p><p class="source-code"> </p><p class="source-code"> freqout.close()</p></li>
</ol>
<p>These are the key techniques for generating one-way frequencies for the categorical features in your data. The real star of the show has been the <strong class="source-inline">value_counts</strong> method. We can use <strong class="source-inline">value_counts</strong> to create frequencies a Series at a time, use it with <strong class="source-inline">apply</strong> for multiple columns, or iterate over several columns and call <strong class="source-inline">value_counts</strong> each time. We have looked at examples of each in this section. Next, let's explore some techniques for examining the distribution of continuous features.</p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Generating summary statistics for continuous and discrete features</h1>
<p>Getting a<a id="_idIndexMarker047"/> feel for the distribution <a id="_idIndexMarker048"/>of continuous or discrete <a id="_idIndexMarker049"/>features is a little more<a id="_idIndexMarker050"/> complicated than it is for categorical features. A continuous feature can take an infinite number of values. An example of a continuous feature is weight, as someone can weigh 70 kilograms, or 70.1, or 70.01. Discrete features have a finite number of values, such as the number of birds sighted, or the number of apples purchased. One way of thinking about the difference is that a discrete feature is typically something that has been counted, while a continuous feature is usually captured by measurement, weighing, or timekeeping.</p>
<p>Continuous features will generally be stored as floating-point numbers unless they have been constrained to be whole numbers. In that case, they may be stored as integers. Age for individual humans, for example, is continuous but is usually truncated to an integer.</p>
<p>For most modeling purposes, continuous and discrete features are treated similarly. We would not model age as a categorical feature. We assume that the interval between ages has largely the same meaning between 25 and 26 as it has between 35 and 36, though this breaks down at the extremes. The interval between 1 and 2 years of age for humans is not at all like that between 71 and 72. Data analysts and scientists are usually skeptical of assumed linear relationships between continuous features and targets, though modeling is much easier when that is true.</p>
<p>To understand how a continuous feature (or discrete feature) is distributed, we must examine its central tendency, shape, and spread. Key summary statistics are mean and median for central tendency, skewness and kurtosis for shape, and range, interquartile range, variance, and standard deviation for spread. In this section, we will learn how to use pandas, supplemented by the <strong class="bold">SciPy</strong> library, to get these statistics. We will also discuss important implications for modeling.</p>
<p>We will work with COVID-19 data in this section. The dataset contains one row per country, with total cases and deaths through June 2021, as well as demographic data for each country.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">Our World in Data</em> provides COVID-19 public use data at <a href="https://ourworldindata.org/coronavirus-source-data">https://ourworldindata.org/coronavirus-source-data</a>. The data that will be used in this section was downloaded on July 9, 2021. There are more columns in the data than I have included. I created the region column based on country.</p>
<p>Follow<a id="_idIndexMarker051"/> these <a id="_idIndexMarker052"/>steps to generate the <a id="_idIndexMarker053"/>summary<a id="_idIndexMarker054"/> statistics:</p>
<ol>
<li value="1">Let's load the COVID <strong class="source-inline">.csv</strong> file into pandas, set the index, and look at the data. There are 221 rows and 16 columns. The index we set, <strong class="source-inline">iso_code</strong>, contains a unique value for each row. We use <strong class="source-inline">sample</strong> to view two countries randomly, rather than the first two (we set a value for <strong class="source-inline">random_state</strong> to get the same results each time we run the code): <p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import scipy.stats as scistat</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv",</p><p class="source-code">    parse_dates=['lastdate'])</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p><p class="source-code">covidtotals.shape</p><p class="source-code"><strong class="bold">(221, 16)</strong></p><p class="source-code">covidtotals.index.nunique()</p><p class="source-code"><strong class="bold">221</strong></p><p class="source-code">covidtotals.sample(2, random_state=6).T</p><p class="source-code"><strong class="bold">iso_code                         ISL               CZE</strong></p><p class="source-code"><strong class="bold">lastdate                  2021-07-07        2021-07-07</strong></p><p class="source-code"><strong class="bold">location                     Iceland           Czechia</strong></p><p class="source-code"><strong class="bold">total_cases                    6,555         1,668,277</strong></p><p class="source-code"><strong class="bold">total_deaths                      29            30,311</strong></p><p class="source-code"><strong class="bold">total_cases_mill              19,209           155,783</strong></p><p class="source-code"><strong class="bold">total_deaths_mill                 85             2,830</strong></p><p class="source-code"><strong class="bold">population                   341,250        10,708,982</strong></p><p class="source-code"><strong class="bold">population_density                 3               137</strong></p><p class="source-code"><strong class="bold">median_age                        37                43</strong></p><p class="source-code"><strong class="bold">gdp_per_capita                46,483            32,606</strong></p><p class="source-code"><strong class="bold">aged_65_older                     14                19</strong></p><p class="source-code"><strong class="bold">total_tests_thous                NaN               NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy                   83                79</strong></p><p class="source-code"><strong class="bold">hospital_beds_thous                3                 7</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence                5                 7</strong></p><p class="source-code"><strong class="bold">region                Western Europe    Western Europe</strong></p></li>
</ol>
<p>Just by <a id="_idIndexMarker055"/>looking at these two<a id="_idIndexMarker056"/> rows, we can see<a id="_idIndexMarker057"/> significant differences in<a id="_idIndexMarker058"/> cases and deaths between Iceland and Czechia, even in terms of population size. (<strong class="source-inline">total_cases_mill</strong> and <strong class="source-inline">total_deaths_mill</strong> divide cases and deaths per million of population, respectively.) Data analysts are very used to wondering if there is anything else in the data that may explain substantially higher cases and deaths in Czechia than in Iceland. In a sense, we are always engaging in feature selection.</p>
<ol>
<li value="2">Let's take a <a id="_idIndexMarker059"/>look at the data<a id="_idIndexMarker060"/> types and number<a id="_idIndexMarker061"/> of non-null values for <a id="_idIndexMarker062"/>each column. Almost all of the columns are continuous or discrete. We have data on cases and deaths, as well as likely targets, for 192 and 185 rows, respectively. An important data cleaning task we'll have to do will be figuring out what, if anything, we can do about countries that have missing values for our targets. We'll discuss how to handle missing values later:<p class="source-code">covidtotals.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Index: 221 entries, AFG to ZWE</strong></p><p class="source-code"><strong class="bold">Data columns (total 16 columns):</strong></p><p class="source-code"><strong class="bold"> #   Column             Non-Null Count         Dtype </strong></p><p class="source-code"><strong class="bold">---  -------            --------------  --------------</strong></p><p class="source-code"><strong class="bold"> 0   lastdate             221 non-null  datetime64[ns]</strong></p><p class="source-code"><strong class="bold"> 1   location             221 non-null          object</strong></p><p class="source-code"><strong class="bold"> 2   total_cases          192 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 3   total_deaths         185 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 4   total_cases_mill     192 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 5   total_deaths_mill    185 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 6   population           221 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 7   population_density   206 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 8   median_age           190 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 9   gdp_per_capita       193 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 10  aged_65_older        188 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 11  total_tests_thous     13 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 12  life_expectancy      217 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 13  hospital_beds_thous  170 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 14  diabetes_prevalence  200 non-null         float64</strong></p><p class="source-code"><strong class="bold"> 15  region               221 non-null          object</strong></p><p class="source-code"><strong class="bold">dtypes: datetime64[ns](1), float64(13), object(2)</strong></p><p class="source-code"><strong class="bold">memory usage: 29.4+ KB</strong></p></li>
<li>Now, we <a id="_idIndexMarker063"/>are ready to <a id="_idIndexMarker064"/>examine the distribution<a id="_idIndexMarker065"/> of some of the features. We<a id="_idIndexMarker066"/> can get most of the summary statistics we want by using the <strong class="source-inline">describe</strong> method. The mean and median (50%) are good indicators of the center of the distribution, each with its strengths. It is also good to notice substantial differences between the mean and median, as an indication of skewness. For example, we can see that the mean cases per million is almost twice the median, with 36.7 thousand compared to 19.5 thousand. This is a clear indicator of positive skew. This is also true for deaths per million.</li>
</ol>
<p>The interquartile range is also quite large for cases and deaths, with the 75th percentile value being about 25 times larger than the 25th percentile value in both cases. We can compare that with the percentage of the population aged 65 and older and diabetes prevalence, where the 75th percentile is just four times or two times that of the 25th percentile, respectively. We can tell right away that those two possible features (<strong class="source-inline">aged_65_older</strong> and <strong class="source-inline">diabetes_prevalence</strong>) would have to do a lot of work to explain the huge variance in our targets:</p>
<p class="source-code"> keyvars = ['location','total_cases_mill','total_deaths_mill',</p>
<p class="source-code">...  'aged_65_older','diabetes_prevalence']</p>
<p class="source-code"> covidkeys = covidtotals[keyvars]</p>
<p class="source-code"> covidkeys.describe()</p>
<p class="source-code">total_cases_mill total_deaths_mill aged_65_older diabetes_prevalence</p>
<p class="source-code"><strong class="bold">count        192.00       185.00    188.00     200.00</strong></p>
<p class="source-code"><strong class="bold">mean      36,649.37       683.14      8.61       8.44</strong></p>
<p class="source-code"><strong class="bold">std       41,403.98       861.73      6.12       4.89</strong></p>
<p class="source-code"><strong class="bold">min            8.52         0.35      1.14       0.99</strong></p>
<p class="source-code"><strong class="bold">25%        2,499.75        43.99      3.50       5.34</strong></p>
<p class="source-code"><strong class="bold">50%       19,525.73       293.50      6.22       7.20</strong></p>
<p class="source-code"><strong class="bold">75%       64,834.62     1,087.89     13.92      10.61</strong></p>
<p class="source-code"><strong class="bold">max      181,466.38     5,876.01     27.05      30.53</strong></p>
<ol>
<li value="4">I<a id="_idIndexMarker067"/> sometimes find it helpful <a id="_idIndexMarker068"/>to look at the decile <a id="_idIndexMarker069"/>values to get a better sense <a id="_idIndexMarker070"/>of the distribution. The <strong class="source-inline">quantile</strong> method can take a single value for quantile, such as <strong class="source-inline">quantile(0.25)</strong> for the 25th percentile, or a list or tuple, such as <strong class="source-inline">quantile((0.25,0.5))</strong> for the 25th and 50th percentiles. In the following code, we're using <strong class="source-inline">arange</strong> from NumPy (<strong class="source-inline">np.arange(0.0, 1.1, 0.1)</strong>) to generate an array that goes from 0.0 to 1.0 with a 0.1 increment. We would get the same result if we were to use <strong class="source-inline">covidkeys.quantile([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])</strong>:<p class="source-code"> covidkeys.quantile(np.arange(0.0, 1.1, 0.1))</p><p class="source-code">      total_cases_mill  total_deaths_mill  aged_65_older  diabetes_prevalence</p><p class="source-code"><strong class="bold">0.00         8.52       0.35       1.14     0.99</strong></p><p class="source-code"><strong class="bold">0.10       682.13      10.68       2.80     3.30</strong></p><p class="source-code"><strong class="bold">0.20     1,717.39      30.22       3.16     4.79</strong></p><p class="source-code"><strong class="bold">0.30     3,241.84      66.27       3.86     5.74</strong></p><p class="source-code"><strong class="bold">0.40     9,403.58      145.06      4.69     6.70</strong></p><p class="source-code"><strong class="bold">0.50     19,525.73     293.50      6.22     7.20</strong></p><p class="source-code"><strong class="bold">0.60     33,636.47     556.43      7.93     8.32</strong></p><p class="source-code"><strong class="bold">0.70     55,801.33     949.71     11.19    10.08</strong></p><p class="source-code"><strong class="bold">0.80     74,017.81    1,333.79    14.92    11.62</strong></p><p class="source-code"><strong class="bold">0.90     94,072.18    1,868.89    18.85    13.75</strong></p><p class="source-code"><strong class="bold">1.00    181,466.38    5,876.01    27.05    30.53</strong></p></li>
</ol>
<p>For cases, deaths, and diabetes prevalence, much of the range (the distance between the min and max values) is in the last 10% of the distribution. This is particularly true for deaths. This hints at possible modeling problems and invites us to take a close look at outliers, something we will do in the next section.</p>
<ol>
<li value="5">Some<a id="_idIndexMarker071"/> machine learning<a id="_idIndexMarker072"/> algorithms assume that<a id="_idIndexMarker073"/> our features have <a id="_idIndexMarker074"/>normal (also referred to as Gaussian) distributions, that they are distributed symmetrically (have low skew), and that they have relatively normal tails (neither excessively high nor excessively low kurtosis). The statistics we have seen so far already suggest a high positive skew for our two likely targets – that is, total cases and deaths per million people in the population. Let's put a finer point on this by calculating both skew and kurtosis for some of the features. For a Gaussian distribution, we expect a value near 0 for skew and 3 for kurtosis. <strong class="source-inline">total_deaths_mill</strong> has values for skew and kurtosis that are worth noting, and the <strong class="source-inline">total_cases_mill</strong> and <strong class="source-inline">aged_65_older</strong> features have excessively low kurtosis (skinny tails):<p class="source-code">covidkeys.skew()</p><p class="source-code"><strong class="bold">total_cases_mill        1.21</strong></p><p class="source-code"><strong class="bold">total_deaths_mill       2.00</strong></p><p class="source-code"><strong class="bold">aged_65_older           0.84</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence     1.52</strong></p><p class="source-code"><strong class="bold">dtype: float64</strong></p><p class="source-code"> covidkeys.kurtosis()</p><p class="source-code"><strong class="bold">total_cases_mill        0.91</strong></p><p class="source-code"><strong class="bold">total_deaths_mill       6.58</strong></p><p class="source-code"><strong class="bold">aged_65_older          -0.56</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence     3.31</strong></p><p class="source-code"><strong class="bold">dtype: float64</strong></p></li>
<li>We can <a id="_idIndexMarker075"/>also explicitly test<a id="_idIndexMarker076"/> each distribution's<a id="_idIndexMarker077"/> normality by <a id="_idIndexMarker078"/>looping over the features in the <strong class="source-inline">keyvars</strong> list and running<a id="_idIndexMarker079"/> a <strong class="bold">Shapiro-Wilk</strong> test on the distribution (<strong class="source-inline">scistat.shapiro(covidkeys[var].dropna())</strong>). Notice that we need to drop missing values with <strong class="source-inline">dropna</strong> for the test to run. p-values less than 0.05 indicate that we can reject the null hypothesis of normal, which is the case for each of the four features:<p class="source-code">for var in keyvars[1:]:</p><p class="source-code">      stat, p = scistat.shapiro(covidkeys[var].dropna())</p><p class="source-code">      print("feature=", var, "     p-value=", '{:.6f}'.format(p))</p><p class="source-code"> </p><p class="source-code"><strong class="bold">feature= total_cases_mill       p-value= 0.000000</strong></p><p class="source-code"><strong class="bold">feature= total_deaths_mill      p-value= 0.000000</strong></p><p class="source-code"><strong class="bold">feature= aged_65_older          p-value= 0.000000</strong></p><p class="source-code"><strong class="bold">feature= diabetes_prevalence    p-value= 0.000000</strong></p></li>
</ol>
<p>These results should make us pause if we are considering parametric models such as linear regression. None of the distributions approximates a normal distribution. However, this is not determinative. It is not as simple as deciding that we should use certain models when we have normally distributed features and non-parametric models (say, k-nearest neighbors) when we do not. </p>
<p>We want to do additional data cleaning before we make any modeling decisions. For example, we may decide to remove outliers or determine that it is appropriate to transform the data. We will explore transformations, such as log and polynomial transformations, in several chapters in this book.</p>
<p>This section showed you how to use pandas and SciPy to understand how continuous and discrete features are distributed, including their central tendency, shape, and spread. It makes sense to generate these statistics for any feature or target that might be included in our modeling. This also points us in the direction of more work we need to do to prepare our data for analysis. We need to identify missing values and outliers and figure out how we will handle them. We should also visualize the distribution of our continuous features. This rarely fails to yield additional insights. We will learn how to identify outliers in the next section and create visualizations in the following section.</p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Identifying extreme values and outliers in univariate analysis</h1>
<p>An<a id="_idIndexMarker080"/> outlier can be thought of as an<a id="_idIndexMarker081"/> observation with feature values, or relationships between <a id="_idIndexMarker082"/>feature values, that are so unusual that<a id="_idIndexMarker083"/> they cannot help explain relationships in the rest of the data. This matters for modeling because we cannot assume that the outliers will have a neutral impact on our parameter estimates. Sometimes, our models work so hard to construct parameter estimates that can account for patterns in outlier observations that we compromise the model's explanatory or predictive power for all other observations. Raise your hand if you have ever spent days trying to interpret a model only to discover that your coefficients and predictions completely changed once you removed a few outliers.</p>
<p>I should quickly add that there is no agreed-upon definition of an outlier. I offer the preceding definition for use in this book because it helps us distinguish between outliers, as I have described them, and extreme values. There is a fair bit of overlap between the two, but many extreme values are not outliers. This is because such values reflect a natural and explainable trend in a feature, or because they reflect the same relationship between features as is observed throughout the data. The reverse is also true. Some outliers are not extreme values. For example, a target value might be right in the middle of the distribution but have quite unexpected predictor values.</p>
<p>For our <a id="_idIndexMarker084"/>modeling, then, it is hard to say <a id="_idIndexMarker085"/>that a particular feature or target value is an outlier without referencing multivariate relationships. But it should at least raise a red flag when, in our univariate analysis, we see values well to the left or right of the center. This should prompt us to investigate the observation at that value further, including examining the values of other features. We will look at multivariate relationships in more detail in the next two chapters. Here, and in the next section on visualizations, we will focus on identifying extreme values and outliers when looking at one variable.</p>
<p>A good starting point for identifying an extreme value is to look at its distance from the middle of the distribution. One common method for doing that is to calculate each value's distance<a id="_idIndexMarker086"/> from the <strong class="bold">interquartile range</strong> (<strong class="bold">IQR</strong>), which is the distance between the first quartile value and the third quartile value. We often flag any value that is more than 1.5 times the interquartile range above the third quartile or below the first quartile. We can use this method to identify outliers in the COVID-19 data. </p>
<p>Let's get started:</p>
<ol>
<li value="1">Let's start by<a id="_idIndexMarker087"/> importing the libraries we will<a id="_idIndexMarker088"/> need. In addition to <a id="_idIndexMarker089"/>pandas and NumPy, we will <a id="_idIndexMarker090"/>use Matplotlib and statsmodels for the plots we will create. We will also load the COVID data and select the variables we need:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import statsmodels.api as sm</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p><p class="source-code">keyvars = ['location','total_cases_mill','total_deaths_mill',</p><p class="source-code">  'aged_65_older','diabetes_prevalence','gdp_per_capita']</p><p class="source-code">covidkeys = covidtotals[keyvars]</p></li>
<li>Let's take a look at <strong class="source-inline">total_cases_mill</strong>. We get the first and third quartile values and calculate the interquartile range, <strong class="source-inline">1.5*(thirdq-firstq)</strong>. Then, we calculate their thresholds to determine the high and low extreme values, which are <strong class="source-inline">interquartilerange+thirdq</strong> and <strong class="source-inline">firstq-interquartilerange</strong>, respectively (if you are familiar with boxplots, you will notice that this is the same calculation that's used for the whiskers of a boxplot; we will cover boxplots in the next section):<p class="source-code">thirdq, firstq = covidkeys.total_cases_mill.quantile(0.75), covidkeys.total_cases_mill.quantile(0.25)</p><p class="source-code">interquartilerange = 1.5*(thirdq-firstq)</p><p class="source-code">extvalhigh, extvallow = interquartilerange+thirdq, firstq-interquartilerange</p><p class="source-code">print(extvallow, extvalhigh, sep=" &lt;--&gt; ")</p><p class="source-code"><strong class="bold">-91002.564625 &lt;--&gt; 158336.930375</strong></p></li>
<li>This<a id="_idIndexMarker091"/> calculation indicates that any <a id="_idIndexMarker092"/>value for <strong class="source-inline">total_cases_mill</strong> that's<a id="_idIndexMarker093"/> above 158,337 can be <a id="_idIndexMarker094"/>considered extreme. We can ignore extreme values on the low end because they would be negative:<p class="source-code">covidtotals.loc[covidtotals.total_cases_mill&gt;extvalhigh].T</p><p class="source-code"><strong class="bold">iso_code                   AND         MNE         SYC</strong></p><p class="source-code"><strong class="bold">lastdate            2021-07-07  2021-07-07  2021-07-07</strong></p><p class="source-code"><strong class="bold">location            Andorra     Montenegro  Seychelles</strong></p><p class="source-code"><strong class="bold">total_cases          14,021        100,392      16,304</strong></p><p class="source-code"><strong class="bold">total_deaths            127          1,619          71</strong></p><p class="source-code"><strong class="bold">total_cases_mill    181,466        159,844     165,792</strong></p><p class="source-code"><strong class="bold">total_deaths_mill     1,644          2,578         722</strong></p><p class="source-code"><strong class="bold">population           77,265        628,062      98,340</strong></p><p class="source-code"><strong class="bold">population_density      164             46         208</strong></p><p class="source-code"><strong class="bold">median_age              NaN             39          36</strong></p><p class="source-code"><strong class="bold">gdp_per_capita          NaN         16,409      26,382</strong></p><p class="source-code"><strong class="bold">aged_65_older           NaN             15           9</strong></p><p class="source-code"><strong class="bold">total_tests_thous       NaN            NaN         NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy          84             77          73</strong></p><p class="source-code"><strong class="bold">hospital_beds_thous     NaN              4           4</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence       8             10          11</strong></p><p class="source-code"><strong class="bold">region              Western        Eastern        East</strong> </p><p class="source-code"><strong class="bold">                     Europe         Europe      Africa</strong></p></li>
<li>Andorra, Montenegro, and Seychelles all have <strong class="source-inline">total_cases_mill</strong> above the <a id="_idIndexMarker095"/>threshold amount. This invites us<a id="_idIndexMarker096"/> to explore other ways<a id="_idIndexMarker097"/> these countries might be<a id="_idIndexMarker098"/> exceptional, and whether our features can capture that. We will not dive deeply into multivariate analysis here since we will do that in the next chapter, but it is a good idea to start wrapping our brains around why these extreme values may or may not make sense. Having some means across the full dataset may help us here:<p class="source-code">covidtotals.mean()</p><p class="source-code"><strong class="bold">total_cases               963,933</strong></p><p class="source-code"><strong class="bold">total_deaths              21,631</strong></p><p class="source-code"><strong class="bold">total_cases_mill          36,649</strong></p><p class="source-code"><strong class="bold">total_deaths_mill         683</strong></p><p class="source-code"><strong class="bold">population                35,134,957</strong></p><p class="source-code"><strong class="bold">population_density        453</strong></p><p class="source-code"><strong class="bold">median_age                30</strong></p><p class="source-code"><strong class="bold">gdp_per_capita            19,141</strong></p><p class="source-code"><strong class="bold">aged_65_older             9</strong></p><p class="source-code"><strong class="bold">total_tests_thous         535</strong></p><p class="source-code"><strong class="bold">life_expectancy           73</strong></p><p class="source-code"><strong class="bold">hospital_beds_thous       3</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence       8</strong> </p></li>
</ol>
<p>The main difference between these three countries and others is that they have very low populations. Surprisingly, each has a much lower population density than average. That is the opposite of what you would expect and merits further consideration in the analysis we will do throughout this book. </p>
<p class="callout-heading">An Alternative to the IQR Calculation</p>
<p class="callout">An alternative to using the interquartile range to identify an extreme value would be to use several standard deviations away from the mean, say 3. One drawback of this method is that it is a little more susceptible to extreme values than using the interquartile range.</p>
<p>I find it <a id="_idIndexMarker099"/>helpful to produce this kind of analysis for <a id="_idIndexMarker100"/>all the key targets and features in<a id="_idIndexMarker101"/> my data, so let's automate this <a id="_idIndexMarker102"/>method of identifying extreme values. We should also output the results to a file so that we can use them when we need them:</p>
<ol>
<li value="5">Let's define a function, <strong class="source-inline">getextremevalues</strong>, that iterates over all of the columns of our DataFrame (except for the first one, which contains the location column), calculates the interquartile range for that column, selects all the observations with values above the high threshold or below the low threshold for that column, and then appends the results to a new DataFrame (<strong class="source-inline">dfout</strong>):<p class="source-code">def getextremevalues(dfin):</p><p class="source-code">      dfout = pd.DataFrame(columns=dfin.columns, </p><p class="source-code">                           data=None)</p><p class="source-code">      for col in dfin.columns[1:]:</p><p class="source-code">        thirdq, firstq = dfin[col].quantile(0.75), \</p><p class="source-code">          dfin[col].quantile(0.25)</p><p class="source-code">        interquartilerange = 1.5*(thirdq-firstq)</p><p class="source-code">        extvalhigh, extvallow = \</p><p class="source-code">          interquartilerange+thirdq, \</p><p class="source-code">          firstq-interquartilerange</p><p class="source-code">        df = dfin.loc[(dfin[col]&gt;extvalhigh) | (dfin[col]&lt;extvallow)]</p><p class="source-code">        df = df.assign(varname = col,</p><p class="source-code">          threshlow = extvallow,</p><p class="source-code">          threshhigh = extvalhigh)</p><p class="source-code">        dfout = pd.concat([dfout, df])</p><p class="source-code">    return dfout</p></li>
<li>Now, we <a id="_idIndexMarker103"/>can pass our <strong class="source-inline">covidkeys</strong> DataFrame<a id="_idIndexMarker104"/> to the <strong class="source-inline">getextremevalues</strong> function to get a DataFrame that contains the extreme values<a id="_idIndexMarker105"/> for each column. Then, we <a id="_idIndexMarker106"/>can display the number of extreme values for each column, which tells us that there were four extreme values for the total deaths per million people in the population (<strong class="source-inline">total_deaths_mill</strong>). Now, we can output the data to an Excel file:<p class="source-code">extremevalues = getextremevalues(covidkeys)</p><p class="source-code">extremevalues.varname.value_counts()</p><p class="source-code"><strong class="bold">gdp_per_capita         9</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence    8</strong></p><p class="source-code"><strong class="bold">total_deaths_mill      4</strong></p><p class="source-code"><strong class="bold">total_cases_mill       3</strong></p><p class="source-code"><strong class="bold">Name: varname, dtype: int64</strong></p><p class="source-code">extremevalues.to_excel("views/extremevaluescases.xlsx")</p></li>
<li>Let's take a <a id="_idIndexMarker107"/>closer look at the extreme<a id="_idIndexMarker108"/> deaths per million values. We<a id="_idIndexMarker109"/> can query the DataFrame <a id="_idIndexMarker110"/>we just created to get the <strong class="source-inline">threshhigh</strong> value for <strong class="source-inline">total_deaths_mill</strong>, which is <strong class="source-inline">2654</strong>. We can also get other key features for those countries with the extreme values since we have included that data in the new DataFrame:<p class="source-code">extremevalues.loc[extremevalues.varname=="total_deaths_mill",</p><p class="source-code">    'threshhigh'][0]</p><p class="source-code"><strong class="bold">2653.752</strong></p><p class="source-code">extremevalues.loc[extremevalues.varname=="total_deaths_mill",</p><p class="source-code">      keyvars].sort_values(['total_deaths_mill'], ascending=False)</p><p class="source-code">      <strong class="bold">location              total_cases_mill  total_deaths_mill</strong></p><p class="source-code"><strong class="bold">PER   Peru                    62,830            5,876</strong></p><p class="source-code"><strong class="bold">HUN   Hungary                 83,676            3,105</strong></p><p class="source-code"><strong class="bold">BIH   Bosnia and Herzegovina  62,499            2,947</strong></p><p class="source-code"><strong class="bold">CZE   Czechia                 155,783           2,830</strong></p><p class="source-code"><strong class="bold">     _65_older  diabetes_prevalence  gdp_per_capita  </strong></p><p class="source-code"><strong class="bold">PER      7              6                12,237</strong></p><p class="source-code"><strong class="bold">HUN     19              8                26,778</strong></p><p class="source-code"><strong class="bold">BIH     17             10                11,714</strong></p><p class="source-code"><strong class="bold">CZE     19              7                32,606</strong></p></li>
</ol>
<p>Peru, Hungary, Bosnia<a id="_idIndexMarker111"/> and Herzegovina, and<a id="_idIndexMarker112"/> Czechia have <strong class="source-inline">total_deaths_mill</strong> above the extreme value threshold. One thing that stands out for three of these countries is how much above the average for percent of population 65 or older they are as well (the average for that feature, as we displayed in a preceding table, is 9). Although these are extreme values for <a id="_idIndexMarker113"/>deaths, the relationship between the <a id="_idIndexMarker114"/>elderly percentage of the population and COVID deaths may account for much of this and can do so without overfitting the model to these extreme cases. We will go through some strategies for teasing that out in the next chapter.</p>
<p>So far, we have discussed outliers and extreme values without referencing distribution shape. What we have implied so far is that an extreme value is a rare value – significantly rarer than the values near the center of the distribution. But this makes the most sense when the feature's distribution approaches normal. If, on the other hand, a feature had a <a id="_idIndexMarker115"/>uniform distribution, a very high value <a id="_idIndexMarker116"/>would be no rarer than any<a id="_idIndexMarker117"/> other value.</p>
<p>In practice, then, we think about extreme values or outliers relative to the distribution of the feature. <strong class="bold">Quantile-quantile</strong> (<strong class="bold">Q-Q</strong>) plots<a id="_idIndexMarker118"/> can improve our sense of that distribution by allowing us to view it graphically relative to a theoretical distribution: normal, uniform, log, or others. Let's take a look:</p>
<ol>
<li value="1">Let's create a Q-Q plot of total cases per million that's relative to the normal distribution. We can use the <strong class="source-inline">statsmodels</strong> library for this:<p class="source-code">sm.qqplot(covidtotals[['total_cases_mill']]. \</p><p class="source-code">  sort_values(['total_cases_mill']).dropna(),line='s')</p><p class="source-code">)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer007">
<img alt="Figure 1.1 – Q-Q plot of total cases per million " height="454" src="image/B17978_01_001.jpg" width="613"/>
</div>
</div>
<p class="figure-caption">Figure 1.1 – Q-Q plot of total cases per million</p>
<p>This <a id="_idIndexMarker119"/>Q-Q plot makes it clear that the distribution of total cases across countries is not normal. We can see this by how much the data points deviate from the red line. It is a Q-Q plot that we would expect from a distribution with some positive skew. This is consistent with the summary statistics we have already calculated for the total cases feature. It further reinforces our developing sense, in that we will need to be cautious about parametric models and that we will probably have to account for more than just one or two outlier observations.</p>
<p>Let's look at a Q-Q plot for a feature with a distribution that is a little closer to normal. There isn't a great candidate in the COVID data, so we will work with data from the United States National Oceanic and Atmospheric Administration on land temperatures in 2019.</p>
<p class="callout-heading">Data Note</p>
<p class="callout">The land temperature dataset contains the average temperature readings (in Celsius) in 2019 from over 12,000 stations across the world, though the majority of the stations are in the United States. The raw data was retrieved from the Global Historical Climatology Network integrated database. It has been made available for public use by the United States National Oceanic and Atmospheric Administration at <a href="https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4">https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-</a>2.</p>
<ol>
<li value="2">First, let's load <a id="_idIndexMarker120"/>the data into a pandas DataFrame and run some descriptive statistics on the temperature feature, <strong class="source-inline">avgtemp</strong>. We must add a few percentile statistics to the normal <strong class="source-inline">describe</strong> output to get a better sense of the range of values. <strong class="source-inline">avgtemp</strong> is the average temperature for the year at each of the 12,095 weather stations. The average temperature across all stations was 11.2 degrees Celsius. The median was 10.4. However, there were some very negative values, including 14 weather stations with an average temperature of less than -25. This contributes to a moderately negative skew, though both the skew and kurtosis are closer to what would be expected from a normal distribution:<p class="source-code">landtemps = pd.read_csv("data/landtemps2019avgs.csv")</p><p class="source-code">landtemps.avgtemp.describe(percentiles=[0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])</p><p class="source-code"><strong class="bold">count        12,095.0</strong></p><p class="source-code"><strong class="bold">mean             11.2</strong></p><p class="source-code"><strong class="bold">std               8.6</strong></p><p class="source-code"><strong class="bold">min             -60.8</strong></p><p class="source-code"><strong class="bold">5%               -0.7</strong></p><p class="source-code"><strong class="bold">10%               1.7</strong></p><p class="source-code"><strong class="bold">25%               5.4</strong></p><p class="source-code"><strong class="bold">50%              10.4</strong></p><p class="source-code"><strong class="bold">75%              16.9</strong></p><p class="source-code"><strong class="bold">90%              23.1</strong></p><p class="source-code"><strong class="bold">95%              27.0</strong></p><p class="source-code"><strong class="bold">max              33.9</strong></p><p class="source-code"><strong class="bold">Name: avgtemp, dtype: float64</strong></p><p class="source-code"> landtemps.loc[landtemps.avgtemp&lt;-25,'avgtemp'].count()</p><p class="source-code"><strong class="bold">14</strong></p><p class="source-code">landtemps.avgtemp.skew()</p><p class="source-code"><strong class="bold">-0.2678382583481768</strong></p><p class="source-code">landtemps.avgtemp.kurtosis()</p><p class="source-code"><strong class="bold">2.169831370706107</strong>3</p></li>
<li>Now, let's take a look at a <a id="_idIndexMarker121"/>Q-Q plot of the average temperature:<p class="source-code">sm.qqplot(landtemps.avgtemp.sort_values().dropna(), line='s')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<img alt="Figure 1.2 – Q-Q plot of average temperatures " height="453" src="image/B17978_01_002.jpg" width="615"/>
</div>
</div>
<p class="figure-caption">Figure 1.2 – Q-Q plot of average temperatures</p>
<p>Along most of<a id="_idIndexMarker122"/> the range, the distribution of average temperatures looks pretty close to normal. The exceptions are the extremely low temperatures, contributing to a small amount of negative skew. There is also some deviation from normal at the high end, though this is much less of an issue (you may have noticed that Q-Q plots for features with negative skew have an umbrella-like shape, while those with positive skews, such as total cases, have more of a bowl-like shape).</p>
<p>We are off to a good start in our efforts to understand the distribution of possible features and targets and, to a related effort, to identify extreme values and outliers. This is important information to have at our fingertips when we construct, refine, and interpret our models. But there is more that we can do to improve our intuition about the data. A good next step is to construct visualizations of key features.</p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Using histograms, boxplots, and violin plots to examine the distribution of features</h1>
<p>We have already generated many of the numbers that would make up the data points of a histogram or boxplot. But we often improve our understanding of the data when we see it represented graphically. We see observations bunched around the mean, we notice the size of the tails, and we see what seem to be extreme values.</p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Using histograms</h2>
<p>Follow<a id="_idIndexMarker123"/> these steps to create a histogram:</p>
<ol>
<li value="1">We will work with both the COVID data and the temperatures data in this section. In addition to the libraries we have worked with so far, we must import Seaborn to create some plots more easily than we could in Matplotlib:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">landtemps = pd.read_csv("data/landtemps2019avgs.csv")</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv", parse_dates=["lastdate"])</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p></li>
<li>Now, let's create a simple histogram. We can use Matplotlib's <strong class="source-inline">hist</strong> method to create a histogram of total cases per million. We will also draw lines for the mean and median:<p class="source-code">plt.hist(covidtotals['total_cases_mill'], bins=7)</p><p class="source-code">plt.axvline(covidtotals.total_cases_mill.mean(), color='red',</p><p class="source-code">   linestyle='dashed', linewidth=1, label='mean')</p><p class="source-code">plt.axvline(covidtotals.total_cases_mill.median(), color='black',</p><p class="source-code">   linestyle='dashed', linewidth=1, label='median')</p><p class="source-code">plt.title("Total COVID Cases")</p><p class="source-code">plt.xlabel('Cases per Million')</p><p class="source-code">plt.ylabel("Number of Countries")</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker124"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<img alt="Figure 1.3 – Total COVID cases " height="433" src="image/B17978_01_003.jpg" width="555"/>
</div>
</div>
<p class="figure-caption">Figure 1.3 – Total COVID cases</p>
<p>One <a id="_idIndexMarker125"/>aspect of the total distribution that this histogram highlights is that most countries (more than 100 of the 192) are in the very first bin, between 0 cases per million and 25,000 cases per million. Here, we can see the positive skew, with the mean pulled to the right by extreme high values. This is consistent with what we discovered when we used Q-Q plots in the previous section.</p>
<ol>
<li value="3">Let's create a histogram of average temperatures from the land temperatures dataset:<p class="source-code">plt.hist(landtemps['avgtemp'])</p><p class="source-code">plt.axvline(landtemps.avgtemp.mean(), color='red', linestyle='dashed', linewidth=1, label='mean')</p><p class="source-code">plt.axvline(landtemps.avgtemp.median(), color='black', linestyle='dashed', linewidth=1, label='median')</p><p class="source-code">plt.title("Average Land Temperatures")</p><p class="source-code">plt.xlabel('Average Temperature')</p><p class="source-code">plt.ylabel("Number of Weather Stations")</p><p class="source-code">plt.legend()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker126"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.4 – Average land temperatures " height="433" src="image/B17978_01_004.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 1.4 – Average land temperatures</p>
<p>The histogram for the average land temperatures from the land temperatures dataset looks quite different. Except for a few highly negative values, this distribution looks closer to normal. Here, we can see that the mean and the median are quite close and that the distribution looks fairly symmetrical. </p>
<ol>
<li value="4">We <a id="_idIndexMarker127"/>should take a look at the observations at the extreme left of the distribution. They are all in Antarctica or the extreme north of Canada. Here, we have to wonder if it makes sense to include observations with such extreme values in the models we construct. However, it would be premature to make that determination based on these results alone. We will come back to this in the next chapter when we examine multivariate techniques for identifying outliers:<p class="source-code"> landtemps.loc[landtemps.avgtemp&lt;-25,['station','country','avgtemp']].\</p><p class="source-code">...  sort_values(['avgtemp'], ascending=True)</p><p class="source-code">      <strong class="bold">station               country          avgtemp</strong></p><p class="source-code"><strong class="bold">827   DOME_PLATEAU_DOME_A   Antarctica      -60.8</strong></p><p class="source-code"><strong class="bold">830   VOSTOK                Antarctica      -54.5</strong></p><p class="source-code"><strong class="bold">837   DOME_FUJI             Antarctica      -53.4</strong></p><p class="source-code"><strong class="bold">844   DOME_C_II             Antarctica      -50.5</strong></p><p class="source-code"><strong class="bold">853   AMUNDSEN_SCOTT        Antarctica      -48.4</strong></p><p class="source-code"><strong class="bold">842   NICO                  Antarctica      -48.4</strong></p><p class="source-code"><strong class="bold">804   HENRY                 Antarctica      -47.3</strong></p><p class="source-code"><strong class="bold">838   RELAY_STAT            Antarctica      -46.1</strong></p><p class="source-code"><strong class="bold">828   DOME_PLATEAU_EAGLE    Antarctica      -43.0</strong></p><p class="source-code"><strong class="bold">819   KOHNENEP9             Antarctica      -42.4</strong></p><p class="source-code"><strong class="bold">1299  FORT_ROSS             Canada          -30.3</strong></p><p class="source-code"><strong class="bold">1300  GATESHEAD_ISLAND      Canada          -28.7</strong></p><p class="source-code"><strong class="bold">811   BYRD_STATION          Antarctica      -25.8</strong></p><p class="source-code"><strong class="bold">816   GILL                  Antarctica      -25.5</strong></p></li>
</ol>
<p>An excellent<a id="_idIndexMarker128"/> way to visualize central tendency, spread, and outliers at the same time is with a boxplot. </p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Using boxplots</h2>
<p><strong class="bold">Boxplots</strong> show us the <a id="_idIndexMarker129"/>interquartile range, with whiskers representing 1.5 times the interquartile range, and data points beyond that range that can be considered extreme values. If this calculation seems familiar, it's because it's the same one we used earlier in this chapter to identify extreme values! Let's get started:</p>
<ol>
<li value="1">We can use the Matplotlib <strong class="source-inline">boxplot</strong> method to create a boxplot of total cases per million people in the population. We can draw arrows to show the interquartile range (the first quartile, median, and third quartile) and the extreme value threshold. The three circles above the threshold can be considered extreme values. The line from the interquartile range to the extreme value threshold is typically referred to as the whisker. There are usually whiskers above and below the interquartile range, but the threshold value below the first quartile value would be negative in this case:<p class="source-code">plt.boxplot(covidtotals.total_cases_mill.dropna(), labels=['Total Cases per Million'])</p><p class="source-code">plt.annotate('Extreme Value Threshold', xy=(1.05,157000), xytext=(1.15,157000), size=7, arrowprops=dict(facecolor='black', headwidth=2, width=0.5, shrink=0.02))</p><p class="source-code">plt.annotate('3rd quartile', xy=(1.08,64800), xytext=(1.15,64800), size=7, arrowprops=dict(facecolor='black', headwidth=2, width=0.5, shrink=0.02))</p><p class="source-code">plt.annotate('Median', xy=(1.08,19500), xytext=(1.15,19500), size=7, arrowprops=dict(facecolor='black', headwidth=2, width=0.5, shrink=0.02))</p><p class="source-code">plt.annotate('1st quartile', xy=(1.08,2500), xytext=(1.15,2500), size=7, arrowprops=dict(facecolor='black', headwidth=2, width=0.5, shrink=0.02))</p><p class="source-code">plt.title("Boxplot of Total Cases")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This<a id="_idIndexMarker130"/> produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<img alt="Figure 1.5 – Boxplot of total cases " height="416" src="image/B17978_01_005.jpg" width="560"/>
</div>
</div>
<p class="figure-caption">Figure 1.5 – Boxplot of total cases</p>
<p>It is helpful to take a closer look at the interquartile range, specifically where the median falls within the range. For this boxplot, the median is at the lower end of the range. This is what we see in distributions with positive skews.</p>
<ol>
<li value="2">Now, let's<a id="_idIndexMarker131"/> create a boxplot for the average temperature. All of the extreme values are now at the low end of the distribution. Unsurprisingly, given what we have already seen with the average temperature feature, the median line is closer to the center of the interquartile range than with our previous boxplot (we will not annotate the plot this time – we only did this last time for explanatory purposes):<p class="source-code"> plt.boxplot(landtemps.avgtemp.dropna(), labels=['Boxplot of Average Temperature'])</p><p class="source-code"> plt.title("Average Temperature")</p><p class="source-code"> plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer012">
<img alt="Figure 1.6 – Boxplot of average temperature " height="414" src="image/B17978_01_006.jpg" width="537"/>
</div>
</div>
<p class="figure-caption">Figure 1.6 – Boxplot of average temperature</p>
<p>Histograms<a id="_idIndexMarker132"/> help us see the spread of a distribution, while boxplots make it easy to identify outliers. We can get a good sense of both the spread of the distribution and the outliers in one graphic with a violin plot.</p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Using violin plots</h2>
<p>Violin plots <a id="_idIndexMarker133"/>combine histograms and boxplots into one plot. They show the IQR, median, and whiskers, as well as the frequency of the observations at all the value ranges. </p>
<p>Let's get started:</p>
<ol>
<li value="1">We can use Seaborn to create violin plots of both the COVID cases per million and the average temperature features. I am using Seaborn here, rather than Matplotlib, because I prefer its default options for violin plots:<p class="source-code">import seaborn as sns</p><p class="source-code">fig = plt.figure()</p><p class="source-code">fig.suptitle("Violin Plots of COVID Cases and Land Temperatures")</p><p class="source-code">ax1 = plt.subplot(2,1,1)</p><p class="source-code">ax1.set_xlabel("Cases per Million")</p><p class="source-code">sns.violinplot(data=covidtotals.total_cases_mill, color="lightblue", orient="h")</p><p class="source-code">ax1.set_yticklabels([])</p><p class="source-code">ax2 = plt.subplot(2,1,2)</p><p class="source-code">ax2.set_xlabel("Average Temperature")</p><p class="source-code">sns.violinplot(data=landtemps.avgtemp, color="wheat",  orient="h")</p><p class="source-code">ax2.set_yticklabels([])</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker134"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.7 – Violin plots of COVID cases and land temperatures " height="446" src="image/B17978_01_007.jpg" width="610"/>
</div>
</div>
<p class="figure-caption">Figure 1.7 – Violin plots of COVID cases and land temperatures</p>
<p>The black bar <a id="_idIndexMarker135"/>with the white dot in the middle is the interquartile range, while the white dot represents the median. The height at each point (when the violin plot is horizontal) gives us the relative frequency. The thin black lines to the right of the interquartile range for cases per million, and to the right and left for the average temperature are the whiskers. The extreme values are shown in the part of the distribution beyond the whiskers.</p>
<p>If I am going to create just one plot for a numeric feature, I will create a violin plot. Violin plots allow me to see central tendency, shape, and spread all in one graphic. Space does not permit it here, but I usually like to create violin plots of all of my continuous features and save those to a PDF file for later reference.</p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Summary</h1>
<p>In this chapter, we looked at some common techniques for exploring data. We learned how to retrieve subsets of data when that is required for our analysis. We also used pandas methods to generate key statistics on features such as mean, interquartile range, and skew. This gave us a better sense of the central tendency, spread, and shape of the distribution of each feature. It also put us in a better position to identify outliers. Finally, we used the Matplotlib and Seaborn libraries to create histograms, boxplots, and violin plots. This yielded additional insights about the distribution of features, such as the length of the tail and divergence from the normal distribution.</p>
<p>Visualizations are a great supplement to the tools for univariate analysis that we have discussed in this chapter. Histograms, boxplots, and violin plots display the shape and spread of each feature's distribution. Graphically, they show what we may miss by examining a few summary statistics, such as where there is a bulge (or bulges) in the distribution and where the extreme values are. These visualizations will be every bit as helpful when we explore bivariate and multivariate relationships, which we will do in <a href="B17978_02_ePub.xhtml#_idTextAnchor025"><em class="italic">Chapter 2</em></a>, <em class="italic">Examining Bivariate and Multivariate Relationships between Features and Targets</em>.</p>
</div>
</div>
</body></html>