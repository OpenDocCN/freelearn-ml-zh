["```py\n%pip install snorkel\n```", "```py\nUser message :\"The age is 70 and blood pressure is high and diabetes is yes.\nDoes this patient had death event? \"\nSystem: Yes\n```", "```py\nUser message :\"The age is 50 and blood pressure is normal and diabetes is yes.\nDoes this patient had death event? \"\nThen system responds \"yes\"\n```", "```py\n# Set the system role and user message variables\nsystem_role = \"\"\"Provide text sentiment in given text:\n: I like Arkansas state. Because it is natural state and beautiful!!!\npositive\n: I hate winters. It's freezing and bad weather.\nnegative\"\"\"\nuser_message = f\"\"\"\nChatCompletion API to send a response:\n\n```", "```py\n\n When you check the output, you will see `Positive` in the response. We have seen how to send text messages as few-shot examples, along with a prompt, to the LLM using `ChatCompletion` API and how the LLM responds with the sentiment based on user input messages. Next, we’ll discuss labeling data using Snorkel library.\nData labeling using Snorkel\nIn this section, we are going to learn what Snorkel is and how we can use it to label data in Python programmatically.\nLabeling data is an important step of a data science project and critical for training models to solve specific business problems.\nIn many real-world cases, training data does not have labels, or very little data with labels is available. For example, in a housing dataset, in some neighborhoods, historical housing prices may not be available for most of the houses. Another example, in the case of finance, is all transactions may not have an associated invoice number. Historical data with labels is critical for businesses to train models and automate their business processes using **machine learning** (**ML**) and artificial intelligence. However, this requires either outsourcing the data labeling to expensive domain experts or the business waiting for a long time to get new training data with labels.\nThis is where Snorkel comes into the picture to help us programmatically build labels and manage the training data at a relatively lower cost than hiring domain experts. It is also faster than manual labeling. In some cases, data may be sensitive due to it including personal information such as date of birth and social security number. In such cases, the organization cannot share data outside of the organization with a third party. So, organizations prefer to label the data programmatically with a team of in-house developers to protect data privacy and comply with government and consumer data regulations.\nWhat is Snorkel?\nSnorkel is a Python open source library ([https://www.snorkel.org/](https://www.snorkel.org/)) that is used to create labels based on heuristics for tabular data. Using this library saves the time and cost of manual labeling. Snorkel uses a list of defined rules to label the data programmatically. With Snorkel, business knowledge is applied to generate “weak” labels for the dataset. This is also called a weak supervision model.\nThe weak generative model created by Snorkel generates noisy labels. These noisy labels are used to train a classifier that generalizes the model.\n![Figure 2.2 – Programmatic data labeling (preparing training data)](img/B18944_02_01.jpg)\n\nFigure 2.2 – Programmatic data labeling (preparing training data)\nHere is an overview of the Snorkel system:\n\n1.  Domain specific rules are extracted from various sources such as **subject matter experts** (**SMEs**), knowledge bases, legacy rule based systems, patterns and heuristics using **exploratory data** **analysis** (**EDA**).\n2.  Labeling functions are created based on the rules obtained from various sources. One labeling function is created for each business rule.\n3.  These labeling functions are applied to the unlabeled dataset to generate the label matrix.\n4.  Label matrix (`i*j`) consists of `i` rows and `j` columns where `i` is the number of observations in the dataset and `j` is the number of labeling functions. Each labeling function creates one label for each row in the dataset.\n5.  Then, Snorkel’s label model is trained on this label matrix to create a generative model. This generative label model combines the outputs of the various **labeling functions** (**LFs**) and produces a single, noise-aware probabilistic training label set.\n6.  Finally, this probabilistic training label set is used to train a downstream discriminative ML model such as logistic regression.\n\nWhy is Snorkel popular?\nSnorkel is used by several large companies, such as Google, Apple, Stanford Medicine, and Tide, and it has proven to solve business problems on a large scale. For example, as mentioned on the *snorkel.ai* website, Google used Snorkel to replace 100K+ hand-annotated labels in critical ML pipelines for text classification. Another example is, researchers at Stanford Medicine used Snorkel to label medical imaging and monitoring datasets, converting years of work of hand-labeling into a several-hour job using Snorkel. Apple built a solution called Overton that utilizes Snorkel’s framework of weak supervision to overcome cost, privacy, and cold-start issues. A UK-based fintech company, Tide, used Snorkel to label matching invoices with transactions, which otherwise would have required investing in expensive subject matter experts to hand-label historical data. Excerpts of these case studies and more can be found at [https://snorkel.ai/case-studies](https://snorkel.ai/case-studies).\nSnorkel creates weak labels using business rules and patterns in the absence of labeled data. Using Snorkel requires relatively less development eﬀort compared to crowdsourcing manual labeling. The cost of development also comes down due to automation using Python programming instead of hiring expensive business domain experts.\nThe weak labels developed by Snorkel label model can be used to train a machine learning classifier for classification and information extraction tasks. In the following sections, we are going to see the step-by-step process of generating weak labels using Snorkel. For that, first, let’s load some unlabeled data from a CSV file.\nIn the following example, we have a limited dataset related to adult income with labels.\nIn this adult income dataset, we have the `income` label and features such as age, working hours, education, and work class. The `income` label class is available only for a few observations. The majority of observations are unlabeled without any value for `income`. So, our goal is to generate the labels using the Snorkel Python library.\nWe will come up with business rules after analyzing the correlation between each individual feature of the `income` label class. Using those business rules, we will create one labeling function for each business rule with the Snorkel Python library. Then, we will generate labels for the unlabeled income dataset by applying those labeling functions.\nLoading unlabeled data\nWe will first load the adult income dataset for predicting weak labels. Let us load the data using Pandas into a DataFrame:\n\n```", "```py\n\n Here, don’t forget to replace `<YourPath>` with the path in your system. The target column name is `income`. Let us make sure the data is loaded correctly:\n\n```", "```py\n\n Let us model the input and output:\n\n```", "```py\n\n In this section, we have loaded the income dataset from a CSV file into a Pandas DataFrame.\nCreating the labeling functions\nLabeling functions implement the rules that are used to create the labeling model. Let us import the following method, which is required to implement the labeling functions:\n\n```", "```py\n\n We need to create at least four labeling functions to create an accurate labeling generative model. In order to create labeling functions, let’s first define the labeling rules based on the available small dataset labels. If no data is available, subject matter expert knowledge is required to define the rules for labeling. Labeling functions use the following labeling rules for the classification of income range for the given adult income dataset.\nLabeling rules\nThe labeling rules from our small-volume income dataset are as follows:\n\n*   **Age rule**: Based on exploratory bi-variate analysis of age and income, we come up with a heuristics rule that if the age range is between 28 and 58, then income is greater than $50K; else, it is less than $50K.\n*   **Education rule**: Based on a bi-variate analysis of education and income, we come up with a heuristic rule that if education is bachelor’s or master’s, then income is > $50K; otherwise, income is < $50K.\n*   **Working hours rule**: Based on a bi-variate analysis of working hours and income, we come up with the heuristic rule that if working hours are greater than 40, then income is greater than $50K; otherwise, it is less than $50K\n*   `Self-emp-inc` or `Federal-gov`, then income is greater than $50K; otherwise, it is less than $50K.\n\nConstants\nLet us define various constants that are used in our labeling functions as follows.\n\n*   Each labeling function returns either one of the following labels as output:\n    *   `income_high` indicates income > $50K\n    *   `income_low` indicates income < $50K\n*   Let us assign the numerical values to the output labels as follows:\n    *   `income_high` = `1`\n    *   `income_low` = `0`\n    *   `Abstain` = `-1`\n\n    `Abstain` indicates that income does not fall within any range for that observation data point:\n\nLabeling functions\nNow, let’s create labeling functions using these labeling rules and constants. We’ll discuss each function one by one as follows.\nAge rule function\nThe age rule function is used to check whether the age of the person is greater than 28; if so, the income is greater than $50K.\nWe have defined our function with the help of the `@labeling_function()` decorator. This decorator, when applied to a Python function, returns a label. Let us apply this decorator to the `age` function to return the label based on age:\n\n```", "```py\n\n Later, we will apply this age rule labeling function to the unlabeled income dataset using the Pandas LF applier, which returns a label accordingly for each observation in the dataset.\nEducation rule function\nThe education rule function is used to check whether the education of the person is bachelor’s or master’s; if so, the income is greater than $50K.\nWe have defined our labeling function for education with the help of the labeling function decorator as follows:\n\n```", "```py\n\n Later, we are going to apply this labeling function to our unlabeled income dataset using the Pandas LF applier, which returns the label accordingly for each observation in the dataset.\nHours per week rule function\nThe hours per week rule function is used to check whether the working hours per week of the person is greater than 40; if so, the income is greater than $50K.\nWe have defined our labeling function for education with the help of the labeling function decorator as follows:\n\n```", "```py\n\n Later, we are going to apply this labeling function to our unlabeled income dataset using the Pandas LF applier, which returns the label accordingly for each observation in the dataset.\nWork class rule function\nThe work class rule function is used to check whether the work class of the person is greater than self-employed or federal government; if so, the income is greater than $50K.\nWe have defined our labeling function for the work class with the help of the labeling function decorator as follows:\n\n```", "```py\n\n Creating a label model\nNow, we are going to apply all the labeling functions that we created to the unlabeled income dataset using the Pandas LF applier:\n\n```", "```py\n\n Here is the output that we got after applying the label functions (`lfs`) to the input data (`df`):\n![Figure 2.3 – Label matrix](img/B18944_02_2.jpg)\n\nFigure 2.3 – Label matrix\nHere, `L_train` is the label matrix that is returned after applying the four label functions on the unlabeled dataset. For each labeling function, one label (`-1`, `1`, or `0`) will be returned. As we have created four labeling functions, four labels will be returned for each observation. The size of the label matrix will be [n*m], where `n` is the number of observations and `m` is the number of labeling functions, which is the same as the number of labels for each observation.\nNext, we will use the label matrix (`L_train`) that was returned in the previous step:\n\n```", "```py\n\n Train the label model using the `L_train` label matrix:\n\n```", "```py\n\n Now, let us predict the labels using the trained label model.\nPredicting labels\n`L_train` is passed to the label model to predict the labels, as follows:\n\n```", "```py\n\n We get the following result:\n![Figure 2.4 – Probabilistic labels](img/B18944_02_3.jpg)\n\nFigure 2.4 – Probabilistic labels\nAs we have seen, we have generated a label matrix containing labels after applying label functions to a dataset. Then, we used that label matrix to train the label model, and then finally used that generative label model to predict the noise-aware probabilistic labels. Here, the label value `0` indicates `income` < $50K and the label value `1` indicates `income` > $50K. These values are defined using the `income_high` and `income_low` constants before implementing the labeling functions.\nWe can further use these noise-aware probabilistic labels training set to train the discriminative ML model and then predict accurate labels.\nLabeling data using the Compose library\nCompose is an open source Python library developed to generate the labels for supervised machine learning. Compose creates labels from historical data using LabelMaker.\nSubject matter experts or end users write labeling functions for the outcome of interest. For example, if the outcome of interest is the amount spent by customers in the last five days, then the labeling function returns the amount spent by taking the last five days of transaction data as input. We will take a look at this example as follows.\nLet us first install the `composeml` Python package. It is an open source Python library for prediction engineering:\n\n```", "```py\n\n We will create the label for the total purchase spend amount in the next five days based on the customer’s transactions data history.\nFor this, let us first import `composeml`:\n\n```", "```py\n\n Then, load the sample data:\n\n```", "```py\n\n Next, to start using the Compose function, we need to define the labeling function:\n\n```", "```py\n\n Next, we will build `LabelMaker` to run a search to automatically extract training examples from historical data. LabelMaker generates labels using labeling functions. Now, let us build `LabelMaker` using the labeling function:\n\n```", "```py\n\n Now, let us search and extract the labels using label search:\n\n```", "```py\n\n We can transform the labels by applying a threshold to binary labels as follows:\n\n```", "```py\n\n Now, the labels DataFrame contains a label for the amount (`amount_spent`) forecast to be spent in the next five days. So, we have seen that Compose is used to generate labels for the transactions by defining labeling function and LabelMaker. This labeled data will be used for supervised learning for prediction problems. More documentation about *Compose* can be found at ([https://compose.alteryx.com/en/stable/index.html](https://compose.alteryx.com/en/stable/index.html)).\nLabeling data using semi-supervised learning\nIn this section, let us see how to generate labels using semi-supervised learning.\nWhat is semi-supervised learning?\nSemi-supervised learning falls in between supervised learning and unsupervised learning:\n\n*   In the case of supervised learning, all the training dataset is labeled\n*   In the case of unsupervised learning, all the training dataset is unlabeled\n*   In the case of semi-supervised learning, a very small set of data is labeled and the majority of the dataset is unlabeled\n\nIn this case, first we will generate the pseudo-labels using a small part of the labeled dataset with supervised learning:\n\n1.  In this first step, we use this training dataset to train the supervised model and generate the additional pseudo labeled dataset:\n\n    *Training dataset = small set of* *labeled dataset*\n\n2.  In this second step, we will use the small set of labeled dataset along with the pseudo-labeled dataset generated in the first step:\n\n    *Training dataset = small set of labeled dataset +* *pseudo-labeled dataset*\n\nWhat is pseudo-labeling?\nPseudo-labeling is a **semi-supervised machine learning** technique that generates labels for unlabeled data by leveraging a model trained on a separate small set of labeled data using supervised learning.\nFor this demonstration, let’s download the *heart failure dataset* from GitHub repo using the link in the *Technical* *requirements* section.\nWe know our demo dataset, heart failure dataset, is already labeled. But we are going to modify the dataset by splitting it into two parts. One will have labels and the other will be unlabeled. We generate pseudo-labels for unlabeled data from a labeled dataset. Then, finally, we will combine this pseudo-labeled dataset with the initial labeled dataset to train the final model.\nLet’s import the libraries as follows:\n\n```", "```py\n\n Let’s download the income dataset:\n\n```", "```py\n\n Let’s split the data with and without labels in the 30:70 ratio. Here, we want 30% of the data to have labels and 70% of the data to be unlabeled to demonstrate a real-world scenario where we have less data with labels and more data without labels:\n\n```", "```py\n\n Now, let’s fit the data to the model and create the model. Here, we will use the Random Forest classifier. Think of Random Forest as a group of clever friends in the world of computers. Each friend knows a little something about the data we give them and can make decisions. When we put all their ideas together, we get a smart tool that can help us understand and predict things. It’s like having a team of experts to guide us through the jungle of data. In this chapter, we’ll explore how this friendly Random Forest can assist us in making sense of data and making better decisions.\nRandom Forest is a versatile ML algorithm that can be used for both regression and classification tasks. The following subsection provides an explanation of both.\nRandom Forest classifier\nIn classification tasks, the Random Forest classifier is used to predict and assign a class or category to a given input based on a set of input features. It’s particularly useful for tasks such as spam detection, image classification, and sentiment analysis. The algorithm works by constructing multiple decision trees during the training process. Each decision tree is like a “vote” on which class an input should be assigned to.\nThe final prediction is made by taking a majority vote among all the decision trees in the forest. This ensemble approach often results in robust and accurate classification:\n\n```", "```py\n\n We are training the pseudo-model with 30% of the labeled training data:\n\n```", "```py\n\n Let us calculate the accuracy score for data label model training:\n\n```", "```py\n\n Now, let us use this pseudo-model to predict the labels for the 70% unlabeled data with `x_test`:\n\n```", "```py\n\n Let us concatenate both the datasets’ features and labels, that is, the original labels and pseudo-labels:\n\n```", "```py\n\n Now, let us create a model for the entire dataset using `RandomForestClassifier`:\n\n```", "```py\n\n We get the following result:\n![Figure 2.5 – final model](img/B18944_02_4.jpg)\n\nFigure 2.5 – final model\nTo summarize, we have imported the *heart failure dataset* and then changed it to split the dataset into labeled and unlabeled datasets. Then, we applied semi-supervised learning to generate pseudo-labels. After that, we combined the original and pseudo-labeled training datasets and again applied the Random Forest regressor to generate the final labels for the unseen dataset using the final model.\nLabeling data using K-means clustering\nIn this section, we are going to learn what the K-means clustering algorithm is and how the K-means clustering algorithm is used to predict labels. Let us understand what unsupervised learning is.\nWhat is unsupervised learning?\n**Unsupervised learning** is a category of machine learning where the algorithm is tasked with discovering patterns, structures, or relationships within a dataset without explicit guidance or labeled outputs. In other words, the algorithm explores the inherent structure of the data on its own. The primary goal of unsupervised learning is often to uncover hidden patterns, group similar data points, or reduce the dimensionality of the data.\nIn the case of **unsupervised learning**, we have data with no target variable. We group observations based on similarity into different clusters. Once we have a limited number of clusters, we can register the labels for those clusters.\nFor example, in the case of customer segmentation, customers with a similar income range are grouped and formed into clusters, and then their spending power is associated with those clusters. Any new training data without labels can be measured for similarity to identify the cluster, and then the same label can be associated with all observations in the same cluster.\nK-means clustering is a specific algorithm within the realm of unsupervised learning. It is a partitioning technique that divides a dataset into distinct, non-overlapping subsets or clusters. Now, let’s dive deep into K-means clustering.\nK-means clustering\nTraversing the world of data is like finding your way through a maze. Imagine you have a bunch of colorful marbles and you want to group similar ones. K-means clustering is your helper in this quest. It’s like a sorting magician that organizes your marbles into distinct groups, making it easier to understand them. K-means clustering is a cool trick that helps you make sense of your data by finding hidden patterns and grouping things that go together.\nLet us download the *adult income* dataset, using the link in the *Technical* *requirements* section.\nThe selected features from this dataset will be used as input to the K-means algorithm, which will then partition the data into clusters based on the patterns in these selected features.\n\n```", "```py\n\n `selected_features` is a list that contains the names of the features (columns) you want to use for clustering. In this case, the features selected are `‘hoursperweek’` and `‘education-num’`. These are likely columns from a dataset containing information about individuals, where `‘hoursperweek’` represents the number of hours worked per week, and `‘education-num’` represents the number of years of education.\nWhen performing K-means clustering, it’s common to choose relevant features that you believe might contribute to the formation of meaningful clusters. The choice of features depends on the nature of your data and the problem you’re trying to solve. We will identify similar features and then group them into clusters using K-means clustering.\nIn the case of K-means clustering, the following sequence of steps needs to be followed:\n\n1.  First, identify the number of clusters randomly:\n\n    ```", "```py\n\n    A random number of clusters (between 2 and 4) is chosen.\n\n     2.  Initialize K-means with random centroids:\n\n    ```", "```py\n\n    The K-means algorithm is initialized with random centroids and fitted to the scaled data.\n\n     3.  Perform K-means clustering and update centroids until convergence:\n\n    ```", "```py\n\n    The code calculates new centroids by computing the mean of the data points in each cluster:\n\n    ```", "```py\n\n    The loop checks for convergence by comparing the old centroids with the new centroids. If they are very close, indicating that the centroids have not changed significantly, the loop breaks, and the clustering process stops:\n\n    ```", "```py\n\n    `kmeans.cluster_centers_` and `inertia` are updated within the loop, ensuring that the model parameters are adjusted in each iteration.\n\n     4.  Calculate metrics (Dunn’s Index). Inter-cluster and intra-cluster distances are calculated to compute Dunn’s Index:\n\n    ```", "```py\n\n    For each cluster, `pdist` (pairwise distance) is used to calculate the distances between all pairs of data points within the cluster. `np.max` then finds the maximum distance within each cluster. This results in an array where each element represents the maximum intra-cluster distance for a specific cluster:\n\n    ```", "```py\n\n    `min_intercluster_distances` is updated by finding the minimum inter-cluster distance, excluding diagonal elements (distances between a cluster and itself), and comparing it with the current minimum.\n\n    `max_intracluster_distances` is updated by finding the maximum intra-cluster distance across all clusters and comparing it with the current maximum.\n\n    **Dunn’s index** is calculated by dividing the minimum inter-cluster distance by the maximum intra-cluster distance:\n\n    *dunn_index = min_intercluster_distances /* *max_intracluster_distances*\n\n    It is a measure of how well-separated clusters are relative to their internal cohesion:\n\n    ```", "```py\n\n    We get the following result:\n\n![Figure 2.6 – K-means clustering metrics](img/B18944_02_5.jpg)\n\nFigure 2.6 – K-means clustering metrics\nLet us understand the metrics Inertia and Dunn's index in the results.\nInertia\nInertia is nothing but the sum of the distance of all the points from the centroid.\nIf all the points are close to each other, that means they are similar and it is a good cluster. So, we aim for a small distance for all points from the centroid.\nDunn's index\nIf the clusters are far from each other, that means the different clusters are clearly separate groups of observations. So, we measure the distance between the centroids of each cluster, which is called the inter-cluster distance. A large inter-cluster distance is ideal for good clusters.\nNow, let us print the cluster labels obtained in K-means clustering:\n\n```", "```py\n\n We get the following result:\n![Figure 2.7 ﻿– Cluster labels](img/B18944_02_6.jpg)\n\nFigure 2.7 – Cluster labels\nWe can use the cluster labels as a feature to predict the adult income using regression.\nTo summarize, we have seen what the K-means clustering algorithm is and how it is used to create clusters for *adult income* dataset. The selected features from this dataset will be used as input to the K-means algorithm, which will then partition the data into clusters based on the patterns in these selected features.\nThe complete code notebook is available in this book’s GitHub repository.\nSummary\nIn this chapter, we have seen the implementation of rules using Snorkel labeling functions for predicting the income range and labeling functions using the Compose library to predict the total amount spent by a customer during a given period. We have learned how semi-supervised learning can be used to generate pseudo-labels and data augmentation. We also learned how K-means clustering can be used to cluster the income features and then predict the income for each cluster based on business knowledge.\nIn the next chapter, we are going to learn how we can label data for regression using the Snorkel Python library, semi-supervised learning, and K-means clustering. Let us explore that in the next chapter.\n\n```"]