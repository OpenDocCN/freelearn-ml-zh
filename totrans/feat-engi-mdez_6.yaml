- en: Feature Transformations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征转换
- en: So far, in this text, we have encountered feature engineering tools from what
    seems like all possible angles of data. From analyzing tabular data in order to
    ascertain levels of data to constructing and selecting columns using statistical
    measures in order to optimize our machine learning pipelines, we have been on
    a remarkable journey of dealing with features in our data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这篇文本中，我们已经从看似所有可能的角度遇到了特征工程工具。从分析表格数据以确定数据级别，到使用统计量构建和选择列以优化我们的机器学习流程，我们经历了一段处理数据中特征的非凡旅程。
- en: It is worth mentioning once more that enhancements of machine learning come
    in many forms. We generally consider our two main metrics as accuracy and prediction/fit
    times. This means that if we can utilize feature engineering tools to make our
    pipeline have higher accuracy in a cross-validated setting, or be able to fit
    and/or predict data quicker, then we may consider that a success. Of course, our
    ultimate hope is to optimize for both accuracy and time, giving us a much better
    pipeline to work with.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 值得再次提及的是，机器学习的改进形式多种多样。我们通常将我们的两个主要指标视为准确性和预测/拟合时间。这意味着，如果我们能够利用特征工程工具使我们的流程在交叉验证设置中具有更高的准确性，或者能够更快地拟合和/或预测数据，那么我们可以将其视为成功。当然，我们最终的希望是优化准确性和时间，从而为我们提供一个更好的工作流程。
- en: 'The past five chapters have dealt with what is considered classical feature
    engineering. We have looked at five main categories/steps in feature engineering
    so far:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的五章内容处理了被认为是经典的特征工程。到目前为止，我们已经研究了特征工程的五个主要类别/步骤：
- en: '**Exploratory data analysis**: In the beginning of our work with machine learning
    pipelines, before even touching machine learning algorithms or feature engineering
    tools, it is encouraged to perform some basic descriptive statistics on our datasets
    and create visualizations to better understand the nature of the data'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索性数据分析**：在我们开始使用机器学习流程的工作之初，甚至在接触机器学习算法或特征工程工具之前，鼓励我们对数据集执行一些基本的描述性统计，并创建可视化，以更好地理解数据的本质。'
- en: '**Feature understanding**: Once we have a sense of the size and shape of the
    data, we should take a closer look at each of the columns in our dataset (if possible)
    and outline characteristics, including the level of data, as that will dictate
    how to clean specific columns if necessary'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征理解**：一旦我们对数据的规模和形状有了概念，我们应该仔细查看我们的数据集（如果可能的话）中的每一列，并概述特征，包括数据的级别，因为这将决定在必要时如何清理特定的列。'
- en: '**Feature improvement**: This phase is about altering data values and entire
    columns by imputing missing values depending on the level of the columns and performing
    dummy variable transformations and scaling operations if possible'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征改进**：这一阶段是关于通过根据列的级别插补缺失值，并在可能的情况下执行虚拟变量转换和缩放操作，来改变数据值和整个列。'
- en: '**Feature construction**: Once we have the best possible dataset at our disposal,
    we can think about constructing new columns to account for feature interaction'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征构建**：一旦我们有了最佳可能的数据集，我们就可以考虑构建新的列来解释特征交互。'
- en: '**Feature selection**: In the selection phase of our pipeline, we take all
    original and constructed columns and perform (usually univariate) statistical
    tests in order to isolate the best performing columns for the purpose of removing
    noise and speeding up calculations'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：在我们的流程选择阶段，我们取所有原始和构建的列，并执行（通常是单变量）统计测试，以隔离表现最佳的列，目的是去除噪声并加快计算速度。'
- en: 'The following figure sums up this procedure and shows us how to think about
    each step in the process:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图总结了这一过程，并展示了我们如何思考过程中的每一步：
- en: '![](img/84d94f72-4d12-4648-a745-0c7b592d5869.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84d94f72-4d12-4648-a745-0c7b592d5869.png)'
- en: Machine learning pipeline
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习流程
- en: 'This is an example of a machine learning pipeline using methods from earlier
    in this text. It consists of five main steps: analysis, understanding, improvement,
    construction, and selection. In the upcoming chapters, we will be focusing on
    a new method of transforming data that partly breaks away from this classical
    notion.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用本文前面方法示例的机器学习流程。它由五个主要步骤组成：分析、理解、改进、构建和选择。在接下来的章节中，我们将关注一种新的数据转换方法，这种方法部分打破了这种经典观念。
- en: At this stage of the book, the reader is more than ready to start tackling the
    datasets of the world with reasonable confidence and expectations of performance.
    The following two [Chapters 6](8dc49afd-2a3a-4063-9c38-ac6a049bbfe6.xhtml), *Feature
    Transformations*, and [Chapter 7](e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml), *Feature
    Learning**, *will focus on two subsets of feature engineering that are quite heavy
    in both programming and mathematics, specifically linear algebra. We will, as
    always, do our best to explain all lines of code used in this chapter and only
    describe mathematical procedures where necessary.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这个阶段，读者已经准备好以合理的信心和期望去处理世界上的数据集。接下来的两个章节，[第6章](8dc49afd-2a3a-4063-9c38-ac6a049bbfe6.xhtml)，*特征转换*，和[第7章](e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml)，*特征学习*，将重点关注特征工程中的两个子集，这两个子集在编程和数学上都非常重要，特别是线性代数。我们将一如既往地尽力解释本章中使用的所有代码行，并在必要时仅描述数学过程。
- en: This chapter will deal with **feature transformations**, a suite of algorithms
    designed to alter the internal structure of data to produce mathematically superior
    *super-columns, *while the following chapter will focus on feature learning using
    non-parametric algorithms (those that do not depend on the shape of the data)
    to automatically learn new features. The final chapter of this text contains several
    worked out case studies to show the end-to-end process of feature engineering
    and its effects on machine learning pipelines.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将处理**特征转换**，这是一套旨在改变数据内部结构以产生数学上更优的*超级列*的算法，而下一章将专注于使用非参数算法（不依赖于数据形状的算法）来自动学习新特征的**特征学习**。本文的最后一章包含几个案例研究，以展示特征工程的端到端过程及其对机器学习管道的影响。
- en: For now, let us begin with our discussion of feature transformation. As we mentioned
    before, feature transformations are a set of matrix algorithms that will structurally
    alter our data and produce what is essentially a brand new matrix of data. The
    basic idea is that original features of a dataset are the descriptors/characteristics
    of data-points and we should be able to create a new set of features that explain
    the data-points just as well, perhaps even better, with fewer columns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从对特征转换的讨论开始。正如我们之前提到的，特征转换是一组矩阵算法，将结构性地改变我们的数据，并产生一个本质上全新的数据矩阵。基本思想是，数据集的原始特征是数据点的描述符/特征，我们应该能够创建一组新的特征，这些特征能够像原始特征一样，甚至可能更好地解释数据点，而且列数更少。
- en: Imagine a simple, rectangular room. The room is empty except for a single mannequin
    standing in the center. The mannequin never moves and is always facing the same
    way. You have been charged with the task of monitoring that room 24/7\. Of course,
    you come up with the idea of adding security cameras to the room to make sure
    that all activity is captured and recorded. You place a single camera in a top
    corner of the room, facing down to look at the face of the mannequin and, in the
    process, catch a large part of the room on camera. With one camera, you are able
    to see virtually all aspects of the room. The problem is that the camera has blind
    spots. For example, you won't be able to see directly below the camera (due to
    its physical inability to see there) and behind the mannequin (as the dummy itself
    is blocking the camera's view). Being brilliant, you add a second camera to the
    opposite top corner, behind the mannequin, to compensate for the blind spots of
    the first camera. Using two cameras, you can now see greater than 99% of the room
    from a security office.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个简单的、矩形的房间。房间是空的，除了一个站在中央的人体模特。人体模特从不移动，并且始终朝同一个方向。你被赋予了24/7监控这个房间的任务。当然，你提出了在房间里安装安全摄像头的想法，以确保所有活动都被捕捉和记录。你将一个摄像头放置在房间的角落，向下对着人体模特的脸，在这个过程中，捕捉到房间的大部分区域。使用一个摄像头，你几乎可以看到房间的所有方面。问题是，摄像头有盲点。例如，你将无法直接看到摄像头下方（由于物理上的无法看到那里）和人体模特后面（因为人体模特本身阻挡了摄像头的视线）。作为一个聪明人，你向对面的角落添加了第二个摄像头，位于人体模特后面，以补偿第一个摄像头的盲点。使用两个摄像头，你现在可以从安全办公室看到超过99%的房间。
- en: 'In this example, the room represents the original feature space of data and
    the mannequin represents a data-point, standing at a certain section of the feature
    space. More formally, I''m asking you to consider a three-dimensional feature
    space with a single data-point:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，房间代表数据的原始特征空间，而人体模特代表一个数据点，位于特征空间的一个特定部分。更正式地说，我要求你考虑一个三维特征空间和一个单一的数据点：
- en: '*[X, Y, Z]*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*[X, Y, Z]*'
- en: 'To try and capture this data-point with a single camera is like squashing down
    our dataset to have only one new dimension, namely, the data seen by camera one:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试用单个相机捕捉这个数据点就像将我们的数据集压缩成只有一个新维度，即相机一看到的数据：
- en: '*[X, Y, Z] ≈ [C1]*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*[X, Y, Z] ≈ [C1]*'
- en: 'However, only using one dimension likely will not be enough, as we were able
    to conceive blind spots for that single camera so we added a second camera:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅使用一个维度可能是不够的，因为我们能够构想出那个单相机的盲点，所以我们添加了第二个相机：
- en: '*[X, Y, Z] ≈ [C1, C2]*'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*[X, Y, Z] ≈ [C1, C2]*'
- en: These two cameras (new dimensions produced by feature transformations) capture
    the data in a new way, but give us enough of the information we needed with only
    two columns instead of three. The toughest part of feature transformations is
    the suspension of our belief that the original feature space is the best. We must
    be open to the fact that there may be other mathematical axes and systems that
    describe our data just as well with fewer features, or possibly even better.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个相机（由特征转换产生的新维度）以新的方式捕捉数据，但只用两列而不是三列就提供了我们所需的大部分信息。特征转换中最困难的部分是我们对原始特征空间是最佳空间的信念的放弃。我们必须开放地接受可能存在其他数学轴和系统，它们可以用更少的特征，甚至可能更好地描述我们的数据。
- en: Dimension reduction – feature transformations versus feature selection versus
    feature construction
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度降低 – 特征转换与特征选择与特征构造
- en: 'In the last section, I mentioned how we could squish datasets to have fewer
    columns to describe data in new ways. This sounds similar to the concept of feature
    selection: removing columns from our original dataset to create a different, potentially
    better, views of our dataset by cutting out the noise and enhancing signal columns.
    While both feature selection and feature transformation are methods of performing
    dimension reduction, it is worth mentioning that they could not be more different
    in their methodologies.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我提到了如何将数据集压缩成更少的列，以用新的方式描述数据。这听起来与特征选择的概念相似：从我们的原始数据集中删除列，通过剔除噪声并增强信号列来创建不同的、可能更好的数据集视图。虽然特征选择和特征转换都是执行降维的方法，但它们在方法论上却大相径庭。
- en: Feature selection processes are limited to only being able to select features
    from the original set of columns, while feature transformation algorithms use
    these original columns and combine them in useful ways to create new columns that
    are better at describing the data than any single column from the original dataset.
    Therefore, feature selection methods reduce dimensions by isolating signal columns
    and ignoring noise columns.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择过程仅限于从原始列集中选择特征，而特征转换算法使用这些原始列并以有用的方式将它们组合起来，创建出比原始数据集中任何单列都更好地描述数据的新的列。因此，特征选择方法通过隔离信号列和忽略噪声列来降低维度。
- en: Feature transformation methods create new columns using hidden structures in
    the original datasets to produce an entirely new, structurally different dataset.
    These algorithms create brand new columns that are so powerful that we only need
    a few of them to explain our entire dataset accurately.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 特征转换方法通过使用原始数据集中的隐藏结构来创建新的列，从而产生一个完全不同、结构上不同的数据集。这些算法创建了全新的列，它们如此强大，我们只需要其中的一小部分就能准确地解释整个数据集。
- en: 'We also mentioned that feature transformation works by producing new columns
    that capture the essence (variance) of the data. This is similar to the crux of
    feature construction: creating new features for the purpose of capturing latent
    structures in data. Again, we should mention that these two different processes
    achieve similar results using vastly different methods.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提到，特征转换通过产生新的列来捕捉数据的本质（方差）。这与特征构造的核心思想相似：为了捕捉数据中的潜在结构而创建新的特征。再次强调，这两个不同的过程使用截然不同的方法实现了类似的结果。
- en: Feature construction is again limited to constructing new columns using simple
    operations (addition, multiplication, and so on) between a few columns at a time.
    This implies that any constructed features using classical feature construction
    are constructed using only a few columns from the original dataset at a time.
    If our goal is to create enough features to capture all possible feature interactions,
    that might take an absurd number of additional columns. For example, if given
    a dataset had 1,000 features or more, we would need to create tens of thousands
    of columns to construct enough features to capture even a subset of all possible
    feature interactions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 特征构建再次仅限于通过每次在几个列之间进行简单操作（加法、乘法等）来构建新的列。这意味着任何使用经典特征构建方法构建的特征都只使用原始数据集的一小部分列。如果我们目标是创建足够多的特征来捕捉所有可能的特征交互，那么可能需要大量的额外列。例如，如果给定的数据集有1,000个特征或更多，我们需要创建数以万计的列来构建足够多的特征来捕捉所有可能特征交互的子集。
- en: Feature transformation methods are able to utilize small bits of information
    from all columns in every new super-column, so we do not need to create an inordinate
    amount of new columns to capture latent feature interactions. Due to the nature
    of feature transformation algorithms and its use of matrixes/linear algebra, feature
    transformation methods never create more columns than we start with, and are still
    able to extract the latent structure that features construction columns attempt
    to extract.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 特征转换方法能够利用每个新超列中所有列的一小部分信息，因此我们不需要创建大量的新列来捕捉潜在的特征交互。由于特征转换算法的性质及其对矩阵/线性代数的使用，特征转换方法永远不会创建比我们开始时更多的列，并且仍然能够提取特征构建列试图提取的潜在结构。
- en: 'Feature transformation algorithms are able to *construct* new features by *selecting*
    the best of all columns and combining this latent structure with a few brand new
    columns. In this way, we may consider feature transformation as one of the most
    powerful sets of algorithms that we will discuss in this text. That being said,
    it is time to introduce our first algorithm and dataset in the book: **Principal
    Components Analysis** (**PCA**) and the `iris` dataset.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 特征转换算法能够通过选择所有列中的最佳列并组合这种潜在结构与一些全新的列来*构建*新的特征。通过这种方式，我们可以将特征转换视为本文中将要讨论的最强大的算法集之一。话虽如此，现在是时候介绍本书中的第一个算法和数据集了：**主成分分析**（**PCA**）和`iris`数据集。
- en: Principal Component Analysis
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Principal Component Analysis is a technique that takes datasets that have several
    correlated features and projects them onto a coordinate (axis) system that has
    fewer correlated features. These new, uncorrelated features (which I referred
    to before as a super-columns) are called **principal components***.* The principal
    components serve as an alternative coordinate system to the original feature space
    that requires fewer features and captures as much variance as possible. If we
    refer back to our example with the cameras, the principal components are exemplified
    by the cameras themselves.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析是一种技术，它将具有多个相关特征的数据集投影到一个具有较少相关特征的坐标（轴）系统上。这些新的、不相关的特征（我之前称之为超列）被称为**主成分**。主成分作为替代坐标系统，用于原始特征空间，它需要更少的特征，并尽可能多地捕捉方差。如果我们回顾我们关于相机的例子，主成分就是相机本身。
- en: Put another way, the goal of the PCA is to identify patterns and latent structures
    within datasets in order to create new columns and use these columns instead of
    the original features. Just as in feature selection, if we start with a data matrix
    of size *n x d* where *n* is the number of observations and *d* is the number
    of original features, we are projecting this matrix onto a matrix of size *n x
    k *(where *k < d*).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，PCA的目标是在数据集中识别模式和潜在结构，以便创建新的列，并使用这些列而不是原始特征。正如在特征选择中，如果我们从一个大小为 *n x d*
    的数据矩阵开始，其中 *n* 是观测值的数量，*d* 是原始特征的数目，我们将这个矩阵投影到一个大小为 *n x k* 的矩阵上（其中 *k < d*）。
- en: 'Our principal components give rise to new columns that maximize the variance
    in our data. This means that each column is trying to explain the shape of our
    data. Principal components are ordered by variance explained so that the first
    principal component does the most to explain the variance of the data, while the
    second component does the second most to explain the variance of the data. The
    goal is to utilize as many components as we need in order to optimize the machine
    learning task, whether it be supervised or unsupervised learning:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主成分产生新的列，这些列最大化了数据中的方差。这意味着每一列都在尝试解释数据的形状。主成分按解释的方差排序，因此第一个主成分对解释数据的方差贡献最大，而第二个成分对解释数据的方差贡献次之。目标是利用尽可能多的成分来优化机器学习任务，无论是监督学习还是无监督学习：
- en: '![](img/ea40a6ea-891d-4855-b4a2-cc59691284da.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea40a6ea-891d-4855-b4a2-cc59691284da.png)'
- en: Feature transformation is about transforming datasets into matrices with the
    same number of rows with a reduced number of features. This is similar to the
    point of feature selection but in this case, we are concerned with the creation
    of brand new features.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 特征转换是将数据集转换成具有相同行数但特征数量减少的矩阵。这与特征选择的目的类似，但在这个情况下，我们关注的是创建全新的特征。
- en: PCA is itself an unsupervised task, meaning that it does not utilize a response
    column in order to make the projection/transformation. This matters because the
    second feature transformation algorithm that we will work with will be supervised
    and will utilize the response variable in order to create super-columns in a different
    way that optimizes predictive tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PCA本身是一个无监督任务，这意味着它不利用响应列来执行投影/转换。这很重要，因为我们将要处理的第二个特征转换算法将是监督的，并将利用响应变量以不同的方式创建优化预测任务的超级列。
- en: How PCA works
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA是如何工作的
- en: A PCA works by invoking a process called the **eigenvalue decomposition** of
    the covariance of a matrix. The mathematics behind this was first published in
    the 1930s and involves a bit of multivariable calculus and linear algebra. For
    the purposes of this text, we will skip over that and get to the good part.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PCA通过调用协方差矩阵的**特征值分解**过程来工作。这一数学原理最早在20世纪30年代发表，涉及一些多元微积分和线性代数。为了本文本的目的，我们将跳过这部分内容，直接进入重点。
- en: PCA may also work on the correlation matrix. You may choose to use the correlation
    matrix if the features are on a similar scale while covariance matrices are more
    useful when using different scales. We generally recommend using the covariance
    matrix with scaled data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PCA也可能适用于相关矩阵。如果你选择使用相关矩阵，那么特征处于相似尺度时更为合适，而协方差矩阵在处理不同尺度时更有用。我们通常推荐使用缩放数据的协方差矩阵。
- en: 'This process happens in four steps:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程分为四个步骤：
- en: Create the covariance matrix of the dataset
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集的协方差矩阵
- en: Calculate the eigenvalues of the covariance matrix
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征值
- en: Keep the top *k* eigenvalues (sorted by the descending eigenvalues)
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留最大的*k*个特征值（按特征值降序排列）
- en: Use the kept eigenvectors to transform new data-points
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用保留的特征向量来转换新的数据点
- en: Let's look at an example of this using a dataset called the `iris` dataset.
    In this fairly small dataset, we will take a look at a step by step performance
    of a PCA followed by the scikit-learn implementation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个名为`iris`数据集的例子来看看这一点。在这个相对较小的数据集中，我们将逐步查看PCA的性能，然后是scikit-learn的实现。
- en: PCA with the Iris dataset – manual example
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Iris数据集的PCA – 手动示例
- en: The `iris` dataset consists of 150 rows and four columns. Each row/observation
    represents a single flower while the columns/features represent four different
    quantitative characteristics about the flower. The goal of the dataset is to fit
    a classifier that attempts to predict one of three types of `iris` given the four
    features. The flower may be considered either a setosa, a virginica, or a versicolor.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`iris`数据集包含150行和四列。每一行/观测值代表一朵花，而列/特征代表花的四个不同的定量特征。数据集的目标是拟合一个分类器，尝试根据四个特征预测三种类型的`iris`。这朵花可以是setosa、virginica或versicolor之一。'
- en: 'This dataset is so common in the field of machine learning instruction, scikit-learn
    has a built-in module for downloading the dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集在机器学习教学领域非常常见，scikit-learn内置了一个模块用于下载数据集：
- en: 'Let''s first import the module and then extract the dataset into a variable
    called `iris`:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入模块，然后将数据集提取到一个名为`iris`的变量中：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let''s store the extracted data matrix and response variables into two
    new variables, `iris_X` and `iris_y`, respectively:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将提取的数据矩阵和响应变量分别存储到两个新变量`iris_X`和`iris_y`中：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s take a look at the names of the flowers that we are trying to predict:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们试图预测的花的名称：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Along with the names of the flowers, we can also look at the names of the features
    that we are utilizing to make these predictions:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了花的名称外，我们还可以查看我们用于做出这些预测的特征的名称：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To get a sense of what our data looks like, let''s write some code that will
    display the data-points of two of the four features:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了了解我们的数据看起来像什么，让我们编写一些代码来显示四个特征中的两个的数据点：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output of the preceding code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为前述代码的输出：
- en: '![](img/354912f3-dd99-44ba-85b7-2c7c2c9faa88.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/354912f3-dd99-44ba-85b7-2c7c2c9faa88.png)'
- en: Let us now perform a PCA of the `iris` dataset in order to obtain our principal
    components. Recall that this happens in four steps.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对`iris`数据集执行PCA，以获得我们的主成分。请记住，这需要四个步骤。
- en: Creating the covariance matrix of the dataset
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据集的协方差矩阵
- en: To calculate the covariance matrix of `iris`, we will first calculate the feature-wise
    mean vector (for use in the future) and then calculate our covariance matrix using
    NumPy.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算`iris`的协方差矩阵，我们首先将计算特征均值向量（用于未来使用），然后使用NumPy计算我们的协方差矩阵。
- en: 'The covariance matrix is a *d x d* matrix (square matrix with the same number
    of features as the number of rows and columns) that represents feature interactions
    between each feature. It is quite similar to a correlation matrix:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是一个*d x d*矩阵（具有与行数和列数相同数量的特征的方阵），它表示每个特征之间的特征交互。它与相关矩阵非常相似：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The variable `cov_mat` stores our 4 x 4 covariance matrix.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`cov_mat`存储我们的4 x 4协方差矩阵。
- en: Calculating the eigenvalues of the covariance matrix
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征值
- en: 'NumPy is a handy function that computes eigenvectors and eigenvalues that we
    can use in order to get the principal components of our `iris` dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy是一个方便的函数，可以计算特征向量和特征值，我们可以使用它来获取`iris`数据集的主成分：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Keeping the top k eigenvalues (sorted by the descending eigenvalues)
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留最大的k个特征值（按特征值降序排序）
- en: Now that we have our four eigenvalues, we will choose the appropriate number
    of them to keep to consider them principal components. We can choose all four
    if we wish, but we generally wish to choose a number less than the original number
    of features. But what is the right number? We could grid search and find the answer
    using the brute-force method, however, we have another tool in our arsenal, called
    the**scree plot**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了四个特征值，我们将选择适当的数量来考虑它们作为主成分。如果我们愿意，可以选择所有四个，但通常我们希望选择一个小于原始特征数量的数字。但正确的数字是多少？我们可以使用网格搜索并通过暴力方法找到答案，然而，我们还有另一个工具，称为**scree图**。
- en: A scree plot is a simple line graph that shows the percentage of total variance
    explained in the data by each principal component. To build this plot, we will
    sort the eigenvalues in order of descending value and plot the *cumulative* variance
    explained by each component and all components prior. In the case of `iris`, we
    will have four points on our scree plot, one for each principal component. Each
    component on its own explains a percentage of the total variance captured, and
    all components, when the percentages are added up, should account for 100% of
    the total variance in the dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Scree图是一个简单的折线图，显示了每个主成分解释的数据中的总方差百分比。为了构建这个图，我们将特征值按降序排序，并绘制每个组件及其之前所有组件解释的**累积**方差。在`iris`的情况下，我们的scree图上将有四个点，每个主成分一个。每个组件单独解释了捕获的总方差的一部分，当百分比相加时，应该占数据集中总方差的100%。
- en: 'Let''s calculate the percentage of variance explained by each eigenvector (principal
    component) by taking the eigenvalue associated with that eigenvector and dividing
    it by the sum of all eigenvalues:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过将每个特征向量（主成分）关联的特征值除以所有特征值的总和来计算每个特征向量（主成分）解释的方差百分比：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: What this is telling us is that our four principal components differ vastly
    in the amount of variance that they account for. The first principal component,
    as a single feature/column, is able to account for over 92% of the variance in
    the data. That is astonishing! This means that this single super-column theoretically
    can do nearly all of the work of the four original columns.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，我们的四个主成分在解释方差的数量上差异很大。第一个主成分作为一个单独的特征/列，能够解释数据中超过92%的方差。这是惊人的！这意味着这个单独的超列理论上可以完成四个原始列几乎所有的任务。
- en: 'To visualize our scree plot, let''s create a plot with the four principal components
    on the *x* axis and the cumulative variance explained on the *y* axis. For every
    data-point, the *y* position will represent the total percentage of variance explained
    using all principal components up until that one:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化我们的斯克里普图，让我们创建一个图，其中四个主成分在 *x* 轴上，累积方差解释在 *y* 轴上。对于每一个数据点，*y* 位置将代表使用所有主成分直到该点的总百分比方差：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is the output of the preceding code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为前述代码的输出：
- en: '![](img/33a082c7-98b2-495e-92ea-26d6c7aff81a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/33a082c7-98b2-495e-92ea-26d6c7aff81a.png)'
- en: This is telling us that the first two components, by themselves, account for
    nearly 98% of the total variance of the original dataset, meaning that if we only
    used the first two eigenvectors and used them as our new principal components,
    then we would be in good shape. We would be able to shrink the size of our dataset
    by half (from four to two columns) while maintaining integrity in performance
    and speeding up performance. We will taking a closer look at examples of machine
    learning to validate these theoretical notions in the upcoming sections.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，前两个成分单独就几乎解释了原始数据集总方差的98%，这意味着如果我们只使用前两个特征向量并将它们作为新的主成分，那么我们会做得很好。我们能够将数据集的大小减半（从四列减到两列），同时保持性能的完整性并加快性能。我们将在接下来的章节中通过机器学习的例子来更详细地研究这些理论概念。
- en: An eigenvalue decomposition will always result in as many eigenvectors as we
    have features. It is up to us to choose how many principal components we wish
    to use once they are all calculated. This highlights the fact that PCA, like most
    other algorithms in this text, is semi-supervised and require some human input.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值分解总是会产生与我们特征数量一样多的特征向量。一旦计算完毕，选择我们希望使用的多少个主成分取决于我们。这突出了PCA，就像本文中的大多数其他算法一样，是半监督的，需要一些人工输入。
- en: Using the kept eigenvectors to transform new data-points
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用保留的特征向量来转换新的数据点
- en: 'Once we decide to keep two principal components (whether we use a grid search
    module or the analysis of a scree plot to find the optimal number doesn''t matter),
    we have to be able to use these components to transform incoming, out of sample
    data-points. To do this, let''s first isolate the top two eigenvectors and store
    them in a new variable called `top_2_eigenvectors`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定保留两个主成分（无论我们使用网格搜索模块还是通过斯克里普图分析来找到最佳数量无关紧要），我们必须能够使用这些成分来转换进入样本外的数据点。为此，让我们首先隔离前两个特征向量并将它们存储在一个名为
    `top_2_eigenvectors` 的新变量中：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This array represents the top two eigenvectors:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此数组表示前两个特征向量：
- en: '`[ 0.36158968, -0.08226889, 0.85657211, 0.35884393]`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[ 0.36158968, -0.08226889, 0.85657211, 0.35884393]`'
- en: '`[-0.65653988, -0.72971237, 0.1757674 , 0.07470647]]`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[-0.65653988, -0.72971237, 0.1757674 , 0.07470647]]`'
- en: 'With these vectors in place, we can use them to project our data into the new
    and improved super-dataset by multiplying the two matrices together: `iris_X`
    and `top_2_eigenvectors`. The following image shows us how we are going to make
    sure that the numbers work out:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些向量就位的情况下，我们可以通过将两个矩阵相乘来使用它们将我们的数据投影到新的、改进的超数据集：`iris_X` 和 `top_2_eigenvectors`。以下图像展示了我们如何确保数字正确：
- en: '![](img/d1f93502-76db-48cc-9102-f480ad539c78.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1f93502-76db-48cc-9102-f480ad539c78.png)'
- en: The preceding figure shows how to utilize principal components to transform
    datasets from their original feature spaces to the new coordinate systems. In
    the case of `iris`, we take our original 150 x 4 dataset and multiply it by the
    transpose of the top two eigenvectors. We utilize the transpose to ensure that
    the matrix sizes match up. The result is a matrix with the same number of rows
    but a reduced number of columns. Each row is multiplied by the two principal components.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示展示了如何利用主成分将数据集从原始特征空间转换到新的坐标系。在`iris`的情况下，我们取原始的150 x 4数据集，并将其乘以前两个特征向量的转置。我们使用转置来确保矩阵的大小匹配。结果是具有相同行数但列数减少的矩阵。每一行都乘以两个主成分。
- en: 'By multiplying these matrices together, we are *projecting* our original dataset
    onto this new space of two dimensions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些矩阵相乘，我们正在将原始数据集**投影**到这个二维空间：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: And that's it. We have transformed our four-dimensional iris data into a new
    matrix with only two columns. This new matrix may serve in place of the original
    dataset in our machine learning pipeline.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们已经将四维的iris数据转换成了一个只有两列的新矩阵。这个新矩阵可以在我们的机器学习流程中代替原始数据集。
- en: Scikit-learn's PCA
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-learn的PCA
- en: 'As usual, scikit-learn saves the day by implementing this procedure in an easy
    to use transformer so that we don''t have to go through that manual process each
    time we wish to use this powerful process:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，scikit-learn通过实现一个易于使用的转换器来拯救这一天，这样我们就不必每次使用这个强大的过程时都进行手动处理：
- en: 'We can import it from scikit-learn''s decomposition module:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以从scikit-learn的分解模块中导入它：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To mimic the process we performed with the `iris` dataset, let''s instantiate
    a `PCA` object with only two components:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了模拟我们在`iris`数据集上执行的过程，让我们实例化一个只有两个成分的`PCA`对象：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can fit our PCA to the data:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的PCA拟合到数据上：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s take a look at some of the attributes of the PCA object to see if they
    match up with what we achieved in our manual process. Let''s take a look at the
    `components_ attribute` of our object to see if this matches up without the `top_2_eigenvectors`
    variable:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看PCA对象的一些属性，看看它们是否与我们在手动过程中实现的结果相匹配。让我们查看对象的`components_`属性，看看它是否与没有`top_2_eigenvectors`变量相匹配：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Our two components match, almost exactly, our previous variable, `top_2_eigenvectors`.
    We say almost because the second component is actually the negative of the eigenvector
    we calculated. This is fine because, mathematically, both eigenvectors are 100%
    valid and still achieve the primary goal of creating uncorrelated columns.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的两个成分几乎与之前的变量`top_2_eigenvectors`相匹配。我们说几乎是因为第二个成分实际上是计算出的特征向量的负值。这是可以的，因为从数学上讲，这两个特征向量都是100%有效的，并且仍然实现了创建不相关列的主要目标。
- en: 'So far, this process is much less painful than what we were doing before. To
    complete the process, we need to use the transform method of the PCA object to
    project data onto our new two-dimensional plane:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，这个过程比我们之前所做的方法痛苦少得多。为了完成这个过程，我们需要使用PCA对象的transform方法将数据投影到新的二维平面上：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that our projected data does not match up with the projected data we
    got before at all. This is because the scikit-learn version of PCA automatically
    centers data in the prediction phase, which changes the outcome.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的投影数据与之前得到的投影数据完全不匹配。这是因为scikit-learn版本的PCA在预测阶段自动将数据居中，这改变了结果。
- en: 'We can mimic this by altering a single line in our version to match:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过修改我们版本中的一行来模拟这个过程：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s make a quick plot of the projected `iris` data and compare what the
    dataset looks like before and after projecting onto our new coordinate system:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们快速绘制一下投影后的`iris`数据，并比较在投影到新的坐标系前后数据集看起来如何：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is the output of the preceding code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是前面的输出：
- en: '![](img/188b6248-8cc6-4c3e-aeec-a99011012f84.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/188b6248-8cc6-4c3e-aeec-a99011012f84.png)'
- en: In our original dataset, we can see the irises in their original feature space
    along the first two columns. Notice that in our projected space, the flowers are
    much more separated from one another and also rotated on their axis a bit. It
    looks like the data clusters are *standing upright*. This phenomenon is because
    our principal components are working to capture variance in our data, and it shows
    in our plots.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的原始数据集中，我们可以看到在第一、第二列中沿原始特征空间的花。注意，在我们的投影空间中，花朵彼此之间分离得更多，并且在其轴上略有旋转。看起来数据簇是“直立”的。这种现象是因为我们的主成分正在努力捕捉数据中的方差，这在我们的图中显示出来。
- en: 'We can extract the amount of variance explained by each component as we did
    in our manual example:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提取每个成分解释的方差量，就像我们在手动示例中所做的那样：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, that we can perform all of the basic functions with scikit-learn''s PCA,
    let''s use this information to display one of the main benefits of PCA: de-correlating
    features.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经可以使用scikit-learn的PCA执行所有基本功能，让我们利用这些信息来展示PCA的一个主要好处：特征去相关。
- en: By nature, in the eigenvalue decomposition procedure, the resulting principal
    components are perpendicular to each other, meaning that they are linearly independent
    of one another.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 按照本质，在特征值分解过程中，得到的主成分彼此垂直，这意味着它们彼此之间线性无关。
- en: This is a major benefit because many machine learning models and preprocessing
    techniques make the assumption that inputted features are independent, and utilizing
    PCA ensures this for us.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个主要的好处，因为许多机器学习模型和预处理技术都假设输入的特征是独立的，而利用PCA可以确保这一点。
- en: To show this, let's create the correlation matrix of the original `iris` dataset
    and find the average linear correlation coefficient between each of the features.
    Then, we will do the same for a PCA projected dataset and compare the values.
    We expect that the average correlation of the projected dataset should be much
    closer to zero, implying that they are all linearly independent.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一点，让我们创建原始 `iris` 数据集的相关矩阵，并找出每个特征之间的平均线性相关系数。然后，我们将对PCA投影数据集做同样的事情，并比较这些值。我们预计投影数据集的平均相关性应该接近于零，这意味着它们都是线性无关的。
- en: 'Let''s begin by calculating the correlation matrix of the original `iris` dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先计算原始 `iris` 数据集的相关矩阵：
- en: 'It will be a 4 x 4 matrix where the values represent the correlation coefficient
    of every feature versus each other:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将是一个4x4的矩阵，其中的值代表每个特征与其他每个特征之间的相关系数：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will then extract all values above the diagonal of 1s to use them to find
    the average correlation between all of the features:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将提取所有对角线以上1的值，以使用它们来找出所有特征之间的平均相关性：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we will take the mean of this array:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将取这个数组的平均值：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The average correlation coefficient of the original features is `.16`, which
    is pretty small, but definitely not zero. Now, let''s create a full PCA that captures
    all four principal components:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始特征的平均相关系数是 `.16`，相当小，但绝对不是零。现在，让我们创建一个包含所有四个主成分的完整PCA：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once we''ve done this, we will use the same method as before and calculate
    the average correlation coefficient between the new, supposedly linearly independent
    columns:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们完成这个操作，我们将使用之前的方法来计算新、理论上线性无关的列之间的平均相关系数：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This shows how data projected onto principal components end up having fewer
    correlated features, which is helpful in general in machine learning.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了数据投影到主成分上最终会有更少的相关特征，这在机器学习中通常是有帮助的。
- en: How centering and scaling data affects PCA
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据中心化和缩放如何影响主成分分析（PCA）
- en: 'As with many of the transformations that we have worked with previously in
    this text, the scaling of features tends to matter a great deal to the transformations.
    PCA is no different. Previously, we mentioned that the scikit-learn version of
    PCA automatically centers data in the prediction phase, but why doesn''t it do
    so at the fitting time? If the scikit-learn PCA module goes through the trouble
    of centering data in the predict method, why doesn''t it do so while calculating
    the eigenvectors? The hypothesis here is that centering data doesn''t affect the
    principal components. Let''s test this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在本文中之前使用过的许多转换一样，特征的缩放往往对转换有很大影响。PCA也不例外。之前我们提到，scikit-learn版本的PCA在预测阶段自动对数据进行中心化，但为什么在拟合时不这样做呢？如果scikit-learn的PCA模块在预测方法中费尽心机对数据进行中心化，为什么不在计算特征向量时这样做呢？这里的假设是数据中心化不会影响主成分。让我们来测试这个假设：
- en: 'Let''s import out `StandardScaler` module from scikit-learn and center the
    `iris` dataset:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入scikit-learn的 `StandardScaler` 模块并将 `iris` 数据集进行中心化：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s take a look at the now centered dataset:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看一下现在中心化的数据集：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We get the following output for the code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下代码输出：
- en: '![](img/4ee17ee2-bd94-4bc9-b58d-cd0ec31ee608.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ee17ee2-bd94-4bc9-b58d-cd0ec31ee608.png)'
- en: 'We can then fit the PCA class that we instanstiated before, with `n_components`
    set to `2`, to our centered `iris` dataset:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将之前实例化的PCA类拟合到我们的中心化 `iris` 数据集，其中 `n_components` 设置为 `2`：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once this is done, we can call the `components_ attribute` of the PCA module
    and compare the resulting principal components with the PCs that we got with the
    original `iris` dataset:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成此操作后，我们可以调用PCA模块的`components_属性`，并将得到的特征成分与使用原始`iris`数据集得到的PCs进行比较：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It seems that the PCs that resulted from the centered data are exactly the
    same as the PCs that we saw earlier. To clarify this, let''s transform the centered
    data using the PCA module and look at the first five rows and see if they match
    up with the previously obtained projection:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来，由中心化数据产生的主成分（PCs）与我们之前看到的是完全相同的。为了澄清这一点，让我们使用PCA模块对中心化数据进行转换，并查看前五行，看看它们是否与之前获得的投影对应：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The rows match up! If we look at the graph of the projected centered data and
    the explained variance ratios, we will that these also match up:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行与行对应！如果我们查看投影中心化数据的图表和解释方差比，我们会发现这些也对应：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We get the following output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/ef82f7e6-d094-41f5-9679-68ef402b5118.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef82f7e6-d094-41f5-9679-68ef402b5118.png)'
- en: 'For percentage variance, we implement the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于百分比方差，我们实现以下操作：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The reason this is happening is because matrices have the same covariance matrix
    as their centered counterparts. If two matrices have the same covariance matrix,
    then they will have the same eignenvalue decomposition. This is why the scikit-learn
    version of PCA doesn't bother to center data while finding the eigenvalues and
    eigenvectors, because they would have found the same ones regardless of centering,
    so why add an extra, unnecessary step?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 发生这种情况的原因是因为矩阵与其中心化对应矩阵具有相同的协方差矩阵。如果两个矩阵具有相同的协方差矩阵，那么它们将具有相同的特征值分解。这就是为什么scikit-learn版本的PCA在寻找特征值和特征向量时不需要对数据进行中心化，因为无论是否中心化，它们都会找到相同的值，所以为什么还要增加一个额外的、不必要的步骤？
- en: 'Now, let''s take a look at what happens to our principal components when we
    scale data using standard z-score scaling:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看当我们使用标准z-score缩放数据时，我们的主成分会发生什么变化：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We get the output, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/75c9d3bc-0aa6-46b2-b17b-17142b8738d6.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75c9d3bc-0aa6-46b2-b17b-17142b8738d6.png)'
- en: It is worth mentioning that at this point, we have plotted the iris data in
    its original format, centered, and now scaled completely. In each graph, the data-points
    are exactly the same, but the axes are different. This is expected. Centering
    and scaling data doesn't change the shape of the data, but it does effect feature
    interaction for our feature engineering and machine learning pipelines.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，到目前为止，我们已经在原始格式、中心化和现在完全缩放的情况下绘制了iris数据。在每个图表中，数据点都是完全相同的，但坐标轴是不同的。这是预期的。中心化和缩放数据不会改变数据的形状，但它确实会影响我们的特征工程和机器学习管道中的特征交互。
- en: 'Let''s fit our PCA module on our newly scaled data and see if our PCs are different:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在新缩放的数据上拟合我们的PCA模块，看看我们的PCs是否不同：
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'These are different components, as before. PCA is scale-invariant, meaning
    that scale affects the components. Note that when we say scaling, we mean centeringand
    dividing by the standard deviation. Let''s project our dataset onto our new components
    and ensure that the newly projected data is indeed different:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是与之前不同的成分。PCA是尺度不变的，这意味着尺度会影响成分。请注意，当我们说缩放时，我们指的是中心化和除以标准差。让我们将我们的数据集投影到我们的新成分上，并确保新投影的数据确实不同：
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, let''s take a look at our explained variance ratios:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一下我们的解释方差比：
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This is interesting. Scaling our data is usually a good idea when performing
    feature engineering/machine learning and usually we recommend it to our readers,
    but why does our first component have a much lower explained variance ratio than
    it did before?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣。在执行特征工程/机器学习时，对数据进行缩放通常是一个好主意，我们通常也向我们的读者推荐这样做，但为什么我们的第一个成分的解释方差比比之前低得多？
- en: It's because once we scaled our data, the columns' covariance with one another
    became more consistent and the variance explained by each principal component
    was spread out instead of being solidified in a single PC. In practice and production,
    we generally recommend scaling, but it is a good idea to test your pipeline's
    performance on both scaled and un-scaled data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为一旦我们缩放了数据，列之间的协方差变得更加一致，每个主成分解释的方差被分散开来，而不是固化在一个单独的PC中。在实际生产和实践中，我们通常推荐缩放，但测试您的管道在缩放和非缩放数据上的性能是一个好主意。
- en: 'Let''s top off this section with a look at the projected iris data on our scaled
    data:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用查看缩放数据上的投影iris数据来结束本节：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We get the following output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/4566df9f-079e-4813-9bcd-7d20710b4f9d.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4566df9f-079e-4813-9bcd-7d20710b4f9d.png)'
- en: It is subtle, but if you look at this graph and compare it to the previous plots
    of projected data under the original and centered data, you will notice slight
    differences between them.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这很微妙，但如果你看看这个图表，并将其与原始数据和中心数据下之前的项目数据图表进行比较，你会注意到它们之间有细微的差异。
- en: A deeper look into the principal components
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解主成分
- en: 'Before we take a look at our second feature transformation algorithm, it is
    important to take a look at how principal components are interpreted:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看第二个特征变换算法之前，重要的是要看看如何解释主成分：
- en: 'Our `iris` dataset is a 150 x 4 matrix, and when we calculated our PCA components
    when `n_components` was set to `2`, we obtained a components matrix of size `2
    x 4`:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的`iris`数据集是一个150 x 4的矩阵，当我们计算`n_components`设置为`2`时的PCA组件时，我们得到了一个`2 x 4`大小的组件矩阵：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Just like in our manual example of calculating eigenvectors, the `components_`
    attribute can be used to project data using matrix multiplication. We do so by
    multiplying our original dataset with the transpose of the `components_ matrix`:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们在手动计算特征向量的例子中一样，`components_`属性可以通过矩阵乘法来投影数据。我们通过将原始数据集与`components_矩阵`的转置相乘来实现这一点：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We invoke the transpose function here so that the matrix dimensions match up.
    What is happening at a low level is that for every row, we are calculating the
    dot product between the original row and each of the principal components. The
    results of the dot product become the elements of the new row:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里调用转置函数，以便矩阵维度匹配。在底层发生的事情是，对于每一行，我们都在计算原始行与每个主成分之间的点积。点积的结果成为新行的元素：
- en: '[PRE38]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Luckily, we can rely on the built-in transform method to do this work for us:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以依赖内置的transform方法来为我们完成这项工作：
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Put another way, we can interpret each component as being acombination of the
    original columns. In this case, our first principal component is:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将每个成分解释为原始列的组合。在这种情况下，我们的第一个主成分是：
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The first scaled flower is:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个缩放后的花是：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To get the first element of the first row of our projected data, we can use
    the following formula:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到投影数据第一行的第一个元素，我们可以使用以下公式：
- en: '![](img/b58fbd99-ad4b-48bb-b7c1-2b599036fc5c.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b58fbd99-ad4b-48bb-b7c1-2b599036fc5c.png)'
- en: 'In fact, in general, for any flower with the coordinates (a, b, c, d), where
    a is the sepal length of the iris, b the sepal width, c the petal length, and
    d the petal width (this order was taken from `iris.feature_names` from before),
    the first value of the new coordinate system can be calculated by the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，一般来说，对于任何坐标为（a，b，c，d）的花朵，其中a是鸢尾花的萼片长度，b是萼片宽度，c是花瓣长度，d是花瓣宽度（这个顺序是从之前的`iris.feature_names`中取的），新坐标系统的第一个值可以通过以下公式计算：
- en: '![](img/cf9df89f-785d-4c53-bb39-8deb95e36aba.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf9df89f-785d-4c53-bb39-8deb95e36aba.png)'
- en: 'Let''s take this a step further and visualize the components in space alongside
    our data. We will truncate our original data to only keep two of its original
    features, sepal length and sepal width. The reason we are doing this is so that
    we can visualize the data easier without having to worry about four dimensions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更进一步，并在空间中将这些成分与我们的数据可视化。我们将截断原始数据，只保留其原始特征中的两个，即萼片长度和萼片宽度。我们这样做的原因是，这样我们可以更容易地可视化数据，而不用担心四个维度：
- en: '[PRE42]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We get the output, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/0d09ec40-d4da-42b9-9a18-9df50d0e0677.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d09ec40-d4da-42b9-9a18-9df50d0e0677.png)'
- en: We can see a cluster of flowers (**setosas**) on the bottom left and a larger
    cluster of both **versicolor** and **virginicia** flowers on the top right. It
    appears obvious right away that the data, as a whole, is stretched along a diagonal
    line stemming from the bottom left to the top right. The hope is that our principal
    components also pick up on this and rearrange our data accordingly.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到左下角有一簇**setosa**花，右上角有一簇较大的**versicolor**和**virginicia**花。一开始很明显，数据整体上沿着从左下角到右上角的对角线拉伸。希望我们的主成分也能捕捉到这一点，并相应地重新排列我们的数据。
- en: 'Let''s instantiate a PCA class that keeps two principal components and then
    use that class to transform our truncated **iris data** into new columns:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实例化一个保留两个主成分的PCA类，然后使用这个类将我们的截断**iris数据**转换成新的列：
- en: '[PRE43]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We get the output, as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下输出：
- en: '![](img/ae0c4b07-f56d-4e03-aafc-8479da51f616.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae0c4b07-f56d-4e03-aafc-8479da51f616.png)'
- en: '**PCA 1**, our first principal component, should be carrying the majority of
    the variance within it, which is why the projected data is spread out mostly across
    the new *x* axis. Notice how the scale of the *x* axis is between -3 and 3 while
    the *y* axis is only between -0.4 and 0.6\. To further clarify this, the following
    code block will graph both the original and projected iris scatter plots, as well
    as an overlay the principal components of `twodim_pca` on top of them, in both
    the original coordinate system as well as the new coordinate system.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**PCA 1**，我们的第一个主成分，应该包含大部分方差，这就是为什么投影数据主要分布在新的*x*轴上。注意*x*轴的刻度在-3到3之间，而*y*轴只在-0.4到0.6之间。为了进一步阐明这一点，以下代码块将绘制原始和投影的鸢尾花散点图，以及将`twodim_pca`的主成分叠加在它们之上，既在原始坐标系中，也在新坐标系中。'
- en: 'The goal is to interpret the components as being guiding vectors, showing the
    way in which the data is moving and showing how these guiding vectors become perpendicular
    coordinate systems:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将成分解释为引导向量，显示数据移动的方式以及这些引导向量如何成为垂直坐标系：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This is the **Original Iris Dataset** and **Projected Data** using PCA:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是**原始鸢尾花数据集**和**使用PCA投影的数据**：
- en: '![](img/b7706a4e-311a-4d62-b81b-852c2153ed34.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7706a4e-311a-4d62-b81b-852c2153ed34.png)'
- en: The top graph is showing the principal components as they exist in the original
    data's axis system. They are not perpendicular and they are pointing in the direction
    that the data naturally follows. We can see that the longer of the two vectors,
    the first principal component, is clearly following that diagonal direction that
    the iris data is following the most.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部图表显示了原始数据轴系统中的主成分。它们不是垂直的，并且指向数据自然遵循的方向。我们可以看到，两个向量中较长的第一个主成分明显遵循鸢尾花数据最遵循的对角线方向。
- en: The secondary principal component is pointing in a direction of variance that
    explains a portion of the shape of the data, but not all of it. The bottom graph
    shows the projected iris data onto these new components accompanied by the same
    components, but acting as perpendicular coordinate systems. They have become the
    new *x* and *y* axes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 次级主成分指向一个解释数据形状一部分但不是全部的方差方向。底部图表显示了将鸢尾花数据投影到这些新成分上，并伴随着相同的成分，但作为垂直坐标系。它们已成为新的*x*轴和*y*轴。
- en: The PCA is a feature transformation tool that allows us to construct brand new
    super-features as linear combinations of previous features. We have seen that
    these components carry the maximum amount of variance within them, and act as
    new coordinate systems for our data. Our next feature transformation algorithm
    is similar in that it, too, will extract components from our data, but it does
    so in a machine learning-type manner.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种特征转换工具，它允许我们通过先前特征的线性组合构建全新的超级特征。我们已经看到，这些成分包含最大量的方差，并作为我们数据的新坐标系。我们的下一个特征转换算法与此类似，因为它也会从我们的数据中提取成分，但它以机器学习的方式这样做。
- en: Linear Discriminant Analysis
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: '**Linear Discriminant Analysis** (**LDA**) is a feature transformation technique
    as well as a supervised classifier. It is commonly used as a preprocessing step
    for classification pipelines. The goal of LDA, like PCA, is to extract a new coordinate
    system and project datasets onto a lower-dimensional space. The main difference
    between LDA and PCA is that instead of focusing on the variance of the data as
    a whole like PCA, LDA optimizes the lower-dimensional space for the best class
    separability. This means that the new coordinate system is more useful in finding
    decision boundaries for classification models, which is perfect for us when building
    classification pipelines.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）是一种特征转换技术，也是一种监督分类器。它通常用作分类流程的预处理步骤。LDA的目标，就像PCA一样，是提取一个新的坐标系并将数据集投影到低维空间。LDA与PCA的主要区别在于，与PCA关注数据的整体方差不同，LDA优化低维空间以实现最佳类别可分性。这意味着新的坐标系在寻找分类模型的决策边界方面更有用，这对于我们构建分类流程来说非常完美。'
- en: The reason that LDA is extremely useful is that separating based on class separability helps
    us avoid overfitting in our machine learning pipelines. This is also known as
    *preventing the curse of dimensionality*. LDA also reduces computational costs.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: LDA之所以极其有用，是因为基于类别可分性的分离有助于我们在机器学习流程中避免过拟合。这也被称为*防止维度灾难*。LDA还可以降低计算成本。
- en: How LDA works
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA是如何工作的
- en: 'LDA works as a dimensionality reduction tool, just like PCA, however instead
    of calculating the eigenvalues of the covariance matrix of the data as a whole,
    LDA calculates eigenvalues and eigenvectors of within-class and between-class scatter
    matrices. Performing LDA can be broken down into five steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: LDA作为一个降维工具，就像PCA一样工作，然而，LDA不是计算整个数据集协方差矩阵的特征值，而是计算类内和类间散布矩阵的特征值和特征向量。执行LDA可以分为五个步骤：
- en: Calculate mean vectors of each class
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个类别的均值向量
- en: Calculate within-class and between-class scatter matrices
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算类内和类间散布矩阵
- en: Calculate eigenvalues and eigenvectors for ![](img/5f94c65e-d2c4-4dfe-99a3-7651808d28cc.png)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为![图片](img/5f94c65e-d2c4-4dfe-99a3-7651808d28cc.png)计算特征值和特征向量
- en: Keep the top k eigenvectors by ordering them by descending eigenvalues
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过按降序排列特征值来保留最大的k个特征向量
- en: Use the top eigenvectors to project onto the new space
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最大的特征向量将数据投影到新的空间
- en: Let's look at an example.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。
- en: Calculating the mean vectors of each class
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算每个类别的均值向量
- en: 'First, we need to calculate a column-wise mean vector for each of our classes.
    One for `setosa`, one for `versicolor`, and another for `virginica`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要为我们的每个类别计算一个列向量的均值向量。一个用于`setosa`，一个用于`versicolor`，另一个用于`virginica`：
- en: '[PRE45]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Calculating within-class and between-class scatter matrices
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算类内和类间散布矩阵
- en: 'We will now calculate a **within-class** scatter matrix, defined by the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将计算一个**类内**散布矩阵，其定义为以下：
- en: '![](img/df75dfcc-4a6c-404a-9943-42dfacfee667.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/df75dfcc-4a6c-404a-9943-42dfacfee667.png)'
- en: 'Where we define *S[i]* as:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 *S[i]* 为：
- en: '![](img/535dfd0c-a501-43e8-a657-a23ac23c137c.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/535dfd0c-a501-43e8-a657-a23ac23c137c.png)'
- en: 'Here, *m*[*i* ]represents the mean vector for the *i* class, and a **between-class
    scatter **matrix defined by the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m*[*i*] 代表第 *i* 类的均值向量，以及以下定义的**类间散布矩阵**：
- en: '![](img/c2f68b67-ceba-4e44-8cc7-ea5b743fdf4a.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2f68b67-ceba-4e44-8cc7-ea5b743fdf4a.png)'
- en: '*m* is the overall mean of the dataset, *m[i]* is the sample mean for each
    class, and *N[i]* is the sample size for each class (number of observations per
    class):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*m* 是数据集的整体均值，*m[i]* 是每个类别的样本均值，*N[i]* 是每个类别的样本大小（每个类别的观测数）：'
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Within-class and between-class scatter matrices are generalizations of a step
    in the ANOVA test (mentioned in the previous chapter). The idea here is to decompose our
    iris dataset into two distinct parts.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 类内和类间散布矩阵是ANOVA测试中一个步骤的推广（如前一章所述）。这里的想法是将我们的鸢尾花数据集分解成两个不同的部分。
- en: Once we have calculated these matrices, we can move onto the next step, which
    uses matrix algebra to extract linear discriminants.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了这些矩阵，我们就可以进行下一步，即使用矩阵代数来提取线性判别式。
- en: Calculating eigenvalues and eigenvectors for SW-1SB
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算SW-1SB的特征值和特征向量
- en: 'Just as we did in PCA, we rely on eigenvalue decompositions of a specific matrix.
    In the case of LDA, we will be decomposing the matrix **![](img/f9376114-0b5d-4325-b3af-b5f0bb83ac01.png)**:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在PCA中所做的那样，我们依赖于特定矩阵的特征值分解。在LDA的情况下，我们将分解矩阵**![图片](img/f9376114-0b5d-4325-b3af-b5f0bb83ac01.png)**：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note that the third and fourth eigenvalues are basically zero. This is because
    the way LDA is trying to work is by drawing decision boundaries between our classes.
    Because we only have three classes in the iris, we may only draw up to two decision
    boundaries. In general, fitting LDA to a dataset with n classes will only produce
    up to n-1 components.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第三个和第四个特征值基本上为零。这是因为LDA试图通过在我们类别之间绘制决策边界来工作。因为我们只有三个鸢尾花类别，所以我们可能只能绘制两个决策边界。一般来说，将LDA拟合到具有n个类别的数据集将只产生至多n-1个成分。
- en: Keeping the top k eigenvectors by ordering them by descending eigenvalues
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过按降序排列特征值来保留最大的k个特征向量
- en: 'As in PCA, we only wish to keep the eigenvectors that are doing most of the
    work:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 正如PCA一样，我们只想保留做大部分工作的特征向量：
- en: '[PRE48]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can look at the ratio of explained variance in each component/linear discriminant
    by dividing each eigenvalue by the sum total of all eigenvalues:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将每个特征值除以所有特征值的总和来查看每个成分/线性判别式的解释方差比：
- en: '[PRE49]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: It appears that the first component is doing a vast majority of the work and
    holding over 99% of the information on its own.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来第一个成分做了大部分工作，并且独自占据了超过99%的信息。
- en: Using the top eigenvectors to project onto the new space
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用最大的特征向量将数据投影到新的空间
- en: 'Now that we have our components, let''s plot the projected iris data by first
    using the eigenvectors to project the original data onto the new space and then
    calling our plot function:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这些成分，让我们通过首先使用特征向量将原始数据投影到新空间，然后调用我们的绘图函数来绘制投影的鸢尾花数据：
- en: '[PRE50]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We get the following output:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/6860bc3a-3cd5-4604-95d1-175c6e72de30.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6860bc3a-3cd5-4604-95d1-175c6e72de30.png)'
- en: Notice that in this graph, the data is *standing* almost fully upright (even
    more than PCA projected data), as if the LDA components are trying to help machine
    learning models separate the flowers as much as possible by drawing these decision
    boundaries and providing eigenvectors/linear discriminants. This helps us project
    data into a space that separates classes as much as possible.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个图中，数据几乎完全垂直（甚至比 PCA 投影数据更垂直），就像 LDA 成分试图通过绘制这些决策边界并提供特征向量/线性判别分析来尽可能帮助机器学习模型分离花朵一样。这有助于我们将数据投影到尽可能分离类的空间中。
- en: How to use LDA in scikit-learn
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 scikit-learn 中使用 LDA
- en: 'LDA has an implementation in scikit-learn to avoid this very laborious process.
    It is easily imported:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 在 scikit-learn 中有一个实现，以避免这个过程非常繁琐。它很容易导入：
- en: '[PRE51]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: From there, let's use it to fit and transform our original iris data and plot
    the resulting projected dataset so that we may compare it to the projection using
    PCA. The biggest thing to notice in the following code block is that the fit function
    requires two inputs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，让我们用它来拟合和转换我们的原始鸢尾花数据，并绘制结果投影数据集，以便我们可以将其与使用 PCA 的投影进行比较。在下面的代码块中要注意的最大事情是
    fit 函数需要两个输入。
- en: 'Recall how we mentioned that LDA is actually a classifier disguised as a feature
    transformation algorithm. Unlike PCA, which finds components in an unsupervised
    manner (without a response variable), LDA will attempt to find the best coordinate
    system *with respect to* a response variable that optimizes for class separability.
    This implies that LDA only works if we have a response variable. If we do, we
    input the response as a second input to our fit method and let LDA do its thing:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们提到 LDA 实际上是一个伪装成特征变换算法的分类器。与 PCA 不同，PCA 以无监督的方式（没有响应变量）找到成分，而 LDA 将尝试找到最佳坐标系，该坐标系相对于响应变量优化了类可分性。这意味着
    LDA 只在我们有响应变量时才起作用。如果有，我们将响应作为 fit 方法的第二个输入，让 LDA 做其事：
- en: '[PRE52]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We get the following output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/66fb18f8-cbba-4bf9-a562-31c5492fc672.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66fb18f8-cbba-4bf9-a562-31c5492fc672.png)'
- en: 'This graph is a mirror image of the manual LDA that we performed. This is OK.
    Recall in PCA that the manual version we had contained eigenvectors that had opposite
    signs (positive versus negative). This does not affect our machine learning pipelines. Within
    the LDA module, we have some differences to keep note of. Instead of a `.components_
    attribute`, we have a `.scalings_ attribute`, which acts almost the same:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此图是手动 LDA 的镜像。这是可以的。回想一下在 PCA 中，我们手动版本的特征向量具有相反的符号（正与负）。这不会影响我们的机器学习流程。在 LDA
    模块中，我们有一些差异需要注意。我们没有 `.components_` 属性，而是有 `.scalings_` 属性，它几乎以相同的方式工作：
- en: '[PRE53]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The explained variance ratios for the two linear discriminants are exactly the
    same as the ones we calculated before and notice that they omit the third and
    fourth eigenvalues because they are virtually zero.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 两个线性判别分析的解释方差比与之前计算出的完全相同，并且注意它们省略了第三个和第四个特征值，因为它们几乎为零。
- en: 'These components, however, at first glance, look nothing like the manual eigenvectors
    that we achieved before. The reason for this is that the way that scikit-learn
    calculates the eigenvectors produces the same eigenvectors, but scaled by a scalar,
    as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些成分乍一看与之前我们得到的手动特征向量完全不同。原因是 scikit-learn 计算特征向量的方式产生了相同的特征向量，但通过一个标量进行了缩放，如下所示：
- en: '[PRE54]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The scikit-learn linear discriminants are a scalar multiplication of the manual
    eigenvectors, which means that they are both valid eigenvectors. The only difference
    is in the scaling of the projected data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的线性判别分析是对手动特征向量的标量乘积，这意味着它们都是有效的特征向量。唯一的区别在于投影数据的缩放。
- en: These components are organized as a 4 x 2 matrix, instead of the PCA components,
    which was given to us as a 2 x 4 matrix. This was a choice when developing the
    module and doesn't affect the math at all for us. LDA, like PCA, scales invariant,
    so scaling the data matters.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这些成分被组织成一个 4 x 2 的矩阵，而不是 PCA 成分，PCA 成分以 2 x 4 矩阵的形式提供给我们。这是在开发模块时做出的选择，对我们来说并不会影响数学计算。LDA，就像
    PCA 一样，具有缩放不变性，因此数据的缩放很重要。
- en: 'Let''s fit the LDA module to scaled iris data and look at the components to
    see the difference:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将LDA模块拟合到缩放后的鸢尾花数据，并查看组件以查看差异：
- en: '[PRE55]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `scalings_ attribute` (akin to PCA's `components_ attribute`) is showing
    us different arrays, which means that the projection will also be different. To
    finish our (briefer) description of LDA, let's apply the same code block that
    we did with PCA and interpret the `scalings_ arrays` as we did with the `components_
    attribute` of PCA.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`缩放_属性`（类似于PCA的`components_属性`）向我们展示了不同的数组，这意味着投影也将不同。为了完成我们对LDA（线性判别分析）的（更简短的）描述，让我们应用与PCA相同的代码块，并将`缩放_数组`解释为PCA的`components_属性`。'
- en: 'Let''s first fit and transform LDA on our truncated iris dataset, where we
    have only kept the first two features:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先对截断的鸢尾花数据集进行LDA拟合和转换，我们只保留了前两个特征：
- en: '[PRE56]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s take a look at the first five rows of our projected dataset:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们投影数据集的前五行：
- en: '[PRE57]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Our `scalings_ matrix` is now a 2 x 2 matrix (2 rows and 2 columns) where the
    columns are the components (instead of the rows being components in PCA). To adjust
    for this, let''s make a new variable called components that holds the transposed
    version of the `scalings_ attribute`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`缩放_矩阵`现在是一个2 x 2矩阵（2行2列），其中列是组件（而不是PCA中行是组件）。为了调整这一点，让我们创建一个新的变量`components`，它包含`缩放_属性`的转置版本：
- en: '[PRE58]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can see that se uses the components variable in the same way that we did
    the PCA `components_ attribute`. This implies that the projection is another linear
    combination of original columns, just as they were in PCA. It is also worth noting
    that LDA still de-correlates features, just as PCA did. To show this, let us calculate
    the correlation coefficient matrix of both the original truncated iris data and
    the correlation matrix of the projected data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，它使用与PCA `components_属性`相同的方式使用组件变量。这意味着投影是原始列的另一个线性组合，就像在PCA中一样。还值得注意的是，LDA仍然去相关特征，就像PCA一样。为了展示这一点，让我们计算原始截断的鸢尾花数据和投影数据的协方差矩阵：
- en: '[PRE59]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Note that in the top right value in each matrix, the original matrix is showing *highly*
    correlated features, while the projected data using LDA has highly independent
    features (given the close to zero correlation coefficient). To wrap up our interpretation
    of LDA before we move onto the real fun (using both PCA and LDA for machine learning),
    let''s take a look at a visualization of the `scalings_ attribute` of LDA, just
    as we did for PCA:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在每个矩阵的右上角值，原始矩阵显示的是高度相关的特征，而使用LDA投影的数据具有高度独立的特点（考虑到接近零的协方差系数）。在我们转向真正的乐趣（使用PCA和LDA进行机器学习）之前，让我们看一下LDA的`缩放_属性`的可视化，就像我们对PCA所做的那样：
- en: '[PRE60]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We get the following output:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/52940eb2-fb8c-4065-8dac-fca01817fe43.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/52940eb2-fb8c-4065-8dac-fca01817fe43.png)'
- en: Notice how the component, instead of going with the variance of the data, goes
    almost perpendicular to it; it's following the separation of the classes instead.
    Also, note how it's almost parallel with the gap between the flowers on the left
    and right side. LDA is trying to capture the separation between classes
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到组件，而不是随着数据的方差变化，几乎垂直于它；它遵循类别的分离。还要注意它几乎与左右两侧花朵之间的间隙平行。LDA试图捕捉类别的分离。
- en: In the top graph, we can see the original iris dataset with the `scalings_ vectors`
    overlaid on top of the data-points. The longer vector is pointing almost parallel
    to the large gap between the setosas on the bottom left and the rest of the flowers
    on the top right. This is indicative that the LDA is trying to point out the best
    directions to look in to separate the classes of flowers in the original coordinate
    system.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部图表中，我们可以看到原始的鸢尾花数据集，其`缩放向量`叠加在数据点上。较长的向量几乎与左下角塞托萨花和右上角其他花朵之间的大间隙平行。这表明LDA正在尝试指出在原始坐标系中分离花朵类别的最佳方向。
- en: It is important to note here that the `scalings_ attribute` of LDA does not
    correlate 1:1 to the new coordinate system as it did in PCA. This is because the
    goal of `scalings_` is not to create a new coordinate system, but just to point
    in the direction of boundaries in the data that optimizes for class separability.
    We will not go into detail about the calculation of these new coordinate systems
    as we did with PCA. It is sufficient to understand that the main difference between
    PCA and LDA is that PCA is an unsupervised method that captures the variance of
    the data as a whole whereas LDA, a supervised method, uses the response variable
    to capture class separability.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的是，LDA的`scalings_`属性与PCA中的新坐标系不呈1:1的相关性。这是因为`scalings_`的目标不是创建一个新的坐标系，而只是指向数据中边界方向，这些方向优化了类分离性。我们不会像PCA那样详细讨论这些新坐标系的计算。理解PCA和LDA的主要区别就足够了，PCA是一个无监督方法，它捕捉数据的整体方差，而LDA是一个监督方法，它使用响应变量来捕捉类分离性。
- en: Limitations of supervised feature transformations like LDA mean that they cannot
    help with tasks such as clustering, whereas PCA can help. This is because clustering
    is an unsupervised task and does not have a response variable for LDA to use.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 监督特征转换（如LDA）的局限性意味着它们无法帮助诸如聚类等任务，而PCA则可以。这是因为聚类是一个无监督任务，没有LDA可以使用的响应变量。
- en: LDA versus PCA – iris dataset
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA与PCA – 爱ris数据集
- en: Finally, we arrive at the moment where we can try using both PCA and LDA in
    our machine learning pipelines. Because we have been working with the `iris` dataset
    extensively in this chapter, we will continue to demonstrate the utility of both
    LDA and PCA as feature transformational pre-processing steps for supervised and
    unsupervised machine learning.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到了一个时刻，可以尝试在我们的机器学习管道中使用PCA和LDA。因为我们在这章中广泛使用了`iris`数据集，我们将继续展示LDA和PCA作为监督和未监督机器学习的特征转换预处理步骤的效用。
- en: 'We will start with supervised machine learning and attempt to build a classifier
    to recognize the species of flower given the four quantitative flower traits:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从监督机器学习开始，并尝试构建一个分类器，根据四种定量花性状来识别花的种类：
- en: 'We begin by importing three modules from scikit-learn:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从scikit-learn导入三个模块：
- en: '[PRE61]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We will use KNN as our supervised model and the pipeline module to combine
    our KNN model with our feature transformation tools to create machine learning
    pipelines that can be cross-validated using the `cross_val_score` module. We will
    try a few different machine learning pipelines and record their performance:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用KNN作为我们的监督模型，并使用管道模块将我们的KNN模型与特征转换工具结合，以创建可以使用`cross_val_score`模块进行交叉验证的机器学习管道。我们将尝试几个不同的机器学习管道并记录它们的性能：
- en: 'Let''s begin by creating three new variables, one to hold our LDA, one to hold
    our PCA, and another to hold a KNN model:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从创建三个新变量开始，一个用于存储我们的LDA，一个用于存储我们的PCA，另一个用于存储KNN模型：
- en: '[PRE62]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s invoke the KNN model without any transformational techniques to get
    the baseline accuracy. We will use this to compare the two feature transformation
    algorithms:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们不使用任何转换技术来调用KNN模型，以获取基线准确率。我们将使用这个结果来比较两个特征转换算法：
- en: '[PRE63]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The baseline accuracy to beat is 98.04%. Let''s use our LDA, which keeps only
    the most powerful component:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要击败的基线准确率是98.04%。让我们使用我们的LDA，它只保留最强大的组件：
- en: '[PRE64]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'It seems that only using a single linear discriminant isn''t enough to beat
    our baseline accuracy. Let us now try the PCA. Our hypothesis here is that the
    PCA will not outperform the LDA for the sole reason that the PCA is not trying
    to optimize for class separation as LDA is:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 似乎仅使用单个线性判别分析不足以击败我们的基线准确率。现在让我们尝试PCA。我们的假设是，PCA不会优于LDA，仅仅是因为PCA不是像LDA那样试图优化类分离：
- en: '[PRE65]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Definitely the worst so far.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 肯定是目前最差的。
- en: 'It is worth exploring whether adding another LDA component will help us:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 值得探索的是，添加另一个LDA组件是否会帮助我们：
- en: '[PRE66]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'With two components, we are able to achieve the original accuracy! This is
    great, but we want to do better than our baseline. Let''s see if a feature selection
    module from the last chapter can help us. Let''s import and use the `SelectKBest`
    module and see if statistical feature selection would best our LDA module:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个组件，我们能够达到原始准确率！这很好，但我们想做得比基线更好。让我们看看上一章中的特征选择模块是否能帮助我们。让我们导入并使用`SelectKBest`模块，看看统计特征选择是否能优于我们的LDA模块：
- en: '[PRE67]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Our LDA with two components is so far winning. In production, it is quite common
    to use both unsupervised and supervised feature transformations. Let''s set up
    a `GridSearch` module to find the best combination across:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们具有两个成分的LDA迄今为止是获胜的。在生产中，同时使用无监督和监督特征变换是很常见的。让我们设置一个`GridSearch`模块，以找到以下最佳组合：
- en: Scaling data (with or without mean/std)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据缩放（带或不带均值/标准差）
- en: PCA components
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA成分
- en: LDA components
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA成分
- en: KNN neighbors
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KNN邻居
- en: 'The following code block is going to set up a function called `get_best_model_and_accuracy` which
    will take in a model (scikit-learn pipeline or other), a parameter grid in the
    form of a dictionary, our `X` and `y` datasets, and output the result of the grid
    search module. The output will be the model''s best performance (in terms of accuracy),
    the best parameters that led to the best performance, the average time it took
    to fit, and the average time it took to predict:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块将设置一个名为`get_best_model_and_accuracy`的函数，该函数将接受一个模型（scikit-learn管道或其他），一个字典形式的参数网格，我们的`X`和`y`数据集，并输出网格搜索模块的结果。输出将是模型的最佳性能（以准确率衡量），导致最佳性能的最佳参数，拟合的平均时间和预测的平均时间：
- en: '[PRE68]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once we have our function set up to take in models and parameters, let''s use
    it to test our pipeline with our combinations of scaling, PCA, LDA, and KNN:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了接受模型和参数的函数，让我们使用它来测试我们的管道，包括缩放、PCA、LDA和KNN的组合：
- en: '[PRE69]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The best accuracy so far (near 99%) uses a combination of scaling, PCA, and
    LDA. It is common to correctly use all three of these algorithms in the same pipelines
    and perform hyper-parameter tuning to fine-tune the process. This shows us that
    more often than not, the best production-ready machine learning pipelines are
    in fact a combination of multiple feature engineering methods.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止最佳准确率（接近99%）使用缩放、PCA和LDA的组合。正确使用这三个算法在同一管道中，并执行超参数调整以微调过程是很常见的。这表明，很多时候，最佳的生产就绪机器学习管道实际上是由多种特征工程方法组合而成的。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: To summarize our findings, both PCA and LDA are feature transformation tools
    in our arsenal that are used to find optimal new features to use. LDA specifically
    optimizes for class separation while PCA works in an unsupervised way to capture
    variance in the data in fewer columns. Usually, the two are used in conjunction
    with supervised pipelines, as we showed in the iris pipeline. In the final chapter,
    we will go through two longer case studies that utilize both PCA and LDA for text
    clustering and facial recognition software.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们的发现，PCA和LDA都是我们工具箱中的特征变换工具，用于寻找最佳的新特征。LDA特别优化类分离，而PCA以无监督方式工作，以在更少的列中捕获数据中的方差。通常，这两个算法与监督管道结合使用，正如我们在鸢尾花管道中所示。在最后一章，我们将通过两个更长的案例研究来介绍，这两个案例研究都利用了PCA和LDA进行文本聚类和面部识别软件。
- en: PCA and LDA are extremely powerful tools, but have limitations. Both of them
    are linear transformations, which means that they can only create linear boundaries
    and capture linear qualities in our data. They are also static transformations.
    No matter what data we input into a PCA or LDA, the output is expected and mathematical.
    If the data we are using isn't a good fit for PCA or LDA (they exhibit non-linear
    qualities, for example, they are circular), then the two algorithms will not help
    us, no matter how much we grid search.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和LDA是极其强大的工具，但存在局限性。两者都是线性变换，这意味着它们只能创建线性边界并捕获数据中的线性特性。它们也是静态变换。无论我们输入什么数据到PCA或LDA中，输出都是预期和数学的。如果我们使用的数据不适合PCA或LDA（例如，它们表现出非线性特性，它们是圆形的），那么这两个算法将不会帮助我们，无论我们进行多少网格搜索。
- en: The next chapter will focus on feature learning algorithms. These are arguably
    the most powerful feature engineering algorithms. They are built to learn new
    features based on the input data without assuming qualities such as PCA and LDA.
    In this chapter, we will use complex structures including neural networks to achieve
    the highest level of feature engineering yet.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重点介绍特征学习算法。这些算法可以说是最强大的特征工程算法。它们旨在根据输入数据学习新特征，而不假设PCA和LDA等特性。在本章中，我们将使用包括神经网络在内的复杂结构，以达到迄今为止最高级别的特征工程。
