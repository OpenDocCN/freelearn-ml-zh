- en: Feature Transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, in this text, we have encountered feature engineering tools from what
    seems like all possible angles of data. From analyzing tabular data in order to
    ascertain levels of data to constructing and selecting columns using statistical
    measures in order to optimize our machine learning pipelines, we have been on
    a remarkable journey of dealing with features in our data.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning once more that enhancements of machine learning come
    in many forms. We generally consider our two main metrics as accuracy and prediction/fit
    times. This means that if we can utilize feature engineering tools to make our
    pipeline have higher accuracy in a cross-validated setting, or be able to fit
    and/or predict data quicker, then we may consider that a success. Of course, our
    ultimate hope is to optimize for both accuracy and time, giving us a much better
    pipeline to work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The past five chapters have dealt with what is considered classical feature
    engineering. We have looked at five main categories/steps in feature engineering
    so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory data analysis**: In the beginning of our work with machine learning
    pipelines, before even touching machine learning algorithms or feature engineering
    tools, it is encouraged to perform some basic descriptive statistics on our datasets
    and create visualizations to better understand the nature of the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature understanding**: Once we have a sense of the size and shape of the
    data, we should take a closer look at each of the columns in our dataset (if possible)
    and outline characteristics, including the level of data, as that will dictate
    how to clean specific columns if necessary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature improvement**: This phase is about altering data values and entire
    columns by imputing missing values depending on the level of the columns and performing
    dummy variable transformations and scaling operations if possible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature construction**: Once we have the best possible dataset at our disposal,
    we can think about constructing new columns to account for feature interaction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: In the selection phase of our pipeline, we take all
    original and constructed columns and perform (usually univariate) statistical
    tests in order to isolate the best performing columns for the purpose of removing
    noise and speeding up calculations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure sums up this procedure and shows us how to think about
    each step in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84d94f72-4d12-4648-a745-0c7b592d5869.png)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an example of a machine learning pipeline using methods from earlier
    in this text. It consists of five main steps: analysis, understanding, improvement,
    construction, and selection. In the upcoming chapters, we will be focusing on
    a new method of transforming data that partly breaks away from this classical
    notion.'
  prefs: []
  type: TYPE_NORMAL
- en: At this stage of the book, the reader is more than ready to start tackling the
    datasets of the world with reasonable confidence and expectations of performance.
    The following two [Chapters 6](8dc49afd-2a3a-4063-9c38-ac6a049bbfe6.xhtml), *Feature
    Transformations*, and [Chapter 7](e1c6751c-a892-4cf3-9c54-53e9bb3e1431.xhtml), *Feature
    Learning**, *will focus on two subsets of feature engineering that are quite heavy
    in both programming and mathematics, specifically linear algebra. We will, as
    always, do our best to explain all lines of code used in this chapter and only
    describe mathematical procedures where necessary.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will deal with **feature transformations**, a suite of algorithms
    designed to alter the internal structure of data to produce mathematically superior
    *super-columns, *while the following chapter will focus on feature learning using
    non-parametric algorithms (those that do not depend on the shape of the data)
    to automatically learn new features. The final chapter of this text contains several
    worked out case studies to show the end-to-end process of feature engineering
    and its effects on machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let us begin with our discussion of feature transformation. As we mentioned
    before, feature transformations are a set of matrix algorithms that will structurally
    alter our data and produce what is essentially a brand new matrix of data. The
    basic idea is that original features of a dataset are the descriptors/characteristics
    of data-points and we should be able to create a new set of features that explain
    the data-points just as well, perhaps even better, with fewer columns.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a simple, rectangular room. The room is empty except for a single mannequin
    standing in the center. The mannequin never moves and is always facing the same
    way. You have been charged with the task of monitoring that room 24/7\. Of course,
    you come up with the idea of adding security cameras to the room to make sure
    that all activity is captured and recorded. You place a single camera in a top
    corner of the room, facing down to look at the face of the mannequin and, in the
    process, catch a large part of the room on camera. With one camera, you are able
    to see virtually all aspects of the room. The problem is that the camera has blind
    spots. For example, you won't be able to see directly below the camera (due to
    its physical inability to see there) and behind the mannequin (as the dummy itself
    is blocking the camera's view). Being brilliant, you add a second camera to the
    opposite top corner, behind the mannequin, to compensate for the blind spots of
    the first camera. Using two cameras, you can now see greater than 99% of the room
    from a security office.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the room represents the original feature space of data and
    the mannequin represents a data-point, standing at a certain section of the feature
    space. More formally, I''m asking you to consider a three-dimensional feature
    space with a single data-point:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[X, Y, Z]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To try and capture this data-point with a single camera is like squashing down
    our dataset to have only one new dimension, namely, the data seen by camera one:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[X, Y, Z] ≈ [C1]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, only using one dimension likely will not be enough, as we were able
    to conceive blind spots for that single camera so we added a second camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[X, Y, Z] ≈ [C1, C2]*'
  prefs: []
  type: TYPE_NORMAL
- en: These two cameras (new dimensions produced by feature transformations) capture
    the data in a new way, but give us enough of the information we needed with only
    two columns instead of three. The toughest part of feature transformations is
    the suspension of our belief that the original feature space is the best. We must
    be open to the fact that there may be other mathematical axes and systems that
    describe our data just as well with fewer features, or possibly even better.
  prefs: []
  type: TYPE_NORMAL
- en: Dimension reduction – feature transformations versus feature selection versus
    feature construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, I mentioned how we could squish datasets to have fewer
    columns to describe data in new ways. This sounds similar to the concept of feature
    selection: removing columns from our original dataset to create a different, potentially
    better, views of our dataset by cutting out the noise and enhancing signal columns.
    While both feature selection and feature transformation are methods of performing
    dimension reduction, it is worth mentioning that they could not be more different
    in their methodologies.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection processes are limited to only being able to select features
    from the original set of columns, while feature transformation algorithms use
    these original columns and combine them in useful ways to create new columns that
    are better at describing the data than any single column from the original dataset.
    Therefore, feature selection methods reduce dimensions by isolating signal columns
    and ignoring noise columns.
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation methods create new columns using hidden structures in
    the original datasets to produce an entirely new, structurally different dataset.
    These algorithms create brand new columns that are so powerful that we only need
    a few of them to explain our entire dataset accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also mentioned that feature transformation works by producing new columns
    that capture the essence (variance) of the data. This is similar to the crux of
    feature construction: creating new features for the purpose of capturing latent
    structures in data. Again, we should mention that these two different processes
    achieve similar results using vastly different methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature construction is again limited to constructing new columns using simple
    operations (addition, multiplication, and so on) between a few columns at a time.
    This implies that any constructed features using classical feature construction
    are constructed using only a few columns from the original dataset at a time.
    If our goal is to create enough features to capture all possible feature interactions,
    that might take an absurd number of additional columns. For example, if given
    a dataset had 1,000 features or more, we would need to create tens of thousands
    of columns to construct enough features to capture even a subset of all possible
    feature interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation methods are able to utilize small bits of information
    from all columns in every new super-column, so we do not need to create an inordinate
    amount of new columns to capture latent feature interactions. Due to the nature
    of feature transformation algorithms and its use of matrixes/linear algebra, feature
    transformation methods never create more columns than we start with, and are still
    able to extract the latent structure that features construction columns attempt
    to extract.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature transformation algorithms are able to *construct* new features by *selecting*
    the best of all columns and combining this latent structure with a few brand new
    columns. In this way, we may consider feature transformation as one of the most
    powerful sets of algorithms that we will discuss in this text. That being said,
    it is time to introduce our first algorithm and dataset in the book: **Principal
    Components Analysis** (**PCA**) and the `iris` dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Principal Component Analysis is a technique that takes datasets that have several
    correlated features and projects them onto a coordinate (axis) system that has
    fewer correlated features. These new, uncorrelated features (which I referred
    to before as a super-columns) are called **principal components***.* The principal
    components serve as an alternative coordinate system to the original feature space
    that requires fewer features and captures as much variance as possible. If we
    refer back to our example with the cameras, the principal components are exemplified
    by the cameras themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Put another way, the goal of the PCA is to identify patterns and latent structures
    within datasets in order to create new columns and use these columns instead of
    the original features. Just as in feature selection, if we start with a data matrix
    of size *n x d* where *n* is the number of observations and *d* is the number
    of original features, we are projecting this matrix onto a matrix of size *n x
    k *(where *k < d*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our principal components give rise to new columns that maximize the variance
    in our data. This means that each column is trying to explain the shape of our
    data. Principal components are ordered by variance explained so that the first
    principal component does the most to explain the variance of the data, while the
    second component does the second most to explain the variance of the data. The
    goal is to utilize as many components as we need in order to optimize the machine
    learning task, whether it be supervised or unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea40a6ea-891d-4855-b4a2-cc59691284da.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature transformation is about transforming datasets into matrices with the
    same number of rows with a reduced number of features. This is similar to the
    point of feature selection but in this case, we are concerned with the creation
    of brand new features.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is itself an unsupervised task, meaning that it does not utilize a response
    column in order to make the projection/transformation. This matters because the
    second feature transformation algorithm that we will work with will be supervised
    and will utilize the response variable in order to create super-columns in a different
    way that optimizes predictive tasks.
  prefs: []
  type: TYPE_NORMAL
- en: How PCA works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A PCA works by invoking a process called the **eigenvalue decomposition** of
    the covariance of a matrix. The mathematics behind this was first published in
    the 1930s and involves a bit of multivariable calculus and linear algebra. For
    the purposes of this text, we will skip over that and get to the good part.
  prefs: []
  type: TYPE_NORMAL
- en: PCA may also work on the correlation matrix. You may choose to use the correlation
    matrix if the features are on a similar scale while covariance matrices are more
    useful when using different scales. We generally recommend using the covariance
    matrix with scaled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process happens in four steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the covariance matrix of the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvalues of the covariance matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the top *k* eigenvalues (sorted by the descending eigenvalues)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the kept eigenvectors to transform new data-points
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at an example of this using a dataset called the `iris` dataset.
    In this fairly small dataset, we will take a look at a step by step performance
    of a PCA followed by the scikit-learn implementation.
  prefs: []
  type: TYPE_NORMAL
- en: PCA with the Iris dataset – manual example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `iris` dataset consists of 150 rows and four columns. Each row/observation
    represents a single flower while the columns/features represent four different
    quantitative characteristics about the flower. The goal of the dataset is to fit
    a classifier that attempts to predict one of three types of `iris` given the four
    features. The flower may be considered either a setosa, a virginica, or a versicolor.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is so common in the field of machine learning instruction, scikit-learn
    has a built-in module for downloading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first import the module and then extract the dataset into a variable
    called `iris`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s store the extracted data matrix and response variables into two
    new variables, `iris_X` and `iris_y`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the names of the flowers that we are trying to predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Along with the names of the flowers, we can also look at the names of the features
    that we are utilizing to make these predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a sense of what our data looks like, let''s write some code that will
    display the data-points of two of the four features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/354912f3-dd99-44ba-85b7-2c7c2c9faa88.png)'
  prefs: []
  type: TYPE_IMG
- en: Let us now perform a PCA of the `iris` dataset in order to obtain our principal
    components. Recall that this happens in four steps.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the covariance matrix of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To calculate the covariance matrix of `iris`, we will first calculate the feature-wise
    mean vector (for use in the future) and then calculate our covariance matrix using
    NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance matrix is a *d x d* matrix (square matrix with the same number
    of features as the number of rows and columns) that represents feature interactions
    between each feature. It is quite similar to a correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The variable `cov_mat` stores our 4 x 4 covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the eigenvalues of the covariance matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NumPy is a handy function that computes eigenvectors and eigenvalues that we
    can use in order to get the principal components of our `iris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Keeping the top k eigenvalues (sorted by the descending eigenvalues)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our four eigenvalues, we will choose the appropriate number
    of them to keep to consider them principal components. We can choose all four
    if we wish, but we generally wish to choose a number less than the original number
    of features. But what is the right number? We could grid search and find the answer
    using the brute-force method, however, we have another tool in our arsenal, called
    the**scree plot**.
  prefs: []
  type: TYPE_NORMAL
- en: A scree plot is a simple line graph that shows the percentage of total variance
    explained in the data by each principal component. To build this plot, we will
    sort the eigenvalues in order of descending value and plot the *cumulative* variance
    explained by each component and all components prior. In the case of `iris`, we
    will have four points on our scree plot, one for each principal component. Each
    component on its own explains a percentage of the total variance captured, and
    all components, when the percentages are added up, should account for 100% of
    the total variance in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the percentage of variance explained by each eigenvector (principal
    component) by taking the eigenvalue associated with that eigenvector and dividing
    it by the sum of all eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: What this is telling us is that our four principal components differ vastly
    in the amount of variance that they account for. The first principal component,
    as a single feature/column, is able to account for over 92% of the variance in
    the data. That is astonishing! This means that this single super-column theoretically
    can do nearly all of the work of the four original columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize our scree plot, let''s create a plot with the four principal components
    on the *x* axis and the cumulative variance explained on the *y* axis. For every
    data-point, the *y* position will represent the total percentage of variance explained
    using all principal components up until that one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33a082c7-98b2-495e-92ea-26d6c7aff81a.png)'
  prefs: []
  type: TYPE_IMG
- en: This is telling us that the first two components, by themselves, account for
    nearly 98% of the total variance of the original dataset, meaning that if we only
    used the first two eigenvectors and used them as our new principal components,
    then we would be in good shape. We would be able to shrink the size of our dataset
    by half (from four to two columns) while maintaining integrity in performance
    and speeding up performance. We will taking a closer look at examples of machine
    learning to validate these theoretical notions in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: An eigenvalue decomposition will always result in as many eigenvectors as we
    have features. It is up to us to choose how many principal components we wish
    to use once they are all calculated. This highlights the fact that PCA, like most
    other algorithms in this text, is semi-supervised and require some human input.
  prefs: []
  type: TYPE_NORMAL
- en: Using the kept eigenvectors to transform new data-points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we decide to keep two principal components (whether we use a grid search
    module or the analysis of a scree plot to find the optimal number doesn''t matter),
    we have to be able to use these components to transform incoming, out of sample
    data-points. To do this, let''s first isolate the top two eigenvectors and store
    them in a new variable called `top_2_eigenvectors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This array represents the top two eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[ 0.36158968, -0.08226889, 0.85657211, 0.35884393]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[-0.65653988, -0.72971237, 0.1757674 , 0.07470647]]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these vectors in place, we can use them to project our data into the new
    and improved super-dataset by multiplying the two matrices together: `iris_X`
    and `top_2_eigenvectors`. The following image shows us how we are going to make
    sure that the numbers work out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1f93502-76db-48cc-9102-f480ad539c78.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows how to utilize principal components to transform
    datasets from their original feature spaces to the new coordinate systems. In
    the case of `iris`, we take our original 150 x 4 dataset and multiply it by the
    transpose of the top two eigenvectors. We utilize the transpose to ensure that
    the matrix sizes match up. The result is a matrix with the same number of rows
    but a reduced number of columns. Each row is multiplied by the two principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'By multiplying these matrices together, we are *projecting* our original dataset
    onto this new space of two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: And that's it. We have transformed our four-dimensional iris data into a new
    matrix with only two columns. This new matrix may serve in place of the original
    dataset in our machine learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn's PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As usual, scikit-learn saves the day by implementing this procedure in an easy
    to use transformer so that we don''t have to go through that manual process each
    time we wish to use this powerful process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can import it from scikit-learn''s decomposition module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To mimic the process we performed with the `iris` dataset, let''s instantiate
    a `PCA` object with only two components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can fit our PCA to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at some of the attributes of the PCA object to see if they
    match up with what we achieved in our manual process. Let''s take a look at the
    `components_ attribute` of our object to see if this matches up without the `top_2_eigenvectors`
    variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Our two components match, almost exactly, our previous variable, `top_2_eigenvectors`.
    We say almost because the second component is actually the negative of the eigenvector
    we calculated. This is fine because, mathematically, both eigenvectors are 100%
    valid and still achieve the primary goal of creating uncorrelated columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So far, this process is much less painful than what we were doing before. To
    complete the process, we need to use the transform method of the PCA object to
    project data onto our new two-dimensional plane:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice that our projected data does not match up with the projected data we
    got before at all. This is because the scikit-learn version of PCA automatically
    centers data in the prediction phase, which changes the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can mimic this by altering a single line in our version to match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make a quick plot of the projected `iris` data and compare what the
    dataset looks like before and after projecting onto our new coordinate system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/188b6248-8cc6-4c3e-aeec-a99011012f84.png)'
  prefs: []
  type: TYPE_IMG
- en: In our original dataset, we can see the irises in their original feature space
    along the first two columns. Notice that in our projected space, the flowers are
    much more separated from one another and also rotated on their axis a bit. It
    looks like the data clusters are *standing upright*. This phenomenon is because
    our principal components are working to capture variance in our data, and it shows
    in our plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extract the amount of variance explained by each component as we did
    in our manual example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, that we can perform all of the basic functions with scikit-learn''s PCA,
    let''s use this information to display one of the main benefits of PCA: de-correlating
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: By nature, in the eigenvalue decomposition procedure, the resulting principal
    components are perpendicular to each other, meaning that they are linearly independent
    of one another.
  prefs: []
  type: TYPE_NORMAL
- en: This is a major benefit because many machine learning models and preprocessing
    techniques make the assumption that inputted features are independent, and utilizing
    PCA ensures this for us.
  prefs: []
  type: TYPE_NORMAL
- en: To show this, let's create the correlation matrix of the original `iris` dataset
    and find the average linear correlation coefficient between each of the features.
    Then, we will do the same for a PCA projected dataset and compare the values.
    We expect that the average correlation of the projected dataset should be much
    closer to zero, implying that they are all linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by calculating the correlation matrix of the original `iris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It will be a 4 x 4 matrix where the values represent the correlation coefficient
    of every feature versus each other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then extract all values above the diagonal of 1s to use them to find
    the average correlation between all of the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will take the mean of this array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The average correlation coefficient of the original features is `.16`, which
    is pretty small, but definitely not zero. Now, let''s create a full PCA that captures
    all four principal components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we''ve done this, we will use the same method as before and calculate
    the average correlation coefficient between the new, supposedly linearly independent
    columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This shows how data projected onto principal components end up having fewer
    correlated features, which is helpful in general in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: How centering and scaling data affects PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with many of the transformations that we have worked with previously in
    this text, the scaling of features tends to matter a great deal to the transformations.
    PCA is no different. Previously, we mentioned that the scikit-learn version of
    PCA automatically centers data in the prediction phase, but why doesn''t it do
    so at the fitting time? If the scikit-learn PCA module goes through the trouble
    of centering data in the predict method, why doesn''t it do so while calculating
    the eigenvectors? The hypothesis here is that centering data doesn''t affect the
    principal components. Let''s test this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import out `StandardScaler` module from scikit-learn and center the
    `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the now centered dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output for the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ee17ee2-bd94-4bc9-b58d-cd0ec31ee608.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then fit the PCA class that we instanstiated before, with `n_components`
    set to `2`, to our centered `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is done, we can call the `components_ attribute` of the PCA module
    and compare the resulting principal components with the PCs that we got with the
    original `iris` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that the PCs that resulted from the centered data are exactly the
    same as the PCs that we saw earlier. To clarify this, let''s transform the centered
    data using the PCA module and look at the first five rows and see if they match
    up with the previously obtained projection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The rows match up! If we look at the graph of the projected centered data and
    the explained variance ratios, we will that these also match up:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef82f7e6-d094-41f5-9679-68ef402b5118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For percentage variance, we implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The reason this is happening is because matrices have the same covariance matrix
    as their centered counterparts. If two matrices have the same covariance matrix,
    then they will have the same eignenvalue decomposition. This is why the scikit-learn
    version of PCA doesn't bother to center data while finding the eigenvalues and
    eigenvectors, because they would have found the same ones regardless of centering,
    so why add an extra, unnecessary step?
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at what happens to our principal components when we
    scale data using standard z-score scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75c9d3bc-0aa6-46b2-b17b-17142b8738d6.png)'
  prefs: []
  type: TYPE_IMG
- en: It is worth mentioning that at this point, we have plotted the iris data in
    its original format, centered, and now scaled completely. In each graph, the data-points
    are exactly the same, but the axes are different. This is expected. Centering
    and scaling data doesn't change the shape of the data, but it does effect feature
    interaction for our feature engineering and machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s fit our PCA module on our newly scaled data and see if our PCs are different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'These are different components, as before. PCA is scale-invariant, meaning
    that scale affects the components. Note that when we say scaling, we mean centeringand
    dividing by the standard deviation. Let''s project our dataset onto our new components
    and ensure that the newly projected data is indeed different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s take a look at our explained variance ratios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This is interesting. Scaling our data is usually a good idea when performing
    feature engineering/machine learning and usually we recommend it to our readers,
    but why does our first component have a much lower explained variance ratio than
    it did before?
  prefs: []
  type: TYPE_NORMAL
- en: It's because once we scaled our data, the columns' covariance with one another
    became more consistent and the variance explained by each principal component
    was spread out instead of being solidified in a single PC. In practice and production,
    we generally recommend scaling, but it is a good idea to test your pipeline's
    performance on both scaled and un-scaled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s top off this section with a look at the projected iris data on our scaled
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4566df9f-079e-4813-9bcd-7d20710b4f9d.png)'
  prefs: []
  type: TYPE_IMG
- en: It is subtle, but if you look at this graph and compare it to the previous plots
    of projected data under the original and centered data, you will notice slight
    differences between them.
  prefs: []
  type: TYPE_NORMAL
- en: A deeper look into the principal components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we take a look at our second feature transformation algorithm, it is
    important to take a look at how principal components are interpreted:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `iris` dataset is a 150 x 4 matrix, and when we calculated our PCA components
    when `n_components` was set to `2`, we obtained a components matrix of size `2
    x 4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like in our manual example of calculating eigenvectors, the `components_`
    attribute can be used to project data using matrix multiplication. We do so by
    multiplying our original dataset with the transpose of the `components_ matrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We invoke the transpose function here so that the matrix dimensions match up.
    What is happening at a low level is that for every row, we are calculating the
    dot product between the original row and each of the principal components. The
    results of the dot product become the elements of the new row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Luckily, we can rely on the built-in transform method to do this work for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Put another way, we can interpret each component as being acombination of the
    original columns. In this case, our first principal component is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The first scaled flower is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the first element of the first row of our projected data, we can use
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b58fbd99-ad4b-48bb-b7c1-2b599036fc5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In fact, in general, for any flower with the coordinates (a, b, c, d), where
    a is the sepal length of the iris, b the sepal width, c the petal length, and
    d the petal width (this order was taken from `iris.feature_names` from before),
    the first value of the new coordinate system can be calculated by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf9df89f-785d-4c53-bb39-8deb95e36aba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take this a step further and visualize the components in space alongside
    our data. We will truncate our original data to only keep two of its original
    features, sepal length and sepal width. The reason we are doing this is so that
    we can visualize the data easier without having to worry about four dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d09ec40-d4da-42b9-9a18-9df50d0e0677.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see a cluster of flowers (**setosas**) on the bottom left and a larger
    cluster of both **versicolor** and **virginicia** flowers on the top right. It
    appears obvious right away that the data, as a whole, is stretched along a diagonal
    line stemming from the bottom left to the top right. The hope is that our principal
    components also pick up on this and rearrange our data accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s instantiate a PCA class that keeps two principal components and then
    use that class to transform our truncated **iris data** into new columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae0c4b07-f56d-4e03-aafc-8479da51f616.png)'
  prefs: []
  type: TYPE_IMG
- en: '**PCA 1**, our first principal component, should be carrying the majority of
    the variance within it, which is why the projected data is spread out mostly across
    the new *x* axis. Notice how the scale of the *x* axis is between -3 and 3 while
    the *y* axis is only between -0.4 and 0.6\. To further clarify this, the following
    code block will graph both the original and projected iris scatter plots, as well
    as an overlay the principal components of `twodim_pca` on top of them, in both
    the original coordinate system as well as the new coordinate system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to interpret the components as being guiding vectors, showing the
    way in which the data is moving and showing how these guiding vectors become perpendicular
    coordinate systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the **Original Iris Dataset** and **Projected Data** using PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7706a4e-311a-4d62-b81b-852c2153ed34.png)'
  prefs: []
  type: TYPE_IMG
- en: The top graph is showing the principal components as they exist in the original
    data's axis system. They are not perpendicular and they are pointing in the direction
    that the data naturally follows. We can see that the longer of the two vectors,
    the first principal component, is clearly following that diagonal direction that
    the iris data is following the most.
  prefs: []
  type: TYPE_NORMAL
- en: The secondary principal component is pointing in a direction of variance that
    explains a portion of the shape of the data, but not all of it. The bottom graph
    shows the projected iris data onto these new components accompanied by the same
    components, but acting as perpendicular coordinate systems. They have become the
    new *x* and *y* axes.
  prefs: []
  type: TYPE_NORMAL
- en: The PCA is a feature transformation tool that allows us to construct brand new
    super-features as linear combinations of previous features. We have seen that
    these components carry the maximum amount of variance within them, and act as
    new coordinate systems for our data. Our next feature transformation algorithm
    is similar in that it, too, will extract components from our data, but it does
    so in a machine learning-type manner.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis** (**LDA**) is a feature transformation technique
    as well as a supervised classifier. It is commonly used as a preprocessing step
    for classification pipelines. The goal of LDA, like PCA, is to extract a new coordinate
    system and project datasets onto a lower-dimensional space. The main difference
    between LDA and PCA is that instead of focusing on the variance of the data as
    a whole like PCA, LDA optimizes the lower-dimensional space for the best class
    separability. This means that the new coordinate system is more useful in finding
    decision boundaries for classification models, which is perfect for us when building
    classification pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason that LDA is extremely useful is that separating based on class separability helps
    us avoid overfitting in our machine learning pipelines. This is also known as
    *preventing the curse of dimensionality*. LDA also reduces computational costs.
  prefs: []
  type: TYPE_NORMAL
- en: How LDA works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LDA works as a dimensionality reduction tool, just like PCA, however instead
    of calculating the eigenvalues of the covariance matrix of the data as a whole,
    LDA calculates eigenvalues and eigenvectors of within-class and between-class scatter
    matrices. Performing LDA can be broken down into five steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate mean vectors of each class
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate within-class and between-class scatter matrices
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate eigenvalues and eigenvectors for ![](img/5f94c65e-d2c4-4dfe-99a3-7651808d28cc.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep the top k eigenvectors by ordering them by descending eigenvalues
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the top eigenvectors to project onto the new space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the mean vectors of each class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to calculate a column-wise mean vector for each of our classes.
    One for `setosa`, one for `versicolor`, and another for `virginica`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Calculating within-class and between-class scatter matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now calculate a **within-class** scatter matrix, defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df75dfcc-4a6c-404a-9943-42dfacfee667.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where we define *S[i]* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/535dfd0c-a501-43e8-a657-a23ac23c137c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *m*[*i* ]represents the mean vector for the *i* class, and a **between-class
    scatter **matrix defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2f68b67-ceba-4e44-8cc7-ea5b743fdf4a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*m* is the overall mean of the dataset, *m[i]* is the sample mean for each
    class, and *N[i]* is the sample size for each class (number of observations per
    class):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Within-class and between-class scatter matrices are generalizations of a step
    in the ANOVA test (mentioned in the previous chapter). The idea here is to decompose our
    iris dataset into two distinct parts.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have calculated these matrices, we can move onto the next step, which
    uses matrix algebra to extract linear discriminants.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating eigenvalues and eigenvectors for SW-1SB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Just as we did in PCA, we rely on eigenvalue decompositions of a specific matrix.
    In the case of LDA, we will be decomposing the matrix **![](img/f9376114-0b5d-4325-b3af-b5f0bb83ac01.png)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Note that the third and fourth eigenvalues are basically zero. This is because
    the way LDA is trying to work is by drawing decision boundaries between our classes.
    Because we only have three classes in the iris, we may only draw up to two decision
    boundaries. In general, fitting LDA to a dataset with n classes will only produce
    up to n-1 components.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the top k eigenvectors by ordering them by descending eigenvalues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As in PCA, we only wish to keep the eigenvectors that are doing most of the
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look at the ratio of explained variance in each component/linear discriminant
    by dividing each eigenvalue by the sum total of all eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the first component is doing a vast majority of the work and
    holding over 99% of the information on its own.
  prefs: []
  type: TYPE_NORMAL
- en: Using the top eigenvectors to project onto the new space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our components, let''s plot the projected iris data by first
    using the eigenvectors to project the original data onto the new space and then
    calling our plot function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6860bc3a-3cd5-4604-95d1-175c6e72de30.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that in this graph, the data is *standing* almost fully upright (even
    more than PCA projected data), as if the LDA components are trying to help machine
    learning models separate the flowers as much as possible by drawing these decision
    boundaries and providing eigenvectors/linear discriminants. This helps us project
    data into a space that separates classes as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: How to use LDA in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LDA has an implementation in scikit-learn to avoid this very laborious process.
    It is easily imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: From there, let's use it to fit and transform our original iris data and plot
    the resulting projected dataset so that we may compare it to the projection using
    PCA. The biggest thing to notice in the following code block is that the fit function
    requires two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall how we mentioned that LDA is actually a classifier disguised as a feature
    transformation algorithm. Unlike PCA, which finds components in an unsupervised
    manner (without a response variable), LDA will attempt to find the best coordinate
    system *with respect to* a response variable that optimizes for class separability.
    This implies that LDA only works if we have a response variable. If we do, we
    input the response as a second input to our fit method and let LDA do its thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66fb18f8-cbba-4bf9-a562-31c5492fc672.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This graph is a mirror image of the manual LDA that we performed. This is OK.
    Recall in PCA that the manual version we had contained eigenvectors that had opposite
    signs (positive versus negative). This does not affect our machine learning pipelines. Within
    the LDA module, we have some differences to keep note of. Instead of a `.components_
    attribute`, we have a `.scalings_ attribute`, which acts almost the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The explained variance ratios for the two linear discriminants are exactly the
    same as the ones we calculated before and notice that they omit the third and
    fourth eigenvalues because they are virtually zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'These components, however, at first glance, look nothing like the manual eigenvectors
    that we achieved before. The reason for this is that the way that scikit-learn
    calculates the eigenvectors produces the same eigenvectors, but scaled by a scalar,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The scikit-learn linear discriminants are a scalar multiplication of the manual
    eigenvectors, which means that they are both valid eigenvectors. The only difference
    is in the scaling of the projected data.
  prefs: []
  type: TYPE_NORMAL
- en: These components are organized as a 4 x 2 matrix, instead of the PCA components,
    which was given to us as a 2 x 4 matrix. This was a choice when developing the
    module and doesn't affect the math at all for us. LDA, like PCA, scales invariant,
    so scaling the data matters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s fit the LDA module to scaled iris data and look at the components to
    see the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `scalings_ attribute` (akin to PCA's `components_ attribute`) is showing
    us different arrays, which means that the projection will also be different. To
    finish our (briefer) description of LDA, let's apply the same code block that
    we did with PCA and interpret the `scalings_ arrays` as we did with the `components_
    attribute` of PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first fit and transform LDA on our truncated iris dataset, where we
    have only kept the first two features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the first five rows of our projected dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `scalings_ matrix` is now a 2 x 2 matrix (2 rows and 2 columns) where the
    columns are the components (instead of the rows being components in PCA). To adjust
    for this, let''s make a new variable called components that holds the transposed
    version of the `scalings_ attribute`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that se uses the components variable in the same way that we did
    the PCA `components_ attribute`. This implies that the projection is another linear
    combination of original columns, just as they were in PCA. It is also worth noting
    that LDA still de-correlates features, just as PCA did. To show this, let us calculate
    the correlation coefficient matrix of both the original truncated iris data and
    the correlation matrix of the projected data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that in the top right value in each matrix, the original matrix is showing *highly*
    correlated features, while the projected data using LDA has highly independent
    features (given the close to zero correlation coefficient). To wrap up our interpretation
    of LDA before we move onto the real fun (using both PCA and LDA for machine learning),
    let''s take a look at a visualization of the `scalings_ attribute` of LDA, just
    as we did for PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52940eb2-fb8c-4065-8dac-fca01817fe43.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how the component, instead of going with the variance of the data, goes
    almost perpendicular to it; it's following the separation of the classes instead.
    Also, note how it's almost parallel with the gap between the flowers on the left
    and right side. LDA is trying to capture the separation between classes
  prefs: []
  type: TYPE_NORMAL
- en: In the top graph, we can see the original iris dataset with the `scalings_ vectors`
    overlaid on top of the data-points. The longer vector is pointing almost parallel
    to the large gap between the setosas on the bottom left and the rest of the flowers
    on the top right. This is indicative that the LDA is trying to point out the best
    directions to look in to separate the classes of flowers in the original coordinate
    system.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note here that the `scalings_ attribute` of LDA does not
    correlate 1:1 to the new coordinate system as it did in PCA. This is because the
    goal of `scalings_` is not to create a new coordinate system, but just to point
    in the direction of boundaries in the data that optimizes for class separability.
    We will not go into detail about the calculation of these new coordinate systems
    as we did with PCA. It is sufficient to understand that the main difference between
    PCA and LDA is that PCA is an unsupervised method that captures the variance of
    the data as a whole whereas LDA, a supervised method, uses the response variable
    to capture class separability.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of supervised feature transformations like LDA mean that they cannot
    help with tasks such as clustering, whereas PCA can help. This is because clustering
    is an unsupervised task and does not have a response variable for LDA to use.
  prefs: []
  type: TYPE_NORMAL
- en: LDA versus PCA – iris dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we arrive at the moment where we can try using both PCA and LDA in
    our machine learning pipelines. Because we have been working with the `iris` dataset
    extensively in this chapter, we will continue to demonstrate the utility of both
    LDA and PCA as feature transformational pre-processing steps for supervised and
    unsupervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with supervised machine learning and attempt to build a classifier
    to recognize the species of flower given the four quantitative flower traits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing three modules from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use KNN as our supervised model and the pipeline module to combine
    our KNN model with our feature transformation tools to create machine learning
    pipelines that can be cross-validated using the `cross_val_score` module. We will
    try a few different machine learning pipelines and record their performance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by creating three new variables, one to hold our LDA, one to hold
    our PCA, and another to hold a KNN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s invoke the KNN model without any transformational techniques to get
    the baseline accuracy. We will use this to compare the two feature transformation
    algorithms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The baseline accuracy to beat is 98.04%. Let''s use our LDA, which keeps only
    the most powerful component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that only using a single linear discriminant isn''t enough to beat
    our baseline accuracy. Let us now try the PCA. Our hypothesis here is that the
    PCA will not outperform the LDA for the sole reason that the PCA is not trying
    to optimize for class separation as LDA is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Definitely the worst so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth exploring whether adding another LDA component will help us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'With two components, we are able to achieve the original accuracy! This is
    great, but we want to do better than our baseline. Let''s see if a feature selection
    module from the last chapter can help us. Let''s import and use the `SelectKBest`
    module and see if statistical feature selection would best our LDA module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Our LDA with two components is so far winning. In production, it is quite common
    to use both unsupervised and supervised feature transformations. Let''s set up
    a `GridSearch` module to find the best combination across:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling data (with or without mean/std)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code block is going to set up a function called `get_best_model_and_accuracy` which
    will take in a model (scikit-learn pipeline or other), a parameter grid in the
    form of a dictionary, our `X` and `y` datasets, and output the result of the grid
    search module. The output will be the model''s best performance (in terms of accuracy),
    the best parameters that led to the best performance, the average time it took
    to fit, and the average time it took to predict:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our function set up to take in models and parameters, let''s use
    it to test our pipeline with our combinations of scaling, PCA, LDA, and KNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The best accuracy so far (near 99%) uses a combination of scaling, PCA, and
    LDA. It is common to correctly use all three of these algorithms in the same pipelines
    and perform hyper-parameter tuning to fine-tune the process. This shows us that
    more often than not, the best production-ready machine learning pipelines are
    in fact a combination of multiple feature engineering methods.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize our findings, both PCA and LDA are feature transformation tools
    in our arsenal that are used to find optimal new features to use. LDA specifically
    optimizes for class separation while PCA works in an unsupervised way to capture
    variance in the data in fewer columns. Usually, the two are used in conjunction
    with supervised pipelines, as we showed in the iris pipeline. In the final chapter,
    we will go through two longer case studies that utilize both PCA and LDA for text
    clustering and facial recognition software.
  prefs: []
  type: TYPE_NORMAL
- en: PCA and LDA are extremely powerful tools, but have limitations. Both of them
    are linear transformations, which means that they can only create linear boundaries
    and capture linear qualities in our data. They are also static transformations.
    No matter what data we input into a PCA or LDA, the output is expected and mathematical.
    If the data we are using isn't a good fit for PCA or LDA (they exhibit non-linear
    qualities, for example, they are circular), then the two algorithms will not help
    us, no matter how much we grid search.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will focus on feature learning algorithms. These are arguably
    the most powerful feature engineering algorithms. They are built to learn new
    features based on the input data without assuming qualities such as PCA and LDA.
    In this chapter, we will use complex structures including neural networks to achieve
    the highest level of feature engineering yet.
  prefs: []
  type: TYPE_NORMAL
