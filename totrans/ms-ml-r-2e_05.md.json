["```py\n    > library(class) #k-nearest neighbors\n    > library(kknn) #weighted k-nearest neighbors\n    > library(e1071) #SVM\n    > library(caret) #select tuning parameters\n    > library(MASS) # contains the data\n    > library(reshape2) #assist in creating boxplots\n    > library(ggplot2) #create boxplots\n    > library(kernlab) #assist with SVM feature selection\n\n```", "```py\n    > data(Pima.tr)\n    > str(Pima.tr)\n    'data.frame':200 obs. of  8 variables:\n     $ npreg: int  5 7 5 0 0 5 3 1 3 2 ...\n     $ glu  : int  86 195 77 165 107 97 83 193 142 128 ...\n     $ bp   : int  68 70 82 76 60 76 58 50 80 78 ...\n     $ skin : int  28 33 41 43 25 27 31 16 15 37 ...\n     $ bmi  : num  30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 \n       ...\n     $ ped  : num  0.364 0.163 0.156 0.259 0.133 ...\n     $ age  : int  24 55 35 26 23 52 25 24 63 31 ...\n     $ type : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 1 1 2 ...\n    > data(Pima.te)\n    > str(Pima.te)\n    'data.frame':332 obs. of  8 variables:\n     $ npreg: int  6 1 1 3 2 5 0 1 3 9 ...\n     $ glu  : int  148 85 89 78 197 166 118 103 126 119 ...\n     $ bp   : int  72 66 66 50 70 72 84 30 88 80 ...\n     $ skin : int  35 29 23 32 45 19 47 38 41 35 ...\n     $ bmi  : num  33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ...\n     $ ped  : num  0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 \n       0.704 0.263 ...\n     $ age  : int  50 31 21 26 53 51 31 33 27 29 ...\n     $ type : Factor w/ 2 levels \"No\",\"Yes\": 2 1 1 2 2 2 2 1 1 2 ...\n\n```", "```py\n    > pima <- rbind(Pima.tr, Pima.te)\n\n```", "```py\n    > str(pima)\n    'data.frame':532 obs. of  8 variables:\n     $ npreg: int  5 7 5 0 0 5 3 1 3 2 ...\n     $ glu  : int  86 195 77 165 107 97 83 193 142 128 ...\n     $ bp   : int  68 70 82 76 60 76 58 50 80 78 ...\n     $ skin : int  28 33 41 43 25 27 31 16 15 37 ...\n     $ bmi  : num  30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 \n      ...\n     $ ped  : num  0.364 0.163 0.156 0.259 0.133 ...\n     $ age  : int  24 55 35 26 23 52 25 24 63 31 ...\n     $ type : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 1 1 2 ...\n\n```", "```py\n    > pima.melt <- melt(pima, id.var = \"type\")\n\n```", "```py\n    > ggplot(data = pima.melt, aes(x = type, y = value)) + \n        geom_boxplot() + facet_wrap(~ variable, ncol = 2)\n\n```", "```py\n    > pima.scale <- data.frame(scale(pima[, -8]))\n    > str(pima.scale)\n    'data.frame':532 obs. of  7 variables:\n     $ npreg: num  0.448 1.052 0.448 -1.062 -1.062 ...\n     $ glu  : num  -1.13 2.386 -1.42 1.418 -0.453 ...\n     $ bp   : num  -0.285 -0.122 0.852 0.365 -0.935 ...\n     $ skin : num  -0.112 0.363 1.123 1.313 -0.397 ...\n     $ bmi  : num  -0.391 -1.132 0.423 2.181 -0.943 ...\n     $ ped  : num  -0.403 -0.987 -1.007 -0.708 -1.074 ...\n     $ age  : num  -0.708 2.173 0.315 -0.522 -0.801 ...\n\n```", "```py\n    > pima.scale$type <- pima$type\n\n```", "```py\n    > pima.scale.melt <- melt(pima.scale, id.var = \"type\")\n    > ggplot(data = pima.scale.melt, aes(x = type, y = value)) +\n         geom_boxplot() + facet_wrap(~ variable, ncol = 2)\n\n```", "```py\n    > cor(pima.scale[-8])\n                npreg       glu          bp       skin\n    npreg 1.000000000 0.1253296 0.204663421 0.09508511\n    glu   0.125329647 1.0000000 0.219177950 0.22659042\n    bp    0.204663421 0.2191779 1.000000000 0.22607244\n    skin  0.095085114 0.2265904 0.226072440 1.00000000\n    bmi   0.008576282 0.2470793 0.307356904 0.64742239\n    ped   0.007435104 0.1658174 0.008047249 0.11863557\n    age   0.640746866 0.2789071 0.346938723 0.16133614\n                  bmi         ped        age\n    npreg 0.008576282 0.007435104 0.64074687\n    glu   0.247079294 0.165817411 0.27890711\n    bp    0.307356904 0.008047249 0.34693872\n    skin  0.647422386 0.118635569 0.16133614\n    bmi   1.000000000 0.151107136 0.07343826\n    ped   0.151107136 1.000000000 0.07165413\n    age   0.073438257 0.071654133 1.00000000\n\n```", "```py\n    > table(pima.scale$type)\n     No Yes\n    355 177\n\n```", "```py\n    > set.seed(502)\n    > ind <- sample(2, nrow(pima.scale), replace = TRUE, prob = c(0.7, \n      0.3))\n    > train <- pima.scale[ind == 1, ]\n    > test <- pima.scale[ind == 2, ]\n    > str(train)\n    'data.frame':385 obs. of  8 variables:\n     $ npreg: num  0.448 0.448 -0.156 -0.76 -0.156 ...\n     $ glu  : num  -1.42 -0.775 -1.227 2.322 0.676 ...\n     $ bp   : num  0.852 0.365 -1.097 -1.747 0.69 ...\n     $ skin : num  1.123 -0.207 0.173 -1.253 -1.348 ...\n     $ bmi  : num  0.4229 0.3938 0.2049 -1.0159 -0.0712 ...\n     $ ped  : num  -1.007 -0.363 -0.485 0.441 -0.879 ...\n     $ age  : num  0.315 1.894 -0.615 -0.708 2.916 ...\n     $ type : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 2 1 1 1 ...\n    > str(test)\n    'data.frame':147 obs. of  8 variables:\n     $ npreg: num  0.448 1.052 -1.062 -1.062 -0.458 ...\n     $ glu  : num  -1.13 2.386 1.418 -0.453 0.225 ...\n     $ bp   : num  -0.285 -0.122 0.365 -0.935 0.528 ...\n     $ skin : num  -0.112 0.363 1.313 -0.397 0.743 ...\n     $ bmi  : num  -0.391 -1.132 2.181 -0.943 1.513 ...\n     $ ped  : num  -0.403 -0.987 -0.708 -1.074 2.093 ...\n     $ age  : num  -0.7076 2.173 -0.5217 -0.8005 -0.0571 ...\n     $ type : Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 2 1 2 1 1 1 ...\n\n```", "```py\n    > grid1 <- expand.grid(.k = seq(2, 20, by = 1))\n\n```", "```py\n    > control <- trainControl(method = \"cv\")\n\n```", "```py\n    > set.seed(502)\n\n```", "```py\n    > knn.train <- train(type ~ ., data = train,\n      method = \"knn\",\n      trControl = control,\n      tuneGrid = grid1)\n\n```", "```py\n    > knn.train\n    k-Nearest Neighbors\n    385 samples\n      7 predictor\n      2 classes: 'No', 'Yes'\n    No pre-processing\n    Resampling: Cross-Validated (10 fold)\n    Summary of sample sizes: 347, 347, 345, 347, 347, 346, ...\n    Resampling results across tuning parameters:\n      k   Accuracy  Kappa  Accuracy SD  Kappa SD\n    2  0.736     0.359  0.0506       0.1273 \n    3  0.762     0.416  0.0526       0.1313 \n    4  0.761     0.418  0.0521       0.1276 \n    5  0.759     0.411  0.0566       0.1295 \n    6  0.772     0.442  0.0559       0.1474 \n    7  0.767     0.417  0.0455       0.1227 \n    8  0.767     0.425  0.0436       0.1122 \n    9  0.772     0.435  0.0496       0.1316 \n    10  0.780     0.458  0.0485       0.1170 \n    11  0.777     0.446  0.0437       0.1120 \n    12  0.775     0.440  0.0547       0.1443 \n    13  0.782     0.456  0.0397       0.1084 \n    14  0.780     0.449  0.0557       0.1349 \n    15  0.772     0.427  0.0449       0.1061 \n    16  0.782     0.453  0.0403       0.0954 \n    17  0.795     0.485  0.0382       0.0978 \n    18  0.782     0.451  0.0461       0.1205 \n    19  0.785     0.455  0.0452       0.1197 \n    20  0.782     0.446  0.0451       0.1124 \n    Accuracy was used to select the optimal model using the largest \n      value.\n    The final value used for the model was k = 17\\. \n\n```", "```py\n    > knn.test <- knn(train[, -8], test[, -8], train[, 8], k = 17)\n\n```", "```py\n    > table(knn.test, test$type)\n    knn.test No Yes\n         No  77  26\n         Yes 16  28\n\n```", "```py\n    > (77 + 28) / 147\n    [1] 0.7142857\n\n```", "```py\n    > #calculate Kappa\n    > prob.agree <- (77 + 28) / 147 #accuracy\n    > prob.chance <- ((77 + 26) / 147) * ((77 + 16) / 147)\n    > prob.chance\n    [1] 0.4432875\n    > kappa <- (prob.agree - prob.chance) / (1 - prob.chance)\n    > kappa\n    [1] 0.486783\n\n```", "```py\n    > set.seed(123)\n    > kknn.train <- train.kknn(type ~ ., data = train, kmax = 25, \n        distance = 2, \n        kernel = c(\"rectangular\", \"triangular\", \"epanechnikov\"))\n\n```", "```py\n    > plot(kknn.train)\n\n```", "```py\n    > kknn.train \n    Call:\n    train.kknn(formula = type ~ ., data = train, kmax = 25, distance = \n      2, kernel\n     = c(\"rectangular\", \"triangular\", \"epanechnikov\"))\n Type of response variable: nominal\n Minimal misclassification: 0.212987\n Best kernel: rectangular\n Best k: 19 \n\n```", "```py\n > kknn.pred <- predict(kknn.train, newdata = test)\n > table(kknn.pred, test$type)\n kknn.pred No Yes\n No 76  27\n Yes 17  27\n\n```", "```py\n    > linear.tune <- tune.svm(type ~ ., data = train,\n      kernel = \"linear\",\n      cost = c(0.001, 0.01, 0.1, 1, 5, 10))\n    > summary(linear.tune)\n    Parameter tuning of 'svm':\n    - sampling method: 10-fold cross validation\n    - best parameters:\n     cost\n        1\n    - best performance: 0.2051957\n    - Detailed performance results:\n       cost     error dispersion\n    1 1e-03 0.3197031 0.06367203\n    2 1e-02 0.2080297 0.07964313\n    3 1e-01 0.2077598 0.07084088\n    4 1e+00 0.2051957 0.06933229\n    5 5e+00 0.2078273 0.07221619\n    6 1e+01 0.2078273 0.07221619\n\n```", "```py\n    > best.linear <- linear.tune$best.model\n    > tune.test <- predict(best.linear, newdata = test)\n    > table(tune.test, test$type)\n    tune.test No Yes\n          No  82  22\n          Yes 13  30 \n    > (82 + 30)/147\n    [1] 0.7619048\n\n```", "```py\n    > set.seed(123) \n    > poly.tune <- tune.svm(type ~ ., data = train,\n      kernel = \"polynomial\",\n      degree = c(3, 4, 5),\n      coef0 = c(0.1, 0.5, 1, 2, 3, 4)) \n    > summary(poly.tune)\n    Parameter tuning of 'svm': \n    - sampling method: 10-fold cross validation\n    - best parameters:\n     degree coef0\n          3   0.1\n    - best performance: 0.2310391\n\n```", "```py\n    > best.poly <- poly.tune$best.model\n    > poly.test <- predict(best.poly, newdata = test)\n    > table(poly.test, test$type)\n    poly.test No Yes\n          No  81  28\n          Yes 12  26\n    > (81 + 26) / 147\n    [1] 0.7278912\n\n```", "```py\n    > set.seed(123)\n    > rbf.tune <- tune.svm(type ~ ., data = train, \n      kernel = \"radial\", \n      gamma = c(0.1, 0.5, 1, 2, 3, 4))\n    > summary(rbf.tune)\n    Parameter tuning of 'svm':\n    - sampling method: 10-fold cross validation\n    - best parameters:\n     gamma\n       0.5\n    - best performance: 0.2284076\n\n```", "```py\n    > best.rbf <- rbf.tune$best.model\n    > rbf.test <- predict(best.rbf, newdata = test)\n    > table(rbf.test, test$type)\n    rbf.test No Yes\n         No  73  33\n         Yes 20  21\n    > (73+21)/147\n    [1] 0.6394558\n\n```", "```py\n    > set.seed(123)\n    > sigmoid.tune <- tune.svm(type ~ ., data = train,\n      kernel = \"sigmoid\",\n      gamma = c(0.1, 0.5, 1, 2, 3, 4),\n      coef0 = c(0.1, 0.5, 1, 2, 3, 4)) \n    > summary(sigmoid.tune)\n    Parameter tuning of 'svm':\n    - sampling method: 10-fold cross validation\n    - best parameters:\n     gamma coef0\n       0.1     2\n    - best performance: 0.2080972\n\n```", "```py\n    > best.sigmoid <- sigmoid.tune$best.model\n    > sigmoid.test <- predict(best.sigmoid, newdata = test)\n    > table(sigmoid.test, test$type)\n    sigmoid.test No Yes\n             No  82  19\n             Yes 11  35\n    > (82+35)/147\n    [1] 0.7959184\n\n```", "```py\n    > confusionMatrix(sigmoid.test, test$type, positive = \"Yes\")\n    Confusion Matrix and Statistics\n              Reference\n    Prediction No Yes\n           No  82  19\n           Yes 11  35\n    Accuracy : 0.7959 \n                     95% CI : (0.7217, 0.8579)\n    No Information Rate : 0.6327 \n    P-Value [Acc > NIR] : 1.393e-05 \n    Kappa : 0.5469 \n    Mcnemar's Test P-Value : 0.2012 \n    Sensitivity : 0.6481 \n    Specificity : 0.8817 \n    Pos Pred Value : 0.7609 \n    Neg Pred Value : 0.8119 \n    Prevalence : 0.3673 \n    Detection Rate : 0.2381 \n    Detection Prevalence : 0.3129 \n    Balanced Accuracy : 0.7649 \n    'Positive' Class : Yes \n\n```", "```py\n    > confusionMatrix(tune.test, test$type, positive = \"Yes\")\n             Reference\n    Prediction No Yes\n           No  82  24\n           Yes 11  30\n    Accuracy : 0.7619 \n                     95% CI : (0.6847, 0.8282)\n    No Information Rate : 0.6327 \n    P-Value [Acc > NIR] : 0.0005615 \n    Kappa : 0.4605 \n    Mcnemar's Test P-Value : 0.0425225 \n    Sensitivity : 0.5556 \n    Specificity : 0.8817 \n    Pos Pred Value : 0.7317 \n    Neg Pred Value : 0.7736 \n    Prevalence : 0.3673 \n    Detection Rate : 0.2041 \n    Detection Prevalence : 0.2789 \n    Balanced Accuracy : 0.7186 \n    'Positive' Class : Yes \n\n```", "```py\n    > set.seed(123)\n    > rfeCNTL <- rfeControl(functions = lrFuncs, method = \"cv\", number \n      = 10)\n    > svm.features <- rfe(train[, 1:7], train[, 8],\n      sizes = c(7, 6, 5, 4), \n      rfeControl = rfeCNTL, \n      method = \"svmLinear\")\n\n```", "```py\n    > svm.features\n    Recursive feature selection\n    Outer resampling method: Cross-Validated (10 fold) \n    Resampling performance over subset size:\n     Variables Accuracy  Kappa AccuracySD KappaSD Selected\n    4   0.7797 0.4700    0.04969  0.1203 \n             5   0.7875 0.4865    0.04267  0.1096        *\n    6   0.7847 0.4820    0.04760  0.1141 \n    7   0.7822 0.4768    0.05065  0.1232 \n    The top 5 variables (out of 5):\n\n```", "```py\n    > svm.5 <- svm(type ~ glu + ped + npreg + bmi + age,\n      data = train,\n      kernel = \"linear\")\n    > svm.5.predict <- predict(svm.5, newdata = test[c(1, 2, 5, 6, 7)])\n    > table(svm.5.predict, test$type)\n    svm.5.predict No Yes\n              No  79  21\n              Yes 14  33\n\n```"]