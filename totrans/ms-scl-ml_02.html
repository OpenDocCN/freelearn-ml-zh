<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 2. Data Pipelines and Modeling"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Data Pipelines and Modeling</h1></div></div></div><p>We have looked at basic hands-on tools for exploring the data in the previous chapter, thus we now can delve into more complex topics of statistical model building and optimal control or science-driven tools and problems. I will go ahead and say that we will only touch on some topics in optimal control since this book really is just about ML in Scala and not the theory of data-driven business management, which might be an exciting topic for a book on its own.</p><p>In this chapter, I will stay away from specific implementations in Scala and discuss the problem of building a data-driven enterprise at a high level. Later chapters will address how to solve these smaller pieces of the puzzle. A special emphasis will be given to handing uncertainty. Uncertainty usually comes in several favors: first, there can be noise in the information we are provided with. Secondly, the information can be incomplete. The system may have some degree of freedom in filling the missing pieces, which results in uncertainty. Finally, there may be variations in the interpretation of the models and the resulting metrics. The final point is subtle, as most classic textbooks assume that we can measure things directly. Not only the measurements may be noisy, but the definition of the measure may change in time—try measuring satisfaction or happiness. Certainly, we can avoid the ambiguity by saying that we can optimize only measurable metrics, as people usually do, but it will significantly limit the application domain in practice. Nothing prevents the scientific machinery from handling the uncertainty in the interpretation into account as well.</p><p>The predictive models are often built just for data understanding. From the linguistic derivation, model is a simplified representation of the actual complex buildings or processes for exactly the purpose of making a point and convincing people, one or another way. The ultimate goal for predictive modeling, the modeling I am concerned about in this book and this chapter specifically, is to optimize the business processes by taking the most important factors into account in order to make the world a better place. This was certainly a sentence with a lot of uncertainty entrenched, but at least it looks like a much better goal than optimizing a click-through rate.</p><p>Let's look at a traditional business decision-making process: a traditional business might involve a set of C-level executives making decisions based on information that is usually obtained from a set of dashboards with graphical representation of the data in one or several DBs. The promise of an automated data-driven business is to be able to automatically make most of the decisions provided the uncertainties eliminating human bias. This is not to say that we no longer need C-level executives, but the C-level executives will be busy helping the machines to make the decisions instead of the other way around.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Going through the basics of influence diagrams as a tool for decision making</li><li class="listitem" style="list-style-type: disc">Looking at variations of the pure decision making optimization in the context of adaptive <span class="strong"><strong>Markov Decision</strong></span> making process and <span class="strong"><strong>Kelly Criterion</strong></span></li><li class="listitem" style="list-style-type: disc">Getting familiar with at least three different practical strategies for exploration-exploitation trade-off</li><li class="listitem" style="list-style-type: disc">Describing the architecture of a data-driven enterprise</li><li class="listitem" style="list-style-type: disc">Discussing major architectural components of a decision-making pipeline</li><li class="listitem" style="list-style-type: disc">Getting familiar with standard tools for building data pipelines</li></ul></div><div class="section" title="Influence diagrams"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Influence diagrams</h1></div></div></div><p>While the <a id="id45" class="indexterm"/>decision making process can have multiple facets, a book about decision making under uncertainty would be incomplete without mentioning influence diagrams (<span class="emphasis"><em>Influence Diagrams for Team Decision Analysis</em></span>, Decision Analysis 2 (4): 207–228), which help the analysis and understanding of the decision-making process. The decision may be as mundane as selection of the next news article to show to a user in a personalized environment or a complex one as detecting malware on an enterprise network or selecting the next research project.</p><p>Depending on the<a id="id46" class="indexterm"/> weather she can try and go on a boat trip.  We can represent the decision-making process as a diagram. Let's decide whether to take a river boat tour during her stay in Portland, Oregon:</p><div class="mediaobject"><img src="Images/B04935_02_01.jpg" alt="Influence diagrams" width="716" height="421"/><div class="caption"><p>Figure 02-1. A simple vacation influence diagram to represent a simple decision-making process. The diagram contains decision nodes such as Vacation Activity, observable and unobservable information nodes such as Weather Forecast and Weather, and finally the value node such as Satisfaction</p></div></div><p>The preceding<a id="id47" class="indexterm"/> diagram represents this situation. The decision whether to participate in the activity is clearly driven by the potential to get certain satisfaction, which is a function of the decision itself and the weather at the time of the activity. While the actual weather conditions are unknown at the time of the trip planning, we believe there is a certain correlation between the weather forecast and the actual weather experienced during the trip, which is represented by the edge between the <span class="strong"><strong>Weather</strong></span> and <span class="strong"><strong>Weather Forecast</strong></span> nodes. The <span class="strong"><strong>Vacation Activity</strong></span> node is the decision node, it has only one parent as the decision is made solely based on <span class="strong"><strong>Weather Forecast</strong></span>. The final node in the DAG is <span class="strong"><strong>Satisfaction</strong></span>, which is a function of the actual whether and the decision we made during the trip planning—obviously, <span class="emphasis"><em>yes + good weather</em></span> and <span class="emphasis"><em>no + bad weather</em></span> are likely to have the highest scores. The <span class="emphasis"><em>yes + bad weather</em></span> and <span class="emphasis"><em>no + good weather</em></span> would be a bad outcome—the latter case is probably just a missed opportunity, but not necessarily a bad decision, provided an inaccurate weather forecast.</p><p>The absence of an edge carries an independence assumption. For example, we believe that <span class="strong"><strong>Satisfaction</strong></span> should not depend on <span class="strong"><strong>Weather Forecast</strong></span>, as the latter becomes irrelevant once we are on the boat. Once the vacation plan is finalized, the actual weather during the boating activity can no longer affect the decision, which was made solely based on the weather forecast; at least in our simplified model, where we exclude the option of buying a trip insurance.</p><p>The graph shows different stages of decision making and the flow of information (we will provide an actual graph implementation in Scala in <a class="link" href="ch07.xhtml" title="Chapter 7. Working with Graph Algorithms">Chapter 7</a>, <span class="emphasis"><em>Working with Graph Algorithms</em></span>). There is only one piece of information required to make the decision in our simplified diagram: the weather forecast. Once the decision is made, we can no longer change it, even if we have information about the actual weather at the time of the trip. The weather and the decision data can be used to model her satisfaction with the decision she has made.</p><p>Let's map this<a id="id48" class="indexterm"/> approach to an advertising problem as an illustration: the ultimate goal is to get user satisfaction with the targeted ads, which results in additional revenue for an advertiser. The satisfaction is the function of user-specific environmental state, which is unknown at the time of decision making. Using machine learning algorithms, however, we can forecast this state based on the user's recent Web visit history and other information that we can gather, such as geolocation, browser-agent string, time of day, category of the ad, and so on (refer to <span class="emphasis"><em>Figure 02-2</em></span>).</p><p>While we are unlikely to measure the level of dopamine in the user's brain, which will certainly fall under the realm of measurable metrics and probably reduce the uncertainty, we can measure the user satisfaction indirectly by the user's actions, either the fact that they responded to the ad or even the measure of time the user spent between the clicks to browse relevant information, which can be used to estimate the effectiveness of our modeling and algorithms. Here is an influence diagram, similar to the one for "vacation", adjusted for the advertising decision-making process:</p><div class="mediaobject"><img src="Images/B04935_02_02.jpg" alt="Influence diagrams" width="600" height="349"/><div class="caption"><p>Figure 02-2. The vacation influence diagram adjusted to the online advertising decision-making case. The decisions for online advertising can be made thousand times per second</p></div></div><p>The actual process <a id="id49" class="indexterm"/>might be more complex, representing a chain of decisions, each one depending on a few previous time slices. For example, the so-called <span class="strong"><strong>Markov Chain Decision Process</strong></span>. In this case, the diagram might be repeated<a id="id50" class="indexterm"/> over multiple time slices.</p><p>Yet another example might be Enterprise Network Internet malware analytics system. In this case, we try to detect network connections indicative of either <span class="strong"><strong>command and control</strong></span> (<span class="strong"><strong>C2</strong></span>), lateral <a id="id51" class="indexterm"/>movement, or data exfiltration based on the analysis of network packets flowing through the enterprise switches. The goal is to minimize the potential impact of an outbreak with minimum impact on the functioning systems.</p><p>One of the decisions we might take is to reimage a subset of nodes or to at least isolate them. The data we collect may contain uncertainty—many benign software packages may send traffic in suspicious ways, and the models need to differentiate between them based on the risk and potential impact. One of the decisions in this specific case may be to collect additional information.</p><p>I will leave it to the reader to map this and other potential business cases to the corresponding diagram as an exercise. Let's consider a more complex optimization problem now.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Sequential trials and dealing with risk"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Sequential trials and dealing with risk</h1></div></div></div><p>What if my <a id="id52" class="indexterm"/>preferences for making an extra few dollars outweigh <a id="id53" class="indexterm"/>the risk of losing the same amount? I will stop on why one's preferences might be asymmetric in a little while in this section, and there is scientific evidence that this asymmetry is ingrained in our minds for evolutionary reasons, but you are right, I have to optimize the expected value of the asymmetric function of the parameterized utility now, as follows:</p><div class="mediaobject"><img src="Images/B04935_02_01F.jpg" alt="Sequential trials and dealing with risk" width="168" height="62"/></div><p>Why would an asymmetric function surface in the analysis? One example is repeated bets or re-investments, also known as the Kelly Criterion problem. Although originally, the Kelly Criterion<a id="id54" class="indexterm"/> was developed for a specific case of binary outcome as in a gambling machine and the optimization of the fraction of money to bet in each round (<span class="emphasis"><em>A New Interpretation of Information Rate</em></span>, Bell System Technical Journal 35 (4): 917–926, 1956), a more generic formulation as an re-investment problem involves a probabilistic distribution of possible returns.</p><p>The return over multiple bets is a product of individual return rates on each of the bets—the return rate is the ratio between the bankroll after the bet to the original bankroll before each individual bet, as follows:</p><div class="mediaobject"><img src="Images/B04935_02_02F.jpg" alt="Sequential trials and dealing with risk" width="102" height="30"/></div><p>This does not help us much to optimize the total return as we don't know how to optimize the product of <span class="emphasis"><em>i.i.d</em></span>. random variables. However, we can convert the product to a sum using log<a id="id55" class="indexterm"/> transformation and apply the <span class="strong"><strong>central limit theorem</strong></span> (<span class="strong"><strong>CLT</strong></span>) to approximate the sum of <span class="emphasis"><em>i.i.d</em></span>. variables (provided that the distribution of <span class="emphasis"><em>r</em></span>
<span class="emphasis"><em><sub>i</sub></em></span> is subect to CLT conditions, for example, has a finite mean and variance), as follows:</p><div class="mediaobject"><img src="Images/B04935_02_03F.jpg" alt="Sequential trials and dealing with risk" width="348" height="180"/></div><p>Thus, the cumulative result of making <span class="emphasis"><em>N</em></span> bets would look like the result of making <span class="emphasis"><em>N</em></span> bets with expected return of <span class="inlinemediaobject"><img src="Images/B04935_02_04F.jpg" alt="Sequential trials and dealing with risk" width="140" height="40"/></span>, and not <span class="inlinemediaobject"><img src="Images/B04935_02_05F.jpg" alt="Sequential trials and dealing with risk" width="55" height="33"/></span>
</p><p>As I mentioned <a id="id56" class="indexterm"/>before, the problem is most often applied for the case <a id="id57" class="indexterm"/>of binary bidding, although it can be easily generalized, in which case there is an additional parameter: <span class="emphasis"><em>x</em></span>, the amount of money to bid in each round. Let's say I make a profit of <span class="emphasis"><em>W</em></span> with probability <span class="emphasis"><em>p</em></span> or completely lose my bet otherwise with the probability <span class="emphasis"><em>(1-p)</em></span>. Optimizing the expected return with respect to the following additional parameter:</p><div class="mediaobject"><img src="Images/B04935_02_06F.jpg" alt="Sequential trials and dealing with risk" width="455" height="40"/></div><div class="mediaobject"><img src="Images/B04935_02_07F.jpg" alt="Sequential trials and dealing with risk" width="355" height="58"/></div><div class="mediaobject"><img src="Images/B04935_02_08F.jpg" alt="Sequential trials and dealing with risk" width="175" height="57"/></div><p>The last equation is the Kelly Criterion ratio and gives you the optimal amount to bet.</p><p>The reason that one might bet less than the total amount is that even if the average return is positive, there is still a possibility to lose the whole bankroll, particularly, in highly skewed situations. For example, even if the probability of making <span class="emphasis"><em>10 x</em></span> on your bet is <span class="emphasis"><em>0.105</em></span> (<span class="emphasis"><em>W = 10</em></span>, the expected return is <span class="emphasis"><em>5%)</em></span>, the combinatorial analysis show that even after <span class="emphasis"><em>60</em></span> bets, there is roughly a <span class="emphasis"><em>50%</em></span> chance that the overall return will be negative, and there is an <span class="emphasis"><em>11%</em></span> chance, in particular, of losing <span class="emphasis"><em>(57 - 10 x 3) = 27</em></span> times your bet or more:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>akozlov@Alexanders-MacBook-Pro$ scala</strong></span>
<span class="strong"><strong>Welcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).</strong></span>
<span class="strong"><strong>Type in expressions to have them evaluated.</strong></span>
<span class="strong"><strong>Type :help for more information.27 </strong></span>

<span class="strong"><strong>scala&gt; def logFactorial(n: Int) = { (1 to n).map(Math.log(_)).sum }</strong></span>
<span class="strong"><strong>logFactorial: (n: Int)Double</strong></span>

<span class="strong"><strong>scala&gt; def cmnp(m: Int, n: Int, p: Double) = {</strong></span>
<span class="strong"><strong>     |   Math.exp(logFactorial(n) -</strong></span>
<span class="strong"><strong>     |   logFactorial(m) +</strong></span>
<span class="strong"><strong>     |   m*Math.log(p) -</strong></span>
<span class="strong"><strong>     |   logFactorial(n-m) +</strong></span>
<span class="strong"><strong>     |   (n-m)*Math.log(1-p))</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>cmnp: (m: Int, n: Int, p: Double)Double</strong></span>

<span class="strong"><strong>scala&gt; val p = 0.105</strong></span>
<span class="strong"><strong>p: Double = 0.105</strong></span>

<span class="strong"><strong>scala&gt; val n = 60</strong></span>
<span class="strong"><strong>n: Int = 60</strong></span>

<span class="strong"><strong>scala&gt; var cumulative = 0.0</strong></span>
<span class="strong"><strong>cumulative: Double = 0.0</strong></span>

<span class="strong"><strong>scala&gt; for(i &lt;- 0 to 14) {</strong></span>
<span class="strong"><strong>     |   val prob = cmnp(i,n,p)</strong></span>
<span class="strong"><strong>     |   cumulative += prob</strong></span>
<span class="strong"><strong>     |   println(f"We expect $i wins with $prob%.6f probability $cumulative%.3f cumulative (n = $n, p = $p).")</strong></span>
<span class="strong"><strong>     | }</strong></span>
<span class="strong"><strong>We expect 0 wins with 0.001286 probability 0.001 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 1 wins with 0.009055 probability 0.010 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 2 wins with 0.031339 probability 0.042 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 3 wins with 0.071082 probability 0.113 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 4 wins with 0.118834 probability 0.232 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 5 wins with 0.156144 probability 0.388 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 6 wins with 0.167921 probability 0.556 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 7 wins with 0.151973 probability 0.708 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 8 wins with 0.118119 probability 0.826 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 9 wins with 0.080065 probability 0.906 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 10 wins with 0.047905 probability 0.954 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 11 wins with 0.025546 probability 0.979 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 12 wins with 0.012238 probability 0.992 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 13 wins with 0.005301 probability 0.997 cumulative (n = 60, p = 0.105).</strong></span>
<span class="strong"><strong>We expect 14 wins with 0.002088 probability 0.999 cumulative (n = 60, p = 0.105).</strong></span>
</pre></div><p>Note that to recover the <span class="emphasis"><em>27 x</em></span> amount, one would need to play only <span class="inlinemediaobject"><img src="Images/B04935_02_09F.jpg" alt="Sequential trials and dealing with risk" width="198" height="33"/></span> additional rounds on average with these favourable odds, but one must have<a id="id58" class="indexterm"/> something to bet to start with. The Kelly Criterion <a id="id59" class="indexterm"/>provides that the optimal is to bet only <span class="emphasis"><em>1.55%</em></span> of our bankroll. Note that if I bet the whole bankroll, I would lose all my money with 89.5% certainty in the first round (the probability of a win is only <span class="emphasis"><em>0.105</em></span>). If I bet only a fraction of the bankroll, the chances of staying in the game are infinitely better, but the overall returns are smaller. The plot of expected log of return is shown in <span class="emphasis"><em>Figure 02-3</em></span> as a function of the portions of the bankroll to bet, <span class="emphasis"><em>x</em></span>, and possible distribution of outcomes in 60 bets that I just computed. In 24% of the games we'll do worse than the lower curve, in 39% worse than the next curve, in about half—44%—a gambler we'll do the same or better than the black curve in the middle, and in 30% of cases better than the top one. The optimal Kelly Criterion value for <span class="emphasis"><em>x</em></span> is <span class="emphasis"><em>0.0155</em></span>, which will eventually optimize the overall return over infinitely many rounds:</p><div class="mediaobject"><img src="Images/B04935_02_03.jpg" alt="Sequential trials and dealing with risk" width="828" height="520"/><div class="caption"><p>Figure 02-3. The expected log of return as a function of the bet amount and possible outcomes in 60 rounds (see equation (2.2))</p></div></div><p>The Kelly Criterion has been criticized for being both too aggressive (gamblers tend to overestimate their winning potential/ratio and underestimate the probability of a ruin), as well as for being too conservative (the value at risk should be the total available capital, not just the bankroll), but it demonstrates one of the examples where we need to compensate our<a id="id60" class="indexterm"/> intuitive understanding of the "benefit" with <a id="id61" class="indexterm"/>some additional transformations.</p><p>From the financial point of view, the Kelly Criterion is a much better description of risk than the standard definition as volatility or variance of the returns. For a generic parametrized payoff distribution, <span class="emphasis"><em>y(z)</em></span>, with a probability distribution function, <span class="emphasis"><em>f(z)</em></span>, the equation (2.3) can be reformulated as follows. after the substitution <span class="emphasis"><em>r(x) = 1 + x y(z)</em></span>, where <span class="emphasis"><em>x</em></span> is still the amount to bet:</p><div class="mediaobject"><img src="Images/B04935_02_10F.jpg" alt="Sequential trials and dealing with risk" width="323" height="70"/></div><div class="mediaobject"><img src="Images/B04935_02_11F.jpg" alt="Sequential trials and dealing with risk" width="223" height="62"/></div><p>It can also be written in the following manner in the discrete case:</p><div class="mediaobject"><img src="Images/B04935_02_12F.jpg" alt="Sequential trials and dealing with risk" width="223" height="62"/></div><p>Here, the denominator emphasizes the contributions from the regions with negative payoffs. Specifically, the possibility of losing all your bankroll is exactly where the denominator <span class="inlinemediaobject"><img src="Images/B04935_02_13F.jpg" alt="Sequential trials and dealing with risk" width="92" height="33"/></span> is zero.</p><p>As I mentioned before, interestingly, risk aversion is engrained in our intuitions and there seems to be a natural risk-aversion system of preferences encoded in both humans and primates (<span class="emphasis"><em>A Monkey Economy as Irrational as Ours</em></span> by Laurie Santos, TED talk, 2010). Now enough about monkeys and risk, let's get into another rather controversial subject—the <a id="id62" class="indexterm"/>exploration-exploitation trade-off, where one might not <a id="id63" class="indexterm"/>even know the payoff trade-offs initially.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Exploration and exploitation"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Exploration and exploitation</h1></div></div></div><p>The exploration-exploitation trade-off is another problem that has its apparent origin within<a id="id64" class="indexterm"/> gambling, even though the real applications range from allocation of funding to research projects to self-driving cars. The traditional formulation is a multi-armed bandit problem, which refers to an imaginary slot machine with one or more arms. Sequential plays of each arm generate <span class="emphasis"><em>i.i.d</em></span>
<code class="literal">.</code> returns with unknown probabilities for each arm; the successive plays are independent in the simplified models. The rewards are assumed to be independent across the arms. The goal is to maximize the reward—for example, the amount of money won, and to minimize the learning loss, or the amount spend on the arms with less than optimal winning rate, provided an agreed upon arm selection policy. The obvious trade-off is between the <span class="strong"><strong>exploration</strong></span> in search of an arm that produces the best return and <span class="strong"><strong>exploitation</strong></span> of the best-known arm with optimal return:</p><div class="mediaobject"><img src="Images/B04935_02_14F.jpg" alt="Exploration and exploitation" width="127" height="37"/></div><p>The <span class="strong"><strong>pseudo-regret</strong></span><a id="id65" class="indexterm"/> is then the difference:</p><div class="mediaobject"><img src="Images/B04935_02_15F.jpg" alt="Exploration and exploitation" width="173" height="57"/></div><p>Here, <span class="inlinemediaobject"><img src="Images/B04935_02_16F.jpg" alt="Exploration and exploitation" width="18" height="30"/></span> is the <span class="emphasis"><em>i<sup>th</sup></em></span> arm selection out of <span class="emphasis"><em>N</em></span> trials. The multi-armed bandit problem was extensively studied in the 1930s and again during the early 2000s, with the application in finance and ADTECH. While in general, due to stochastic nature of the problem, it is not possible to provide a bound on the expected regret better than the square root of <span class="emphasis"><em>N</em></span>, the pseudo-regret can be controlled so that we are able to bound it by a log of <span class="emphasis"><em>N</em></span> (<span class="emphasis"><em>Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems</em></span> by Sebastien Bubeck and Nicolo Cesa-Bianchi, <a class="ulink" href="http://arxiv.org/pdf/1204.5721.pdf">http://arxiv.org/pdf/1204.5721.pdf</a>).</p><p>One of the most common strategies used in practice is epsilon strategies, where the optimal arm is chosen with the probability of <span class="inlinemediaobject"><img src="Images/B04935_02_17F.jpg" alt="Exploration and exploitation" width="57" height="33"/></span> and one of the other arms with the remaining probability. The drawback of this approach is that we might spend a lot of exploration resources on the arms that are never going to provide any rewards. The UCB strategy improves the epsilon strategy by choosing an arm with the largest estimate of the return, plus some multiple or fraction of the standard deviation of the return estimates. The approach<a id="id66" class="indexterm"/> needs the recomputation of the best arm to pull at each round and suffers from approximations made to estimate the mean and standard deviation. Besides, UCB requires the recomputation of the estimates for each successive pull, which might be a scalability problem.</p><p>Finally, the Thompson sampling strategy uses a fixed random sample from Beta-Bernoulli posterior estimates and assigns the next arm to the one that gives the minimal expected regret, for which real data can be used to avoid parameter recomputation. Although the specific numbers may depend on the assumptions, one available comparison for these model performances is provided in the following diagram:</p><div class="mediaobject"><img src="Images/B04935_02_04.jpg" alt="Exploration and exploitation" width="900" height="660"/><div class="caption"><p>Figure 02-3. The simulation results for different exploration exploitation strategies for K = 5, one-armed bandits, and different strategies.</p></div></div><p>
<span class="emphasis"><em>Figure 02-3</em></span> shows simulation results for different strategies (taken from the Rich Relevance website at <a class="ulink" href="http://engineering.richrelevance.com/recommendations-thompson-sampling">http://engineering.richrelevance.com/recommendations-thompson-sampling</a>). The <span class="strong"><strong>Random</strong></span> strategy just allocates the arms at random and corresponds to pure exploration. The <span class="strong"><strong>Naive</strong></span> strategy is random up to a certain threshold and than switches to pure Exxploitation mode. <span class="strong"><strong>Upper Confidence Bound</strong></span> (<span class="strong"><strong>UCB</strong></span>) with 95% confidence level. UCB1 is a modification of UCB to take into account the log-normality of the distributions. Finally the Thompson sampling strategy makes a random sample from actual posterior distribution to optimize the regret.</p><p>Exploration/exploitation models are known to be very sensitive to the initial conditions and outliers, particularly on the low-response side. One can spend enormous amount of trials on the arms that are essentially dead.</p><p>Other<a id="id67" class="indexterm"/> improvements on the strategies are possible by estimating better priors based on additional information, such as location, or limiting the set of arms to explore—<span class="emphasis"><em>K</em></span>—due to such additional information, but these aspects are more domain-specific (such as personalization or online advertising).</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Unknown unknowns"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Unknown unknowns</h1></div></div></div><p>Unknown unknowns<a id="id68" class="indexterm"/> have been largely made famous due to a phrase from a response the United States Secretary of Defense, Donald Rumsfeld, gave to a question at a United States <span class="strong"><strong>Department of Defense</strong></span> (<span class="strong"><strong>DoD</strong></span>) news briefing on February 12, 2002 about the lack of evidence linking the government of Iraq with the supply of weapons of mass destruction to terrorist groups, and books by Nassim Taleb (<span class="emphasis"><em>The Black Swan: The Impact of the Highly Improbable</em></span> by Nassim Taleb, Random House, 2007).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>
<span class="strong"><strong>Turkey paradox</strong></span>
</p><p>Arguably, the <a id="id69" class="indexterm"/>unknown unknown is better explained by the turkey paradox. Suppose you have a family of turkeys playing in the backyard and enjoying protection and free food. Across the fence, there is another family of turkeys. This all works day after day, and month after month, until Thanksgiving comes—Thanksgiving Day is a national holiday celebrated in Canada and the United States, where it's customary to roast the turkeys in an oven. The turkeys are very likely to be harvested and consumed at this point, although from the turkey's point of view, there is no discernable signal that anything will happen on the second Monday of October in Canada and the fourth Thursday of November in the United States. No amount of modeling on the within-the-year data can fix this prediction problem from the turkey's point of view besides the additional year-over-year information.</p></div></div><p>The unknown unknown is something that is not in the model and cannot be anticipated to be in the model. In reality, the only unknown unknowns that are of interest are the ones that affect the model so significantly that the results that were previously virtually impossible, or possible with infinitesimal probability, now become the reality. Given that most of the practical distributions are from exponential family with really thin tails, the deviation from normal does not have to be more than a few sigmas to have devastating results on the standard model assumptions. While one has still to come up with an actionable strategy of <a id="id70" class="indexterm"/>how to include the unknown factors in the model—a few ways have been proposed, including fractals, but few if any are actionable—the practitioners have to be aware of the risks, and here the definition of the risk is exactly the possibility of delivering the models useless. Of course, the difference between the known unknown and unknown unknown is exactly that we understand the risks and what needs to be explored.</p><p>As we looked at the basic scope of problems that the decision-making systems are facing, let's look at the data pipelines, the software systems that provide information for making the decisions, and more practical aspects of designing the data pipeline for a data-driven system.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Basic components of a data-driven system"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Basic components of a data-driven system</h1></div></div></div><p>In short, a data-driven architecture contains the following components—at least all the systems I've seen have them—or can be reduced to these components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data ingest</strong></span>: We need to collect the data from systems and devices. Most of the <a id="id71" class="indexterm"/>systems have logs, or at least an option to <a id="id72" class="indexterm"/>write files into a local filesystem. Some can have capabilities to report information to network-based interfaces such as syslog, but the absence of persistence layer usually means potential data loss, if not absence of audit information.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data transformation layer</strong></span>: It was also historically called <span class="strong"><strong>extract, transform, and load</strong></span> (<span class="strong"><strong>ETL</strong></span>). Today the data transformation layer can also be used to<a id="id73" class="indexterm"/> have real-time processing, where<a id="id74" class="indexterm"/> the aggregates are computed <a id="id75" class="indexterm"/>on the most recent data. The data transformation layer is also traditionally used to reformat and index the data to be efficiently accessed by a UI component of algorithms down the pipeline.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data analytics and machine learning engine</strong></span>: The reason this is not part of the standard<a id="id76" class="indexterm"/> data transformation layer is<a id="id77" class="indexterm"/> usually that this layer requires<a id="id78" class="indexterm"/> quite different skills. The mindset of people who<a id="id79" class="indexterm"/> build reasonable statistical models is usually different from people who make terabytes of data move fast, even though occasionally I can find people with both skills. Usually, these unicorns are called data scientists, but the skills in any specific field are usually inferior to ones who specialize in a particular field. We need more of <a id="id80" class="indexterm"/>either, though. Another reason is that<a id="id81" class="indexterm"/> machine learning, and to a certain extent, data <a id="id82" class="indexterm"/>analysis, requires multiple<a id="id83" class="indexterm"/> aggregations and passes over the same data, which as opposed to a more stream-like ETL transformations, requires a different engine.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>UI component</strong></span>: Yes, UI stands for user interface, which most often is a set of components<a id="id84" class="indexterm"/> that allow you to communicate <a id="id85" class="indexterm"/>with the system via a browser (it used to be a native GUI, but these days the web-based JavaScript or Scala-based frameworks are much more powerful and portable). From the data pipeline and modeling perspective, this component offers an API to access internal representation of data and models.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Actions engine</strong></span>: This is usually a configurable rules engine to optimize the provided<a id="id86" class="indexterm"/> metrics based on insights. The <a id="id87" class="indexterm"/>actions may be either real-time, like in online advertising, in which case the engine should be able to supply real-time scoring information, or a recommendation for a user action, which can take the form of an e-mail alert.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Correlation engine</strong></span>: This is an emerging component that may analyze the output of data <a id="id88" class="indexterm"/>analysis and machine learning <a id="id89" class="indexterm"/>engine to infer additional insights into data or model behavior. The actions might also be triggered by an output from this layer.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Monitoring</strong></span>: This is a <a id="id90" class="indexterm"/>complex system will be<a id="id91" class="indexterm"/> incomplete without logging, monitoring, and some way to change system parameters. The purpose of monitoring is to have a nested decision-making system regarding the optimal health of the system and either to mitigate the problem(<span class="emphasis"><em>s</em></span>) automatically or to alert the system administrators about the problem(<span class="emphasis"><em>s</em></span>).</li></ul></div><p>Let's discuss each of the components in detail in the following sections.</p><div class="section" title="Data ingest"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec08"/>Data ingest</h2></div></div></div><p>With the <a id="id92" class="indexterm"/>proliferation of smart devices, information gathering has become less of a problem and more of a necessity for any business that does more than a type-written text. For the purpose of this chapter, I will assume that the device or devices are connected to the Internet or have some way of passing this information via home dialing or direct network connection.</p><p>The major purpose of this component is to collect all relevant information that can be relevant for further data-driven decision making. The following table provides details on the most common implementations of the data ingest:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Framework</p>
</th><th style="text-align: left" valign="bottom">
<p>When used</p>
</th><th style="text-align: left" valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Syslog</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Syslog is <a id="id93" class="indexterm"/>one of the most common standards to pass <a id="id94" class="indexterm"/>messages between the machines on Unix. Syslog usually listens on port 514 and the transport protocol can be configured either with UDP (unreliable) or with TCP. The latest enhanced implementation on CentOS and Red Hat Linux is rsyslog, which includes many advanced options such as regex-based filtering that is useful for system-performance tuning and debugging. Apart from slightly inefficient raw message representation—plain text, which might be inefficient for long messages with repeated strings—the syslog system can support tens of thousands of messages per second.</p>
</td><td style="text-align: left" valign="top">
<p>Syslog is one of the oldest protocols developed in the 1980s by Eric Allman as part of Sendmail. While it does not guarantee delivery or durability, particularly for distributed systems, it is one of the most widespread protocols for message passing. Some of the later frameworks, such as Flume and Kafka, have syslog interfaces as well. </p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Rsync</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Rsync is a younger framework developed in the 1990s. If the data is <a id="id95" class="indexterm"/>put in the flat files on a local filesystem, rsync might be an option. While rsync is more traditionally used to synchronize two directories, it also <a id="id96" class="indexterm"/>can be run periodically to transfer log data in batches. Rsync uses a recursive algorithm invented by an Australian computer programmer, Andrew Tridgell, for efficiently detecting the differences and transmitting a structure (such as a file) across a communication link when the receiving computer already has a similar, but not identical, version of the same structure. While it incurs extra communication, it is better from the point of durability, as the original copy can always be retrieved. It is particularly appropriate if the log data is known to arrive in batches in the first place (such as uploads or downloads).</p>
</td><td style="text-align: left" valign="top">
<p>Rsync <a id="id97" class="indexterm"/>has been known to be hampered by network bottlenecks, as it ultimately passes more information over the network when comparing the directory structures. However, the transferred files may be compressed when passed over the network. The network bandwidth can be limited per command-line flags.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Flume</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Flume is one of the youngest frameworks developed by Cloudera in 2009-2011 and open sourced. Flume—we refer to the more popular flume-ng implementation as Flume as opposed to an older regular Flume—consists of sources, pipes, and sinks that may be <a id="id98" class="indexterm"/>configured on multiple nodes for high availability and redundancy purposes. Flume was designed to err on the reliability side at the expense of possible duplication of data. Flume passes the messages in the <span class="strong"><strong>Avro</strong></span> format, which is also open sourced and the transfer protocol, as well as messages can be encoded and compressed.</p>
</td><td style="text-align: left" valign="top">
<p>While Flume originally was developed just to ship records from a file or a set of files, it can<a id="id99" class="indexterm"/> also be configured to listen to a port, or even grab the records from a database. Flume has multiple adapters including the preceding syslog.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Kafka</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Kafka is the latest addition to the <a id="id100" class="indexterm"/>log-processing framework developed by<a id="id101" class="indexterm"/> LinkedIn and is open sourced. Kafka, compared to the previous frameworks, is more like a distributed reliable message queue. Kafka keeps a partitioned, potentially between multiple distributed machines; buffer and one can subscribe to or unsubscribe from<a id="id102" class="indexterm"/> getting messages for a particular<a id="id103" class="indexterm"/> topic. Kafka was built with strong reliability guarantees in mind, which is achieved through replication and consensus protocol.</p>
</td><td style="text-align: left" valign="top">
<p>Kafka might not be appropriate for small systems (&lt; five nodes) as the benefits of the fully distributed system might be evident only at larger scales. Kafka is commercially supported by Confluent.</p>
</td></tr></tbody></table></div><p>The transfer of information usually occurs in batches, or micro batches if the requirements are close to real time. Usually the information first ends up in a file, traditionally called log, in a device's local filesystem, and then is transferred to a central location. Recently<a id="id104" class="indexterm"/> developed Kafka and Flume are often used to manage these transfers, together with a more traditional syslog, rsync, or netcat. Finally, the data can be placed into a local or distributed storage such as HDFS, Cassandra, or Amazon S3.</p></div><div class="section" title="Data transformation layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec09"/>Data transformation layer</h2></div></div></div><p>After the data <a id="id105" class="indexterm"/>ends up in HDFS or other storage, the data needs to be made available for processing. Traditionally, the data is processed on a schedule and ends up partitioned by time-based buckets. The processing can happen daily or hourly, or even on a sub-minute basis with the new Scala streaming framework, depending on the latency requirements. The processing may involve some preliminary feature construction or vectorization, even though it is traditionally considered a machine-learning task. The following table summarizes some available frameworks:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Framework</p>
</th><th style="text-align: left" valign="bottom">
<p>When used</p>
</th><th style="text-align: left" valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Oozie</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This is<a id="id106" class="indexterm"/> one of the oldest open source frameworks developed by Yahoo. This has good integration with big data Hadoop tools. It has limited UI that lists the job history.</p>
</td><td style="text-align: left" valign="top">
<p>The whole workflow is put into <a id="id107" class="indexterm"/>one big XML file, which might be considered a disadvantage from the modularity point of view.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Azkaban</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>This is <a id="id108" class="indexterm"/>an alternative open source workflow-scheduling framework developed by LinkedIn. Compared to Oozie, this arguably has a better UI. The <a id="id109" class="indexterm"/>disadvantage is that all high-level tasks are executed locally, which might present a scalability problem.</p>
</td><td style="text-align: left" valign="top">
<p>The idea behind Azkaban is to create<a id="id110" class="indexterm"/> a fully modularized drop-in architecture where the new jobs/tasks can be added with as few <a id="id111" class="indexterm"/>modifications as possible.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>StreamSets</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>StreamSets is the latest addition build by the former Informix<a id="id112" class="indexterm"/> and Cloudera developers. It has a very <a id="id113" class="indexterm"/>developed UI and supports a much richer set of input sources and output destinations.</p>
</td><td style="text-align: left" valign="top">
<p>This is a fully UI-driven tool with an emphasis on data curation, for example, constantly monitoring the data stream for problems and abnormalities.</p>
</td></tr></tbody></table></div><p>Separate attention should be given to stream-processing frameworks, where the latency requirements are reduced to one or a few records at a time. First, stream processing usually requires much more resources dedicated to processing, as it is more expensive to process individual records at a time as opposed to batches of records, even if it is tens or hundreds of records. So, the architect needs to justify the additional costs based on the value of more recent result, which is not always warranted. Second, stream processing requires a few adjustments to the architecture as handling the more recent data becomes a priority; for example, a delta architecture where the more recent data is handled by a separate substream <a id="id114" class="indexterm"/>or a <a id="id115" class="indexterm"/>set of nodes became very popular recently with systems such as <span class="strong"><strong>Druid</strong></span> (<a class="ulink" href="http://druid.io">http://druid.io</a>).</p></div><div class="section" title="Data analytics and machine learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec10"/>Data analytics and machine learning</h2></div></div></div><p>For the purpose of this chapter, <span class="strong"><strong>Machine Learning</strong></span> (<span class="strong"><strong>ML</strong></span>) is any algorithm that can compute aggregates or summaries <a id="id116" class="indexterm"/>that are actionable. We will cover more complex algorithms from <a class="link" href="ch03.xhtml" title="Chapter 3. Working with Spark and MLlib">Chapter 3</a>, <span class="emphasis"><em>Working with Spark and MLlib</em></span> to <a class="link" href="ch06.xhtml" title="Chapter 6. Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>, but in some cases, a simple sliding-window average and deviation from the average <a id="id117" class="indexterm"/>may be sufficient signal for taking an action. In the past few years, it just works in A/B testing somehow became a <a id="id118" class="indexterm"/>convincing argument for model building and deployment. I am not speculating that solid scientific principles might or might not apply, but many fundamental assumptions such as <span class="emphasis"><em>i.i.d.</em></span>, balanced designs, and the thinness of the tail just fail to hold for many big data situation. Simpler models tend to be faster and to have better performance and stability.</p><p>For example, in online advertising, one might just track average performance of a set of ads over a certain similar properties over times to make a decision whether to have this ad displayed. The information about anomalies, or deviation from the previous behavior, may be a signal a new unknown unknown, which signals that the old data no longer applies, in <a id="id119" class="indexterm"/>which case, the system has no choice but to start the new <a id="id120" class="indexterm"/>exploration cycle.</p><p>I will talk about more complex non-structured, graph, and pattern mining later in <a class="link" href="ch06.xhtml" title="Chapter 6. Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span>, <a class="link" href="ch08.xhtml" title="Chapter 8. Integrating Scala with R and Python">Chapter 8</a>, <span class="emphasis"><em>Integrating Scala with R and Python</em></span> and <a class="link" href="ch09.xhtml" title="Chapter 9. NLP in Scala">Chapter 9</a>, <span class="emphasis"><em>NLP in Scala</em></span>.</p></div><div class="section" title="UI component"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/>UI component</h2></div></div></div><p>Well, UI is<a id="id121" class="indexterm"/> for wimps! Just joking...maybe it's too harsh, but in reality, UI usually presents a syntactic sugar that is necessary to convince the population beyond the data scientists. A good analyst should probably be able to figure out t-test probabilities by just looking at a table with numbers.</p><p>However, one should probably apply the same methodologies we used at the beginning of the chapter, assessing the usefulness of different components and the amount of cycles put into them. The presence of a good UI is often justified, but depends on the target audience.</p><p>First, there are a number of existing UIs and reporting frameworks. Unfortunately, most of them are not aligned with the functional programming methodologies. Also, the presence of complex/semi-structured data, which I will describe in <a class="link" href="ch06.xhtml" title="Chapter 6. Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Working with Unstructured Data</em></span> in more detail, presents a new twist that many frameworks are not ready to deal with without implementing some kind of DSL. Here are a few frameworks for building the UI in a Scala project that I find particularly worthwhile:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Framework</p>
</th><th style="text-align: left" valign="bottom">
<p>When used</p>
</th><th style="text-align: left" valign="bottom">
<p>Comments</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Scala Swing</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>If you <a id="id122" class="indexterm"/>used Swing<a id="id123" class="indexterm"/> components in Java and are proficient with them, Scala Swing is a good choice for you. Swing component is arguably the least portable component of Java, so your mileage can vary on different platforms.</p>
</td><td style="text-align: left" valign="top">
<p>The <code class="literal">Scala.swing</code> package uses the standard Java Swing library under the hood, but it has some nice additions. Most notably, as it's made for Scala, it can be used in a much more concise way than the standard Swing.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Lift</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Lift is a <a id="id124" class="indexterm"/>secure, developer-centric, scalable, and interactive<a id="id125" class="indexterm"/> framework written in Scala. Lift is open sourced under Apache 2.0 license.</p>
</td><td style="text-align: left" valign="top">
<p>The open source Lift framework was launched in 2007 by David Polak, who was dissatisfied with certain aspects of the Ruby on Rails framework. Any existing Java library and web container can be used in running Lift applications. Lift web applications are thus packaged as WAR files and deployed on any servlet 2.4 engine (for example, Tomcat 5.5.xx, Jetty 6.0, and so on). Lift programmers may use the standard Scala/Java development toolchain, including IDEs such as Eclipse, NetBeans, and IDEA. Dynamic web content is authored via templates using standard HTML5 or XHTML editors. Lift applications also benefit from native support for advanced web development techniques, such as Comet and Ajax.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Play</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Play is arguably better aligned with <a id="id126" class="indexterm"/>Scala as a functional language than any other<a id="id127" class="indexterm"/> platform—it is officially supported by Typesafe, the commercial company behind Scala. The Play framework 2.0 builds on Scala, Akka, and sbt to deliver superior asynchronous request handling, fast and reliable. Typesafe templates, and a powerful build system with flexible deployment options. Play is open sourced under Apache 2.0 license.</p>
</td><td style="text-align: left" valign="top">
<p>The open source Play framework was created in 2007 by Guillaume Bort, who sought to bring a fresh web development experience inspired by modern web frameworks like Ruby on Rails to the long-suffering Java web development community. Play follows a familiar stateless model-view-controller architectural pattern, with a philosophy of convention-over-configuration and an emphasis on developer productivity. Unlike traditional Java web frameworks with their tedious compile-package-deploy-restart cycles, updates to Play applications are instantly visible with a simple browser refresh.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Dropwizard</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>The dropwizard (<a class="ulink" href="http://www.dropwizard.io">www.dropwizard.io</a>) project is an attempt to build a generic RESTful framework in both <a id="id128" class="indexterm"/>Java and Scala, even though one might <a id="id129" class="indexterm"/>end up using more Java than Scala. What is nice about this framework is that it is flexible enough to be used with arbitrary complex data (including semi-structured).This is licensed under Apache License 2.0.</p>
</td><td style="text-align: left" valign="top">
<p>RESTful API assumes state, while <a id="id130" class="indexterm"/>functional languages shy away from using state. Unless you are flexible enough to deviate from a pure functional approach, this framework is probably not good enough for you.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Slick</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>While Slick is not a UI component, it is Typesafe's modern <a id="id131" class="indexterm"/>database query<a id="id132" class="indexterm"/> and access library for Scala, which can serve as a UI backend. It allows you to work with the stored data almost as if you were using Scala collections, while at the same time, giving you full control over when a database access occurs and what data is transferred. You can also use SQL directly. Use it if all of your data is purely relational. This is open sourced under BSD-Style license.</p>
</td><td style="text-align: left" valign="top">
<p>Slick was started in 2012 by Stefan Zeiger and maintained mainly by Typesafe. It is useful for mostly relational data.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>NodeJS</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>Node.js is a JavaScript runtime, built on Chrome's V8 JavaScript<a id="id133" class="indexterm"/> engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient. Node.js' package ecosystem, npm, is the largest ecosystem of open source libraries<a id="id134" class="indexterm"/> in the world. It is open sourced under MIT License.</p>
</td><td style="text-align: left" valign="top">
<p>Node.js <a id="id135" class="indexterm"/>was first introduced in 2009 by Ryan Dahl and other developers working at Joyent. Originally Node.js supported only Linux, but now it runs on OS X<a id="id136" class="indexterm"/> and Windows.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>AngularJS</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>AngularJS (<a class="ulink" href="https://angularjs.org">https://angularjs.org</a>) is a <a id="id137" class="indexterm"/>frontend development framework, built to simplify development of one-page web applications. This is open sourced under MIT License.</p>
</td><td style="text-align: left" valign="top">
<p>AngularJS <a id="id138" class="indexterm"/>was originally developed in 2009 by <a id="id139" class="indexterm"/>Misko Hevery at Brat Tech LLC. AngularJS is mainly maintained by Google and by a community of individual developers and corporations, and thus is specifically for Android platform (support for IE8 is dropped in versions 1.3 and later).</p>
</td></tr></tbody></table></div></div><div class="section" title="Actions engine"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Actions engine</h2></div></div></div><p>While this is <a id="id140" class="indexterm"/>the heart of the data-oriented system pipeline, it is also arguably the easiest one. Once the system of metrics and values is known, the system decides, based on the known equations, whether to take a certain set of actions or not, based on the information provided. While the triggers based on a threshold is the most common implementation, the significance of probabilistic approaches that present the user with a set of possibilities and associated probabilities is emerging—or just presenting the user with the top <span class="emphasis"><em>N</em></span> relevant choices like a search engine does.</p><p>The management of the rules might become pretty involved. It used to be that managing the rules with a rule engine, such as <span class="strong"><strong>Drools</strong></span> (<a class="ulink" href="http://www.drools.org">http://www.drools.org</a>), was sufficient. However, managing complex <a id="id141" class="indexterm"/>rules becomes an issue that often requires development of a DSL (<span class="emphasis"><em>Domain-Specific Languages</em></span> by Martin Fowler, Addison-Wesley, 2010). Scala is particularly fitting language for the development of such an actions engine.</p></div><div class="section" title="Correlation engine"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Correlation engine</h2></div></div></div><p>The more complex the decision-making system is, the more it requires a secondary decision-making system to optimize its management. DevOps is turning into DataOps (<span class="emphasis"><em>Getting Data Right</em></span> by Michael Stonebraker et al., Tamr, 2015). Data collected about the performance of a <a id="id142" class="indexterm"/>data-driven system are used to detect anomalies and semi-automated maintenance.</p><p>Models are often subject to time drift, where the performance might deteriorate either due to the changes in the data collection layers or the behavioral changes in the population (I will cover model drift in <a class="link" href="ch10.xhtml" title="Chapter 10. Advanced Model Monitoring">Chapter 10</a>, <span class="emphasis"><em>Advanced Model Monitoring</em></span>). Another aspect of model management is to track model performance, and in some cases, use "collective" intelligence of the models by various consensus schemes.</p></div><div class="section" title="Monitoring"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/>Monitoring</h2></div></div></div><p>Monitoring a <a id="id143" class="indexterm"/>system involves collecting information about system performance either for audit, diagnostic, or performance-tuning purposes. While it is related to the issues raised in the previous sections, monitoring solution often incorporates diagnostic and historical storage solutions and persistence of critical data, such as a black box on an airplane. In the Java and, thus, Scala world, a popular tool of choice is Java performance beans, which can be monitored in the Java Console. While Java natively supports MBean for exposing JVM information over JMX, <span class="strong"><strong>Kamon</strong></span> (<a class="ulink" href="http://kamon.io">http://kamon.io</a>) is <a id="id144" class="indexterm"/>an open source library that uses this mechanism to<a id="id145" class="indexterm"/> specifically expose Scala and Akka metrics.</p><p>Some other<a id="id146" class="indexterm"/> popular monitoring open source solutions are <span class="strong"><strong>Ganglia</strong></span> (<a class="ulink" href="http://ganglia.sourceforge.net/">http://ganglia.sourceforge.net/</a>) and <a id="id147" class="indexterm"/>
<span class="strong"><strong>Graphite</strong></span> (<a class="ulink" href="http://graphite.wikidot.com">http://graphite.wikidot.com</a>).</p><p>I will stop here, as I will address system and model monitoring in more detail in <a class="link" href="ch10.xhtml" title="Chapter 10. Advanced Model Monitoring">Chapter 10</a>, <span class="emphasis"><em>Advanced Model Monitoring</em></span>.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Optimization and interactivity"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Optimization and interactivity</h1></div></div></div><p>While the <a id="id148" class="indexterm"/>data collected can be just used for understanding the business, the <a id="id149" class="indexterm"/>final goal of any data-driven business is to optimize the business behavior by automatically making data-based and model-based decisions. We want to reduce human intervention to minimum. The following simplified diagram can be depicted as a cycle:</p><div class="mediaobject"><img src="Images/B04935_02_05.jpg" alt="Optimization and interactivity" width="600" height="633"/><div class="caption"><p>Figure 02-4. The predictive model life cycle</p></div></div><p>The cycle is<a id="id150" class="indexterm"/> repeated over and over for new information coming into the <a id="id151" class="indexterm"/>system. The parameters of the system may be tuned to improve the overall system performance.</p><div class="section" title="Feedback loops"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/>Feedback loops</h2></div></div></div><p>While humans are still likely to be kept in the loop for most of the systems, last few years saw an emergence of systems that can manage the complete feedback loop on their own—ranging <a id="id152" class="indexterm"/>from advertisement systems to self-driving cars.</p><p>The classical formulation of this problem is the optimal control theory, which is also an optimization problem to minimize cost functional, given a set of differential equations describing the system. An optimal control is a set of control policies to minimize the cost functional given constraints. For example, the problem might be to find a way to drive the car to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Another control problem is to maximize profit for showing ads on a website, provided the inventory and time constraints. Most software packages for <a id="id153" class="indexterm"/>optimal control are written in other languages such as C or MATLAB (PROPT, SNOPT, RIOTS, DIDO, DIRECT, and GPOPS), but can be interfaced with Scala.</p><p>However, in many cases, the parameters for the optimization or the state transition, or differential equations, are not known with certainty. <span class="strong"><strong>Markov Decision Processes</strong></span> (<span class="strong"><strong>MDPs</strong></span>) provide a mathematical framework to model decision making in situations where outcomes are partly random and partly under the control of the decision maker. In MDPs, we deal with a discrete set of possible states and a set of actions. The "rewards" and state transitions depend both on the state and actions. MDPs are useful for studying a wide range of optimization problems solved via dynamic programming and reinforcement learning.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Summary</h1></div></div></div><p>In this chapter, I described a high-level architecture and approach to design a data-driven enterprise. I also introduced you to influence diagrams, a tool for understanding how the decisions are made in traditional and data-driven enterprises. I stopped on a few key models, such as Kelly Criterion and multi-armed bandit, essential to demonstrate the issues from the mathematical point of view. I built on top of this to introduce some Markov decision process approaches where we deal with decision policies based on the results of the previous decisions and observations. I delved into more practical aspects of building a data pipeline for decision-making, describing major components and frameworks that can be used to built them. I also discussed the issues of communicating the data and modeling results between different stages and nodes, presenting the results to the user, feedback loop, and monitoring.</p><p>In the next chapter, I will describe MLlib, a library for machine learning over distributed set of nodes written in Scala.</p></div></div>



  </body></html>