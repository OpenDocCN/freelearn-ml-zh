- en: Chapter 3. A Simple Machine Learning Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter shows examples of exploratory data analysis and machine learning
    techniques. R provides us with different datasets that can be used to experiment
    with the tools. In this chapter, we will use an interesting dataset about the
    Titanic passengers.
  prefs: []
  type: TYPE_NORMAL
- en: There are some facts that happened during the Titanic event, such as the policy
    of saving the women and children first and the privileges of the first social
    classes. In order to investigate what happened, we can use the data related to
    the event. The R dataset is about some passengers and it displays their personal
    data and who survived. First, we can explore some data in order to understand
    what happened. Then, starting from the personal data of other passengers, the
    goal of the machine learning model is forecasting which new passengers will survive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the data using simple charts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the data using machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting an outcome using machine learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data interactively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section shows you how to visualize the data using simple techniques. We
    process the data using the `data.table` package and visualize the information
    using the basic R charts. A great plotting package is `ggplot2` and it allows
    you to create nice professional charts. Unfortunately, its syntax is more complex
    than the basic R charts, so we don't have enough space for it in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'R provides us with a `Titanic` dataset that contains the survival statistics
    of some passengers. Before starting to analyze the data, let''s take a look at
    their documentation using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The documentation shows that the passengers are divided in groups on the basis
    of their social class, gender, and age. For each group, the dataset shows how
    many people survived and how many didn''t. We can see the format of data using
    `class`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The object, `Titanic`, belongs to the `table` class so it displays the count
    of each combination of categoric variables, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The table shows the frequency, that is, the number of passengers for each combination
    of variables that are the personal data and the data of those who survived.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a table with the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will convert the data in a more convenient format. The
    first step is to define a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the structure of `dfTitanic` using `str`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are four factors representing the passenger''s attributes and `Freq`
    displaying the number of passengers for each combination of attributes. In order
    to use powerful tools to process data, we transform `dfTitanic` into a data table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the top rows of the table using `head`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, `Class`, `Sex`, `Age`, and `Survived` represent the attributes and `Freq`
    shows the number of passengers for each combination. For instance, there are 35
    male third class children that survived. The other five feature combinations having
    no passengers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the analysis, we can define `nTot` containing the total number of
    passengers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There are `2201` passengers. Out of them, how many survived? We can use a simple
    data table aggregation to count the passengers that survived and the ones that
    didn''t. We need to specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operation**: In order to count the passengers, we sum up the `Freq` column,
    so the operation is `n=sum(Freq)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: We count the passengers for each possible value of the `Survived`
    column, so we need to specify that we aggregate by `Survived`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the data table syntax. We use the square brackets and the three arguments
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rows to select**: We are using all the tables, so the argument is empty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: This contains a list containing the operation, that is, `n=sum(Freq)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: We specify that we aggregate `by=''Survived''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing the data through a histogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can visualize `dtSurvived` by building a histogram and the R function is
    `barplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, the arguments that we need are `height` and `names.arg`, specifying
    the height and labels of the bars. Both the arguments require a vector in our
    case. Let''s see how we can build the chart. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the vector with height containing the number of passengers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the vector with names containing the number of passengers that survived:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the data through a histogram](img/7740OS_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The histogram shows the number of passengers that survived or not. The height
    of each bar is equal to the number of passengers and the labels show what the
    bars represent. We could have built the same chart using just one line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This chart shows the total number of passengers. What if we want to visualize
    the percentage instead? Let''s take a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `percentage` column containing the number of passengers divided
    by the total number of passengers. We can define the new column using the `:=`
    data table operation. This column will be the `height` argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `colorPlot` column containing the colors blue and red for visualization.
    We use `ifelse` that is a function specifying that the color is `blue` if `Survived
    == ''Yes''`, and `red` otherwise. This column will be the `col` argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the chart and as anticipated, we include the `col` argument, defining
    the `color` vector. In addition, the percentage ranges between 0 and 1, so we
    can specify that the area of the plot will be between 0 and 1 adding the `ylim`
    argument equal to `c(0, 1)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the data through a histogram](img/7740OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can add a title and a legend to the chart; follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `textPercentage` column containing the percentage as a string. For
    instance, for a percentage of 0.323035, we display 32 percent in the legend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the plot title:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the *y* axis label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the data through a histogram](img/7740OS_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The general survival rate is **32%**, although it varies across different attribute
    combinations. The next subsection shows you how to visualize the impact of an
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the impact of a feature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we identify the impact of gender on the survival rate.
    First, we can define `dtGender` displaying the number of passengers that survived
    or did not survive, for each gender. The operation is `n=sum(Freq)` and it is
    performed for each combination of `Survived` and `Sex`. Similar to the previous
    section, we perform a simple data table aggregation, specifying the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rows to select**: We are using the entire table, so the argument is empty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: This is a list containing the operation, that is, `n=sum(Freq)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregation**: We aggregate by two columns, so we define `by=c(''Survived'',
    ''Sex'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can visualize the new data table through a histogram, as we saw earlier.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `percentage` column dividing `n` by the number of passengers of the
    gender. The operation is `n / sum(n)` and it is done by gender. Then, we use the
    `:=` operation specifying that we compute the sum `by=''Sex''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the plot colors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the *y* axis label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the table with the male survival statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the histogram for males:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instead of extracting `dtGenderMale`, we could have directly built the chart
    adding `Sex == ''Male''` when extracting the vectors. We can build the same histogram
    for females in a similar way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s display the charts that we built:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of a feature](img/7740OS_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The male survival rate is just **21%** compared with 32% of passengers who survived.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of a feature](img/7740OS_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the female survival rate is significantly higher than the average.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compare the two genders in the same chart displaying just the survival
    rate that is the `Yes` column. We can build the plot using the same commands and
    including the `Survived == ''Yes''` condition. The only difference is the `col`
    argument, that in this case is the `Sex` column that is a factor with two levels.
    In this case, `barplot` automatically defines two colors that are black and red:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of a feature](img/7740OS_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The chart allows us to visualize the difference and the legend displays the
    survival rate. As expected, the difference is huge.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the impact of two features combined
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we investigate the impact of another feature: class. How does
    the survival rate vary across the different classes of passengers? First, we can
    just build the same survival rate chart as the one for gender, by following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define `dtClass` containing the passengers that survived or didn''t survive
    for each class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the percentage of passengers that survived or didn''t survive for each
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the percentage text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the histogram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of two features combined](img/7740OS_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The survival rate varies a lot across the classes. We can notice that the passengers
    belonging to higher classes are more likely to survive and that the crew has a
    survival rate similar to the third class. Can we conclude that class has a high
    impact on the survival rate?
  prefs: []
  type: TYPE_NORMAL
- en: The chart shows the overall survival rate for each class. However, knowing that
    the females are more likely to survive, a class with a higher female:male ratio
    will likely have a higher survival rate. If a higher survival rate is explained
    by the gender only, the fact of belonging to a different class doesn't have an
    impact at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to understand whether the difference between the survival rates depends
    on the percentage of females in each class, we can visualize the gender ratio
    by class. The chart is a histogram showing the percentage of females for each
    social class and the commands are similar as earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of two features combined](img/7740OS_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The gender ratio varies a lot across the different classes as the percentage
    of females is higher in the top classes, and that there are almost no females
    in the crew. Therefore, the percentage of females might have biased the survival
    rate by class. In order to have a better understanding of the impact of the two
    attributes on the survival rate, we need to take account of the gender and of
    the class at the same time. For this purpose, we can compute the survival rate
    for each combination of these two features. Use the following steps to build the
    chart:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the sum of the passengers for each combination of `Survived`, `Sex`,
    and `Class`. Now the `by` argument includes three column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `nTot` column specifying the total number of passengers for each feature
    combination (not including `Survived`). The `by` argument includes two features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `percentage` column. The `by` argument includes the two features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the column containing the survival rate using the `Survived == ''Yes''`
    condition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `textPercentage` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `colorPlot` column. The `rainbow` function builds a vector with a defined
    number of rainbow colors. In this case, we define a column for each row, so we
    use `rainbow(nrow(dtGenderClass))`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the group name to be included in the labels. Since the histogram will
    display the survival rate for each combination of both the features, we set the
    name of each group as the gender and the class combined, using `paste`. In order
    to fit the names into the chart, we define `SexAbbr` containing an abbreviation
    of the gender:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the labels containing the plot name and the number of passengers in
    the group. Since we want to display the name and number in two different lines,
    we separate them with `\n` that defines a new line in a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the histogram. Similar to `ylim`, the `xlim` argument defines the
    *x* region to visualize. In this case, we use `xlim` to avoid overlapping the
    legend and the chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram generated is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of two features combined](img/7740OS_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can find the number of passengers of a group under its column. Apart from
    the female crew, each bar contains at least 100 passengers, so we can assume that
    the result is meaningful. In order to measure the meaningfulness, we could have
    used a statistic technique such as confidence intervals or a hypothesis test,
    but it's not the topic of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The class is affecting the males and the females in different ways. In the case
    of males, the survival rate is very low although it is significantly higher for
    the first class. In the case of females, the survival rate is close to 100 percent
    for each class apart from the third class.
  prefs: []
  type: TYPE_NORMAL
- en: We can also look at the chart in the opposite way in order to understand the
    impact of the gender over the passengers belonging to the same class. In all the
    situations, the survival rate is significantly higher, although the difference
    is much higher for some specific classes. The impact of the gender and the class
    are related, so we need to take account of both the features at the same time
    if we want to understand their effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'We haven''t explored the age yet. We can visualize the survival rate for each
    combination of all the features. The code to prepare and plot the table is similar
    as before. In this case, we can just apply the operations to `dtTitanic` directly.
    The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the percentage of people that survived or didn''t survive for each
    combination of the three features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the percentage of passengers surviving for each attribute combination:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the survival rate using the `Survived == ''Yes''` condition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the legend text including the abbreviations of all the three features.
    For the class, we use substring that is a function extracting a part of the string.
    In our case, we extract the first character, so we specify that we extract the
    elements between `1` and `1` using `substring(Class, 1, 1)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the plot color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the percentage to display in the label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the label including the percentage and the total number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the plot. We have more groups than before, so the layout is different
    in order to visualize all the relevant information. The `xlim` argument leaves
    some space for the legend and the `cex.names` argument decreases the label text
    size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The histogram is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the impact of two features combined](img/7740OS_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The legend displays the abbreviated feature combinations. For instance, **1MC**
    means first class, male, child. In the case of combinations with no passengers,
    we don't have any information about the percentage, so the bar label displays
    **NaN%**.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are combining three features, some groups are very small. For instance,
    we have only five first class male children. There are also other groups with
    no passengers at all (for example, children in the crew). Therefore, this approach
    has some limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data using machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualizing the survival rate for each group of passengers provides us with
    an overview of the data. We have an idea about how the different features interact
    with the survival rate and with each other. For instance, we know that the social
    class has a different impact on the survival rate depending on the gender. But
    which of the two features has the highest impact? How big is the impact of each
    feature? We haven't defined a ranking of the features or quantified the impact
    of each of them. Some machine learning techniques allow us to investigate further,
    answering to our questions.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data using a decision tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have three features (class, sex, and age) and we want to divide the passengers
    in groups accordingly. We can't define a group for each combination of features
    since we won't have enough data for some groups such as the female children of
    the first class. A solution is to divide the passengers in groups in such a way
    that each group contains enough data. A group is defined by some conditions on
    the features, such as males not belonging to the first class. The groups should
    cover every possible situation without overlapping. A machine learning technique
    that identifies groups that are big enough is the decision tree learning.
  prefs: []
  type: TYPE_NORMAL
- en: There is a new passenger and we know it's a male child of the second class.
    We don't know if the passenger will survive or not and we want to predict that.
    How can we use the data? We can check whether the passenger is a male or a female.
    Accordingly, with our previous data exploration, he'll survive with a probability
    of 21 percent since he's a male. Taking account of the social class, we can say
    that he will survive with a probability of 14 percent. There are 179 second class
    male passengers, so this result is meaningful. Then, knowing that he is a child,
    we can check the survival rate of the second class male children, which is 100
    percent. Does it make sense to say that he will survive with a probability of
    100 percent? There are only 11 passengers that are second class male children,
    so we don't have enough data to make an accurate prediction. Should we use the
    survival rate of the second class males? What if we use the survival rate of all
    the male children instead? What about the second class children? There are different
    options leading to different results.
  prefs: []
  type: TYPE_NORMAL
- en: A solution is to identify the key features and take account of them only. For
    instance, if the gender and class are the two most important features, we can
    use them to make a prediction. However, in the case of the third class male children,
    we have much more data than the first class male children. What if we take account
    of the age only in the case of third class males? The number of features that
    we want to include depends on the group we're taking account of.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of just selecting the two most important features, we can define a
    criterion of splitting a group only if it is big enough and we can visualize this
    principle through a decision tree. Let''s suppose that in the beginning all the
    passengers belong to the same group. We can split them in two groups based on
    gender. Then, we can split the males in two groups: first class on one side and
    all the other classes on the other side. For the females, the most meaningful
    split might be another: children on one side, the adults on the other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision tree learning technique learns from the data in order to identify
    the most meaningful splits and it can be used to explore the data. The tree continues
    splitting the data until the groups, defined by the tree leaves, are too small.
    Then, for each group, we use the related data to define an attribute that can
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categoric**: This is an attribute whose value belongs to categories. In this
    case, the categories are **Survived** and **Not survived**. The tree performs
    classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numeric**: This is an attribute that can be measured, and in this case it
    is the survival rate. The tree performs regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can build a decision tree in R using the `rpart` package. In addition, we
    can visualize the trees using another package that is `rpart.plot`. In order to
    use the packages, we need to install and load them. In and case of installation
    issues, you can specify the repository as an argument of `install.packages`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'After the installation, we can load both the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The starting point is `dtTitanic` and it contains a row for each combination
    of the features. Before building the decision tree, we need to transform the data
    into another format. We need to have a row for each passenger and the same columns
    apart from `Freq`. In order to generate the new table in the new format, we use
    the data table operation with `list` and `by`.
  prefs: []
  type: TYPE_NORMAL
- en: For each row of `dtTitanic`, we want to generate a table having a number of
    rows equal to `Freq`. Each row corresponds to a combination between `Survived`,
    `Sex`, `Age`, and `Class`, so the `by` argument contains a vector with the four
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the new table, each row contains a passenger, so `Freq` is equal to `1`.
    Then, for each row of `dtTitanic`, we need to define a vector having `Freq` elements
    equal to `1`. In order to do that, we use `rep` that is a function replicating
    an element a defined number of times. In our case, we use `rep(1, Freq))`. The
    other columns replicate the elements defined in `by` that are `Survived`, `Sex`,
    `Age`, and `Class`, so we don''t need to redefine them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`Freq` is `1` for each row, so we don''t need it anymore and can delete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to build a decision tree showing the survival rate, we need to change
    the format of `Survived`. Instead of having `No` and `Yes`, we want `0` and `1`
    respectively. To modify the column, we can use `ifelse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the first six rows of `DtLong` using `head`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The first six rows show six male children that didn't survive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dtLong` object contains the standard input of the decision tree algorithm
    and we can use `rpart` to build the model. Our goal is to define groups of passengers
    about whom we are able to estimate the survival rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The mandatory arguments are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`formula`: This is a formula object defining the attribute to predict and the
    feature used to predict. The formula is defined by a string such as `outcome ~
    feature1 + feature2 + feature3`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data`: This is the data frame or data table that is `dtLong` in our case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to define the formula starting from the `Survived ~ Sex + Age + Class`
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can build `treeRegr` containing a decision tree. Since `Survived` is
    numeric, the function automatically builds a `regrssion` tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The `treeRegr` object contains the decision tree and we can visualize it using
    `prp(treeRegr)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the data using a decision tree](img/7740OS_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a look at the tree. Each interior node is labeled with a condition
    that splits the data in two parts. The node on the top, for instance, splits the
    passengers into males and females. The branch on the left corresponds to the passengers
    fulfilling the condition (in this case, the male passengers), and the branch on
    the right corresponds to the others (the females). Each leaf defines the survival
    rate of the group. For instance, the leaf to the right states that the survival
    rate for the females not belonging to the third class is 93 percent.
  prefs: []
  type: TYPE_NORMAL
- en: The tree doesn't contain all the possible feature combinations because of the
    lack of data. For instance, in the case of the females, there are only 45 children
    and they belong to different social classes, so the tree doesn't divide the females
    based on their age.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that we have a new passenger that is female, child, second class.
    How do we predict if she will survive? She is a female not belonging to the third
    class, so her expected survival rate is 93 percent. Therefore, we can say that
    she will likely survive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tree defines a survival rate that is a number. What if we wanted to predict
    whether the passenger survived or not? We can build a classification tree adding
    `method=''class''` input to `rpart`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the data using a decision tree](img/7740OS_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This tree predicts that the only passengers that will survive are females and
    children not belonging to the third class. This result is useful to explore the
    data. The next step is using machine learning models to predict an outcome. We
    can use this tree for that purpose, although it just defines five groups of passengers
    out of the 16 possible feature combinations, so it might not be the most appropriate
    technique. There are more advanced algorithms and in the next chapter we see one
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting newer outcomes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a new passenger and knowing his or her personal information, we want to
    predict whether he or she will survive. The options that we explored until now
    are based on dividing the passengers into groups and identifying the survival
    rate for each group. For some combinations of features, such as first class female
    children, we don't have enough data, so we have to use the survival rate of a
    larger group such as females not belonging to the third class. We are ignoring
    some details, for instance, the fact that they are children, and in this way we
    are losing information. Is there a way to estimate the survival rate for any combination
    of features, regardless of how many passengers we have?
  prefs: []
  type: TYPE_NORMAL
- en: There are many machine learning algorithms that take account of all the features
    at the same time. In this chapter, we see a very popular algorithm that is the
    **random forest** algorithm. It's not the best option in this context, as it performs
    better when there are much more features, but it's good for the purpose of illustrating
    a general approach.
  prefs: []
  type: TYPE_NORMAL
- en: Building a machine learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As its name suggests, the random forest algorithm is based on many random decision
    trees. The algorithm builds `ntree` trees repeating the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate the data to build the tree choosing a random row from the data (in
    our case, that is `dtLong`) sampsize times. Each row can be chosen more than once
    and in the end we have a table with sampsize random rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly select a `mtry` number of features (unfortunately in our case we don't
    have many features, but it's still possible to select a subset of them).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a decision tree based on the sampled data taking account of the selected
    features only.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A random forest model is composed by `ntree` decision trees. In our context,
    given a new passenger, the model forecasts their survival rate using each tree.
    The final forecasted value is the average between the survival rates. In another
    variation of the algorithm, we have the mode instead of the average.
  prefs: []
  type: TYPE_NORMAL
- en: 'The random forest is a popular algorithm and it is provided by the `randomForest`
    package. Let''s install and load it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Like the simple decision tree learning, the random forest attributes can be
    categoric or numeric.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, all the features are categoric and there are two to four possible
    values for each feature. We can convert the features into a numeric format. For
    instance, in the case of `Sex`, the possible values are `Male` and `Female`. We
    can define a numeric feature that is `1` in the case of `Male` and `0` otherwise.
    The new feature shows the same information in a different way. Numeric features
    derived from categoric in this way are called dummy variables. In the case of
    a categoric feature with more than two categories, we can define a dummy variable
    for each categories apart from one. In this way, looking at the dummy variables,
    if one of them is equal to `1`, we know which the groups are. If they are all
    equal to `0`, we know that a group is remaining.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a new table containing dummy variables through the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a copy of the categoric features table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `Sex` into a dummy variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `Age` into a dummy variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `Class` into three dummy variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `formulaRf` formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build `forest` containing the random forest model. All the parameters are left
    as their default values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We stored the random forest model in `forest` that is a list containing the
    machine learning model, and all the related parameters and information. We can
    explore the model observing the elements of the list. For instance, the number
    of trees that the model has built is contained in the `ntree` element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Another parameter is `mtry` and it defines the number of variables used in
    each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: The number of trees has been defaulted to 500.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is selecting just one feature at once. The reason is that the
    random forest is meant to work with a lot of features, so it doesn't perform well
    in this context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another parameter is `type` and it defines the output of the algorithm. The
    random forest can be used for different purposes and in our case we want to estimate
    the survival rate, so we want to use it for regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: As expected, `forest` is performing regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to change some parameters, we can define them in the arguments.
    In this chapter, we are not defining a criterion to set the parameters, so we
    just assign another value. For instance, we can build `1000` trees using three
    random features and `1500` random rows for each. We can rebuild `forest` changing
    the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We built a random `forest` model and the next subsection shows how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Using the model to predict new outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have built the model, we can use it to perform some predictions.
    If we have a new passenger, what is their survival rate? First, let''s extract
    a random passenger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The random passenger is an adult male of the first class. We can use the `forest`
    model to estimate his survival rate. The `predict` function allows us to apply
    the model on the new data, obtaining a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The estimated survival rate is about 38 percent, so the passenger won''t likely
    survive. We can use the same approach to predict the survival rate of all the
    passengers. However, that means to apply the model on the same data that we used
    to build it. This approach is not good for testing the model because the predicted
    values will be related to the initial data. Keeping in mind that this result cannot
    be used, we can use it just to compare the prediction with the real data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the prediction of six random rows using `sample`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined a survival rate for each passenger. Let''s add the estimated survival
    rate to the `dtDummy` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can predict that a passenger will survive if their survival rate is
    above a threshold, for instance, 50 percent. We can define a new column name,
    `SurvivedPred`, containing our prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can compare the predicted survival with the initial data. In order to
    evaluate how many times the two values match, we can define an `error` column
    that is `TRUE` if the values don''t match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting from the error column, we can compute the general error as the percentage
    of passengers upon which we made a wrong prediction. We need to divide the number
    of errors by the number of passengers. We can have the number of errors applying
    sum to error, since the sum of a vector of Boolean variables is equal to the number
    of `TRUE` values. The total number of passengers is defined by `.N`, which in
    the `data.table` notation is equal to the number of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The model predicted the wrong outcome in 21 percent of the situations, so we
    have an accuracy of 79 percent. Anyway, this result doesn''t make any sense since
    we''re making a prediction on the same data that we used to build the model. In
    addition, knowing how many passengers survived, we could have just guessed the
    most common outcome for each of them. If more than half of them survived, we can
    set `SurvivedPred = TRUE` for all of them and guess more than half. Let''s compute
    the overall probability of surviving. The general survival rate is lower than
    50 percent, so each passenger is more likely not to survive. Then, in absence
    of any other information, we can predict that no one survives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We could have achieved accuracy of more than 65 percent without taking account
    of any feature, so 79 percent is just 15 percent higher. In addition, as already
    said, this accuracy cannot be used because we are applying the model on the same
    data used to build it.
  prefs: []
  type: TYPE_NORMAL
- en: Validating a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to evaluate the real accuracy of a model, we can build it using a part
    of the data, such as 80 percent of the passengers. Then, we can apply the model
    on the remaining 20 percent of data. The data that we use to build the model is
    called the **training set** and the other is called the **test set**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can assign each row to the training set with a probability of 80 percent.
    In this way, the training set will include about 80 percent of the data. In order
    to define which rows should be included in the training set, we can define a logical
    vector, called `indexTrain`, which is `TRUE` for each row belonging to the training
    set. We can generate the vector using sample and the arguments are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: This represents the possible values; in this case, `TRUE` and `FALSE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size`: This represents the vector length; in this case, it is equal to the
    number of rows in `dtDummy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replace`: If the value is `TRUE`, each value (`TRUE` or `FALSE`) can be sampled
    more than once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prob`: This is a vector with the probabilities of sampling the values of `x`;
    in this case, it is `c(0.8, 0.2)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can extract the rows in which `indexTrain` is equal to `TRUE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same way, we extract the rows of the test set. The `!` operator means
    `NOT` and it allows the rows for which `indexTrain` is equal to `FALSE` to be
    extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the model using the same parameters as before. Knowing that
    we have less data, we can just reduce the sampsize parameters that define the
    data to use for each tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We built a model without taking account of `dtTest`, so we can use it to predict
    on `dtTest`. Like before, we predict that a passenger will survive if their survival
    rate is above 50 percent. After the prediction, we can estimate the error, using
    the same R commands as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The estimated error, `percError`, depends on how we have split the data, so
    it's different every time we define a new random training/test split. However,
    we can repeat the steps many times and compute the average error. This approach
    is called cross validation and it's a very useful tool to estimate the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter showed a generic approach to build and validate a machine learning
    model. Using this approach, we can forecast an attribute and estimate the prediction
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to process data using data table operations
    and built some simple R plots for exploratory data analysis. We learned how to
    use decision trees to find useful insights and build machine learning models (random
    forest) to perform predictions. We saw how to change the parameters of a model
    and how to validate it.
  prefs: []
  type: TYPE_NORMAL
- en: The next three chapters show the steps introduced in this chapter in detail.
    [Chapter 4](ch04.html "Chapter 4. Step 1 – Data Exploration and Feature Engineering"),
    *Step 1 - Data Exploration and Feature Engineering*, shows the first step of machine
    learning that consists of data exploration and feature engineering, in depth.
  prefs: []
  type: TYPE_NORMAL
