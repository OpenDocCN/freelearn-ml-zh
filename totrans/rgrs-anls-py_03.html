<html><head></head><body>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03" class="calibre1"/>Chapter 3. Multiple Regression in Action</h1></div></div></div><p class="calibre8">In the previous chapter, we introduced linear regression as a supervised method for machine learning rooted in statistics. Such a method forecasts numeric values using a combination of predictors, which can be continuous numeric values or binary variables, given the assumption that the data we have at hand displays a certain relation (a linear one, measurable by a correlation) with the target variable. To smoothly introduce many concepts and easily explain how the method works, we limited our example models to just a single predictor variable, leaving to it all the burden of modeling the response.</p><p class="calibre8">However, in real-world applications, there may be some very important causes determining the events you want to model but it is indeed rare that a single variable could take the stage alone and make a working predictive model. The world is complex (and indeed interrelated in a mix of causes and effects) and often it cannot be easily explained without considering various causes, influencers, and hints. Usually more variables have to work together to achieve better and reliable results from a prediction.</p><p class="calibre8">Such an intuition decisively affects the complexity of our model, which from this point forth will no longer be easily represented on a two-dimensional plot. Given multiple predictors, each of them will constitute a dimension of its own and we will have to consider that our predictors are not just related to the response but also related among themselves (sometimes very strictly), a characteristic<a id="id231" class="calibre1"/> of data called <strong class="calibre2">multicollinearity</strong>.</p><p class="calibre8">Before starting, we'd like to write just a few words on the selection of topics we are going to deal with. Though in the statistical literature there is a large number of publications and books devoted to regression assumptions and diagnostics, you'll hardly find anything here because we will leave out such topics. We will be limiting ourselves to discussing problems and aspects that could affect the results of a regression model, on the basis of a practical data science approach, not a purely statistical one.</p><p class="calibre8">Given such premises, in this chapter we are going to:</p><div><ul class="itemizedlist"><li class="listitem">Extend the procedures for a single regression to a multiple one, keeping an eye on possible sources of trouble such as multicollinearity</li><li class="listitem">Understand the importance of each term in your linear model equation</li><li class="listitem">Make your variables work together and increase your ability to predict using interactions between variables</li><li class="listitem">Leverage polynomial expansions to increase the fit of your linear model with non-linear functions</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch03lvl1sec17" class="calibre1"/>Using multiple features</h1></div></div></div><p class="calibre8">To recap the tools <a id="id232" class="calibre1"/>seen in the previous chapter, we reload all the packages and the Boston dataset:</p><div><pre class="programlisting">
<strong class="calibre2">In: import numpy as np</strong>
<strong class="calibre2">  import pandas as pd</strong>
<strong class="calibre2">  import matplotlib.pyplot as plt</strong>
<strong class="calibre2">  import matplotlib as mpl</strong>
<strong class="calibre2">  from sklearn.datasets import load_boston</strong>
<strong class="calibre2">  from sklearn import linear_model</strong>
</pre></div><p class="calibre8">If you are working on the code in an IPython Notebook (as we strongly suggest), the following magic command will allow you to visualize plots directly on the interface:</p><div><pre class="programlisting">
<strong class="calibre2">In: %matplotlib inline</strong>
</pre></div><p class="calibre8">We are still using the Boston dataset, a dataset that tries to explain different house prices in the Boston of the 70s, given a series of statistics aggregated at the census zone level:</p><div><pre class="programlisting">
<strong class="calibre2">In: boston = load_boston()</strong>
<strong class="calibre2">  dataset = pd.DataFrame(boston.data, columns=boston.feature_names)</strong>
<strong class="calibre2">  dataset['target'] = boston.target</strong>
</pre></div><p class="calibre8">We will always work by keeping with us a series of informative variables, the number of observation and variable names, the input data matrix, and the response vector at hand:</p><div><pre class="programlisting">
<strong class="calibre2">In: observations = len(dataset)</strong>
<strong class="calibre2">  variables = dataset.columns[:-1]</strong>
<strong class="calibre2">  X = dataset.ix[:,:-1]</strong>
<strong class="calibre2">  y = dataset['target'].values</strong>
</pre></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec36" class="calibre1"/>Model building with Statsmodels</h2></div></div></div><p class="calibre8">As a first step toward extending to more predictors the previously done analysis with Statsmodels, let's reload the necessary modules from the package (one working with matrices and the other with formulas)<a id="id233" class="calibre1"/>
<a id="id234" class="calibre1"/>:</p><div><pre class="programlisting">
<strong class="calibre2">In: import statsmodels.api as sm</strong>
<strong class="calibre2">  import statsmodels.formula.api as smf</strong>
</pre></div><p class="calibre8">Let's also prepare a suitable input matrix, naming it <code class="email">Xc</code> after having it incremented by an extra column containing the bias vector (a constant variable having the unit value):</p><div><pre class="programlisting">
<strong class="calibre2">In: Xc = sm.add_constant(X)</strong>
<strong class="calibre2">  linear_regression = sm.OLS(y,Xc)</strong>
<strong class="calibre2">  fitted_model = linear_regression.fit()</strong>
</pre></div><p class="calibre8">After <a id="id235" class="calibre1"/>having fitted the preceding<a id="id236" class="calibre1"/> specified model, let's immediately ask for a summary:</p><div><pre class="programlisting">
<strong class="calibre2">In: fitted_model.summary()</strong>
<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00034.jpeg" alt="Model building with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><div><img src="img/00035.jpeg" alt="Model building with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><div><img src="img/00036.jpeg" alt="Model building with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Basically, the<a id="id237" class="calibre1"/> enunciations of the various <a id="id238" class="calibre1"/>statistical measures, as presented in the previous chapter, are still valid. We will just devote a few words to remarking on a couple of extra features we couldn't mention before because they are related to the presence of multiple predictors.</p><p class="calibre8">First, the adjusted R-squared is something to take note of now. When working with multiple variables, the standard R-squared can get inflated because of the many coefficients inserted into the model. If you are using too many predictors, its measure will diverge perceptibly from the plain R-squared. The adjusted R-squared considers the complexity<a id="id239" class="calibre1"/> of the model and reports a<a id="id240" class="calibre1"/> much more realistic R-squared measure.</p><div><h3 class="title2"><a id="tip16" class="calibre1"/>Tip</h3><p class="calibre8">Just make a ratio between the plain and the adjusted R-squared measure. Check if their difference exceeds 20%. If it does, it means that we have introduced some redundant variables inside our model specification. Naturally, the larger the ratio difference, the more serious the problem.</p></div><p class="calibre8">This is not the case in our example because the difference is quite slight, approximately between 0.741 and 0.734, which translated into a ratio turns out to be <em class="calibre9">0.741/0.734 = 1.01</em>, that is just 1% over the standard R-squared.</p><p class="calibre8">Then, working with so many variables at a time, coefficients should also be checked for important warnings. The risk involved is having coefficients picking up noisy and non-valuable information. Usually such coefficients will not be far from zero and will be noticeable because of their large standard errors. Statistical t-tests are the right tool to spot them.</p><div><h3 class="title2"><a id="tip17" class="calibre1"/>Tip</h3><p class="calibre8">Be aware that variables with a low p-value are good candidates for being removed from the model because there will probably be little proof that their estimated coefficient is different from zero.</p></div><p class="calibre8">In our example, being largely not significant (p-value major of 0.05), the <code class="email">AGE</code> and <code class="email">INDUS</code> variables are represented in the model by coefficients whose usefulness could be seriously challenged.</p><p class="calibre8">Finally, the condition number test (<code class="email">Cond. No.</code>) is another previously mentioned statistic that now acquires a fresh importance under the light of a system of predictors. It signals numeric unstable results when trying an optimization based on matrix inversion. The cause of such instability is due to multicollinearity, a problem we are going to expand on in the following paragraphs.</p><div><h3 class="title2"><a id="tip18" class="calibre1"/>Tip</h3><p class="calibre8">When a condition number is over the score of 30, there's a clear signal that unstable results are rendering the result less reliable. Predictions may be affected by errors and the coefficients may drastically change when rerunning the same regression analysis with a subset or a different set of observations.</p></div><p class="calibre8">In our<a id="id241" class="calibre1"/> case, the condition number is <a id="id242" class="calibre1"/>well over 30, and that's a serious warning signal.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec37" class="calibre1"/>Using formulas as an alternative</h2></div></div></div><p class="calibre8">To obtain the<a id="id243" class="calibre1"/> same results using the <code class="email">statsmodels.formula.api</code> and thereby explicating a formula to be interpreted by the Patsy<a id="id244" class="calibre1"/> package (<a class="calibre1" href="http://patsy.readthedocs.org/en/latest/">http://patsy.readthedocs.org/en/latest/</a>), we use:</p><div><pre class="programlisting">
<strong class="calibre2">linear_regression = smf.ols(formula = 'target ~ CRIM + ZN +INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', data=dataset)</strong>
<strong class="calibre2">fitted_model = linear_regression.fit()</strong>
</pre></div><p class="calibre8">In this case, you have to explicate all the variables to enter into model building by naming them on the right side of the formula. After fitting the model, you can use all the previously seen Statsmodels methods for reporting the coefficients and results.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch03lvl2sec38" class="calibre1"/>The correlation matrix</h2></div></div></div><p class="calibre8">When trying to <a id="id245" class="calibre1"/>model the response using a single predictor, we <a id="id246" class="calibre1"/>used Pearson's correlation (Pearson was the name of its inventor) to estimate a coefficient of linear association between the predictor and the target. Having more variables in the analysis now, we are still quite interested in how each predictor relates to the response; however, we have to distinguish whether the relation between the variance of the predictor and that of the target is due to unique or shared variance.</p><p class="calibre8">The measurement of the association due to unique variance is called <strong class="calibre2">partial correlation</strong> and it expresses what can be guessed of the response thanks to the information uniquely present in a variable. It represents the exclusive contribution of a variable in predicting the response, its unique impact as a direct cause to the target (if you can view it as being a cause though, because, as seen, correlation is not causation).</p><p class="calibre8">The shared variance is instead the amount of information that is simultaneously present in a variable and in other variables in the dataset at hand. Shared variance can have many causes; maybe one variable causes or it just interferes with the other (as we described in the previous chapter in the <em class="calibre9">Correlation is not causation</em> section ). Shared variance, otherwise <a id="id247" class="calibre1"/>called <strong class="calibre2">collinearity</strong> (between two variables) or multicollinearity (among three or more variables), has an important effect, worrisome for the classical statistical approach, less menacing for the data science one.</p><p class="calibre8">For the statistical approach, it has to be said that high or near perfect multicollinearity not only often renders coefficient estimations impossible (matrix inversion is not working), but also, when it is feasible, it will be affected by imprecision in coefficient estimation, leading to large standard errors of the coefficients. However, the predictions won't be affected in any way and that leads us to the data science points of view.</p><p class="calibre8">Having multicollinear <a id="id248" class="calibre1"/>variables, in fact, renders it difficult to <a id="id249" class="calibre1"/>select the correct variables for the analysis (since the variance is shared, it is difficult to figure out which variable should be its causal source), leading to sub-optimal solutions that could be resolved only by augmenting the number of observations involved in the analysis.</p><p class="calibre8">To determine the manner and number of predictors affecting each other, the right tool is a correlation matrix, which, though a bit difficult to read when the number of the features is high, is still the most direct way to ascertain the presence of shared variance:</p><div><pre class="programlisting">
<strong class="calibre2">X = dataset.ix[:,:-1]</strong>
<strong class="calibre2">correlation_matrix = X.corr()</strong>
<strong class="calibre2">print (correlation_matrix)</strong>
</pre></div><p class="calibre8">This will give the following output:</p><div><img src="img/00037.jpeg" alt="The correlation matrix" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">At first glance, some high correlations appear to be present in the order of the absolute value of 0.70 (highlighted by hand in the matrix) between <code class="email">TAX</code>, <code class="email">NOX</code>, <code class="email">INDUS</code>, and <code class="email">DIS</code>. That's fairly <a id="id250" class="calibre1"/>explainable since <code class="email">DIS</code> is the distance from <a id="id251" class="calibre1"/>employment centers, <code class="email">NOX</code> is a pollution indicator, <code class="email">INDUS</code> is the quota of non-residential or commercial buildings in the area, and <code class="email">TAX</code> is the property tax rate. The right combination of these variables can well hint at what the productive areas are.</p><p class="calibre8">A faster, but less numerical representation is to build a heat map of the correlations:</p><div><pre class="programlisting">
<strong class="calibre2">In: </strong>
<strong class="calibre2">def visualize_correlation_matrix(data, hurdle = 0.0):</strong>
<strong class="calibre2">    R = np.corrcoef(data, rowvar=0)</strong>
<strong class="calibre2">    R[np.where(np.abs(R)&lt;hurdle)] = 0.0</strong>
<strong class="calibre2">    heatmap = plt.pcolor(R, cmap=mpl.cm.coolwarm, alpha=0.8)</strong>
<strong class="calibre2">    heatmap.axes.set_frame_on(False)</strong>
<strong class="calibre2">    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor=False)</strong>
<strong class="calibre2">    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor=False)</strong>
<strong class="calibre2">    heatmap.axes.set_xticklabels(variables, minor=False)</strong>
<strong class="calibre2">    plt.xticks(rotation=90)</strong>
<strong class="calibre2">    heatmap.axes.set_yticklabels(variables, minor=False)</strong>
<strong class="calibre2">    plt.tick_params(axis='both', which='both', bottom='off', \top='off', left = 'off', right = 'off')</strong>
<strong class="calibre2">    plt.colorbar()</strong>
<strong class="calibre2">    plt.show()</strong>

<strong class="calibre2">visualize_correlation_matrix(X, hurdle=0.5)</strong>
</pre></div><p class="calibre8">This will give the following output:</p><div><img src="img/00038.jpeg" alt="The correlation matrix" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Having a cut at 0.5 correlation (which translates into a 25% shared variance), the heat map immediately reveals how <strong class="calibre2">PTRATIO</strong> and <strong class="calibre2">B</strong> are not so related to other predictors. As a reminder <a id="id252" class="calibre1"/>of the meaning of variables, <strong class="calibre2">B</strong> is an indicator <a id="id253" class="calibre1"/>quantifying the proportion of colored people in the area and <strong class="calibre2">PTRATIO</strong> is the pupil-teacher ratio in the schools of the area. Another intuition provided by the map is that a cluster of variables, namely <strong class="calibre2">TAX</strong>, <strong class="calibre2">INDUS</strong>, <strong class="calibre2">NOX</strong>, and <strong class="calibre2">RAD</strong>, is confirmed to be in strong linear association.</p><p class="calibre8">An even more automatic way to detect such associations (and figure out numerical problems in a matrix inversion) is to use eigenvectors. Explained in layman's terms, eigenvectors are a very smart way to recombine the variance among the variables, creating new features accumulating all the shared variance. Such recombination can be achieved using the NumPy <code class="email">linalg.eig</code> function, resulting in a vector of eigenvalues (representing the amount of recombined variance for each new variable) and eigenvectors (a matrix telling us how the new variables relate to the old ones):</p><div><pre class="programlisting">
<strong class="calibre2">In: corr = np.corrcoef(X, rowvar=0)</strong>
<strong class="calibre2">  eigenvalues, eigenvectors = np.linalg.eig(corr)</strong>
</pre></div><p class="calibre8">After extracting the eigenvalues, we print them in descending order and look for any element whose value is near to zero or small compared to the others. Near zero values can represent a real problem for normal equations and other optimization methods based on matrix inversion. Small values represent a high but not critical source of multicollinearity. If you spot any of these low values, keep a note of their index in the list (Python indexes start from zero).</p><div><pre class="programlisting">
<strong class="calibre2">In: print (eigenvalues)</strong>
<strong class="calibre2">Out: [ 6.12265476  1.43206335  1.24116299  0.85779892  0.83456618  0.65965056  0.53901749  0.39654415  0.06351553  0.27743495  0.16916744  0.18616388  0.22025981]</strong>
</pre></div><p class="calibre8">Using their index position in the list of eigenvalues, you can recall their specific vector from eigenvectors, which contains all the variable loadings—that is, the level of association with the original variables. In our example, we investigate the eigenvector at index <code class="email">8</code>. Inside the eigenvector, we notice values at index positions <code class="email">2</code>, <code class="email">8</code>, and <code class="email">9</code>, which are indeed outstanding in terms of absolute value:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (eigenvectors[:,8])</strong>
<strong class="calibre2">Out: [-0.04552843  0.08089873  0.25126664 -0.03590431 -0.04389033 -0.04580522  0.03870705  0.01828389  0.63337285 -0.72024335 -0.02350903  0.00485021 -0.02477196]</strong>
</pre></div><p class="calibre8">We now<a id="id254" class="calibre1"/> print the variables' names to know which ones <a id="id255" class="calibre1"/>contribute so much by their values to build the eigenvector:</p><div><pre class="programlisting">
<strong class="calibre2">In: print (variables[2], variables[8], variables[9])</strong>
<strong class="calibre2">Out: INDUS RAD TAX</strong>
</pre></div><p class="calibre8">Having found the multicollinearity culprits, what remedy could we use for such variables? Removal of some of them is usually the best solution and that will be carried out in an automated way when exploring how variable selection works in <a class="calibre1" title="Chapter 6. Achieving Generalization" href="part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6">Chapter 6</a>, <em class="calibre9">Achieving Generalization</em>.</p></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec18" class="calibre1"/>Revisiting gradient descent</h1></div></div></div><p class="calibre8">In continuity with the previous chapter, we carry on our explanation and experimentation with<a id="id256" class="calibre1"/> gradient descent. As we have already defined both the mathematical formulation and their translation into Python code, using matrix notation, we don't need to worry if now we have to deal with more than one variable at a time. Having used the matrix notation allows us to easily extend our previous introduction and example to multiple predictors with just minor changes to the algorithm.</p><p class="calibre8">In particular, we<a id="id257" class="calibre1"/> have to take note that, by introducing more parameters to be estimated during the optimization procedure, we are actually introducing more dimensions to our line of fit (turning it into a hyperplane, a multidimensional surface) and such dimensions have certain communalities and differences to be taken into account.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec39" class="calibre1"/>Feature scaling</h2></div></div></div><p class="calibre8">Working with<a id="id258" class="calibre1"/> different features requires more attention when<a id="id259" class="calibre1"/> estimating the coefficients because of their similarities which can cause a variance increase of the estimates, as we initially discussed. Multicollinearity between variables also has other drawbacks because it can also make matrix inversion (the matrix operation at the core of the normal equation coefficient estimation) very difficult, if not impossible, to achieve; such a problem is due to a mathematical limitation of the algorithm. Gradient descent, instead, is not affected at all by reciprocal correlation, allowing us to estimate reliable coefficients even in the presence of perfect collinearity.</p><p class="calibre8">Anyway, though being quite resistant to problems that affect other approaches, its simplicity also makes it vulnerable to other common problems, such as the different scale present in each feature. In fact, some features in your data may be represented by measurements in units, some in decimals, and others in thousands, depending on what aspect of reality each feature represents. In our real estate example, one feature could be the number of rooms, another one could be the percentage of certain pollutants in the air, and finally, the average value of a house in the neighborhood. When it is the case that the features have a different scale, though the algorithm will be processing each of them separately, optimization will be dominated by the variables with the more extensive scale. Working in a space of dissimilar dimensions will require more iterations before convergence to a solution (and sometimes there might be no convergence at all).</p><p class="calibre8">The remedy is very easy; it is just necessary to put all the features on the same scale. Such an operation is called <strong class="calibre2">feature scaling</strong>. Feature scaling can be achieved through standardization or normalization. Normalization rescales all the values in the interval between zero and one (usually, but different ranges are also possible), whereas standardization operates by removing the mean and dividing by standard deviation to obtain a unit variance. In our case, standardization is preferable, both because it easily permits retuning the obtained standardized coefficients into their original scale and also because, by centering all the features at the zero mean, it makes the error surface more tractable by many machine learning algorithms, in a much more effective way than just rescaling the maximum and minimum of a variable.</p><p class="calibre8">An important reminder when applying feature scaling is that changing the scale of the features implies that you will have to use rescaled features also for predictions, unless you can recalculate the coefficients as if the variables had never been rescaled.</p><p class="calibre8">Let's try the<a id="id260" class="calibre1"/> algorithm, first using standardization based on the <a id="id261" class="calibre1"/>Scikit-learn <code class="email">preprocessing</code> module:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.preprocessing import StandardScaler</strong>
<strong class="calibre2">  observations = len(dataset)</strong>
<strong class="calibre2">  variables = dataset.columns</strong>
<strong class="calibre2">  standardization = StandardScaler()</strong>
<strong class="calibre2">  Xst = standardization.fit_transform(X)</strong>
<strong class="calibre2">  original_means = standardization.mean_</strong>
<strong class="calibre2">  originanal_stds = standardization.std_</strong>
<strong class="calibre2">  Xst = np.column_stack((Xst,np.ones(observations)))</strong>
<strong class="calibre2">  y  = dataset['target'].values</strong>
</pre></div><p class="calibre8">In the preceding code, we just standardized the variables using the <code class="email">StandardScaler</code> class from Scikit-learn. This class can fit a data matrix, record its column means and standard deviations, and operate a transformation on itself, as well as on any other similar matrix, standardizing the column data. With this method, after fitting we keep a track of means and standard deviations that have been used because they will come in handy later when we have to recalculate the coefficients using the original scale:</p><div><pre class="programlisting">
<strong class="calibre2">In: import random</strong>

<strong class="calibre2">  def random_w( p ):</strong>
<strong class="calibre2">      return np.array([np.random.normal() for j in range(p)])</strong>
<strong class="calibre2">  def hypothesis(X,w):</strong>
<strong class="calibre2">      return np.dot(X,w)</strong>

<strong class="calibre2">  def loss(X,w,y):</strong>
<strong class="calibre2">      return hypothesis(X,w) - y</strong>

<strong class="calibre2">  def squared_loss(X,w,y):</strong>
<strong class="calibre2">      return loss(X,w,y)**2</strong>

<strong class="calibre2">  def gradient(X,w,y):</strong>
<strong class="calibre2">      gradients = list()</strong>
<strong class="calibre2">      n = float(len( y ))</strong>
<strong class="calibre2">      for j in range(len(w)):</strong>
<strong class="calibre2">          gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)</strong>
<strong class="calibre2">      return gradients</strong>

<strong class="calibre2">  def update(X,w,y, alpha=0.01):</strong>
<strong class="calibre2">      return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]</strong>

<strong class="calibre2">  def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):</strong>
<strong class="calibre2">      w = random_w(X.shape[1])</strong>
<strong class="calibre2">      path = list()</strong>
<strong class="calibre2">      for k in range(iterations):</strong>
<strong class="calibre2">          SSL = np.sum(squared_loss(X,w,y))</strong>
<strong class="calibre2">          new_w = update(X,w,y, alpha=alpha)</strong>
<strong class="calibre2">          new_SSL = np.sum(squared_loss(X,new_w,y))</strong>
<strong class="calibre2">          w = new_w</strong>
<strong class="calibre2">          if k&gt;=5 and (new_SSL - SSL &lt;= eta and \new_SSL - SSL &gt;= -eta):</strong>
<strong class="calibre2">              path.append(new_SSL)</strong>
<strong class="calibre2">              return w, path</strong>
<strong class="calibre2">          if k % (iterations / 20) == 0:</strong>
<strong class="calibre2">              path.append(new_SSL)</strong>
<strong class="calibre2">      return w, path</strong>

<strong class="calibre2">  alpha = 0.02</strong>
<strong class="calibre2">  w, path = optimize(Xst, y, alpha, eta = 10**-12, \iterations = 20000)</strong>
<strong class="calibre2">  print ("These are our final standardized coefficients: " + ', \'.join(map(lambda x: "%0.4f" % x, w))) </strong>

<strong class="calibre2">Out: These are our final standardized coefficients: -0.9204, 1.0810, 0.1430, 0.6822, -2.0601, 2.6706, 0.0211, -3.1044, 2.6588, -2.0759, -2.0622, 0.8566, -3.7487, 22.5328</strong>
</pre></div><p class="calibre8">The code <a id="id262" class="calibre1"/>we are using is not at all different from the code we <a id="id263" class="calibre1"/>used in the previous chapter with the exception of its input, which is now made up of multiple standardized variables. In this case, the algorithm reaches a convergence in fewer iterations and uses a smaller alpha than before because in our previous example our single variable actually was unstandardized. Observing the output, we now need a way to rescale the coefficients to their variables' characteristics and we will be able to report the gradient descent solution in unstandardized form.</p><p class="calibre8">Another point to mention is our choice of alpha. After some tests, the value of <code class="email">0.02</code> has been chosen for its good performance on this very specific problem. Alpha is the learning rate and during optimization it can be fixed or changeable, accordingly to a line search method, modifying its value in order to minimize as far as possible the cost function at each single step of the optimization process. In our example, we opted for a fixed learning rate and we had to look for its best value by trying a few optimization values and deciding on which minimized the cost in the minor number of iterations.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec40" class="calibre1"/>Unstandardizing coefficients</h2></div></div></div><p class="calibre8">Given a<a id="id264" class="calibre1"/> vector of standardized coefficients from a linear regression and its bias, we can recall the formulation of the linear regression, which is:</p><div><img src="img/00039.jpeg" alt="Unstandardizing coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The previous formula, transforming the predictors using their mean and standard deviation, is actually equivalent (after a few calculations) to such an expression:</p><div><img src="img/00040.jpeg" alt="Unstandardizing coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In our formula, <img src="img/00041.jpeg" alt="Unstandardizing coefficients" class="calibre22"/> represents the original mean and <em class="calibre9">δ</em> the original variance of the variables.</p><p class="calibre8">By comparing the different parts of the two formulas (the first parenthesis and the second summation), we can calculate the bias and coefficient equivalents when transforming a standardized coefficient into an unstandardized one. Without replicating all the mathematical formulas, we can quickly implement them into Python code and <a id="id265" class="calibre1"/>immediately provide an application showing how such calculations can transform gradient descent coefficients:</p><div><pre class="programlisting">
<strong class="calibre2">In: unstandardized_betas = w[:-1] / originanal_stds</strong>
<strong class="calibre2">  unstandardized_bias  = w[-1]-np.sum((original_means /originanal_stds) * w[:-1])</strong>
<strong class="calibre2">  print ('%8s: %8.4f' % ('bias', unstandardized_bias))</strong>
<strong class="calibre2">  for beta,varname in zip(unstandardized_betas, variables):</strong>
<strong class="calibre2">          print ('%8s: %8.4f' % (varname, beta))</strong>

<strong class="calibre2">Out:</strong>
</pre></div><div><img src="img/00042.jpeg" alt="Unstandardizing coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As an <a id="id266" class="calibre1"/>output from the previous code snippet, you will get a list of coefficients identical to our previous estimations with both Scikit-learn and Statsmodels.</p></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec19" class="calibre1"/>Estimating feature importance</h1></div></div></div><p class="calibre8">After having confirmed the values of the coefficients of the linear model we have built, and after having explored the basic statistics necessary to understand if our model is working correctly, we can start auditing our work by first understanding how a prediction is made up. We can <a id="id267" class="calibre1"/>obtain this by accounting for each variable's role in the constitution of the predicted values. A first check to be done on the coefficients is surely on the directionality they express, which is simply dictated by their sign. Based on our expertise on the subject (so it is advisable to be knowledgeable about the domain we are working on), we can check whether all the coefficients correspond to our expectations in terms of directionality. Some features may decrease the response as we expect, thereby correctly confirming that they have a coefficient with a negative sign, whereas others may increase it, so a positive coefficient should be correct. When coefficients do not correspond to our expectations, we have <strong class="calibre2">reversals</strong>. Reversals <a id="id268" class="calibre1"/>are not uncommon and they can actually reveal that things work in a different way than we expected. However, if there is much multicollinearity between our predictors, reversals could be just due to the higher uncertainty of estimates: some estimates may be so uncertain that the optimization processes have allocated them a wrong sign. Consequently, when a linear regression doesn't confirm our expectations, it is better not to jump to any quick conclusion; instead, closely inspect all the statistical measures pointing toward multicollinearity.</p><p class="calibre8">A second check is done on the impact of the variable on the model—that is, how much of the predicted result is dominated by variations in feature. Usually, if the impact is low, reversals and other difficulties caused by the variable (it could be from an unreliable source, for instance, or very noisy—that is, measured imprecisely) are less disruptive for the predictions or can even be ignored.</p><p class="calibre8">Introducing the<a id="id269" class="calibre1"/> idea of impact also presents the possibility of making our model economic in terms of the number of modeled coefficients. Up to now, we have just concentrated on the idea that it is desirable to fit data in the best way possible and we didn't check if our predictive formula generalizes well to new data. Starting to rank the predictors could help us make new simpler models (by just selecting only the most important features in the model) and simpler models are less prone to errors when in the production phase.</p><p class="calibre8">In fact, if our objective is not simply to fit our present data maximally with a formula, but to also fit future data well, it is necessary to apply the principle of Occam's razor. This suggests that, given more correct answers, simpler models are always preferable to more complex ones. The core idea is not to make an explanation, that is a linear model, more complex than it should be, because complexity may hide overfitting.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec41" class="calibre1"/>Inspecting standardized coefficients</h2></div></div></div><p class="calibre8">Extending our interpretation of the linear model from one single variable to a host of them, we can<a id="id270" class="calibre1"/> still read each single<a id="id271" class="calibre1"/> coefficient as the unit change inducted on the response variable by each predictor (keeping the other predictors constant).</p><p class="calibre8">Intuitively, larger coefficients seem to impact more on the result of the linear combination; however, as we noticed while revisiting gradient descent, different variables may have different scales and their coefficients may incorporate this. Being smaller or larger in terms of coefficient may just be because of the variable's relative scale in comparison to the other features involved in the analysis.</p><p class="calibre8">By standardizing variables, we place them under a similar scale where a unit is the standard deviation of the variable itself. Variables extending from high to low values (with a larger range) tend to have a greater standard deviation and you should expect them to be reduced. By doing so, most variables with a normal-like distribution should be reduced in the range <em class="calibre9">-3 &lt; x &lt; +3</em>, thus allowing a comparison on their contribution to the prediction. Highly skewed distributions won't be standardized in the range <em class="calibre9">-3 &lt;x &lt;+3</em>. The transformation will be beneficial anyway because their range is going to be largely reduced and after that it will even make sense to compare different distributions because then they will all represent the same unit measure—that is, the unit variance. After standardization, larger coefficients can be interpreted as major contributions to establishing the result (a weighted summation, so the result will resemble larger weights more closely). Using standardized coefficients, we can therefore confidently rank our variables and spot those contributing less.</p><p class="calibre8">Let's proceed<a id="id272" class="calibre1"/> to an example using <a id="id273" class="calibre1"/>our Boston dataset. This time we will be using the <code class="email">LinearRegression</code> method from Scikit-learn because we do not need to do a linear model for its statistical properties, but just a working model using a fast and scalable algorithm:</p><div><pre class="programlisting">
<strong class="calibre2">In: linear_regression = linear_model.LinearRegression(normalize=False, fit_intercept=True)</strong>
</pre></div><p class="calibre8">In such an initialization, apart from the <code class="email">fit_intercept</code> parameter that explicates the insertion of a bias into the design of model, the <code class="email">normalize</code> option indicates whether we intend to rescale all the variables in the range between zero and one. Such a transformation is different from statistical standardization and we will omit it for the moment by setting it in a <code class="email">False</code> state:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.preprocessing import StandardScaler</strong>
<strong class="calibre2">  from sklearn.pipeline import make_pipeline</strong>
<strong class="calibre2">  standardization = StandardScaler()</strong>
<strong class="calibre2">  Stand_coef_linear_reg = make_pipeline(standardization,\linear_regression)</strong>
</pre></div><p class="calibre8">Besides the <code class="email">StandardScaler</code> seen before, we also import from Scikit-learn the convenient <code class="email">make_pipeline</code> wrapper, which allows us to establish a sequence of operations to be done automatically on our data before feeding it to the linear regression analysis. Now, the <code class="email">Stand_coef_linear_reg</code> pipeline will execute a statistical standardization on data before regressing it, thus outputting standardized coefficients:</p><div><pre class="programlisting">
<strong class="calibre2">In: linear_regression.fit(X,y)</strong>
<strong class="calibre2">for coef, var in sorted(zip(map(abs,linear_regression.coef_), \</strong>
<strong class="calibre2">           dataset.columns[:-1]), reverse=True):</strong>
<strong class="calibre2">           print ("%6.3f %s" % (coef,var))</strong>

<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00043.jpeg" alt="Inspecting standardized coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As a first<a id="id274" class="calibre1"/> step, we output the <a id="id275" class="calibre1"/>coefficients of the regression on unstandardized data. As seen before, the output seems dominated by the huge coefficient of the NOX variable, which overlooks (with its absolute value of about 17.8) minor coefficients of 3.8 and less. However, we may question whether it could be so after also standardizing the variables:</p><div><pre class="programlisting">
<strong class="calibre2">In: Stand_coef_linear_reg.fit(X,y)</strong>
<strong class="calibre2">for coef, var in \</strong>
<strong class="calibre2">sorted(zip(map(abs,Stand_coef_linear_reg.steps[1][1].coef_), \</strong>
<strong class="calibre2">dataset.columns[:-1]), reverse=True):</strong>
<strong class="calibre2">              print ("%6.3f %s" % (coef,var))</strong>

<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00044.jpeg" alt="Inspecting standardized coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Having all the predictors on a similar scale now, we can easily provide a more realistic interpretation of each coefficient. Clearly, it appears that a unit change has more impact when it involves the variables <code class="email">LSTAT</code>, <code class="email">DIS</code>, <code class="email">RM</code>, <code class="email">RAD</code>, and <code class="email">TAX</code>. <code class="email">LSTAT</code> is the percentage of lower status population, and this aspect explains its relevancy.</p><p class="calibre8">Using<a id="id276" class="calibre1"/> standardized scales has <a id="id277" class="calibre1"/>certainly pointed us at the most important variables but it is still not a complete overview of the predictive power of each variable because:</p><div><ul class="itemizedlist"><li class="listitem">Standardized coefficients represent how well the model works out its predictions because large coefficients heavily impact the resulting response: though having a large coefficient is certainly a hint of how important a variable is, it tells us just a part of the role that the variable plays in reducing the error of the estimates and in making our predictions more accurate</li><li class="listitem">Standardized coefficients can be ranked but their unit, though similar in scale, is somehow abstract (the standard deviation of each variable) and relative to the data at hand (so we shouldn't compare the standardized coefficients of different datasets because their standard deviations could be different)</li></ul></div><p class="calibre8">One solution could be to <a id="id278" class="calibre1"/>integrate the importance estimate based on standardized coefficients, instead using some measure related to the error measure.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec42" class="calibre1"/>Comparing models by R-squared</h2></div></div></div><p class="calibre8">From a<a id="id279" class="calibre1"/> general point of view, we can<a id="id280" class="calibre1"/> evaluate a model by comparing how better it does in respect of a simple mean, and that's the coefficient of determination, R-squared.</p><p class="calibre8">R-squared can estimate how good a model is; therefore, by comparing the R-squared of our model against alternative models where the variables have been removed, we can get an idea of how predictive each removed variable is. All we have to do is compute the difference between the coefficients of determination of the initial model against the model without that variable. If the difference is large, the variable is very important in the determination of a better R-squared and of a better model.</p><p class="calibre8">In our case, we have to first record what the R-squared is when we build the model with all the variables present. We can name such a value our baseline of comparison:</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.metrics import r2_score</strong>
<strong class="calibre2">  linear_regression = linear_model.LinearRegression(normalize=False,\fit_intercept=True)</strong>
<strong class="calibre2">def r2_est(X,y):</strong>
<strong class="calibre2">    return r2_score(y,linear_regression.fit(X,y).predict(X))</strong>

<strong class="calibre2">print ('Baseline R2: %0.3f' %  r2_est(X,y))</strong>

<strong class="calibre2">Out:Baseline R2: 0.741</strong>
</pre></div><p class="calibre8">After that, all we have to do is to remove one variable at a time from the set of predictors, to estimate<a id="id281" class="calibre1"/> again the regression model<a id="id282" class="calibre1"/> recording its coefficient of determination, and subtract it from the baseline value we got from the complete regression model:</p><div><pre class="programlisting">
<strong class="calibre2">In: r2_impact = list()</strong>
<strong class="calibre2">  for j in range(X.shape[1]):</strong>
<strong class="calibre2">    selection = [i for i in range(X.shape[1]) if i!=j]</strong>
<strong class="calibre2">    r2_impact.append(((r2_est(X,y) - \r2_est(X.values [:,selection],y)) ,dataset.columns[j]))</strong>
<strong class="calibre2">  for imp, varname in sorted(r2_impact, reverse=True):</strong>
<strong class="calibre2">    print ('%6.3f %s' %  (imp, varname))</strong>

<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00045.jpeg" alt="Comparing models by R-squared" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">After we get all the differences, each one representing the contribution of each variable to the R-squared, we just have to rank them and we will then have an idea of which variables contribute more in reducing the error of the linear model; this is a different point of view from knowing which variable contributed the most to the response value. Such a contribution is called the partial R-squared.</p><p class="calibre8">Apart from suggesting we use both measures (standardized coefficients and partial Rsquared) to separate relevant variables from irrelevant ones, by using the partial Rsquared you can actually directly compare the importance of the variables because using ratios does make sense here (so you can tell that a variable is twice as important as another because it reduces the error twice as much).</p><p class="calibre8">Another <a id="id283" class="calibre1"/>noticeable point is that partial<a id="id284" class="calibre1"/> R-squareds are not really a decomposition of the initial R-squared measure. In fact, only if the predictors are uncorrelated by summing all the partial scores will you get the precise coefficient of determination of the full model. This is due to collinearity between variables. Therefore, when you remove a variable from the model, you certainly do not remove all its informative variance since correlated variables, containing similar information to the removed variable, are kept in the model. It may happen that, if you have two highly correlated variables, removing each in turn won't change the R-squared by much because, as one is removed, the other one will provide the missing information (thus, the double-check with the standardized coefficient is not redundant at all).</p><p class="calibre8">There are more sophisticated methods to estimate the variable importance, but these two methods should provide you with enough insight. Knowing what variables impact your results more strongly can provide you with the means to:</p><div><ol class="orderedlist"><li class="listitem" value="1">Try to explain the results to management in a reasonable and understandable way.</li><li class="listitem" value="2">Prioritize your work, in terms of data cleaning, preparation, and transformation, by concentrating first on the features most relevant to the success of your project.</li><li class="listitem" value="3">Conserve resources, in particular memory, as less data is used when building and using the regression model.</li></ol><div></div><p class="calibre8">If you would like to use importance to exclude irrelevant variables using one of the measures we presented, the safest way would be to recalculate the ranking every time you decide to <a id="id285" class="calibre1"/>exclude a variable from <a id="id286" class="calibre1"/>the set. Otherwise, using a single ranking may risk hiding the true importance of highly correlated variables (and that's true for both the methodologies, though standardized coefficients are a bit more revealing).</p><p class="calibre8">Such an approach is certainly time-consuming, but it is necessary when you notice that your model, though presenting a good fit on your present data, cannot generalize well to new observations.</p><p class="calibre8">We are going to discuss such circumstances in more depth in <a class="calibre1" title="Chapter 6. Achieving Generalization" href="part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6">Chapter 6</a>, <em class="calibre9">Achieving Generalization</em>, when we will illustrate the best ways to reduce your predictor set, maintaining (simplifying your solution) and even improving your predictive performances (generalizing more effectively).</p></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec20" class="calibre1"/>Interaction models</h1></div></div></div><p class="calibre8">Having explained how <a id="id287" class="calibre1"/>to build a regression model with multiple variables and having touched on the theme of its utilization and interpretation, we start from this paragraph to explore how to improve it. As a first step, we will work on its fit with present data. In the following chapters, devoted to model selection and validation, we will concentrate on how to make it really generalizable—that is, capable of correctly predicting on new, previously unseen data.</p><p class="calibre8">As we previously reasoned, the beta coefficients in a linear regression represent the link between a unit change in the predictors and the response variations. The assumptions at the core of such a model are of a constant and unidirectional relationship between each predictor and the target. It is the linear relationship assumption, having the characteristics of a line where direction and fluctuation are determined by the angular coefficient (hence the name linear regression, hinting at the operation of regressing, tracing back to the linear form from some data evidence). Although a good approximation, a linear relationship is a simplification often not true in real data. In fact, most relationships are non-linear, showing bends and curves and alternating different fluctuations in increase and decrease.</p><p class="calibre8">The good news is that we do not have to limit ourselves to the originally provided features, but we <a id="id288" class="calibre1"/>can modify them in order to <em class="calibre9">straighten</em> their relationship with the target variable. In fact, the more similarity between the predictor and the response, the better the fit and the fewer prediction errors in the training set.</p><p class="calibre8">Consequently, we can say that:</p><div><ul class="itemizedlist"><li class="listitem">We can improve our linear regression by transforming predictor variables in various ways</li><li class="listitem">We can measure such improvement using the partial R-squared, since every transformation should impact on the quantity of residual errors and ultimately on the coefficient of determination</li></ul></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec43" class="calibre1"/>Discovering interactions</h2></div></div></div><p class="calibre8">One of the<a id="id289" class="calibre1"/> first sources of non-linearity is due to possible interactions between predictors. Two predictors interact when the effect of one of them on the response variable varies in respect of the values of the other predictors.</p><p class="calibre8">In mathematical formulation, interaction terms (the interacting variables) have to be multiplied by themselves for our linear model to catch the supplementary information of their relation as expressed in this example of a model with two interacting predictors:</p><div><img src="img/00046.jpeg" alt="Discovering interactions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">An easy-to-remember example of an interaction in a regression model is the role of engine noise in the evaluation of a car. If you are going to model the preference for car models, you will immediately notice that the engine noise can either decrease or increase consumer preference for the car, depending on the price of the car (or its category, which is a proxy of monetary value). In fact, if the car is inexpensive being silent is clearly a must-have, but if the car is expensive (such as a Ferrari or another sports car) the noise is an outstanding benefit (clearly when in it, you want to have everyone notice the cool car you are driving).</p><p class="calibre8">It may sound a little bit tricky to deal with interaction, but actually it isn't; after all, you are just transforming a variable role in a linear regression based on another one. Finding interaction terms can be achieved in two different ways, the first one being domain knowledge—that is, knowing directly the problem you are modeling and incorporating your expertise in it. When you do not have such an expertise, an automatic search over the possible <a id="id290" class="calibre1"/>combinations will suffice if it is well tested using a revealing measure such as R-squared.</p><p class="calibre8">The best way to illustrate the automatic search approach is to show an example in Python using the <code class="email">PolynomialFeatures</code> from Scikit-learn, a function that allows both interactions and polynomial expansions (we are going to talk about them in the next paragraph):</p><div><pre class="programlisting">
<strong class="calibre2">In: from sklearn.preprocessing import PolynomialFeatures</strong>
<strong class="calibre2">  from sklearn.metrics import r2_score</strong>
<strong class="calibre2">  linear_regression = linear_model.LinearRegression(normalize=False,\fit_intercept=True)</strong>
<strong class="calibre2">  create_interactions = PolynomialFeatures(degree=2, \interaction_only=True, include_bias=False)</strong>
</pre></div><p class="calibre8">By the <code class="email">degree</code> parameter we define how many variables to put into the interaction, it being possible to have three or even more variables interact with each other. Interactions in statistics are called two-way effects, three-way effects, and so on, depending on the number of variables involved (whereas the original variables are instead called the main effects).</p><div><pre class="programlisting">
<strong class="calibre2">In: def r2_est(X,y):</strong>
<strong class="calibre2">          return r2_score(y,linear_regression.fit(X,y).predict(X))</strong>
<strong class="calibre2">baseline = r2_est(X,y)</strong>
<strong class="calibre2">print ('Baseline R2: %0.3f' % baseline)</strong>

<strong class="calibre2">Out: Baseline R2: 0.741</strong>

<strong class="calibre2">in: Xi = create_interactions.fit_transform(X)</strong>
<strong class="calibre2">  main_effects = create_interactions.n_input_features_</strong>
</pre></div><p class="calibre8">After recalling the baseline R-squared value, the code creates a new input data matrix using the <code class="email">fit_transform</code> method, enriching the original data with the interaction effects of all the variables. At this point, we create a series of new linear regression models, each one containing all the main effects plus a single interaction. We measure the improvement, calculate the difference with the baseline, and then report only interactions over a certain threshold. We can decide on a threshold just above zero or a threshold we determine based on a statistical test. In our example we decided on an arbitrary threshold, aiming at reporting all R-squared increment above <code class="email">0.01</code>:</p><div><pre class="programlisting">
<strong class="calibre2">In: for k,effect in \ enumerate(create_interactions.powers_[(main_effects):]):</strong>
<strong class="calibre2">      termA, termB = variables[effect==1]</strong>
<strong class="calibre2">      increment = r2_est(Xi[:,list(range(0,main_effects)) \+[main_effects+k]],y) - baseline</strong>
<strong class="calibre2">      if increment &gt; 0.01:</strong>
<strong class="calibre2">        print ('Adding interaction %8s *%8s R2: %5.3f' %  \</strong>
<strong class="calibre2">          (termA, termB, increment))</strong>
<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00047.jpeg" alt="Discovering interactions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Relevant<a id="id291" class="calibre1"/> interaction effects are clearly made up by the variable <code class="email">'RM'</code> (one of the most important ones, as seen before) and the strongest improvement is given by its interaction with another key feature, <code class="email">LSTAT</code>. An important take away would be that we add it to our original data matrix, as a simple multiplication between the two:</p><div><pre class="programlisting">
<strong class="calibre2">In: Xi = X</strong>
<strong class="calibre2">  Xi['interaction'] = X['RM']*X['LSTAT']</strong>
<strong class="calibre2">  print ('R2 of a model with RM*LSTAT interaction: %0.3f' % \r2_est(Xi,y))</strong>

<strong class="calibre2">Out: R2 of a model with RM*LSTAT interaction: 0.805</strong>
</pre></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec21" class="calibre1"/>Polynomial regression</h1></div></div></div><p class="calibre8">As an extension<a id="id292" class="calibre1"/> of interactions, polynomial expansion systematically provides an automatic means of creating both interactions and non-linear power transformations of the original variables. Power transformations are the bends that the line can take in fitting the response. The higher the degree of power, the more bends are available to fit the curve.</p><p class="calibre8">For instance, if you have a simple linear regression of the form:</p><div><img src="img/00048.jpeg" alt="Polynomial regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">By a second degree <a id="id293" class="calibre1"/>transformation, called <strong class="calibre2">quadratic</strong>, you will get a new form:</p><div><img src="img/00049.jpeg" alt="Polynomial regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">By a third degree<a id="id294" class="calibre1"/> transformation, called <strong class="calibre2">cubic</strong>, your equation will turn into:</p><div><img src="img/00050.jpeg" alt="Polynomial regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">If your regression is a multiple one, the expansion will create additional terms (interactions) increasing the number of new features derived from the expansion. For instance, a multiple regression made up of two predictors (<em class="calibre9">x<sub class="calibre20">1</sub></em> and <em class="calibre9">x<sub class="calibre20">2</sub></em>), expanded using the quadratic transformation, will become:</p><div><img src="img/00051.jpeg" alt="Polynomial regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Before proceeding, we have to note two aspects of the expansion procedure:</p><div><ul class="itemizedlist"><li class="listitem">Polynomial expansion rapidly increases the number of predictors</li><li class="listitem">Higher-degree polynomials translate into high powers of the predictors, posing<a id="id295" class="calibre1"/> problems for numeric stability, thus requiring suitable numeric formats or standardizing numeric values that are too large</li></ul></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch03lvl2sec44" class="calibre1"/>Testing linear versus cubic transformation</h2></div></div></div><p class="calibre8">By setting the <code class="email">interaction_only</code> parameter off in the <code class="email">PolynomialFeatures</code> function seen before, we can get the full polynomial transformation of our input matrix, not just simply the interactions as before:</p><div><pre class="programlisting">
<strong class="calibre2">In: </strong>
<strong class="calibre2">from sklearn.preprocessing import PolynomialFeatures</strong>
<strong class="calibre2">from sklearn.pipeline import make_pipeline</strong>
<strong class="calibre2">from sklearn.preprocessing import StandardScaler</strong>

<strong class="calibre2">linear_regression = linear_model.LinearRegression(normalize=False, \fit_intercept=True)</strong>
<strong class="calibre2">create_cubic = PolynomialFeatures(degree=3, interaction_only=False, \include_bias=False)</strong>
<strong class="calibre2">create_quadratic = PolynomialFeatures(degree=2, interaction_only=False, \</strong>
<strong class="calibre2">include_bias=False)</strong>

<strong class="calibre2">linear_predictor = make_pipeline(linear_regression)</strong>
<strong class="calibre2">quadratic_predictor = make_pipeline(create_quadratic, \</strong>
<strong class="calibre2">linear_regression)</strong>
<strong class="calibre2">cubic_predictor = make_pipeline(create_cubic, linear_regression)</strong>
</pre></div><p class="calibre8">By<a id="id296" class="calibre1"/> sending both <code class="email">PolynomialFeatures</code> and <code class="email">LinearRegression</code> into a pipeline we can create a function <a id="id297" class="calibre1"/>automatically by a single <a id="id298" class="calibre1"/>command, expanding out data and regressing it. As an experiment, we try to model the <code class="email">'LSTAT'</code> variable alone for best clarity, remembering that we could have expanded all the variables at once:</p><div><pre class="programlisting">
<strong class="calibre2">predictor = 'LSTAT'</strong>
<strong class="calibre2">x = dataset['LSTAT'].values.reshape((observations,1))</strong>
<strong class="calibre2">xt = np.arange(0,50,0.1).reshape((50/0.1,1))</strong>
<strong class="calibre2">x_range = [dataset[predictor].min(),dataset[predictor].max()]</strong>
<strong class="calibre2">y_range = [dataset['target'].min(),dataset['target'].max()]</strong>

<strong class="calibre2">scatter = dataset.plot(kind='scatter', x=predictor, y='target', \xlim=x_range, ylim=y_range)</strong>
<strong class="calibre2">regr_line = scatter.plot(xt, linear_predictor.fit(x,y).predict(xt), \'-', color='red', linewidth=2)</strong>
</pre></div><div><img src="img/00052.jpeg" alt="Testing linear versus cubic transformation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Our first fit is the linear one (a simple linear regression) and from the scatterplot we can notice<a id="id299" class="calibre1"/> that the line is not <a id="id300" class="calibre1"/>representing well the<a id="id301" class="calibre1"/> cloud of points relating to <code class="email">'LSTAT'</code> with the response; most likely we need a curve. Instead of testing a second degree transformation that will turn into a parabola, we immediately try a cubic transformation: using two bends, we should obtain a better fit:</p><div><pre class="programlisting">
<strong class="calibre2">scatter = dataset.plot(kind='scatter', x=predictor, y='target', \xlim=x_range, ylim=y_range)</strong>
<strong class="calibre2">regr_line = scatter.plot(xt, cubic_predictor.fit(x,y).predict(xt), \'-', color='red', linewidth=2)</strong>
</pre></div><div><img src="img/00053.jpeg" alt="Testing linear versus cubic transformation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Our graphic check confirms that now we have a more credible representation of how <code class="email">'LSTAT'</code> and <a id="id302" class="calibre1"/>the response<a id="id303" class="calibre1"/> relate. We question <a id="id304" class="calibre1"/>whether we cannot do better using even higher-degree transformations.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch03lvl2sec45" class="calibre1"/>Going for higher-degree solutions</h2></div></div></div><p class="calibre8">To test a higher degree of polynomial transformations, we prepare a script that creates the expansion<a id="id305" class="calibre1"/> and reports its R-squared measure. We then try to plot the function with the highest degree in the series and have a look at how it fits the data points:</p><div><pre class="programlisting">
<strong class="calibre2">In: for d in [1,2,3,5,15]:</strong>
<strong class="calibre2">    create_poly = PolynomialFeatures(degree=d,\interaction_only=False, include_bias=False)</strong>
<strong class="calibre2">    poly = make_pipeline(create_poly, StandardScaler(),\linear_regression)</strong>
<strong class="calibre2">    model = poly.fit(x,y)</strong>
<strong class="calibre2">    </strong>
<strong class="calibre2">print ("R2 degree - %2i polynomial :%0.3f" \%(d,r2_score(y,model.predict(x))))</strong>

<strong class="calibre2">Out: </strong>
</pre></div><div><img src="img/00054.jpeg" alt="Going for higher-degree solutions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Noticeably, there is a huge difference in the coefficient of determination between the linear model and the quadratic expansion (second-degree polynomial). The measure jumps from <code class="email">0.544</code> to <code class="email">0.641</code>, a difference that increases up to <code class="email">0.682</code> when reaching the fifth degree. Preceding to even higher degrees, the increment is not so astonishing, though it keeps on growing, reaching <code class="email">0.695</code> when the degree is the fifteenth. As the latter is the best result in terms of coefficient of determination, having a look at the plot on the data cloud will reveal a not so smooth fit, as we can see with lower degrees of polynomial expansion:</p><div><pre class="programlisting">
<strong class="calibre2">In: scatter = dataset.plot(kind='scatter', x=predictor,\y='target', xlim=x_range, ylim=y_range)</strong>
<strong class="calibre2">  regr_line = scatter.plot(xt, model.predict(xt), '-',\color='red', linewidth=2)</strong>
</pre></div><div><img src="img/00055.jpeg" alt="Going for higher-degree solutions" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Observing <a id="id306" class="calibre1"/>closely the resulting curve, you will surely notice how, by such a high degree, the curve tends strictly to follow the distribution of points, going erratic when the density diminishes at the fringes of the range of the predictors' values.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch03lvl2sec46" class="calibre1"/>Introducing underfitting and overfitting</h2></div></div></div><p class="calibre8">Polynomial regression<a id="id307" class="calibre1"/> offers us the right occasion for starting to<a id="id308" class="calibre1"/> talk about model complexity. We have<a id="id309" class="calibre1"/> not explicitly tested it, but you may already have got the<a id="id310" class="calibre1"/> feeling that, by increasing the degree of the polynomial expansion, you are going to always reap better fits. We say more: the more variables you have in your model, the better, until you will have such a large number of betas, likely equal or almost equal to the number of your observations, that your predictions will be perfect.</p><p class="calibre8">Decaying performances due to over-parameterization (an excess of parameters to be learned by the model) is a problem of linear regression and of many other machine learning algorithms. The more parameters you add, the better the fit because the model will cease to intercept the rules and regularities of your data but will start, in such an embarrassment of riches, to populate the many available coefficients with all the erratic and erroneous information present in the data. In such a situation, the model won't learn general rules but it will just be memorizing the dataset itself in another form.</p><p class="calibre8">This is called <strong class="calibre2">overfitting</strong>: fitting the data at hand so well that the result is far from being an extraction of the form of the data to draw predictions from; the result is just a mere memorization. On the other side, another problem is <strong class="calibre2">underfitting</strong>—that is, when you are using too few parameters for your prediction. The most straightforward example is fitting a non-linear relation using a simple linear regression; clearly, it won't match the curve bends and some of its predictions will be misleading.</p><p class="calibre8">There are appropriate<a id="id311" class="calibre1"/> tools for verifying if you are <a id="id312" class="calibre1"/>underfitting or, more likely, overfitting and they<a id="id313" class="calibre1"/> will be <a id="id314" class="calibre1"/>discussed in <a class="calibre1" title="Chapter 6. Achieving Generalization" href="part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6">Chapter 6</a>, <em class="calibre9">Achieving Generalization</em>, (the chapter explaining data science); in the meantime, don't overfit too much with high-degree polynomials!</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch03lvl1sec22" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we have carried on introducing linear regression, extending our example from a simple to a multiple one. We have revisited the previous outputs from the Statsmodels linear functions (the classical statistical approach) and gradient descent (the data science engine).</p><p class="calibre8">We started experimenting with models by removing selected predictors and evaluating the impact of such a move from the point of view of the R-squared measure. Meanwhile we also discovered reciprocal correlations between predictors and how to render more linear relations between each predictor and the target variable by intercepting the interactions and by means of polynomial expansion of the features.</p><p class="calibre8">In the next chapter, we will progress again and extend the regression model to make it viable for classification tasks, turning it into a probabilistic predictor. The conceptual jump into the world of probability will allow us to complete the range of possible problems where linear models can be successfully applied.</p></div></body></html>