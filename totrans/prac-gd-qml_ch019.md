# 第10章

量子神经网络

*心灵不是要被填满的容器，而是一团需要被点燃的火焰。*

——普鲁塔克

在上一章中，我们探讨了我们的第一个量子机器学习模型家族：量子支持向量机。现在是我们进一步探索另一个模型家族的时候了，那就是**量子** **神经网络**（**QNNs**）。

在本章中，你将学习量子神经网络的概念如何自然地从经典神经网络背后的思想中产生。当然，你还将学习量子神经网络的工作原理以及它们的训练方法。然后，你将探索如何使用我们迄今为止一直在使用的两个量子框架——Qiskit和PennyLane——来实现、运行和训练量子神经网络。

本章内容如下：

+   构建和训练量子神经网络

+   PennyLane中的量子神经网络

+   Qiskit中的量子神经网络：评论

量子支持向量机和量子神经网络可能是QML模型中最受欢迎的两个家族，所以，到本章结束时，你将已经在量子机器学习方面打下坚实的基础。

要开始，让我们了解量子神经网络是如何工作的，以及它们如何被有效地训练。让我们着手吧！

# 10.1 构建和训练一个量子神经网络

就像量子支持向量机一样，量子神经网络是我们之前在*第8章*[*8*](ch017.xhtml#x1-1390008)，“什么是量子机器学习？”中提到的“CQ模型”——即纯粹使用经典输入和输出，并在某个阶段使用量子计算的模型。然而，与QSVMs不同，量子神经网络不是任何经典模型的“特殊情况”，尽管它们的行为受到经典神经网络行为的启发。更重要的是，正如我们很快就会看到的，量子神经网络是“纯粹量子”模型，这意味着它们的执行将仅需要经典计算来准备电路和进行测量统计分析。尽管如此，就像QSVMs一样，量子神经网络将依赖于经典参数，这些参数将通过经典优化进行优化。

了解更多...

如你此刻所知，(量子)机器学习是一个术语含义几乎不唯一的广阔领域。在实践中，“量子神经网络”这个术语可以用来指代任何受经典神经网络行为启发的QML模型。因此，你应该记住，人们也可能使用这个名称来指代与我们考虑的量子神经网络不同的模型。

这应该已经足够作为介绍了。现在让我们深入细节。量子神经网络究竟是什么，它们是如何与经典神经网络相关的？

## 10.1.1 从经典神经网络到量子神经网络的旅程

如果我们进行一次抽象的小练习，我们可以将经典神经网络的动作看作由以下阶段组成：

1.  **数据准备**：这仅仅是将一些（经典）输入数据和可能对其进行的某些（简单）转换。这些可能包括对输入数据进行归一化或缩放。

1.  **数据处理**：通过一系列层将数据传递过去，这些层“转换”数据，随着数据流过它们。这种处理的行为取决于一些可优化的参数，这些参数在训练中会被调整。

1.  **数据输出**：通过最终层返回输出。

让我们看看我们如何可以将这个方案用于定义一个类似的量子模型。

1.  **数据准备**：量子神经网络接收经典输入（以数字数组的形式），但量子计算机并不处理经典数据——它们处理量子状态！那么我们如何将这些经典输入嵌入到量子状态空间中呢？

    这是我们已经在 *第9.2节* 中处理过的问题。为了将QNN的经典输入编码成量子状态，我们只需要使用我们选择的任何特征映射。正如你所知，我们当然可能还需要对数据进行归一化或缩放。

    正是这样，我们实际上为量子神经网络“准备数据”：将其输入到特征映射中。

1.  **数据处理**：在这个阶段，我们已经成功地将我们的经典输入转换成了“量子输入”，即以量子状态的形式编码我们的经典数据，根据某个特征映射。现在，我们需要找出一种方法来处理这个输入，可以从经典神经网络的处理中汲取一些灵感。

    在当前量子硬件的状态下，试图在量子神经网络中复制经典神经网络的完整、精确的行为可能并不理想。相反，我们可以从更大的图景来看。

    在本质上，经典神经网络的处理阶段包括应用一些仅依赖于某些可优化参数的转换。这是一个我们可以非常容易地移植到量子计算机上的想法。我们可以简单地将量子神经网络的“处理”阶段定义为…依赖于某些可优化参数的电路的应用！此外，正如我们将在本节后面看到的那样，这个电路可以被分层结构化，以某种方式重新组合经典神经网络的精髓。这个电路将被称为**变分形式**——它们就像我们在 *第7章* 中研究的那些一样，*VQE：变分量子本征值求解器*。

1.  **数据输出**：一旦我们有一个处理过的状态，我们需要返回一个经典输出。这将是某些测量操作的结果；这个操作可以是适合我们问题的最佳选择！

    例如，如果我们想用量子神经网络构建一个二元分类器，这个测量操作的一个自然选择可能是，例如，在计算基上测量第一个量子比特时的期望值。记住，量子比特的期望值简单地对应于在计算基上测量量子比特获得![1](img/file13.png "1")的概率。

这些就是构成量子神经网络的所有成分。

实际上，特征图和变分形式都是**变分电路**的例子：受某些经典参数控制的量子电路。特征图和变分形式之间的唯一实际区别是它们的目的：特征图依赖于输入数据，用于对其进行编码，而变分形式依赖于可优化参数，用于将量子输入状态进行转换。

这种目的上的差异将体现在我们经常会为特征图和变分形式使用不同的电路。一个好的特征图不一定是一个好的变分形式，反之亦然。

你应该记住——就像所有量子机器学习（QML）的东西一样——“特征图”和“变分形式”这两个术语并不完全通用，不同的作者可能会用不同的表达来指代它们。例如，变分形式通常被称为**ansatzs**，正如我们在*第7章*[*7*](ch015.xhtml#x1-1190007) *VQE: 变分量子本征值求解器*中做的那样。

重要提示

量子神经网络将经典输入 ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}") 通过特征图 ![F](img/file1320.png "F") 映射到量子状态。然后，得到的量子状态通过变分形式 ![V](img/file379.png "V")：一个依赖于某些可优化参数 ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}") 的变分电路。量子神经网络的输出是对最终状态的测量操作的结果。所有这些都可以在以下图中 schematically 看到：

![ n⃗ |FV0⟩((⃗x𝜃))] (img/file1322.jpg)

感谢我们对量子支持向量机的研究，我们已经非常熟悉特征图，但我们还没有熟悉变分形式；这正是我们将致力于下一小节的内容。

## 10.1.2 变分形式

在原则上，变分形式可以是任何你选择的变分电路，但通常，QNN的变分形式遵循“分层结构”，试图模仿经典神经网络的精髓。我们现在可以精确地阐述这个想法。

如果我们要定义一个具有![k](img/file317.png "k")层的变分形式，我们可以考虑![k](img/file317.png "k")个独立参数的向量![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}")。为了定义每一层![j](img/file258.png "j")，我们可能需要一个依赖于参数![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}")的变分电路![G_{j}](img/file1324.png "G_{j}")。一种常见的方法是通过连续堆叠这些变分电路并使用一些电路![U_{}](img/file1326.png "U_{}")来准备变分形式

纠缠，独立于任何参数，旨在在量子比特之间创建纠缠。正如![图10.1](img/file1327.png ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")所示。

![图10.1：一个具有k层的变分形式，每个层由一个依赖于某些参数{\overset{\rightarrow}{\theta}}_{j}的变分电路G_{j}定义。电路U_{}用于创建纠缠，状态\left| \psi_{} \right. enc\rangle表示特征图的输出](img/file1331.jpg)

**图10.1**：一个具有![k](img/file317.png "k")层的变分形式，每个层由一个依赖于某些参数![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}")的变分电路![G_{j}](img/file1324.png "G_{j}")定义。电路![U_{}](img/file1326.png "U_{}")用于创建纠缠，状态![\left| \psi_{} \right.](img/file1329.png "\left| \psi_{} \right.")![\rangle](img/file1330.png "\rangle")表示特征图的输出

我们已经概述了变分形式中最常见的结构之一，但变分形式最好通过例子来说明。变分形式有很多，我们不可能在这本书中收集它们所有——实际上，这样做也没有意义。因此，我们将限制自己只介绍三种变分形式，其中一些我们将在本书的后面部分使用：

+   **双局部**：在![n](img/file244.png "n")个量子比特上重复![k](img/file317.png "k")次的**双局部变分形式**依赖于![n \times (k + 1)](img/file1332.png "n \times (k + 1)")个可优化参数，我们将用![\theta_{rj}](img/file1333.png "\theta_{rj}")表示，其中![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k")和![j = 1,\ldots n](img/file1335.png "j = 1,\ldots n")。其电路的构建按照以下步骤进行：

    **过程** TwoLocal([![n,k,\theta](img/file1336.png "n,k,\theta"))](img/file1336.png "n,k,\theta")

    **对所有** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **执行**

    ![\vartriangleright](img/file655.png "\vartriangleright") * *添加* ![r](img/file1337.png "r")*-层。     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")

    **对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**

    在量子位 ![j](img/file258.png "j") 上应用 ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})") 门。

    -

    ![\vartriangleright](img/file655.png "\vartriangleright") * *在层之间创建纠缠。*     * ![\vartriangleleft](img/file1338.png "\vartriangleleft")

    **如果** ![r < k](img/file1340.png "r < k") **则**

    **对所有** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **执行**

    对控制量子位 ![t](img/file48.png "t") 和目标量子位 ![t + 1](img/file1342.png "t + 1") 应用CNOT门。

    -![-](img/file1343.png "-")

    -![-](img/file1343.png "-")**** ***在*图* [*10.2*](#Figure10.2) 中，我们展示了该过程在 ![n = 4](img/file837.png "n = 4") 和 ![k = 3](img/file1344.png "k = 3") 时的输出。听起来熟悉吗？两个局部变分形式使用与角度编码特征图相同的电路作为其层，然后它依赖于一系列受控-NOT操作来创建纠缠。

    顺便说一下，注意两个局部变分形式重复 ![k](img/file317.png "k") 次时具有 ![k + 1](img/file1345.png "k + 1") 层，而不是 ![k](img/file317.png "k") 层。这个小小的细节有时可能会误导。

    两个局部变分形式非常灵活，它可以与任何测量操作一起使用。

    ![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)

    **Figure 10.2**: Two-local variational form on four qubits and two repetitions***

***   **树张量**：具有 ![k + 1](img/file1345.png "k + 1") 层的**树张量**变分形式可以应用于 ![n = 2^{k}](img/file1347.png "n = 2^{k}") 量子位。每一层的参数数量是前一层的一半，因此变分形式依赖于 ![2^{k} + 2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k - 1} + \cdots + 1") 个可优化参数的形式

    | ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |
    | --- |

    定义该过程的步骤比两个局部变分形式稍微难以理解，其内容如下：

    **过程** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))

    在每个量子位 ![j](img/file258.png "j") 上应用旋转 ![R_{Y}(\theta_{0j})](img/file1351.png "R_{Y}(\theta_{0j})")。

    **对所有** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **执行**

    **对所有** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k - r} - 1") **执行**

    对目标在量子位 ![1 + s2^{r}](img/file1354.png "1 + s2^{r}") 上、由量子位 ![1 + s2^{r} + 2^{r - 1}](img/file1355.png "1 + s2^{r} + 2^{r - 1}") 控制的CNOT操作进行应用。

    在量子比特![1 + s2^{r}](img/file1354.png "1 + s2^{r}")上应用一个![R_{Y}(\theta_{r,s})](img/file1356.png "R_{Y}(\theta_{r,s})")旋转。

    -![img/file1343.png "-"]

    -

    一图胜千言，因此，请参考*图* [*10.3*](#Figure10.3)以了解此过程在![k = 3](img/file1344.png "k = 3")时的输出描述。

    树张量变分形式最适合用于作为二分类器的量子神经网络。可以与之结合使用的最自然的测量操作是计算基下第一个量子比特的期望值。

    作为一种好奇，树张量变分形式的名称来源于用于物理系统模拟的数学对象，也用于一些机器学习模型中。有关模型细节，请参阅Román Orús的综述论文 [[71](ch030.xhtml#Xorus2014practical)]。

    ![图10.3：在8 = 2^{3}量子比特上的树张量变分形式](img/file1358.png)

    **图10.3**：在![8 = 2^{3}](img/file1357.png "8 = 2^{3}")量子比特上的树张量变分形式

    +   **强纠缠层**：强纠缠层变分形式作用于![n](img/file244.png "n")个量子比特，并且可以有任意数量的![k](img/file317.png "k")层。每一层![l](img/file514.png "l")都有一个**范围**![r_{l}](img/file1359.png "r_{l}")。总共，变分形式使用了![3nk](img/file1360.png "3nk")个参数，形式如下

    | ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.") |
    | --- |

    形式由以下算法定义：

    **过程** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))

    **对所有** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **执行**

    **对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**

    在量子比特![j](img/file258.png "j")上应用一个![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")旋转。

    在量子比特![j](img/file258.png "j")上应用一个![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")旋转。

    在量子比特![j](img/file258.png "j")上应用一个![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")旋转。

    -

    **对所有** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **执行**

    在量子比特![j](img/file258.png "j")的控制下应用一个CNOT操作，目标为量子比特![\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1](img/file1367.png "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1")。

    -![img/file1343.png "-"]

    -

    你可以在*图* [*10.4*](#Figure10.4)中找到一个这种形式的样本表示。

    ![图10.4：在四个量子比特上形成强纠缠层，以及两个具有相应范围1和2的层](img/file1368.png)

    **图10.4**：在四个量子比特上形成强纠缠层，以及两个具有相应范围![1](img/file13.png "1")和![2](img/file302.png "2")的层**

**作为最后的评论，我们选择在之前的变分形式示例中主要使用![Y](img/file11.png "Y")旋转是有些任意的。我们也可以使用![X](img/file9.png "X")旋转，例如。同样，我们选择在纠缠电路中使用受控-![X](img/file9.png "X")操作也是任意的。我们也可以使用不同的受控操作，例如。此外，在两局部变分形式中，在纠缠电路中门的分布还有更多选择，而不仅仅是我们所考虑的那种。我们的纠缠电路被认为具有“线性”的门排列，但其他可能性在*图10.5*中有所展示。

![（a）线性](img/file1369.png)

**(a)** 线性

![（b）圆形](img/file1370.jpg)

**(b)** 圆形

![（c）完整](img/file1371.jpg)

**(c)** 完整

**图10.5**：不同的纠缠电路

这就是我们目前需要了解的所有关于变分形式的内容。结合我们之前对特征图的了解，这结束了我们对量子神经网络元素的解析……几乎。我们仍然需要深入探究每个量子神经网络末尾看似无辜的测量操作。

## 10.1.3 关于测量的说明

如我们在*第7章*[*7*](ch015.xhtml#x1-1190007)中看到的，“VQE：变分量子本征值求解器”，任何物理可观测量都可以通过一个厄米算符来表示，使得所有可能的测量结果都可以与算符的不同本征值相对应。如果你还不熟悉这一点，请查看*第7.1.1节*[*7.1.1*](ch015.xhtml#x1-1210007.1.1)。

当我们在计算基下测量单个量子比特时，与相关厄米算符的计算基坐标矩阵可能是以下之一

| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.") |
| --- |

这两个算符都表示对量子比特的测量，但它们在关联不同输出时的本征值上有所不同。第一个算符将本征值![1](img/file13.png "1")和![0](img/file12.png "0")分别关联到量子比特的值为![0](img/file12.png "0")和![1](img/file13.png "1")，而第二个可观测量将本征值![1](img/file13.png "1")和![- 1](img/file312.png "- 1")关联到这些结果。

练习10.1

本练习的目的是让你更熟悉狄拉克符号。证明前两个厄米算符可以分别写成

| ![1\left | 0 \right\rangle\left\langle 0 \right | + 0\left | 1 \right\rangle\left\langle 1 \right | = \left | 1 \right\rangle\left\langle 1 \right | ,\quad\left | 0 \right\rangle\left\langle 0 \right | - \left | 1 \right\rangle\left\langle 1 \right | .](img/file1373.png "1\left | 0 \right\rangle\left\langle 0 \right | + 0\left | 1 \right\rangle\left\langle 1 \right | = \left | 1 \right\rangle\left\langle 1 \right | ,\quad\left | 0 \right\rangle\left\langle 0 \right | - \left | 1 \right\rangle\left\langle 1 \right | .") |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |

*提示*：记住，基（列向量）和提（行向量）的乘积是一个矩阵。我们之前在 *第7.2.1节* 中看到了一个例子。

*正如我们将在本章后面看到的那样，框架如PennyLane允许你使用由任何厄米算子定义的测量操作。这可以在定义神经网络的测量操作时给你带来很多灵活性。例如，在一个 ![n](img/file244.png "n")-量子位电路中，你将能够指示PennyLane计算可观测量 ![M \otimes \cdots \otimes M](img/file1374.png "M \otimes \cdots \otimes M") 的期望值，其在计算基的坐标表示是矩阵

| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  & & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |
| --- |

或者，你可能想考虑可观测量 ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes \cdots \otimes Z")。很容易看出，如果测量到偶数个量子位为 ![0](img/file12.png "0")，则该可观测量将返回 ![+ 1](img/file1377.png "+ 1")，否则返回 ![- 1](img/file312.png "- 1")。这就是为什么 ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes \cdots \otimes Z") 被称为**偶数性**可观测量的原因。

当然，你还可以将测量操作视为第一个量子位的经典期望值。但是，重点是，如果你愿意探索，还有很多其他选项可供选择！

如我们之前提到的，可观测量是每个量子神经网络架构的最终构建块。量子神经网络接受一个输入，这通常由通过特征图输入的经典数据组成。然后，通过一个变分形式将得到的量子状态转换，最后通过测量操作获得一些（经典）数值数据。这样，我们就得到了一个将一些数值输入转换为输出的“黑盒”，即一个模型——就像任何其他经典机器学习模型一样——可以被训练。

我们现在已经定义了量子神经网络是什么，并且学习了如何构建它们，至少在理论上是如此。这意味着我们有一个模型。但是，这是量子机器学习，所以一个模型是不够的：我们需要对其进行训练。为此，我们将需要，包括但不限于，一个优化算法。

## 10.1.4 梯度计算和参数平移规则

虽然这不是唯一的选择，但我们将为量子神经网络使用的优化算法将是梯度下降算法；特别是，我们将使用Adam优化器。但是，正如我们在*第8章*[*8*](ch017.xhtml#x1-1390008)*,* *什么是量子机器学习？*中看到的，这个算法需要获得损失函数期望值的梯度，相对于可优化参数而言。

由于我们的模型使用量子电路，这些梯度的计算并不完全简单。我们现在简要地回顾一下三种主要的微分方法，在这些方法中可以执行这些梯度计算：

+   **数值逼近**：当然，我们有一个总是有效的方法。它可能不是最有效的方法，但总是存在的。为了计算梯度，我们可能只需要进行数值估计。为了做到这一点，当然，我们不得不多次运行我们的量子神经网络。

    为了稍微举例说明这一点，如果我们有一个接受![n](img/file244.png "n")个实数输入的实值函数 ![\left. f:R^{n}\rightarrow R \right.](img/file1378.png "\left. f:R^{n}\rightarrow R \right.")，我们可以近似其偏导数如下

    | ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |
    | --- |

    对于![h](img/file519.png "h")足够小的值。当然，这是数值逼近导数最天真的一种方法，但希望这足以让你对它是如何工作的有一个直观的理解。

+   **自动微分**：鉴于当前真实量子硬件的状态，你将训练的大多数量子神经网络很可能会在模拟器上运行。尽管这可能并不理想，但它带来了一些优势。最值得注意的是，在模拟量子神经网络上，经典计算机可以使用类似于在经典神经网络中使用的技巧来计算精确的梯度。如果你感兴趣，Aurélien Géron的书籍 [[104](ch030.xhtml#Xhandsonml)，第10章] 和Shai Shalev-Shwartz以及Shai Ben-David的书籍 [[105](ch030.xhtml#Xunderml)，第20.6节] 讨论了这些经典神经网络的技巧。

+   **参数平移规则**：标准的自动微分技术只能在模拟器上使用。幸运的是，当在真实硬件上执行量子神经网络时，仍然有另一种方法来计算梯度：使用**参数平移规则**。正如其名所示，这项技术使我们能够通过在量子神经网络中使用相同的电路，同时改变可优化参数的值来计算梯度。参数平移规则并不总是适用，但它适用于许多常见情况，并且可以与其他技术结合使用，例如数值近似。

    我们不会深入探讨这种方法是如何工作的细节，但你可以查看Maria Schuld和其他人发表的研究论文[[109](ch030.xhtml#Xpshift-schuld)]以获取更多信息。例如，如果你有一个由单个旋转门![R_{X}(\theta)](img/file1380.png "R_{X}(\theta)")和其期望值![E(\theta)](img/file1381.png "E(\theta)")的测量组成的电路，你将能够计算其对![\theta](img/file89.png "\theta")的导数，如下所示：

    | ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}} \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}} \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |
    | --- |

    这与某些三角函数发生的情况类似：例如，你可以用相同正弦函数的平移值来表达正弦函数的导数。

    对于我们的目的来说，了解它存在并且可以被使用就足够了。当然，参数平移规则也可以用于模拟器！

重要提示

当量子神经网络在模拟器上运行时，可以使用类似于经典机器学习的自动微分技术来计算梯度。当它们在真实硬件或模拟器上运行时，这些梯度也可以计算——至少在许多情况下可以——使用参数平移规则。

或者，数值近似始终是计算梯度的一种有效方法。

正如我们提到的，所有这些方法已经在PennyLane中完全实现，我们将在下一节尝试所有这些方法。

要了解更多…

一切看起来都很好，很有希望，但量子神经网络在训练时也带来了一些挑战。最值得注意的是，它们已知容易受到**贫瘠平原**的影响：训练梯度消失的情况，因此训练无法再进行（参见McClean等人撰写的论文以获取进一步解释[[67](ch030.xhtml#Xmcclean2018barren)]）。还知道所使用的测量操作类型和QNN的深度会影响这些贫瘠平原出现的可能性。例如，在Cerezo及其合作者的论文中对此进行了研究[[24](ch030.xhtml#Xcerezo2021cost)]。无论如何，你在训练你的QNN时应该保持警惕，如果贫瘠平原威胁到你的模型的学习，应遵循文献中的可能解决方案。

我们现在已经拥有了构建和训练量子神经网络所需的所有成分。但在我们实际操作之前，我们将讨论一些技术和技巧，这将帮助你最大限度地发挥我们全新的量子机器学习模型的优势。

## 10.1.5 量子神经网络的实际应用

以下是在设计QNN模型和训练它们时你应该记住的一些想法。你可以将其视为前几节的总结，其中包含来自*第8章*[*8*](ch017.xhtml#x1-1390008)*，什么是量子*机器学习？*的一些亮点：

+   **明智的选择**：当你开始设计一个QNN时，你必须做出三个重要的决定：你必须选择一个特征图、一个变分形式和一个测量操作。对这些选择要有意为之，并考虑你正在处理的问题和数据。你的决定可能会影响你找到贫瘠平原的可能性。一个好的建议是检查文献中类似的问题，并在此基础上构建。

+   **大小很重要**：当你使用一个设计良好的变分形式时，通常，所得到的量子神经网络的力量将直接与其拥有的可优化参数数量相关。使用过多的参数，你可能会得到一个过拟合的模型。使用非常少的参数，你的模型最终可能欠拟合。

+   **优化优化**：对于大多数问题，Adam优化器可以是你训练量子神经网络的默认选择。记住，正如我们在*第8章*[*8*](ch017.xhtml#x1-1390008)*，什么是量子*机器学习？*中讨论的那样，当你使用Adam时，你必须选择一个学习率和批量大小。

    较小的学习率会使算法更准确，但也会更慢。类似地，较大的批量大小应该会使优化更有效，但会损害执行时间。

+   **正确喂养你的QNN**：提供给量子神经网络的应该根据所使用的特征图的要求进行归一化。此外，根据输入数据的维度，你可能想要依赖降维技术。

    当然，你拥有的数据越多，越好。不过，你可能还想考虑的一个额外事实是，在某些条件下，量子神经网络已被证明在成功训练时需要比经典神经网络更少的数据样本 [[112](ch030.xhtml#Xqnn-lowdata)]。

要了解更多...

如果你想要进一步增强你的量子神经网络的能力，你可能需要考虑**数据重上传**技术 [[110](ch030.xhtml#Xperez2020data)]。在传统的QNN中，你有一个依赖于某些输入数据![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")的特征图![F](img/file1320.png "F")，然后是依赖于某些可优化参数![\overset{\rightarrow}{\theta_{0}}](img/file1383.png "\overset{\rightarrow}{\theta_{0}}")的变分形式![V](img/file379.png "V")。数据重上传简单地说就是重复这个方案——任何你想要的次数——在执行QNN的测量操作之前。特征图在每个重复中使用相同的输入数据，但每个变分形式的实例都采用它自己的、独立的、可优化的参数。

这在以下图表中表示，它显示了带有![k](img/file317.png "k")次重复的数据重上传：

![|FVFV0⟩((((n⃗x⃗𝜃⃗x⃗𝜃)1)k)) ... ](img/file1384.jpg)

这已经在实践和理论 [[113](ch030.xhtml#Xdatare-schuld)] 中被证明，与更简单、标准的方法相比，它提供了一些优势，但代价是增加了所使用电路的深度。无论如何，在实现你自己的QNN时，将其牢记在心是好的。

这结束了我们对量子神经网络的纯理论讨论。现在是我们动手实现所有我们讨论过的花哨的元素和技术的时候了。在这方面，我们将主要关注PennyLane。让我们开始吧！

# 10.2 PennyLane中的量子神经网络

现在，我们已准备好使用PennyLane实现和训练我们的第一个量子神经网络。PennyLane框架非常适合许多应用，但在实现量子神经网络模型方面最为出色。这都归功于其灵活性和与经典机器学习框架的良好集成。我们将特别使用PennyLane与TensorFlow结合来训练一个基于QNN的二分类器。我们在*第8章*[*8*](ch017.xhtml#x1-1390008)*，什么是量子机器学习？*中投入的所有努力，最终将得到回报！

重要提示

记住，我们正在使用TensorFlow包的**版本2.9.1**和PennyLane的**版本0.26**。

让我们从导入PennyLane、NumPy和TensorFlow以及为这些包设置一些种子开始，以确保我们的结果是可重复的。我们可以通过以下代码片段实现这一点：

[PRE0]

请记住，如果你使用不同的包版本，你可能会得到与我们略微不同的结果。然而，你获得的结果将在你自己的机器上完全可重复。

在我们面对我们的问题之前，还有一个最后的细节需要我们解决。PennyLane使用双精度浮点数，而TensorFlow使用普通的浮点数。这并不总是问题，但最好让TensorFlow像PennyLane一样使用双精度浮点数。我们可以通过以下方式实现这一点：

[PRE1]

在这个问题解决完毕之后，让我们来面对我们的问题。

## 10.2.1 为QNN准备数据

正如我们已经提到的，我们将训练一个QNN模型来实现二元分类器。我们反复使用二元分类器并不是巧合，因为二元分类器可能是训练起来最简单的机器学习模型。然而，在本书的后面，我们将探索更多令人兴奋的使用案例和架构。

对于我们的示例问题，我们将使用scikit-learn包提供的玩具数据集之一：“威斯康星乳腺癌数据集” [[32](ch030.xhtml#XDua:2019)]。这个数据集总共有![569](img/file1385.png "569")个样本，每个样本有![30](img/file620.png "30")个数值变量。这些变量描述了可以用来表征乳腺肿块是良性还是恶性的特征。每个样本的标签可以是![0](img/file12.png "0")或![1](img/file13.png "1")，分别对应恶性和良性。你可以在网上找到这个数据集的文档[https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)（数据集的原始文档也可以在[https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))找到）。

我们可以通过从`sklearn.datasets`调用`load_breast_cancer`函数来获取这个数据集，将可选参数`return_X_y`设置为true，以便除了样本外还可以检索标签。为此，我们可以使用以下说明：

[PRE2]

当我们训练QSVM时，因为我们不打算在模型之间进行比较，所以一个训练集和测试集就足够了。然而，在我们的情况下，我们将使用验证损失提前停止来训练我们的模型。这意味着——如果你不记得的话——我们将跟踪验证损失，并且一旦它不再改善——根据我们将定义的一些标准，我们将停止训练。更重要的是，我们将保留最佳模型配置，以最小化验证损失。使用测试集来达到这个目的并不是一个好的实践，因为这样测试集就会在训练中发挥作用，它不会给出真实错误的良好估计；这就是为什么我们需要一个单独的验证数据集。

我们可以将我们的数据集分为训练集、验证集和测试集，如下所示：

[PRE3]

数据集中的所有变量都不是零，但它们没有归一化。为了使用它们与我们的任何特征映射，我们应该使用`MaxAbsScaler`将训练数据归一化到![0](img/file12.png "0")和![1](img/file13.png "1")之间，如下所示：

[PRE4]

然后，我们将测试和验证数据集按照训练数据集的比例进行归一化：

[PRE5]

就像我们在上一章训练QSVM时所做的那样！

到目前为止，我们只是进行了一些相当标准的预处理，而不必过多考虑我们未来量子神经网络的实际架构。但现在情况改变了。我们有一个问题要解决：我们的数据集有![30](img/file620.png "30")个变量，这对于当前的量子硬件来说可能是一个相当大的数字。由于我们没有访问到![30](img/file620.png "30")量子比特的量子计算机，我们可能考虑以下选择：

+   在![5](img/file296.png "5")个量子比特上使用幅度编码特征映射，它可以容纳多达![2^{5} = 32](img/file1386.png "2^{5} = 32")个变量

+   使用我们之前使用过的任何其他特征映射，但与降维技术结合使用

我们将选择后者。你可以自己尝试其他可能性：如果你使用我们在*第* [*9*](ch018.xhtml#x1-1600009)*章中学习的`qml`的`AmplitudeEmbedding`模板，它相当直接。

练习10.2

在你跟随本节内容的同时，尝试使用五个量子比特的幅度编码实现一个QNN。

请记住，当通过特征参数将数据喂给`qml`的`AmplitudeEmbedding`对象时，而不是使用`inputs`变量，你应该使用`[``a` `for` `a` `in` `inputs``]`。这是因为PennyLane需要执行一些内部类型转换。

在模拟器上训练量子神经网络是一个计算密集型任务。我们不希望任何人的电脑崩溃，所以，为了确保每个人都能顺利运行这个示例，我们将限制自己使用![4](img/file143.png "4")-量子比特电路。因此，我们将使用降维技术将变量的数量减少到![4](img/file143.png "4")，然后设置一个具有特征映射的QNN，该映射将接受![4](img/file143.png "4")个输入变量。

正如我们在上一章中所做的那样，我们将使用主成分分析来减少数据集中变量的数量到![4](img/file143.png "4")：

[PRE6]

现在我们已经完全准备好了数据，我们需要选择我们的量子神经网络将如何工作。这正是下一小节的重点。

## 10.2.2 构建网络

对于我们的情况，我们将选择 ZZ 特征图和双局部变分形式。这两个都不是内置在 PennyLane 中的，因此我们必须提供我们自己的变分电路实现。然而，PennyLane 包含了一个具有环形纠缠的双局部形式版本 (`qml``.``BasicEntanglerLayers`)，以防你想要在 QNNs 中使用它。为了实现我们需要的电路，我们只需使用我们在 *第 * *10.1.2* *节中提供的伪代码，并执行以下操作：*

*[PRE7]

记得我们在上一章中已经在 PennyLane 中实现了 ZZ 特征图。

在本章中，我们讨论了可观测量，以及这些在量子力学中如何由厄米算子表示。PennyLane 允许我们直接使用这些厄米表示。

记得在 PennyLane 中每个电路都返回某些测量操作的结果吗？例如，你可以在电路定义的末尾使用 `return` `qml``.``probs``(``wires` `=` `[0])` 来获取计算基上每个可能测量结果的概率。嗯，结果是 PennyLane 还提供了一些其他可能性。例如，给定任何厄米矩阵 ![A](img/file183.png "A")（编码为 numpy 数组 `A`），我们只需在电路末尾调用 `return` `qml``.``expval``(``A``,` `wires` `=` `w``)` 就可以检索 `w` 上 `A` 的期望值。当然，![A](img/file183.png "A") 的维度必须与 `w` 的长度兼容。这在我们的情况下很有用，因为为了获取第一个量子比特的期望值，我们只需计算厄米矩阵的期望值

| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |
| --- |

矩阵 ![M](img/file704.png "M") 可以按照以下方式构建：

[PRE8]

在这个构建中，我们使用了之前在本章中讨论的事实，即 ![M = \left| 0 \right\rangle\left\langle 0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|")，这将给我们一个介于 ![0](img/file12.png "0") 和 ![1](img/file13.png "1") 之间的输出值，这对于构建分类器非常完美：像往常一样，我们将值大于等于 ![0.5](img/file1166.png "0.5") 的每个数据实例分配为类别 ![1](img/file13.png "1")，而将所有其他值分配为类别 ![0](img/file12.png "0")。

现在我们已经收集了所有必要的组件来实现我们的量子神经网络。我们将将其构建为一个具有两个参数的量子节点：`inputs` 和 `theta`。第一个参数是强制性的：为了让 PennyLane 能够使用 TensorFlow 训练量子神经网络，它的第一个参数必须接受一个包含网络所有输入的数组，并且这个参数的名称必须是 `inputs`。在这个参数之后，我们可以添加尽可能多的参数。这些可以对应于电路的任何参数，当然，它们需要包括变分形式中的可优化参数。

因此，我们可以这样实现我们的量子神经网络：

[PRE9]

为了保持简单，我们选择只使用一次变分形式的重叠。如果你的数据集更复杂，你可能需要增加这个数字，以便有更多的可训练参数。

顺便说一下，注意我们是如何在量子节点初始化器中添加了 `interface` `=` `"``tf``"` 参数的。这样做是为了让量子节点能够使用张量（TensorFlow 的数据对象）而不是数组来工作，以便 PennyLane 能够与 TensorFlow 平滑通信。如果我们使用了 `@qml``.``qnode` 装饰器，我们就需要在它的调用中包含这个参数。

这定义了实现我们的量子神经网络的量子节点。现在我们需要找出一种方法来训练它，为此我们将依赖 TensorFlow。我们将在下一小节中这样做。

## 10.2.3 使用 TensorFlow 与 PennyLane

在 *第* [*8*](ch017.xhtml#x1-1390008)*章“什么是量子机器学习？”中，我们已经学习了如何使用 TensorFlow 训练经典神经网络。好吧，多亏了 PennyLane 的出色互操作性，我们现在几乎可以像训练经典神经网络一样训练我们的量子神经网络。

要了解更多…

PennyLane 还可以与其他经典机器学习框架集成，例如 PyTorch。此外，它还提供了基于 NumPy 包的模型训练工具，但这些功能更为有限。

记得我们是如何使用 Keras 层构建 TensorFlow 模型并将它们组合成顺序模型吗？看看这个例子：

[PRE10]

这就是如何创建一个包含我们的量子神经网络的 Keras 层——就像它是一个经典模型中的任何其他层一样！为了做到这一点，我们不得不调用 `qml``.``qnn``.``KerasLayer`，并且我们必须向它传递一些东西。首先，当然，我们发送了包含神经网络的量子节点。然后，一个字典通过所有接受可优化参数的节点参数名称索引，并为每个这些参数指定了它们接受的参数数量。由于我们只有一个这样的参数，即 `theta`，并且它应该包含 ![8](img/file506.png "8") 个可优化参数（即，它将是一个长度为 ![8](img/file506.png "8") 的数组），所以我们发送了 `{``"``theta``:`

`8}`。最后，我们不得不指定量子节点的输出维度；因为它只返回一个数值期望值，所以这个维度是![1](img/file13.png "1")。

一旦我们有了量子层，我们就可以轻松地创建一个Keras模型：

[PRE11]

能够以这种程度的灵活性将量子节点集成到神经网络中，将使我们能够轻松地在下一章构建更复杂的模型架构。

在我们的模型准备就绪后，我们现在必须选择一个优化器和损失函数，然后我们可以像任何经典模型一样编译模型。在我们的情况下，我们将使用二元交叉熵损失（因为我们毕竟在训练一个二元分类器）并依赖于学习率为![0.005](img/file1389.png "0.005")的Adam优化器。对于优化器的其余参数，我们将信任默认值。因此，我们的代码如下：

[PRE12]

此外，我们将使用以下指令在验证损失上使用早停机制，耐心设置为两个epoch：

[PRE13]

现在我们已经准备好发送最终指令来训练我们的模型。

要了解更多...

你可能记得，在本章的某个时候，我们讨论了涉及量子神经网络的梯度可以计算的不同方式。你可能会想知道为什么我们不需要处理这些来训练我们的模型。

结果表明，PennyLane已经为我们选择了最佳微分方法来计算梯度。每个量子节点可以使用某些微分方法——例如，充当真实硬件接口的设备上的节点不能使用自动微分方法，但具有模拟器的节点可以，而且大多数都可以。

在本节的后面，我们将详细讨论在PennyLane中可以使用的所有微分方法。

要训练我们的模型，我们只需调用`fit`方法。由于我们将使用早停机制，我们将慷慨地设置epoch的数量，将其设置为![50](img/file1390.png "50")。此外，我们将设置批大小为![20](img/file588.png "20")。为此，我们可以使用以下代码片段：

[PRE14]

执行此指令后，你将得到以下类似的结果：

[PRE15]

要了解更多...

如果你到目前为止一直按照我们的步骤进行，而没有要求TensorFlow使用双精度浮点数，那么一切都会正常工作——尽管你可能会得到略微不同的结果。然而，如果你尝试使用Lightning模拟器拟合模型，你确实需要要求TensorFlow使用双精度浮点数。

注意，我们已手动缩小进度条，以便输出能够适应页面宽度。此外，请记住，执行时间可能会因设备而异，但总体而言，训练在平均设备上不应超过![20](img/file588.png "20")分钟。

computer.

只需查看原始输出，我们就可以看出模型确实在学习，因为随着训练的进行，训练和验证损失都有非常显著的下降。可以争辩说可能存在一点过拟合，因为训练损失的下降略大于验证损失。无论如何，让我们在查看准确率之前，不要得出任何最终结论。

在这种情况下，训练只进行了![16](img/file619.png "16")个epoch，因此很容易从TensorFlow返回的输出中获得洞察。然而，在现实世界中，训练过程可能持续进行到非常大的epoch数量，不用说，在这些情况下，控制台输出并不特别具有信息量。一般来说，始终绘制训练和验证损失与epoch数量的对比图，以便更好地了解训练过程的性能。我们可以使用以下指令来完成：

[PRE16]

我们决定定义一个函数，以便我们可以在未来的训练过程中重用它。生成的图表显示在*图* [*10.6*](#Figure10.6)中。

![图10.6：每个epoch的训练和验证损失函数](img/file1391.png)

**图10.6**：每个epoch的训练和验证损失函数

现在是我们最终测试的时候了。让我们检查我们的模型在所有数据集上的准确率，看看其性能是否可接受。这可以通过以下代码片段来完成：

[PRE17]

运行此代码后，我们得到训练准确率为![71%](img/file1392.png "71%")，验证准确率为![72%](img/file1393.png "72%")，测试准确率为![72%](img/file1393.png "72%")。这些结果并不反映任何过拟合的情况。

而不是实现自己的变分形式，你可能更愿意使用PennyLane内置的电路之一。例如，你可以使用`StronglyEntanglingLayers`类。然而，你应该记住，由此产生的变分形式——与我们的两局部实现相比——不会接受一维输入数组，而是一个三维数组！特别是，这个形式在![n](img/file244.png "n")个量子比特和![l](img/file514.png "l")层的情况下将接受一个大小为![n \times l \times 3](img/file1394.png "n \times l \times 3")的三维数组作为输入。记得在这个变分形式中，我们需要![3](img/file472.png "3")个参数来旋转门，每个![l](img/file514.png "l")层中有![n](img/file244.png "n")个这样的门（你可以再次查看*图* * [*10.4*](#Figure10.4))）。

*如果你有任何疑问，你可以调用`StronglyEntanglingLayers.shape`函数，指定层数和量子比特数，分别在`n_layers`和`n_wires`参数中。这将返回一个三个元素的元组，表示变分形式期望的形状。

例如，我们可以重新定义我们之前的QNN，使其使用以下变分形式：

[PRE18]

在这段代码中，我们将我们想要在每个变分形式实例中重复的次数存储在`nreps`中，将变分形式期望的输入维度存储在`weights_dim`中，并将每个变分形式实例将接受的输入数量存储在`nweights`中。其余部分相当直观。在电路内部，我们必须将参数的`theta`数组重塑以适应变分形式期望的形状；为了做到这一点，我们使用了`tf.reshape`函数，该函数可以在保留所有元数据的同时重塑TensorFlow的张量。我们定义在最后的`weights_strong`字典是我们构建Keras层时将发送给TensorFlow的字典。

我们已经学习了如何使用PennyLane和TensorFlow训练量子神经网络。在结束本节之前，我们将深入讨论一些技术细节。

## 10.2.4 PennyLane中的梯度计算

如我们之前提到的，当你使用PennyLane训练模型时，框架本身会找出计算梯度的最佳方式。不同的量子节点可能基于各种因素与不同的微分方法兼容，最显著的是它们使用的设备类型。

要了解更多...

要查看`default` `qubit`模拟器支持的微分方法的最新参考，你可以查看在线文档[https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations)。

你会发现量子节点与微分方法的兼容性不仅取决于设备本身，还取决于节点的返回类型和机器学习接口（在我们的案例中，接口是TensorFlow）。

这些是可以在PennyLane中使用的微分方法：

+   **反向传播**：这只是经典神经网络中使用的良好旧的反向传播方法。当然，这种微分方法仅在兼容自动微分的模拟器上工作，因为这是分析计算梯度的必要条件。

    在PennyLane中，这种方法的名字是`"backprop"`。

+   **伴随微分**：这是依赖量子计算的一些计算“怪异”特性的更有效版本的反向传播，例如，所有量子电路都是由酉矩阵实现的，这些矩阵可以简单地求逆。像反向传播一样，这种方法仅在兼容自动微分的模拟器上工作，但它更为限制性。

    在PennyLane中，这种方法的名字是`"adjoint"`。

+   **有限差分法**：你在大学里上过数值分析课程吗？那么这听起来会很熟悉。这种方法实现了我们在上一节中讨论的旧式计算梯度数值近似的方法。它几乎适用于每个量子节点。

    在PennyLane中，这种方法的名字是 `"``finite``-``diff``"`.

+   **参数平移规则**：PennyLane完全实现了我们之前介绍过的参数平移规则。它适用于大多数量子节点。

    在PennyLane中，这种方法的名字是 `"``parameter``-``shift``"`.

+   **设备梯度计算**：一些设备提供了自己的计算梯度的方法。相应的微分方法名为 `"``device``"`.

有几件事情需要澄清；其中之一是模拟器如何与自动微分不兼容。稍微简化一下，大多数模拟器通过计算电路的量子态的演化并返回一个相对于参数可微的输出来工作。执行所有这些操作所需的操作本身是可微的，因此可以在使用该模拟器的量子节点上使用自动微分。但是模拟器可能工作方式不同。例如，一个模拟器可能会以“破坏”计算可微性的方式返回单个射击。

另一件可能引起你注意的事情是，有限差分法可以用于“大多数”量子节点，但不是所有。这是因为一些量子节点可能返回的输出使得有限差分法无法与它们一起工作。例如，如果一个节点返回一个样本数组，可微性就会中断。相反，如果它返回一个期望值——即使它只是从样本集合中获得的经验近似——那么就会存在梯度，并且可以使用有限差分法来计算它。

练习10.3

列出所有可以在量子硬件上使用的PennyLane微分方法以及所有可以在模拟器上使用的微分方法。

你可以通过将可选参数 `diff_method` `=` `"``method``"` 传递给量子节点装饰器或初始化器来请求PennyLane使用特定的微分方法——比如说一个名为 `"``method``"` 的方法。也就是说，如果你使用QNode装饰器，你应该写

[PRE19]

或者，如果你决定直接将电路 `circuit` 和设备 `device` 组装成一个量子节点，你应该调用以下操作：

[PRE20]

默认情况下，`diff_method` 被设置为 `"``best``"`，正如我们之前所说的，这会让PennyLane代表我们选择最佳微分方法。

在我们特定的案例中，PennyLane一直使用反向传播微分法，而我们甚至没有注意到这一点！

要了解更多...

如果你想知道PennyLane在设备`dev`和特定接口`inter`（在我们的案例中，`"tensorflow"`）上默认使用的微分方法，你可以直接调用以下函数：

[PRE21]

我们的量子节点与所有微分方法兼容，除了设备微分，因为`default` `qubit`没有实现自己的特殊计算梯度方式。因此，为了更好地理解性能差异，我们可以尝试所有微分方法并观察它们的运行情况。

要了解更多信息...

你可能记得，当使用Lightning模拟器时，我们确实需要要求TensorFlow在整个Keras模型中使用双精度浮点数而不是单精度浮点数——这不是一个选项，而是一个必需品。当我们使用除反向传播之外的微分方法与`default` `qubit`时，情况也是如此。

让我们从伴随微分开始。为了使用这种微分方法重新训练我们的模型，我们将重新运行所有之前的代码，但将量子节点定义更改为以下内容：

[PRE22]

足够合理的是，你可能会想在不重新运行所有代码的情况下，将替代微分方法的执行作为其中的一部分——尤其是如果你将代码保存在笔记本中。如果你想确保在相同条件下（相同的环境和种子）完成训练，以下是你必须运行的这些行：

[PRE23]

运行此操作后，你将获得与反向传播相同的精确训练行为——相同的训练和验证损失演变，当然，还有相同的准确率。然而，值得注意的是，训练时间有所不同。在我们的案例中，使用反向传播进行训练的平均时间大约为![21](img/file1395.png "21")秒每轮。相比之下，使用伴随微分，平均每轮训练时间为![10](img/file161.png "10")秒。这是一个巨大的进步！

实际上，如果你想进一步减少训练时间，你应该尝试使用伴随方法与Lightning模拟器。根据你电脑的硬件配置，它可以在性能上带来非常显著的提升。

现在，让我们使用剩下的两种微分方法来训练我们的模型，这两种方法是硬件兼容的：参数移位规则和有限差分。为了做到这一点，我们只需重新运行我们的代码，更改量子节点定义中的微分方法值。为了避免冗余，我们不会在这里重写所有内容——我们相信这些小的变化可以由你来完成！

当使用这两种模型重新训练时，我们得到了以下结果：

+   使用参数平移规则得到了与其他微分方法相同的结果。关于训练时间，每个epoch平均需要![14](img/file1396.png "14")秒来完成。这比我们用反向传播得到的![21](img/file1395.png "21")秒要好，但不如adjoint方法给出的![10](img/file161.png "10")秒好。

+   当使用有限差分微分时，我们再次得到了与其他方法相同的结果。平均而言，每个epoch需要![10](img/file161.png "10")秒来完成，这与adjoint微分法的训练时间相匹配。

请记住，这个比较仅适用于我们考虑的特定模型。随着模型复杂性的增加，结果可能会有所不同，特别是与硬件兼容的方法在训练复杂的QNN架构时可能在模拟器上表现得更差。

以下是您需要了解的关于PennyLane中可用的微分方法的全部内容。现在让我们看看Qiskit在量子神经网络方面能提供什么。

# 10.3 Qiskit中的量子神经网络：评论

在上一节中，我们有机会深入探讨PennyLane中量子神经网络的实现和训练。我们不会以如此详细的程度对Qiskit进行类似的讨论，但至少会给你一些关于如何开始使用Qiskit来处理量子神经网络的想法。

PennyLane提供了一个非常统一和灵活的体验。无论您是在训练一个简单的二分类器，还是像我们在下一章将要研究的那样复杂的混合架构，都是用同样的方式完成的。

相比之下，Qiskit提供了一种更“结构化”的方法。它提供了一系列可以用来训练不同类型神经网络的类，并允许您以不同的方式定义您的网络。很难判断这是否是一种更好或更差的方法；最终，这只是个人口味的问题。一方面，由于一些专为特定目的设计的类易于使用，在Qiskit中训练基本模型可能比在PennyLane中训练它们要简单。另一方面，有不同方式完成同一件事——有人可能会说——可能会产生一些不必要的复杂性。

Qiskit为量子神经网络的实现提供的类可以从`qiskit_machine_learning``.``neural_networks`导入（请参阅*附录* [*D*](ch027.xhtml#x1-240000D)*，安装工具*，以获取安装说明）。以下是一些例子：

+   **双层QNN**：`TwoLayerQNN`类可以用来实现一个具有单个特征图、变分形式和可观察量的量子神经网络。它适用于任何普通的量子神经网络。

+   **电路QNN**：`CircuitQNN`类允许你从一个参数化电路中实现一个量子神经网络。电路的最终状态将在计算基上被测量，并且每个测量结果可以通过一个解释函数映射到一个整数标签。这很有用，例如，如果你想构建一个分类器。

顺便说一句，在Qiskit的术语中，变分形式被称为**ansatzs**。正如你肯定记得的，这个名字也用于我们在*第7章*中研究的VQE算法的上下文中，VQE：变分量子*本征值求解器*。

*如果你在Qiskit中设计神经网络时想使用ZZ特征图或两个局部的变分形式，你不需要重新实现它们；它们包含在Qiskit中。你可以如下获取它们：

[PRE24]

在调用ZZ特征图类时，我们设置了重复次数为![1](img/file13.png "1")——任何其他数字都会产生具有该数量重复的ZZ特征图方案的特征图。在调用两个局部类时，我们还指定了——除了重复次数之外——我们想要使用的旋转门、受控门和纠缠布局。

为了举例，我们可以定义一个在三个量子比特上使用ZZ特征图和两个局部变分形式的`TwoLayer`量子神经网络。我们可以这样做：

[PRE25]

由于我们没有指定一个可观察量，所以结果QNN将返回在将网络的电路执行后测量的![Z \otimes Z \otimes Z](img/file1397.png "Z \otimes Z \otimes Z")可观察量的期望值。

我们可以在一些随机输入和可优化参数上，如下模拟我们刚刚创建的网络：

[PRE26]

第一个参数是一个包含一些（随机）经典输入的数组，而第二个参数是一个包含（随机）可优化参数值的数组。注意我们是如何使用量子神经网络的`qnum_inputs`和`num_weights`属性的。

我们所展示的所有神经网络类都是`NeuralNetwork`类的子类。例如，如果你想将神经网络训练为一个分类器，你可以依赖Qiskit的`NeuralNetworkClassifier`类。这个类可以通过一个`NeuralNetwork`对象以及指定损失函数和优化器等来初始化。

此外，还有一个`NeuralNetworkClassifier`的子类可以被用来直接创建一个可训练的神经网络分类器，提供特征图、变分形式、优化器、损失函数等。

这个子类被称为`VQC`（代表变分量子分类器），也可以从Qiskit模块`qiskit_machine_learning``.``algorithms``.``classifiers`中导入。

如果你想要使用Qiskit提供的默认参数从我们之前的`qnn`对象创建一个神经网络分类器对象，你可以运行以下指令：

[PRE27]

默认情况下，分类器将使用平方误差损失函数并依赖于SLSQP优化器[[62](ch030.xhtml#Xkraft1988software)]。

然后，如果你有一些带有标签`labels_train`的训练数据`data_train`，你可以通过调用`fit`方法来训练你新创建的分类器，如下所示：

[PRE28]

如果你想要计算训练好的分类器在某个数据`data_test`上的结果，你可以使用`predict`方法如下：

[PRE29]

或者，如果你想要计算训练模型在某个测试数据集（`data_test`和`labels_test`）上的准确度得分，你可以运行以下指令：

[PRE30]

然而，你不必过于关注`NeuralNetworkClassifier`和`VQC`类，因为正如我们所发现的那样，有一个替代方案——并且，在我们看来，这是一个更好的方法来在Qiskit中训练QNNs。我们将在下一章中讨论它，它将涉及与现有机器学习框架PyTorch的接口。更重要的是，能够使用这个接口将使我们能够探索Qiskit的“Torch运行时”：这是一个Qiskit实用工具，将使我们能够更有效地在IBM的真实量子硬件上训练QNNs。这正是我们在*第* *[*5*](ch013.xhtml#x1-940005)* *章*，QAOA:* *量子近似优化算法*中使用的相同技术，用于在量子硬件上运行QAOA执行。令人兴奋，不是吗？请耐心等待下一章的结尾。

*# 摘要

这已经是一段漫长的旅程了，不是吗？在本章中，我们首先介绍了量子神经网络作为经典神经网络的量子模拟。我们看到了量子神经网络的训练过程与经典神经网络非常相似，我们还探讨了使这成为可能的不同化方法。

理论部分结束后，我们准备好了键盘开始工作。我们学习了如何使用PennyLane实现和训练量子神经网络，我们还讨论了关于这个框架的一些技术细节，例如它提供的不同化方法。

PennyLane附带一些出色的模拟器，但——正如我们已经在*第* *[*2*](ch009.xhtml#x1-400002)* *章*，量子计算的工具中所提到的——它还与量子硬件平台（如Amazon Braket和IBM Quantum）集成。因此，你能够在实际量子计算机上训练量子神经网络的能力就在你的指尖！

我们在本章的结尾简要概述了如何在Qiskit中处理量子神经网络。

到目前为止，你已经对量子神经网络有了扎实的理解。结合你之前对量子支持向量机的知识，这为你打下了量子机器学习相当坚实的基础。在接下来的章节——它将非常注重实践——我们将探讨基于量子神经网络的更复杂模型架构。******
