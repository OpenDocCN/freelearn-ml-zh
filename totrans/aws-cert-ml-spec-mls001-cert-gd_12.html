<html><head></head><body>
		<div id="_idContainer166">
			<h1 id="_idParaDest-167"><em class="italic"><a id="_idTextAnchor173"/>Chapter 9</em>: Amazon SageMaker Modeling</h1>
			<p>In the previous chapter, we learned several methods of model optimization and evaluation techniques. We also learned various ways of storing data, processing data, and applying different statistical approaches to data. So, how can we now build a pipeline for this? Well, we can read data, process data, and build machine learning models on the processed data. But what if my first machine learning model does not perform well? Can I fine-tune my model? The answer is <em class="italic">Yes</em>; you can perform nearly everything using Amazon SageMaker. In this chapter, we will walk you through the following topics using Amazon SageMaker:</p>
			<ul>
				<li>Understanding different instances of Amazon SageMaker</li>
				<li>Cleaning and preparing data in Jupyter Notebook in Amazon SageMaker</li>
				<li>Model training in Amazon SageMaker</li>
				<li>Using SageMaker's built-in machine learning algorithms</li>
				<li>Writing custom training and inference code in SageMaker</li>
			</ul>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor174"/>Technical requirements</h1>
			<p>You can download the data used in this chapter's examples from GitHub at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-9">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-9</a>.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor175"/>Creating notebooks in Amazon SageMaker</h1>
			<p>If you're working <a id="_idIndexMarker767"/>with machine learning, then <a id="_idIndexMarker768"/>you need to perform actions such as storing data, processing data, preparing data for model training, model training, and deploying the model for inference. They are not easy, and each of these stages requires a machine to perform the task. With Amazon SageMaker, life becomes much easier when carrying out these steps. </p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor176"/>What is Amazon SageMaker?</h2>
			<p>SageMaker provides training instances to train a model using the data and provides endpoint <a id="_idIndexMarker769"/>instances to infer by using the model. It also provides notebook instances, running Jupyter Notebooks, to clean and understand the data. If you're happy with your cleaning process, then you should store them in S3 as part of the staging for training. You can launch training instances to consume this training data and produce a machine learning model. The machine learning model can be stored in S3, and endpoint instances can consume the model to produce results for end users.</p>
			<p>If you draw this in a block diagram, then it will look similar to <em class="italic">Figure 9.1</em>:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B16735_09_01.jpg" alt="Figure 9.1 – A pictorial representation of the different layers of the Amazon SageMaker instances&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – A pictorial representation of the different layers of the Amazon SageMaker instances</p>
			<p>Now, let's take a look at the Amazon SageMaker console and get a better feel for it. Once you log in to your AWS account and go to Amazon SageMaker, you will see something similar to <em class="italic">Figure 9.2</em>:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B16735_09_02.jpg" alt="Figure 9.2 – A quick look at the SageMaker console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – A quick look at the SageMaker console</p>
			<p>There are three different sections, including <strong class="bold">Notebook</strong>, <strong class="bold">Training</strong>, and <strong class="bold">Inference</strong>, which have been expanded in <em class="italic">Figure 9.2</em> so that we can dive in and understand them better.</p>
			<p><strong class="bold">Notebook</strong> has three <a id="_idIndexMarker770"/>different options that you can use:</p>
			<ul>
				<li><strong class="bold">Notebook instances</strong>: This helps us to create, open, start, and stop notebook instances. These <a id="_idIndexMarker771"/>instances are responsible for running Jupyter Notebooks. They allow us to choose the instance type based on the workload of the use case. The best practice is to use a notebook instance to orchestrate the data pipeline for processing a large dataset. For example, making a call from a notebook instance to AWS Glue for ETL services or Amazon EMR to run Spark applications. If you're asked to create a secured notebook instance outside AWS, then you need to take care of endpoint security, network security, launching the machine, managing storage on it, and managing Jupyter Notebook applications running on the instance. The user does not need to manage any of these with SageMaker.</li>
				<li><strong class="bold">Lifecycle configurations</strong>: This is useful when there is a use case that requires <a id="_idIndexMarker772"/> a different library, which is not available in the notebook instances. To install the library, the user will do either a <strong class="source-inline">pip install</strong> or a <strong class="source-inline">conda install</strong>. However, as soon as the notebook instance is terminated, the customization will be lost. To avoid <a id="_idIndexMarker773"/>such a scenario, you can customize your notebook instance through a script provided through <strong class="bold">Lifecycle configurations</strong>. You can choose any of the environments present in <strong class="source-inline">/home/ec2-user/anaconda3/envs/</strong> and customize the specific environment as required.</li>
				<li><strong class="bold">Git repositories</strong>: AWS CodeCommit, GitHub, or any other Git server can be associated <a id="_idIndexMarker774"/>with the notebook instance for the persistence of your notebooks. If access is given, then the same notebook can be used by other developers to collaborate and save in a source control fashion. Git repositories can either be added separately by using this option or they can be associated with a notebook instance during the creation.</li>
			</ul>
			<p>As you <a id="_idIndexMarker775"/>can see in <em class="italic">Figure 9.2</em>, <strong class="bold">Training</strong> offers <strong class="bold">Algorithms</strong>, <strong class="bold">Training jobs</strong>, and <strong class="bold">Hyperparameter tuning jobs</strong>. Let's understand their usage:</p>
			<ul>
				<li><strong class="bold">Algorithms</strong>: This is the first step toward deciding on an algorithm that we are going <a id="_idIndexMarker776"/>to run on our cleaned data. You can either choose a custom algorithm or create a custom algorithm based on the use case. Otherwise, you can run SageMaker algorithms on the cleaned data.</li>
				<li><strong class="bold">Training jobs</strong>: You can create training jobs from a notebook instance via API calls. You <a id="_idIndexMarker777"/>can set the number of instances, input the data source details, perform checkpoint configuration, and output data configuration. Amazon SageMaker manages the training instances and stores the model artifacts as output in the specified location. Both <a id="_idIndexMarker778"/>incremental training (that is, to train the model from time to time for better results) and managed spot training (that is, to reduce costs) can also be achieved.</li>
				<li><strong class="bold">Hyperparameter tuning jobs</strong>: Usually, hyperparameters are set for an algorithm prior to the training process. During the training process, we let the algorithm <a id="_idIndexMarker779"/>figure out the best values for these parameters. With hyperparameter tuning, we obtain the best model that has the best value of hyperparameters. This can be done through a console or via API calls. The same can be orchestrated from a notebook instance too.</li>
			</ul>
			<p><strong class="bold">Inference</strong> has <a id="_idIndexMarker780"/>many offerings and is evolving every day:</p>
			<ul>
				<li><strong class="bold">Compilation jobs</strong>: If your model is trained using a machine learning framework <a id="_idIndexMarker781"/>such as Keras, MXNet, ONNX, PyTorch, TFLite, TensorFlow, or XGBoost, and your model artifacts <a id="_idIndexMarker782"/>are available on a S3 bucket, then you can choose either <strong class="bold">Target device</strong> or <strong class="bold">Target platform</strong>. Target device is used to deploy <a id="_idIndexMarker783"/>your model, such as an AWS SageMaker machine learning instance or an AWS IoT Greengrass device. Target platform is used to decide the operating system, architecture, and accelerator on which you want your model to run. You can also store the compiled module in your S3 bucket for future use. This essentially helps you in cross-platform model deployment.</li>
				<li><strong class="bold">Model packages</strong>: These <a id="_idIndexMarker784"/>are used to create deployable SageMaker models. You can create your own algorithm, package it using the model package APIs, and publish it to AWS Marketplace.</li>
				<li><strong class="bold">Models</strong>: Models are created using model artifacts. They are similar to mathematical <a id="_idIndexMarker785"/>equations with variables; that is, you input the values for the variables and get an output. These models are stored in S3 and will be used for inference by the endpoints. </li>
				<li><strong class="bold">Endpoint configurations</strong>: Amazon SageMaker allows you to deploy multiple weighted <a id="_idIndexMarker786"/>models to a single endpoint. This means you can route a specific number of requests to one endpoint. <em class="italic">What does this mean?</em> Well, let's say you have one model in use. You want to replace it with a newer model. However, you cannot simply remove the first model that is already in use. In this scenario, you can use the <strong class="source-inline">VariantWeight</strong> API to make the endpoints serve 80% of the request with the older model and 20% of the request with the new model. This is the most common production scenario where the data changes rapidly and the model needs to be trained and tuned periodically. Another possible use case is to test the model results with live data, then a certain percentage of the requests can be routed to the new model, and the results can be monitored to testify the accuracy of the model on real-time unseen data.</li>
				<li><strong class="bold">Endpoints</strong>: These are used to create an URL to which the model is exposed and <a id="_idIndexMarker787"/>can be requested to give the model results as a response.</li>
				<li><strong class="bold">Batch transform jobs</strong>: If the use case demands that you infer results on several <a id="_idIndexMarker788"/>million records, then instead of individual inference jobs, you can run batch transform jobs on the unseen data. Note that there can be some confusion when you receive thousands of results from your model after parsing thousands of pieces of unseen data as a batch. To overcome this confusion, <strong class="source-inline">InputFilter</strong>, <strong class="source-inline">JoinSource</strong>, <strong class="source-inline">OutputFilter</strong> APIs can be used to associate input records with output results.</li>
			</ul>
			<p>Now, we have got an overview of Amazon SageMaker. Let's put our knowledge to work in the next section.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Amazon SageMaker console keeps changing. There's a possibility that when you're reading this book, the console might look different. </p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor177"/>Getting hands-on with Amazon SageMaker notebook instances</h2>
			<p>The very <a id="_idIndexMarker789"/>first step, in this section, is to create a Jupyter Notebook, and this requires a notebook instance. Let's start by creating a notebook instance, as follows:</p>
			<ol>
				<li>Sign in to your AWS account.</li>
				<li>Navigate to <strong class="bold">Services</strong> &gt; <strong class="bold">Amazon SageMaker</strong>.</li>
				<li>In the left navigation pane, click on <strong class="bold">Notebook instances</strong> and then click on the <strong class="bold">Create notebook instance</strong> button.</li>
				<li>Provide a <strong class="bold">Notebook instance name</strong> such as <strong class="source-inline">notebookinstance</strong> and leave the <strong class="bold">Notebook instance type</strong> to its default <strong class="source-inline">ml.t2.medium setting</strong>. In the <strong class="bold">Permissions and encryption</strong> section, select <strong class="source-inline">Create a new role</strong> in <strong class="bold">IAM role</strong>. You will be asked to specify the bucket name. For the purpose of this example, it's chosen as any bucket. </li>
				<li>Following the successful creation of a role, you should see something similar to <em class="italic">Figure 9.3</em>:<div id="_idContainer149" class="IMG---Figure"><img src="image/B16735_09_03.jpg" alt="Figure 9.3 – Amazon SageMaker role creation &#13;&#10;"/></div><p class="figure-caption">Figure 9.3 – Amazon SageMaker role creation </p></li>
				<li>Leave everything else in its default setting and click on the <strong class="bold">Create notebook instance</strong> button.</li>
				<li>Once the instance is in the <strong class="source-inline">InService</strong> state, select the instance. Click on the <strong class="bold">Actions</strong> drop-down menu and choose <strong class="bold">Open Jupyter</strong>. This opens your Jupyter Notebook.</li>
				<li>Now, we are <a id="_idIndexMarker790"/>all set to run our Jupyter Notebook on the newly created instance. We will perform <strong class="bold">Exploratory Data Analysis</strong> (<strong class="bold">EDA</strong>) and plot different types of graphs to visualize the data. Once we are familiar with the Jupyter Notebook, we will build some models to predict house prices in Boston. We will apply the algorithms that we have learned in previous chapters and compare them to find the best model that offers the best prediction according to our data. Let's dive in. </li>
				<li>In the <a id="_idIndexMarker791"/>Jupyter Notebook, click on <strong class="bold">New</strong> and select <strong class="bold">Terminal</strong>. Run the following commands in Command Prompt to download the codes to the instance: <p class="source-code"><strong class="bold">sh-4.2$ cd ~/SageMaker/</strong></p><p class="source-code"><strong class="bold">sh-4.2$ git clone https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide.git</strong></p></li>
				<li>Once the Git repository is cloned to the SageMaker notebook instance, type <strong class="source-inline">exit</strong> into Command Prompt to quit. Now, your code is ready to execute. </li>
				<li>Navigate to <strong class="source-inline">Chapter-9</strong> in the Jupyter Notebook's <strong class="bold">Files</strong> section, as shown in <em class="italic">Figure 9.4</em>:<div id="_idContainer150" class="IMG---Figure"><img src="image/B16735_09_04.jpg" alt="Figure 9.4 – Jupyter Notebook&#13;&#10;"/></div><p class="figure-caption">Figure 9.4 – Jupyter Notebook</p></li>
				<li>Click on <a id="_idIndexMarker792"/>the first notebook in <em class="italic">1.</em><strong class="source-inline">Boston-House-Price-SageMaker-Notebook-Instance-Example.ipynb</strong>. It will prompt you to choose the kernel for the notebook. Please select <strong class="source-inline">conda_python3</strong>, as shown in <em class="italic">Figure 9.5</em>:<div id="_idContainer151" class="IMG---Figure"><img src="image/B16735_09_05.jpg" alt="Figure 9.5 – Jupyter Notebook kernel selection&#13;&#10;"/></div><p class="figure-caption">Figure 9.5 – Jupyter Notebook kernel selection</p></li>
				<li>From the notebook, navigate to <strong class="bold">Kernel</strong> &gt; <strong class="bold">Restart &amp; Clear Output</strong>. Click on the play icon to run the cells one after another. Please ensure you have run each individual cell and inspect the output from each execution/run.</li>
				<li>You can <a id="_idIndexMarker793"/>experiment by adding cells and deleting cells to familiarize yourself with the Jupyter Notebook operations. In one of the paragraphs, there is a bash command that allows you to install the <strong class="source-inline">xgboost</strong> libraries from the notebook. </li>
				<li>The final cell explains how we have compared the different scores of various modeling techniques to draw a conclusion mathematically. <em class="italic">Figure 9.6</em> clearly shows that the best model to predict house prices in Boston is XGBoost:<div id="_idContainer152" class="IMG---Figure"><img src="image/B16735_09_06.jpg" alt="Figure 9.6 – Comparing the models&#13;&#10;"/></div><p class="figure-caption">Figure 9.6 – Comparing the models</p></li>
				<li>Once you've completed the execution of this notebook, please feel free to shut down the kernel and stop your notebook instance from the SageMaker console. This is the best practice to save on cost. </li>
			</ol>
			<p>In the <a id="_idIndexMarker794"/>next hands-on section, we will familiarize ourselves with Amazon SageMaker's training and inference instances. We will also use the Amazon SageMaker API to make this process easier. We will use the same notebook instance as we did in the previous example.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor178"/>Getting hands-on with Amazon SageMaker's training and inference instances</h2>
			<p>In this <a id="_idIndexMarker795"/>section, we will learn about <a id="_idIndexMarker796"/>training a model and hosting the model to generate predicted results. Let's dive in by using the notebook instance from the previous example:</p>
			<ol>
				<li value="1">Sign in to your AWS account at <a href="https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances">https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances</a>.</li>
				<li>Click on <strong class="bold">Start</strong> next to the instance that we created in the previous example, <strong class="bold">notebookinstance</strong>. Once the status moves to <strong class="source-inline">InService</strong>, open it in a new tab, as shown in <em class="italic">Figure 9.7</em>:<div id="_idContainer153" class="IMG---Figure"><img src="image/B16735_09_07.jpg" alt="Figure 9.7 – The InService instance&#13;&#10;"/></div><p class="figure-caption">Figure 9.7 – The InService instance</p></li>
				<li>Navigate to the tab named SageMaker Examples in the Jupyter Notebook home page.</li>
				<li>Select <a id="_idIndexMarker797"/>the <strong class="source-inline">k_nearest_neighbors_covtype.ipynb</strong> notebook. Click on <strong class="bold">Use</strong> and create a copy.</li>
				<li>When <a id="_idIndexMarker798"/>you run the following paragraph, as shown n <em class="italic">Figure 9.8</em>, you can also check a training job in <strong class="bold">Training</strong> &gt; <strong class="bold">Training jobs</strong> of the SageMaker home page:<div id="_idContainer154" class="IMG---Figure"><img src="image/B16735_09_08.jpg" alt="Figure 9.8 – The SageMaker fit API call&#13;&#10;"/></div><p class="figure-caption">Figure 9.8 – The SageMaker fit API call</p></li>
				<li>The training job looks similar to <em class="italic">Figure 9.9</em>. It launches an ECS container in the backend and uses the IAM execution role created in the previous example to run the training job for this request:<div id="_idContainer155" class="IMG---Figure"><img src="image/B16735_09_09.jpg" alt="Figure 9.9 – Training obs&#13;&#10;"/></div><p class="figure-caption">Figure 9.9 – Training obs</p></li>
				<li>If you go inside and check the logs in CloudWatch, it gives you more details about the containers and the steps they performed. As a machine learning engineer, it's worth going in and checking the CloudWatch metrics for Algorithm. </li>
				<li>Now, if you <a id="_idIndexMarker799"/>run the following <a id="_idIndexMarker800"/>paragraph, as shown in <em class="italic">Figure 9.10</em>, in the notebook, then it will create an endpoint configuration and an endpoint where the model from the earlier training job is deployed. </li>
				<li>I have changed the instance type to save on cost. It is the instance or the machine that will host your model. Please choose your instance wisely. We will learn about choosing instance types in the next section. I have also changed <strong class="source-inline">endpoint_name</strong> so that it can be recognized easily:<div id="_idContainer156" class="IMG---Figure"><img src="image/B16735_09_010.jpg" alt="Figure 9.10 – Creating the predictor object with endpoint details&#13;&#10;"/></div><p class="figure-caption">Figure 9.10 – Creating the predictor object with endpoint details</p></li>
				<li>Navigate to <strong class="bold">Inference</strong> &gt; <strong class="bold">Endpoints</strong>. This will show you the endpoint that was created as a result of the previous paragraph execution. This endpoint has a configuration and can be navigated and traced through <strong class="bold">Inference</strong> &gt; <strong class="bold">Endpoint Configurations</strong>. </li>
				<li>If you view the <strong class="bold">Inference</strong> section in the notebook, you will notice that it uses the test <a id="_idIndexMarker801"/>data to predict results. It uses <a id="_idIndexMarker802"/>the predictor object from the SageMaker API to make predictions. The predictor object contains the endpoint details, model name, and instance type.</li>
				<li>The API call to the endpoint occurs in the <strong class="bold">Inference</strong> section, and it is authenticated via the IAM role with which the notebook instance is created. The same API calls can be traced through CloudWatch invocation metrics.</li>
				<li>Finally, running the <strong class="source-inline">delete_endpoint</strong> method in the notebook will delete the endpoint. To delete the endpoint configurations, navigate to <strong class="bold">Inference</strong> &gt;<strong class="bold"> Endpoint Configurations</strong> and select the configuration on the screen. Click on <strong class="bold">Actions</strong> &gt; <strong class="bold">Delete</strong> &gt; <strong class="bold">Delete</strong>.</li>
				<li>Now, please feel free to shut down the kernel and stop your notebook instance from the SageMaker console. This is the best practice to save on cost. </li>
			</ol>
			<p>In this section, we learned about using the notebook instance, training instances, inference endpoints, and endpoint configurations to clean our data, train models, and generate predicted results from them. In the next section, we will learn about model tuning.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor179"/>Model tuning</h1>
			<p>In <a href="B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 8</em></a>, <em class="italic">Evaluating and Optimizing Models</em>, you learned many important concepts <a id="_idIndexMarker803"/>about model tuning. Let's now explore this topic from a practical perspective.</p>
			<p>In order <a id="_idIndexMarker804"/>to tune a model on SageMaker, we have to call <strong class="source-inline">create_hyper_parameter_tuning_job</strong> and pass the following main parameters:</p>
			<ul>
				<li><strong class="source-inline">HyperParameterTuningJobName</strong>: This is the name of the tuning job. It is useful to track the training jobs that have been started on behalf of your tuning job.</li>
				<li><strong class="source-inline">HyperParameterTuningJobConfig</strong>: Here, you can configure your tuning options. For example, which parameters you want to tune, the range of values for them, the type of optimization (such as random search or Bayesian search), the maximum number of training jobs you want to spin up, and more.</li>
				<li><strong class="source-inline">TrainingJobDefinition</strong>: Here, you can configure your training job. For example, the data channels, the output location, the resource configurations, the evaluation metrics, and the stop conditions.</li>
			</ul>
			<p>In SageMaker, the main metric that we want to use to evaluate the models to select the best one <a id="_idIndexMarker805"/>is known as an <strong class="bold">objective metric</strong>.</p>
			<p>In the following example, we are configuring <strong class="source-inline">HyperParameterTuningJobConfig</strong> for a decision <a id="_idIndexMarker806"/>tree-based algorithm. We want to check the best configuration for a <strong class="bold">max_depth</strong> hyperparameter, which is responsible for controlling the depth of the tree.</p>
			<p>In <strong class="source-inline">IntegerParameterRanges</strong>, we have to specify the following: </p>
			<ul>
				<li>The hyperparameter name</li>
				<li>The minimum value that we want to test </li>
				<li>The maximum value that we want to test<p class="callout-heading">Important note</p><p class="callout">Each type of hyperparameter must fit in one of the parameter ranges sections, such as categorical, continuous, or integer parameters.</p></li>
			</ul>
			<p>In <strong class="source-inline">ResourceLimits</strong>, we are specifying the number of training jobs along with the number of parallel jobs that we want to run. Remember that the goal of the tuning process is to <a id="_idIndexMarker807"/>execute many training jobs with different hyperparameter settings. This is so that the best one will be selected for the final model. That's why we have to specify these training job execution rules.</p>
			<p>We then set up our search strategy in <strong class="source-inline">Strategy</strong> and, finally, set up the objective function in <strong class="source-inline">HyperParameterTuningJobObjective</strong>:</p>
			<p class="source-code">tuning_job_config = {</p>
			<p class="source-code">    "ParameterRanges": {</p>
			<p class="source-code">      "CategoricalParameterRanges": [],</p>
			<p class="source-code">      "ContinuousParameterRanges": [],</p>
			<p class="source-code">      "IntegerParameterRanges": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "MaxValue": "10",</p>
			<p class="source-code">          "MinValue": "1",</p>
			<p class="source-code">          "Name": "max_depth"</p>
			<p class="source-code">        }</p>
			<p class="source-code">      ]</p>
			<p class="source-code">    },</p>
			<p class="source-code">    "ResourceLimits": {</p>
			<p class="source-code">      "MaxNumberOfTrainingJobs": 10,</p>
			<p class="source-code">      "MaxParallelTrainingJobs": 2</p>
			<p class="source-code">    },</p>
			<p class="source-code">    "Strategy": "Bayesian",</p>
			<p class="source-code">    "HyperParameterTuningJobObjective": {</p>
			<p class="source-code">      "MetricName": "validation:auc",</p>
			<p class="source-code">      "Type": "Maximize"</p>
			<p class="source-code">    }</p>
			<p class="source-code">  }</p>
			<p>The second important configuration we need to set is <strong class="source-inline">TrainingJobDefinition</strong>. Here, we have to specify all the details regarding the training jobs that will be executed. One of the most <a id="_idIndexMarker808"/>important settings is the <strong class="source-inline">TrainingImage</strong> setting, which refers to the container that will be started to execute the training processes. This container, as expected, must have your training algorithm implemented.</p>
			<p>Here, we present <a id="_idIndexMarker809"/>an example of a built-in algorithm, <strong class="bold">eXtreme Gradient Boosting</strong>, so that you can set the training image as follows:</p>
			<p class="source-code">training_image = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1')</p>
			<p>Then, you can go ahead and set your training definitions:</p>
			<p class="source-code">training_job_definition = {</p>
			<p class="source-code">    "AlgorithmSpecification": {</p>
			<p class="source-code">      "TrainingImage": training_image,</p>
			<p class="source-code">      "TrainingInputMode": "File"</p>
			<p class="source-code">    },</p>
			<p>Next, we have to specify the data input configuration, which is also known as the data channels. In the following section of code, we are setting up two data channels – train and validation:</p>
			<p class="source-code">    "InputDataConfig": [</p>
			<p class="source-code">      {</p>
			<p class="source-code">        "ChannelName": "train",</p>
			<p class="source-code">        "CompressionType": "None",</p>
			<p class="source-code">        "ContentType": "csv",</p>
			<p class="source-code">        "DataSource": {</p>
			<p class="source-code">          "S3DataSource": {</p>
			<p class="source-code">            "S3DataDistributionType": "FullyReplicated",</p>
			<p class="source-code">            "S3DataType": "S3Prefix",</p>
			<p class="source-code">            "S3Uri": s3_input_train</p>
			<p class="source-code">          }</p>
			<p class="source-code">        }</p>
			<p class="source-code">      },</p>
			<p class="source-code">      {</p>
			<p class="source-code">        "ChannelName": "validation",</p>
			<p class="source-code">        "CompressionType": "None",</p>
			<p class="source-code">        "ContentType": "csv",</p>
			<p class="source-code">        "DataSource": {</p>
			<p class="source-code">          "S3DataSource": {</p>
			<p class="source-code">            "S3DataDistributionType": "FullyReplicated",</p>
			<p class="source-code">            "S3DataType": "S3Prefix",</p>
			<p class="source-code">            "S3Uri": s3_input_validation</p>
			<p class="source-code">          }</p>
			<p class="source-code">        }</p>
			<p class="source-code">      }</p>
			<p class="source-code">    ],</p>
			<p>We also need to specify where the results will be stored:</p>
			<p class="source-code">    "OutputDataConfig": {</p>
			<p class="source-code">      "S3OutputPath": "s3://{}/{}/output".format(bucket,prefix)</p>
			<p class="source-code">    },</p>
			<p>Finally, we set <a id="_idIndexMarker810"/>the resource configurations, roles, static parameters, and stopping conditions. In the following section of code, we want to use two instances of type <strong class="source-inline">ml.c4.2xlarge </strong>with 10 GB of storage:</p>
			<p class="source-code">    "ResourceConfig": {</p>
			<p class="source-code">      "InstanceCount": 2,</p>
			<p class="source-code">      "InstanceType": "ml.c4.2xlarge",</p>
			<p class="source-code">      "VolumeSizeInGB": 10</p>
			<p class="source-code">    },</p>
			<p class="source-code">    "RoleArn": &lt;&lt;your_role_name&gt;&gt;,</p>
			<p class="source-code">    "StaticHyperParameters": {</p>
			<p class="source-code">      "eval_metric": "auc",</p>
			<p class="source-code">      "num_round": "100",</p>
			<p class="source-code">      "objective": "binary:logistic",</p>
			<p class="source-code">      "rate_drop": "0.3",</p>
			<p class="source-code">      "tweedie_variance_power": "1.4"</p>
			<p class="source-code">    },</p>
			<p class="source-code">    "StoppingCondition": {</p>
			<p class="source-code">      "MaxRuntimeInSeconds": 43200</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Please note that we are using other variables in this configuration file, <strong class="source-inline">bucket</strong> and <strong class="source-inline">prefix</strong>, which should be replaced by your bucket name and prefix key (if needed), respectively. We are also referring to <strong class="source-inline">s3_input_train</strong> and <strong class="source-inline">s3_input_validation</strong>, which are two variables that point to the train and validation datasets in S3.</p>
			<p>Once you<a id="_idIndexMarker811"/> have set your configurations, you can spin up the tuning process:</p>
			<p class="source-code">smclient.create_hyper_parameter_tuning_job(</p>
			<p class="source-code">     HyperParameterTuningJobName = "my-tuning-example",</p>
			<p class="source-code">     HyperParameterTuningJobConfig = tuning_job_config,</p>
			<p class="source-code">     TrainingJobDefinition = training_job_definition</p>
			<p class="source-code">)</p>
			<p>Next, let's find out how to track the execution of this process.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor180"/>Tracking your training jobs and selecting the best model</h2>
			<p>Once you <a id="_idIndexMarker812"/>have started the tuning process, there are two additional steps that you might want to check: tracking the process of tuning <a id="_idIndexMarker813"/>and selecting the winner model (that is, the one with the best set of hyperparameters).</p>
			<p>In order to find your training jobs, you should go to the SageMaker console and navigate to <strong class="bold">Hyperparameter training jobs</strong>. You will then find a list of executed tuning jobs, including yours:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B16735_09_011.jpg" alt="Figure 9.11 – Finding your tuning job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – Finding your tuning job</p>
			<p>If you access your tuning job, by clicking under its name, you will find a summary page, which includes the most relevant information regarding the tuning process. Under the <strong class="bold">Training jobs</strong> tab, you will see all of the training jobs that have been executed:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B16735_09_012.jpg" alt="Figure 9.12 – Summary of the training jobs in the tuning process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – Summary of the training jobs in the tuning process</p>
			<p>Finally, if you click on the <strong class="bold">Best training job</strong> tab, you will find the best set of hyperparameters <a id="_idIndexMarker814"/>for your model, including a handy <a id="_idIndexMarker815"/>button to create a new model based on those best hyperparameters that have just been found:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B16735_09_013.jpg" alt="Figure 9.13 – Finding the best set of hyperparameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – Finding the best set of hyperparameters</p>
			<p>As you can see, SageMaker is very intuitive, and once you know the main concepts behind model optimization, playing with SageMaker should be easier. By now, we have understood how to use SageMaker for our specific needs. In the next section, we will explore how to select the instance type for various use cases and the security of our notebooks.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor181"/>Choosing instance types in Amazon SageMaker</h1>
			<p>SageMaker is a pay-for-usage model. There is no minimum fee for it. </p>
			<p>When we <a id="_idIndexMarker816"/>think about instances on SageMaker, it all <a id="_idIndexMarker817"/>starts with an EC2 instance. This instance is responsible for all your processing. It's a managed EC2 instance. These instances won't show up in the EC2 console and cannot be SSHed either. The instance type starts with <strong class="source-inline">ml</strong>.</p>
			<p>SageMaker offers instances of the following families:</p>
			<ul>
				<li>The<strong class="bold"> t</strong> family: This is <a id="_idIndexMarker818"/>a burstable CPU family. With this family, you get a normal ratio of CPU and memory. This means that if you have a long-running training job, then you lose performance over time as you spend the CPU credits. If you have very small jobs, then they are cost-effective. For example, if you want a notebook instance to launch training jobs, then this family is the most relevant and cost-effective.</li>
				<li>The<strong class="bold"> m</strong> family: In the <a id="_idIndexMarker819"/>previous family, we saw that CPU credits are consumed faster due to their burstable nature. If you have a long-running machine learning job that requires constant throughput, then this is the right family. It comes with a similar CPU and memory ratio as the <strong class="bold">t</strong> family. </li>
				<li>The<strong class="bold"> r</strong> family: This is <a id="_idIndexMarker820"/>a memory-optimized family. <em class="italic">When do we need this?</em> Well, imagine a use case where you have to load the data in memory and do some data engineering on the data. In this scenario, you will require more memory and your job will be memory-optimized. </li>
				<li>The <strong class="bold">c</strong> family: <strong class="bold">c</strong> family instances are compute-optimized. This is a requirement <a id="_idIndexMarker821"/>for jobs that need higher compute power and less memory to store the data. If you refer to the following table, C5.2x large has 8 vCPU and 16 GiB memory, which makes it compute-optimized with less memory. For example, a use case needs to be tested on a fewer number of records and it is compute savvy, then this instance family is the to-go option to get some sample records from a huge dataframe and test your algorithm.</li>
				<li>The <strong class="bold">p</strong> family: This is a GPU family that supports accelerated computing jobs such as training <a id="_idIndexMarker822"/>and inference. Notably, <strong class="bold">p</strong> family instances are ideal for handling large, distributed training jobs, and this leads to less time required to train. As a result, this becomes cost-effective. The P3/P3dn GPU compute instance can go up to 1 petaFLOP per second compute with up to 256 GB of GPU memory and 100 Gbps (gigabits) of networking with 8x NVIDIA v100 GPUs. They are highly optimized for training and are not fully utilized for inference.</li>
				<li>The <strong class="bold">g</strong> family: For <a id="_idIndexMarker823"/>cost-effective, small-scale training jobs, <strong class="bold">g</strong> family GPU instances are ideal. G4 has the lowest cost per inference for GPU instances. It uses T4 NVIDIA GPUs. The G4 GPU compute instance goes up to 520 TeraFLOPs of compute time with 8x NVIDIA T4 GPUs. This instance family is the best for simple networks.</li>
			</ul>
			<p>In the <a id="_idIndexMarker824"/>following table, 2x large instance <a id="_idIndexMarker825"/>types have been taken from each family for a visual comparison between the CPU and memory ratio:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B16735_09_Table_1.jpg" alt="Table 9.1 – A table showing the CPU and memory ratio of different instance types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 9.1 – A table showing the CPU and memory ratio of different instance types</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To remember this easily, you can think of T for Tiny, M for Medium, C for Compute, and P and G for GPU. CPU family instance types are T, M, R, and C. GPU family instance types are P and G.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor182"/>Choosing the right instance type for a training job</h2>
			<p>There is no rule of thumb to decide the instance type that you require. It changes based on the <a id="_idIndexMarker826"/>size of the data, the complexity of the network, the machine learning algorithm, and several other factors such as time and cost. Asking the right question will save money and make it cost-effective.</p>
			<p>If the deciding factor is <em class="italic">Instance Size</em>, then classifying the problem for CPU or GPU is the right step. Once that is done, then it is good to consider whether it can be multi-GPU or multi-CPU. That would solve your question about distributed training. This also solves your <em class="italic">Instance Count</em> factor. If it's compute-intensive, then it would be wise to check the memory requirements too.</p>
			<p>The next deciding factor is <em class="italic">Instance Family</em>. The right questions here would be, <em class="italic">Is it optimized for time and cost?</em>  In the previous step, we have already figured out the problem to be solved in either the CPU or GPU, and this narrows down the selection process. Now, let's learn about inference jobs.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor183"/>Choosing the right instance type for an inference job</h2>
			<p>The majority of the cost and complexity of machine learning in production is inference. Usually, inference <a id="_idIndexMarker827"/>runs on a single input in real time. They are usually less compute/memory-intensive. They have to be highly available as they run all the time and serve the end user requests or are integrated as part of an application. </p>
			<p>You can choose any of the instance types that we learned about recently based on the workload. Other <a id="_idIndexMarker828"/>than that, AWS has <strong class="bold">Inf1</strong> and <strong class="bold">Elastic Inference</strong> type instances for inference. Elastic inference <a id="_idIndexMarker829"/>allows you to attach a fraction of a GPU instance to any CPU instance. </p>
			<p>Let's look at an example where an application is integrated with inference jobs, then the requirement of CPU and memory for the application is different from the inference jobs. For those use cases, you need to choose the right instance type and size. In such scenarios, it is good to have a separation between your application fleets and inference fleets. This might require some management. If such management is a problem for your requirement, then choose elastic inference, where both the application and inference jobs can be colocated. This means that you can host multiple models on the same fleet, and you can load all of these different models on different accelerators in memory and concurrent requests can be served.</p>
			<p>It's always <a id="_idIndexMarker830"/>recommended that you run some examples in a lower environment before deciding on your instance types and family in the production environment. In the next section, we will dive into and understand the different ways of securing our Amazon SageMaker notebooks.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor184"/>Securing SageMaker notebooks</h1>
			<p>If you are <a id="_idIndexMarker831"/>reading this section of the chapter, then you have already learned how to use notebook instances, which type of training instances should be chosen, and how to configure and use endpoints. Now, let's learn about securing those instances. The following aspects will help to secure the instances:</p>
			<ul>
				<li><strong class="bold">Encryption</strong>: When we say or think about securing via encryption, then it is all about <a id="_idIndexMarker832"/>the data. But what does this mean? It means protecting data at rest using encryption, protecting data in transit with encryption, and using KMS for better role separation and internet traffic privacy through TLS 1.2 encryption. SageMaker instances can be launched with encrypted volumes by using an AWS-managed KMS key. This helps you to secure the Jupyter Notebook server by default.</li>
				<li><strong class="bold">Root access</strong>: When a user opens a shell terminal from the Jupyter Web UI, they <a id="_idIndexMarker833"/>will be logged in as ec2-user, which is the default username in Amazon Linux. Now the user can run sudo to the root user. With root access, users can access and edit files. In many use cases, an administrator might not want data scientists to manage, control, or modify the system of the notebook server. This requires restrictions to be placed on the root access. This can be done by setting the <strong class="source-inline">RootAccess</strong> field to <strong class="source-inline">Disabled</strong> when you call <strong class="source-inline">CreateNotebookInstance</strong> or <strong class="source-inline">UpdateNotebookInstance</strong>. The data scientist will have access to their user space and can install Python packages. However, they cannot sudo into the root user and make changes to the operating system.</li>
				<li><strong class="bold">IAM role</strong>: During the launch of a notebook instance, it is necessary to create an IAM role <a id="_idIndexMarker834"/>for execution or to use an existing role for execution. This is used to launch the service-managed EC2 instance with an instance profile associated with the role. This role will restrict the API calls based on the policies attached to this role.</li>
				<li><strong class="bold">VPC connection</strong>: When you launch a SageMaker notebook instance, by default, it gets created within the SageMaker Service Account, which has a service-managed VPC, and it will, by default, have access to the internet via an internet <a id="_idIndexMarker835"/>gateway, and that gateway is managed by the service. If you are only dealing with AWS-related services, then it is recommended that you launch a SageMaker notebook instance in your VPC within a private subnet and with a well-customized security group. The AWS services can be invoked or used from this notebook instance via VPC endpoints attached to that VPC. The best practice is to control them via endpoint policies for better API controls. This ensures the restriction on data egress outside your VPC and secured environment. In order to capture all the network traffic, you can turn on the VPC flow logs, which can be monitored and tracked via CloudWatch.</li>
				<li><strong class="bold">Internet access</strong>: You can launch a Jupyter Notebook server without direct internet <a id="_idIndexMarker836"/>access. It can be launched in a private subnet with a NAT or to access the internet through a virtual private gateway. For training and deploying inference containers, you can set the <strong class="source-inline">EnableNetworkIsolation</strong> parameter to <strong class="source-inline">True</strong> when you are calling <strong class="source-inline">CreateTrainingJob</strong>, <strong class="source-inline">CreateHyperParameterTuningJob</strong>, or <strong class="source-inline">CreateModel</strong>. Network isolation can be used along with the VPC, which ensures that containers cannot make any outbound network calls.</li>
				<li><strong class="bold">Connecting a private network to your VPC</strong>: You can launch your SageMaker <a id="_idIndexMarker837"/>notebook instance inside the private subnet of your VPC. This can access data from your private network by communicating to the private network, which can be done by connecting your private network to your VPC by using Amazon VPN or AWS Direct Connect.</li>
			</ul>
			<p>In this <a id="_idIndexMarker838"/>section, we learned about several ways in which we can secure our SageMaker notebooks. In the next section, we will learn about creating SageMaker pipelines with Lambda Functions.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor185"/>Creating alternative pipelines with Lambda Functions</h1>
			<p>Indeed, SageMaker <a id="_idIndexMarker839"/>is an awesome <a id="_idIndexMarker840"/>platform that you can use to create training and inference pipelines. However, we can always work with different services to come up with similar solutions. One of these services that we will learn <a id="_idIndexMarker841"/>about next is known as <strong class="bold">Lambda Functions</strong>.</p>
			<p>AWS Lambda is a serverless compute service where you can literally run a function as a service. In other words, you can concentrate your efforts on just writing your function. Then, you just need to tell AWS how to run it (that is, the environment and resource configurations), so all the necessary resources will be provisioned to run your code and then discontinued once it is completed.</p>
			<p>Throughout <a href="B16735_06_Final_VK_ePub.xhtml#_idTextAnchor115"><em class="italic">Chapter 6</em></a>, <em class="italic">AWS Services for Data Processing</em>, you explored how Lambda Functions integrate with many different services, such as Kinesis and AWS Batch. Indeed, AWS did a very good job of integrating Lambda with 140 services (and the list is constantly increasing). That means that when you are working with a specific AWS service, you will remember that it is likely to integrate with Lambda.</p>
			<p>It is important to bear this in mind because Lambda Functions can really expand your possibilities to create scalable and integrated architectures. For example, you can trigger a Lambda Function when a file is uploaded to S3 in order to preprocess your data before loading it to Redshift. Alternatively, you can create an API that triggers a Lambda Function at each endpoint execution. Again, the possibilities are endless with this powerful service.</p>
			<p>It is also useful to know that you can write your function in different programming languages, such as Node.js, Python, Go, Java, and more. Your function does not necessarily have to be triggered by another AWS service, that is, you can trigger it manually for your web or mobile application, for example.</p>
			<p>When it comes to deployment, you can upload your function as a ZIP file or as a container image. Although this is not ideal for an automated deployment process, coding directly into the AWS Lambda console is also possible.</p>
			<p>As with <a id="_idIndexMarker842"/>any other service, this <a id="_idIndexMarker843"/>one also has some downsides that you should be aware of:</p>
			<ul>
				<li>Memory allocation for your function: This is from 128 MB to 10,240 MB (AWS has recently increased this limit from 3 GB to 10 GB, as stated previously).</li>
				<li>Function timeout: This is a maximum of 900 seconds (15 minutes).</li>
				<li>Function layer: This is a maximum of 5 layers.</li>
				<li>Burst concurrency: This is from 500 to 3,000, depending on the region.</li>
				<li>Deployment package size: This is 250 MB unzipped, including layers.</li>
				<li>Container image code package size: This is 10 GB.</li>
				<li>Available space in the <strong class="source-inline">/tmp</strong> directory: This is 512 MB.</li>
			</ul>
			<p>Before going for Lambda Functions, make sure these restrictions fit your use case. Bringing Lambda Functions closer to your scope of alternative pipelines for SageMaker, one potential use of Lambda is to create inference pipelines for our models.</p>
			<p>As you know, SageMaker has a very handy <strong class="source-inline">.deploy()</strong> method that will create endpoints for model inference. This is so that you can call to pass the input data in order to receive predictions back. Here, we can create this inference endpoint by using the API gateway and Lambda Functions.</p>
			<p>In case you don't need an inference endpoint and you just want to make predictions and store results somewhere (in a batch fashion), then all we need is a Lambda Function, which is able to fetch the input data, instantiate the model object, make predictions, and store the results in the appropriated location. Of course, it does this by considering all the limitations that we have discussed earlier.</p>
			<p>Alright, now that we have a good background about Lambda and some use cases, let's take a look at the most important configurations that we should be aware of for the exam.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor186"/>Creating and configuring a Lambda Function</h2>
			<p>First of all, you should know that you can create a Lambda Function in different ways, such as via the <a id="_idIndexMarker844"/>AWS CLI (Lambda API reference), the AWS Lambda console, or even <a id="_idIndexMarker845"/>deployment frameworks (for example, <em class="italic">the serverless framework</em>). </p>
			<p>Serverless frameworks are usually provider and programming language-independent. In other words, they usually allow you to choose where you want to deploy a serverless infrastructure from a variate list of cloud providers and programming languages. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The concept <a id="_idIndexMarker846"/>of serverless architecture is not specific to AWS. In fact, many cloud providers offer other services that are similar to AWS Lambda Functions. That's why these serverless frameworks have been built: to help developers and engineers to deploy their services, wherever they want, including AWS. This is unlikely to come up in your exam, but it is definitely something that you should know so that you are aware of different ways in which to solve your challenges as a data scientist or data engineer. </p>
			<p>Since we want to pass the AWS Machine Learning Specialty exam, here, we have taken the approach to walk through the AWS Lambda console. This is so that you will become more familiar with their interface and the most important configuration options.</p>
			<p>When you navigate to the Lambda console and request a new Lambda Function, AWS will provide you with some starting options:</p>
			<ul>
				<li>Author from scratch: This is if you want to create your function from scratch.</li>
				<li>Use a blueprint: This is if you want to create your function from a sample code and configuration preset for common use cases.</li>
				<li>Container image: This is if you want to select a container image to deploy your function.</li>
				<li>Browse serverless app repository: This is if you want to deploy a sample Lambda application from the AWS Serverless Application Repository.</li>
			</ul>
			<p>Starting from scratch, the next step is to set up your Lambda configurations. AWS splits these configurations between basic and advanced settings. In the basic configuration, you will <a id="_idIndexMarker847"/>set your function name, runtime environment, and <a id="_idIndexMarker848"/>permissions. <em class="italic">Figure 9.14</em> shows these configurations:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B16735_09_014.jpg" alt="Figure 9.14 – Creating a new Lambda Function from the AWS Lambda console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – Creating a new Lambda Function from the AWS Lambda console</p>
			<p>Here, we have a very important configuration that you should remember during your exam: the <strong class="bold">execution role</strong>. Your Lambda <a id="_idIndexMarker849"/>Function might need permissions to access other AWS resources, such as S3, Redshift, and more. The execution role grants permissions to your Lambda Function so that it can access resources as needed. </p>
			<p>You have <a id="_idIndexMarker850"/>to remember that your VPC and security group <a id="_idIndexMarker851"/>configurations will also interfere with how your Lambda Function runs. For example, if you want to create a function that needs internet access to download something, then you have to deploy this function in a VPC with internet access. The same logic applies to other resources, such as access to relational databases, Kinesis, Redshift, and more. </p>
			<p>Furthermore, in order to properly configure a Lambda Function, we have to, at least, write its code, set the execution role, and make sure the VPC and security group configurations match our needs. Next, let's take a look at other configurations.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor187"/>Completing your configurations and deploying a Lambda Function </h2>
			<p>Once your <a id="_idIndexMarker852"/>Lambda is created in the AWS console, you can set additional configurations before deploying the function. One of these configurations is the event trigger. As we mentioned earlier, your Lambda Function can be triggered from a variety of services or even manually. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A very common <a id="_idIndexMarker853"/>example of a trigger is <strong class="bold">Event Bridge</strong>. This is an AWS service where you can schedule the execution of your function. </p>
			<p>Depending on the event trigger you choose, your function will have access to different event metadata. For example, if your <a id="_idIndexMarker854"/>function is triggered by a <strong class="bold">PUT</strong> event on S3 (for example, someone uploads a file to a particular S3 bucket), then your function will receive the metadata associated with this event, for example, bucket name and object key. Other types of triggers will give you different types of event metadata!</p>
			<p>You have access to those metadata through the event parameter that belongs to the signature of the entry point of your function. Not clear enough? OK, let's see how your function code should be declared, as follows:</p>
			<p class="source-code">def lambda_handler(event, context):</p>
			<p class="source-code">TODO   </p>
			<p>Here, <strong class="source-inline">lambda_handler</strong> is the method that represents the entry point of your function. When it is triggered, this method will be called, and it will receive the event metadata associated with the event trigger (through the <strong class="source-inline">event</strong> parameter). That's how you have access to the information associated with the underlying event that has triggered your function! The <strong class="source-inline">event</strong> parameter is a JSON-like object.</p>
			<p>If you want <a id="_idIndexMarker855"/>to test your function, but you don't want to <a id="_idIndexMarker856"/>trigger it directly from the underlying event, that is no problem; you <a id="_idIndexMarker857"/>can use <strong class="bold">test events</strong>. They simulate the underlying event by preparing a JSON object that will be passed to your function.</p>
			<p><em class="italic">Figure 9.15</em> shows a very intuitive example. Let's suppose you have created a function that is triggered when a user uploads a file to S3 and now you want to test your function. You can either upload a file to S3 (which forces the trigger) or create a test event.</p>
			<p>By creating a test event, you can prepare a JSON object that simulates the S3-put event and then pass this object to your function:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B16735_09_015.jpg" alt="Figure 9.15 – Creating a test event from the Lambda console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.15 – Creating a test event from the Lambda console</p>
			<p>Another type of <a id="_idIndexMarker858"/>configuration you can set is an <strong class="bold">environment variable</strong>, which <a id="_idIndexMarker859"/>will be available on your function. <em class="italic">Figure 9.16</em> shows how to <a id="_idIndexMarker860"/>add environment variables in a Lambda Function:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B16735_09_016.jpg" alt="Figure 9.16 – Adding environment variables to a Lambda Function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – Adding environment variables to a Lambda Function</p>
			<p>You can always come back to these basic configurations to make adjustments as necessary. <em class="italic">Figure 9.17</em> shows what you will find in the basic configurations section:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B16735_09_017.jpg" alt="Figure 9.17 – Changing the basic configurations of a Lambda Function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – Changing the basic configurations of a Lambda Function</p>
			<p>In terms of <a id="_idIndexMarker861"/>monitoring, by default, Lambda Functions produce a <strong class="bold">CloudWatch</strong> Logs stream and <a id="_idIndexMarker862"/>standard metrics. You can access log information <a id="_idIndexMarker863"/>by navigating through your Lambda Function monitoring section and clicking on <em class="italic">View logs in CloudWatch</em>.</p>
			<p>In CloudWatch, each Lambda Function <a id="_idIndexMarker864"/>will have a <strong class="bold">Log group</strong> and, inside that Log group, many <strong class="bold">Log streams</strong>. Log streams <a id="_idIndexMarker865"/>store the execution logs of the associated function. In other words, a log stream is a sequence of logs that share the same source, which, in this case, is your Lambda Function. A log group is a group of log streams that share the same retention, monitoring, and access control settings.</p>
			<p>We are reaching the end of this section, but not the end of this topic on Lambda Functions. As we <a id="_idIndexMarker866"/>mentioned earlier, this AWS service has a <a id="_idIndexMarker867"/>lot of use cases and integrates with many other services. In the next section, we will take a look at another AWS service that will help us to orchestrate executions of Lambda Functions. These are known as <strong class="bold">AWS Step Functions</strong>.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor188"/>Working with Step Functions</h1>
			<p>Step Functions <a id="_idIndexMarker868"/>is an AWS service that allows you to create workflows in order to orchestrate the execution of Lambda Functions. This is so that <a id="_idIndexMarker869"/>you can connect them in a sort of event sequence, known as <strong class="bold">steps</strong>. These steps <a id="_idIndexMarker870"/>are grouped in a <strong class="bold">state machine</strong>.</p>
			<p>Step Functions incorporates retry functionality so that you can configure your pipeline to proceed only after a particular step has succeeded. The way you set these retry configurations <a id="_idIndexMarker871"/>is by creating a <strong class="bold">retry policy</strong>. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Just like the majority of AWS services, AWS Step Functions also integrates with other services, not only AWS Lambda.  </p>
			<p>Creating a state machine is relatively simple. All you have to do is navigate to the AWS Step Functions console, then create a new state machine. On the <em class="italic">Create state machine</em> page, you can specify whether you want to create your state machine from scratch, from a template, or whether you just want to run a sample project.</p>
			<p>AWS will help you with this state machine creation, so even if you choose to create it from scratch, you will find code snippets for a variate list of tasks, such as AWS Lambda invocation, SNS topic publication, running Athena queries, and more.</p>
			<p>For the sake of demonstration, we will create a very simple, but still helpful, example of how to use Step Functions to execute a Lambda Function with the retry option activated:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "Comment": "A very handy example of how to call a lamnbda function with retry option",</p>
			<p class="source-code">  "StartAt": "Invoke Lambda function",</p>
			<p class="source-code">  "States": {</p>
			<p class="source-code">    "Invoke Lambda function": {</p>
			<p class="source-code">      "Type": "Task",</p>
			<p class="source-code">      "Resource": "arn:aws:states:::lambda:invoke",</p>
			<p class="source-code">      "Parameters": {</p>
			<p class="source-code">        "FunctionName": "arn:aws:lambda:your-function-identification",</p>
			<p class="source-code">        "Payload": {</p>
			<p class="source-code">          "Input": {</p>
			<p class="source-code">            "env": "STAGE"</p>
			<p class="source-code">          }</p>
			<p class="source-code">        }</p>
			<p class="source-code">      },</p>
			<p class="source-code">      "Retry": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "ErrorEquals": ["States.ALL"],</p>
			<p class="source-code">          "IntervalSeconds": 60,</p>
			<p class="source-code">          "MaxAttempts": 5,</p>
			<p class="source-code">          "BackoffRate": 2.0</p>
			<p class="source-code">        }</p>
			<p class="source-code">      ],</p>
			<p class="source-code">      "Next": "Example"</p>
			<p class="source-code">     },</p>
			<p class="source-code">    "Example": {</p>
			<p class="source-code">      "Type": "Pass",</p>
			<p class="source-code">      "Result": "Just to show you how to configure other steps",</p>
			<p class="source-code">      "End": true</p>
			<p class="source-code">    }</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>In the <a id="_idIndexMarker872"/>preceding example, we created a state machine with two steps:</p>
			<ul>
				<li>Invoke Lambda function: This will start the execution of your underlying Lambda.</li>
				<li>Example: This is a simple pass task just to show you how to connect a second step in the pipeline.</li>
			</ul>
			<p>In the first step, we have also set up a retry policy, which will try to re-execute this task if there are any failures. We are setting up the interval (in seconds) to try again and to show here the number of attempts. <em class="italic">Figure 9.18</em> shows the state machine:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B16735_09_018.jpg" alt="Figure 9.18 – The state machine&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.18 – The state machine</p>
			<p>We have <a id="_idIndexMarker873"/>now reached the end of this section and the end of this chapter. Next, let's summarize what we have learned.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor189"/>Summary</h1>
			<p>In this chapter, we learned about the usage of SageMaker for creating notebook instances and training instances. As we went through we learned how to use SageMaker for hyperparameter tuning jobs. As the security of our assets in AWS is an essential part, we learned about the various ways to secure SageMaker instances. With hands-on practices, we created Step Functions and orchestrated our pipeline using AWS Lambda. </p>
			<p>AWS products are evolving every day to help us solve our IT problems. It's not easy to remember all the product names. The only way to learn is through practice. When you're solving a problem or building a product, then focus on the different technological areas of your product. Those areas can be an AWS service, for example, scheduling jobs, logging, tracing, monitoring metrics, autoscaling, and more.</p>
			<p>Compute time, storage, and networking are the baselines. It is recommended that you practice some examples for each of these services. Referring to the AWS documentation for clarifying any doubts is also the best option. It is always important to design your solutions in a cost-effective way, so exploring the cost-effective way to use these services is equally important as building the solution. I wish you all the best!</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor190"/>Questions</h2>
			<ol>
				<li value="1">Which of the following models are supervised algorithms? Select two options.<p>A. Clustering</p><p>B. Classification</p><p>C. Association rule mining</p><p>D. Regression</p></li>
				<li>You would like to turn your Amazon SageMaker machine learning models and endpoints into customer-facing applications. You decide to put these on a single web server that can be accessed by customers via a browser. However, you realize that the web server is not inherently scalable; if it receives a lot of traffic, it could run out of CPU or memory. How can you make this approach more scalable and secure? Select three answers.<p>A. Create an IAM role, so the webserver can access the SageMaker endpoints.</p><p>B. Deploy a load balancer and set up autoscaling.</p><p>C. Make all customers IAM users so that they can access SageMaker.</p><p>D. Keep the operating system and language runtimes for the web server patch secured.</p></li>
				<li>For the preceding situation, what would be a better AWS service to automate server and operating system maintenance, capacity provisioning, and automatic scaling? <p>A. AWS Lambda</p><p>B. AWS Fargate</p><p>C. AWS ELB</p></li>
				<li>Amazon SageMaker is a fully managed service that enables you to quickly and easily integrate machine learning-based models into your applications. It also provides services such as notebook, training, and endpoint instances to help you get the job done.<p>A. TRUE</p><p>B. FALSE</p></li>
				<li>Chose three correct statements from the following:<p>A. Notebook instances clean and understand data. </p><p>B. Training instances use data to train the model. </p><p>C. Endpoint instances use models to produce inferences.</p><p>D. Notebook instances clean, understand, and build models. </p><p>E. Training instances are used to predict results.</p></li>
				<li>What is the first step of creating a notebook?<p>A. Give it a name.</p><p>B. Choose a kernel.</p><p>C. Starting developing code in paragraph format.</p></li>
				<li>Linear learner and XGBoost algorithms can be used in supervised learning models such as regression and classification.<p>A. TRUE</p><p>B. FALSE</p></li>
				<li>Which of these statements about hyperparameter tuning is true?<p>A. Hyperparameter tuning is a guaranteed way to improve your model.</p><p>B. Hyperparameter tuning does not require any input values.</p><p>C. Hyperparameter tuning uses regression to choose the best value to test.</p><p>D. Hyperparameter tuning is an unsupervised machine learning regression problem.</p></li>
			</ol>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor191"/>Answers</h2>
			<p>1. B and D</p>
			<p>2. A, B, and D</p>
			<p>3. A</p>
			<p>4. A</p>
			<p>5. A, B, and C</p>
			<p>6. B</p>
			<p>7. A</p>
			<p>8. C</p>
		</div>
	</body></html>