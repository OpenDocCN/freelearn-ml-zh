<html><head></head><body>
        

                            
                    <h1 class="header-title">Fun with Filters</h1>
                
            
            
                
<p>The goal of this chapter is to develop a number of image processing filters and then apply them to the video stream of a webcam in real time. These filters will rely on various OpenCV functions to manipulate matrices through splitting, merging, arithmetic operations, and applying lookup tables for complex functions.</p>
<p>We will cover the following three effects, which will help familiarize you with OpenCV, and we will build on these effects in future chapters of this book:</p>
<ul>
<li><strong>Warming and cooling filters</strong>: We will implement our own <strong>curve filters</strong> using a lookup table.</li>
<li><strong>Black-and-white pencil sketch</strong>: We will make use of two image-blending techniques, known as <strong>dodging</strong> and <strong>burning</strong>.</li>
<li><strong>Cartoonizer</strong>: We will combine a bilateral filter, a median filter, and adaptive thresholding.</li>
</ul>
<p>OpenCV is an advanced toolchain. It often raises the question, that is, not how to implement something from scratch, but which precanned implementation to choose for your needs. Generating complex effects is not hard if you have a lot of computing resources to spare. The challenge usually lies in finding an approach that not only gets the job done but also gets it done in time.</p>
<p>Instead of teaching the basic concepts of image manipulation through theoretical lessons, we will take a practical approach and develop a single end-to-end app that integrates a number of image filtering techniques. We will apply our theoretical knowledge to arrive at a solution that not only works but also speeds up seemingly complex effects so that a laptop can produce them in real time.</p>
<p>In this chapter, you will learn how to do the following using OpenCV:</p>
<ul>
<li>Creating a black-and-white pencil sketch</li>
<li>Applying pencil sketch transformation</li>
<li>Generating a warming and cooling filter</li>
<li>Cartoonizing an image</li>
<li>Putting it all together</li>
</ul>
<p>Learning this will allow you to familiarize yourself with loading images into OpenCV and applying different transformations to those images using OpenCV. This chapter will help you learn the basics of how OpenCV operates, so we can focus on the internals of the algorithms in the following chapters.</p>
<p>Now, let's take a look at how to get everything up and running.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting started</h1>
                
            
            
                
<p class="mce-root">All of the code in this book is targeted for <strong>OpenCV 4.2</strong> and has been tested on <strong>Ubuntu 18.04</strong>. Throughout this book, we will make extensive use of the <kbd>NumPy</kbd> package (<a href="http://www.numpy.org">http://www.numpy.org</a>).</p>
<p class="mce-root">Additionally, this chapter requires the <kbd>UnivariateSpline</kbd> module of the <kbd>SciPy</kbd> package (<a href="http://www.scipy.org">http://www.scipy.org</a>) and the <strong>wxPython 4.0</strong> <strong>Graphical User Interface</strong> (<strong>GUI</strong>) (<a href="http://www.wxpython.org/download.php">http://www.wxpython.org/download.php</a>) for cross-platform GUI applications. We will try to avoid further dependencies where possible.</p>
<p class="mce-root">For more book-level dependencies, see <a href="a4f1f102-9f62-4644-bcde-f478cd28621a.xhtml">Appendix A</a>, <em>Profiling and Accelerating Your Apps</em>, and <a href="c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml">Appendix B</a>, <em>Setting Up a Docker Container</em>.</p>
<p>You can find the code that we present in this chapter at our GitHub repository here: <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter1">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter1</a>.</p>
<p>Let's begin by planning the application we are going to create in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the app</h1>
                
            
            
                
<p>The final app must consist of the following modules and scripts:</p>
<ul>
<li><kbd>wx_gui.py</kbd>: This module is our implementation of a basic GUI using <kbd>wxpython</kbd>. We will make extensive use of this file throughout the book. This module includes the following layouts:
<ul>
<li><kbd>wx_gui.BaseLayout</kbd>: This is a generic layout class from which more complicated layouts can be built.</li>
</ul>
</li>
<li><kbd>chapter1.py</kbd>: This is the main script for this chapter. It contains the following functions and classes:
<ul>
<li><kbd>chapter1.FilterLayout</kbd>: This is a custom layout based on <kbd>wx_gui.BaseLayout</kbd>, which displays the camera feed and a row of radio buttons that allows the user to select from the available image filters to be applied to each frame of the camera feed.</li>
<li><kbd>chapter1.main</kbd>: This is the main routine function for starting the GUI application and accessing the webcam.</li>
</ul>
</li>
<li><kbd>tools.py</kbd>: This is a Python module and has a lot of helper functions that we use in this chapter, which you can reuse for your projects.</li>
</ul>
<p>The next section demonstrates how to create a black-and-white pencil sketch.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a black-and-white pencil sketch</h1>
                
            
            
                
<p>In order to obtain a pencil sketch (that is, a black-and-white drawing) of the camera frame, we will make use of two image-blending techniques, known as <strong>dodging</strong> and <strong>burning</strong>. These terms refer to techniques employed during the printing process in traditional photography; here, photographers would manipulate the exposure time of a certain area of a darkroom print in order to lighten or darken it. Dodging <em>lightens</em> an image, whereas burning <em>darkens</em> it. Areas that were not supposed to undergo changes were protected with a <strong>mask</strong>.</p>
<p>Today, modern image editing programs, such as <strong>Photoshop</strong> and <strong>Gimp</strong>, offer ways to mimic these effects in digital images. For example, masks are still used to mimic the effect of changing the exposure time of an image, wherein areas of a mask with relatively intense values will <em>expose</em> the image more, thus lightening the image. OpenCV does not offer a native function to implement these techniques; however, with a little insight and a few tricks, we will arrive at our own efficient implementation that can be used to produce a beautiful pencil sketch effect.</p>
<p>If you search on the internet, you might stumble upon the following common procedure to achieve a pencil sketch from an <strong>RGB</strong> (<strong>red</strong>, <strong>green</strong>, and <strong>blue</strong>) color image:</p>
<ol>
<li>First, convert the color image to grayscale.</li>
<li>Then, invert the grayscale image to get a negative.</li>
<li>Apply a <strong>Gaussian blur</strong> to the negative from <em>step 2</em>.</li>
<li>Blend the grayscale image (from <em>step 1</em>) with the blurred negative (from <em>step 3</em>) by using <strong>color dodge</strong>.</li>
</ol>
<p>Whereas <em>steps 1</em> to <em>3</em> are straightforward, <em>step 4</em> can be a little tricky. Let's get that one out of the way first.</p>
<p>OpenCV 3 came with a pencil sketch effect right out of the box. The <kbd>cv2.pencilSketch</kbd> function uses a domain filter introduced in the 2011 paper, <em>Domain Transform for Edge-Aware Image and Video Processing</em>, by Eduardo Gastal and Manuel Oliveira. However, for the purposes of this book, we will develop our own filter.</p>
<p>The next section shows you how to implement dodging and burning in OpenCV.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding approaches for using dodging and burning techniques</h1>
                
            
            
                
<p>Dodging decreases the exposure for areas of the image that we wish to make lighter (than before) in an image, <kbd>A</kbd>. In image processing, we usually select or specify areas of the image that need to be altered using masks. A mask, <kbd>B</kbd>, is an array of the same dimensions as the image on which it can be applied (think of it as a sheet of paper you use to cover the image that has holes in it). "Holes" in the sheet of paper are represented with <kbd>255</kbd> (or ones if we are working on the 0-1 range) in an opaque region with zeros.</p>
<p>In modern image editing tools, such as Photoshop, the color dodging of the image <kbd>A</kbd> with the mask <kbd>B</kbd> is implemented by using the following ternary statement that acts on every pixel using the index <kbd>i</kbd>:</p>
<pre>((B[i] == 255) ? B[i] : <br/>    min(255, ((A[i] &lt;&lt; 8) / (255 - B[i]))))</pre>
<p>The previous code essentially divides the value of the <kbd>A[i]</kbd> image pixel by the inverse of the <kbd>B[i]</kbd> mask pixel value (which are in the range of <kbd>0</kbd>-<kbd>255</kbd>), while making sure that the resulting pixel value will be in the range of (0, 255) and that we do not divide by 0.</p>
<p>We could translate the previous complex-looking expression or code into the following naive Python function, which accepts two OpenCV matrices (<kbd>image</kbd> and <kbd>mask</kbd>) and returns the blended image:</p>
<pre>def dodge_naive(image, mask):<br/>    # determine the shape of the input image<br/>    width, height = image.shape[:2]<br/><br/>    # prepare output argument with same size as image<br/>    blend = np.zeros((width, height), np.uint8)<br/><br/>    for c in range(width):<br/>        for r in range(height):<br/><br/>            # shift image pixel value by 8 bits<br/>            # divide by the inverse of the mask<br/>            result = (image[c, r] &lt;&lt; 8) / (255 - mask[c, r])<br/><br/>            # make sure resulting value stays within bounds<br/>            blend[c, r] = min(255, result)<br/>    return blend</pre>
<p>As you might have guessed, although the previous code might be functionally correct, it will undoubtedly be horrendously slow. Firstly, the function uses the <kbd>for</kbd> loops, which are almost always a bad idea in Python. Secondly, the NumPy arrays (the underlying format of OpenCV images in Python) are optimized for the array calculations, so accessing and modifying each <kbd>image[c, r]</kbd> pixel separately will be really slow.</p>
<p>Instead, we should realize that the <kbd>&lt;&lt;8</kbd> operation is the same as multiplying the pixel value with the number <strong>2<sup>8</sup></strong> (<strong>=256</strong>), and that pixel-wise division can be achieved with the <kbd>cv2.divide</kbd> function. Thus, an improved version of our <kbd>dodge</kbd> function that takes advantage of matrix multiplication (which is faster) looks like this:</p>
<pre>import cv2 
 
def dodge(image, mask): 
    return cv2.divide(image, 255 - mask, scale=256) </pre>
<p>Here, we have reduced the <kbd>dodge</kbd> function to a single line! The new <kbd>dodge</kbd> function produces the same result as <kbd>dodge_naive</kbd>, but it is orders of magnitude faster than the naive version. In addition to this, <kbd>cv2.divide</kbd> automatically takes care of the division by zero, making the result zero, where <kbd>255 - mask</kbd> is zero.</p>
<p>Here is a dodged version of <kbd>Lena.png</kbd> where we have dodges in the square with pixels in the range of (100:300, 100:300<strong>)</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/12387677-69ad-460a-9cc9-b339d74f9acd.png" style="width:61.58em;height:31.00em;"/></p>
<p>Image credit—"Lenna" by Conor Lawless is licensed under CC BY 2.0</p>
<p class="mce-root">As you can see, the lightened region is very obvious in the right photograph because the transition is very sharp. There are ways to correct this, one of which we will take a look at in the next section.</p>
<p class="mce-root">Let's learn how to obtain a Gaussian blur by using two-dimensional convolution next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing a Gaussian blur with two-dimensional convolution</h1>
                
            
            
                
<p>A Gaussian blur is implemented by convolving the image with a kernel of Gaussian values. Two- dimensional convolution is something that is used very widely in image processing. Usually, we have a big picture (let's look at a 5 x 5 subsection of that particular image), and we have a kernel (or filter) that is another matrix of a smaller size (in our example, 3 x 3).</p>
<p>In order to get the convolution values, let's suppose that we want to get the value at <em>location (2, 3)</em>. We place the kernel centered at the <em>location</em> (<em>2</em>, <em>3</em>), and we calculate the pointwise product of the overlay matrix (highlighted area, in the following image (red color)) with the kernel and take the overall sum. The resulting value (that is, 158.4) is the value we write on the other matrix at the <em>location (2, 3)</em>.</p>
<p>We repeat this process for all elements, and the resulting matrix (the matrix on the right) is the convolution of the kernel with the image. In the following diagram, on the left, you can see the original image with the pixel values in the boxes (values higher than 100). We also see an orange filter with values in the bottom right of each cell (a collection of 0.1 or 0.2 that sum to 1). In the matrix on the right, you see the values when the filter is applied to the image on the left:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img src="img/658345ff-7977-4546-94d7-559959cbcdb2.png" style="width:64.50em;height:32.58em;"/></p>
<p>Note that, for points on the boundaries, the kernel is not aligned with the matrix, so we have to figure out a strategy to give values for those points. There is no single good strategy that works for everything; some of the approaches are to either extend the border with zeros or extend with border values.</p>
<p>Let's take a look at how to transform a normal picture into a pencil sketch.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Applying pencil sketch transformation</h1>
                
            
            
                
<p>With the tricks that we learned from the previous sections in our bag, we are now ready to take a look at the entire procedure.</p>
<p>The final code can be found in the <kbd>convert_to_pencil_sketch</kbd> function within the <kbd>tools.py</kbd> file.</p>
<p>The following procedure shows you how to convert a color image into grayscale. After that, we aim to blend the grayscale image with its blurred negative:</p>
<ol start="1">
<li>First, we convert an RGB image (<kbd>imgRGB</kbd>) into grayscale:</li>
</ol>
<pre style="padding-left: 60px">img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY) </pre>
<p style="padding-left: 60px">As you can see, we have used <kbd>cv2.COLOR_RGB2GRAY</kbd> as a parameter to the <kbd>cv2.cvtColor</kbd> function, which changes the color spaces. Note that it does not matter whether the input image is RGB or BGR (which is the default for OpenCV); we will get a nice grayscale image in the end.</p>
<ol start="2">
<li>Then, we invert the image and blur it with a large Gaussian kernel of size <kbd>(21,21)</kbd>:</li>
</ol>
<pre style="padding-left: 60px">    inv_gray = 255 - gray_image<br/>    blurred_image = cv2.GaussianBlur(inv_gray, (21, 21), 0, 0)</pre>
<ol start="3">
<li>We use <kbd>dodge</kbd> to blend the original grayscale image with the blurred inverse:</li>
</ol>
<pre style="padding-left: 60px">    gray_sketch = cv2.divide(gray_image, 255 - blurred_image, <br/>    scale=256)</pre>
<p>The resulting image looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/edd66dc7-744c-4e1e-80ef-facc5258db11.png" style="width:63.25em;height:32.08em;"/></p>
<p>Image credit—"Lenna" by Conor Lawless is licensed under CC BY 2.0</p>
<p>Did you notice that our code can be optimized further? Let's take a look at how to optimize with OpenCV next.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using an optimized version of a Gaussian blur</h1>
                
            
            
                
<p>A Gaussian blur is basically a convolution with a Gaussian function. Well, one of the features of convolutions is their associative property. This means that it does not matter whether we first invert the image and then blur it, or first blur the image and then invert it.</p>
<p>If we start with a blurred image and pass its inverse to the <kbd>dodge</kbd> function, then within that function the image will be inverted again (the <kbd>255-mask</kbd> part), essentially yielding the original image. If we get rid of these redundant operations, the optimized <kbd>convert_to_pencil_sketch</kbd> function will look like this:</p>
<pre>def convert_to_pencil_sketch(rgb_image):<br/>    gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)<br/>    blurred_image = cv2.GaussianBlur(gray_image, (21, 21), 0, 0)<br/>    gray_sketch = cv2.divide(gray_image, blurred_image, scale=256)<br/>    return cv2.cvtColor(gray_sketch, cv2.COLOR_GRAY2RGB)</pre>
<p class="mce-root">For kicks and giggles, we want to lightly blend our transformed image (<kbd>img_sketch</kbd>) with a background image (<kbd>canvas</kbd>) that makes it look as though we drew the image on a canvas. So, before returning, we would like to blend with <kbd>canvas</kbd> if it exists:</p>
<pre>    if canvas is not None:<br/>        gray_sketch = cv2.multiply(gray_sketch, canvas, scale=1 / 256)</pre>
<p>We name our final function <kbd>pencil_sketch_on_canvas</kbd>, and it looks like this (together with optimizations):</p>
<pre>def pencil_sketch_on_canvas(rgb_image, canvas=None):<br/>    gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)<br/>    blurred_image = cv2.GaussianBlur(gray_image, (21, 21), 0, 0)<br/>    gray_sketch = cv2.divide(gray_image, blurred_image, scale=256)<br/>    if canvas is not None:<br/>        gray_sketch = cv2.multiply(gray_sketch, canvas, scale=1 / 256)<br/>    return cv2.cvtColor(gray_sketch, cv2.COLOR_GRAY2RGB)</pre>
<p>This is just our <kbd>convert_to_pencil_sketch</kbd> function, with the optional <kbd>canvas</kbd> argument that can add an artistic touch to the pencil sketch.</p>
<p>And we're done! The final output looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/719a8a97-3961-4214-b22e-17abd4c22c02.png" style="width:29.00em;height:29.00em;"/></p>
<p class="mce-root">Let's take a look at how to generate a warming and cooling filter in the next section, where you'll learn how to use <strong>lookup tables</strong> for image manipulation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Generating a warming and cooling filter</h1>
                
            
            
                
<p>When we perceive images, our brain picks up on a number of subtle clues to infer important details about the scene. For example, in broad daylight, highlights may have a slightly yellowish tint because they are in direct sunlight, whereas shadows may appear slightly bluish because of the ambient light of the blue sky. When we view an image with such color properties, we might immediately think of a sunny day.</p>
<p>This effect is not a mystery to photographers, who sometimes purposely manipulate the white balance of an image to convey a certain mood. Warm colors are generally perceived as more pleasant, whereas cool colors are associated with night and drabness.</p>
<p>To manipulate the perceived color temperature of an image, we will implement a curve filter. These filters control how color transitions appear between different regions of an image, allowing us to subtly shift the color spectrum without adding an unnatural-looking overall tint to the image.</p>
<p class="mce-root">In the next section, we'll look at how to manipulate color using curve shifting.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using color manipulation via curve shifting</h1>
                
            
            
                
<p>A curve filter is essentially a function, <strong><em>y = f (x)</em></strong>, that maps an input pixel value, <em>x</em>, to an output pixel value, <em>y</em>. The curve is parameterized by a set of <em><strong>n + 1</strong></em> anchor points, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/2e3aa2ee-d206-42ee-bdeb-a0fc7aa27fb3.png" style="width:17.58em;height:1.50em;"/></p>
<p>Here, each anchor point is a pair of numbers that represent the input and output pixel values. For example, the pair (30, 90) means that an input pixel value of 30 is increased to an output value of 90. Values between anchor points are interpolated along a smooth curve (hence, the name curve filter).</p>
<p>Such a filter can be applied to any image channel, be it a single grayscale channel or the <strong>R</strong> (<strong>red</strong>), <strong>G</strong> (<strong>green</strong>), and <strong>B</strong> (<strong>blue</strong>) channels of an RGB color image. Therefore, for our purposes, all values of <em>x</em> and <em>y</em> must stay between 0 and 255.</p>
<p>For example, if we wanted to make a grayscale image slightly brighter, we could use a curve filter with the following set of control points:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/8b4f25f1-a4eb-491d-9e13-2fab7cf96333.png" style="width:13.50em;height:1.25em;"/></p>
<p>This would mean that all input pixel values except <strong>0</strong> and <strong>255</strong> would be increased slightly, resulting in an overall brightening effect on the image.</p>
<p>If we want such filters to produce natural-looking images, it is important to respect the following two rules:</p>
<ul>
<li>Every set of anchor points should include <strong>(0,0)</strong> and <strong>(255,255)</strong>. This is important in order to prevent the image from appearing as if it has an overall tint, as black remains black and white remains white.</li>
<li>The <em>f(x)</em> function should be monotonously increasing. In other words, by increasing <em>x</em>, <em>f(x)</em> either stays the same or increases (that is, it never decreases). This is important for making sure that shadows remain shadows and highlights remain highlights.</li>
</ul>
<p class="mce-root">The next section demonstrates how to implement a curve filter using lookup tables.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing a curve filter using lookup tables</h1>
                
            
            
                
<p>Curve filters are computationally expensive because the values of <em>f(x)</em> must be interpolated whenever <em>x</em> does not coincide with one of the prespecified anchor points. Performing this computation for every pixel of every image frame that we encounter would have dramatic effects on performance.</p>
<p>Instead, we make use of a lookup table. Since there are only 256 possible pixel values for our purposes, we need to calculate <strong><em>f(x)</em></strong> only for all the 256 possible values of <strong><em>x</em></strong>. Interpolation is handled by the <kbd>UnivariateSpline</kbd> function of the <kbd>scipy.interpolate</kbd> module, as shown in the following code snippet:</p>
<pre>from scipy.interpolate import UnivariateSpline 
 
def spline_to_lookup_table(spline_breaks: list, break_values: list):<br/>    spl = UnivariateSpline(spline_breaks, break_values)<br/>    return spl(range(256)</pre>
<p>The <kbd>return</kbd> argument of the function is a list of 256 elements that contains the interpolated <em>f(x)</em> values for every possible value of <em>x</em>.</p>
<p>All we need to do now is to come up with a set of anchor points, (<em>x<sub>i</sub></em>, <em>y<sub>i</sub></em>), and we are ready to apply the filter to a grayscale input image (<kbd>img_gray</kbd>):</p>
<pre>import cv2 <br/>import numpy as np 
 
x = [0, 128, 255] 
y = [0, 192, 255] 
myLUT = spline_to_lookup_table(x, y) 
img_curved = cv2.LUT(img_gray, myLUT).astype(np.uint8) </pre>
<p>The result looks like this (the original image is on the <em>left</em>, and the transformed image is on the <em>right</em>):</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9e47feb6-4d50-4fac-b177-981ba3253ca7.png" style="width:52.08em;height:18.58em;"/></p>
<p class="mce-root">In the next section, we'll design the warming and cooling effect. You will also learn how to apply lookup tables to colored images, and how warming and cooling effects work.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Designing the warming and cooling effect</h1>
                
            
            
                
<p>With the mechanism to quickly apply a generic curve filter to any image channel in place, we can now turn to the question of how to manipulate the perceived color temperature of an image. Again, the final code will have its own function in the <kbd>tools</kbd> module.</p>
<p>If you have a minute to spare, I advise you to play around with the different curve settings for a while. You can choose any number of anchor points and apply the curve filter to any image channel you can think of (red, green, blue, hue, saturation, brightness, lightness, and so on). You could even combine multiple channels, or decrease one and shift another to the desired region. <em>What will the result look like?</em></p>
<p>However, if the number of possibilities dazzles you, take a more conservative approach. First, by making use of our <kbd>spline_to_lookup_table</kbd> function developed in the preceding steps, let's define two generic curve filters: one that (by trend) increases all the pixel values of a channel and one that generally decreases them:</p>
<pre>INCREASE_LOOKUP_TABLE = spline_to_lookup_table([0, 64, 128, 192, 256],<br/>                                               [0, 70, 140, 210, 256])<br/>DECREASE_LOOKUP_TABLE = spline_to_lookup_table([0, 64, 128, 192, 256],<br/>                                               [0, 30, 80, 120, 192])</pre>
<p class="mce-root">Now, let's examine how we could apply lookup tables to an RGB image. OpenCV has a nice function called <kbd>cv2.LUT</kbd> that takes a lookup table and applies it to a matrix. So, first, we have to decompose the image into different channels:</p>
<pre>    c_r, c_g, c_b = cv2.split(rgb_image)</pre>
<p class="mce-root">Then, we apply a filter to each channel if desired:</p>
<pre>    if green_filter is not None:<br/>        c_g = cv2.LUT(c_g, green_filter).astype(np.uint8)</pre>
<p>Doing this for all the three channels in an RGB image, we get the following helper function:</p>
<pre>def apply_rgb_filters(rgb_image, *,<br/>                      red_filter=None, green_filter=None, blue_filter=None):<br/>    c_r, c_g, c_b = cv2.split(rgb_image)<br/>    if red_filter is not None:<br/>        c_r = cv2.LUT(c_r, red_filter).astype(np.uint8)<br/>    if green_filter is not None:<br/>        c_g = cv2.LUT(c_g, green_filter).astype(np.uint8)<br/>    if blue_filter is not None:<br/>        c_b = cv2.LUT(c_b, blue_filter).astype(np.uint8)<br/>    return cv2.merge((c_r, c_g, c_b))</pre>
<p>The easiest way to make an image appear as if it was taken on a hot, sunny day (maybe close to sunset) is to increase the reds in the image and make the colors appear vivid by increasing the color saturation. We will achieve this in two steps:</p>
<ol>
<li>Increase the pixel values in the <strong>R channel</strong> (from RGB image) and decrease the pixel values in the <strong>B channel</strong> of an RGB color image using <kbd>INCREASE_LOOKUP_TABLE</kbd> and <kbd>DECREASE_LOOKUP_TABLE</kbd>, respectively:</li>
</ol>
<pre style="padding-left: 60px">        interim_img = apply_rgb_filters(rgb_image,<br/>                                        red_filter=INCREASE_LOOKUP_TABLE,<br/>                                        blue_filter=DECREASE_LOOKUP_TABLE)</pre>
<ol start="2">
<li>Transform the image into the <strong>HSV</strong> color space (<strong>H</strong> means <strong>hue</strong>, <strong>S</strong> means <strong>saturation</strong>, and <strong>V</strong> means <strong>value</strong>), and increase the <strong>S channel</strong> using <kbd>INCREASE_LOOKUP_TABLE</kbd>. This can be achieved with the following function, which expects an RGB color image and a lookup table to apply (similar to the <kbd>apply_rgb_filters</kbd> function) as input:</li>
</ol>
<pre style="padding-left: 60px">def apply_hue_filter(rgb_image, hue_filter):<br/>    c_h, c_s, c_v = cv2.split(cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV))<br/>    c_s = cv2.LUT(c_s, hue_filter).astype(np.uint8)<br/>    return cv2.cvtColor(cv2.merge((c_h, c_s, c_v)), cv2.COLOR_HSV2RGB)</pre>
<p>The result looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5c931305-7b18-4362-8a56-26647fa3c649.png" style="width:52.75em;height:18.83em;"/></p>
<p>Analogously, we can define a cooling filter that increases the pixel values in the B channel, decreases the pixel values in the R channel of an RGB image, converts the image into the HSV color space, and decreases color saturation via the S channel:</p>
<pre>    def _render_cool(rgb_image: np.ndarray) -&gt; np.ndarray:<br/>        interim_img = apply_rgb_filters(rgb_image,<br/>                                        red_filter=DECREASE_LOOKUP_TABLE,<br/>                                        blue_filter=INCREASE_LOOKUP_TABLE)<br/>        return apply_hue_filter(interim_img, DECREASE_LOOKUP_TABLE)</pre>
<p>Now the result looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e4683d58-a8b6-418d-b0eb-03c672dd4c87.png" style="width:52.42em;height:18.33em;"/></p>
<p class="mce-root">Let's explore how to cartoonize an image in the next section, where we'll learn what a bilateral filter is and much more.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cartoonizing an image</h1>
                
            
            
                
<p>Over the past few years, professional cartoonizer software has popped up all over the place. In order to achieve a basic cartoon effect, all we need is a <strong>bilateral filter</strong> and some <strong>edge detection</strong>.</p>
<p>The bilateral filter will reduce the color palette or the numbers of colors that are used in the image. This mimics a cartoon drawing, wherein a cartoonist typically has few colors to work with. Then, we can apply edge detection to the resulting image to generate bold silhouettes. The real challenge, however, lies in the computational cost of bilateral filters. We will, therefore, use some tricks to produce an acceptable cartoon effect in real time.</p>
<p>We will adhere to the following procedure to transform an RGB color image into a cartoon:</p>
<ol>
<li>First, apply a bilateral filter to reduce the color palette of the image.</li>
<li>Then, convert the original color image into grayscale.</li>
<li>After that, apply a <strong>median blur</strong> to reduce image noise.</li>
<li>Use <strong>adaptive thresholding</strong> to detect and emphasize the edges in an edge mask.</li>
<li>Finally, combine the color image from <em>step 1</em> with the edge mask from <em>step 4</em>.</li>
</ol>
<p class="mce-root">In the upcoming sections, we will learn about the previously mentioned steps in detail. First, we'll learn how to use a bilateral filter for edge-aware smoothing.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using a bilateral filter for edge-aware smoothing</h1>
                
            
            
                
<p>A strong bilateral filter is ideally suitable for converting an RGB image into a color painting or a cartoon, because it smoothens the flat regions while keeping the edges sharp. The only drawback of this filter is its computational cost—it is orders of magnitude slower than other smoothing operations, such as a Gaussian blur.</p>
<p>The first measure to take when we need to reduce the computational cost is to perform an operation on an image of low resolution. In order to downscale an RGB image (<kbd>imgRGB</kbd>) to a quarter of its size (that is, reduce the width and height to half), we could use <kbd>cv2.resize</kbd>:</p>
<pre>    img_small = cv2.resize(img_rgb, (0, 0), fx=0.5, fy=0.5) </pre>
<p>A pixel value in the resized image will correspond to the pixel average of a small neighborhood in the original image. However, this process may produce image artifacts, which is also known as <strong>aliasing</strong>. While image aliasing is a big problem on its own, the negative effect might be enhanced by subsequent processing, for example, edge detection.</p>
<p>A better alternative might be to use the <strong>Gaussian pyramid</strong> for downscaling (again to a quarter of the original size). The Gaussian pyramid consists of a blur operation that is performed before the image is resampled, which reduces any aliasing effects:</p>
<pre>    downsampled_img = cv2.pyrDown(rgb_image)</pre>
<p>However, even at this scale, the bilateral filter might still be too slow to run in real time. Another trick is to repeatedly (say, five times) apply a small bilateral filter to the image instead of applying a large bilateral filter once:</p>
<pre>    for _ in range(num_bilaterals):<br/>        filterd_small_img = cv2.bilateralFilter(downsampled_img, 9, 9, 7)</pre>
<p>The three parameters in <kbd>cv2.bilateralFilter</kbd> control the diameter of the pixel neighborhood (<kbd>d=9</kbd>) and the standard deviation of the filter in the color space (<kbd>sigmaColor=9</kbd>) and coordinate space (<kbd>sigmaSpace=7</kbd>).</p>
<p>So, the final code to run the bilateral filter that we use is as follows:</p>
<ol>
<li>Downsample the image using multiple <kbd>pyrDown</kbd> calls:</li>
</ol>
<pre style="padding-left: 60px">    downsampled_img = rgb_image<br/>    for _ in range(num_pyr_downs):<br/>        downsampled_img = cv2.pyrDown(downsampled_img)</pre>
<ol start="2">
<li>Then, apply multiple bilateral filters:</li>
</ol>
<pre style="padding-left: 60px">    for _ in range(num_bilaterals):<br/>        filterd_small_img = cv2.bilateralFilter(downsampled_img, 9, 9, 7)</pre>
<ol start="3">
<li>And finally, upsample it to the original size:</li>
</ol>
<pre style="padding-left: 60px">    filtered_normal_img = filterd_small_img<br/>    for _ in range(num_pyr_downs):<br/>        filtered_normal_img = cv2.pyrUp(filtered_normal_img)</pre>
<p>The result looks like a blurred color painting of a creepy programmer, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/96cbe5b4-6504-43aa-9795-00cd3cd0caaa.png" style="width:54.92em;height:19.58em;"/></p>
<p class="mce-root">The next section shows you how to detect and emphasize prominent edges.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Detecting and emphasizing prominent edges</h1>
                
            
            
                
<p>Again, when it comes to edge detection, the challenge often does not lie in how the underlying algorithm works, but instead lies in which particular algorithm to choose for the task at hand. You might already be familiar with a variety of edge detectors. For example, <strong>Canny edge detection</strong> (<kbd>cv2.Canny</kbd>) provides a relatively simple and effective method to detect edges in an image, but it is susceptible to noise.</p>
<p>The <strong>Sobel</strong> operator (<kbd>cv2.Sobel</kbd>) can reduce such artifacts, but it is not rotationally symmetric. The <strong>Scharr</strong> operator (<kbd>cv2.Scharr</kbd>) was targeted at correcting this but only looks at the first image derivative. If you are interested, there are even more operators for you, such as the <strong>Laplacian ridge operator</strong> (which includes the second derivative), but they are far more complex. And in the end, for our specific purposes, they might not look better, perhaps because they are as susceptible to lighting conditions as any other algorithm.</p>
<p>For the purposes of this project, we will choose a function that might not even be associated with conventional edge detection—<kbd>cv2.adaptiveThreshold</kbd>. Like <kbd>cv2.threshold</kbd>, this function uses a threshold pixel value to convert a grayscale image into a binary image. That is, if a pixel value in the original image is above the threshold, then the pixel value in the final image will be 255. Otherwise, it will be 0.</p>
<p>However, the beauty of adaptive thresholding is that it does not look at the overall properties of the image. Instead, it detects the most salient features in each small neighborhood independently, without regard to the global image characteristics. This makes the algorithm extremely robust to lighting conditions, which is exactly what we want when we seek to draw bold black outlines around objects, and people in a cartoon.</p>
<p>However, it also makes the algorithm susceptible to noise. To counteract this, we will preprocess the image with a median filter. A median filter does what its name suggests: it replaces each pixel value with the median value of all the pixels in a small pixel neighborhood. Therefore, to detect edges, we follow this short procedure:</p>
<ol>
<li>We first convert the RGB image (<kbd>rgb_image</kbd>) into grayscale (<kbd>img_gray</kbd>) and then apply a median blur with a seven-pixel local neighborhood:</li>
</ol>
<pre style="padding-left: 60px">    # convert to grayscale and apply median blur<br/>    img_gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)<br/>    img_blur = cv2.medianBlur(img_gray, 7)</pre>
<ol start="2">
<li>After reducing the noise, it is now safe to detect and enhance the edges using adaptive thresholding. Even if there is some image noise left, the <kbd>cv2.ADAPTIVE_THRESH_MEAN_C</kbd> algorithm with <kbd>blockSize=9</kbd> will ensure that the threshold is applied to the mean of a 9 x 9 neighborhood minus <kbd>C=2</kbd>:</li>
</ol>
<pre style="padding-left: 60px">    gray_edges = cv2.adaptiveThreshold(img_blur, 255,<br/>                                       cv2.ADAPTIVE_THRESH_MEAN_C,<br/>                                       cv2.THRESH_BINARY, 9, 2)</pre>
<p>The result of the adaptive thresholding looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/e3f8ac30-e43d-4040-a25d-2880426348d6.png" style="width:55.33em;height:19.75em;"/></p>
<p>Next, let's look at how to combine colors and outlines to produce a cartoon in the following section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Combining colors and outlines to produce a cartoon</h1>
                
            
            
                
<p>The last step is to combine the two previously achieved effects. Simply fuse the two effects together into a single image using <kbd>cv2.bitwise_and</kbd>. The complete function is as follows:</p>
<pre>def cartoonize(rgb_image, *,<br/>               num_pyr_downs=2, num_bilaterals=7):<br/>    # STEP 1 -- Apply a bilateral filter to reduce the color palette of<br/>    # the image.<br/>    downsampled_img = rgb_image<br/>    for _ in range(num_pyr_downs):<br/>        downsampled_img = cv2.pyrDown(downsampled_img)<br/><br/>    for _ in range(num_bilaterals):<br/>        filterd_small_img = cv2.bilateralFilter(downsampled_img, 9, 9, 7)<br/><br/>    filtered_normal_img = filterd_small_img<br/>    for _ in range(num_pyr_downs):<br/>        filtered_normal_img = cv2.pyrUp(filtered_normal_img)<br/><br/>    # make sure resulting image has the same dims as original<br/>    if filtered_normal_img.shape != rgb_image.shape:<br/>        filtered_normal_img = cv2.resize(<br/>            filtered_normal_img, rgb_image.shape[:2])<br/><br/>    # STEP 2 -- Convert the original color image into grayscale.<br/>    img_gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)<br/>    # STEP 3 -- Apply amedian blur to reduce image noise.<br/>    img_blur = cv2.medianBlur(img_gray, 7)<br/><br/>    # STEP 4 -- Use adaptive thresholding to detect and emphasize the edges<br/>    # in an edge mask.<br/>    gray_edges = cv2.adaptiveThreshold(img_blur, 255,<br/>                                       cv2.ADAPTIVE_THRESH_MEAN_C,<br/>                                       cv2.THRESH_BINARY, 9, 2)<br/>    # STEP 5 -- Combine the color image from step 1 with the edge mask<br/>    # from step 4.<br/>    rgb_edges = cv2.cvtColor(gray_edges, cv2.COLOR_GRAY2RGB)<br/>    return cv2.bitwise_and(filtered_normal_img, rgb_edges)</pre>
<p>The result looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/087be866-ebdc-44ac-a391-a39305957070.png" style="width:36.25em;height:27.17em;"/></p>
<p class="mce-root">In the next section, we'll set up the main script and design a GUI application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Putting it all together</h1>
                
            
            
                
<p>In the previous sections, we implemented a couple of nice filters that show how we can get nice effects with OpenCV. In this section, we want to build an interactive application that will allow you to apply these filters in real time to your laptop camera.</p>
<p>So, we need to write a <strong>user interface</strong> (<strong>UI</strong>) that will allow us to capture the camera stream and have some buttons so that you can select which filter you want to apply. We will start by setting up the camera capture with OpenCV. Then, we will build a nice interface around it using <kbd>wxPython</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the app</h1>
                
            
            
                
<p>To run the application, we will turn to the <kbd>chapter1.py</kbd> script. Follow these steps to do so:</p>
<ol>
<li>We first start by importing all the necessary modules:</li>
</ol>
<pre style="padding-left: 60px">import wx<br/>import cv2<br/>import numpy as np</pre>
<ol start="2">
<li>We will also have to import a generic GUI layout (from <kbd>wx_gui</kbd>) and all the designed image effects (from <kbd>tools</kbd>):</li>
</ol>
<pre style="padding-left: 60px">from wx_gui import BaseLayout<br/>from tools import apply_hue_filter<br/>from tools import apply_rgb_filters<br/>from tools import load_img_resized<br/>from tools import spline_to_lookup_table<br/>from tools import cartoonize<br/>from tools import pencil_sketch_on_canvas</pre>
<ol start="3">
<li>OpenCV provides a straightforward way to access a computer's webcam or camera device. The following code snippet opens the default camera ID (<kbd>0</kbd>) of a computer using <kbd>cv2.VideoCapture</kbd>:</li>
</ol>
<pre style="padding-left: 60px">def main(): 
    capture = cv2.VideoCapture(0) </pre>
<ol start="4">
<li>In order to give our application a fair chance to run in real time, we will limit the size of the video stream to <kbd>640</kbd> x <kbd>480</kbd> pixels:</li>
</ol>
<pre style="padding-left: 60px">    capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)<br/>    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)</pre>
<ol start="5">
<li>Then, the <kbd>capture</kbd> stream can be passed to our GUI application, which is an instance of the <kbd>FilterLayout</kbd> class:</li>
</ol>
<pre style="padding-left: 60px">    # start graphical user interface<br/>    app = wx.App()<br/>    layout = FilterLayout(capture, title='Fun with Filters')<br/>    layout.Center()<br/>    layout.Show()<br/>    app.MainLoop()</pre>
<p>After we create <kbd>FilterLayout</kbd>, we center the layout, so it appears in the center of the screen. And we call <kbd>Show()</kbd> to actually show the layout. Finally, we call <kbd>app.MainLoop()</kbd>, so the application starts working, receiving, and processing events.</p>
<p>The only thing left to do now is to design the said GUI.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Mapping the GUI base class</h1>
                
            
            
                
<p>The <kbd>FilterLayout</kbd> GUI will be based on a generic, plain layout class called <kbd>BaseLayout</kbd>, which we will be able to use in subsequent chapters as well.</p>
<p>The <kbd>BaseLayout</kbd> class is designed as an <strong>abstract base class</strong>. You can think of this class as a blueprint or recipe that will apply to all the layouts that we are yet to design, that is, a skeleton class that will serve as the backbone for all of our future GUI code.</p>
<p>We start the file by importing the packages that we will use—the <kbd>wxPython</kbd> module, which we use to create the GUI; <kbd>numpy</kbd>, which we use to do matrix manipulations; and OpenCV (of course):</p>
<pre>import numpy as np<br/>import wx<br/>import cv2</pre>
<p>The class is designed to be derived from the blueprint or skeleton, that is, the <kbd>wx.Frame</kbd> class:</p>
<pre>class BaseLayout(wx.Frame):</pre>
<p>Later on, when we write our own custom layout (<kbd>FilterLayout</kbd>), we will use the same notation to specify that the class is based on the <kbd>BaseLayout</kbd> blueprint (or skeleton) class, for example, in <kbd>class FilterLayout(BaseLayout):</kbd>. But for now, let's focus on the <kbd>BaseLayout</kbd> class.</p>
<p>An abstract class has at least one abstract method. We are going to make the method abstract by ensuring that if the method stays unimplemented, the application will not run and we throw an exception:</p>
<pre>class BaseLayout(wx.Frame):<br/>    ...<br/>    ...<br/>    ...<br/>    def process_frame(self, frame_rgb: np.ndarray) -&gt; np.ndarray:<br/>        """Process the frame of the camera (or other capture device)<br/><br/>        :param frame_rgb: Image to process in rgb format, of shape (H, W, 3)<br/>        :return: Processed image in rgb format, of shape (H, W, 3)<br/>        """<br/>        raise NotImplementedError()</pre>
<p>Then, any class that is derived from it, such as <kbd>FilterLayout</kbd>, must specify a full implementation of that method. This will allow us to create custom layouts, as you will see in a moment.</p>
<p>But first, let's proceed to the GUI constructor.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the GUI constructor</h1>
                
            
            
                
<p>The <kbd>BaseLayout</kbd> constructor accepts an ID (<kbd>-1</kbd>), a title string (<kbd>'Fun with Filters'</kbd>), a video capture object, and an optional argument that specifies the number of frames per second. Then, the first thing to do in the constructor is to try to read a frame from the captured object in order to determine the image size:</p>
<pre>    def __init__(self,<br/>                 capture: cv2.VideoCapture,<br/>                 title: str = None,<br/>                 parent=None,<br/>                 window_id: int = -1,  # default value<br/>                 fps: int = 10):<br/>        self.capture = capture
        _, frame = self._acquire_frame()<br/>        self.imgHeight, self.imgWidth = frame.shape[:2]</pre>
<p>We will use the image size to prepare a buffer that will store each video frame as a bitmap and to set the size of the GUI. Because we want to display a bunch of control buttons below the current video frame, we set the height of the GUI to <kbd>self.imgHeight + 20</kbd>:</p>
<pre>        super().__init__(parent, window_id, title,<br/>                         size=(self.imgWidth, self.imgHeight + 20))<br/>        self.fps = fps<br/>        self.bmp = wx.Bitmap.FromBuffer(self.imgWidth, self.imgHeight, frame)</pre>
<p class="mce-root">In the next section, we will build a basic layout for our application with a video stream and some buttons using <kbd>wxPython</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about a basic GUI layout</h1>
                
            
            
                
<p>The most basic layout consists of only a large black panel that provides enough room to display the video feed:</p>
<pre>        self.video_pnl = wx.Panel(self, size=(self.imgWidth, self.imgHeight))<br/>        self.video_pnl.SetBackgroundColour(wx.BLACK)</pre>
<p>In order for the layout to be extendable, we add it to a vertically arranged <kbd>wx.BoxSizer</kbd> object:</p>
<pre>        # display the button layout beneath the video stream<br/>        self.panels_vertical = wx.BoxSizer(wx.VERTICAL)<br/>        self.panels_vertical.Add(self.video_pnl, 1, flag=wx.EXPAND | wx.TOP,<br/>                                 border=1)</pre>
<p>Next, we specify an abstract method, <kbd>augment_layout</kbd>, for which we will not fill in any code. Instead, any user of our base class can make their own custom modifications to the basic layout:</p>
<pre>        self.augment_layout()</pre>
<p>Then, we just need to set the minimum size of the resulting layout and center it:</p>
<pre>        self.SetMinSize((self.imgWidth, self.imgHeight))<br/>        self.SetSizer(self.panels_vertical)<br/>        self.Centre()</pre>
<p class="mce-root">The next section shows you how to handle video streams.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Handling video streams</h1>
                
            
            
                
<p>The video stream of the webcam is handled by a series of steps that begin with the <kbd>__init__</kbd> method. These steps might appear overly complicated at first, but they are necessary in order to allow the video to run smoothly, even at higher frame rates (that is, to counteract flickering).</p>
<p>The <kbd>wxPython</kbd> module works with events and callback methods. When a certain event is triggered, it can cause a certain class method to be executed (in other words, a method can <em>bind</em> to an event). We will use this mechanism to our advantage and display a new frame every so often using the following steps:</p>
<ol>
<li>We create a timer that will generate a <kbd>wx.EVT_TIMER</kbd> event whenever <kbd>1000./self.fps</kbd> milliseconds have passed:</li>
</ol>
<pre style="padding-left: 60px">        self.timer = wx.Timer(self)<br/>        self.timer.Start(1000. / self.fps)</pre>
<ol start="2">
<li>Whenever the timer is up, we want the <kbd>_on_next_frame</kbd> method to be called. It will try to acquire a new video frame:</li>
</ol>
<pre style="padding-left: 60px">        self.Bind(wx.EVT_TIMER, self._on_next_frame)</pre>
<ol start="3">
<li>The <kbd>_on_next_frame</kbd> method will process the new video frame and store the processed frame in a bitmap. This will trigger another event, <kbd>wx.EVT_PAINT</kbd>. We want to bind this event to the <kbd>_on_paint</kbd> method, which will paint the display of the new frame. So, we create a placeholder for the video and bind <kbd>wx.EVT_PAINT</kbd> to it:</li>
</ol>
<pre style="padding-left: 60px">        self.video_pnl.Bind(wx.EVT_PAINT, self._on_paint)</pre>
<p>The <kbd>_on_next_frame</kbd> method grabs a new frame and, once done, sends the frame to another method, <kbd>process_frame</kbd>, for further processing (which is an abstract method and should be implemented by the child class):</p>
<pre>    def _on_next_frame(self, event):<br/>        """<br/>        Capture a new frame from the capture device,<br/>        send an RGB version to `self.process_frame`, refresh.<br/>        """<br/>        success, frame = self._acquire_frame()<br/>        if success:<br/>            # process current frame<br/>            frame = self.process_frame(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))<br/>            ...</pre>
<p>The processed frame (<kbd>frame</kbd>) is then stored in a bitmap buffer (<kbd>self.bmp</kbd>). Calling <kbd>Refresh</kbd> triggers the aforementioned <kbd>wx.EVT_PAINT</kbd> event, which binds to <kbd>_on_paint</kbd>:</p>
<pre>            ...<br/>            # update buffer and paint (EVT_PAINT triggered by Refresh)<br/>            self.bmp.CopyFromBuffer(frame)<br/>            self.Refresh(eraseBackground=False)</pre>
<p>The <kbd>paint</kbd> method then grabs the frame from the buffer and displays it:</p>
<pre>    def _on_paint(self, event):<br/>        """ Draw the camera frame stored in `self.bmp` onto `self.video_pnl`.<br/>        """<br/>        wx.BufferedPaintDC(self.video_pnl).DrawBitmap(self.bmp, 0, 0)</pre>
<p class="mce-root">The next section shows you how to create a custom filter layout.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Drafting a custom filter layout</h1>
                
            
            
                
<p>Now we are almost done! If we want to use the <kbd>BaseLayout</kbd> class, we need to provide code for the two methods that were previously left blank, which are as follows:</p>
<ul>
<li><kbd>augment_layout</kbd>: This is where we can make task-specific modifications to the GUI layout.</li>
<li><kbd>process_frame</kbd>: This is where we perform task-specific processing on each captured frame of the camera feed.</li>
</ul>
<p>We also need to change the constructor to initialize any parameters we will need—in this case, the canvas background for the pencil sketch:</p>
<pre>    def __init__(self, *args, **kwargs):<br/>        super().__init__(*args, **kwargs)<br/>        color_canvas = load_img_resized('pencilsketch_bg.jpg',<br/>                                        (self.imgWidth, self.imgHeight))<br/>        self.canvas = cv2.cvtColor(color_canvas, cv2.COLOR_RGB2GRAY)</pre>
<p>To customize the layout, we arrange a number of radio buttons horizontally—one button per image effect mode. Here, the <kbd>style=wx.RB_GROUP</kbd> option makes sure that only one of <kbd>radio buttons</kbd> can be selected at a time. And to make these changes visible, <kbd>pnl</kbd> needs to be added to a list of existing panels—<kbd>self.panels_vertical</kbd>:</p>
<pre>    def augment_layout(self):<br/>        """ Add a row of radio buttons below the camera feed. """<br/><br/>        # create a horizontal layout with all filter modes as radio buttons<br/>        pnl = wx.Panel(self, -1)<br/>        self.mode_warm = wx.RadioButton(pnl, -1, 'Warming Filter', (10, 10),<br/>                                        style=wx.RB_GROUP)<br/>        self.mode_cool = wx.RadioButton(pnl, -1, 'Cooling Filter', (10, 10))<br/>        self.mode_sketch = wx.RadioButton(pnl, -1, 'Pencil Sketch', (10, 10))<br/>        self.mode_cartoon = wx.RadioButton(pnl, -1, 'Cartoon', (10, 10))<br/>        hbox = wx.BoxSizer(wx.HORIZONTAL)<br/>        hbox.Add(self.mode_warm, 1)<br/>        hbox.Add(self.mode_cool, 1)<br/>        hbox.Add(self.mode_sketch, 1)<br/>        hbox.Add(self.mode_cartoon, 1)<br/>        pnl.SetSizer(hbox)<br/><br/>        # add panel with radio buttons to existing panels in a vertical<br/>        # arrangement<br/>        self.panels_vertical.Add(pnl, flag=wx.EXPAND | wx.BOTTOM | wx.TOP,<br/>                                 border=1</pre>
<p>The last method to be specified is <kbd>process_frame</kbd>. Recall that this method is triggered whenever a new camera frame is received. All that we need to do is pick the right image effect to be applied, which depends on the radio button configuration. We simply check which of the buttons is currently selected and call the corresponding <kbd>render</kbd> method:</p>
<pre>    def process_frame(self, frame_rgb: np.ndarray) -&gt; np.ndarray:<br/>        """Process the frame of the camera (or other capture device)<br/><br/>        Choose a filter effect based on the which of the radio buttons<br/>        was clicked.<br/><br/>        :param frame_rgb: Image to process in rgb format, of shape (H, W, 3)<br/>        :return: Processed image in rgb format, of shape (H, W, 3)<br/>        """<br/>        if self.mode_warm.GetValue():<br/>            return self._render_warm(frame_rgb)<br/>        elif self.mode_cool.GetValue():<br/>            return self._render_cool(frame_rgb)<br/>        elif self.mode_sketch.GetValue():<br/>            return pencil_sketch_on_canvas(frame_rgb, canvas=self.canvas)<br/>        elif self.mode_cartoon.GetValue():<br/>            return cartoonize(frame_rgb)<br/>        else:<br/>            raise NotImplementedError()</pre>
<p>And we're done! The following screenshot shows us the output pictures with different filters:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/15078696-d7c1-48e3-a98f-b0a981674cbb.png" style="width:65.92em;height:54.50em;"/></p>
<p>The preceding screenshot shows all of the four filters that we created applied to a single image.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we explored a number of interesting image processing effects. We used dodging and burning to create a black-and-white pencil sketch effect, explored lookup tables to arrive at an efficient implementation of curve filters, and got creative to produce a cartoon effect.</p>
<p>One of the techniques used was two-dimesional convolution, which takes a filter and an image and creates a new image. In this chapter, we provided the filters to get the results we wanted, but we don't always have the filters that are necessary to produce the results we want. Recently, deep learning has emerged, which tries to learn the values for different filters to help it get the results it wants.</p>
<p>In the next chapter, we will shift gears a bit and explore the use of depth sensors, such as <strong>Microsoft Kinect 3D</strong>, to recognize hand gestures in real time.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Attributions</h1>
                
            
            
                
<p><kbd>Lenna.png</kbd>—the image of Lenna is available at <a href="http://www.flickr.com/photos/15489034@N00/3388463896">http://www.flickr.com/photos/15489034@N00/3388463896</a> by Conor Lawless under the generic CC 2.0 attribution.</p>


            

            
        
    </body></html>