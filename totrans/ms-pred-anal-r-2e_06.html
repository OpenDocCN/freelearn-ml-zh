<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;6.&#xA0;Support Vector Machines"><div class="book" id="1KEEU2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Support Vector Machines</h1></div></div></div><p class="calibre8">In this chapter, we are going to take a fresh look at nonlinear predictive models by introducing support vector machines. Support vector machines, often abbreviated as SVMs, are very commonly used for classification problems, although there are certainly ways to perform function approximation and regression tasks with them. In this chapter, we will focus on the more typical case of their role in classification. To do this, we'll first present the notion of maximal margin classification, which presents an alternative formulation of how to choose between many possible classification boundaries and differs from approaches we have seen thus far, such as maximum likelihood. We'll introduce the related idea of support vectors and how, together with maximal margin classification, we can obtain a linear model in the form of a support vector classifier. Finally, we'll present how we can generalize these ideas in order to introduce nonlinearity through the use of certain functions known as kernels to finally arrive at our destination, the support vector machine.</p></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Support Vector Machines">
<div class="book" title="Maximal margin classification"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec48" class="calibre1"/>Maximal margin classification</h1></div></div></div><p class="calibre8">We'll <a id="id485" class="calibre1"/>begin this chapter by returning to a situation that should be very familiar by now: the binary classification task. Once again, we'll be thinking about the problem of how to design a model that will correctly predict whether an observation belongs to one of two possible classes. We've already seen that this task is simplest when the two classes are linearly separable; that is, when we can find a <span class="strong"><em class="calibre9">separating hyperplane</em></span> (a plane in a multidimensional space) in the space of our features so that all the observations on one side of the hyperplane belong to one class and all the observations that lie on the other side belong to the second class. Depending on the structure, assumptions, and optimizing criterion that our particular model uses, we could end up with one of infinite such hyperplanes.</p><p class="calibre8">Let's visualize this scenario using some data in a two-dimensional feature space, where the separating hyperplane is just a separating line:</p><div class="mediaobject"><img src="../images/00108.jpeg" alt="Maximal margin classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the preceding diagram, we can see two clusters of observations, each of which belongs to a <a id="id486" class="calibre1"/>different class. We've used different symbols for the two classes to denote this explicitly. Next, we show three different lines that could serve as the decision boundary of a classifier, all of which would generate 100 percent classification accuracy on the entire dataset. We'll remind ourselves that the equation of a hyperplane can be expressed as a linear combination of the input features, which are the dimensions of the space in which the hyperplane resides:</p><div class="mediaobject"><img src="../images/00109.jpeg" alt="Maximal margin classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">A separating hyperplane has this property:</p><div class="mediaobject"><img src="../images/00110.jpeg" alt="Maximal margin classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first equation simply says that the data points that belong to class 1 all lie above the hyperplane, and the second equation says that the data points that belong to class -1 all lie below the hyperplane. The subscript <span class="strong"><em class="calibre9">i</em></span> is used to index observations, and the subscript <span class="strong"><em class="calibre9">k</em></span> is used to index features, so that <span class="strong"><em class="calibre9">x<sub class="calibre14">ik</sub></em></span> means the value of the <span class="strong"><em class="calibre9">k<sup class="calibre15">th</sup></em></span> feature in the <span class="strong"><em class="calibre9">i<sup class="calibre15">th</sup></em></span> observation. We can <a id="id487" class="calibre1"/>combine these two equations into a single equation for simplicity, as follows:</p><div class="mediaobject"><img src="../images/00111.jpeg" alt="Maximal margin classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To see why this simplification works, consider an observation of the class -1 (<span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span>
<span class="strong"><em class="calibre9"> = -1</em></span>). This observation will lie below the separating hyperplane, so the linear combination in brackets will produce a negative value. Multiplying this with its <span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span> value of -1 results in a positive value. A similar argument works for observations of class 1.</p><p class="calibre8">Looking back at our diagram, note that the two dashed lines come quite close to certain observations. The solid line intuitively feels better than the other two lines as a decision boundary, as it separates the two classes without coming too close to either by traversing the center of the space between them. In this way, it distributes the space between the two classes equally. We <a id="id488" class="calibre1"/>can define a quantity known as the <span class="strong"><strong class="calibre2">margin</strong></span> that a particular separating hyperplane generates, as the smallest perpendicular distance from any point in the dataset to the hyperplane. In two dimensions and two classes, we will always have at least two points that lie at a perpendicular distance equal to the margin from the separating line, one on each side of the line. Sometimes, as is the case with our data, we may have more than two points whose perpendicular distance from the separating line equals the margin.</p><p class="calibre8">The next plot shows the margin of the solid line from the previous plot, demonstrating that we have three points at a distance equal to the margin from this separating line:</p><div class="mediaobject"><img src="../images/00112.jpeg" alt="Maximal margin classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now that <a id="id489" class="calibre1"/>we have the definition of the margin under our belt, we have a way to codify our intuition that led us to choose the solid line as the better <a id="id490" class="calibre1"/>decision boundary among the three lines that we saw in the first plot. We can go a step further and define the <span class="strong"><strong class="calibre2">maximal margin hyperplane</strong></span> as the hyperplane whose margin is the largest amongst all possible separating hyperplanes. In our 2D example, we are essentially looking for the line that will separate the two classes while at the same time being as far away from the observations as possible. It turns out that the solid line from our example is actually the maximal margin line, so that there is no other line that can be drawn with a higher margin than two units. This explains why we chose to label it as the line of maximum margin separation in our first plot.</p><p class="calibre8">In order to understand how we found the maximal margin hyperplane in our simple example, we need to formalize the problem as an optimization problem with <span class="strong"><em class="calibre9">p</em></span> features by using the following equations:</p><div class="mediaobject"><img src="../images/00113.jpeg" alt="Maximal margin classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Together, these two constraints in our optimization problem express the idea that observations in our data need to not only be correctly classified, but also lie at least <span class="strong"><em class="calibre9">M</em></span> units away from <a id="id491" class="calibre1"/>the separating hyperplane. The goal is to maximize this distance <span class="strong"><em class="calibre9">M</em></span> by appropriately choosing the coefficients <span class="strong"><em class="calibre9">β<sub class="calibre14">i</sub></em></span>. Thus, we need an optimization procedure that handles this type of problem. The details of how the optimization is actually implemented in practice are beyond the scope of this book, but we will see them in action later on when we do some programming with R.</p><p class="calibre8">We have a natural way forward now, which is to start looking at how the situation changes when our data is not linearly separable, something that we know by now is the typical scenario for a real-world dataset. Let us take a step back before doing this. We've already studied two different methods for estimating the parameters of a model: namely, maximum likelihood estimation and the least squared error criterion for linear regression. For example, when we looked at classification with logistic regression, we considered the idea of maximizing the likelihood of our data. This takes into account all of the available data points. This is also the case when classifying with multilayer perceptrons. With the maximum margin classifier, however, the construction of our decision boundary is only supported by the points that lie on the margin. Put differently, with the data in our 2D example, we can freely adjust the position of any observation except the three on the margin, and as long as the adjustment does not result in the observation falling inside the margin, our separating line will stay exactly in the same position. For this reason, we define the <a id="id492" class="calibre1"/>perpendicular vectors from the points that lie on the margin to the separating hyperplane as the <span class="strong"><strong class="calibre2">support vectors</strong></span>. Thus, we've seen that our 2D example has three support vectors. The fact that only a subset of all the points in our data essentially determines the placement of the separating hyperplane means that we have the potential to overfit our training data.</p><p class="calibre8">On the other hand, this approach does yield a couple of nice properties. We split the space between the two classes equally between them without applying any bias toward either. Points that clearly lie well inside the area of space occupied by a particular class do not play such a big role in the model compared to points on the fringes, which is where <a id="id493" class="calibre1"/>we need to place our decision boundary.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Support vector classification" id="1LCVG1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec49" class="calibre1"/>Support vector classification</h1></div></div></div><p class="calibre8">We need <a id="id494" class="calibre1"/>our data to be linearly separable in order to classify it with a maximal margin classifier. When our data is not linearly separable, we can still use the notion of support vectors that define a margin, but this time, we will allow some <a id="id495" class="calibre1"/>examples to be misclassified. Thus, we essentially define a <span class="strong"><strong class="calibre2">soft margin</strong></span>, in that some of the observations in our dataset can violate the constraint that they need to be at least as far as the margin from the separating hyperplane. It is also important to note that sometimes we may want to use a soft margin even for linearly separable data. The reason for this is in order to limit the degree of overfitting the data. Note that the larger the margin, the more confident we are about our ability to correctly classify new observations, because the classes are further apart from each other in our training data. If we achieve separation using a very small margin, we are less confident about our ability to correctly classify our data and we may, instead, want to allow a few errors and come up with a larger margin that is more robust. Study the following plot:</p><div class="mediaobject"><img src="../images/00114.jpeg" alt="Support vector classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In order <a id="id496" class="calibre1"/>to get a firmer grasp of the reason why a soft margin may be preferable to a hard margin, even for linearly separable data, we've changed our data slightly. We used the same data that we had previously, but we added an extra observation to class 1 and placed it close to the boundary of class -1. Note that with the addition of this single new data point, with feature values f1=16 and f2=40, our maximal margin line has moved drastically! The margin has been reduced from two units to 0.29 units. Looking at this graph, we are tempted to feel that the new point might either be an outlier or a mislabeling in our dataset. If we were to allow our model to make one single misclassification using a soft margin, we would go back to our previous line, which separates the two classes with a much wider margin and is less likely to have overfit on the data. We formalize the notion of our soft classifier by modifying our optimization problem set up:</p><div class="mediaobject"><img src="../images/00115.jpeg" alt="Support vector classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Under this new setup, we've introduced a new set of variables <span class="strong"><em class="calibre9">ξ</em></span>i, known as the <span class="strong"><strong class="calibre2">slack variables</strong></span>. There is one slack variable for every observation in <a id="id497" class="calibre1"/>our dataset and the value of the <span class="strong"><em class="calibre9">ξ</em></span>i slack variable depends on where the <span class="strong"><em class="calibre9">i</em></span>th observation falls with respect to the margin. When an observation is on the correct side of the separating hyperplane and outside the margin, the slack variable for that observation takes the value 0. This is the ideal situation that we have seen for all observations under a hard boundary. When an observation is correctly classified but falls at a distance within the margin, the corresponding <a id="id498" class="calibre1"/>slack variable takes a small positive value less than 1. When an observation is actually misclassified, thus falling on the wrong side of the hyperplane altogether, then its associated slack variable takes a value greater than 1. In summary, take a look at the following:</p><div class="mediaobject"><img src="../images/00116.jpeg" alt="Support vector classification" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">When an observation is incorrectly classified, the magnitude of the slack variables is proportional to the distance between that observation and the boundary of the separating hyperplane. The fact that the sum of the slack variables must be less than a constant <span class="strong"><em class="calibre9">C</em></span> means that we can think of this constant as an error budget that we are prepared to tolerate. As a misclassification of a single particular observation results in a slack variable taking at least the value 1, and our constant <span class="strong"><em class="calibre9">C</em></span> is the sum of all the slack variables, setting a value of <span class="strong"><em class="calibre9">C</em></span> less than 1 means that our model will tolerate a few observations falling inside the margin, but no misclassifications. A high value of <span class="strong"><em class="calibre9">C</em></span> often results in many observations either falling inside the margin or being misclassified, and as these are all support vectors, we end up having a greater number of support vectors. This results in a model that has a lower variance, but because we have shifted our boundary in a way that has increased tolerance to margin violations and errors, we may have a higher bias. By contrast, depending on fewer support vectors caused by having a much stricter model (and hence a <a id="id499" class="calibre1"/>lower value of <span class="strong"><em class="calibre9">C</em></span>) may result in a lower bias in our model. These support vectors, however, will individually affect the position of our boundary to a much higher degree. Consequently, we will experience a higher variance in our model performance across different training sets. Once again, the interplay between model bias and variance resurfaces in the design decisions that we must make as predictive modelers.</p></div>

<div class="book" title="Support vector classification" id="1LCVG1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Inner products"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec57" class="calibre1"/>Inner products</h2></div></div></div><p class="calibre8">The exact <a id="id500" class="calibre1"/>details of how the parameters of the support vector classifier model are computed are beyond the scope of this book. However, it turns out that the model itself can be simplified into a more convenient form that <a id="id501" class="calibre1"/>uses the <span class="strong"><strong class="calibre2">inner products</strong></span> of the observations. An inner product of two vectors of identical length, <span class="strong"><em class="calibre9">v1</em></span> and <span class="strong"><em class="calibre9">v2</em></span>, is computed by first computing the element-wise multiplication of the two vectors and then taking the sum of the resulting elements. In R, we obtain an element-wise multiplication of two vectors by simply using the multiplication symbol. So we can compute the inner product of two vectors as follows:</p><div class="informalexample"><pre class="programlisting">&gt; v1 &lt;- c(1.2, 3.3, -5.6, 4.5, 0, 9.0)
&gt; v2 &lt;- c(-3.5, 0.1, -0.2, 1.0, -8.7, 0)
&gt; v1 * v2
[1] -4.20  0.33  1.12  4.50  0.00  0.00
&gt;inner_product&lt;- sum(v1 * v2)
&gt;inner_product
[1] 1.75</pre></div><p class="calibre8">In mathematical terms, we use triangular brackets to denote the inner product operation, and we represent the process as follows:</p><div class="mediaobject"><img src="../images/00117.jpeg" alt="Inner products" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the preceding equation, for the two vectors <span class="strong"><em class="calibre9">v<sub class="calibre14">1</sub></em></span> and <span class="strong"><em class="calibre9">v<sub class="calibre14">2</sub></em></span>, the index <span class="strong"><em class="calibre9">i</em></span> is iterating over the <span class="strong"><em class="calibre9">p</em></span> features or dimensions. Now, here is the original form of our support vector classifier:</p><div class="mediaobject"><img src="../images/00118.jpeg" alt="Inner products" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This is just the standard equation for a linear combination of the input features. It turns out that for the support vector classifier, the model's solution can be expressed in terms of the inner product between the <span class="strong"><em class="calibre9">x</em></span> observation that we are trying to classify and all other <span class="strong"><em class="calibre9">x</em></span>i observations that are in our training dataset. More concretely, the form of our support vector classifier can also be written as:</p><div class="mediaobject"><img src="../images/00119.jpeg" alt="Inner products" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">For this equation, we have explicitly indicated that our model predicts <span class="strong"><em class="calibre9">y</em></span> as a function of an input observation <span class="strong"><em class="calibre9">x</em></span>. The summing function now computes a weighted sum of all the inner products <a id="id502" class="calibre1"/>of the current observation with every other observation in the dataset, which is why we are now summing across the <span class="strong"><em class="calibre9">n</em></span> observations. We want to make it very clear that we haven't changed anything in the original model itself; we have simply written two different representations of the same model. Note that we cannot assume that a linear model takes this form in general; this is only true for the support vector classifier. Now, in a real-world scenario, the number of observations in our data set, <span class="strong"><em class="calibre9">n</em></span>, is typically much greater than the number of parameters, <span class="strong"><em class="calibre9">p</em></span>, so the number of <span class="strong"><em class="calibre9">α</em></span> coefficients is seemingly larger than the number of <span class="strong"><em class="calibre9">β</em></span> coefficients. </p><p class="calibre8">Additionally, whereas in the first equation we were considering observations independently of each other, the form of the second equation shows us that to classify all our observations, we need to consider all possible pairs and compute their inner product. There are such pairs, which are of the order of <span class="strong"><em class="calibre9">n</em></span>2. Thus, it would seem like we are introducing complexity rather than producing a representation that is simpler. It turns out, however, that all <span class="strong"><em class="calibre9">α</em></span> coefficients are zero for all observations in our dataset, except those that are support vectors.</p><p class="calibre8">The number of support vectors in our dataset is typically much smaller than the total number of observations. Thus, we can simplify our new representation by explicitly showing that we sum over elements from the set of support vectors, <span class="strong"><em class="calibre9">S</em></span>, in our dataset:</p><div class="mediaobject"><img src="../images/00120.jpeg" alt="Inner products" class="calibre10"/></div><p class="calibre11"> </p></div></div>
<div class="book" title="Kernels and support vector machines" id="1MBG21-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec50" class="calibre1"/>Kernels and support vector machines</h1></div></div></div><p class="calibre8">So far, we've introduced the notion of maximum margin classification under linearly separable <a id="id503" class="calibre1"/>conditions and its extension to the support vector classifier, which still uses a hyperplane as the separating boundary but handles datasets that are not linearly separable <a id="id504" class="calibre1"/>by specifying a budget for tolerating errors. The observations that are on or within the margin, or are misclassified by the support vector classifier, are support vectors. The critical role that these play in the positioning of the decision boundary was also seen in an alternative model representation of the support vector classifier that uses inner products.</p><p class="calibre8">What is common in the situations that we have seen so far in this chapter is that our model is always linear in terms of the input features. We've seen that the ability to create models <a id="id505" class="calibre1"/>that implement nonlinear boundaries between the classes to be separated is far more flexible in terms of the different kinds of underlying target functions that they can handle. One way to introduce nonlinearity in our model that uses our new representation involving inner products is to apply a nonlinear transformation to this result. We can define a general function <span class="strong"><em class="calibre9">K</em></span>, which we'll call a <span class="strong"><strong class="calibre2">kernel function</strong></span>, that operates on two vectors and produces a scalar result. This allows us to generalize our model as follows:</p><div class="mediaobject"><img src="../images/00121.jpeg" alt="Kernels and support vector machines" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Our model <a id="id506" class="calibre1"/>now has as many features as there are support vectors, and each feature is defined as the result of a kernel acting upon the current observation and one of the support vectors. For the support vector classifier, the kernel we applied is known as the <span class="strong"><strong class="calibre2">linear kernel,</strong></span> as this just uses the inner product itself, producing a linear model.</p><div class="mediaobject"><img src="../images/00122.jpeg" alt="Kernels and support vector machines" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Kernel functions are also known as similarity functions, as we can consider the output they produce as a measure of the similarity between the two input vectors provided. We introduce nonlinearity in our model using nonlinear kernels, and when we do this, our model <a id="id507" class="calibre1"/>is known as a <span class="strong"><strong class="calibre2">support vector machine</strong></span>. There are a number of different <a id="id508" class="calibre1"/>types of nonlinear kernels. The two most common ones are the <span class="strong"><strong class="calibre2">polynomial kernel</strong></span> and the <span class="strong"><strong class="calibre2">radial basis function kernel</strong></span>. The polynomial <a id="id509" class="calibre1"/>kernel uses a power expansion of the inner product between <a id="id510" class="calibre1"/>two vectors. For a polynomial of degree <span class="strong"><em class="calibre9">d</em></span>, the form of the polynomial kernel is:</p><div class="mediaobject"><img src="../images/00123.jpeg" alt="Kernels and support vector machines" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Using this kernel, we are essentially transforming our feature space into a higher dimensional space. Computing the kernel applied to the inner product is much more efficient than first transforming all the features into a high-dimensional space and then trying to fit a linear model into that space. This is especially true when we use the <span class="strong"><strong class="calibre2">radial basis function kernel</strong></span>, often referred to simply as the <span class="strong"><strong class="calibre2">radial kernel</strong></span>, where the number of dimensions of <a id="id511" class="calibre1"/>the transformed feature space is actually infinite due to the infinite number of terms in the expansion. The form of the radial kernel is:</p><div class="mediaobject"><img src="../images/00124.jpeg" alt="Kernels and support vector machines" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Upon <a id="id512" class="calibre1"/>close inspection, we should be able to spot that the radial kernel does not use the inner product between two vectors. Instead, the summation in the exponent is just the square of the Euclidean distance between these two vectors. The radial kernel is often referred to as a <span class="strong"><strong class="calibre2">local kernel</strong></span>, because when the Euclidean distance between the two input vectors is large, the resulting value that the kernel computes is very small because of the negative sign in the exponent. Consequently, when we use a radial kernel, only vectors close to the current observation for which we want to get a prediction play a significant role in the computation. We're now ready to put all this to practice with some real-world datasets.</p></div>
<div class="book" title="Predicting chemical biodegration"><div class="book" id="1NA0K2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec51" class="calibre1"/>Predicting chemical biodegration</h1></div></div></div><p class="calibre8">In this <a id="id513" class="calibre1"/>section, we are going to use R's <code class="email">e1071</code> package to try out the models we've discussed on a real-world dataset. As our first example, we have chosen the <span class="strong"><em class="calibre9">QSARbiodegration data set</em></span>, which can be found at <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation">https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation</a>. This is a dataset <a id="id514" class="calibre1"/>containing 41 numerical variables that describe the molecular composition and properties of 1,055 chemicals. The modeling task is to predict whether a particular chemical will be biodegradable based on these properties. Example properties are the percentages of carbon, nitrogen, and oxygen atoms, <a id="id515" class="calibre1"/>as well as the number of heavy atoms in the molecule. These features are highly specialized and sufficiently numerous, so a full listing won't be given here. The complete list and further details of the quantities involved can be found on the website. For now, we've downloaded the data into a <code class="email">bdf</code> data frame:</p><div class="informalexample"><pre class="programlisting">&gt;bdf&lt;- read.table("biodeg.csv", sep = ";", quote = "\"")
&gt; head(bdf, n = 3)
     V1     V2 V3 V4 V5 V6 V7   V8 V9 V10 V11 V12   V13
1 3.919 2.6909  0  0  0  0  0 31.4  2   0   0   0 3.106
2 4.170 2.1144  0  0  0  0  0 30.8  1   1   0   0 2.461
3 3.932 3.2512  0  0  0  0  0 26.7  2   4   0   0 3.279
    V14   V15 V16   V17   V18 V19 V20 V21   V22 V23 V24
1 2.550 9.002   0 0.960 1.142   0   0   0 1.201   0   0
2 1.393 8.723   1 0.989 1.144   0   0   0 1.104   1   0
3 2.585 9.110   0 1.009 1.152   0   0   0 1.092   0   0
  V25 V26   V27    V28 V29 V30   V31 V32 V33 V34 V35
1   0   0 1.932  0.011   0   0 4.489   0   0   0   0
2   0   0 2.214 -0.204   0   0 1.542   0   0   0   0
3   0   0 1.942 -0.008   0   0 4.891   0   0   0   1
    V36   V37 V38   V39 V40 V41 V42
1 2.949 1.591   0 7.253   0   0  RB
2 3.315 1.967   0 7.257   0   0  RB
3 3.076 2.417   0 7.601   0   0  RB</pre></div><p class="calibre8">The final column, <code class="email">V42</code>, contains the output variable, which takes the value <code class="email">NRB</code> for chemicals that are not biodegradable and <code class="email">RB</code> for those that are. We'll recode this into the familiar labels of <code class="email">0</code> and <code class="email">1</code>:</p><div class="informalexample"><pre class="programlisting">&gt; levels(bdf$V42) &lt;- c(0, 1)</pre></div><p class="calibre8">Now that we have our data ready, we'll begin, as usual, by splitting them into training and testing sets, with an 80-20 split:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt;set.seed(23419002)
&gt;bdf_sampling_vector&lt;- createDataPartition(bdf$V42, p = 0.80, 
                                             list = FALSE)
&gt;bdf_train&lt;- bdf[bdf_sampling_vector,]
&gt;bdf_test&lt;- bdf[-bdf_sampling_vector,] </pre></div><p class="calibre8">There are a number of packages available in R that implement support vector machines. In this chapter, we'll explore the use of the <code class="email">e1071</code> package, which provides us with the <code class="email">svm()</code> function. If we examine our training data, we'll quickly notice that on the one hand, the scales of the various features are quite different, and on the other hand, many features are sparse features, which means that for many entries they take a zero value. It is a good idea to scale features as we did with neural networks, especially if we want to work with radial kernels. Fortunately for us, the <code class="email">svm()</code> function has a <code class="email">scale</code> parameter, which <a id="id516" class="calibre1"/>is set to <code class="email">TRUE</code> by default. This normalizes the input features so that they have zero mean and unit variance before the model is trained. This circumvents the need for us to manually carry out this preprocessing step. The first model that we will investigate will use a linear kernel:</p><div class="informalexample"><pre class="programlisting">&gt; library(e1071)
&gt;model_lin&lt;- svm(V42 ~ ., data = bdf_train, kernel = "linear", cost = 10)</pre></div><p class="calibre8">The call to the <code class="email">svm()</code> function follows the familiar paradigm of first providing a formula, then providing the name of the data frame, and finally, other parameters relevant to the model. In our case, we want to train a model where the final <code class="email">V42</code> column is the predictor column and all other columns are to be used as features. For this reason, we can just use the simple formula <code class="email">V42 ~ </code> instead of having to fully enumerate all the other columns. After specifying our data frame, we then specify the type of kernel we will use, and in this case, we've opted for a linear kernel. We'll also specify a value for the <code class="email">cost</code> parameter, which is related to the error budget C in our model:</p><div class="informalexample"><pre class="programlisting">&gt;model_lin

Call:
svm(formula = V42 ~ ., data = biodeg_training2, kernel = "linear", cost = 10)
Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  linear 
       cost:  10 
      gamma:  0.02439024 

Number of Support Vectors:  272</pre></div><p class="calibre8">Our model doesn't provide us with too much information on its performance beyond the details of the parameters that we specified. One interesting piece of information is the number of data points that were support vectors in our model; in this case, <code class="email">272</code>. If we use the <code class="email">str()</code> function to examine the structure of the fitted model, however, we will find that it contains a number of useful attributes. For example, the fitted attribute contains the model's predictions on the training data. We'll use this to gauge the quality of model fit by computing the accuracy of the training data and the confusion matrix:</p><div class="informalexample"><pre class="programlisting">&gt; mean(bdf_train[,42] == model_lin$fitted)
[1] 0.8887574
&gt; table(actual = bdf_train[,42], predictions = model_lin$fitted)
      predictions
actual   0   1
     0 519  41
     1  53 232</pre></div><p class="calibre8">We have a training accuracy of just under 89 percent, which is a decent start. Next, we'll examine the performance of the test data using the <code class="email">predict()</code> function to see whether we can get a test accuracy close to this or whether we have ended up overfitting the data:</p><div class="informalexample"><pre class="programlisting">&gt;test_predictions&lt;- predict(model_lin, bdf_test[,1:41])
&gt; mean(bdf_test[,42] == test_predictions)
[1] 0.8619048</pre></div><p class="calibre8">We do have a slightly lower test accuracy than we'd expect, but we are sufficiently close to the training accuracy we obtained earlier in order to be relatively confident that we are not in a <a id="id517" class="calibre1"/>position where we are overfitting the training data. Now, we've seen that the <code class="email">cost</code> parameter plays an important role in our model, and that choosing this involves a trade-off in model bias and variance. Consequently, we want to try different values of our <code class="email">cost</code> parameter before settling on a final model. After manually repeating the preceding code for a few values of this parameter, we obtained the following set of results:</p><div class="informalexample"><pre class="programlisting">&gt;linearPerformances
         0.01  0.1   1     10    100   1000 
training 0.858 0.888 0.883 0.889 0.886 0.886
test     0.886 0.876 0.876 0.862 0.862 0.862</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip15" class="calibre1"/>Tip</h3><p class="calibre8">Sometimes, when building a model, we may see a warning informing us that the maximum number of iterations has been reached. If this happens, we should be doubtful of the model that we produced, as it may be an indication that a solution was not found and the optimization procedure did not converge. In such a case, it is best to experiment with a different <code class="email">cost</code> value and/or kernel type.</p></div><p class="calibre8">These results show that for most values of the <code class="email">cost</code> parameter, we are seeing a very similar level of quality of fit on our training data, roughly 88 percent. Ironically, the best performance on the test data was obtained using the model whose fit on the training data was the worst, using a cost of 0.01. In short, although we have reasonable performance on our training and test datasets, the low variance in the results shown in the table essentially tells us that we are not going to get a significant improvement in the quality of fit by tweaking the <code class="email">cost</code> parameter on this particular dataset.</p><p class="calibre8">Now let's try using a radial kernel to see whether introducing some nonlinearity can allow us to improve our performance. When we specify a radial kernel, we must also specify a positive <code class="email">gamma</code> parameter. This corresponds to the <span class="strong"><em class="calibre9">1/2σ2</em></span> parameter in the equation of a radial kernel. The role that this parameter plays is that it controls the locality of the similarity computation between its two vector inputs. A large <code class="email">gamma</code> means that the kernel will produce values that are close to zero, unless the two vectors are very close together. A smaller <code class="email">gamma</code> results in a smoother kernel and takes into account pairs of vectors that are farther away. Again, this choice boils down to a trade-off between bias and variance, so just as with the <code class="email">cost</code> parameter, we'll have to try out different values of <code class="email">gamma</code>. For now, let's see how we can create a support vector machine model using a radial kernel with a specific configuration:</p><div class="informalexample"><pre class="programlisting">&gt;model_radial&lt;- svm(V42 ~ ., data = bdf_train, kernel = "radial", 
                      cost = 10, gamma = 0.5)
&gt; mean(bdf_train[,42] == model_radial$fitted)
[1] 0.9964497 
&gt;test_predictions&lt;- predict(model_radial, bdf_test[,1:41])
&gt; mean(bdf_test[,42] == test_predictions)
[1] 0.8047619</pre></div><p class="calibre8">Note that <a id="id518" class="calibre1"/>the radial kernel under these settings is able to fit the training data much more closely, as indicated by the nearly 100 percent training accuracy; but when we see the performance on the test dataset, the result is substantially lower than what we obtained on the training data. Consequently, we have a very clear indication that this model is overfitting the data. To get around this problem, we will manually experiment with a few different settings of the <code class="email">gamma</code> and <code class="email">cost</code> parameters to see whether we can improve the fit:</p><div class="informalexample"><pre class="programlisting">&gt;radialPerformances
         [,1]  [,2]  [,3] [,4]  [,5]  [,6]  [,7]  [,8]  [,9] 
cost     0.01  0.1   1    10    100   0.01  0.1   1     10   
gamma    0.01  0.01  0.01 0.01  0.01  0.05  0.05  0.05  0.05 
training 0.663 0.824 0.88 0.916 0.951 0.663 0.841 0.918 0.964
test     0.662 0.871 0.89 0.89  0.886 0.662 0.848 0.89  0.89 
         [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17]
cost     100   0.01  0.1   1     10    100   0.01  0.1  
gamma    0.05  0.1   0.1   0.1   0.1   0.1   0.5   0.5  
training 0.989 0.663 0.815 0.937 0.985 0.995 0.663 0.663
test     0.838 0.662 0.795 0.886 0.867 0.824 0.662 0.662
         [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]
cost     1     10    100   0.01  0.1   1     10    100  
gamma    0.5   0.5   0.5   1     1     1     1     1    
training 0.98  0.996 0.998 0.663 0.663 0.991 0.996 0.999
test     0.79  0.805 0.805 0.662 0.662 0.748 0.757 0.757</pre></div><p class="calibre8">As we can see, the combination of the two parameters, <code class="email">cost</code> and <code class="email">gamma</code>, yields a much wider range of results using the radial kernel. From the data frame we built previously, we can see that some combinations, such as <span class="strong"><em class="calibre9">cost = 1</em></span> and <span class="strong"><em class="calibre9">gamma = 0.05</em></span>, have brought our accuracy up to 89 percent on the test data, while still maintaining an analogous performance on our training data. Also, in the data frame, we see many examples of settings in which the training accuracy is nearly 100 percent, but the test accuracy is well below this.</p><p class="calibre8">As a result, we conclude that using a nonlinear kernel, such as the radial kernel, needs to be done with care in order to avoid overfitting. Nonetheless, radial kernels are very powerful and <a id="id519" class="calibre1"/>can be quite effective at modeling a highly nonlinear decision boundary, often allowing us to achieve higher rates of classification accuracy than a linear kernel. At this point in our analysis, we would usually want to settle on a particular value for the <code class="email">cost</code> and <code class="email">gamma</code> parameters and then retrain <a id="id520" class="calibre1"/>our model using the entire data available before deploying it in the real world.</p><p class="calibre8">Unfortunately, after using the test set to guide our decision on what parameters to use, it no longer represents an unseen dataset that would enable us to predict the model's accuracy in the real world. One possible solution to this problem is to use a validation set, but this would require us to set aside some of our data, resulting in smaller training and test set sizes.</p><p class="calibre8">
<span class="strong"><em class="calibre9">Cross-validation</em></span>, which we covered in <a class="calibre1" title="Chapter 2. Tidying Data and Measuring Performance" href="part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7">Chapter 2</a>, <span class="strong"><em class="calibre9">Tidying Data and Measuring Performance</em></span>, should be considered as a practical way out of this dilemma.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note22" class="calibre1"/>Note</h3><p class="calibre8">A very readable book on support vector machines is <span class="strong"><em class="calibre9">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</em></span> by Nello Christiani and John Shawe-Taylor. Another good reference that presents an insightful link between SVMs and a related type of neural network known as a <span class="strong"><strong class="calibre2">Radial Basis Function Network</strong></span> is <span class="strong"><em class="calibre9">Neural Networks and Learning Machines</em></span> by Simon Haykin, which we also referenced in <a class="calibre1" title="Chapter 5. Neural Networks" href="part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7">Chapter 5</a>, <span class="strong"><em class="calibre9">Neural Networks</em></span>.</p></div></div>
<div class="book" title="Predicting credit scores"><div class="book" id="1O8H62-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec52" class="calibre1"/>Predicting credit scores</h1></div></div></div><p class="calibre8">In this <a id="id521" class="calibre1"/>section, we will explore another dataset, this time in the field of banking and finance. The particular dataset in question is known as the <span class="strong"><em class="calibre9">German Credit Dataset</em></span> and is also hosted by the UCI Machine Learning Repository. The link <a id="id522" class="calibre1"/>to the data is <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29">https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29</a>.</p><p class="calibre8">The observations in the dataset are loan applications made by individuals at a bank. The goal of <a id="id523" class="calibre1"/>the data is to determine whether an application constitutes a high credit risk.</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">checking</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The status of the existing checking account</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">duration</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The duration in months</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">creditHistory</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The applicant's credit history</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">purpose</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The purpose of the loan</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">credit</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The credit amount</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">savings</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Savings account/bonds</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">employment</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Present employment since</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">installmentRate</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The installment rate (as a percentage of disposable income)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">personal</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Personal status and gender</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">debtors</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Other debtors/guarantors</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">presentResidence</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Present residence since</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">property</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The type of property</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">age</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The applicant's age in years</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">otherPlans</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Other installment plans</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">housing</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The applicant's housing situation</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">existingBankCredits</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of existing credits at this bank</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">job</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The applicant's job situation</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">dependents</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of dependents</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">telephone</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The status of the applicant's telephone</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">foreign</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Foreign worker</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">risk</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Credit risk (1 = good, 2 = bad)</p>
</td></tr></tbody></table></div><p class="calibre8">First, we will load the data into a data frame called <code class="email">german_raw</code> and provide it with column names that match the previous table:</p><div class="informalexample"><pre class="programlisting">&gt;german_raw&lt;- read.table("german.data", quote = "\"")
&gt; names(german_raw) &lt;- c("checking", "duration", "creditHistory", "purpose", "credit", "savings", "employment", "installmentRate", "personal", "debtors", "presentResidence", "property", "age", "otherPlans", "housing", "existingBankCredits", "job", "dependents", "telephone", "foreign", "risk")</pre></div><p class="calibre8">Note from <a id="id524" class="calibre1"/>the table describing the features that we have a lot of categorical features to deal with. For this reason, we will employ <code class="email">dummyVars()</code> once again to create dummy binary variables for these. In addition, we will record the <code class="email">risk</code> variable, our output, as a factor with level 0 for good credit and level 1 for bad credit:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; dummies &lt;- dummyVars(risk ~ ., data = german_raw)
&gt;german&lt;- data.frame(predict(dummies, newdata = german_raw), 
                       risk = factor((german_raw$risk - 1)))
&gt; dim(german)
[1] 1000   62</pre></div><p class="calibre8">As a result of this processing, we now have a data frame with 61 features, because several of the categorical input features had many levels. Next, we will partition our data into training and test sets:</p><div class="informalexample"><pre class="programlisting">&gt;set.seed(977)
&gt;german_sampling_vector&lt;- createDataPartition(german$risk, 
                                      p = 0.80, list = FALSE)
&gt;german_train&lt;- german[german_sampling_vector,]
&gt;german_test&lt;- german[-german_sampling_vector,]</pre></div><p class="calibre8">One particularity of this dataset that is mentioned on the website is that the data comes from a scenario where the two different types of errors have different costs. Specifically, the cost of misclassifying a high-risk customer as a low-risk customer is five times more expensive for the bank than misclassifying a low-risk customer as a high-risk customer. This is understandable, as in the first case, the bank stands to lose a lot of money from a loan it gives out that cannot be repaid, whereas in the second case, the bank misses out on an opportunity to give out a loan that will yield interest for the bank.</p><p class="calibre8">The <code class="email">svm()</code> function has a <code class="email">class.weights</code> parameter, which we use to specify the cost of misclassifying an observation to each class. This is how we will incorporate our asymmetric error cost information into our model. First, we'll create a vector of class weights, noting that we need to specify names that correspond to the output factor levels. Then, we will use the <code class="email">tune()</code> function to train various SVM models with a radial kernel:</p><div class="informalexample"><pre class="programlisting">&gt;class_weights&lt;- c(1, 5)
&gt; names(class_weights) &lt;- c("0", "1")
&gt;class_weights
0 1 
1 5
&gt;set.seed(2423)
&gt;german_radial_tune&lt;- tune(svm,risk ~ ., data = german_train, 
  kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 10, 100), 
  gamma = c(0.01, 0.05, 0.1, 0.5, 1)), class.weights = class_weights)
&gt;german_radial_tune$best.parameters
   cost gamma
9  10  0.05
&gt;german_radial_tune$best.performance
[1] 0.26</pre></div><p class="calibre8">The suggested <a id="id525" class="calibre1"/>best model has <span class="strong"><em class="calibre9">cost = 10</em></span> and <span class="strong"><em class="calibre9">gamma = 0.05</em></span> and achieves 74 percent training accuracy. Let's see how this model fares on our test dataset:</p><div class="informalexample"><pre class="programlisting">&gt;german_model&lt;- german_radial_tune$best.model
&gt;test_predictions&lt;- predict(german_model, german_test[,1:61])
&gt; mean(test_predictions == german_test[,62])
[1] 0.735
&gt; table(predicted = test_predictions, actual = german_test[,62])
         actual
predicted   0   1
        0 134  47
        1   6  13</pre></div><p class="calibre8">The performance on our test set is 73.5 percent and very close to what we saw in training. As expected, our model tends to make many more errors that misclassify a low risk customer as a high risk customer. Predictably, this takes a toll on the overall classification accuracy, which just computes the ratio of correctly classified observations to the overall number of observations. In fact, were we to remove this cost imbalance, we would actually select a different set of parameters for our model, and our performance, from the perspective of the unbiased classification accuracy, would be better:</p><div class="informalexample"><pre class="programlisting">&gt;set.seed(2423)
&gt;german_radial_tune_unbiased&lt;- tune(svm,risk ~ ., 
  data = german_train, kernel = "radial", ranges = list( 
cost = c(0.01, 0.1, 1, 10, 100), gamma = c(0.01, 0.05, 0.1, 0.5, 1)))
&gt;german_radial_tune_unbiased$best.parameters
  cost gamma
3    1  0.01
&gt;german_radial_tune_unbiased$best.performance
[1] 0.23875</pre></div><p class="calibre8">Of course, this last model will tend to make a greater number of costly misclassifications of high-risk customers as low-risk customers, which we know is very undesirable. We'll conclude <a id="id526" class="calibre1"/>this section with two final thoughts. Firstly, we have used relatively small ranges for the <code class="email">gamma</code> and <code class="email">cost</code> parameters. It is left as an exercise for the reader to rerun our analysis with a greater spread of values for these two in order to see whether we can get even better performance. This will, however, necessarily result in longer training times. Secondly, this particular dataset is quite challenging in that its baseline accuracy is actually 70 percent. This is because 70 percent of the customers in the data are low-risk customers (the two output classes are not balanced). For this reason, computing the Kappa statistic, which we saw in <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, might be a better metric to use instead of classification accuracy.</p></div>
<div class="book" title="Multiclass classification with support vector machines" id="1P71O1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec53" class="calibre1"/>Multiclass classification with support vector machines</h1></div></div></div><p class="calibre8">Just like <a id="id527" class="calibre1"/>with logistic regression, we've seen that the basic premise behind the support vector machine is that it is designed to handle two classes. Of course, we often have situations where we would like to be able to handle a greater number of classes, such as when classifying different plant species <a id="id528" class="calibre1"/>based on a variety of physical characteristics. One way to do this is the <span class="strong"><strong class="calibre2">one versus all</strong></span> approach. Here, if we have <span class="strong"><em class="calibre9">K</em></span> classes, we create <span class="strong"><em class="calibre9">K</em></span> SVM classifiers, and for each classifier, we are attempting to distinguish one particular class from all the rest. </p><p class="calibre8">To determine the best class to pick, we assign the class for which the observation produces the highest distance from the separating hyperplane, thus lying farthest away from all other classes. More formally, we pick the class for which our linear feature combination has a maximum value across all the different classifiers.</p><p class="calibre8">An alternative <a id="id529" class="calibre1"/>approach is known as the (balanced) <span class="strong"><strong class="calibre2">one versus one</strong></span> approach. We create a classifier for all possible pairs of output classes. We then classify our observation with every one of these classifiers and tally up the totals for every winning class. Finally, we pick the class that has the most votes. This latter approach is actually what is implemented by the <code class="email">svm()</code> function in the <code class="email">e1071</code> package. We can, therefore, use <a id="id530" class="calibre1"/>this function when we have a problem with multiple classes.</p></div>
<div class="book" title="Summary" id="1Q5IA1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec54" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we presented the maximal margin hyperplane as a decision boundary that is designed to separate two classes by finding the maximum distance from either of them. When the two classes are linearly separable, this creates a situation where the space between the two classes is evenly split.</p><p class="calibre8">We've seen that there are circumstances where this is not always desirable, such as when the classes are close to each other because of a few observations. An improvement to this approach is the support vector classifier that allows us to tolerate a few margin violations, or even misclassifications, in order to obtain a more stable result. This also allows us to handle classes that aren't linearly separable. The form of the support vector classifier can be written in terms of inner products between the observation that is being classified and the support vectors. This transforms our feature space from <span class="strong"><em class="calibre9">p</em></span> features into as many features as we have support vectors. Using kernel functions on these new features, we can introduce nonlinearity in our model and thus obtain a support vector machine.</p><p class="calibre8">In practice, we saw that training a support vector classifier, which is a support vector machine with a linear kernel, involves adjusting the <code class="email">cost</code> parameter. The performance we obtain on our training data can be close to what we get in our test data. By contrast, we saw that by using a radial kernel, we have the potential to fit our training data much more closely, but we are far more likely to fall into the trap of overfitting.</p><p class="calibre8">To deal with this, it is useful to try different combinations of the <code class="email">cost</code> and the <code class="email">gamma</code> parameters.</p><p class="calibre8">In the next chapter, we are going to explore another cornerstone of machine learning: tree-based models. Also known as decision trees, they can handle regression and classification problems with many classes, are highly interpretable, and have a built-in way of handling missing data.</p></div></body></html>