["```py\n03/02/2019 11:11:10 ERROR util.Shell: Failed to locate the winutils binary in the hadoop binary path\n java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n```", "```py\n<properties>\n     <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n     <java.version>1.8</java.version>\n     <jdk.version>1.8</jdk.version>\n     <spark.version>2.3.0</spark.version>\n </properties>\n```", "```py\n<dependencies>\n     <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-core_2.11</artifactId>\n         <version>${spark.version}</version>\n     </dependency>\n     <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-sql_2.11</artifactId>\n         <version>${spark.version}</version>\n         </dependency>\n     <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-mllib_2.11</artifactId>\n         <version>${spark.version}</version>\n         </dependency>\n     <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-graphx_2.11</artifactId>\n         <version>${spark.version}</version>\n     </dependency>\n     <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-yarn_2.11</artifactId>\n         <version>${spark.version}</version>\n         </dependency>\n     <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-network-shuffle_2.11</artifactId>\n         <version>${spark.version}</version>\n         </dependency>\n    <dependency>\n         <groupId>org.apache.spark</groupId>\n         <artifactId>spark-streaming-flume_2.11</artifactId>\n         <version>${spark.version}</version>\n     </dependency>\n     <dependency>\n         <groupId>com.databricks</groupId>\n         <artifactId>spark-csv_2.11</artifactId>\n         <version>1.3.0</version>\n         </dependency>\n </dependencies>\n```", "```py\nimport org.apache.spark.sql.SparkSession\nval spark = SparkSession\n      .builder // the builder itself\n      .master(\"local[4]\") // number of cores (i.e. 4, use * for all cores) \n      .config(\"spark.sql.warehouse.dir\", \"/temp\") // Spark SQL Hive Warehouse location\n      .appName(\"SparkSessionExample\") // name of the Spark application\n      .getOrCreate() // get the existing session or create a new one\n```", "```py\nval dataDF = spark.read\n      .option(\"header\", \"true\") // we read the header to know the column and structure\n      .option(\"inferSchema\", \"true\") // we infer the schema preserved in the CSV\n      .format(\"com.databricks.spark.csv\") // we're using the CSV reader from DataBricks\n      .load(\"data/inputData.csv\") // Path of the CSV file\n      .cache // [Optional] cache if necessary \n```", "```py\ndataDF.show() // show first 10 rows \ndataDF.printSchema() // shows the schema (including column name and type)\ndataDF.describe().show() // shows descriptive statistics\n```", "```py\ndataDF.createOrReplaceTempView(\"myTempDataFrame\") // create or replace a local temporary view with dataDF\ndataDF.createGlobalTempView(\"myGloDataFrame\") // create a global temporary view with dataframe dataDF\n```", "```py\nspark.sql(\"SELECT * FROM myTempDataFrame\")// will show all the records\n```", "```py\nval spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"/temp\")\n      .appName(\"CryotherapyPrediction\")\n      .getOrCreate()\n\nimport spark.implicits._\n```", "```py\nvar CryotherapyDF = spark.read.option(\"header\", \"true\")\n              .option(\"inferSchema\", \"true\")\n              .csv(\"data/Cryotherapy.csv\")\n```", "```py\nCryotherapyDF.printSchema()\n```", "```py\nCryotherapyDF.show(5)\n```", "```py\n//Spark ML algorithm expect a 'label' column, which is in our case 'Survived\". Let's rename it to 'label'\nCryotherapyDF = CryotherapyDF.withColumnRenamed(\"Result_of_Treatment\", \"label\")\nCryotherapyDF.printSchema()\n```", "```py\nval selectedCols = Array(\"sex\", \"age\", \"Time\", \"Number_of_Warts\", \"Type\", \"Area\")\n```", "```py\nval vectorAssembler = new VectorAssembler()\n          .setInputCols(selectedCols)\n          .setOutputCol(\"features\")\nval numericDF = vectorAssembler.transform(CryotherapyDF)\n                    .select(\"label\", \"features\")\nnumericDF.show()\n```", "```py\nval splits = numericDF.randomSplit(Array(0.8, 0.2))\nval trainDF = splits(0)\nval testDF = splits(1)\n```", "```py\nval dt = new DecisionTreeClassifier()\n      .setImpurity(\"gini\")\n      .setMaxBins(10)\n      .setMaxDepth(30)\n      .setLabelCol(\"label\")\n      .setFeaturesCol(\"features\")\n```", "```py\nval dtModel = dt.fit(trainDF)\n```", "```py\nval evaluator = new BinaryClassificationEvaluator()\n      .setLabelCol(\"label\")\n```", "```py\nval predictionDF = dtModel.transform(testDF)\n```", "```py\nval accuracy = evaluator.evaluate(predictionDF)\nprintln(\"Accuracy =  \" + accuracy)    \n```", "```py\nAccuracy =  0.9675436785432\n```", "```py\nspark.stop()\n```"]