<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer258">&#13;
			<h1 id="_idParaDest-287" class="chapter-number"><a id="_idTextAnchor294"/>14</h1>&#13;
			<h1 id="_idParaDest-288"><a id="_idTextAnchor295"/>Bayesian Statistics</h1>&#13;
			<p>In this chapter, we will introduce the Bayesian inference framework, covering its core components and implementation details. Bayesian inference introduces a useful framework that provides an educated guess on the predictions of the target outcome as well as quantified uncertainty estimates. Starting from a prior distribution that embeds domain expertise, the Bayesian inference approach allows us to continuously learn updated information from the data and update the posterior distribution to form a more realistic view of the <span class="No-Break">underlying parameters.</span></p>&#13;
			<p>By the end of this chapter, you will have grasped essential skills when working with the Bayesian inference framework. You will learn the core theory behind Bayes’ theorem and its use in the Bayesian linear <span class="No-Break">regression model.</span></p>&#13;
			<p>We will cover the following main topics in <span class="No-Break">this chapter:</span></p>&#13;
			<ul>&#13;
				<li>Introducing <span class="No-Break">Bayesian statistics</span></li>&#13;
				<li>Diving deeper into <span class="No-Break">Bayesian inference</span></li>&#13;
				<li>The full Bayesian <span class="No-Break">inference procedure</span></li>&#13;
				<li>Bayesian linear regression with a <span class="No-Break">categorical variable</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-289"><a id="_idTextAnchor296"/>Technical requirements</h1>&#13;
			<p>To run the code in this chapter, you will need to have the latest versions of the <span class="No-Break">following packages:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">ggplot2</strong></span><span class="No-Break">, 3.4.0</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">ggridges</strong></span><span class="No-Break">, 0.5.4</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">rjags</strong></span><span class="No-Break">, 4.13</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">coda</strong></span><span class="No-Break">, 0.19.4</span></li>&#13;
			</ul>&#13;
			<p>Please note that the versions mentioned along with the packages in the preceding list are the latest ones while I am writing <span class="No-Break">this chapter.</span></p>&#13;
			<p>All the code and data for this chapter is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_14/working.R"><span class="No-Break">https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_14/working.R</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-290"><a id="_idTextAnchor297"/>Introducing Bayesian statistics</h1>&#13;
			<p>The Bayesian approach to statistics and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) provides a logical, transparent, and interpretable<a id="_idIndexMarker1306"/> framework. This<a id="_idIndexMarker1307"/> is a uniform framework that can build problem-specific models for both statistical inference and prediction. In particular, Bayesian inference offers a method to figure out unknown or unobservable quantities given known facts (observed data), employing probability to describe the uncertainty over the possible values of unknown quantities—namely, random variables <span class="No-Break">of interest.</span></p>&#13;
			<p>Using Bayesian statistics, we are able to express our prior assumption about unknown quantities and adjust this based on the observed data. It provides the Bayesian versions of common statistical procedures such as hypothesis testing and linear regression, covered in <em class="italic">Chapters 11</em>, <em class="italic">Statistics estimation</em>, and <em class="italic">12</em>, <em class="italic">Linear Regression in R</em>. Compared to the frequentist approach, which we have adopted in all the models covered so far, the Bayesian approach additionally allows us to construct problem-specific models that can make the best use of the data in a continuous learning fashion (via the Bayesian posterior update, to be covered in the <span class="No-Break">following section).</span></p>&#13;
			<p>For example, the unknown quantities correspond to the parameters we are trying to estimate in a linear or logistic regression model. Instead of treating them as fixed quantities and using the principle of maximum likelihood to estimate their values, the Bayesian approach treats them as moving variables with their respective probability distribution of <span class="No-Break">possible values.</span></p>&#13;
			<p>Let us get a first glimpse of the famous <span class="No-Break">Bayesian theorem.</span></p>&#13;
			<h2 id="_idParaDest-291"><a id="_idTextAnchor298"/>A first look into the Bayesian theorem</h2>&#13;
			<p>The Bayesian theorem<a id="_idIndexMarker1308"/> describes the relationship between conditional probabilities of statistical quantities. In the context of linear regression, we would treat the parameter <span class="_-----MathTools-_Math_Variable">β</span> as a random variable instead of fixed quantities as we did with linear regression in <a href="B18680_12.xhtml#_idTextAnchor258"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>. For example, in a simple linear regression model <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">x</span>, instead of obtaining the single best parameter <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span> by minimizing the <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>) given the available data <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">)</span>, we would instead<a id="_idIndexMarker1309"/> treat <span class="_-----MathTools-_Math_Variable">β</span> as a random variable that follows a <span class="No-Break">specific distribution.</span></p>&#13;
			<p>Doing this involves two distributions about <span class="_-----MathTools-_Math_Variable">β</span>, the parameter of interest, <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li>The first distribution is the <strong class="bold">prior distribution</strong> <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span>, which corresponds to a subjective distribution<a id="_idIndexMarker1310"/> we assign to <span class="_-----MathTools-_Math_Variable">β</span> before we observe any data. This distribution encapsulates our prior belief about the probabilities of possible values of <span class="_-----MathTools-_Math_Variable">β</span> before we observe any <span class="No-Break">actual data.</span></li>&#13;
				<li>The second distribution is the <strong class="bold">posterior distribution</strong> <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">)</span>, which corresponds<a id="_idIndexMarker1311"/> to the updated belief about this distribution after we observe the data. This is the distribution we want to estimate through the update. Such an update is necessary in order to conform our prior belief to what we actually observe <span class="No-Break">in reality.</span></li>&#13;
			</ul>&#13;
			<p>Naturally, we would hope that the posterior distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">)</span> stays closer to what the data reflects as the training size gets large, and correspondingly stays further away from the prior belief. Here, the data refers to <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">)</span>. To proceed with the update, the data would enter as what we call the likelihood<a id="_idIndexMarker1312"/> function <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span>, also referred to as the <strong class="bold">generative model</strong>. That is, <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span> represents the likelihood (similar to probability, although in an unnormalized way) of observing the target <span class="_-----MathTools-_Math_Variable">y</span> given the input feature <span class="_-----MathTools-_Math_Variable">x</span> and parameter value <span class="_-----MathTools-_Math_Variable">β</span>, where we have treated <span class="_-----MathTools-_Math_Variable">β</span> as a specific value instead of a random variable. In other words, we would first sample from the distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span> to obtain a concrete value of <span class="_-----MathTools-_Math_Variable">β</span>, and then follow the specific observation model to obtain the actual data point <span class="_-----MathTools-_Math_Variable">y</span> given the input feature <span class="_-----MathTools-_Math_Variable">x</span>. For example, we would assume a normal distribution for the observation model if the errors are assumed to follow a <span class="No-Break">normal distribution.</span></p>&#13;
			<p>Now, we are ready to compile these three quantities together via the following <span class="No-Break">Bayesian theorem:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> is referred to as the evidence, which acts as a normalizing constant to ensure that the posterior distribution is a valid probability distribution, meaning each probability is non-negative and sums (or integrates) <span class="No-Break">to 1.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.1</em> illustrates the <span class="No-Break">Bayesian theorem:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer242" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_001.jpg" alt="Figure 14.1 – Illustrating the Bayesian theorem that calculates the posterior distribution ​P​(&lt;?AID d835?&gt;&lt;?AID df37?&gt; | x, y)​​" width="1245" height="604"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Illustrating the Bayesian theorem that calculates the posterior distribution <span class="_-----MathTools-_Math_Variable_v-bold-italic">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol_Extended">𝜷</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">y</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">)</span></p>&#13;
			<p>Note that the prior distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span> in Bayesian linear regression<a id="_idIndexMarker1313"/> could be chosen to model our prior belief about the parameter <span class="_-----MathTools-_Math_Variable">β</span>, which is something not available when using the frequentist framework in OLS-based linear regression. In practice, we often go with a normal prior distribution, but it could be any distribution that captures the prior belief about the probabilities of possible values of <span class="_-----MathTools-_Math_Variable">β</span> before we observe any data. The Bayesian framework thus allows us to incorporate the prior knowledge into the modeling in a principled manner. For example, if we believe that all features should have similar effects, we can then configure the prior distributions of the coefficients to be centered around the <span class="No-Break">same value.</span></p>&#13;
			<p>Since the parameter <span class="_-----MathTools-_Math_Variable">β</span> follows a posterior distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span>, the resulting prediction <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span> given a new input data <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span> will not be a single number, as in the case of the frequentist approach. Instead, we will obtain a series of possible values of  <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span>, which follows a posterior predictive distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span>. That is, the prediction <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span> is treated as a random variable due to the randomness in the parameter <span class="_-----MathTools-_Math_Variable">β</span>. We can then use this distribution to understand the uncertainty in the resulting predictions. For example, if the posterior predictive distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span> is wide, the resulting predictions, which are sampled from <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span>, contain a higher degree of uncertainty. On the other hand, if the distribution is narrow, the resulting predictions are more concentrated and thus more confident. The posterior distribution <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span> will also continue<a id="_idIndexMarker1314"/> to evolve as new data <span class="No-Break">becomes available.</span></p>&#13;
			<p>The next section introduces more on the <span class="No-Break">generative model.</span></p>&#13;
			<h2 id="_idParaDest-292"><a id="_idTextAnchor299"/>Understanding the generative model</h2>&#13;
			<p>In Bayesian inference, the generative model<a id="_idIndexMarker1315"/> specifies the probability distribution that governs how the data is generated. For example, when the available target data is binary, we could assume it is generated following a Bernoulli distribution with a parameter <span class="_-----MathTools-_Math_Variable">p</span> that represents the probability of success. To get a list of binary outcomes, we would first assign a probability value to <span class="_-----MathTools-_Math_Variable">p</span> and then use this Bernoulli distribution to generate binary labels by repeated sampling from <span class="No-Break">this distribution.</span></p>&#13;
			<p>Let us go through an exercise to understand the <span class="No-Break">generative process.</span></p>&#13;
			<h3>Exercise 14.1 – Generating binary outcomes</h3>&#13;
			<p>In this exercise, we will generate a list of binary outcomes<a id="_idIndexMarker1316"/> based on a Bernoulli distribution. This involves comparing a random sample from a uniform distribution valued between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> to the preset probability of success. Follow the <span class="No-Break">next steps:</span></p>&#13;
			<ol>&#13;
				<li>We begin with generating a random number in the range of <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> using a <span class="No-Break">uniform distribution:</span><pre class="source-code">&#13;
set.seed(1)&#13;
random_prob = runif(1, min = 0, max = 1)&#13;
&gt;&gt;&gt; random_prob&#13;
0.2655087</pre><p class="list-inset">Again, remember to set the random seed for <span class="No-Break">reproducibility purposes.</span></p></li>				<li>Next, we compare the number to a preset probability <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.2</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
prop_success = 0.2&#13;
&gt;&gt;&gt; random_prob &lt; prop_success&#13;
FALSE</pre><p class="list-inset">This completes the generation of a single binary number. Now, let us expand it to <span class="No-Break">10 numbers.</span></p></li>				<li>With the help of the following code, we’ll generate 10 <span class="No-Break">binary numbers:</span><pre class="source-code">&#13;
n_samples = 10&#13;
data = c()&#13;
for(sample_idx in 1:n_samples) {&#13;
  data[sample_idx] &lt;- runif(1, min = 0, max = 1) &lt; prop_success&#13;
}&#13;
&gt;&gt;&gt; data&#13;
FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE</pre><p class="list-inset">Here, we used a <strong class="source-inline">for</strong> loop to repeatedly generate<a id="_idIndexMarker1317"/> a uniform random number, compare it with the preset probability of success, and then store the result in <span class="No-Break">a vector.</span></p></li>				<li>Convert the vector to numbers <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
data = as.numeric(data)&#13;
&gt;&gt;&gt; data&#13;
0 0 0 0 0 0 0 0 1 0</pre><p class="list-inset">Note that we have observed 1 instance of success out of 10 draws, despite a 20% probability of success. As the sample size increases, we would expect the empirical probability of success (10% in this case) to be close to the theoretical <span class="No-Break">value (20%).</span></p></li>			</ol>&#13;
			<p>It turns out that this generative model corresponds to a binomial process or a binomial distribution, which allows us to generate binary outcomes in one shot. Specifically, we can use the <strong class="source-inline">rbinom</strong><strong class="source-inline">()</strong> function to simulate data from a binomial distribution, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
set.seed(1)&#13;
&gt;&gt;&gt; rbinom(n = n_samples, size = 1, prob = prop_success)</pre>			<p>Here, <strong class="source-inline">n</strong> is the number of samples to be generated from the generative model, <strong class="source-inline">size</strong> is the number of trials to run, and <strong class="source-inline">prob</strong> is the underlying probability of success valued between <strong class="source-inline">0.0</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">1.0</strong></span><span class="No-Break">.</span></p>&#13;
			<p>Note that we are essentially working with a known parameter <span class="_-----MathTools-_Math_Variable">p</span>, which is the probability of success. In practice, this would be an unknown parameter, something we are interested in estimating from the data. Bayesian inference<a id="_idIndexMarker1318"/> would allow us to do that, with the assistance of a prior distribution and the likelihood function, as introduced in the <span class="No-Break">following section.</span></p>&#13;
			<h2 id="_idParaDest-293"><a id="_idTextAnchor300"/>Understanding prior distributions</h2>&#13;
			<p>A prior distribution, an essential component<a id="_idIndexMarker1319"/> of Bayesian inference, represents the prior knowledge or belief about the underlying parameter before we observe the actual data. It essentially specifies the probability distribution of the parameters based on domain-specific preference or expertise. If we have a valid reason to believe that certain values of the parameters are more likely, we can choose a prior distribution that reflects <span class="No-Break">this preference.</span></p>&#13;
			<p>The prior distribution, denoted as <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span>, treats <span class="_-----MathTools-_Math_Variable">β</span> as a random variable and specifies its probability distribution. That is, it tells us which values of <span class="_-----MathTools-_Math_Variable">β</span> are more likely than others. In our running example on Bayesian linear regression, the prior distribution for <span class="_-----MathTools-_Math_Variable">β</span> is often chosen to be a multivariate Gaussian distribution. This is mainly for mathematical convenience, as the Gaussian distribution has nice properties that make it easier to work with. However, if we have no prior preference, a uniform distribution (which gives the same probability to all possible choices and is thus uninformed) could be a <span class="No-Break">good candidate.</span></p>&#13;
			<p>Note that we can also use the prior to impose a form of regularization on the model. By choosing a prior distribution that favors smaller values of the parameters (such as a Gaussian distribution centered at <strong class="source-inline">0</strong>), we can discourage the model from finding solutions with large coefficients, thus <span class="No-Break">preventing overfitting.</span></p>&#13;
			<p>In the following code snippet, we randomly<a id="_idIndexMarker1320"/> generate 10 samples of <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base">)</span> following a uniform distribution between <strong class="source-inline">0</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">0.2</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
set.seed(1)&#13;
prop_successes = runif(n_samples, min = 0.0, max = 0.2)&#13;
&gt;&gt;&gt; prop_successes&#13;
0.05310173 0.07442478 0.11457067 0.18164156 0.04033639 0.17967794 0.18893505 0.13215956 0.12582281 0.01235725</pre>			<p>When we model the probability of success <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">]</span> as a random variable, we often assign a beta distribution as the prior. For example, in the following code snippet, we generate 1,000 samples of <span class="_-----MathTools-_Math_Variable">p</span> from the beta distribution <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">B</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">35,55</span><span class="_-----MathTools-_Math_Base">)</span> using the <strong class="source-inline">rbeta()</strong> function, followed by showing its density plot after converting it to a <span class="No-Break">DataFrame format:</span></p>&#13;
			<pre class="source-code">&#13;
library(ggplot2)&#13;
# Sample 1000 draws from Beta(35,55) prior&#13;
prior_A = rbeta(n = 1000, shape1 = 35, shape2 = 55)&#13;
# Store the results in a data frame&#13;
prior_sim = data.frame(prior_A)&#13;
# Construct a density plot of the prior sample&#13;
ggplot(prior_sim, aes(x = prior_A)) +&#13;
  geom_density()</pre>			<p>Running the preceding code will generate the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.2</em>. It shows a prior concentration of the probability of success close to <strong class="source-inline">0.4</strong>, within the range of <strong class="source-inline">0</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer243" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_002.jpg" alt="Figure 14.2 – Visualizing the density plot of the prior distribution" width="730" height="602"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – Visualizing the density plot of the prior distribution</p>&#13;
			<p>The <span class="_-----MathTools-_Math_Variable">B</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base">)</span> distribution is defined on the interval from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, thus providing a natural and flexible prior to the probability<a id="_idIndexMarker1321"/> random variable. We can tune the <strong class="source-inline">Beta</strong> shape parameters <span class="_-----MathTools-_Math_Variable">a</span> and <span class="_-----MathTools-_Math_Variable">b</span> to produce alternative prior models. In the following code snippet, we compare the origina<strong class="source-inline">l</strong> <strong class="source-inline">B</strong><strong class="source-inline">e</strong><strong class="source-inline">t</strong><strong class="source-inline">a</strong><strong class="source-inline">(</strong><strong class="source-inline">35</strong><strong class="source-inline">,</strong><strong class="source-inline"> </strong><strong class="source-inline">55</strong><strong class="source-inline">)</strong> prior distributions with two alternatives: <strong class="source-inline">B</strong><strong class="source-inline">e</strong><strong class="source-inline">t</strong><strong class="source-inline">a</strong><strong class="source-inline">(</strong><strong class="source-inline">1</strong><strong class="source-inline">,</strong> <strong class="source-inline">1</strong><strong class="source-inline">)</strong> and <strong class="source-inline">B</strong><strong class="source-inline">e</strong><strong class="source-inline">t</strong><strong class="source-inline">a</strong><strong class="source-inline">(</strong><strong class="source-inline">100</strong><strong class="source-inline">,</strong> <strong class="source-inline">100</strong><strong class="source-inline">)</strong>. We then plot all three prior <span class="No-Break">distributions together:</span></p>&#13;
			<pre class="source-code">&#13;
# Sample draws from the Beta(1,1) prior&#13;
prior_B = rbeta(n = 1000, shape1 = 1, shape2 = 1)&#13;
# Sample draws from the Beta(100,100) prior&#13;
prior_C = rbeta(n = 1000, shape1 = 100, shape2 = 100)&#13;
# Combine the results in a single data frame&#13;
prior_all = data.frame(samples = c(prior_A, prior_B, prior_C),&#13;
                        priors = rep(c("A","B","C"), each = 1000))&#13;
# Plot the 3 priors&#13;
ggplot(prior_all, aes(x = samples, fill = priors)) +&#13;
  geom_density(alpha = 0.5)</pre>			<p>The preceding code returns the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.3</em>. Each prior distribution has a different preference region. For example, distribution B is close to a uniform distribution and has no specific preference, while distribution C places a strong preference for <strong class="source-inline">0.5</strong>, as indicated<a id="_idIndexMarker1322"/> by the peak value around <strong class="source-inline">0.5</strong> and a <span class="No-Break">narrow spread:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer244" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_003.jpg" alt="Figure 14.3 – Visualizing three different prior distributions" width="726" height="602"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – Visualizing three different prior distributions</p>&#13;
			<p>In the next section, we’ll introduce you to the <span class="No-Break">likelihood function.</span></p>&#13;
			<h2 id="_idParaDest-294"><a id="_idTextAnchor301"/>Introducing the likelihood function</h2>&#13;
			<p>The likelihood function<a id="_idIndexMarker1323"/> describes how likely the observed data is, given a set of fixed model parameters. In a parametric model (a model that assumes a certain set of parameters), the likelihood is the probability of the observed data as a function of the parameters. The specific form of the likelihood function depends on the distribution (more specifically, the observation model) assumed for the data. For example, if we assume the data follows a normal distribution, the likelihood function would take the form of a normal probability <span class="No-Break">density function.</span></p>&#13;
			<p>Let us look at a concrete example. Suppose we are developing a simple linear regression model with standard normal errors, expressed as <span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span>, where <span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">)</span>. Here, we have ignored the intercept term and only considered the slope. For a specific data point <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span>, we can express the likelihood <span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> as the probability evaluated at the probability density function of the <span class="No-Break">error term:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">π</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span></p>&#13;
			<p>Since the dataset consists of a total of <span class="_-----MathTools-_Math_Variable">n</span> input-output pairs, the joint likelihood of all data points can be expressed <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Π</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">π</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span></p>&#13;
			<p>In practice, we would often work with the log-likelihood after introducing the log transformation, as <span class="No-Break">shown here:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">π</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.5</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">π</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.5</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>&#13;
			<p>Compared to the objective function used in OLS-based linear regression, we find that the exponent term <span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is exactly the sum of squared errors. When using the maximum likelihood estimation procedure, these two different objective functions become equivalent to each other. In other words, we have <span class="No-Break">the following:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>&#13;
			<p>Here, we ignored the constant term <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">π</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span> at the <span class="No-Break">last step.</span></p>&#13;
			<p>Let us go through an example<a id="_idIndexMarker1324"/> of how to calculate the joint likelihood of a set of observed data. In the following code listing, we create a list of data points in <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> and a simple linear regression model with coefficient <strong class="source-inline">b</strong> to generate the predicted values in <strong class="source-inline">y_pred</strong>, along with the residual terms <span class="No-Break">in residuals:</span></p>&#13;
			<pre class="source-code">&#13;
# observed data&#13;
x = c(1, 2, 3, 4, 5)&#13;
y = c(2, 3, 5, 6, 7)&#13;
# parameter value&#13;
b = 0.8&#13;
# calculate the predicted values&#13;
y_pred = b * x&#13;
# calculate the residuals&#13;
residuals = y - y_pred</pre>			<p>We can then plug in the closed-form expression for the joint likelihood and calculate the total log-likelihood, <span class="No-Break">as follows:</span></p>&#13;
			<pre class="source-code">&#13;
log_likelihood = -0.5 * length(y) * log(2 * pi) - 0.5 * sum(residuals^2)&#13;
log_likelihood&#13;
-18.09469</pre>			<p>Let us look at another example<a id="_idIndexMarker1325"/> of the binomial model. As discussed earlier, when the underlying parameter <span class="_-----MathTools-_Math_Variable">p</span> represents the probability of success, we can use the <strong class="source-inline">rbinom()</strong> function to obtain the probability of observing a certain outcome (number of successes) in a total number of draws and with a specific probability of success. In the following code snippet, we first create a vector of probabilities to indicate different probabilities of success and calculate the corresponding likelihood in a total of 1,000 trials of sampling from a binomial model <span class="_-----MathTools-_Math_Variable">B</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10</span><span class="_-----MathTools-_Math_Base">)</span>, where 10 is the total number of draws. Lastly, we visualize all likelihood functions via a stacked density plot using the <strong class="source-inline">geom_density_ridges()</strong> function from the <span class="No-Break"><strong class="source-inline">ggridges</strong></span><span class="No-Break"> package:</span></p>&#13;
			<pre class="source-code">&#13;
library(ggridges)&#13;
# Define a vector of 1000 p values&#13;
p_grid = seq(from = 0, to = 1, length.out = 1000)&#13;
# Simulate 10 trials for each p in p_grid, each trial has 1000 samples&#13;
sim_result = rbinom(n = 1000, size = 10, prob = p_grid)&#13;
# Collect results in a data frame&#13;
likelihood_sim = data.frame(p_grid, sim_result)&#13;
# Density plots of p_grid grouped by sim_result&#13;
ggplot(likelihood_sim, aes(x = p_grid, y = sim_result, group = sim_result)) +&#13;
  geom_density_ridges()</pre>			<p>The preceding code results<a id="_idIndexMarker1326"/> in the <span class="No-Break">following output:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer245" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_004.jpg" alt="Figure 14.4 – Visualizing the stacked density plot as the likelihood functions of different sampling from the binomial distribution" width="1132" height="924"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – Visualizing the stacked density plot as the likelihood functions of different sampling from the binomial distribution</p>&#13;
			<p>The next section introduces the <span class="No-Break">posterior model.</span></p>&#13;
			<h2 id="_idParaDest-295"><a id="_idTextAnchor302"/>Introducing the posterior model</h2>&#13;
			<p>A posterior distribution<a id="_idIndexMarker1327"/> represents what we know about unknown parameters after observing the available data. It combines the prior beliefs, expressed via the prior distribution, with the evidence presented by the data, expressed in the likelihood function, to form a new distribution over the possible parameter values, which can be either discrete <span class="No-Break">or continuous.</span></p>&#13;
			<p>Based on Bayes’ theorem, the posterior distribution is proportional to the product of the prior distribution and the <span class="No-Break">likelihood function:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∝</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">P</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">P</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>&#13;
			<p>Note that we do not need to know the evidence term in the denominator when solving the optimal value of the parameter <span class="_-----MathTools-_Math_Variable">β</span>, since <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> is totally independent of <span class="_-----MathTools-_Math_Variable">β</span>. As we gather more data and update our beliefs, the posterior distribution would often become more sharply peaked around the true parameter value, indicating an increased level <span class="No-Break">of confidence.</span></p>&#13;
			<p>However, when we need to know <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> in order to calculate <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span>, the task is not so straightforward since a closed-form solution may not be available, or the parameters are multi-dimensional<a id="_idIndexMarker1328"/> and prohibit a direct calculation of nested integration. In such cases, we would often resort to numerical methods such as <strong class="bold">Markov chain Monte Carlo</strong> (<strong class="bold">MCMC</strong>) or approximate methods such as <span class="No-Break">variational inference.</span></p>&#13;
			<p>Let us go through an exercise<a id="_idIndexMarker1329"/> to understand the overall inference process. We will use the <strong class="source-inline">rjags</strong> package to do the calculations based on our <span class="No-Break">running example.</span></p>&#13;
			<h3>Exercise 14.2 – Obtaining the posterior distribution</h3>&#13;
			<p>In this exercise, we will use the <strong class="source-inline">rjags</strong> package to perform Bayesian<a id="_idIndexMarker1330"/> inference, including specifying the model architecture and obtaining the posterior distribution for the underlying parameter. Follow the <span class="No-Break">next steps:</span></p>&#13;
			<ol>&#13;
				<li>We begin with defining the likelihood function as a binomial distribution (using <strong class="source-inline">dbin</strong>) with parameters <strong class="source-inline">p</strong> for the probability of success and <strong class="source-inline">n</strong> for the total number of samples. We will also, define the prior distribution for <strong class="source-inline">p</strong> as a beta distribution (using <strong class="source-inline">dbeta</strong>) with parameters <strong class="source-inline">a</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">b</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
library(rjags)&#13;
# define the model&#13;
bayes_model = "model{&#13;
    # Likelihood model for X&#13;
    X ~ dbin(p, n)&#13;
    # Prior model for p&#13;
    p ~ dbeta(a, b)&#13;
}"&#13;
Compile the model using the bayes.model() function.&#13;
# compile the model&#13;
bayes_jags = jags.model(textConnection(bayes_model),&#13;
                        data = list(a = 1, b = 1, X = 3, n = 10))</pre><p class="list-inset">Here, we specify <strong class="source-inline">textConnection(bayes_model)</strong> to pass the model specification string to <strong class="source-inline">jags</strong>. The data argument<a id="_idIndexMarker1331"/> is a list specifying the observed data and the parameters for the <span class="No-Break">prior distribution.</span></p></li>				<li>Next, we draw samples from the <span class="No-Break">posterior distribution:</span><pre class="source-code">&#13;
# simulate the posterior&#13;
bayes_sim = coda.samples(model = bayes_jags, variable.names = c("p"), n.iter = 10000)</pre><p class="list-inset">Here, the <strong class="source-inline">coda.samples()</strong> function is used to run MCMC simulations and draw samples of the parameter. The <strong class="source-inline">n.iter</strong> argument specifies the number of iterations for the <span class="No-Break">MCMC simulation.</span></p></li>				<li>Finally, we plot <span class="No-Break">the posterior:</span><pre class="source-code">&#13;
# plot the posterior&#13;
plot(bayes_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,3))</pre><p class="list-inset">Running the preceding<a id="_idIndexMarker1332"/> code will result in the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer246" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_005.jpg" alt="Figure 14.5 – Visualizing the posterior distribution for the underlying parameter (probability of success)" width="1021" height="836"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5 – Visualizing the posterior distribution for the underlying parameter (probability of success)</p>&#13;
			<p>In the next section, we’ll dive deeper into Bayesian inference, starting by introducing the normal-normal model, a commonly used type of model in <span class="No-Break">Bayesian inference.</span></p>&#13;
			<h1 id="_idParaDest-296"><a id="_idTextAnchor303"/>Diving deeper into Bayesian inference</h1>&#13;
			<p>Bayesian inference is a statistical method<a id="_idIndexMarker1333"/> that makes use of conditional probability to update the prior beliefs about the parameters of a statistical model given the observed data. The output of Bayesian inference is a posterior distribution, which is a probability distribution that represents our updated beliefs about the parameter after observing <span class="No-Break">the data.</span></p>&#13;
			<p>When calculating the exact posterior distribution is difficult, we would often resort to MCMC, which is a technique for estimating the distribution of a random variable. It’s a method commonly used to generate samples from the posterior distribution in Bayesian inference, especially when the dimensionality of the model parameters is high, making an analytical <span class="No-Break">solution</span><span class="No-Break"><a id="_idIndexMarker1334"/></span><span class="No-Break"> intractable.</span></p>&#13;
			<p>The following section introduces the normal-normal model and uses MCMC to estimate its <span class="No-Break">posterior distribution.</span></p>&#13;
			<h2 id="_idParaDest-297"><a id="_idTextAnchor304"/>Introducing the normal-normal model</h2>&#13;
			<p>The normal-normal model<a id="_idIndexMarker1335"/> is another foundational model<a id="_idIndexMarker1336"/> in Bayesian inference. It refers to the case when the likelihood is normally distributed and the prior is also normally distributed, both following a bell-shaped curve. This type of model is often used in Bayesian statistics as it has a closed-form solution for the posterior distribution, which also happens to be <span class="No-Break">normally distributed.</span></p>&#13;
			<p>Let us look at a concrete exercise of the normal-normal model using MCMC-based <span class="No-Break">Bayesian inference.</span></p>&#13;
			<h3>Exercise 14.3 – Working with the normal-normal model</h3>&#13;
			<p>In this exercise, we will define<a id="_idIndexMarker1337"/> a normal likelihood function whose mean follows a normal prior and whose standard deviation follows a uniform prior. We will then use <strong class="source-inline">rjags</strong> to obtain the posterior estimates for the mean and the standard deviation. Follow the <span class="No-Break">next steps:</span></p>&#13;
			<ol>&#13;
				<li>Let’s start with simulating 100 data points that follow a normal distribution with a true mean of <strong class="source-inline">2</strong> and a true standard deviation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
library(coda)&#13;
set.seed(1)&#13;
mu_true = 2&#13;
sd_true = 1&#13;
n = 100&#13;
data = rnorm(n, mean = mu_true, sd = sd_true)</pre></li>				<li>Now, we specify the model architecture, including a normal likelihood function for the data, a normal prior distribution for the mean variable, and a uniform prior for the standard deviation variable. The normal prior is parameterized by <strong class="source-inline">0</strong> and <strong class="source-inline">0.1</strong> for the mean and standard deviation, respectively, and the uniform prior ranges from <strong class="source-inline">0</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
model_string = "model {&#13;
    for (i in 1:n) {&#13;
        y[i] ~ dnorm(mu, prec)&#13;
    }&#13;
    mu ~ dnorm(0, 0.1)&#13;
    sigma ~ dunif(0, 10)&#13;
    prec &lt;- pow(sigma, -2)&#13;
}"</pre><p class="list-inset">The preceding model states<a id="_idIndexMarker1338"/> that each observation in our data (<strong class="source-inline">y[i]</strong>) follows a normal distribution with mean <strong class="source-inline">mu</strong> and precision, <strong class="source-inline">prec</strong>, (defined as the reciprocal of the variance, hence <strong class="source-inline">prec &lt;- pow(sigma, -2)</strong>). The mean <strong class="source-inline">mu</strong> follows a normal distribution with mean <strong class="source-inline">0</strong> and precision close to <strong class="source-inline">0</strong>, which corresponds to a relatively large variance and, therefore, a weak prior belief about <strong class="source-inline">mu</strong>. The standard deviation, <strong class="source-inline">sigma</strong>, follows a uniform distribution from <strong class="source-inline">0</strong> to <strong class="source-inline">10</strong>, which expresses complete uncertainty about its value between these <span class="No-Break">two bounds.</span></p></li>				<li>Next, compile the model in <strong class="source-inline">jags</strong> and burn in the Markov chain, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
data_jags = list(y = data, n = n)&#13;
model = jags.model(textConnection(model_string), data = data_jags)&#13;
update(model, 1000)  # burn-in</pre><p class="list-inset">Here, the <strong class="source-inline">burn-in</strong> period is a number of initial iterations that we discard when performing Bayesian inference, under the assumption that the resulting chain may not have converged during this period. The idea is to let the Markov chain burn in until it reaches a distribution that is stable and reflective of the posterior distribution we are interested in. In this case, we would discard the first 1,000 iterations of the Markov chain, and these are not used in the <span class="No-Break">subsequent analysis.</span></p></li>				<li>Here, we generate samples<a id="_idIndexMarker1339"/> from the <span class="No-Break">posterior distribution:</span><pre class="source-code">&#13;
params = c("mu", "sigma")&#13;
samples = coda.samples(model, params, n.iter = 10000)&#13;
# print summary statistics for the posterior samples&#13;
&gt;&gt;&gt; summary(samples)&#13;
Iterations = 2001:12000&#13;
Thinning interval = 1&#13;
Number of chains = 1&#13;
Sample size per chain = 10000&#13;
1. Empirical mean and standard deviation for each variable,&#13;
   plus standard error of the mean:&#13;
        Mean      SD  Naive SE Time-series SE&#13;
mu    2.1066 0.09108 0.0009108      0.0009108&#13;
sigma 0.9097 0.06496 0.0006496      0.0008996&#13;
2. Quantiles for each variable:&#13;
       2.5%    25%    50%    75% 97.5%&#13;
mu    1.929 2.0453 2.1064 2.1666 2.287&#13;
sigma 0.792 0.8652 0.9055 0.9513 1.046</pre><p class="list-inset">Here, the <strong class="source-inline">summary()</strong> function provides<a id="_idIndexMarker1340"/> useful summary statistics for the posterior samples of <strong class="source-inline">mu</strong> and <strong class="source-inline">sigma</strong>, including their means, medians, and <span class="No-Break">credible intervals.</span></p></li>				<li>Finally, let’s visualize the results by running the <span class="No-Break">following command:</span><pre class="source-code">&#13;
&gt;&gt;&gt; plot(samples)</pre><p class="list-inset">Running the preceding line of code will generate the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.6</em>, which shows trace plots and density plots for the posterior samples of <strong class="source-inline">mu</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">sigma</strong></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer247" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_006.jpg" alt="Figure 14.6 – Visualizing the trace plots and density plots for the posterior samples" width="1358" height="1164"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – Visualizing the trace plots and density plots for the posterior samples</p>&#13;
			<p>In the next section, we will discuss more on <span class="No-Break">the MCMC.</span></p>&#13;
			<h2 id="_idParaDest-298"><a id="_idTextAnchor305"/>Introducing MCMC</h2>&#13;
			<p>A Markov chain<a id="_idIndexMarker1341"/> is a mathematical model that transits from one state to another within a finite or countable number of possible states. It is a sequence of random variables where the future state depends only on the present state<a id="_idIndexMarker1342"/> and not on the sequence<a id="_idIndexMarker1343"/> of events before it. This property is known as the <strong class="bold">Markov property</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="bold">memorylessness</strong></span><span class="No-Break">.</span></p>&#13;
			<p>In the context of statistical inference, we can sample from complex probability distributions and create models of sequence data via Monte Carlo simulations, and thus the term MCMC. MCMC algorithms construct a Markov chain of parameter values where the stationary distribution of the chain is the posterior distribution of the underlying parameters. The chain is generated by iteratively proposing new parameter values and accepting or rejecting these candidate values based on a preset rule, ensuring that the samples converge to the <span class="No-Break">posterior distribution.</span></p>&#13;
			<p>Let us analyze the details of the previous<a id="_idIndexMarker1344"/> MCMC chain. In the following code snippet, we convert the chain to a DataFrame and print its first <span class="No-Break">few rows:</span></p>&#13;
			<pre class="source-code">&#13;
# Store the chains in a data frame&#13;
mcmc_chains &lt;- data.frame(samples[[1]], iter = 1:10000)&#13;
# Check out the head&#13;
&gt;&gt;&gt; head(mcmc_chains)&#13;
        mu     sigma iter&#13;
1 2.159540 0.8678513    1&#13;
2 2.141280 0.8719263    2&#13;
3 1.975057 0.8568497    3&#13;
4 2.054670 0.9313297    4&#13;
5 2.144810 1.0349093    5&#13;
6 2.001104 1.0597861    6</pre>			<p>These are MCMC samples generated to approximate the posterior distribution for <strong class="source-inline">mu</strong> and <strong class="source-inline">sigma</strong>, respectively. Each sample depends on the previous sample only and is unrelated to other prior samples. We can plot these samples in a line plot called a trace plot, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
# Use plot() to construct trace plots&#13;
&gt;&gt;&gt; plot(samples, density = FALSE)</pre>			<p>Running the preceding code generates the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.7</em>. A trace plot is a commonly used diagnostic tool in MCMC. It is a graphical representation of the values of the samples at each iteration or step of the MCMC algorithm. In this case, the trace plot shows no obvious trend, suggesting that both chains have <span class="No-Break">converged stably:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer248" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_007.jpg" alt="Figure 14.7 – Visualizing the trace plots of MCMC chains" width="862" height="517"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.7 – Visualizing the trace plots of MCMC chains</p>&#13;
			<p>Let us observe<a id="_idIndexMarker1345"/> the first 100 samples using <strong class="source-inline">ggplot()</strong> <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">mu</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
# Trace plot the first 100 iterations of the mu chain&#13;
&gt;&gt;&gt; ggplot(mcmc_chains[1:100, ], aes(x = iter, y = mu)) +&#13;
  geom_line() +&#13;
  theme(axis.title.x = element_text(size = 20),  # Increase x-axis label size&#13;
        axis.title.y = element_text(size = 20))  # Increase y-axis label size</pre>			<p>Running the preceding code generates the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.8</em>. We can see that the sampler moves to another region after sufficiently exploring the <span class="No-Break">previous region:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer249" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_008.jpg" alt="Figure 14.8 – Visualizing the first 100 iterations of the mu chain" width="1189" height="813"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.8 – Visualizing the first 100 iterations of the mu chain</p>&#13;
			<p>Note that we can also display<a id="_idIndexMarker1346"/> the density plot only by setting <strong class="source-inline">trace = FALSE</strong>, <span class="No-Break">as follows:</span></p>&#13;
			<pre class="source-code">&#13;
# Use plot() to construct density plots&#13;
&gt;&gt;&gt; plot(samples, trace = FALSE)</pre>			<p>The preceding code returns the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer250" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_009.jpg" alt="Figure 14.9 – Visualizing the density plot of both chains" width="1357" height="683"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.9 – Visualizing the density plot of both chains</p>&#13;
			<p>In practice, we would often run multiple chains in MCMC. This allows us to check whether all chains converge to the same distribution, which is a critical step to ensure that MCMC sampling is done correctly. Since each chain in MCMC starts at a different initial point, running multiple chains could help check whether the posterior distribution is dependent on the starting values. When all chains converge to the same distribution regardless of the initial points, we have a higher level of confidence in the stability and robustness of the <span class="No-Break">MCMC process.</span></p>&#13;
			<p>The following code snippet runs MCMC over <span class="No-Break">four chains:</span></p>&#13;
			<pre class="source-code">&#13;
model2 = jags.model(textConnection(model_string), data = data_jags, n.chains = 4)&#13;
# simulate the posterior&#13;
samples2 &lt;- coda.samples(model = model2, variable.names = params, n.iter = 1000)</pre>			<p>We can check the trace plot <span class="No-Break">as follows:</span></p>&#13;
			<pre class="source-code">&#13;
# Construct trace plots&#13;
&gt;&gt;&gt; plot(samples2, density = FALSE)</pre>			<p>As shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.10</em>, all four chains have more or less converged to a <span class="No-Break">stable state:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer251" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_010.jpg" alt="Figure 14.10 – Visualizing the density plot of both chains" width="1351" height="679"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.10 – Visualizing the density plot of both chains</p>&#13;
			<p>Finally, we can check <a id="_idIndexMarker1347"/>the summary of the MCMC samples, <span class="No-Break">as follows:</span></p>&#13;
			<pre class="source-code">&#13;
&gt;&gt;&gt; summary(samples2)&#13;
Iterations = 1001:2000&#13;
Thinning interval = 1&#13;
Number of chains = 4&#13;
Sample size per chain = 1000&#13;
1. Empirical mean and standard deviation for each variable,&#13;
   plus standard error of the mean:&#13;
        Mean      SD Naive SE Time-series SE&#13;
mu    2.1052 0.09046  0.00143        0.00144&#13;
sigma 0.9089 0.06454  0.00102        0.00134&#13;
2. Quantiles for each variable:&#13;
        2.5%    25%   50%    75% 97.5%&#13;
mu    1.9282 2.0456 2.104 2.1655 2.282&#13;
sigma 0.7952 0.8626 0.906 0.9522 1.041</pre>			<p>In the next section, we will cover the full Bayesian inference procedure, including quantifying the posterior uncertainty and making predictions based on the posterior distribution of <span class="No-Break">the parameters.</span></p>&#13;
			<h1 id="_idParaDest-299"><a id="_idTextAnchor306"/>The full Bayesian inference procedure</h1>&#13;
			<p>The full Bayesian inference<a id="_idIndexMarker1348"/> starts by specifying the model architecture, including the prior distribution for unknown (unobserved) parameters and the likelihood function that determines how the data is generated. We can then perform MCMC to infer the posterior distribution of these parameters given the observed dataset. Finally, we can use the posterior distribution to either quantify the uncertainty about these parameters or make predictions for new input data with quantified uncertainty<a id="_idIndexMarker1349"/> about <span class="No-Break">the predictions.</span></p>&#13;
			<p>The following exercise illustrates this process using the <span class="No-Break"><strong class="source-inline">mtcars</strong></span><span class="No-Break"> dataset.</span></p>&#13;
			<h3>Exercise 14.4 – Performing full Bayesian inference</h3>&#13;
			<p>In this exercise, we will perform Bayesian linear regression<a id="_idIndexMarker1350"/> with a single feature and two unknown parameters: <strong class="source-inline">intercept</strong> and <strong class="source-inline">slope</strong>. The model looks at the relationship between car weight (<strong class="source-inline">wt</strong>) and horsepower (<strong class="source-inline">hp</strong>) in the <strong class="source-inline">mtcars</strong> dataset. Follow the <span class="No-Break">next steps:</span></p>&#13;
			<ol>&#13;
				<li>Specify a Bayesian inference model where each target <strong class="source-inline">wt</strong> is modeled as a realization of a random variable following a normal distribution. The <strong class="source-inline">mean</strong> parameter is a linear combination of two parameters (<strong class="source-inline">a</strong> for <strong class="source-inline">intercept</strong> and <strong class="source-inline">b</strong> for <strong class="source-inline">slope</strong>) with the corresponding input feature. Both <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> follow a normal distribution, and the <strong class="source-inline">variance</strong> parameter follows a uniform distribution. The code is illustrated in the <span class="No-Break">following snippet:</span><pre class="source-code">&#13;
# load the necessary libraries&#13;
library(rjags)&#13;
library(coda)&#13;
# define the model&#13;
model = "model{&#13;
    # Define model for data Y[i]&#13;
    for(i in 1:length(Y)) {&#13;
      Y[i] ~ dnorm(m[i], s^(-2))&#13;
      m[i] &lt;- a + b * X[i]&#13;
    }&#13;
    # Define the a, b, s priors&#13;
    a ~ dnorm(0, 0.5^(-2))&#13;
    b ~ dnorm(1, 0.5^(-2))&#13;
    s ~ dunif(0, 20)&#13;
}"</pre></li>				<li>Compile the model with three chains and control<a id="_idIndexMarker1351"/> the random seed for <span class="No-Break">model reproducibility:</span><pre class="source-code">&#13;
# compile the model&#13;
model = jags.model(textConnection(model),&#13;
                    data = list(Y = mtcars$wt, X = mtcars$hp),&#13;
                    n.chains = 3,&#13;
                    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))</pre></li>				<li>Run a burn-in period of <span class="No-Break"><strong class="source-inline">three</strong></span><span class="No-Break"> iterations:</span><pre class="source-code">&#13;
# burn-in&#13;
update(model, 1000)</pre></li>				<li>MCMC samples from the posterior distribution of the parameters, as <span class="No-Break">seen here:</span><pre class="source-code">&#13;
# generate MCMC samples&#13;
samples = coda.samples(model, variable.names = c("a", "b", "s"), n.iter = 5000)</pre></li>				<li>Create a trace plot of the MCMC samples to assess convergence, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
# check convergence using trace plot&#13;
&gt;&gt;&gt; plot(samples)</pre><p class="list-inset">Running the preceding code generates the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.11</em>, suggesting a decent convergence for all <span class="No-Break">three parameters:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer252" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_011.jpg" alt="Figure 14.11 – Visualizing the trace plots of all three parameters" width="1034" height="933"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.11 – Visualizing the trace plots of all three parameters</p>&#13;
			<ol>&#13;
				<li value="6">Calculate the mean of the posterior<a id="_idIndexMarker1352"/> distributions for parameters <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong>, and use these mean values to make point predictions and plot the prediction line on a scatterplot of the data, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
# Get the posterior estimates&#13;
posterior_estimates = summary(samples)&#13;
# Calculate the mean for each parameter&#13;
a_mean = posterior_estimates$statistics["a", "Mean"]&#13;
b_mean = posterior_estimates$statistics["b", "Mean"]&#13;
# Plot the prediction line&#13;
ggplot(mtcars, aes(x = hp, y = wt)) +&#13;
  geom_point() +&#13;
  geom_abline(intercept = a_mean, slope = b_mean) +&#13;
  labs(title = "Bayesian Linear Regression",&#13;
       x = "Horsepower",&#13;
       y = "Weight") +&#13;
  theme(plot.title = element_text(hjust = 0.5))</pre><p class="list-inset">Here, we use the mean of the posterior distribution<a id="_idIndexMarker1353"/> as the parameter value to make a point prediction for each input feature. The commands generate the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer253" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_012.jpg" alt="Figure 14.12 – Making points predictions using the mean value of the posterior distributions for each parameter" width="752" height="562"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.12 – Making points predictions using the mean value of the posterior distributions for each parameter</p>&#13;
			<p class="list-inset">It is worth noting that Bayesian linear regression offers quantified uncertainty compared to frequentist linear regression covered in previous chapters. Such uncertainty comes in the form of credible intervals, which differ from the confidence interval. Specifically, we obtain the credible interval by treating the parameter as a random variable and the data as a fixed quantity, which makes more sense as we only get to observe the data once in <span class="No-Break">most cases.</span></p>&#13;
			<ol>&#13;
				<li value="7">Compute the <strong class="bold">highest posterior density interval</strong> (<strong class="bold">HPDI</strong>) for parameters <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> using the <strong class="source-inline">HPDinterval()</strong> function and plot<a id="_idIndexMarker1354"/> the confidence<a id="_idIndexMarker1355"/> intervals in the histogram, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
# Extract samples&#13;
a_samples = as.matrix(samples[, "a"])&#13;
b_samples = as.matrix(samples[, "b"])&#13;
# Calculate credible intervals&#13;
a_hpd = coda::HPDinterval(coda::as.mcmc(a_samples))&#13;
b_hpd = coda::HPDinterval(coda::as.mcmc(b_samples))&#13;
# Plot histograms and credible intervals&#13;
par(mfrow=c(2,1))  # Create 2 subplots&#13;
# Parameter a&#13;
hist(a_samples, freq=FALSE, xlab="a", main="Posterior distribution of a", col="lightgray")&#13;
abline(v=a_hpd[1,1], col="red", lwd=2)  # Lower limit of the credible interval&#13;
abline(v=a_hpd[1,2], col="red", lwd=2)  # Upper limit of the credible interval&#13;
# Parameter b&#13;
hist(b_samples, freq=FALSE, xlab="b", main="Posterior distribution of b", col="lightgray")&#13;
abline(v=b_hpd[1,1], col="red", lwd=2)  # Lower limit of the credible interval&#13;
abline(v=b_hpd[1,2], col="red", lwd=2)  # Upper limit of the credible interval</pre><p class="list-inset">Running the commands generates the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.13</em>. Here, we calculate the 95% credible interval <span class="No-Break">by default:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer254" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_013.jpg" alt="Figure 14.13 – Visualizing the posterior distribution and credible interval of the model parameters" width="1038" height="830"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.13 – Visualizing the posterior distribution and credible interval of the model parameters</p>&#13;
			<ol>&#13;
				<li value="8">Finally, make posterior predictions<a id="_idIndexMarker1356"/> for a new input value, 120, and plot the posterior predictive distribution based on the list of posterior samples for the parameters, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
# make posterior predictions&#13;
# Obtain the mean values of the MCMC samples for each parameter&#13;
a_mean = mean(samples[[1]][,"a"])&#13;
b_mean = mean(samples[[1]][,"b"])&#13;
# New input (e.g., horsepower = 120)&#13;
new_input = 120&#13;
# Prediction&#13;
predicted_weight = a_mean + b_mean * new_input&#13;
print(predicted_weight)&#13;
# Predictive distribution&#13;
predicted_weights = samples[[1]][,"a"] + samples[[1]][,"b"] * new_input&#13;
# Plot the predictive distribution&#13;
&gt;&gt;&gt; hist(predicted_weights, breaks = 30, main = "Posterior predictive distribution", xlab = "Predicted weight")</pre><p class="list-inset">Running the commands generates the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.14</em>. This posterior predictive distribution<a id="_idIndexMarker1357"/> captures the model’s uncertainty about <span class="No-Break">the prediction:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer255" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_014.jpg" alt="Figure 14.14 – Visualizing the posterior predictive distribution for a new input feature" width="756" height="375"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.14 – Visualizing the posterior predictive distribution for a new input feature</p>&#13;
			<p>The next section covers a Bayesian linear regression model using a categorical <span class="No-Break">input variable.</span></p>&#13;
			<h1 id="_idParaDest-300"><a id="_idTextAnchor307"/>Bayesian linear regression with a categorical variable</h1>&#13;
			<p>When the predictor is categorical, such as a binary<a id="_idIndexMarker1358"/> feature, we would set one<a id="_idIndexMarker1359"/> parameter for each corresponding category. The following exercise demonstrates such <span class="No-Break">an example.</span></p>&#13;
			<h3>Exercise 14.5 – Performing Bayesian inference with a categorical variable</h3>&#13;
			<p>In this exercise, we will examine<a id="_idIndexMarker1360"/> the relationship<a id="_idIndexMarker1361"/> between <strong class="source-inline">am</strong> (automatic or manual transmission, a categorical variable) and <strong class="source-inline">mpg</strong> (miles per gallon, a continuous variable). We will define the mean of the normal likelihood for <strong class="source-inline">mpg</strong> as a function of <strong class="source-inline">am</strong>, with a different mean <strong class="source-inline">mu[i]</strong> for each level of <strong class="source-inline">am</strong>. We’ll also give <strong class="source-inline">mu</strong> a normal prior and standard deviation <strong class="source-inline">s</strong> a uniform prior. Follow the <span class="No-Break">next steps:</span></p>&#13;
			<ol>&#13;
				<li>Specify the aforementioned model architecture, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
# define the model&#13;
model = "model{&#13;
    # Define model for data Y[i]&#13;
    for(i in 1:length(Y)) {&#13;
      Y[i] ~ dnorm(mu[am[i]+1], s^(-2))&#13;
    }&#13;
    # Define the mu, s priors&#13;
    for(j in 1:2){&#13;
      mu[j] ~ dnorm(20, 10^(-2))&#13;
    }&#13;
    s ~ dunif(0, 20)&#13;
}"</pre></li>				<li>Compile the model, generate the posterior samples, and show the <span class="No-Break">convergence plots:</span><pre class="source-code">&#13;
# compile the model&#13;
model = jags.model(textConnection(model),&#13;
                    data = list(Y = mtcars$mpg, am = mtcars$am),&#13;
                    n.chains = 3,&#13;
                    inits = list(.RNG.name = "base::Wichmann-Hill", .RNG.seed = 100))&#13;
# burn-in&#13;
update(model, 1000)&#13;
# generate MCMC samples&#13;
samples = coda.samples(model, variable.names = c("mu", "s"), n.iter = 5000)&#13;
# check convergence using trace plot&#13;
&gt;&gt;&gt; plot(samples)</pre><p class="list-inset">Running the preceding<a id="_idIndexMarker1362"/> code generates<a id="_idIndexMarker1363"/> the output shown in <span class="No-Break"><em class="italic">Figure 14</em></span><em class="italic">.15</em>, suggesting a decent convergence of all <span class="No-Break">model parameters:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer256" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_015.jpg" alt="Figure 14.15 – Visualizing the convergence plots" width="752" height="858"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.15 – Visualizing the convergence plots</p>&#13;
			<ol>&#13;
				<li value="3">Plot the distribution<a id="_idIndexMarker1364"/> of the dataset along with the mean estimate<a id="_idIndexMarker1365"/> of parameters for both levels of the categorical variable, <span class="No-Break">as follows:</span><pre class="source-code">&#13;
# Get the posterior estimates&#13;
posterior_estimates = summary(samples)&#13;
# Calculate the mean for each parameter&#13;
mu1_mean = posterior_estimates$statistics["mu[1]", "Mean"]&#13;
mu2_mean = posterior_estimates$statistics["mu[2]", "Mean"]&#13;
# Plot the prediction line&#13;
ggplot(mtcars, aes(x = as.factor(am), y = mpg)) +&#13;
  geom_jitter(width = 0.2) +&#13;
  geom_hline(aes(yintercept = mu1_mean, color = "Automatic"), linetype = "dashed") +&#13;
  geom_hline(aes(yintercept = mu2_mean, color = "Manual"), linetype = "dashed") +&#13;
  scale_color_manual(name = "Transmission", values = c("Automatic" = "red", "Manual" = "blue")) +&#13;
  labs(title = "Bayesian Linear Regression",&#13;
       x = "Transmission (0 = automatic, 1 = manual)",&#13;
       y = "Miles Per Gallon (mpg)") +&#13;
  theme(plot.title = element_text(hjust = 0.5),&#13;
        legend.position = "bottom")</pre><p class="list-inset">The preceding code<a id="_idIndexMarker1366"/> will generate the output<a id="_idIndexMarker1367"/> shown in <span class="No-Break"><em class="italic">Figure 14</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer257" class="IMG---Figure">&#13;
					<img src="Images/B18680_14_016.jpg" alt="Figure 14.16 – Visualizing the distribution and the mean estimates of the dataset" width="818" height="644"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.16 – Visualizing the distribution and the mean estimates of the dataset</p>&#13;
			<h1 id="_idParaDest-301"><a id="_idTextAnchor308"/>Summary</h1>&#13;
			<p>This chapter provides a comprehensive introduction to Bayesian statistics, beginning with an exploration of the fundamental Bayes’ theorem. We delved into its components, starting with understanding the generative model, which helps us simulate data and examine how changes in parameters affect the data <span class="No-Break">generation process.</span></p>&#13;
			<p>We then focused on understanding the prior distribution, an essential part of Bayesian statistics that represents our prior knowledge about an uncertain parameter. This was followed by an introduction to the likelihood function, a statistical function that determines how likely it is for a set of observations to occur given specific <span class="No-Break">parameter values.</span></p>&#13;
			<p>Next, we introduced the concept of the posterior model. This combines our prior distribution and likelihood to give a new probability distribution that represents updated beliefs after having seen the data. We also explored more complex models, such as the normal-normal model, wherein both the likelihood and the prior are normally distributed. We further investigated the mechanics of Bayesian inference through the MCMC method, a powerful tool for estimating the distribution of parameters and making predictions. A detailed walk-through of the full Bayesian inference procedure <span class="No-Break">accompanied this.</span></p>&#13;
			<p>Lastly, we discussed Bayesian linear regression with a categorical variable, which extends the methodology to models that include <span class="No-Break">categorical predictors.</span></p>&#13;
			<p>Congratulations! You’ve successfully navigated to the end of this book, a testament to your dedication and effort. This journey, hopefully enriching for you and certainly so for me, marks a significant milestone in your ongoing exploration of the dynamic field of statistics and ML. I’m honored that you chose this book as a companion in this voyage, and I trust it has laid a solid foundation for your <span class="No-Break">future pursuits.</span></p>&#13;
		</div>&#13;
	</div></body></html>