- en: Feature Construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we worked with the `Pima Indian Diabetes Prediction`
    dataset to get a better understanding of which given features in our dataset are
    most valuable. Working with the features that were available to us, we identified
    missing values within our columns and employed techniques of dropping missing
    values, imputing, and normalizing/standardizing our data to improve the accuracy
    of our machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that, up to this point, we have only worked with features
    that are quantitative. We will now shift into dealing with categorical data, in
    addition to the quantitative data that has missing values. Our main focus will
    be to work with our given features to construct entirely new features for our
    models to learn from.
  prefs: []
  type: TYPE_NORMAL
- en: There are various methods we can utilize to construct our features, with the
    most basic starting with the pandas library in Python to scale an existing feature
    by a multiples. We will be diving into some more mathematically intensive methods,
    and will employ various packages available to us through the scikit-learn library;
    we will also create our own custom classes. We will go over these classes in detail
    as we get into the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in our discussions:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining our dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing categorical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding categorical variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending numerical features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-specific feature construction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining our dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For demonstrative purposes, in this chapter, we will utilize a dataset that
    we have created, so that we can showcase a variety of data levels and types. Let's
    set up our DataFrame and dive into our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use pandas to create the DataFrame we will work with, as this is the
    primary data structure in pandas. The advantage of a pandas DataFrame is that
    there are several attributes and methods available for us to perform on our data.
    This allows us to logically manipulate the data to develop a thorough understanding
    of what we are working with, and how best to structure our machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can set up our `DataFrame X`. To do this, we will utilize the `DataFrame`
    method in pandas, which creates a tabular data structure (table with rows and
    columns). This method can take in a few types of data (NumPy arrays or dictionaries,
    to name a couple). Here, we will be passing-in a dictionary with keys as column
    headers and values as lists, with each list representing a column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us a DataFrame with four columns and six rows. Let''s print
    our DataFrame `X` and take a look at the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | like | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | somewhat like | -0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s take a look at our columns and identify our data levels and types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean`: This column is represented by binary categorical data (yes/no),
    and is at the nominal level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`city`: This column is represented by categorical data, also at the nominal
    level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ordinal_column`: As you may have guessed by the column name, this column is
    represented by ordinal data, at the ordinal level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quantitative_column`: This column is represented by integers at the ratio
    level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imputing categorical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have an understanding of the data we are working with, let''s take
    a look at our missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we can use the `isnull` method available to us in pandas for DataFrames.
    This method returns a `boolean` same-sized object indicating if the values are
    null.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will then `sum` these to see which columns have missing data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that three of our columns are missing values. Our course of
    action will be to impute these missing values.
  prefs: []
  type: TYPE_NORMAL
- en: If you recall, we implemented scikit-learn's `Imputer` class in a previous chapter
    to fill in numerical data. `Imputer` does have a categorical option, `most_frequent`,
    however it only works on categorical data that has been encoded as integers.
  prefs: []
  type: TYPE_NORMAL
- en: We may not always want to transform our categorical data this way, as it can
    change how we interpret the categorical information, so we will build our own
    transformer. By transformer, we mean a method by which a column will impute missing
    values.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, we will build several custom transformers in this chapter, as they
    are quite useful for making transformations to our data, and give us options that
    are not readily available in pandas or scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with our categorical column, `city`. Just as we have the strategy
    of imputing the mean value to fill missing rows for numerical data, we have a
    similar method for categorical data. To impute values for categorical data, fill
    missing rows with the most common category.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we will need to find out what the most common category is in our
    `city` column:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we need to specify the column we are working with to employ a method
    called `value_counts`. This will return an object that will be in descending order
    so that the first element is the most frequently-occurring element.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will grab only the first element in the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that `tokyo` appears to be the most common city. Now that we know
    which value to use to impute our missing rows, let''s fill these slots. There
    is a `fillna` function that allows us to specify exactly how we want to fill missing
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `city` column now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Great, now our `city` column no longer has missing values. However, our other
    categorical column, `boolean`, still does. Rather than going through the same
    method, let's build a custom imputer that will be able to handle imputing all
    categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Custom imputers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we jump into the code, let''s have a quick refresher of pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines allow us to sequentially apply a list of transforms and a final estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intermediate steps of the pipeline must be **transforms**, meaning they must
    implement `fit` and `transform` methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final estimator only needs to implement `fit`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of the pipeline is to assemble several steps that can be cross-validated
    together while setting different parameters. Once we have built our custom transformers
    for each column that needs imputing, we will pass them all through a pipeline
    so that our data can be transformed in one go. Let's build our custom category
    imputer to start.
  prefs: []
  type: TYPE_NORMAL
- en: Custom category imputer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will utilize the scikit-learn `TransformerMixin` base class to create
    our own custom categorical imputer. This transformer (and all other custom transformers
    in this chapter) will work as an element in a pipeline with a fit and `transform`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code block will become very familiar throughout this chapter,
    so we will go over each line in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a lot happening in this code block, so let''s break it down by line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have a new `import` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will inherit the `TransformerMixin` class from scikit-learn, which includes
    a `.fit_transform` method that calls upon the `.fit` and `.transform` methods
    we will create. This allows us to maintain a similar structure in our transformer
    to that of scikit-learn. Let''s initialize our custom class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now instantiated our custom class and have our `__init__` method that
    initializes our attributes. In our case, we only need to initialize one instance
    attribute, `self.cols` (which will be the columns that we specify as a parameter).
    Now, we can build our `fit` and `transform` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have our `transform` method. It takes in a DataFrame, and the first
    step is to copy and rename the DataFrame to `X`. Then, we will iterate over the
    columns we have specified in our `cols` parameter to fill in the missing slots.
    The `fillna` portion may feel familiar, as it is the function we employed in our
    first example. We are using the same function and setting it up so that our custom
    categorical imputer can work across several columns at once. After the missing
    values have been filled, we return our filled DataFrame. Next comes our `fit`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have set up our `fit` method to simply `return self`, as is the standard
    of `.fit` methods in scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a custom method that allows us to impute our categorical data!
    Let''s see it in action with our two categorical columns, `city` and `boolean`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We have initialized our custom categorical imputer, and we now need to `fit_transform`
    this imputer to our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Our dataset now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | tokyo | like | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | no | london | somewhat like | -0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: Great! Our `city` and `boolean` columns are no longer missing values. However,
    our quantitative column still has null values. Since the default imputer cannot
    select columns, let's make another custom one.
  prefs: []
  type: TYPE_NORMAL
- en: Custom quantitative imputer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same structure as our custom category imputer. The main difference
    here is that we will utilize scikit-learn''s `Imputer` class to actually make
    the transformation on our columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: For our `CustomQuantitativeImputer`, we have added a `strategy` parameter that
    will allow us to specify exactly how we want to impute missing values for our
    quantitative data. Here, we have selected the `mean` to replace missing values
    and still employ the `transform` and `fit` methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, in order to impute our data, we will call the `fit_transform` method,
    this time specifying both the column and the `strategy` to use to impute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, rather than calling and `fit_transforming` our `CustomCategoryImputer`
    and our `CustomQuantitativeImputer` separately, we can also set them up in a pipeline
    so that we can transform our dataset in one go. Let''s see how:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with our `import` statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can pass through our custom imputers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what our dataset looks like after our pipeline transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | tokyo | like | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | no | london | somewhat like | -0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | 8.3 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: Now we have a dataset with no missing values to work with!
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To recap, thus far we have successfully imputed our dataset—both our categorical
    and quantitative columns. At this point, you may be wondering, *how do we utilize
    the categorical data with a machine learning algorithm?*
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, we need to transform this categorical data into numerical data.
    So far, we have ensured that the most common category was used to fill the missing
    values. Now that this is done, we need to take it a step further.
  prefs: []
  type: TYPE_NORMAL
- en: Any machine learning algorithm, whether it is a linear-regression or a KNN-utilizing
    Euclidean distance, requires numerical input features to learn from. There are
    several methods we can rely on to transform our categorical data into numerical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding at the nominal level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with data at the nominal level. The main method we have is to
    transform our categorical data into dummy variables. We have two options to do
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilize pandas to automatically find the categorical variables and dummy code
    them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create our own custom transformer using dummy variables to work in a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we delve into these options, let's go over exactly what dummy variables
    are.
  prefs: []
  type: TYPE_NORMAL
- en: Dummy variables take the value zero or one to indicate the absence or presence
    of a category. They are proxy variables, or numerical stand-ins, for qualitative
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a simple regression analysis for wage determination. Say we are given
    gender, which is qualitative, and years of education, which is quantitative. In
    order to see if gender has an effect on wages, we would dummy code when the person
    is a female to female = 1, and female = 0 when the person is male.
  prefs: []
  type: TYPE_NORMAL
- en: When working with dummy variables, it is important to be aware of and avoid
    the dummy variable trap. The dummy variable trap is when you have independent
    variables that are multicollinear, or highly correlated. Simply put, these variables
    can be predicted from each other. So, in our gender example, the dummy variable
    trap would be if we include both female as (0|1) and male as (0|1), essentially
    creating a duplicate category. It can be inferred that a 0 female value indicates
    a male.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the dummy variable trap, simply leave out the constant term or one
    of the dummy categories. The left out dummy can become the base category to which
    the rest are compared to.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s come back to our dataset and employ some methods to encode our categorical
    data into dummy variables. pandas has a handy `get_dummies` method that actually
    finds all of the categorical variables and dummy codes them for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We have to be sure to specify which columns we want to apply this to because
    it will also dummy code the ordinal columns, and this wouldn't make much sense.
    We will take a more in-depth look into why dummy coding ordinal data doesn't makes
    sense shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data, with our dummy coded columns, now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ordinal_column** | **quantitative_column** | **city__london** | **city_san
    francisco** | **city_seattle** | **city_tokyo** | **boolean_no** | **boolean_yes**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | somewhat like | 1.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | like | 11.0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | somewhat like | -0.5 | 1 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | like | 10.0 | 0 | 0 | 1 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | somewhat like | NaN | 0 | 1 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | dislike | 20.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Our other option for dummy coding our data is to create our own custom dummifier.
    Creating this allows us to set up a pipeline to transform our whole dataset in
    one go.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we will use the same structure as our previous two custom imputers.
    Here, our `transform` method will use the handy pandas `get_dummies` method to
    create dummy variables for specified columns. The only parameter we have in this
    custom dummifier is `cols`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our custom dummifier mimics scikit-learn's `OneHotEncoding`, but with the added
    advantage of working on our entire DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding at the ordinal level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's take a look at our ordinal columns. There is still useful information
    here, however, we need to transform the strings into numerical data. At the ordinal
    level, since there is meaning in the data having a specific order, it does not
    make sense to use dummy variables. To maintain the order, we will use a label
    encoder.
  prefs: []
  type: TYPE_NORMAL
- en: By a label encoder, we mean that each label in our ordinal data will have a
    numerical value associated to it. In our example, this means that the ordinal
    column values (`dislike`, `somewhat like`, and `like`) will be represented as
    `0`, `1`, and `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the simplest form, the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have set up a list for ordering our labels. This is key, as we will
    be utilizing the index of our list to transform the labels to numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will implement a function called `map` on our column, that allows
    us to specify the function we want to implement on the column. We specify this
    function using a construct called `lambda`, which essentially allows us to create
    an anonymous function, or one that is not bound to a name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This specific code is creating a function that will apply the index of our
    list called `ordering` to each element. Now, we map this to our ordinal column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Our ordinal column is now represented as labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that scikit-learn has a `LabelEncoder`, but we are not using this method
    because it does not include the ability to order categories (`0` for dislike,
    `1` for somewhat like, `2` for like) as we have done previously. Rather, the default
    is a sorting method, which is not what we want to use here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, let us make a custom label encoder that will fit into our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We have maintained the structure of the other custom transformers in this chapter.
    Here, we have utilized the `map` and `lambda` functions detailed previously to
    transform the specified columns. Note the key parameter, `ordering`, which will
    determine which numerical values the labels will be encoding into.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call our custom encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Our dataset after these transformations looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | 1 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | 2 | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | 1 | -0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | 2 | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | 1 | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | 0 | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: Our ordinal column is now labeled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to this point, we have transformed the following columns accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean`, `city`: dummy encoding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ordinal_column`: label encoding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucketing continuous features into categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, when you have continuous numerical data, it may make sense to transform
    a continuous variable into a categorical variable. For example, say you have ages,
    but it would be more useful to work with age ranges.
  prefs: []
  type: TYPE_NORMAL
- en: pandas has a useful function called `cut` that will bin your data for you. By
    binning, we mean it will create the ranges for your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this function could work on our `quantitative_column`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the `cut` function for our quantitative column looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: When we specify `bins` to be an integer (`bins = 3`), it defines the number
    of equal–width bins in the range of `X`. However, in this case, the range of `X`
    is extended by .1% on each side to include the min or max values of `X`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set labels to `False`, which will return only integer indicators
    of the `bins`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what the integer indicators look like for our `quantitative_column`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Seeing our options with the `cut` function, we can also build our own `CustomCutter`
    for our pipeline. Once again, we will mimic the structure of our transformers.
    Our `transform` method will use the `cut` function, and so we will need to set
    `bins` and `labels` as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we have set the default labels parameter to `False`. Initialize our
    `CustomCutter`, specifying the column to transform and the number of bins to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With our `CustomCutter` transforming our `quantitative_column`, our data now
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | like | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | somewhat like | -0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: Note that our `quantitative_column` is now ordinal, and so there is no need
    to dummify the data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To review, we have transformed the columns in our dataset in the following
    ways thus far:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean, city`: dummy encoding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ordinal_column`: label encoding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quantitative_column`: ordinal level data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we now have transformations for all of our columns, let's put everything
    together in a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with importing our `Pipeline` class from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will bring together each of the custom transformers that we have created.
    Here is the order we will follow in our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will utilize the `imputer` to fill in missing values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will dummify our categorical columns
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will encode the `ordinal_column`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we will bucket the `quantitative_column`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s set up our pipeline as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to see the full transformation of our data using our pipeline, let''s
    take a look at our data with zero transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what our data looked like in the beginning before any transformations
    were made:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | like | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | somewhat like | -0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: 'We can now `fit` our pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created our pipeline object, let''s transform our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what our final dataset looks like after undergoing all of the appropriate
    transformations by column:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ordinal_column** | **quantitative_column** | **boolean_no** | **boolean_yes**
    | **city_london** | **city_san francisco** | **city_seattle** | **city_tokyo**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2 | 1 | 1 | 0 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0 | 2 | 0 | 1 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Extending numerical features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numerical features can undergo various methods to create extended features from
    them. Previously, we saw how we can transform continuous numerical data into ordinal
    data. Now, we will dive into extending our numerical features further.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go any deeper into these methods, we will introduce a new dataset
    to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Activity recognition from the Single Chest-Mounted Accelerometer dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This dataset collects data from a wearable accelerometer, mounted on the chest,
    collected from fifteen participants performing seven activities. The sampling
    frequency of the accelerometer is 52 Hz and the accelerometer data is uncalibrated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is separated by participant and contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: z acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labels are codified by numbers and represent an activity, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Working at a computer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standing up, walking, and going up/down stairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going up/down stairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking and talking with someone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talking while standing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Further information on this dataset is available on the UCI *Machine Learning
    Repository* at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at our data. First, we need to load in our CSV file and
    set our column headers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s examine the first few rows with the `.head` method, which will
    default to the first five rows, unless we specify how many rows to show:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **index** | **x** | **y** | **z** | **activity** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 0.0 | 1502 | 2215 | 2153 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1.0 | 1667 | 2072 | 2047 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 2.0 | 1611 | 1957 | 1906 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 3.0 | 1601 | 1939 | 1831 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 4.0 | 1643 | 1965 | 1879 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'This dataset is meant to train models to recognize a user''s current physical
    activity given an accelerometer''s `x`, `y`, and `z` position on a device such
    as a smartphone. According to the website, the options for the `activity` column
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1**: Working at a computer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: Standing Up and Going updown stairs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: Standing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4**: Walking'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5**: Going UpDown Stairs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**6**: Walking and Talking with Someone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**7**: Talking while Standing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `activity` column will be the target variable we will be trying to predict,
    using the other columns. Let''s determine the null accuracy to beat in our machine
    learning model. To do this, we will invoke the `value_counts` method with the
    `normalize` option set to `True` to give us the most commonly occurring activity
    as a percentage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The null accuracy to beat is 51.53%, meaning that if we guessed seven (talking
    while standing), then we would be right over half of the time. Now, let's do some
    machine learning! Let's step through, line by line, setting up our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have our `import` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You may be familiar with these import statements from last chapter. Once again,
    we will be utilizing scikit-learn''s **K-Nearest Neighbors** (**KNN**) classification
    model. We will also use the grid search module that automatically finds the best
    combination of parameters for the KNN model that best fits our data with respect
    to cross-validated accuracy. Next, we create a feature matrix (`X`) and a response
    variable (`y`) for our predictive model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Once our `X` and `y` are set up, we can introduce the variables and instances
    we need to successfully run a grid search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will instantiate a KNN model and a grid search module and fit it to
    our feature matrix and response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can `print` the best accuracy and parameters that were used to learn
    from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Using five neighbors as its parameter, our KNN model was able to achieve a 72.07%
    accuracy, much better than our null accuracy of around 51.53%! Perhaps we can
    utilize another method to get our accuracy up even more.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key method of working with numerical data and creating more features is through
    scikit-learn's `PolynomialFeatures` class. In its simplest form, this constructor will
    create new columns that are products of existing columns to capture feature interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, this class will generate a new feature matrix with all of
    the polynomial combinations of the features with a degree less than or equal to
    the specified degree. Meaning that, if your input sample is two-dimensional, like
    so: [a, b], then the degree-2 polynomial features are as follows: [1, a, b, a^2,
    ab, b^2].'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When instantiating polynomial features, there are three parameters to keep
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: degree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interaction_only`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_bias`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Degree corresponds to the degree of the polynomial features, with the default
    set to two.
  prefs: []
  type: TYPE_NORMAL
- en: '`interaction_only` is a boolean that, when true, only interaction features
    are produced, meaning features that are products of degree distinct features.
    The default for `interaction_only` is false.'
  prefs: []
  type: TYPE_NORMAL
- en: '`include_bias` is also a boolean that, when true (default), includes a `bias`
    column, the feature in which all polynomial powers are zero, adding a column of
    all ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up a polynomial feature instance by first importing the class and
    instantiating with our parameters. At first, let''s take a look at what features
    we get when setting `interaction_only` to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can `fit_transform` these polynomial features to our dataset and look
    at the `shape` of our extended dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Our dataset has now expanded to `162501` rows and `9` columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s place our data into a DataFrame, setting the column headers to the `feature_names`,
    and taking a look at the first few rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **x0** | **x1** | **x2** | **x0^2** | **x0 x1** | **x0 x2** | **x1^2**
    | **x1 x2** | **x2^2** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1502.0 | 2215.0 | 2153.0 | 2256004.0 | 3326930.0 | 3233806.0 | 4906225.0
    | 4768895.0 | 4635409.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1667.0 | 2072.0 | 2047.0 | 2778889.0 | 3454024.0 | 3412349.0 | 4293184.0
    | 4241384.0 | 4190209.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1611.0 | 1957.0 | 1906.0 | 2595321.0 | 3152727.0 | 3070566.0 | 3829849.0
    | 3730042.0 | 3632836.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1601.0 | 1939.0 | 1831.0 | 2563201.0 | 3104339.0 | 2931431.0 | 3759721.0
    | 3550309.0 | 3352561.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1643.0 | 1965.0 | 1879.0 | 2699449.0 | 3228495.0 | 3087197.0 | 3861225.0
    | 3692235.0 | 3530641.0 |'
  prefs: []
  type: TYPE_TB
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can conduct some exploratory data analysis. Since the purpose of polynomial
    features is to get a better sense of feature interaction in the original data,
    the best way to visualize this is through a correlation `heatmap`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to import a data visualization tool that will allow us to create a
    `heatmap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Matplotlib and Seaborn are popular data visualization tools. We can now visualize
    our correlation `heatmap` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`.corr` is a function we can call on our DataFrame that gives us a correlation
    matrix of our features. Let''s take a look at our feature interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d80a9fbb-3ed3-461d-add6-513e6fb79d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: The colors on the `heatmap` are based on pure values; the darker the color,
    the greater the correlation of the features.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked at our polynomial features with our `interaction_only`
    parameter set to `False`. Let's set this to `True` and see what our features look
    like without repeat variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up this polynomial feature instance the same as we did previously.
    Note the only difference is that `interaction_only` is now `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have `162501` rows by `6` columns. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame now looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **x0** | **x1** | **x2** | **x0 x1** | **x0 x2** | **x1 x2** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1502.0 | 2215.0 | 2153.0 | 3326930.0 | 3233806.0 | 4768895.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1667.0 | 2072.0 | 2047.0 | 3454024.0 | 3412349.0 | 4241384.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1611.0 | 1957.0 | 1906.0 | 3152727.0 | 3070566.0 | 3730042.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1601.0 | 1939.0 | 1831.0 | 3104339.0 | 2931431.0 | 3550309.0 |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1643.0 | 1965.0 | 1879.0 | 3228495.0 | 3087197.0 | 3692235.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Since `interaction_only` has been set to `True` this time, `x0^2`, `x1^2`,
    and `x2^2` have disappeared since they were repeat variables. Let''s see what
    our correlation matrix looks like now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17488850-dc5c-4e56-8aed-7882370d9704.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are able to see how the features interact with each other. We can also perform
    a grid search of our KNN model with the new polynomial features, which can also
    be grid searched in a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up our pipeline parameters first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, instantiate our `Pipeline`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can set up our grid search and print the best score and parameters
    to learn from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Our accuracy is now 72.12%, which is an improvement from our accuracy without
    expanding our features using polynomial features!
  prefs: []
  type: TYPE_NORMAL
- en: Text-specific feature construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until this point, we have been working with categorical and numerical data.
    While our categorical data has come in the form of a string, the text has been
    part of a single category. We will now dive deeper into longer—form text data.
    This form of text data is much more complex than single—category text, because
    we now have a series of categories, or tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get any further into working with text data, let's make sure we have
    a good understanding of what we mean when we refer to text data. Consider a service
    like Yelp, where users write up reviews of restaurants and businesses to share
    their thoughts on their experience. These reviews, all written in text format,
    contain a wealth of information that would be useful for machine learning purposes,
    for example, in predicting the best restaurant to visit.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a large part of how we communicate in today's world is through written
    text, whether in messaging services, social media, or email. As a result, so much
    can be garnered from this information through modeling. For example, we can conduct
    a sentiment analysis from Twitter data.
  prefs: []
  type: TYPE_NORMAL
- en: This type of work can be referred to as **natural language processing** (**NLP**).
    This is a field primarily concerned with interactions between computers and humans,
    specifically where computers can be programmed to process natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we've mentioned before, it's important to note that all machine learning
    models require numerical inputs, so we have to be creative and think strategically
    when we work with text and convert such data into numerical features. There are
    several options for doing so, so let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn has a handy module called `feature_extraction` that allows
    us to, as the name suggests, extract features for data such as text in a format
    supported by machine learning algorithms. This module has methods for us to utilize
    when working with text.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we may refer to our text data as a corpus, specifically meaning
    an aggregate of text content or documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common method to transform a corpus into a numerical representation,
    a process known as vectorization, is through a method called **bag-of-words**.
    The basic idea behind the bag of words approach is that documents are described
    by word occurrences while completely ignoring the positioning of words in the
    document. In its simplest form, text is represented as a **bag**,without regard
    for grammar or word order, and is maintained as a set, with importance given to
    multiplicity. A bag of words representation is achieved in the following three
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with tokenizing. This process uses white spaces and punctuation
    to separate words from each other, turning them into tokens. Each possible token
    is given an integer ID.
  prefs: []
  type: TYPE_NORMAL
- en: Next comes counting. This step simply counts the occurrences of tokens within
    a document.
  prefs: []
  type: TYPE_NORMAL
- en: Last comes normalizing, meaning that tokens are weighted with diminishing importance
    when they occur in the majority of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a couple more methods for vectorizing.
  prefs: []
  type: TYPE_NORMAL
- en: CountVectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CountVectorizer` is the most commonly used method to convert text data into
    their vector representations. It is similar to dummy variables, in the sense that
    `CountVectorizer` converts text columns into matrices where columns are tokens
    and cell values are counts of occurrences of each token in each document. The
    resulting matrix is referred to as a **document-term matrix** because each row
    will represent a **document** (in this case, a tweet) and each column represents
    a **term** (a word).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at a new dataset, and see how `CountVectorizer` works. The
    Twitter Sentiment Analysis dataset contains 1,578,627 classified tweets, and each
    row is marked as one for positive sentiment and zero for negative sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Further information on this dataset can be found at [http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/.](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load in our data using pandas'' `read_csv` method. Note that we are
    specifying an `encoding` as an optional parameter to ensure that we handle all
    special characters in the tweets properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to load in our data in a specific format and map text characters
    appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the first few rows of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ItemID** | **Sentiment** | **SentimentText** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 0 | is so sad for my APL frie... |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2 | 0 | I missed the New Moon trail... |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 3 | 1 | omg its already 7:30 :O |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 4 | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 5 | 0 | i think mi bf is cheating on me!!! ... |'
  prefs: []
  type: TYPE_TB
- en: 'We are only concerned with the `Sentiment` and `SentimentText` columns, so
    we will delete the `ItemID` column for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Sentiment** | **SentimentText** |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 0 | is so sad for my APL frie... |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | I missed the New Moon trail... |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1 | omg its already 7:30 :O |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0 | i think mi bf is cheating on me!!! ... |'
  prefs: []
  type: TYPE_TB
- en: 'Now, we can import `CountVectorizer` and get a better understanding of the
    text we are working with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up our `X` and `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CountVectorizer` class works very similarly to the custom transformers
    we have been working with so far, and has a `fit_transform` function to manipulate
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: After our `CountVectorizer` has transformed our data, we have 99,989 rows and
    105,849 columns.
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` has many different parameters that can change the number
    of features that are constructed. Let''s go over a few of these parameters to
    get a better sense of how these features are created.'
  prefs: []
  type: TYPE_NORMAL
- en: CountVectorizer parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A few parameters that we will go over include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stop_words`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_df`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_df`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ngram_range`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`analyzer`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop_words` is a frequently used parameter in `CountVectorizer`. You can pass
    in the string `english` to this parameter, and a built-in stop word list for English
    is used. You can also specify a list of words yourself. These words will then
    be removed from the tokens and will not appear as features in your data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the feature columns have gone down from 105,849 when stop words
    were not used, to 105,545 when English stop words have been set. The idea behind
    using stop words is to remove noise within your features and take out words that
    occur so often that there won't be much meaning to garner from them in your models.
  prefs: []
  type: TYPE_NORMAL
- en: Another parameter is called `min_df`. This parameter is used to skim the number
    of features, by ignoring terms that have a document frequency lower than the given
    threshold or cut-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an implementation of our `CountVectorizer` with `min_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: This is a method that is utilized to significantly reduce the number of features
    created.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a parameter called `max_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to trying to understand what stop words exist in the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the `ngram_range` parameter. This parameter takes in a
    tuple where the lower and upper boundary of the range of n-values indicates the
    number of different n-grams to be extracted. N-grams represent phrases, so a value
    of one would represent one token, however a value of two would represent two tokens
    together. As you can imagine, this will expand our feature set quite significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: See, we now have 3,219,557 features. Since sets of words (phrases) can sometimes
    have more meaning, using n-gram ranges can be useful for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set an analyzer as a parameter in `CountVectorizer`. The analyzer
    determines whether the feature should be made of word or character n-grams. Word
    is the default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Given that word is the default, our feature column number doesn't change much
    from the original.
  prefs: []
  type: TYPE_NORMAL
- en: We can even create our own custom analyzer. Conceptually, words are built from
    root words, or stems, and we can construct a custom analyzer that accounts for
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming is a common natural language processing method that allows us to stem
    our vocabulary, or make it smaller by converting words to their roots. There is
    a natural language toolkit, known as NLTK, that has several packages that allow
    us to perform operations on text data. One such package is a `stemmer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import our `stemmer` and then initialize it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see how some words are stemmed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the word `interesting` can be reduced to the root stem. We can now use
    this to create a function that will allow us to tokenize words into their stems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what our function outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now place this tokenizer function into our analyzer parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: This yields us fewer features, which intuitively makes sense since our vocabulary
    has reduced with stemming.
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` is a very useful tool to help us expand our features and
    convert text to numerical features. There is another common vectorizer that we
    will look into.'
  prefs: []
  type: TYPE_NORMAL
- en: The Tf-idf vectorizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `Tf-idfVectorizer` can be broken down into two components. First, the *tf*
    part, which represents **term frequency**, and the *idf* part, meaning **inverse
    document frequency**. It is a term—weighting method that has applications in information—retrieval
    and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'A weight is given to evaluate how important a word is to a document in a corpus.
    Let''s look into each part a little more:'
  prefs: []
  type: TYPE_NORMAL
- en: '**tf: term frequency**: Measures how frequently a term occurs in a document.
    Since documents can be different in length, it is possible that a term would appear
    many more times in longer documents than shorter ones. Thus, the term frequency
    is often divided by the document length, or the total number of terms in the document,
    as a way of normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**idf: i****nverse document frequency**: Measures how important a term is.
    While computing term frequency, all terms are considered equally important. However,
    certain terms, such as *is*, *of*, and *that*, may appear a lot of times but have
    little importance. So, we need to weight the frequent terms less, while we scale
    up the rare ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To re-emphasize, a `TfidfVectorizer` is the same as `CountVectorizer`, in that
    it constructs features from tokens, but it takes a step further and normalizes
    counts to frequency of occurrences across a corpus. Let's see an example of this
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, our import:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'To bring up some code from before, a plain vanilla `CountVectorizer` will output
    a document-term matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Our  `TfidfVectorizer` can be set up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: We can see that both vectorizers output the same number of rows and columns,
    but produce different values in each cell. This is because `TfidfVectorizer` and
    `CountVectorizer` are both used to transform text data into quantitative data,
    but the way in which they fill in cell values differ.
  prefs: []
  type: TYPE_NORMAL
- en: Using text in machine learning pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, the ultimate goal of our vectorizers is to use them to make text
    data ingestible for our machine learning pipelines. Because `CountVectorizer`
    and `TfidfVectorizer` act like any other transformer we have been working with
    in this book, we will have to utilize a scikit-learn pipeline to ensure accuracy
    and honesty in our machine learning pipeline. In our example, we are going to
    be working with a large number of columns (in the hundreds of thousands), so I
    will use a classifier that is known to be more efficient in this case, a Naive
    Bayes model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start building our pipelines, let''s get our null accuracy of the
    response column, which is either zero (negative) or one (positive):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Making the accuracy beat 56.5%. Now, let''s create a pipeline with two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` to featurize the tweets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiNomialNB` Naive Bayes model to classify between positive and negative
    sentiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First let''s start with setting up our pipeline parameters as follows, and
    then instantiate our grid search as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: And we got 75.6%, which is great! Now, let's kick things into high-gear and
    incorporate the `TfidfVectorizer`. Instead of rebuilding the pipeline using tf-idf
    instead of `CountVectorizer`, let's try using something a bit different. The scikit-learn
    has a `FeatureUnion` module that facilitates horizontal stacking of features (side-by-side).
    This allows us to use multiple types of text featurizers in the same pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can build a `featurizer` that runs both a `TfidfVectorizer`
    and a `CountVectorizer` on our tweets and concatenates them horizontally (keeping
    the same number of rows but increasing the number of columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we build the `featurizer`, we can use it to see how it affects the shape
    of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that unioning the two featurizers results in a dataset with the
    same number of rows, but doubles the number of either the `CountVectorizer` or
    the `TfidfVectorizer`. This is because the resulting dataset is literally both
    datasets side-by-side. This way, our machine learning models may learn from both
    sets of data simultaneously. Let''s change the `params` of our `featurizer` object
    slightly and see what difference it makes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build a much more comprehensive pipeline that incorporates the feature
    union of both of our vectorizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Nice, even better than just `CountVectorizer` alone! It is also interesting
    to note that the best `ngram_range` for the `CountVectorizer` was `(1, 2)`, while
    it was `(1, 1)` for the `TfidfVectorizer`, implying that word occurrences alone
    were not as important as two-word phrase occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: 'By this point, it should be obvious that we could have made our pipeline much
    more complicated by:'
  prefs: []
  type: TYPE_NORMAL
- en: Grid searching across dozens of parameters for each vectorizer
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding in more steps to our pipeline such as polynomial feature construction
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: But this would have been very cumbersome for this text and would take hours
    to run on most commercial laptops. Feel free to expand on this pipeline and beat
    our score!
  prefs: []
  type: TYPE_NORMAL
- en: Phew, that was a lot. Text can be difficult to work with. Between sarcasm, misspellings,
    and vocabulary size, data scientists and machine learning engineers have their
    hands full. This introduction to working with text will allow you, the reader,
    to experiment with your own large text datasets and obtain your own results!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we have gone over several methods of imputing missing values in our
    categorical and numerical data, encoding our categorical variables, and creating
    custom transformers to fit into a pipeline. We also dove into several feature
    construction methods for both numerical data and text-based data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at the features we have constructed,
    and consider appropriate methods of selecting the right features to use for our
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
