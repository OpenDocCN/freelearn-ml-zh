- en: Feature Construction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征构建
- en: In the previous chapter, we worked with the `Pima Indian Diabetes Prediction`
    dataset to get a better understanding of which given features in our dataset are
    most valuable. Working with the features that were available to us, we identified
    missing values within our columns and employed techniques of dropping missing
    values, imputing, and normalizing/standardizing our data to improve the accuracy
    of our machine learning model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 `Pima Indian Diabetes Prediction` 数据集来更好地了解我们数据集中哪些给定的特征最有价值。使用我们可用的特征，我们识别了列中的缺失值，并采用了删除缺失值、填充以及归一化/标准化数据的技术，以提高我们机器学习模型的准确性。
- en: It is important to note that, up to this point, we have only worked with features
    that are quantitative. We will now shift into dealing with categorical data, in
    addition to the quantitative data that has missing values. Our main focus will
    be to work with our given features to construct entirely new features for our
    models to learn from.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，到目前为止，我们只处理了定量特征。现在，我们将转向处理除了具有缺失值的定量数据之外，还要处理分类数据。我们的主要焦点将是使用给定的特征来构建模型可以从中学习的新特征。
- en: There are various methods we can utilize to construct our features, with the
    most basic starting with the pandas library in Python to scale an existing feature
    by a multiples. We will be diving into some more mathematically intensive methods,
    and will employ various packages available to us through the scikit-learn library;
    we will also create our own custom classes. We will go over these classes in detail
    as we get into the code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用各种方法来构建我们的特征，最基本的方法是从 Python 中的 pandas 库开始，通过乘数来缩放现有特征。我们将深入研究一些更数学密集的方法，并使用通过
    scikit-learn 库提供的各种包；我们还将创建自己的自定义类。随着我们进入代码，我们将详细介绍这些类。
- en: 'We will be covering the following topics in our discussions:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的讨论中涵盖以下主题：
- en: Examining our dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查我们的数据集
- en: Imputing categorical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充分类特征
- en: Encoding categorical variables
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码分类变量
- en: Extending numerical features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展数值特征
- en: Text-specific feature construction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本特定特征构建
- en: Examining our dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查我们的数据集
- en: For demonstrative purposes, in this chapter, we will utilize a dataset that
    we have created, so that we can showcase a variety of data levels and types. Let's
    set up our DataFrame and dive into our data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，在本章中，我们将使用我们创建的数据集，这样我们就可以展示各种数据级别和类型。让我们设置我们的 DataFrame 并深入了解我们的数据。
- en: 'We will use pandas to create the DataFrame we will work with, as this is the
    primary data structure in pandas. The advantage of a pandas DataFrame is that
    there are several attributes and methods available for us to perform on our data.
    This allows us to logically manipulate the data to develop a thorough understanding
    of what we are working with, and how best to structure our machine learning models:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 pandas 创建我们将要工作的 DataFrame，因为这是 pandas 中的主要数据结构。pandas DataFrame 的优势在于，我们有几个属性和方法可用于对数据进行操作。这使我们能够逻辑地操作数据，以全面了解我们正在处理的内容，以及如何最好地构建我们的机器学习模型：
- en: 'First, let''s import `pandas`:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 `pandas`：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we can set up our `DataFrame X`. To do this, we will utilize the `DataFrame`
    method in pandas, which creates a tabular data structure (table with rows and
    columns). This method can take in a few types of data (NumPy arrays or dictionaries,
    to name a couple). Here, we will be passing-in a dictionary with keys as column
    headers and values as lists, with each list representing a column:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以设置我们的 `DataFrame X`。为此，我们将利用 pandas 中的 `DataFrame` 方法，该方法创建一个表格数据结构（具有行和列的表格）。此方法可以接受几种类型的数据（例如
    NumPy 数组或字典）。在这里，我们将传递一个字典，其键为列标题，值为列表，每个列表代表一列：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give us a DataFrame with four columns and six rows. Let''s print
    our DataFrame `X` and take a look at the data:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将给我们一个具有四列和六行的 DataFrame。让我们打印我们的 DataFrame `X` 并查看数据：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the output as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | **布尔值** | **城市** | **有序列** | **定量列** |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 是 | 东京 | 有点像 | 1.0 |'
- en: '| **1** | no | None | like | 11.0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 否 | 无 | 喜欢的 | 11.0 |'
- en: '| **2** | None | london | somewhat like | -0.5 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 无 | 伦敦 | 有点像 | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 否 | 西雅图 | 喜欢的 | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 否 | 旧金山 | 有点像 | NaN |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 是 | 东京 | 不喜欢的 | 20.0 |'
- en: 'Let''s take a look at our columns and identify our data levels and types:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们的列，并确定我们的数据级别和类型：
- en: '`boolean`: This column is represented by binary categorical data (yes/no),
    and is at the nominal level'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`布尔`：这个列由二元分类数据（是/否）表示，处于名称级别'
- en: '`city`: This column is represented by categorical data, also at the nominal
    level'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`城市`：这个列由分类数据表示，也处于名称级别'
- en: '`ordinal_column`: As you may have guessed by the column name, this column is
    represented by ordinal data, at the ordinal level'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`序号列`：正如你可能从列名猜到的，这个列由序数数据表示，处于序数级别'
- en: '`quantitative_column`: This column is represented by integers at the ratio
    level'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`定量列`：这个列由整数在比例级别表示'
- en: Imputing categorical features
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充分类特征
- en: 'Now that we have an understanding of the data we are working with, let''s take
    a look at our missing values:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们正在处理的数据，让我们看看我们的缺失值：
- en: To do this, we can use the `isnull` method available to us in pandas for DataFrames.
    This method returns a `boolean` same-sized object indicating if the values are
    null.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以使用pandas为我们提供的`isnull`方法。此方法返回一个与值大小相同的`boolean`对象，指示值是否为空。
- en: 'We will then `sum` these to see which columns have missing data:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将`sum`这些值以查看哪些列有缺失数据：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we can see that three of our columns are missing values. Our course of
    action will be to impute these missing values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们有三列数据缺失。我们的行动方案将是填充这些缺失值。
- en: If you recall, we implemented scikit-learn's `Imputer` class in a previous chapter
    to fill in numerical data. `Imputer` does have a categorical option, `most_frequent`,
    however it only works on categorical data that has been encoded as integers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们在上一章中实现了scikit-learn的`Imputer`类来填充数值数据。`Imputer`确实有一个分类选项，`most_frequent`，然而它只适用于已经被编码为整数的分类数据。
- en: We may not always want to transform our categorical data this way, as it can
    change how we interpret the categorical information, so we will build our own
    transformer. By transformer, we mean a method by which a column will impute missing
    values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能并不总是想以这种方式转换我们的分类数据，因为它可能会改变我们解释分类信息的方式，因此我们将构建自己的转换器。在这里，我们所说的转换器是指一种方法，通过这种方法，列将填充缺失值。
- en: In fact, we will build several custom transformers in this chapter, as they
    are quite useful for making transformations to our data, and give us options that
    are not readily available in pandas or scikit-learn.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在本章中，我们将构建几个自定义转换器，因为它们对于对我们的数据进行转换非常有用，并为我们提供了在pandas或scikit-learn中不可轻易获得的选择。
- en: Let's start with our categorical column, `city`. Just as we have the strategy
    of imputing the mean value to fill missing rows for numerical data, we have a
    similar method for categorical data. To impute values for categorical data, fill
    missing rows with the most common category.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的分类列`城市`开始。正如我们用均值填充缺失行来填充数值数据，我们也有一个类似的方法用于分类数据。为了填充分类数据的值，用最常见的类别填充缺失行。
- en: 'To do so, we will need to find out what the most common category is in our
    `city` column:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要这样做，我们需要找出`城市`列中最常见的类别：
- en: Note that we need to specify the column we are working with to employ a method
    called `value_counts`. This will return an object that will be in descending order
    so that the first element is the most frequently-occurring element.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要指定我们正在处理的列来应用一个名为`value_counts`的方法。这将返回一个按降序排列的对象，因此第一个元素是最频繁出现的元素。
- en: 'We will grab only the first element in the object:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只获取对象中的第一个元素：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see that `tokyo` appears to be the most common city. Now that we know
    which value to use to impute our missing rows, let''s fill these slots. There
    is a `fillna` function that allows us to specify exactly how we want to fill missing
    values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`东京`似乎是最常见的城市。现在我们知道要使用哪个值来填充我们的缺失行，让我们填充这些空位。有一个`fillna`函数允许我们指定我们想要如何填充缺失值：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `city` column now looks like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`城市`列现在看起来是这样的：'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Great, now our `city` column no longer has missing values. However, our other
    categorical column, `boolean`, still does. Rather than going through the same
    method, let's build a custom imputer that will be able to handle imputing all
    categorical data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在我们的`城市`列不再有缺失值。然而，我们的其他分类列`布尔`仍然有。与其重复同样的方法，让我们构建一个能够处理所有分类数据填充的自定义填充器。
- en: Custom imputers
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义填充器
- en: 'Before we jump into the code, let''s have a quick refresher of pipelines:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，让我们快速回顾一下管道：
- en: Pipelines allow us to sequentially apply a list of transforms and a final estimator
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道允许我们按顺序应用一系列转换和一个最终估计器
- en: Intermediate steps of the pipeline must be **transforms**, meaning they must
    implement `fit` and `transform` methods
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道的中间步骤必须是 **转换器**，这意味着它们必须实现 `fit` 和 `transform` 方法
- en: The final estimator only needs to implement `fit`
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终估计器只需要实现 `fit`
- en: The purpose of the pipeline is to assemble several steps that can be cross-validated
    together while setting different parameters. Once we have built our custom transformers
    for each column that needs imputing, we will pass them all through a pipeline
    so that our data can be transformed in one go. Let's build our custom category
    imputer to start.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的目的是组装几个可以一起交叉验证的步骤，同时设置不同的参数。一旦我们为需要填充的每一列构建了自定义的转换器，我们将它们全部通过管道传递，以便我们的数据可以一次性进行转换。让我们从构建自定义类别填充器开始。
- en: Custom category imputer
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义类别填充器
- en: First, we will utilize the scikit-learn `TransformerMixin` base class to create
    our own custom categorical imputer. This transformer (and all other custom transformers
    in this chapter) will work as an element in a pipeline with a fit and `transform`
    method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将利用 scikit-learn 的 `TransformerMixin` 基类来创建我们自己的自定义类别填充器。这个转换器（以及本章中的所有其他自定义转换器）将作为一个具有
    `fit` 和 `transform` 方法的管道元素工作。
- en: 'The following code block will become very familiar throughout this chapter,
    so we will go over each line in detail:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块将在本章中变得非常熟悉，因此我们将逐行详细讲解：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is a lot happening in this code block, so let''s break it down by line:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码块中发生了很多事情，所以让我们逐行分解：
- en: 'First, we have a new `import` statement:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们有一个新的 `import` 语句：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will inherit the `TransformerMixin` class from scikit-learn, which includes
    a `.fit_transform` method that calls upon the `.fit` and `.transform` methods
    we will create. This allows us to maintain a similar structure in our transformer
    to that of scikit-learn. Let''s initialize our custom class:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从 scikit-learn 继承 `TransformerMixin` 类，它包括一个 `.fit_transform` 方法，该方法调用我们将创建的
    `.fit` 和 `.transform` 方法。这允许我们在转换器中保持与 scikit-learn 相似的结构。让我们初始化我们的自定义类：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have now instantiated our custom class and have our `__init__` method that
    initializes our attributes. In our case, we only need to initialize one instance
    attribute, `self.cols` (which will be the columns that we specify as a parameter).
    Now, we can build our `fit` and `transform` methods:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经实例化了我们的自定义类，并有了我们的 `__init__` 方法，该方法初始化我们的属性。在我们的情况下，我们只需要初始化一个实例属性 `self.cols`（它将是我们指定的参数中的列）。现在，我们可以构建我们的
    `fit` 和 `transform` 方法：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we have our `transform` method. It takes in a DataFrame, and the first
    step is to copy and rename the DataFrame to `X`. Then, we will iterate over the
    columns we have specified in our `cols` parameter to fill in the missing slots.
    The `fillna` portion may feel familiar, as it is the function we employed in our
    first example. We are using the same function and setting it up so that our custom
    categorical imputer can work across several columns at once. After the missing
    values have been filled, we return our filled DataFrame. Next comes our `fit`
    method:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们有我们的 `transform` 方法。它接受一个 DataFrame，第一步是复制并重命名 DataFrame 为 `X`。然后，我们将遍历我们在
    `cols` 参数中指定的列来填充缺失的槽位。`fillna` 部分可能感觉熟悉，因为我们已经在第一个例子中使用了这个函数。我们正在使用相同的函数，并设置它，以便我们的自定义类别填充器可以一次跨多个列工作。在填充了缺失值之后，我们返回填充后的
    DataFrame。接下来是我们的 `fit` 方法：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have set up our `fit` method to simply `return self`, as is the standard
    of `.fit` methods in scikit-learn.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置了我们的 `fit` 方法简单地 `return self`，这是 scikit-learn 中 `.fit` 方法的标准。
- en: 'Now we have a custom method that allows us to impute our categorical data!
    Let''s see it in action with our two categorical columns, `city` and `boolean`:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有一个自定义方法，允许我们填充我们的类别数据！让我们通过我们的两个类别列 `city` 和 `boolean` 来看看它的实际效果：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have initialized our custom categorical imputer, and we now need to `fit_transform`
    this imputer to our dataset:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经初始化了我们的自定义类别填充器，现在我们需要将这个填充器 `fit_transform` 到我们的数据集中：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our dataset now looks like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集现在看起来像这样：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | somewhat like | 1.0 |'
- en: '| **1** | no | tokyo | like | 11.0 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | tokyo | like | 11.0 |'
- en: '| **2** | no | london | somewhat like | -0.5 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **2** | no | london | somewhat like | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | like | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | somewhat like | NaN |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | dislike | 20.0 |'
- en: Great! Our `city` and `boolean` columns are no longer missing values. However,
    our quantitative column still has null values. Since the default imputer cannot
    select columns, let's make another custom one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们的`city`和`boolean`列不再有缺失值。然而，我们的定量列仍然有null值。由于默认的填充器不能选择列，让我们再做一个自定义的。
- en: Custom quantitative imputer
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义定量填充器
- en: 'We will use the same structure as our custom category imputer. The main difference
    here is that we will utilize scikit-learn''s `Imputer` class to actually make
    the transformation on our columns:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与我们的自定义分类填充器相同的结构。这里的主要区别是我们将利用scikit-learn的`Imputer`类来实际上在我们的列上执行转换：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For our `CustomQuantitativeImputer`, we have added a `strategy` parameter that
    will allow us to specify exactly how we want to impute missing values for our
    quantitative data. Here, we have selected the `mean` to replace missing values
    and still employ the `transform` and `fit` methods.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`CustomQuantitativeImputer`，我们增加了一个`strategy`参数，这将允许我们指定我们想要如何为我们的定量数据填充缺失值。在这里，我们选择了`mean`来替换缺失值，并且仍然使用`transform`和`fit`方法。
- en: 'Once again, in order to impute our data, we will call the `fit_transform` method,
    this time specifying both the column and the `strategy` to use to impute:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了填充我们的数据，我们将调用`fit_transform`方法，这次指定了列和用于填充的`strategy`：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Alternatively, rather than calling and `fit_transforming` our `CustomCategoryImputer`
    and our `CustomQuantitativeImputer` separately, we can also set them up in a pipeline
    so that we can transform our dataset in one go. Let''s see how:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，而不是分别调用和`fit_transform`我们的`CustomCategoryImputer`和`CustomQuantitativeImputer`，我们也可以将它们设置在一个pipeline中，这样我们就可以一次性转换我们的dataset。让我们看看如何：
- en: 'Start with our `import` statement:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的`import`语句开始：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can pass through our custom imputers:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以传递我们的自定义填充器：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s see what our dataset looks like after our pipeline transformations:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的dataset在pipeline转换后看起来像什么：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | somewhat like | 1.0 |'
- en: '| **1** | no | tokyo | like | 11.0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | tokyo | like | 11.0 |'
- en: '| **2** | no | london | somewhat like | -0.5 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **2** | no | london | somewhat like | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | like | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | 8.3 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | somewhat like | 8.3 |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | dislike | 20.0 |'
- en: Now we have a dataset with no missing values to work with!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个没有缺失值的dataset可以工作了！
- en: Encoding categorical variables
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码分类变量
- en: To recap, thus far we have successfully imputed our dataset—both our categorical
    and quantitative columns. At this point, you may be wondering, *how do we utilize
    the categorical data with a machine learning algorithm?*
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，到目前为止，我们已经成功填充了我们的dataset——包括我们的分类和定量列。在这个时候，你可能想知道，*我们如何利用分类数据与机器学习算法结合使用？*
- en: Simply put, we need to transform this categorical data into numerical data.
    So far, we have ensured that the most common category was used to fill the missing
    values. Now that this is done, we need to take it a step further.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们需要将这个分类数据转换为数值数据。到目前为止，我们已经确保使用最常见的类别来填充缺失值。现在这件事已经完成，我们需要更进一步。
- en: Any machine learning algorithm, whether it is a linear-regression or a KNN-utilizing
    Euclidean distance, requires numerical input features to learn from. There are
    several methods we can rely on to transform our categorical data into numerical
    data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习算法，无论是线性回归还是使用欧几里得距离的KNN，都需要数值输入特征来学习。我们可以依赖几种方法将我们的分类数据转换为数值数据。
- en: Encoding at the nominal level
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 名义级别的编码
- en: 'Let''s begin with data at the nominal level. The main method we have is to
    transform our categorical data into dummy variables. We have two options to do
    this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从名义级别的数据开始。我们主要的方法是将我们的分类数据转换为虚拟变量。我们有两种方法来做这件事：
- en: Utilize pandas to automatically find the categorical variables and dummy code
    them
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用pandas自动找到分类变量并将它们转换为虚拟变量
- en: Create our own custom transformer using dummy variables to work in a pipeline
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟变量创建我们自己的自定义转换器以在pipeline中工作
- en: Before we delve into these options, let's go over exactly what dummy variables
    are.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨这些选项之前，让我们先了解一下虚拟变量究竟是什么。
- en: Dummy variables take the value zero or one to indicate the absence or presence
    of a category. They are proxy variables, or numerical stand-ins, for qualitative
    data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟变量取值为零或一，以表示类别的缺失或存在。它们是代理变量，或数值替代变量，用于定性数据。
- en: Consider a simple regression analysis for wage determination. Say we are given
    gender, which is qualitative, and years of education, which is quantitative. In
    order to see if gender has an effect on wages, we would dummy code when the person
    is a female to female = 1, and female = 0 when the person is male.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的回归分析来确定工资。比如说，我们被给出了性别，这是一个定性变量，以及教育年限，这是一个定量变量。为了看看性别是否对工资有影响，我们会在女性时将虚拟编码为女性
    = 1，在男性时将女性编码为0。
- en: When working with dummy variables, it is important to be aware of and avoid
    the dummy variable trap. The dummy variable trap is when you have independent
    variables that are multicollinear, or highly correlated. Simply put, these variables
    can be predicted from each other. So, in our gender example, the dummy variable
    trap would be if we include both female as (0|1) and male as (0|1), essentially
    creating a duplicate category. It can be inferred that a 0 female value indicates
    a male.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用虚拟变量时，重要的是要意识到并避免虚拟变量陷阱。虚拟变量陷阱是指你拥有独立的变量是多线性的，或者高度相关的。简单来说，这些变量可以从彼此预测。所以，在我们的性别例子中，虚拟变量陷阱就是如果我们同时包含女性作为（0|1）和男性作为（0|1），实际上创建了一个重复的分类。可以推断出0个女性值表示男性。
- en: To avoid the dummy variable trap, simply leave out the constant term or one
    of the dummy categories. The left out dummy can become the base category to which
    the rest are compared to.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免虚拟变量陷阱，只需省略常数项或其中一个虚拟类别。省略的虚拟变量可以成为与其他变量比较的基础类别。
- en: 'Let''s come back to our dataset and employ some methods to encode our categorical
    data into dummy variables. pandas has a handy `get_dummies` method that actually
    finds all of the categorical variables and dummy codes them for us:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的数据集，并采用一些方法将我们的分类数据编码为虚拟变量。pandas有一个方便的`get_dummies`方法，实际上它会找到所有的分类变量，并为我们进行虚拟编码：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have to be sure to specify which columns we want to apply this to because
    it will also dummy code the ordinal columns, and this wouldn't make much sense.
    We will take a more in-depth look into why dummy coding ordinal data doesn't makes
    sense shortly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保指定我们想要应用到的列，因为它也会对序数列进行虚拟编码，这不会很有意义。我们将在稍后更深入地探讨为什么对序数数据进行虚拟编码没有意义。
- en: 'Our data, with our dummy coded columns, now looks like this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据，加上我们的虚拟编码列，现在看起来是这样的：
- en: '|  | **ordinal_column** | **quantitative_column** | **city__london** | **city_san
    francisco** | **city_seattle** | **city_tokyo** | **boolean_no** | **boolean_yes**
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | **ordinal_column** | **quantitative_column** | **city__london** | **city_san
    francisco** | **city_seattle** | **city_tokyo** | **boolean_no** | **boolean_yes**
    |'
- en: '| **0** | somewhat like | 1.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **0** | somewhat like | 1.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
- en: '| **1** | like | 11.0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **1** | like | 11.0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **2** | somewhat like | -0.5 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **2** | somewhat like | -0.5 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| **3** | like | 10.0 | 0 | 0 | 1 | 0 | 1 | 0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **3** | like | 10.0 | 0 | 0 | 1 | 0 | 1 | 0 |'
- en: '| **4** | somewhat like | NaN | 0 | 1 | 0 | 0 | 1 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **4** | somewhat like | NaN | 0 | 1 | 0 | 0 | 1 | 0 |'
- en: '| **5** | dislike | 20.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **5** | dislike | 20.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
- en: Our other option for dummy coding our data is to create our own custom dummifier.
    Creating this allows us to set up a pipeline to transform our whole dataset in
    one go.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据进行虚拟编码的另一种选择是创建自己的自定义虚拟化器。创建这个虚拟化器允许我们设置一个管道，一次将整个数据集转换。
- en: 'Once again, we will use the same structure as our previous two custom imputers.
    Here, our `transform` method will use the handy pandas `get_dummies` method to
    create dummy variables for specified columns. The only parameter we have in this
    custom dummifier is `cols`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们将使用与之前两个自定义填充器相同的结构。在这里，我们的`transform`方法将使用方便的pandas `get_dummies`方法为指定的列创建虚拟变量。在这个自定义虚拟化器中，我们唯一的参数是`cols`：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our custom dummifier mimics scikit-learn's `OneHotEncoding`, but with the added
    advantage of working on our entire DataFrame.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的定制虚拟化器模仿scikit-learn的`OneHotEncoding`，但具有在完整DataFrame上工作的附加优势。
- en: Encoding at the ordinal level
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对序数级别的编码
- en: Now, let's take a look at our ordinal columns. There is still useful information
    here, however, we need to transform the strings into numerical data. At the ordinal
    level, since there is meaning in the data having a specific order, it does not
    make sense to use dummy variables. To maintain the order, we will use a label
    encoder.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: By a label encoder, we mean that each label in our ordinal data will have a
    numerical value associated to it. In our example, this means that the ordinal
    column values (`dislike`, `somewhat like`, and `like`) will be represented as
    `0`, `1`, and `2`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'In the simplest form, the code is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we have set up a list for ordering our labels. This is key, as we will
    be utilizing the index of our list to transform the labels to numerical data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will implement a function called `map` on our column, that allows
    us to specify the function we want to implement on the column. We specify this
    function using a construct called `lambda`, which essentially allows us to create
    an anonymous function, or one that is not bound to a name:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This specific code is creating a function that will apply the index of our
    list called `ordering` to each element. Now, we map this to our ordinal column:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our ordinal column is now represented as labeled data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Note that scikit-learn has a `LabelEncoder`, but we are not using this method
    because it does not include the ability to order categories (`0` for dislike,
    `1` for somewhat like, `2` for like) as we have done previously. Rather, the default
    is a sorting method, which is not what we want to use here.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, let us make a custom label encoder that will fit into our pipeline:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We have maintained the structure of the other custom transformers in this chapter.
    Here, we have utilized the `map` and `lambda` functions detailed previously to
    transform the specified columns. Note the key parameter, `ordering`, which will
    determine which numerical values the labels will be encoding into.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call our custom encoder:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Our dataset after these transformations looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | 1 | 1.0 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | 2 | 11.0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | 1 | -0.5 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | 2 | 10.0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | 1 | NaN |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | 0 | 20.0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: Our ordinal column is now labeled.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to this point, we have transformed the following columns accordingly:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean`, `city`: dummy encoding'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ordinal_column`: label encoding'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bucketing continuous features into categories
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, when you have continuous numerical data, it may make sense to transform
    a continuous variable into a categorical variable. For example, say you have ages,
    but it would be more useful to work with age ranges.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: pandas has a useful function called `cut` that will bin your data for you. By
    binning, we mean it will create the ranges for your data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how this function could work on our `quantitative_column`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the `cut` function for our quantitative column looks like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When we specify `bins` to be an integer (`bins = 3`), it defines the number
    of equal–width bins in the range of `X`. However, in this case, the range of `X`
    is extended by .1% on each side to include the min or max values of `X`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set labels to `False`, which will return only integer indicators
    of the `bins`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is what the integer indicators look like for our `quantitative_column`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Seeing our options with the `cut` function, we can also build our own `CustomCutter`
    for our pipeline. Once again, we will mimic the structure of our transformers.
    Our `transform` method will use the `cut` function, and so we will need to set
    `bins` and `labels` as parameters:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that we have set the default labels parameter to `False`. Initialize our
    `CustomCutter`, specifying the column to transform and the number of bins to use:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With our `CustomCutter` transforming our `quantitative_column`, our data now
    looks like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | like | 11.0 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | somewhat like | -0.5 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: Note that our `quantitative_column` is now ordinal, and so there is no need
    to dummify the data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Creating our pipeline
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To review, we have transformed the columns in our dataset in the following
    ways thus far:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean, city`: dummy encoding'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ordinal_column`: label encoding'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quantitative_column`: ordinal level data'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we now have transformations for all of our columns, let's put everything
    together in a pipeline.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with importing our `Pipeline` class from scikit-learn:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will bring together each of the custom transformers that we have created.
    Here is the order we will follow in our pipeline:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: First, we will utilize the `imputer` to fill in missing values
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will dummify our categorical columns
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will encode the `ordinal_column`
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we will bucket the `quantitative_column`
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s set up our pipeline as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In order to see the full transformation of our data using our pipeline, let''s
    take a look at our data with zero transformations:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is what our data looked like in the beginning before any transformations
    were made:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| **1** | no | None | like | 11.0 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| **2** | None | london | somewhat like | -0.5 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: 'We can now `fit` our pipeline:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have created our pipeline object, let''s transform our DataFrame:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is what our final dataset looks like after undergoing all of the appropriate
    transformations by column:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ordinal_column** | **quantitative_column** | **boolean_no** | **boolean_yes**
    | **city_london** | **city_san francisco** | **city_seattle** | **city_tokyo**
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| **3** | 2 | 1 | 1 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| **5** | 0 | 2 | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: Extending numerical features
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numerical features can undergo various methods to create extended features from
    them. Previously, we saw how we can transform continuous numerical data into ordinal
    data. Now, we will dive into extending our numerical features further.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Before we go any deeper into these methods, we will introduce a new dataset
    to work with.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Activity recognition from the Single Chest-Mounted Accelerometer dataset
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This dataset collects data from a wearable accelerometer, mounted on the chest,
    collected from fifteen participants performing seven activities. The sampling
    frequency of the accelerometer is 52 Hz and the accelerometer data is uncalibrated.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is separated by participant and contains the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Sequential number
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x acceleration
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: y acceleration
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: z acceleration
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Labels are codified by numbers and represent an activity, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Working at a computer
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standing up, walking, and going up/down stairs
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standing
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going up/down stairs
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walking and talking with someone
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talking while standing
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Further information on this dataset is available on the UCI *Machine Learning
    Repository* at:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at our data. First, we need to load in our CSV file and
    set our column headers:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s examine the first few rows with the `.head` method, which will
    default to the first five rows, unless we specify how many rows to show:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This shows us:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **index** | **x** | **y** | **z** | **activity** |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| **0** | 0.0 | 1502 | 2215 | 2153 | 1 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1.0 | 1667 | 2072 | 2047 | 1 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| **2** | 2.0 | 1611 | 1957 | 1906 | 1 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| **3** | 3.0 | 1601 | 1939 | 1831 | 1 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: '| **4** | 4.0 | 1643 | 1965 | 1879 | 1 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
- en: 'This dataset is meant to train models to recognize a user''s current physical
    activity given an accelerometer''s `x`, `y`, and `z` position on a device such
    as a smartphone. According to the website, the options for the `activity` column
    are:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '**1**: Working at a computer'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: Standing Up and Going updown stairs'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: Standing'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4**: Walking'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**5**: Going UpDown Stairs'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**6**: Walking and Talking with Someone'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**7**: Talking while Standing'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `activity` column will be the target variable we will be trying to predict,
    using the other columns. Let''s determine the null accuracy to beat in our machine
    learning model. To do this, we will invoke the `value_counts` method with the
    `normalize` option set to `True` to give us the most commonly occurring activity
    as a percentage:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The null accuracy to beat is 51.53%, meaning that if we guessed seven (talking
    while standing), then we would be right over half of the time. Now, let's do some
    machine learning! Let's step through, line by line, setting up our model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have our `import` statements:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You may be familiar with these import statements from last chapter. Once again,
    we will be utilizing scikit-learn''s **K-Nearest Neighbors** (**KNN**) classification
    model. We will also use the grid search module that automatically finds the best
    combination of parameters for the KNN model that best fits our data with respect
    to cross-validated accuracy. Next, we create a feature matrix (`X`) and a response
    variable (`y`) for our predictive model:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once our `X` and `y` are set up, we can introduce the variables and instances
    we need to successfully run a grid search:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we will instantiate a KNN model and a grid search module and fit it to
    our feature matrix and response variable:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can `print` the best accuracy and parameters that were used to learn
    from:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Using five neighbors as its parameter, our KNN model was able to achieve a 72.07%
    accuracy, much better than our null accuracy of around 51.53%! Perhaps we can
    utilize another method to get our accuracy up even more.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial features
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key method of working with numerical data and creating more features is through
    scikit-learn's `PolynomialFeatures` class. In its simplest form, this constructor will
    create new columns that are products of existing columns to capture feature interactions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, this class will generate a new feature matrix with all of
    the polynomial combinations of the features with a degree less than or equal to
    the specified degree. Meaning that, if your input sample is two-dimensional, like
    so: [a, b], then the degree-2 polynomial features are as follows: [1, a, b, a^2,
    ab, b^2].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Parameters
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When instantiating polynomial features, there are three parameters to keep
    in mind:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: degree
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interaction_only`'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_bias`'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Degree corresponds to the degree of the polynomial features, with the default
    set to two.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '`interaction_only` is a boolean that, when true, only interaction features
    are produced, meaning features that are products of degree distinct features.
    The default for `interaction_only` is false.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '`include_bias` is also a boolean that, when true (default), includes a `bias`
    column, the feature in which all polynomial powers are zero, adding a column of
    all ones.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up a polynomial feature instance by first importing the class and
    instantiating with our parameters. At first, let''s take a look at what features
    we get when setting `interaction_only` to `False`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can `fit_transform` these polynomial features to our dataset and look
    at the `shape` of our extended dataset:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Our dataset has now expanded to `162501` rows and `9` columns.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s place our data into a DataFrame, setting the column headers to the `feature_names`,
    and taking a look at the first few rows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This shows us:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **x0** | **x1** | **x2** | **x0^2** | **x0 x1** | **x0 x2** | **x1^2**
    | **x1 x2** | **x2^2** |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1502.0 | 2215.0 | 2153.0 | 2256004.0 | 3326930.0 | 3233806.0 | 4906225.0
    | 4768895.0 | 4635409.0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1667.0 | 2072.0 | 2047.0 | 2778889.0 | 3454024.0 | 3412349.0 | 4293184.0
    | 4241384.0 | 4190209.0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1611.0 | 1957.0 | 1906.0 | 2595321.0 | 3152727.0 | 3070566.0 | 3829849.0
    | 3730042.0 | 3632836.0 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1601.0 | 1939.0 | 1831.0 | 2563201.0 | 3104339.0 | 2931431.0 | 3759721.0
    | 3550309.0 | 3352561.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1643.0 | 1965.0 | 1879.0 | 2699449.0 | 3228495.0 | 3087197.0 | 3861225.0
    | 3692235.0 | 3530641.0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: Exploratory data analysis
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we can conduct some exploratory data analysis. Since the purpose of polynomial
    features is to get a better sense of feature interaction in the original data,
    the best way to visualize this is through a correlation `heatmap`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to import a data visualization tool that will allow us to create a
    `heatmap`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Matplotlib and Seaborn are popular data visualization tools. We can now visualize
    our correlation `heatmap` as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`.corr` is a function we can call on our DataFrame that gives us a correlation
    matrix of our features. Let''s take a look at our feature interactions:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d80a9fbb-3ed3-461d-add6-513e6fb79d8f.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: The colors on the `heatmap` are based on pure values; the darker the color,
    the greater the correlation of the features.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked at our polynomial features with our `interaction_only`
    parameter set to `False`. Let's set this to `True` and see what our features look
    like without repeat variables.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up this polynomial feature instance the same as we did previously.
    Note the only difference is that `interaction_only` is now `True`:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We now have `162501` rows by `6` columns. Let''s take a look:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The DataFrame now looks as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **x0** | **x1** | **x2** | **x0 x1** | **x0 x2** | **x1 x2** |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1502.0 | 2215.0 | 2153.0 | 3326930.0 | 3233806.0 | 4768895.0 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| **1** | 1667.0 | 2072.0 | 2047.0 | 3454024.0 | 3412349.0 | 4241384.0 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1611.0 | 1957.0 | 1906.0 | 3152727.0 | 3070566.0 | 3730042.0 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| **3** | 1601.0 | 1939.0 | 1831.0 | 3104339.0 | 2931431.0 | 3550309.0 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| **4** | 1643.0 | 1965.0 | 1879.0 | 3228495.0 | 3087197.0 | 3692235.0 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: 'Since `interaction_only` has been set to `True` this time, `x0^2`, `x1^2`,
    and `x2^2` have disappeared since they were repeat variables. Let''s see what
    our correlation matrix looks like now:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We get the following result:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17488850-dc5c-4e56-8aed-7882370d9704.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: 'We are able to see how the features interact with each other. We can also perform
    a grid search of our KNN model with the new polynomial features, which can also
    be grid searched in a pipeline:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s set up our pipeline parameters first:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, instantiate our `Pipeline`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'From here, we can set up our grid search and print the best score and parameters
    to learn from:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Our accuracy is now 72.12%, which is an improvement from our accuracy without
    expanding our features using polynomial features!
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Text-specific feature construction
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until this point, we have been working with categorical and numerical data.
    While our categorical data has come in the form of a string, the text has been
    part of a single category. We will now dive deeper into longer—form text data.
    This form of text data is much more complex than single—category text, because
    we now have a series of categories, or tokens.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Before we get any further into working with text data, let's make sure we have
    a good understanding of what we mean when we refer to text data. Consider a service
    like Yelp, where users write up reviews of restaurants and businesses to share
    their thoughts on their experience. These reviews, all written in text format,
    contain a wealth of information that would be useful for machine learning purposes,
    for example, in predicting the best restaurant to visit.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In general, a large part of how we communicate in today's world is through written
    text, whether in messaging services, social media, or email. As a result, so much
    can be garnered from this information through modeling. For example, we can conduct
    a sentiment analysis from Twitter data.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: This type of work can be referred to as **natural language processing** (**NLP**).
    This is a field primarily concerned with interactions between computers and humans,
    specifically where computers can be programmed to process natural language.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we've mentioned before, it's important to note that all machine learning
    models require numerical inputs, so we have to be creative and think strategically
    when we work with text and convert such data into numerical features. There are
    several options for doing so, so let's get started.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Bag of words representation
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn has a handy module called `feature_extraction` that allows
    us to, as the name suggests, extract features for data such as text in a format
    supported by machine learning algorithms. This module has methods for us to utilize
    when working with text.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, we may refer to our text data as a corpus, specifically meaning
    an aggregate of text content or documents.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common method to transform a corpus into a numerical representation,
    a process known as vectorization, is through a method called **bag-of-words**.
    The basic idea behind the bag of words approach is that documents are described
    by word occurrences while completely ignoring the positioning of words in the
    document. In its simplest form, text is represented as a **bag**,without regard
    for grammar or word order, and is maintained as a set, with importance given to
    multiplicity. A bag of words representation is achieved in the following three
    steps:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with tokenizing. This process uses white spaces and punctuation
    to separate words from each other, turning them into tokens. Each possible token
    is given an integer ID.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Next comes counting. This step simply counts the occurrences of tokens within
    a document.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Last comes normalizing, meaning that tokens are weighted with diminishing importance
    when they occur in the majority of documents.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a couple more methods for vectorizing.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: CountVectorizer
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CountVectorizer` is the most commonly used method to convert text data into
    their vector representations. It is similar to dummy variables, in the sense that
    `CountVectorizer` converts text columns into matrices where columns are tokens
    and cell values are counts of occurrences of each token in each document. The
    resulting matrix is referred to as a **document-term matrix** because each row
    will represent a **document** (in this case, a tweet) and each column represents
    a **term** (a word).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at a new dataset, and see how `CountVectorizer` works. The
    Twitter Sentiment Analysis dataset contains 1,578,627 classified tweets, and each
    row is marked as one for positive sentiment and zero for negative sentiment.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Further information on this dataset can be found at [http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/.](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/)
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load in our data using pandas'' `read_csv` method. Note that we are
    specifying an `encoding` as an optional parameter to ensure that we handle all
    special characters in the tweets properly:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This allows us to load in our data in a specific format and map text characters
    appropriately.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the first few rows of data:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We get the following data:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **ItemID** | **Sentiment** | **SentimentText** |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| **0** | 1 | 0 | is so sad for my APL frie... |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| **1** | 2 | 0 | I missed the New Moon trail... |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| **2** | 3 | 1 | omg its already 7:30 :O |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| **3** | 4 | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| **4** | 5 | 0 | i think mi bf is cheating on me!!! ... |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: 'We are only concerned with the `Sentiment` and `SentimentText` columns, so
    we will delete the `ItemID` column for now:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Our data looks as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Sentiment** | **SentimentText** |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| **0** | 0 | is so sad for my APL frie... |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0 | I missed the New Moon trail... |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| **2** | 1 | omg its already 7:30 :O |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| **4** | 0 | i think mi bf is cheating on me!!! ... |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: 'Now, we can import `CountVectorizer` and get a better understanding of the
    text we are working with:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s set up our `X` and `y`:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The `CountVectorizer` class works very similarly to the custom transformers
    we have been working with so far, and has a `fit_transform` function to manipulate
    the data:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: After our `CountVectorizer` has transformed our data, we have 99,989 rows and
    105,849 columns.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` has many different parameters that can change the number
    of features that are constructed. Let''s go over a few of these parameters to
    get a better sense of how these features are created.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: CountVectorizer parameters
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A few parameters that we will go over include:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '`stop_words`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_df`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_df`'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ngram_range`'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`analyzer`'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop_words` is a frequently used parameter in `CountVectorizer`. You can pass
    in the string `english` to this parameter, and a built-in stop word list for English
    is used. You can also specify a list of words yourself. These words will then
    be removed from the tokens and will not appear as features in your data.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You can see that the feature columns have gone down from 105,849 when stop words
    were not used, to 105,545 when English stop words have been set. The idea behind
    using stop words is to remove noise within your features and take out words that
    occur so often that there won't be much meaning to garner from them in your models.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Another parameter is called `min_df`. This parameter is used to skim the number
    of features, by ignoring terms that have a document frequency lower than the given
    threshold or cut-off.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an implementation of our `CountVectorizer` with `min_df`:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This is a method that is utilized to significantly reduce the number of features
    created.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a parameter called `max_df`:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This is similar to trying to understand what stop words exist in the document.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the `ngram_range` parameter. This parameter takes in a
    tuple where the lower and upper boundary of the range of n-values indicates the
    number of different n-grams to be extracted. N-grams represent phrases, so a value
    of one would represent one token, however a value of two would represent two tokens
    together. As you can imagine, this will expand our feature set quite significantly:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: See, we now have 3,219,557 features. Since sets of words (phrases) can sometimes
    have more meaning, using n-gram ranges can be useful for modeling.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also set an analyzer as a parameter in `CountVectorizer`. The analyzer
    determines whether the feature should be made of word or character n-grams. Word
    is the default:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Given that word is the default, our feature column number doesn't change much
    from the original.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: We can even create our own custom analyzer. Conceptually, words are built from
    root words, or stems, and we can construct a custom analyzer that accounts for
    this.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Stemming is a common natural language processing method that allows us to stem
    our vocabulary, or make it smaller by converting words to their roots. There is
    a natural language toolkit, known as NLTK, that has several packages that allow
    us to perform operations on text data. One such package is a `stemmer`.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it works:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import our `stemmer` and then initialize it:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, let''s see how some words are stemmed:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'So, the word `interesting` can be reduced to the root stem. We can now use
    this to create a function that will allow us to tokenize words into their stems:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s see what our function outputs:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We can now place this tokenizer function into our analyzer parameter:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This yields us fewer features, which intuitively makes sense since our vocabulary
    has reduced with stemming.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` is a very useful tool to help us expand our features and
    convert text to numerical features. There is another common vectorizer that we
    will look into.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: The Tf-idf vectorizer
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A `Tf-idfVectorizer` can be broken down into two components. First, the *tf*
    part, which represents **term frequency**, and the *idf* part, meaning **inverse
    document frequency**. It is a term—weighting method that has applications in information—retrieval
    and clustering.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: 'A weight is given to evaluate how important a word is to a document in a corpus.
    Let''s look into each part a little more:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '**tf: term frequency**: Measures how frequently a term occurs in a document.
    Since documents can be different in length, it is possible that a term would appear
    many more times in longer documents than shorter ones. Thus, the term frequency
    is often divided by the document length, or the total number of terms in the document,
    as a way of normalization.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**idf: i****nverse document frequency**: Measures how important a term is.
    While computing term frequency, all terms are considered equally important. However,
    certain terms, such as *is*, *of*, and *that*, may appear a lot of times but have
    little importance. So, we need to weight the frequent terms less, while we scale
    up the rare ones.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To re-emphasize, a `TfidfVectorizer` is the same as `CountVectorizer`, in that
    it constructs features from tokens, but it takes a step further and normalizes
    counts to frequency of occurrences across a corpus. Let's see an example of this
    in action.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'First, our import:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'To bring up some code from before, a plain vanilla `CountVectorizer` will output
    a document-term matrix:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Our  `TfidfVectorizer` can be set up as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We can see that both vectorizers output the same number of rows and columns,
    but produce different values in each cell. This is because `TfidfVectorizer` and
    `CountVectorizer` are both used to transform text data into quantitative data,
    but the way in which they fill in cell values differ.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Using text in machine learning pipelines
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, the ultimate goal of our vectorizers is to use them to make text
    data ingestible for our machine learning pipelines. Because `CountVectorizer`
    and `TfidfVectorizer` act like any other transformer we have been working with
    in this book, we will have to utilize a scikit-learn pipeline to ensure accuracy
    and honesty in our machine learning pipeline. In our example, we are going to
    be working with a large number of columns (in the hundreds of thousands), so I
    will use a classifier that is known to be more efficient in this case, a Naive
    Bayes model:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Before we start building our pipelines, let''s get our null accuracy of the
    response column, which is either zero (negative) or one (positive):'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Making the accuracy beat 56.5%. Now, let''s create a pipeline with two steps:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '`CountVectorizer` to featurize the tweets'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultiNomialNB` Naive Bayes model to classify between positive and negative
    sentiment'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First let''s start with setting up our pipeline parameters as follows, and
    then instantiate our grid search as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: And we got 75.6%, which is great! Now, let's kick things into high-gear and
    incorporate the `TfidfVectorizer`. Instead of rebuilding the pipeline using tf-idf
    instead of `CountVectorizer`, let's try using something a bit different. The scikit-learn
    has a `FeatureUnion` module that facilitates horizontal stacking of features (side-by-side).
    This allows us to use multiple types of text featurizers in the same pipeline.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can build a `featurizer` that runs both a `TfidfVectorizer`
    and a `CountVectorizer` on our tweets and concatenates them horizontally (keeping
    the same number of rows but increasing the number of columns):'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Once we build the `featurizer`, we can use it to see how it affects the shape
    of our data:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We can see that unioning the two featurizers results in a dataset with the
    same number of rows, but doubles the number of either the `CountVectorizer` or
    the `TfidfVectorizer`. This is because the resulting dataset is literally both
    datasets side-by-side. This way, our machine learning models may learn from both
    sets of data simultaneously. Let''s change the `params` of our `featurizer` object
    slightly and see what difference it makes:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Let''s build a much more comprehensive pipeline that incorporates the feature
    union of both of our vectorizers:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Nice, even better than just `CountVectorizer` alone! It is also interesting
    to note that the best `ngram_range` for the `CountVectorizer` was `(1, 2)`, while
    it was `(1, 1)` for the `TfidfVectorizer`, implying that word occurrences alone
    were not as important as two-word phrase occurrences.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'By this point, it should be obvious that we could have made our pipeline much
    more complicated by:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Grid searching across dozens of parameters for each vectorizer
  id: totrans-458
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding in more steps to our pipeline such as polynomial feature construction
  id: totrans-459
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: But this would have been very cumbersome for this text and would take hours
    to run on most commercial laptops. Feel free to expand on this pipeline and beat
    our score!
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: Phew, that was a lot. Text can be difficult to work with. Between sarcasm, misspellings,
    and vocabulary size, data scientists and machine learning engineers have their
    hands full. This introduction to working with text will allow you, the reader,
    to experiment with your own large text datasets and obtain your own results!
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we have gone over several methods of imputing missing values in our
    categorical and numerical data, encoding our categorical variables, and creating
    custom transformers to fit into a pipeline. We also dove into several feature
    construction methods for both numerical data and text-based data.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a look at the features we have constructed,
    and consider appropriate methods of selecting the right features to use for our
    machine learning models.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
