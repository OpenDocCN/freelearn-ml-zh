- en: Chapter 6. Step 3 – Validating the Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we estimated the language of new countries starting
    from their flag. For this purpose, we used KNN algorithm that is a supervised
    learning algorithm. We built KNN and measured its accuracy cross validating the
    estimated language. In this chapter, we will see how to measure the accuracy in
    a more reliable way and we will tune the KNN parameters to improve its performance.
    To be able to do the tasks in this chapter, it's not necessary for you to have
    read the previous chapter, although it is recommended so that you can order to
    understand how the KNN algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Validate the accuracy of an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the algorithm parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the most relevant data features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the parameters and the features together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating a machine learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting from a table describing the countries, flags and their language, the
    KNN estimates a new country language starting from its flag attributes. In this
    chapter, we will evaluate the performance of KNN.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the accuracy of an algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already evaluated the algorithm accuracy by cross validating the estimated
    language. First, we split the data in two parts that are the training set and
    the test set. Then, we built the KNN algorithm using the training set in order
    to estimate the test set countries' language. Counting how many times the estimated
    language was correct, we defined an accuracy index as the percentage of correct
    guesses. The accuracy depends on which data we put into the test set. Since we
    randomly defined the training set countries, the accuracy changes every time we
    repeat the cross validation. Then, the result of this approach is not reliable.
  prefs: []
  type: TYPE_NORMAL
- en: The target of this chapter is to evaluate KNN using a reliable technique in
    the sense that the accuracy doesn't change validating the same model twice. Repeating
    the train/set split and the validation many times, almost every country will be
    in both the training and the test set at least once. We can compute the average
    accuracy and it will take account of all the countries in both the training and
    the test sets. After a few iterations, the average accuracy will be reliable since
    it won't significantly change increasing the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before evaluating KNN, we need to load the `kknn` and `data.table` packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can define a function building and cross validating KNN using a defined
    set of parameters and data so that we can quickly evaluate the algorithm with
    any configuration. Since the R commands are similar to the previous chapter, we
    will go quickly through them. The input of the functions is:'
  prefs: []
  type: TYPE_NORMAL
- en: A table containing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector containing the name of the features that we use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define which rows belong to the training and test sets. We build `indexTrain`,
    which is a vector specifying which rows will be in the training set. We set a
    probability of 10 percent for a flag to be in the test set. In [Chapter 5](ch05.html
    "Chapter 5. Step 2 – Applying Machine Learning Techniques"), *Step 2 – Applying
    Machine Learning Techniques*, we set a probability of 20 percent, but in this
    chapter we will repeat the validation many times, so 10 percent is enough.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Starting from `indexTrain`, extract the rows going into `dtTrain` and into `dtTest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the formula defining the features and the attribute to predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build KNN using the input parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the `languageFitted` vector containing the estimated language of the
    test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count how many times `languageFitted` is the same as the real language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the accuracy index as the number of times the predicted language and
    the real language match, divided by the number of countries in the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the R code to build the function. The comments reflect the numbered
    bullet points, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `validateKnn` is the starting point to validate the KNN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the average accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to use `validateKnn`, we need to define the input, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data table with the features, as shown:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The vector containing all the possible features to include in KNN:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The KNN parameters that can either be set or left as their defaults.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we have all the elements to be able to use `validateKnn`. We can use a
    random subset of them, for instance, the first 10 features. With regard to the
    parameters, we can leave all of them to their default, except `k` that is equal
    to `8`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Running `validateKnn` more than once, we can notice that the result changes
    every time, as expected. However, now we can define another function running `validateKnn`
    multiple times. Then, we compute the accuracy average and use it as a reliable
    performance index. Our new function is called `cvKnn` because it cross validates
    KNN a defined number of times.
  prefs: []
  type: TYPE_NORMAL
- en: The `cvKnn` arguments are the data table, the number of iterations, the feature
    names, and the KNN parameters. Let's start defining the data table and the number
    of iterations. All the other input is the same as `validateKnn`. In order to have
    clear and compact code, we can use the ellipsis (...) specifying that we can add
    other arguments. Then, we can pass these arguments to any function using the ellipsis
    again. This means that when we will call `validateKnn`, we can use `validateKnn(...)`
    to specify that any extra argument of `cvKnn` will be an input for `validateKnn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining an empty vector `arrayPercCorrect` that will contain the accuracies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running `validateKnn` and defining `arrayPercCorrect`, which contains the accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding the accuracy `arrayPercCorrect` to `arrayPercCorrect`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the code that builds the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use `cvKnn` to build and validate KNN 500 times. Then, we compute
    the average accuracy as a KNN performance index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We define `percCorrectMean`, which can be used as an accuracy index.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the average accuracy computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to see how much the result changed at any iteration, we can compare
    each step''s accuracy with their average. First, we build a chart with the accuracies
    using `plot` and the parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: This is the vector that we want to plot (`arrayPercCorrect`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ylim`: This is the accuracy that is a number between 0 and 1\. With `ylim
    = c(0, 1)`, we specify that the region that we visualize is between 0 and 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlab` and `ylab`: These are the axis labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main`: This is the title.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to compare the accuracies with their average, we can display the average
    by drawing a red dashed horizontal line with `abline`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the values'' range by drawing a horizontal line for both the
    minimum and the maximum range, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the average accuracy computation](img/7740OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy varies a lot from one iteration to another and the range is between
    0 percent and 70 percent. As expected, a single accuracy is completely unreliable.
    What about the average among 500 iterations? How many iterations do we need to
    have a stable result?
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the accuracy index at the first iteration, then the average
    among the first two iterations, then the average among the first three, and so
    on. If at any point the average stops changing, we don't need to go any further.
    By building a chart we can observe how many iterations it takes to reach a stable
    average.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define `arrayCumulate` containing the cumulated average that
    is the partial average until each iteration, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same commands as before, we build a new chart. The only new argument
    is `type=''l''` and it specifies that we display a line instead of points. In
    order to zoom into the area with the averages, we remove the `ylim` argument,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing the average accuracy computation](img/7740OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can notice that the accuracy is nearly stable after the 100 iterations. Assuming
    that it won't change too much with different parameter configuration, we can use
    100 iterations to validate the KNN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how to automatically evaluate a model performance
    using a specific set of features and some defined parameters. In the following
    sections, we will use this function to optimize the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section shows you how to improve the performance of KNN by tuning its
    parameters. We are dealing with the *k* parameter that defines the number of neighbors.
    Use these steps to identify the *k* parameter performing best:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define which values of *k* we will test. The KNN works locally, in the sense
    that given a new country flag it identifies just a few similar flags. How many
    of them should we use at most? Since there are less than 200 flags in total, we
    don''t want to use more than 50 flags. Then, we should test each *k* between 1
    and 50 and we can define `arrayK` containing the options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the number of iterations. For each *k* in `arrayK`, we need to build
    and validate the KNN a sufficiently high amount of times defined by `nIterations`.
    In the previous chapter, we learned that we need at least 100 iterations to have
    a meaningful KNN accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluate the accuracy for each *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the *k* that maximizes the accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last two steps are more detailed and we will explore them in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to measure the accuracy for each *k*, we define `dtAccuracyK` as an
    empty data table that will contain the accuracies. Then, we use a `for` loop to
    run KNN with each *k* in *arrayK* and add the new results. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Run and validate KNN using `cvKnn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the rows that we will add to `dtAccuracyK` containing the accuracy and
    the *k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the new rows to `dtAccuracyK` using `rbind`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s take a look at `result.head(dtAccuracyK)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Each row of `dtAccuracyK` contains an iteration of KNN. The first column displays
    the accuracy and the second column displays the *k* used in the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to visualize the results, we can use `plot`. The two dimensions that
    we want to visualize are the *k* and the accuracy. The input is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`, `y`: These are the plot dimensions that are the `k` and `accuracy` columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xlab`, `ylab`: These are the axis labels that are `k` and `accuracy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main`: This is the chart title'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ylim`: These are the *y* region limits that are `0` and `1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col`: This is the color of the points that is gray, in order to put emphasis
    on the black points that we will add later'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning the parameters](img/7740OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also use `type = 'str(dtCvK)'` instead of `type = 'o'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We cannot notice any relevant difference depending on *k*. The reason is that
    the accuracy varies a lot from one iteration to another. In order to identify
    *k* performing better, we can compute the average performance for each *k*. We
    call the new data table `dtCvK` because we''re cross validating the model, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `dtCvK` contains the average accuracy of each *k*. We can add it to the
    chart using points that is a function adding the new points to the current chart.
    In order to make the points more visible, we display full points using `pch =
    16`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning the parameters](img/7740OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The average accuracies vary across the *k*, but it is hard to notice the difference
    because it is always around 0.3 to 0.4\. In order to see the difference more clearly,
    we can plot just the average without visualizing the *y* limits, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also use `type = 'str(dtCvK)'` instead of type = `'o'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can identify the *k* performing best and add it to the chart using `abline`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also use `kOpt <- 27` instead of `kOpt <- dtCvK[accuracy == max(accuracy),
    k]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning the parameters](img/7740OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The optimal *k* is 27 and the KNN performs very well if the *k* is in the 22
    to 30 range.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we identified *k* performing at its best. However, there are
    still other parameters that we haven't optimized, such as the distance method.
    In addition, we can improve the algorithm selecting the features to include and
    we will explore it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the data features to include in the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we set a KNN parameter maximizing the performance.
    Another tuning option is to define which data we use to build the model. Our table
    describes the flags using 37 features and we included all of them in the model.
    However, KNN might perform better including only a subset of them.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to select the features is to use a filter (as anticipated in
    the *Ranking the features using a filter or a dimensionality reduction* section
    in [Chapter 4](ch04.html "Chapter 4. Step 1 – Data Exploration and Feature Engineering"),
    *Step 1 – Data Exploration and Feature Engineering*) that estimates the impact
    of each feature and includes only the most relevant features. After ranking all
    the features on the basis of their relevance, we can define the `n` parameters
    specifying how many of them we include in the model. Then, we can maximize the
    accuracy depending on `n`, using an approach similar to the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is defining how to rank the features. We can use the information
    gain ratio filter that estimated the impact of each feature ignoring the others.
    We have already talked about the information gain ratio and its limitations (refer
    to the *Ranking the features using a filter or a dimensionality reduction* section
    in [Chapter 4](ch04.html "Chapter 4. Step 1 – Data Exploration and Feature Engineering"),
    *Step 1 – Data Exploration and Feature Engineering*) and we will use the same
    R commands, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `arrayFeatures` contains the features sorted by relevance. Now, we can
    build the model choosing the top *n* features. The options for *n* are the numbers
    between `1` and the total number of features, and we define `arrayN` containing
    them, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to store the accuracy of each iteration, we define `dtAccuracyN` as
    an empty data table and we iteratively add the rows using a `for` loop. The steps
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Validate KNN using `cvKnn` and store the accuracies in `arrayAccuracy`. We set
    the *k* parameter equal to `kOpt (27)`, that is, the optimal *k* defined in the
    previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the `rowsAccuracyN` data table with the rows to add.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the new rows to `dtAccuracyN` using `rbind`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the code generating the `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `dtAccuracyN` contains each iteration accuracy, depending on *n*. We
    can build a chart containing all the accuracies and their average across different
    values of *n*, by using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a chart displaying the accuracy at each iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Starting from `dtAccuracyN`, compute the average accuracy for each iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the points with the average accuracy to the chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Selecting the data features to include in the model](img/7740OS_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The chart shows that we achieved the best accuracies using high values of *n*.
    In order to identify the best *n*, we can plot just their averages. Then, we define
    `nOpt` that is the *n* performing best and we add a red vertical line corresponding
    to it, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Selecting the data features to include in the model](img/7740OS_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of features performing best is **15** and the performance decreases
    slowly after this point.
  prefs: []
  type: TYPE_NORMAL
- en: In the chart, we can notice that there are some points in which the accuracy
    decreases a lot adding a new feature (for example, **3**, **11**, **13**). In
    these points, we are adding a feature that decreases the performance. What if
    we just decide not to include it? We can start building the model using the most
    relevant feature only, and then add the second most relevant feature. If the performance
    improves, we keep the second feature; otherwise, we discard it. After that, we
    do the same with the third feature and we repeat this until we have added or discarded
    each feature. This approach is called wrapper and it allows us to define a better
    feature set than the filter.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we identified the best *n* and the best *k*, so we use them
    to build KNN with a good performance.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning features and parameters together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two sections, we identified the best *k* using all the features
    (`n=37`). Then, using the optimal *k*, we identified the best *n*. What if the
    algorithm performs better with `k=30` and `n=25`? We haven't explored that combination
    as well as many other options, so there might be a combination performing better
    than `k=27` and `n=15`.
  prefs: []
  type: TYPE_NORMAL
- en: In order to identify the best option, the most simple approach is to test all
    the alternatives. However, if there are too many possible combinations between
    the variables, we don't have enough computational power to test all of them. In
    that case, we can identify the optimal parameters using optimization algorithms
    such as the gradient descend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, in our case, we are tuning only two parameters and we can test
    just a part of their possible values. For instance, if we choose 20 values of
    *n* and 20 values of *k*, we have 400 combinations. In order to do that, we carry
    out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the options for *k*. Include all features, the KNN had the best performance
    with `k=26` and it performed badly after `40`. However, setting a lower *n*, things
    may change, so we need to test all the possible *k*. In order to limit the number
    of options, we can limit our testing to the odd numbers. Let''s generate all the
    odd numbers between 1 and 49 using `seq`. The `from` and `to` arguments define
    the start and end of the sequence. The `by` argument defines the increment that
    is 2 to generate the odd numbers. Using `seq`, we build `arrayK` containing all
    the options for *k*, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the options for *n*. We have already seen that the algorithm performs
    very badly using just a small feature set, so we can test *n* between 10 and the
    total number of features, that is, 37\. Similar to *k*, we include only the odd
    numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate all the possible combinations between *k* and *n*. For this purpose,
    we can use `expand.grid`. Given two or more vectors, `expand.grid` generates a
    data frame with all their possible combinations. In our case, we generate a `k`
    column starting from `arrayK` and a `n` column starting from `arrayN`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `dfParameters` into a data table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can take a look at `dtParameters` using `head`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `dtParameters` contains a row for each of the 350 combinations. We need
    to determine the accuracies and store them in a new column called `accuracy`.
    In order to do that, we use a `for` loop running over the rows. The `iConfig`
    variable is the row index defined as a number between 1 and the number of rows
    `nrow(dtParameters)`. There are different combinations, so it might take a while
    to run this part of the code. After each iteration we build the model using the
    parameters contained in the row that are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**k**: This has the `dtParameters[iConfig, k]` parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n**: This has the `dtParameters[iConfig, n]` parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compute the `arrayAccuracy` average and add it to `dtParameters`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Each row of `dtParameters` contains a parameter set and its related accuracy.
    In order to view the accuracies in a more convenient way, we can build a matrix
    whose rows correspond to `n` and whose columns correspond to `k`. Each element
    of the matrix displays the accuracy. In order to build the matrix, we can use
    `reshape`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reshape` syntax is quite complex. In our case, that matrix that we want
    to build is in a `wide` format, so we need to specify `direction = "wide"`. The
    other arguments define the columns that we use and they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`v.names`: This column defines the matrix values (the accuracies)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`idvar`: This column defines the matrix rows (the values of `n`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timevar`: This column defines the matrix columns (the values of `k`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using `reshape`, we can build the `dfAccuracy` data frame, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `n` column contains the *n* parameter and we remove it in order to have
    a data frame with the accuracy only. Then, we convert the data frame into a matrix,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can specify `n` and `k` as the row names and column names respectively,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to visualize the accuracy depending on the parameters, we can build
    a heat map that is a chart representing the matrix. The two chart dimensions are
    `k` and `n` and the color represents the value. We can build this chart using
    `image`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments that we use are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`z`: This is the matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`x` and `y`: These are the dimension names, contained in `arrayN` and `arrayK`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xLab` and `yLab`: These are the axis labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col`: This is the vector of colors that we display (we can use the `heat.colors`
    function)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning features and parameters together](img/7740OS_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A high accuracy is represented by the pale yellow color and a low accuracy is
    represented by the red color. We can notice that we achieved the best accuracy
    with *k* in the 9 to 19 range and *n* in the 29 to 33 range. The worst performance
    is when *n* is low and *k* is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the best performing combination is. Consider the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The best combination is `k=11` and `n=33` and we were not able to identify it
    maximizing the parameters separately. The reason is that the KNN performs well
    with `k=11` only if we don't include all the features.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw a simple way to optimize two parameters. In other contexts,
    we need more advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: A limit of this approach is that we tuned only two parameters. We can achieve
    better performances tuning other KNN parameters such as the distance method.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to evaluate the performance of a model as the
    average accuracy of the prediction. We understood how to determine an accurate
    cross-validation index expressing the accuracy. Starting from the cross-validation
    index, we tuned the parameters. In addition, we learned how to select the features
    using a filter or a frapper and how to tune features and parameters at the same
    time. This chapter described the last part of building a machine learning solution
    and the next chapter shows an overview of some of the most important machine learning
    techniques.
  prefs: []
  type: TYPE_NORMAL
