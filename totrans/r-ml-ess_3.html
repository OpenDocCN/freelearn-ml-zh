<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;A Simple Machine Learning Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. A Simple Machine Learning Analysis</h1></div></div></div><p>This chapter shows examples of exploratory data analysis and machine learning techniques. R provides us with different datasets that can be used to experiment with the tools. In this chapter, we will use an interesting dataset about the Titanic passengers.</p><p>There are some facts that happened during the Titanic event, such as the policy of saving the women and children first and the privileges of the first social classes. In order to investigate what happened, we can use the data related to the event. The R dataset is about some passengers and it displays their personal data and who survived. First, we can explore some data in order to understand what happened. Then, starting from the personal data of other passengers, the goal of the machine learning model is forecasting which new passengers will survive.</p><p>In this chapter, we'll cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exploring the data</li><li class="listitem" style="list-style-type: disc">Visualizing the data using simple charts</li><li class="listitem" style="list-style-type: disc">Exploring the data using machine learning techniques</li><li class="listitem" style="list-style-type: disc">Predicting an outcome using machine learning techniques</li></ul></div><div class="section" title="Exploring data interactively"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Exploring data interactively</h1></div></div></div><p>This section <a id="id117" class="indexterm"/>shows you how to visualize the data using simple techniques. We process the data using the <code class="literal">data.table</code> package and visualize the information using the basic R charts. A great plotting package is <code class="literal">ggplot2</code> and it allows you to create nice professional charts. Unfortunately, its syntax is more complex than the basic R charts, so we don't have enough space for it in this book.</p><p>R provides us with a <code class="literal">Titanic</code> dataset that contains the survival statistics of some passengers. Before starting to analyze the data, let's take a look at their documentation using the following code:</p><div class="informalexample"><pre class="programlisting">help(Titanic)</pre></div><p>The documentation shows that the passengers are divided in groups on the basis of their social class, gender, and age. For each group, the dataset shows how many people survived and how many didn't. We can see the format of data using <code class="literal">class</code>:</p><div class="informalexample"><pre class="programlisting">class(Titanic)
<span class="strong"><strong>[1] "table"</strong></span>
</pre></div><p>The object, <code class="literal">Titanic</code>, belongs to the <code class="literal">table</code> class so it displays the count of each combination of <a id="id118" class="indexterm"/>categoric variables, as shown:</p><div class="informalexample"><pre class="programlisting">Titanic
<span class="strong"><strong>, , Age = Child, Survived = No</strong></span>

<span class="strong"><strong>      Sex</strong></span>
<span class="strong"><strong>Class  Male Female</strong></span>
<span class="strong"><strong>  1st     0      0</strong></span>
<span class="strong"><strong>  2nd     0      0</strong></span>
<span class="strong"><strong>  3rd    35     17</strong></span>
<span class="strong"><strong>  Crew    0      0</strong></span>
<span class="strong"><strong>...</strong></span>
</pre></div><p>The table shows the frequency, that is, the number of passengers for each combination of variables that are the personal data and the data of those who survived.</p><div class="section" title="Defining a table with the data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec26"/>Defining a table with the data</h2></div></div></div><p>In this <a id="id119" class="indexterm"/>subsection, we will convert the data in a more convenient <a id="id120" class="indexterm"/>format. The first step is to define a data frame:</p><div class="informalexample"><pre class="programlisting">dfTitanic &lt;- data.frame(Titanic)</pre></div><p>We can see the structure of <code class="literal">dfTitanic</code> using <code class="literal">str</code>:</p><div class="informalexample"><pre class="programlisting">str(dfTitanic)
<span class="strong"><strong>'data.frame':   32 obs. of  5 variables:</strong></span>
<span class="strong"><strong> $ Class   : Factor w/ 4 levels "1st","2nd","3rd",..: 1 2 3 4 1 2 3 4 1 2 ...</strong></span>
<span class="strong"><strong> $ Sex     : Factor w/ 2 levels "Male","Female": 1 1 1 1 2 2 2 2 1 1 ...</strong></span>
<span class="strong"><strong> $ Age     : Factor w/ 2 levels "Child","Adult": 1 1 1 1 1 1 1 1 2 2 ...</strong></span>
<span class="strong"><strong> $ Survived: Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 1 1 1 1 ...</strong></span>
<span class="strong"><strong> $ Freq    : num  0 0 35 0 0 0 17 0 118 154 ...</strong></span>
</pre></div><p>There are four factors representing the passenger's attributes and <code class="literal">Freq</code> displaying the number of passengers for each combination of attributes. In order to use powerful tools to process data, we transform <code class="literal">dfTitanic</code> into a data table:</p><div class="informalexample"><pre class="programlisting">library(data.table)
dtTitanic &lt;- data.table(dfTitanic)</pre></div><p>We can visualize the top rows of the table using <code class="literal">head</code>:</p><div class="informalexample"><pre class="programlisting">head(dtTitanic)
<span class="strong"><strong>   Class    Sex   Age Survived Freq</strong></span>
<span class="strong"><strong>1:   1st   Male Child       No    0</strong></span>
<span class="strong"><strong>2:   2nd   Male Child       No    0</strong></span>
<span class="strong"><strong>3:   3rd   Male Child       No   35</strong></span>
<span class="strong"><strong>4:  Crew   Male Child       No    0</strong></span>
<span class="strong"><strong>5:   1st Female Child       No    0</strong></span>
<span class="strong"><strong>6:   2nd Female Child       No    0</strong></span>
</pre></div><p>Here, <code class="literal">Class</code>, <code class="literal">Sex</code>, <code class="literal">Age</code>, and <code class="literal">Survived</code> represent the attributes and <code class="literal">Freq</code> shows the number of passengers for each combination. For instance, there are 35 male third class children that survived. The other five feature combinations having no passengers.</p><p>To start the <a id="id121" class="indexterm"/>analysis, we can define <code class="literal">nTot</code> containing the total number of <a id="id122" class="indexterm"/>passengers:</p><div class="informalexample"><pre class="programlisting">nTot &lt;- dtTitanic[, sum(Freq)]
nTot
<span class="strong"><strong>[1] 2201</strong></span>
</pre></div><p>There are <code class="literal">2201</code> passengers. Out of them, how many survived? We can use a simple data table aggregation to count the passengers that survived and the ones that didn't. We need to specify the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Operation</strong></span>: In <a id="id123" class="indexterm"/>order to count the passengers, we sum up the <code class="literal">Freq</code> column, so the operation is <code class="literal">n=sum(Freq)</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Aggregation</strong></span>: We <a id="id124" class="indexterm"/>count the passengers for each possible value of the <code class="literal">Survived</code> column, so we need to specify that we aggregate by <code class="literal">Survived</code></li></ul></div><p>This is the data table syntax. We <a id="id125" class="indexterm"/>use the square brackets and the three arguments are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rows to select</strong></span>: We are using all the tables, so the argument is empty</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Operations</strong></span>: This contains a list containing the operation, that is, <code class="literal">n=sum(Freq)</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Aggregation</strong></span>: We specify that we aggregate <code class="literal">by='Survived'</code></li></ul></div><p>Consider the following code:</p><div class="informalexample"><pre class="programlisting">dtSurvived &lt;- dtTitanic[, list(n=sum(Freq)), by='Survived']
dtSurvived
<span class="strong"><strong>   Survived    n</strong></span>
<span class="strong"><strong>1:       No 1490</strong></span>
<span class="strong"><strong>2:      Yes  711</strong></span>
</pre></div></div><div class="section" title="Visualizing the data through a histogram"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec27"/>Visualizing the data through a histogram</h2></div></div></div><p>We can <a id="id126" class="indexterm"/>visualize <code class="literal">dtSurvived</code> by building a histogram <a id="id127" class="indexterm"/>and the R function is <code class="literal">barplot</code>:</p><div class="informalexample"><pre class="programlisting">help(barplot)</pre></div><p>In our case, the arguments that we need are <code class="literal">height</code> and <code class="literal">names.arg</code>, specifying the height and labels of the bars. Both the arguments require a vector in our case. Let's see how we can build the chart. Follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define the vector with height containing the number of passengers:<div class="informalexample"><pre class="programlisting">vectorHeight &lt;- dtSurvived[, n]</pre></div></li><li class="listitem">Define the vector with names containing the number of passengers that survived:<div class="informalexample"><pre class="programlisting">vectorNames &lt;- dtSurvived[, Survived]</pre></div></li><li class="listitem">Build the chart:<div class="informalexample"><pre class="programlisting">barplot(height=vectorHeight, names.arg=vectorNames)</pre></div></li></ol></div><p>The histogram is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_01.jpg" alt="Visualizing the data through a histogram"/></div><p>The histogram shows the number of passengers that survived or not. The height of each bar is equal to the number of passengers and the labels show what the bars represent. We could have built the same chart using just one line of code:</p><div class="informalexample"><pre class="programlisting">barplot(height=dtSurvived[, n], names.arg=dtSurvived[, Survived])</pre></div><p>This chart <a id="id128" class="indexterm"/>shows the total number of passengers. What <a id="id129" class="indexterm"/>if we want to visualize the percentage instead? Let's take a look at the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define the <code class="literal">percentage</code> column containing the number of passengers divided by the total number of passengers. We can define the new column using the <code class="literal">:=</code> data table operation. This column will be the <code class="literal">height</code> argument:<div class="informalexample"><pre class="programlisting">dtSurvived[, percentage := n / sum(n)]</pre></div></li><li class="listitem">Define the <code class="literal">colorPlot</code> column containing the colors blue and red for visualization. We use <code class="literal">ifelse</code> that is a function specifying that the color is <code class="literal">blue</code> if <code class="literal">Survived == 'Yes'</code>, and <code class="literal">red</code> otherwise. This column will be the <code class="literal">col</code> argument:<div class="informalexample"><pre class="programlisting">dtSurvived[, colorPlot := ifelse(Survived == 'Yes', 'blue', 'red')]</pre></div></li><li class="listitem">Build the chart and as anticipated, we include the <code class="literal">col</code> argument, defining the <code class="literal">color</code> vector. In addition, the percentage ranges between 0 and 1, so we can specify that the area of the plot will be between 0 and 1 adding the <code class="literal">ylim</code> argument equal to <code class="literal">c(0, 1)</code>:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtSurvived[, percentage],
  names.arg=dtSurvived[, Survived],
  col=dtSurvived[, colorPlot],
  ylim=c(0, 1)
)</pre></div></li></ol></div><p>The <a id="id130" class="indexterm"/>histogram is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_02.jpg" alt="Visualizing the data through a histogram"/></div><p>We can <a id="id131" class="indexterm"/>add a title and a legend to the chart; follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define the <code class="literal">textPercentage</code> column containing the percentage as a string. For instance, for a percentage of 0.323035, we display 32 percent in the legend:<div class="informalexample"><pre class="programlisting">dtSurvived[, textPercentage := paste(round(percentage * 100), '%', sep='')]</pre></div></li><li class="listitem">Define the <a id="id132" class="indexterm"/>plot title:<div class="informalexample"><pre class="programlisting">plotTitle &lt;- 'Proportion of passengers surviving or not'</pre></div></li><li class="listitem">Define the <a id="id133" class="indexterm"/><span class="emphasis"><em>y</em></span> axis label:<div class="informalexample"><pre class="programlisting">ylabel &lt;- 'percentage'</pre></div></li><li class="listitem">Build the plot:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtSurvived[, percentage],
  names.arg=dtSurvived[, Survived],
  col=dtSurvived[, colorPlot],
  ylim=c(0, 1),
  legend.text=dtSurvived[, textPercentage],
  ylab=ylabel,
  main=plotTitle
)</pre></div></li></ol></div><p>The histogram is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_03.jpg" alt="Visualizing the data through a histogram"/></div><p>The general <a id="id134" class="indexterm"/>survival rate is <span class="strong"><strong>32%</strong></span>, although it varies across <a id="id135" class="indexterm"/>different attribute combinations. The next subsection shows you how to visualize the impact of an attribute.</p></div><div class="section" title="Visualizing the impact of a feature"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/>Visualizing the impact of a feature</h2></div></div></div><p>In this <a id="id136" class="indexterm"/>subsection, we identify the impact of gender on the survival rate. First, we can define <code class="literal">dtGender</code> displaying the number of passengers that survived or did not survive, for each gender. The operation is <code class="literal">n=sum(Freq)</code> and it is performed for each combination of <code class="literal">Survived</code> and <code class="literal">Sex</code>. Similar to the previous section, we perform a simple data table aggregation, specifying the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Rows to select</strong></span>: We are using the entire table, so the argument is empty</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Operations</strong></span>: This is a list containing the operation, that is, <code class="literal">n=sum(Freq)</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Aggregation</strong></span>: We aggregate by two columns, so we define <code class="literal">by=c('Survived', 'Sex')</code></li></ul></div><p>Consider the following code:</p><div class="informalexample"><pre class="programlisting">dtGender &lt;- dtTitanic[, list(n=sum(Freq)), by=c('Survived', 'Sex')]
dtGender
<span class="strong"><strong>   Survived    Sex    n</strong></span>
<span class="strong"><strong>1:       No   Male 1364</strong></span>
<span class="strong"><strong>2:       No Female  126</strong></span>
<span class="strong"><strong>3:      Yes   Male  367</strong></span>
<span class="strong"><strong>4:      Yes Female  344</strong></span>
</pre></div><p>Now, we can <a id="id137" class="indexterm"/>visualize the new data table through a histogram, as we saw earlier. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Add the <code class="literal">percentage</code> column dividing <code class="literal">n</code> by the number of passengers of the gender. The operation is <code class="literal">n / sum(n)</code> and it is done by gender. Then, we use the <code class="literal">:=</code> operation specifying that we compute the sum <code class="literal">by='Sex'</code>:<div class="informalexample"><pre class="programlisting">dtGender[, percentage := n / sum(n), by='Sex']</pre></div></li><li class="listitem">Define the plot colors:<div class="informalexample"><pre class="programlisting">dtGender[, colorPlot := ifelse(Survived == 'Yes', 'blue', 'red')]</pre></div></li><li class="listitem">Define the <span class="emphasis"><em>y</em></span> axis label:<div class="informalexample"><pre class="programlisting">dtGender[, textPercentage := paste(round(percentage * 100), '%', sep='')]</pre></div></li><li class="listitem">Extract the table with the male survival statistics:<div class="informalexample"><pre class="programlisting">dtGenderMale &lt;- dtGender[Sex == 'Male']</pre></div></li><li class="listitem">Build the histogram for males:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtGenderMale[, percentage],
  names.arg=dtGenderMale[, Survived],
  col=dtGenderMale[, colorPlot],
  ylim=c(0, 1),
  legend.text=dtGenderMale[, textPercentage],
  ylab='percentage',
  main='Survival rate for the males'
)</pre></div></li><li class="listitem">Instead of extracting <code class="literal">dtGenderMale</code>, we could have directly built the chart adding <code class="literal">Sex == 'Male'</code> when extracting the vectors. We can build the same histogram for females in a similar way:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtGender[Sex == 'Female', percentage],
  names.arg=dtGender[Sex == 'Female', Survived],
  col=dtGender[Sex == 'Female', colorPlot],
  ylim=c(0, 1),
  legend.text=dtGender[Sex == 'Female', textPercentage],
  ylab='percentage',
  main='Survival rate for the females'
)</pre></div></li></ol></div><p>Let's display the charts that we built:</p><div class="mediaobject"><img src="graphics/7740OS_03_04.jpg" alt="Visualizing the impact of a feature"/></div><p>The male <a id="id138" class="indexterm"/>survival rate is just <span class="strong"><strong>21%</strong></span> compared with 32% of passengers who survived.</p><div class="mediaobject"><img src="graphics/7740OS_03_05.jpg" alt="Visualizing the impact of a feature"/></div><p>As expected, the female survival rate is significantly higher than the average.</p><p>We can compare the two genders in the same chart displaying just the survival rate that is the <code class="literal">Yes</code> column. We can build the plot using the same commands and including the <code class="literal">Survived == 'Yes'</code> condition. The only difference is the <code class="literal">col</code> argument, that in this case is the <code class="literal">Sex</code> column that is a factor with two levels. In this case, <code class="literal">barplot</code> automatically defines two <a id="id139" class="indexterm"/>colors that are black and red:</p><div class="informalexample"><pre class="programlisting">barplot(
  height=dtGender[Survived == 'Yes', percentage],
  names.arg=dtGender[Survived == 'Yes', Sex],
  col=dtGender[Survived == 'Yes', Sex],
  ylim=c(0, 1),
  legend.text=dtGender[Survived == 'Yes', textPercentage],
  ylab='percentage',
  main='Survival rate by gender'
)</pre></div><p>The histogram is shown as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_06.jpg" alt="Visualizing the impact of a feature"/></div><p>The chart allows <a id="id140" class="indexterm"/>us to visualize the difference and the legend displays the survival rate. As expected, the difference is huge.</p></div><div class="section" title="Visualizing the impact of two features combined"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"/>Visualizing the impact of two features combined</h2></div></div></div><p>In this <a id="id141" class="indexterm"/>chapter, we investigate the impact of another feature: class. How does the survival rate vary across the different classes of passengers? First, we can just build the same survival rate chart as the one for gender, by following these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Define <code class="literal">dtClass</code> containing the passengers that survived or didn't survive for each class:<div class="informalexample"><pre class="programlisting">dtClass &lt;- dtTitanic[, list(n=sum(Freq)), by=c('Survived', 'Class')]</pre></div></li><li class="listitem">Define the percentage of passengers that survived or didn't survive for each class:<div class="informalexample"><pre class="programlisting">dtClass[, percentage := n / sum(n), by='Class']</pre></div></li><li class="listitem">Define the percentage text:<div class="informalexample"><pre class="programlisting">dtClass[, textPercentage := paste(round(percentage * 100), '%', sep='')]</pre></div></li><li class="listitem">Build the histogram:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtClass[Survived == 'Yes', percentage],
  names.arg=dtClass[Survived == 'Yes', Class],
  col=dtClass[Survived == 'Yes', Class],
  ylim=c(0, 1),
  legend.text=dtClass[Survived == 'Yes', textPercentage],
  ylab='survival rate',
  main='Survival rate by class'
)</pre></div></li></ol></div><p>The <a id="id142" class="indexterm"/>histogram is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_07.jpg" alt="Visualizing the impact of two features combined"/></div><p>The survival rate varies a lot across the classes. We can notice that the passengers belonging to higher classes are more likely to survive and that the crew has a survival rate similar to the third class. Can we conclude that class has a high impact on the survival rate?</p><p>The chart shows the overall survival rate for each class. However, knowing that the females are more likely to survive, a class with a higher female:male ratio will likely have a higher survival rate. If a higher survival rate is explained by the gender only, the fact of belonging to a different class doesn't have an impact at all.</p><p>In order to understand whether the difference between the survival rates depends on the percentage of <a id="id143" class="indexterm"/>females in each class, we can visualize the gender ratio by class. The chart is a histogram showing the percentage of females for each social class and the commands are similar as earlier:</p><div class="informalexample"><pre class="programlisting">dtGenderFreq &lt;- dtTitanic[, list(n=sum(Freq)), by=c('Sex', 'Class')]
dtGenderFreq[, percentage := n / sum(n), by='Class']
dtGenderFreq &lt;- dtGenderFreq[Sex == 'Female']
dtGenderFreq[, textPercentage := paste(round(percentage * 100), '%', sep='')]
barplot(
  height=dtGenderFreq[, percentage],
  names.arg=dtGenderFreq[, Class],
  col=dtGenderFreq[, Class],
  ylim=c(0, 1),
  legend.text=dtGenderFreq[, textPercentage],
  ylab='survival rate',
  main='Percentage of females'
)</pre></div><p>The histogram is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_08.jpg" alt="Visualizing the impact of two features combined"/></div><p>The gender ratio varies a lot across the different classes as the percentage of females is higher in the top classes, and that there are almost no females in the crew. Therefore, the percentage of females might have biased the survival rate by class. In order to have a better understanding of the impact of the two attributes on the survival rate, we need to take account of <a id="id144" class="indexterm"/>the gender and of the class at the same time. For this purpose, we can compute the survival rate for each combination of these two features. Use the following steps to build the chart:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Compute the sum of the passengers for each combination of <code class="literal">Survived</code>, <code class="literal">Sex</code>, and <code class="literal">Class</code>. Now the <code class="literal">by</code> argument includes three column names:<div class="informalexample"><pre class="programlisting">dtGenderClass &lt;- dtTitanic[, list(n=sum(Freq)), by=c('Survived', 'Sex', 'Class')]</pre></div></li><li class="listitem">Add the <code class="literal">nTot</code> column specifying the total number of passengers for each feature combination (not including <code class="literal">Survived</code>). The <code class="literal">by</code> argument includes two features:<div class="informalexample"><pre class="programlisting">dtGenderClass[, nTot := sum(n), by=c('Sex', 'Class')]</pre></div></li><li class="listitem">Add the <code class="literal">percentage</code> column. The <code class="literal">by</code> argument includes the two features:<div class="informalexample"><pre class="programlisting">dtGenderClass[, percentage := n / sum(n), by=c('Sex', 'Class')]</pre></div></li><li class="listitem">Extract the column containing the survival rate using the <code class="literal">Survived == 'Yes'</code> condition:<div class="informalexample"><pre class="programlisting">dtGenderClass &lt;- dtGenderClass[Survived == 'Yes']</pre></div></li><li class="listitem">Add the <code class="literal">textPercentage</code> column:<div class="informalexample"><pre class="programlisting">dtGenderClass[, textPercentage := paste(round(percentage * 100), '%', sep='')]</pre></div></li><li class="listitem">Add the <code class="literal">colorPlot</code> column. The <code class="literal">rainbow</code> function builds a vector with a defined number of rainbow colors. In this case, we define a column for each row, so we use <code class="literal">rainbow(nrow(dtGenderClass))</code>:<div class="informalexample"><pre class="programlisting">dtGenderClass[, colorPlot := rainbow(nrow(dtGenderClass))]</pre></div></li><li class="listitem">Define the group name to be included in the labels. Since the histogram will display the survival rate for each combination of both the features, we set the name of each group as the gender and the class combined, using <code class="literal">paste</code>. In order to fit the names into the chart, we define <code class="literal">SexAbbr</code> containing an abbreviation of the gender:<div class="informalexample"><pre class="programlisting">dtGenderClass[, SexAbbr := ifelse(Sex == 'Male', 'M', 'F')]
dtGenderClass[, barName := paste(Class, SexAbbr, sep='')]</pre></div></li><li class="listitem">Define the <a id="id145" class="indexterm"/>labels containing the plot name and the number of passengers in the group. Since we want to display the name and number in two different lines, we separate them with <code class="literal">\n</code> that defines a new line in a string:<div class="informalexample"><pre class="programlisting">dtGenderClass[, barLabel := paste(barName, nTot, sep='\n')]</pre></div></li><li class="listitem">Generate the histogram. Similar to <code class="literal">ylim</code>, the <code class="literal">xlim</code> argument defines the <span class="emphasis"><em>x</em></span> region to visualize. In this case, we use <code class="literal">xlim</code> to avoid overlapping the legend and the chart:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtGenderClass[, percentage],
  names.arg=dtGenderClass[, barLabel],
  col=dtGenderClass[, colorPlot],
  xlim=c(0, 11),
  ylim=c(0, 1),
  ylab='survival rate',
  legend.text=dtGenderClass[, textPercentage]
)</pre></div></li></ol></div><p>The histogram generated is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_09.jpg" alt="Visualizing the impact of two features combined"/></div><p>We can find the number of passengers of a group under its column. Apart from the female crew, each bar contains at least 100 passengers, so we can assume that the result is meaningful. In order to measure the meaningfulness, we could have used a statistic technique such as <a id="id146" class="indexterm"/>confidence intervals or a hypothesis test, but it's not the topic of this book.</p><p>The class is affecting the males and the females in different ways. In the case of males, the survival rate is very low although it is significantly higher for the first class. In the case of females, the survival rate is close to 100 percent for each class apart from the third class.</p><p>We can also look at the chart in the opposite way in order to understand the impact of the gender over the passengers belonging to the same class. In all the situations, the survival rate is significantly higher, although the difference is much higher for some specific classes. The impact of the gender and the class are related, so we need to take account of both the features at the same time if we want to understand their effect.</p><p>We haven't explored the age yet. We can visualize the survival rate for each combination of all the features. The code to prepare and plot the table is similar as before. In this case, we can just apply the operations to <code class="literal">dtTitanic</code> directly. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Compute the percentage of people that survived or didn't survive for each combination of the three features:<div class="informalexample"><pre class="programlisting">dtTitanic[, nTot := sum(Freq), by=c('Sex', 'Class', 'Age')]</pre></div></li><li class="listitem">Add the percentage of passengers surviving for each attribute combination:<div class="informalexample"><pre class="programlisting">dtTitanic[, percentage := Freq / nTot]</pre></div></li><li class="listitem">Extract the survival rate using the <code class="literal">Survived == 'Yes'</code> condition:<div class="informalexample"><pre class="programlisting">dtAll &lt;- dtTitanic[Survived == 'Yes', ]</pre></div></li><li class="listitem">Add the legend text including the abbreviations of all the three features. For the class, we use substring that is a function extracting a part of the string. In our case, we <a id="id147" class="indexterm"/>extract the first character, so we specify that we extract the elements between <code class="literal">1</code> and <code class="literal">1</code> using <code class="literal">substring(Class, 1, 1)</code>:<div class="informalexample"><pre class="programlisting">dtAll[, ClassAbbr := substring(Class, 1, 1)]
dtAll[, SexAbbr := ifelse(Sex == 'Male', 'M', 'F')]
dtAll[, AgeAbbr := ifelse(Age == 'Child', 'C', 'A')]
dtAll[, textLegend := paste(ClassAbbr, SexAbbr, AgeAbbr, sep='')];</pre></div></li><li class="listitem">Add the plot color:<div class="informalexample"><pre class="programlisting">dtAll[, colorPlot := rainbow(nrow(dtAll))]</pre></div></li><li class="listitem">Add the percentage to display in the label:<div class="informalexample"><pre class="programlisting">dtAll[, labelPerc := paste(round(percentage * 100), '%', sep='')]</pre></div></li><li class="listitem">Add the label including the percentage and the total number:<div class="informalexample"><pre class="programlisting">dtAll[, label := paste(labelPerc, nTot, sep='\n')]</pre></div></li><li class="listitem">Generate the plot. We have more groups than before, so the layout is different in order to visualize all the relevant information. The <code class="literal">xlim</code> argument leaves some space for the legend and the <code class="literal">cex.names</code> argument decreases the label text size:<div class="informalexample"><pre class="programlisting">barplot(
  height=dtAll[, percentage],
  names.arg=dtAll[, label],
  col=dtAll[, colorPlot],
  xlim=c(0, 23),
  legend.text=dtAll[, textLegend],
  cex.names=0.5
)</pre></div></li></ol></div><p>The histogram is as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_10.jpg" alt="Visualizing the impact of two features combined"/></div><p>The legend displays the abbreviated feature combinations. For instance, <span class="strong"><strong>1MC</strong></span> means first class, male, child. In the case of combinations with no passengers, we don't have any information about the percentage, so the bar label displays <span class="strong"><strong>NaN%</strong></span>.</p><p>Since we are combining three features, some groups are very small. For instance, we have only five first <a id="id148" class="indexterm"/>class male children. There are also other groups with no passengers at all (for example, children in the crew). Therefore, this approach has some limitations.</p></div></div></div>
<div class="section" title="Exploring the data using machine learning models"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Exploring the data using machine learning models</h1></div></div></div><p>Visualizing the <a id="id149" class="indexterm"/>survival rate for each group of passengers provides us with an overview of the data. We have an idea about how the <a id="id150" class="indexterm"/>different features interact with the survival rate and with each other. For instance, we know that the social class has a different impact on the survival rate depending on the gender. But which of the two features has the highest impact? How big is the impact of each feature? We haven't defined a ranking of the features or quantified the impact of each of them. Some machine learning techniques allow us to investigate further, answering to our questions.</p><div class="section" title="Exploring the data using a decision tree"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec30"/>Exploring the data using a decision tree</h2></div></div></div><p>We have <a id="id151" class="indexterm"/>three features (class, sex, and age) and we want to divide the passengers in groups accordingly. We can't define a group for each <a id="id152" class="indexterm"/>combination of features since we won't have enough data for some groups such as the female children of the first class. A solution is to divide the passengers in groups in such a way that each group contains enough data. A group is defined by some conditions on the features, such as males not belonging to the first class. The groups should cover every possible situation without overlapping. A machine learning technique that identifies groups that are big enough is the decision tree learning.</p><p>There is a new passenger and we know it's a male child of the second class. We don't know if the passenger will survive or not and we want to predict that. How can we use the data? We can check whether the passenger is a male or a female. Accordingly, with our previous data exploration, he'll survive with a probability of 21 percent since he's a male. Taking account of the social class, we can say that he will survive with a probability of 14 percent. There are 179 second class male passengers, so this result is meaningful. Then, knowing that he is a child, we can check the survival rate of the second class male children, which is 100 percent. Does it make sense to say that he will survive with a probability of 100 percent? There are only 11 passengers that are second class male children, so we don't have enough data to make an accurate prediction. Should we use the survival rate of the second class males? What if we use the survival rate of all the male children instead? What about the second class children? There are different options leading to different results.</p><p>A solution is to identify the key features and take account of them only. For instance, if the gender and class are the two most important features, we can use them to make a prediction. However, in the case of the third class male children, we have much more data than the first class male children. What if we take account of the age only in the case of third class males? The number of features that we want to include depends on the group we're taking account of.</p><p>Instead of just selecting the two most important features, we can define a criterion of splitting a group only if it is big enough and we can visualize this principle through a decision tree. Let's suppose that in the beginning all the passengers belong to the same group. We can split them in two groups based on gender. Then, we can split the males in two groups: first class on one side and all the other classes on the other side. For the females, the most meaningful split might be another: children on one side, the adults on the other.</p><p>The decision tree learning technique learns from the data in order to identify the most meaningful splits and it can be used to explore the data. The tree continues splitting the data until the groups, defined by the tree leaves, are too small. Then, for each group, we use the related data to define an attribute that can be:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Categoric</strong></span>: This is an <a id="id153" class="indexterm"/>attribute whose value belongs to categories. In this case, the categories are <span class="strong"><strong>Survived</strong></span> and <span class="strong"><strong>Not survived</strong></span>. The tree performs classification.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Numeric</strong></span>: This is <a id="id154" class="indexterm"/>an attribute that can be measured, and in this case it is the survival rate. The tree performs regression.</li></ul></div><p>We can build a decision tree in R using the <code class="literal">rpart</code> package. In addition, we can visualize the trees using another package that is <code class="literal">rpart.plot</code>. In order to use the packages, we need to install and load them. In and case of installation issues, you can specify the repository as an argument of <code class="literal">install.packages</code>:</p><div class="informalexample"><pre class="programlisting">install.packages('rpart')
install.packages('rpart.plot')</pre></div><p>After the installation, we can load both the packages:</p><div class="informalexample"><pre class="programlisting">library('rpart')
library('rpart.plot')</pre></div><p>The starting point is <code class="literal">dtTitanic</code> and it contains a row for each combination of the features. Before building the decision tree, we need to transform the data into another format. We need to <a id="id155" class="indexterm"/>have a row for each passenger and <a id="id156" class="indexterm"/>the same columns apart from <code class="literal">Freq</code>. In order to generate the new table in the new format, we use the data table operation with <code class="literal">list</code> and <code class="literal">by</code>.</p><p>For each row of <code class="literal">dtTitanic</code>, we want to generate a table having a number of rows equal to <code class="literal">Freq</code>. Each row corresponds to a combination between <code class="literal">Survived</code>, <code class="literal">Sex</code>, <code class="literal">Age</code>, and <code class="literal">Class</code>, so the <code class="literal">by</code> argument contains a vector with the four features.</p><p>In the new table, each row contains a passenger, so <code class="literal">Freq</code> is equal to <code class="literal">1</code>. Then, for each row of <code class="literal">dtTitanic</code>, we need to define a vector having <code class="literal">Freq</code> elements equal to <code class="literal">1</code>. In order to do that, we use <code class="literal">rep</code> that is a function replicating an element a defined number of times. In our case, we use <code class="literal">rep(1, Freq))</code>. The other columns replicate the elements defined in <code class="literal">by</code> that are <code class="literal">Survived</code>, <code class="literal">Sex</code>, <code class="literal">Age</code>, and <code class="literal">Class</code>, so we don't need to redefine them:</p><div class="informalexample"><pre class="programlisting">dtLong &lt;- dtTitanic[
  , list(Freq = rep(1, Freq)),
  by=c('Survived', 'Sex', 'Age', 'Class')
  ]</pre></div><p>
<code class="literal">Freq</code> is <code class="literal">1</code> for each row, so we don't need it anymore and can delete it:</p><div class="informalexample"><pre class="programlisting">dtLong[, Freq := NULL]</pre></div><p>In order to build a decision tree showing the survival rate, we need to change the format of <code class="literal">Survived</code>. Instead of having <code class="literal">No</code> and <code class="literal">Yes</code>, we want <code class="literal">0</code> and <code class="literal">1</code> respectively. To modify the column, we can use <code class="literal">ifelse</code>:</p><div class="informalexample"><pre class="programlisting">dtLong[, Survived := ifelse(Survived == 'Yes', 1, 0)]</pre></div><p>Let's see the first six rows of <code class="literal">DtLong</code> using <code class="literal">head</code>:</p><div class="informalexample"><pre class="programlisting">head(dtLong)
<span class="strong"><strong>   Survived  Sex   Age Class</strong></span>
<span class="strong"><strong>1:        0 Male Child   3rd</strong></span>
<span class="strong"><strong>2:        0 Male Child   3rd</strong></span>
<span class="strong"><strong>3:        0 Male Child   3rd</strong></span>
<span class="strong"><strong>4:        0 Male Child   3rd</strong></span>
<span class="strong"><strong>5:        0 Male Child   3rd</strong></span>
<span class="strong"><strong>6:        0 Male Child   3rd</strong></span>
</pre></div><p>The first six rows show six male children that didn't survive.</p><p>The <code class="literal">dtLong</code> object <a id="id157" class="indexterm"/>contains the standard input of the decision tree algorithm and we can use <code class="literal">rpart</code> to build the model. Our goal is to define groups of passengers about whom we are able to estimate the survival rate:</p><div class="informalexample"><pre class="programlisting">help(rpart)</pre></div><p>The mandatory <a id="id158" class="indexterm"/>arguments are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">formula</code>: This is a formula object defining the attribute to predict and the feature used to predict. The formula is defined by a string such as <code class="literal">outcome ~ feature1 + feature2 + feature3</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">data</code>: This is the data frame or data table that is <code class="literal">dtLong</code> in our case.</li></ul></div><p>We need to define <a id="id159" class="indexterm"/>the formula starting from the <code class="literal">Survived ~ Sex + Age + Class</code> string:</p><div class="informalexample"><pre class="programlisting">formulaRpart &lt;- formula('Survived ~ Sex + Age + Class')</pre></div><p>Now we can build <code class="literal">treeRegr</code> containing a decision tree. Since <code class="literal">Survived</code> is numeric, the function automatically builds a <code class="literal">regrssion</code> tree:</p><div class="informalexample"><pre class="programlisting">treeRegr &lt;- rpart(
  formula=formulaRpart,
  data=dtLong
)</pre></div><p>The <code class="literal">treeRegr</code> object contains the decision tree and we can visualize it using <code class="literal">prp(treeRegr)</code>:</p><div class="mediaobject"><img src="graphics/7740OS_03_11.jpg" alt="Exploring the data using a decision tree"/></div><p>Let's take a look at the tree. Each interior node is labeled with a condition that splits the data in two parts. The node on the top, for instance, splits the passengers into males and females. The branch on the left corresponds to the passengers fulfilling the condition (in this case, the male passengers), and the branch on the right corresponds to the others (the females). Each leaf defines the survival rate of the group. For instance, the leaf to the right states that the survival rate for the females not belonging to the third class is 93 percent.</p><p>The tree doesn't <a id="id160" class="indexterm"/>contain all the possible feature combinations because of the lack of data. For instance, in the case of the females, there are only 45 children and they belong to different social classes, so the tree doesn't divide the females based on their age.</p><p>Let's suppose that we have a new passenger that is female, child, second class. How do we predict if she will <a id="id161" class="indexterm"/>survive? She is a female not belonging to the third class, so her expected survival rate is 93 percent. Therefore, we can say that she will likely survive.</p><p>The tree defines a survival rate that is a number. What if we wanted to predict whether the passenger survived or not? We can build a classification tree adding <code class="literal">method='class'</code> input to <code class="literal">rpart</code>:</p><div class="informalexample"><pre class="programlisting">treeClass = rpart(
    formula='Survived ~ Sex + Age + Class',
    data=dtLong,
    method='class'
)
prp(treeClass)</pre></div><p>The tree is <a id="id162" class="indexterm"/>shown as follows:</p><div class="mediaobject"><img src="graphics/7740OS_03_12.jpg" alt="Exploring the data using a decision tree"/></div><p>This tree predicts that the only passengers that will survive are females and children not belonging to the third class. This result is useful to explore the data. The next step is using machine learning models to predict an outcome. We can use this tree for that purpose, although it <a id="id163" class="indexterm"/>just defines five groups of passengers out of the 16 possible feature combinations, so it might not be the most appropriate <a id="id164" class="indexterm"/>technique. There are more advanced algorithms and in the next chapter we see one of them.</p></div></div>
<div class="section" title="Predicting newer outcomes"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Predicting newer outcomes</h1></div></div></div><p>Given a new <a id="id165" class="indexterm"/>passenger and knowing his or her personal information, we want to predict whether he or she will survive. The options that we explored until now are based on dividing the passengers into groups and identifying the survival rate for each group. For some combinations of features, such as first class female children, we don't have enough data, so we have to use the survival rate of a larger group such as females not belonging to the third class. We are ignoring some details, for instance, the fact that they are children, and in this way we are losing information. Is there a way to estimate the survival rate for any combination of features, regardless of how many passengers we have?</p><p>There are many <a id="id166" class="indexterm"/>machine learning algorithms that take account of all the features at the same time. In this chapter, we see a very popular algorithm that is the <span class="strong"><strong>random forest</strong></span> algorithm. It's not the best option in this context, as it performs better when there are much more features, but it's good for the purpose of illustrating a general approach.</p><div class="section" title="Building a machine learning model"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec31"/>Building a machine learning model</h2></div></div></div><p>As its <a id="id167" class="indexterm"/>name suggests, the random forest algorithm is based on many random decision trees. The algorithm builds <code class="literal">ntree</code> trees repeating the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Generate the data to build the tree choosing a random row from the data (in our case, that is <code class="literal">dtLong</code>) sampsize times. Each row can be chosen more than once and in the end we have a table with sampsize random rows.</li><li class="listitem">Randomly select a <code class="literal">mtry</code> number of features (unfortunately in our case we don't have many features, but it's still possible to select a subset of them).</li><li class="listitem">Build a decision tree based on the sampled data taking account of the selected features only.</li></ol></div><p>A random forest model is composed by <code class="literal">ntree</code> decision trees. In our context, given a new passenger, the model forecasts their survival rate using each tree. The final forecasted value is the average between the survival rates. In another variation of the algorithm, we have the mode instead of the average.</p><p>The random forest is a popular algorithm and it is provided by the <code class="literal">randomForest</code> package. Let's install and load it:</p><div class="informalexample"><pre class="programlisting">install.packages('randomForest')
library('randomForest')</pre></div><p>Like the simple decision tree learning, the random forest attributes can be categoric or numeric.</p><p>In our case, all the features are categoric and there are two to four possible values for each feature. We can convert the features into a numeric format. For instance, in the case of <code class="literal">Sex</code>, the possible values are <code class="literal">Male</code> and <code class="literal">Female</code>. We can define a numeric feature that is <code class="literal">1</code> in the case of <code class="literal">Male</code> and <code class="literal">0</code> otherwise. The new feature shows the same information in a different way. Numeric features derived from categoric in this way are called dummy variables. In the case of a categoric feature with more than two categories, we can define a dummy variable for each categories apart from one. In this way, looking at the dummy variables, if one of them is equal to <code class="literal">1</code>, we know which the groups are. If they are all equal to <code class="literal">0</code>, we know that a group is remaining.</p><p>We can define a <a id="id168" class="indexterm"/>new table containing dummy variables through the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Build a copy of the categoric features table:<div class="informalexample"><pre class="programlisting">dtDummy &lt;- copy(dtLong)</pre></div></li><li class="listitem">Convert <code class="literal">Sex</code> into a dummy variable:<div class="informalexample"><pre class="programlisting">dtDummy[, Male := Sex == 'Male']
dtDummy[, Sex := NULL]</pre></div></li><li class="listitem">Convert <code class="literal">Age</code> into a dummy variable:<div class="informalexample"><pre class="programlisting">dtDummy[, Child := Age == 'Child']
dtDummy[, Age := NULL]</pre></div></li><li class="listitem">Convert <code class="literal">Class</code> into three dummy variables:<div class="informalexample"><pre class="programlisting">dtDummy[, Class1 := Class == '1st']
dtDummy[, Class2 := Class == '2nd']
dtDummy[, Class3 := Class == '3rd']
dtDummy[, Class := NULL]</pre></div></li><li class="listitem">Define the <code class="literal">formulaRf</code> formula:<div class="informalexample"><pre class="programlisting">formulaRf &lt;- formula('Survived ~ Male + Child + Class1 + Class2 + Class3')</pre></div></li><li class="listitem">Build <code class="literal">forest</code> containing the random forest model. All the parameters are left as their default values:<div class="informalexample"><pre class="programlisting">forest &lt;- randomForest(
  formula=formulaRf,
  data=dtDummy
)</pre></div></li></ol></div><p>We stored the random forest model in <code class="literal">forest</code> that is a list containing the machine learning model, and all the related parameters and information. We can explore the model observing the elements of the list. For instance, the number of trees that the model has built is contained in the <code class="literal">ntree</code> element:</p><div class="informalexample"><pre class="programlisting">forest$ntree
<span class="strong"><strong>[1] 500</strong></span>
</pre></div><p>Another parameter is <code class="literal">mtry</code> and it defines the number of variables used in each iteration:</p><div class="informalexample"><pre class="programlisting">forest$mtry
<span class="strong"><strong>[1] 1</strong></span>
</pre></div><p>The number of trees has been defaulted to 500.</p><p>The algorithm is selecting just one feature at once. The reason is that the random forest is meant to work with a lot of features, so it doesn't perform well in this context.</p><p>Another parameter is <code class="literal">type</code> and it defines the output of the algorithm. The random forest can be used for different purposes and in our case we want to estimate the survival rate, so we want to use it for regression:</p><div class="informalexample"><pre class="programlisting">forest$type
<span class="strong"><strong>[1] "regression"</strong></span>
</pre></div><p>As expected, <code class="literal">forest</code> is performing regression.</p><p>If we want to <a id="id169" class="indexterm"/>change some parameters, we can define them in the arguments. In this chapter, we are not defining a criterion to set the parameters, so we just assign another value. For instance, we can build <code class="literal">1000</code> trees using three random features and <code class="literal">1500</code> random rows for each. We can rebuild <code class="literal">forest</code> changing the parameters:</p><div class="informalexample"><pre class="programlisting">forest &lt;- randomForest(
  formula=formulaRf,
  data=dtDummy,
  ntree=1000,
  mtry=3,
  sampsize=1500
)</pre></div><p>We built a random <code class="literal">forest</code> model and the next subsection shows how to use it.</p></div><div class="section" title="Using the model to predict new outcomes"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec32"/>Using the model to predict new outcomes</h2></div></div></div><p>Now that <a id="id170" class="indexterm"/>we have built the model, we can use it to <a id="id171" class="indexterm"/>perform some predictions. If we have a new passenger, what is their survival rate? First, let's extract a random passenger:</p><div class="informalexample"><pre class="programlisting">rowRandom &lt;- dtDummy[100]
rowRandom
<span class="strong"><strong>   Survived Freq Male Child Class1 Class2 Class3</strong></span>
<span class="strong"><strong>1:       No    1 TRUE FALSE   TRUE  FALSE  FALSE</strong></span>
</pre></div><p>The random passenger is an adult male of the first class. We can use the <code class="literal">forest</code> model to estimate his survival rate. The <code class="literal">predict</code> function allows us to apply the model on the new data, obtaining a prediction:</p><div class="informalexample"><pre class="programlisting">predict(forest, rowRandom)
<span class="strong"><strong>        1 </strong></span>
<span class="strong"><strong>0.3830159</strong></span>
</pre></div><p>The estimated survival rate is about 38 percent, so the passenger won't likely survive. We can use the same approach to predict the survival rate of all the passengers. However, that means to apply the model on the same data that we used to build it. This approach is not good for testing the model because the predicted values will be related to the initial data. Keeping in mind that this result cannot be used, we can use it just to compare the prediction with the real data:</p><div class="informalexample"><pre class="programlisting">prediction = predict(forest, dtDummy)</pre></div><p>We can see the prediction of six random rows using <code class="literal">sample</code>:</p><div class="informalexample"><pre class="programlisting">sample(prediction, 6)
<span class="strong"><strong>     1895       448       967      1553      1683         4 </strong></span>
<span class="strong"><strong>0.6934046 0.2260507 0.2499303 0.3830159 0.2260507 0.2974706</strong></span>
</pre></div><p>We defined a survival rate for each passenger. Let's add the estimated survival rate to the <code class="literal">dtDummy</code> table:</p><div class="informalexample"><pre class="programlisting">dtDummy[, SurvivalRatePred := predict(forest, dtDummy)]</pre></div><p>Now, we can predict that a passenger will survive if their survival rate is above a threshold, for <a id="id172" class="indexterm"/>instance, 50 percent. We can define a <a id="id173" class="indexterm"/>new column name, <code class="literal">SurvivedPred</code>, containing our prediction:</p><div class="informalexample"><pre class="programlisting">dtDummy[, SurvivedPred := ifelse(SurvivalRatePred &gt; 0.5, 1, 0)]</pre></div><p>Now we can compare the predicted survival with the initial data. In order to evaluate how many times the two values match, we can define an <code class="literal">error</code> column that is <code class="literal">TRUE</code> if the values don't match:</p><div class="informalexample"><pre class="programlisting">dtDummy[, error := SurvivedPred != Survived]</pre></div><p>Starting from the error column, we can compute the general error as the percentage of passengers upon which we made a wrong prediction. We need to divide the number of errors by the number of passengers. We can have the number of errors applying sum to error, since the sum of a vector of Boolean variables is equal to the number of <code class="literal">TRUE</code> values. The total number of passengers is defined by <code class="literal">.N</code>, which in the <code class="literal">data.table</code> notation is equal to the number of rows:</p><div class="informalexample"><pre class="programlisting">percError &lt;- dtDummy[, sum(error) / .N]
percError
<span class="strong"><strong>[1] 0.2094502</strong></span>
</pre></div><p>The model predicted the wrong outcome in 21 percent of the situations, so we have an accuracy of 79 percent. Anyway, this result doesn't make any sense since we're making a prediction on the same data that we used to build the model. In addition, knowing how many passengers survived, we could have just guessed the most common outcome for each of them. If more than half of them survived, we can set <code class="literal">SurvivedPred = TRUE</code> for all of them and guess more than half. Let's compute the overall probability of surviving. The general survival rate is lower than 50 percent, so each passenger is more likely not to survive. Then, in absence of any other information, we can predict that no one survives:</p><div class="informalexample"><pre class="programlisting">dtTitanic[Survived == 'No', sum(Freq)] / dtTitanic[, sum(Freq)]
<span class="strong"><strong>[1] 0.676965</strong></span>
</pre></div><p>We could have <a id="id174" class="indexterm"/>achieved accuracy of more than 65 percent <a id="id175" class="indexterm"/>without taking account of any feature, so 79 percent is just 15 percent higher. In addition, as already said, this accuracy cannot be used because we are applying the model on the same data used to build it.</p></div><div class="section" title="Validating a model"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Validating a model</h2></div></div></div><p>In order to <a id="id176" class="indexterm"/>evaluate the real accuracy of a model, we can build it using a part of the data, such as 80 percent of the passengers. Then, we can apply the model on the remaining 20 percent of data. The data that we use to build the model is called the <a id="id177" class="indexterm"/>
<span class="strong"><strong>training set</strong></span> <a id="id178" class="indexterm"/>and the other is called the <span class="strong"><strong>test set</strong></span>.</p><p>We can assign each row to the training set with a probability of 80 percent. In this way, the training set will include about 80 percent of the data. In order to define which rows should be included <a id="id179" class="indexterm"/>in the training set, we can define a logical vector, called <code class="literal">indexTrain</code>, which is <code class="literal">TRUE</code> for each row belonging to the training set. We can generate the vector using sample and the arguments are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">x</code>: This represents the possible values; in this case, <code class="literal">TRUE</code> and <code class="literal">FALSE</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">size</code>: This represents the vector length; in this case, it is equal to the number of rows in <code class="literal">dtDummy</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">replace</code>: If the value is <code class="literal">TRUE</code>, each value (<code class="literal">TRUE</code> or <code class="literal">FALSE</code>) can be sampled more than once</li><li class="listitem" style="list-style-type: disc"><code class="literal">prob</code>: This is a vector with the probabilities of sampling the values of <code class="literal">x</code>; in this case, it is <code class="literal">c(0.8, 0.2)</code></li></ul></div><p>Consider the following code:</p><div class="informalexample"><pre class="programlisting">indexTrain &lt;- sample(
  x=c(TRUE, FALSE),
  size=nrow(dtDummy),
  replace=TRUE,
  prob=c(0.8, 0.2)
)</pre></div><p>Now, we can extract the rows in which <code class="literal">indexTrain</code> is equal to <code class="literal">TRUE</code>:</p><div class="informalexample"><pre class="programlisting">dtTrain &lt;- dtDummy[indexTrain]</pre></div><p>In the same way, we extract the rows of the test set. The <code class="literal">!</code> operator means <code class="literal">NOT</code> and it allows the rows for which <code class="literal">indexTrain</code> is equal to <code class="literal">FALSE</code> to be extracted:</p><div class="informalexample"><pre class="programlisting">dtTest &lt;- dtDummy[!indexTrain]</pre></div><p>Now, we can build the model using the same parameters as before. Knowing that we have less data, we can just reduce the sampsize parameters that define the data to use for each tree:</p><div class="informalexample"><pre class="programlisting">forest &lt;- randomForest(
  formula=formulaRf,
  data=dtTrain,
  ntree=1000,
  mtry=3,
  sampsize=1200
)</pre></div><p>We built a model <a id="id180" class="indexterm"/>without taking account of <code class="literal">dtTest</code>, so we can use it to predict on <code class="literal">dtTest</code>. Like before, we predict that a passenger will survive if their survival rate is above 50 percent. After the prediction, we can estimate the error, using the same R commands as before:</p><div class="informalexample"><pre class="programlisting">dtTest[, SurvivalRatePred := predict(forest, dtTest)]
dtTest[, SurvivedPred := ifelse(SurvivalRatePred &gt; 0.5, 1, 0)]
dtTest[, error := SurvivedPred != Survived]
percError &lt;- dtTest[, sum(error) / .N]
percError
<span class="strong"><strong>[1] 0.2416107</strong></span>
</pre></div><p>The estimated error, <code class="literal">percError</code>, depends on how we have split the data, so it's different every time we define a new random training/test split. However, we can repeat the steps many times and compute the average error. This approach is called cross validation and it's a very useful tool to estimate the accuracy.</p><p>This chapter showed a generic approach to build and validate a machine learning model. Using this approach, we can forecast an attribute and estimate the prediction accuracy.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Summary</h1></div></div></div><p>In this chapter, we learned how to process data using data table operations and built some simple R plots for exploratory data analysis. We learned how to use decision trees to find useful insights and build machine learning models (random forest) to perform predictions. We saw how to change the parameters of a model and how to validate it.</p><p>The next three chapters show the steps introduced in this chapter in detail. <a class="link" href="ch04.html" title="Chapter 4. Step 1 – Data Exploration and Feature Engineering">Chapter 4</a>, <span class="emphasis"><em>Step 1 - Data Exploration and Feature Engineering</em></span>, shows the first step of machine learning that consists of data exploration and feature engineering, in depth.</p></div></body></html>