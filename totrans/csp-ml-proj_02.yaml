- en: Spam Email Filtering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤
- en: In this chapter, we are going to start building real-world **machine learning**
    (**ML**) models in C# using the two packages we installed in [Chapter 1](part0020.html#J2B80-5ebdf09927b7492888e31e8436526470), *Basics
    of Machine Learning Modeling*; Accord.NET for ML and Deedle for data manipulation.
    In this chapter, we are going to build a classification model for spam email filtering.
    We will be working with a raw email dataset that contains both spam and ham (non-spam)
    emails and use it to train our ML model. We are going to start following the steps
    for developing ML models that we discussed in the previous chapter. This will
    help us understand the workflow and approaches in ML modeling better and make
    them second nature. While we work to build a spam email classification model,
    we will also discuss feature engineering techniques for text datasets and basic
    model validation methods for classification models, and compare the logistic regression
    classifier and Naive Bayes classifier for spam email filtering. Familiarizing
    ourselves with these model-building steps, basic text feature engineering techniques,
    and basic classification model validation methods will lay the groundwork for
    more advanced feature engineering techniques using **natural language processing**
    (**NLP**) and for building multi-class classification models in [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470),
    *Twitter Sentiment Analysis*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始使用我们在[第1章](part0020.html#J2B80-5ebdf09927b7492888e31e8436526470)“机器学习建模基础”中安装的两个包，即Accord.NET
    for ML和Deedle for data manipulation，在C#中构建真实的**机器学习**（**ML**）模型。在本章中，我们将构建一个用于垃圾邮件过滤的分类模型。我们将使用包含垃圾邮件和正常邮件（非垃圾邮件）的原始电子邮件数据集来训练我们的ML模型。我们将开始遵循上一章中讨论的ML模型开发步骤。这将帮助我们更好地理解ML建模的工作流程和方法，并使它们变得自然而然。在我们努力构建垃圾邮件分类模型的同时，我们还将讨论文本数据集的特征工程技术和分类模型的初步验证方法，并比较逻辑回归分类器和朴素贝叶斯分类器在垃圾邮件过滤中的应用。熟悉这些模型构建步骤、基本的文本特征工程技术和基本的分类模型验证方法将为使用**自然语言处理**（**NLP**）进行更高级的特征工程以及在第3章“Twitter情感分析”中构建多类分类模型奠定基础。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Problem definition for the spam email filtering project
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤项目的定义问题
- en: Data preparation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Email data analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件数据分析
- en: Feature engineering for email data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件数据特征工程
- en: Logistic regression versus Naive Bayes for spam email filtering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归与朴素贝叶斯在垃圾邮件过滤中的应用
- en: Classification model validations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型验证
- en: Problem definition for the spam email filtering project
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾邮件过滤项目的定义问题
- en: 'Let''s start by defining the problem that we are going to solve in this chapter.
    You are probably familiar with what spam emails are already; spam email filtering
    is an essential feature for email services such as Gmail, Yahoo Mail, and Outlook.
    Spam emails can be annoying for users, but they bring more issues and risks with
    them. For example, a spam email can be designed to solicit credit card numbers
    or bank account information, which can be used for credit card fraud or money
    laundering. A spam email can also be used to obtain personal data, such as a social
    security number or user IDs and passwords, which then can be used for identity
    theft and various other crimes. Having spam email filtering technology in place
    is an essential step for email services to save users from being exposed to such
    crimes. However, having the right spam email filtering solution is difficult.
    You want to filter out suspicious emails, but at the same time, you do not want
    to filter too much so that non-spam emails go into the spam folder and never get
    looked at by users. To solve this problem, we are going to have our ML models
    learn from the raw email dataset and classify suspicious emails as spam using
    the subject line. We are going to look at two performance metrics to measure our
    success: precision and recall. We will discuss these metrics in detail in the
    following sections.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义一下本章将要解决的问题。你可能已经熟悉了垃圾邮件是什么；垃圾邮件过滤是像Gmail、Yahoo Mail和Outlook这样的电子邮件服务的基本功能。垃圾邮件可能会让用户感到烦恼，但它们带来了更多的问题和风险。例如，垃圾邮件可能被设计成索要信用卡号码或银行账户信息，这些信息可能被用于信用卡欺诈或洗钱。垃圾邮件也可能被用来获取个人信息，如社会保障号码或用户ID和密码，然后可以用来进行身份盗窃和其他各种犯罪。拥有垃圾邮件过滤技术是电子邮件服务保护用户免受此类犯罪侵害的关键步骤。然而，拥有正确的垃圾邮件过滤解决方案是困难的。你希望过滤掉可疑邮件，但同时，你又不希望过滤太多，以至于非垃圾邮件被放入垃圾邮件文件夹，用户永远不会查看。为了解决这个问题，我们将让我们的机器学习模型从原始电子邮件数据集中学习，并使用主题行将可疑邮件分类为垃圾邮件。我们将查看两个性能指标来衡量我们的成功：精确率和召回率。我们将在以下章节中详细讨论这些指标。
- en: 'To summarize our problem definition:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我们的问题定义：
- en: What is the problem? We need a spam email filtering solution to prevent our
    users from being victims of fraudulent activities and to improve user experience
    at the same time.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题是啥？我们需要一个垃圾邮件过滤解决方案，以防止我们的用户成为欺诈活动的受害者，同时提高用户体验。
- en: Why is it a problem? Having the right balance between filtering suspicious emails
    and not filtering too much, so that non-spam emails still get the Inbox, is difficult.
    We are going to rely on ML models to learn how to classify such suspicious emails
    statistically.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么这是一个问题？在过滤可疑邮件和不过度过滤之间取得平衡，使得非垃圾邮件仍然进入收件箱，是困难的。我们将依赖机器学习模型来学习如何从统计上分类这类可疑邮件。
- en: What are some of the approaches to solving this problem? We will build a classification
    model that flags potential spam emails based on the subject lines of emails. We
    will use precision and recall rates as a way to balance the amount of emails being
    filtered.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的方法有哪些？我们将构建一个分类模型，根据邮件的主题行标记潜在的垃圾邮件。我们将使用精确率和召回率作为平衡过滤邮件数量的方式。
- en: What are the success criteria? We want high recall rates (the percentage of
    actual spam emails retrieved over the total number of spam emails) without sacrificing
    too much for precision rates (the percentage of correctly classified spam emails
    among those predicted as spam).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功的标准是什么？我们希望有高的召回率（实际垃圾邮件被检索的百分比与垃圾邮件总数的比例），同时不牺牲太多的精确率（被预测为垃圾邮件的正确分类垃圾邮件的百分比）。
- en: Data preparation
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Now that we have clearly stated and defined the problem that we are going to
    solve with ML, we need the data. No data, no ML. Typically, you need to take an
    extra step prior to the data preparation step to collect and gather the data that
    you need, but in this book we are going to use a pre-compiled and labeled dataset
    that is publicly available. In this chapter, we are going to use the CSDMC2010
    SPAM corpus dataset ([http://csmining.org/index.php/spam-email-datasets-.html](http://csmining.org/index.php/spam-email-datasets-.html))
    to train and test our models. You can follow the link and download the compressed
    data at the bottom of the web page. When you have downloaded and decompressed
    the data, you will see two folders named `TESTING` and `TRAINING`, and a text
    file named `SPAMTrain.label`. The `SPAMTrain.label` file has encoded labels for
    each email in the `TRAINING` folder—`0` stands for spam and `1` stands for ham
    (non-spam). We will use this text file with the email data in the `TRAINING` folder
    to build spam email classification models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经清楚地陈述并定义了我们打算使用机器学习解决的问题，我们就需要数据。没有数据，就没有机器学习。通常，在数据准备步骤之前，您需要额外的一步来收集和整理所需的数据，但在这本书中，我们将使用一个预先编译并标记的公开可用的数据集。在本章中，我们将使用CSDMC2010
    SPAM语料库数据集（[http://csmining.org/index.php/spam-email-datasets-.html](http://csmining.org/index.php/spam-email-datasets-.html)）来训练和测试我们的模型。您可以点击链接并下载网页底部的压缩数据。当您下载并解压缩数据后，您将看到两个名为`TESTING`和`TRAINING`的文件夹，以及一个名为`SPAMTrain.label`的文本文件。`SPAMTrain.label`文件包含了`TRAINING`文件夹中每封电子邮件的编码标签——`0`代表垃圾邮件，`1`代表非垃圾邮件（非垃圾邮件）。我们将使用这个文本文件以及`TRAINING`文件夹中的电子邮件数据来构建垃圾邮件分类模型。
- en: 'Once you have downloaded the data and put it in a place where you can load
    it from, you need to prepare it for future feature engineering and model building
    steps. What we have now is a raw dataset that contains a number of EML files that
    contain information about individual emails and a text file that contains labeling
    information. To make this raw dataset usable for building spam email classification
    models using the email subject lines, we need to do the following tasks:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您下载了数据并将其放置在可以从中加载的位置，您需要为未来的特征工程和模型构建步骤准备它。我们现在有一个包含多个包含有关单个电子邮件信息的EML文件和包含标记信息的文本文件的原始数据集。为了使这个原始数据集可用于使用电子邮件主题行构建垃圾邮件分类模型，我们需要执行以下任务：
- en: '**Extract subject lines from EML files**: The first step to prepare our data
    for future tasks is to extract the subject and body from individual EML files.
    We are going to use a package called `EAGetMail` to load and extract information
    from EML files. You can install this package using the Package Manager in Visual
    Studio. Take a look at lines 4 to 6 of the code to install the package. Using
    the `EAGetMail` package, you can easily load and extract the subject and body
    contents from the EML files (lines 24–30). Once you have extracted the subject
    and body from an email, you need to append each line of data as a row to a Deedle
    data frame. Take a look at the `ParseEmails` function from line 18 in the following
    code to see how to create a Deedle data frame, where each row contains each email''s
    index number, subject line, and body content.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从EML文件中提取主题行**：为准备我们的数据以供未来任务使用，第一步是从单个EML文件中提取主题和正文。我们将使用一个名为`EAGetMail`的包来加载和提取EML文件中的信息。您可以使用Visual
    Studio中的包管理器来安装此包。请查看代码的第4到6行以了解如何安装此包。使用`EAGetMail`包，您可以轻松地加载和提取EML文件的主题和正文内容（第24-30行）。一旦您从电子邮件中提取了主题和正文，您需要将每行数据作为一行追加到一个Deedle数据框中。请查看以下代码中的`ParseEmails`函数（从第18行开始），以了解如何创建一个Deedle数据框，其中每行包含每封电子邮件的索引号、主题行和正文内容。'
- en: '**Combine the extracted data with the labels**: After extracting the subject
    and body contents from individual EML files, there is one more thing we need to
    do. We need to map the encoded labels (0 for spam versus 1 for ham) to each row
    of the data frame that we created in the previous step. If you open the `SPAMTrain.label`
    file with any text editor, you can see that the encoded label is in the first
    column and the corresponding email file name is in the second column, separated
    by a space. Using Deedle frame''s `ReadCsv` function, you can easily load this
    label data into a data frame by specifying a space as a separator (see line 50
    in the code). Once you have loaded this labeled data into a data frame, you can
    simply add the first column of this data frame to the other data frame we created
    in the previous step using the `AddColumn` function of Deedle''s frame. Take a
    look at lines 49-52 of the following code to see how we can combine the labeling
    information with the extracted email data.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将提取的数据与标签合并**：在从单个EML文件中提取主题和正文内容之后，我们还需要做一件事。我们需要将编码后的标签（0表示垃圾邮件，1表示正常邮件）映射到我们在上一步创建的DataFrame的每一行。如果您用任何文本编辑器打开`SPAMTrain.label`文件，您会看到编码的标签位于第一列，相应的电子邮件文件名位于第二列，由空格分隔。使用Deedle框架的`ReadCsv`函数，您可以通过指定空格作为分隔符轻松地将这些标签数据加载到DataFrame中（请参阅代码中的第50行）。一旦您将标记数据加载到DataFrame中，您只需使用Deedle框架的`AddColumn`函数将此DataFrame的第一列添加到我们在上一步创建的另一个DataFrame中。查看以下代码的第49-52行，了解我们如何将标签信息与提取的电子邮件数据合并。'
- en: '**Export this merged data as a CSV file**: Now that we have one data frame
    that contains both email and labeling data, it is time to export this data frame
    into a CSV file for future usage. As shown in line 54 in the following code, it
    takes one line to export the data frame into a CSV file. Using Deedle frame''s
    `SaveCsv` function, you can easily save the data frame as a CSV file.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将合并后的数据导出为CSV文件**：现在我们有一个包含邮件和标签数据的DataFrame，是时候将这个DataFrame导出为CSV文件以供将来使用。如以下代码的第54行所示，导出DataFrame到CSV文件只需要一行代码。使用Deedle框架的`SaveCsv`函数，您可以轻松地将DataFrame保存为CSV文件。'
- en: 'The code for this data preparation step is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据准备步骤的代码如下：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will need to replace `<path-to-data-directory>` in line 44 with the actual
    path where you have your data stored before you run this code. Once you run this
    code, a file named `transformed.csv` should be created and it will contain four
    columns (`emailNum`, `subject`, `body`, and `is_ham`). We will use this output
    data as an input to the following steps for building ML models for the spam email
    filtering project. However, feel free to be creative and play around with the
    Deedle framework and `EAGetMail` package to tweak and prepare this data in a different
    way. The code we presented here is one way to prepare this raw email data for
    future usage and some of the information you can extract from the raw email data.
    Using the `EAGetMail` package, you can extract other features, such as the sender's
    email addresses and attachments in the emails, and these extra features can potentially
    help improve your spam email classification models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此代码之前，您需要将第44行中的`<path-to-data-directory>`替换为您存储数据的实际路径。运行此代码后，应创建一个名为`transformed.csv`的文件，它将包含四列（`emailNum`、`subject`、`body`和`is_ham`）。我们将使用此输出数据作为以下步骤构建用于垃圾邮件过滤项目的机器学习模型的输入。不过，您可以自由发挥创意，尝试使用Deedle框架和`EAGetMail`包以不同的方式调整和准备这些数据。我们在这里展示的代码是准备原始邮件数据以供将来使用的一种方法，以及您可以从原始邮件数据中提取的一些信息。使用`EAGetMail`包，您可以提取其他特征，例如发件人的电子邮件地址和邮件中的附件，这些额外特征可能有助于提高您的垃圾邮件分类模型。
- en: The code for this data preparation step can also be found in the following repository: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/EmailParser.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/EmailParser.cs).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据准备步骤的代码也可以在以下仓库中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/EmailParser.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/EmailParser.cs).
- en: Email data analysis
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邮件数据分析
- en: In the data preparation step, we transformed the raw dataset into a more readable
    and usable dataset. We now have one file to look at to figure out which emails
    are spam and which emails are not. Also, we can easily find out the subject lines
    for spam emails and non-spam emails. With this transformed data, let's start looking
    at what the data actually looks like and see if we can find any patterns or issues
    within the data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备步骤中，我们将原始数据集转换成了一个更易于阅读和使用的数据集。现在我们有一个文件可以查看，以确定哪些电子邮件是垃圾邮件，哪些不是。此外，我们还可以轻松地找到垃圾邮件和非垃圾邮件的主题行。使用这个转换后的数据，让我们开始查看数据的实际样子，看看我们能否在数据中找到任何模式或问题。
- en: 'Since we are dealing with text data, the first thing we want to look at is
    how the word distributions differ between spam and non-spam emails. In order to
    do this, we need to transform the data output from the previous step into a matrix
    representation of word occurrences. Let''s work through this step by step, taking
    the first three subject lines from our data as an example. The first three subject
    lines we have are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是文本数据，我们首先想查看的是垃圾邮件和非垃圾邮件之间单词分布的差异。为了做到这一点，我们需要将之前步骤输出的数据转换成单词出现的矩阵表示。让我们一步一步地来做，以我们数据中的前三个主题行为例。我们拥有的前三个主题行如下所示：
- en: '![](img/00017.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00017.jpeg)'
- en: 'If we transform this data such that each column corresponds to each word in
    each subject line and encode the value of each cell as `1`, if the given subject
    line has the word, and `0` if not, then the resulting matrix looks something like
    the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些数据转换成这样，即每一列对应每个主题行中的每个单词，并将每个单元格的值编码为`1`，如果给定的主题行包含该单词，否则为`0`，那么得到的矩阵看起来可能如下所示：
- en: '![](img/00018.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00018.jpeg)'
- en: 'This specific way of encoding is called **one-hot encoding**, where we only
    care about whether the specific word occurred in the subject line or not and we
    do not care about the actual number of occurrences of each word in the subject
    line. In the aforementioned case, we also took out all the punctuation marks,
    such as colons, question marks, and exclamation points. To do this programmatically,
    we can use a `regex` to split each subject line into words that only contain alpha-numeric
    characters and then build a data frame with one-hot encoding. The code to do this
    encoding step looks like the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特定的编码方式被称为**独热编码**，我们只关心特定单词是否出现在主题行中，而不关心每个单词在主题行中实际出现的次数。在上述情况下，我们还移除了所有的标点符号，例如冒号、问号和感叹号。为了程序化地完成这项工作，我们可以使用正则表达式将每个主题行分割成只包含字母数字字符的单词，然后使用独热编码构建一个数据框。执行此编码步骤的代码如下所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Having this one-hot encoded matrix representation of words makes our data analysis
    process much easier. For example, if we want to take a look at the top ten frequently
    occurring words in spam emails, we can simply sum the values in each column of
    the one-hot encoded word matrix for spam emails and take the ten words with the
    highest summed values. This is exactly what we do in the following code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种独热编码的单词矩阵表示形式使我们的数据分析过程变得更加容易。例如，如果我们想查看垃圾邮件中前十位频繁出现的单词，我们只需简单地对垃圾邮件独热编码单词矩阵的每一列求和，然后取求和值最高的十个单词。这正是我们在以下代码中所做的：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see from this code, we use the `Sum` method of Deedle''s data frame
    to sum the values in each column and sort in reverse order. We do this once for
    spam emails and again for ham emails. Then, we use the `Take` method to get the
    top ten words that appear the most frequently in spam and ham emails. When you
    run this code, it will generate two CSV files: `ham-frequencies.csv` and `spam-frequencies.csv`.
    These two files contain information about the number of word occurrences in spam
    and ham emails, which we are going to use later for the feature engineering and
    model building steps.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中可以看到，我们使用了Deedle数据框的`Sum`方法对每一列的值进行求和，并按降序排序。我们为垃圾邮件和ham邮件各做一次。然后，我们使用`Take`方法获取在垃圾邮件和ham邮件中出现频率最高的前十个单词。运行此代码将生成两个CSV文件：`ham-frequencies.csv`和`spam-frequencies.csv`。这两个文件包含有关垃圾邮件和ham邮件中单词出现次数的信息，我们将在后续的特征工程和模型构建步骤中使用这些信息。
- en: 'Let''s now visualize some of the data for further analysis. First, take a look
    at the following plot for the top ten frequently appearing terms in ham emails
    in the dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们可视化一些数据以进行进一步分析。首先，看一下以下关于数据集中ham电子邮件中前十位频繁出现的术语的图表：
- en: '![](img/00019.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00019.jpeg)'
- en: A bar plot for the top ten frequently appearing terms in ham emails
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个条形图，用于展示正常邮件中频率最高的前十项术语
- en: 'As you can see from this bar chart, there are more ham emails than spam emails
    in the dataset, as in the real world. We typically get more ham emails than spam
    emails in our inbox. We used the following code to generate this bar chart to
    visualize the distribution of ham and spam emails in the dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如从该条形图中可以看出，在数据集中，正常邮件的数量多于垃圾邮件，这与现实世界的情况相符。我们通常在我们的收件箱中收到比垃圾邮件更多的正常邮件。我们使用了以下代码来生成此条形图，以可视化数据集中正常邮件和垃圾邮件的分布：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using the `DataBarBox` class in the Accord.NET framework, we can easily visualize
    data in bar charts. Let''s now visualize the top ten frequently occurring terms
    in ham and spam emails. You can use the following code to generate bar charts
    for the top ten terms in ham and spam emails:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Accord.NET框架中的`DataBarBox`类，我们可以轻松地将数据可视化在条形图中。现在让我们可视化正常邮件和垃圾邮件中频率最高的前十项术语。你可以使用以下代码生成正常邮件和垃圾邮件中前十项术语的条形图：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Similarly, we used `DataBarBox` class to display bar charts. When you run this
    code, you will see the following plot for the top ten frequently appearing terms
    in ham emails:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们使用了`DataBarBox`类来显示条形图。当你运行此代码时，你会看到以下条形图，用于展示正常邮件中频率最高的前十项术语：
- en: '![](img/00020.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00020.jpeg)'
- en: A plot for the top ten frequently appearing terms in ham emails
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一个展示正常邮件中频率最高的前十项术语的图表
- en: 'The bar plot for the top ten frequently occurring terms in spam emails looks
    like the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件中频率最高的前十项术语的条形图如下所示：
- en: '![](img/00021.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00021.jpeg)'
- en: A bar plot for the top ten frequently occurring terms in spam emails
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个条形图，用于展示垃圾邮件中频率最高的前十项术语
- en: As expected, the word distribution in spam emails is quite different from non-spam
    emails. For example, if you look at the chart on the right, the words **spam**
    and **hibody** appear frequently in spam emails, but not so much in non-spam emails.
    However, there is something that does not make much sense. If you look closely,
    the two words **trial** and **version** appear in all of the spam and ham emails,
    which is very unlikely to be true. If you open some of the raw EML files in a
    text editor, you can easily find out that not all of the emails contain those
    two words in their subject lines. So, what is happening? Did our data get polluted
    by our previous data preparation or data analysis steps?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '如预期的那样，垃圾邮件中的单词分布与非垃圾邮件有很大的不同。例如，如果你看右边的图表，单词**垃圾邮件**和**hibody**在垃圾邮件中频繁出现，但在非垃圾邮件中并不常见。然而，有些事情并不合理。如果你仔细观察，单词**试用**和**版本**出现在所有的垃圾邮件和正常邮件中，这很不可能是真的。如果你在文本编辑器中打开一些原始的EML文件，你可以很容易地发现并非所有的邮件都包含这两个单词在它们的主题行中。那么，发生了什么？我们的数据是否在之前的数据准备或数据分析步骤中受到了污染？ '
- en: 'Further research suggests that one of the packages that we used caused this
    issue. The `EAGetMail` package, which we used to load and extract email contents,
    automatically appends `(Trial Version)` to the end of the subject lines when we
    use their trial version. Now that we know the root cause of this data issue, we
    need to go back and fix it. One solution is to go back to the data preparation
    step and update our `ParseEmails` function with the following code, which simply
    drops the appended `(Trial Version)` flag from the subject lines:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的研究表明，我们使用的其中一个软件包导致了这个问题。我们使用的`EAGetMail`软件包，用于加载和提取电子邮件内容，当我们使用他们的试用版时，会自动将`(Trial
    Version)`附加到主题行的末尾。既然我们已经知道了这个数据问题的根本原因，我们需要回去修复它。一个解决方案是回到数据准备步骤，并更新我们的`ParseEmails`函数，使用以下代码，该代码简单地从主题行中删除附加的`(Trial
    Version)`标志：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After updating this code and running the previous data preparation and analysis
    code again, the bar charts for word distribution make much more sense.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新此代码并再次运行之前的数据准备和分析代码后，单词分布的条形图变得更加有意义。
- en: 'The following bar plot shows the top ten frequently occurring terms in ham
    emails after fixing and removing `(Trial Version)` flags:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下条形图显示了修复并移除`(Trial Version)`标志后的正常邮件中频率最高的前十项术语：
- en: '![](img/00022.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022.jpeg)'
- en: 'The following bar plot shows the top ten frequently occurring terms in spam
    emails after fixing and removing `(Trial Version)` flags:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下条形图显示了修复并移除`(Trial Version)`标志后的垃圾邮件中频率最高的前十项术语：
- en: '![](img/00023.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpeg)'
- en: This is a good example of the importance of a data analysis step when building
    ML models. Iterating between the data preparation and data analysis steps is very
    common, as we typically find issues with the data in the analysis step and often
    we can improve the data quality by updating some of the code used in the data
    preparation step. Now that we have clean data with a matrix representation of
    words used in subject lines, it is time to start working on the actual features
    that we will use for building ML models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在构建机器学习模型时数据分析步骤重要性的一个很好的例子。在数据准备和数据分析步骤之间迭代是非常常见的，因为我们通常在分析步骤中发现数据问题，并且我们通常可以通过更新数据准备步骤中使用的部分代码来提高数据质量。现在我们已经有了以矩阵形式表示主题行中使用的单词的干净数据，是时候开始着手构建机器学习模型所使用的实际特征了。
- en: Feature engineering for email data
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邮件数据的特征工程
- en: 'We briefly looked at word distributions for spam and ham emails in the previous
    step and there are a couple things that we noticed. First, a large number of the
    most frequently occurring words are commonly used words with out much meaning.
    For example, words like *to*, *the*, *for*, and *a* are commonly used words and
    our ML algorithms would not learn much from these words. These type of words are
    called **stop words** and are often ignored or dropped from the feature set. We
    will use NLTK''s list of stop words to filter out commonly used words from our
    feature set. You can download the NLTK list of stop words from here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/stopwords.txt](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/stopwords.txt).
    One way to filter out these stop words is shown in the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一步中，我们简要地查看了一下垃圾邮件和正常邮件的单词分布，并注意到了一些事情。首先，大多数最频繁出现的单词是常用词，意义不大。例如，像 *to*、*the*、*for*
    和 *a* 这样的单词是常用词，我们的机器学习算法从这些单词中不会学到很多东西。这类单词被称为**停用词**，通常会被忽略或从特征集中删除。我们将使用NLTK的停用词列表来过滤掉特征集中的常用词。您可以从这里下载NLTK的停用词列表：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/stopwords.txt](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/stopwords.txt)。过滤掉这些停用词的一种方法如下所示：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After filtering out these stop words, the new top ten frequently occurring
    terms for non-spam emails are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤掉这些停用词后，非垃圾邮件的新十大高频词如下：
- en: '![](img/00024.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00024.jpeg)'
- en: 'The top ten frequently occurring terms for spam emails, after filtering out
    stop words, look as the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在过滤掉停用词后，垃圾邮件的前十大高频词如下所示：
- en: '![](img/00025.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00025.jpeg)'
- en: 'As you can see from these bar charts, filtering out these stop words from the
    feature set made more meaningful words come to the top of the frequently appearing
    word lists. However, there is one more thing we can notice here. Numbers seem
    to come up as some of the top frequently occurring words. For example, the numbers
    **3** and **2** made it to the top ten frequently appearing words in ham emails.
    Numbers **80** and **70** made it to the top ten frequently appearing words in
    spam emails. However, it is hard to establish whether or not those numbers would
    contribute much in training ML models to classify an email as a spam or ham. There
    are multiple ways to filter out these numbers from the feature set, but we will
    show you one way to do it here. We updated the `regex` we used in the previous
    step to match words that contain alphabetical characters only, not alphanumeric
    characters. The following code shows how we updated the `CreateWordVec` function
    to filter out the numbers from the feature set:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这些条形图中可以看到，从特征集中过滤掉这些停用词使得更有意义的单词出现在高频出现的单词列表中。然而，我们还可以注意到另一件事。数字似乎出现在一些高频出现的单词中。例如，数字
    **3** 和 **2** 成为了垃圾邮件中前十位高频出现的单词。数字 **80** 和 **70** 成为了垃圾邮件中前十位高频出现的单词。然而，很难确定这些数字是否会在训练机器学习模型以将电子邮件分类为垃圾邮件或正常邮件时做出很大贡献。有多种方法可以从特征集中过滤掉这些数字，但在这里我们将向您展示一种方法。我们更新了之前步骤中使用的
    `regex`，以匹配仅包含字母字符的单词，而不是字母数字字符。以下代码显示了如何更新 `CreateWordVec` 函数以从特征集中过滤掉数字：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once we filter out those numbers from the feature set, the word distributions
    for ham emails looks like the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从特征集中过滤掉这些数字，垃圾邮件的单词分布看起来如下：
- en: '![](img/00026.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00026.jpeg)'
- en: 'And the word distributions for spam emails, after filtering out the numbers
    from the feature set, looks like the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在过滤掉特征集中的数字后，垃圾邮件的单词分布如下所示：
- en: '![](img/00027.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00027.jpeg)'
- en: As you can see from these bar charts, we have more meaningful words on the top
    lists and there seems to be a greater distinction between the word distributions
    for spam and ham emails. Those words that frequently appear in spam emails do
    not seem to appear much in ham emails and vice versa.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如从这些条形图中可以看出，我们列出了更有意义的词语，并且垃圾邮件和正常邮件的词语分布似乎有更大的区别。那些在垃圾邮件中频繁出现的词语似乎在正常邮件中很少出现，反之亦然。
- en: The full code for the data analysis and feature engineering step can be found
    in the following repo: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/DataAnalyzer.cs).
    Once you run this code, it will generate bar charts that show word distributions
    in spam and ham emails and two CSV files—one for the list of words in ham emails
    with the corresponding counts of occurrences and another for the list of words
    in spam emails with the corresponding counts of occurrences. We are going to use
    this term frequency output for feature selection processes when we build classification
    models for spam email filtering in the following model building section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和特征工程步骤的完整代码可以在以下仓库中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/DataAnalyzer.cs)。运行此代码后，将生成条形图，显示垃圾邮件和正常邮件中的词语分布，以及两个
    CSV 文件——一个用于包含出现次数的词语列表（正常邮件），另一个用于包含出现次数的词语列表（垃圾邮件）。在下一节构建分类模型进行垃圾邮件过滤时，我们将使用这个词频输出进行特征选择过程。
- en: Logistic regression versus Naive Bayes for email spam filtering
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归与朴素贝叶斯在电子邮件垃圾邮件过滤中的应用
- en: We have come a long way to finally build our very first ML models in C#. In
    this section, we are going to train logistic regression and Naive Bayes classifiers
    to classify emails into spam and ham. We are going to run cross-validations with
    those two learning algorithms to estimate and get a better understanding of how
    our classification models will perform in practice. As discussed briefly in the
    previous chapter, in k-fold cross-validation, the training set is divided into
    *k* equally sized subsets and one of those *k* subsets is held out as a validation
    set, and the rest of the *k-1* subsets are used to train a model. It then repeats
    this process *k* times, where different subsets or folds are used in each iteration
    as a validation set for testing, and the corresponding *k* validation results
    are then averaged to report a single estimation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走了很长的路，终于用 C# 构建了我们第一个机器学习模型。在本节中，我们将训练逻辑回归和朴素贝叶斯分类器，将电子邮件分类为垃圾邮件和正常邮件。我们将运行这两个学习算法的交叉验证，以估计并更好地理解我们的分类模型在实际应用中的表现。如前一章简要讨论的，在
    k 折交叉验证中，训练集被分成 *k* 个大小相等的子集，其中一个 *k* 个子集被保留作为验证集，其余的 *k-1* 个子集用于训练模型。然后重复这个过程
    *k* 次，其中每个迭代使用不同的子集或折作为验证集进行测试，然后将相应的 *k* 个验证结果平均报告为一个估计值。
- en: 'Let''s first look at how we can instantiate a cross-validation algorithm with
    logistic regression in C# using the Accord.NET framework. The code is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如何使用 Accord.NET 框架在 C# 中通过逻辑回归实现交叉验证算法的实例化。代码如下：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's take a deeper look at this code. We can create a new `CrossValidation`
    algorithm using the static `Create` function by supplying the type of model to
    train, the type of learning algorithm to fit the model, the type of input data,
    and the type of output data. For this example, we created a new `CrossValidation`
    algorithm with `LogisticRegression` as the model, `IterativeReweightedLeastSquares`
    as the learning algorithm, a double array as the type of input, and an integer
    as the type of output (each label). You can experiment with different learning
    algorithms to train a logistic regression model. In Accord.NET, you have the option
    to choose the stochastic gradient descent algorithm (`LogisticGradientDescent`)
    as a learning algorithm to fit a logistic regression model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地看看这段代码。我们可以通过提供要训练的模型类型、拟合模型的算法类型、输入数据类型和输出数据类型，使用静态 `Create` 函数创建一个新的
    `CrossValidation` 算法。在这个例子中，我们创建了一个新的 `CrossValidation` 算法，其中 `LogisticRegression`
    作为模型，`IterativeReweightedLeastSquares` 作为学习算法，双精度数组作为输入类型，整数作为输出类型（每个标签）。你可以尝试不同的学习算法来训练逻辑回归模型。在
    Accord.NET 中，你可以选择随机梯度下降算法 (`LogisticGradientDescent`) 作为拟合逻辑回归模型的学习算法。
- en: For the parameters, you can specify the number of folds for the k-fold cross-validation
    (*k*), the learning method with custom parameters (`learner`), the loss/cost function
    of your choice (`loss`), and a function that knows how to fit a model using the
    learning algorithm (`fit`), the input (`x`), and the output (`y`). For illustration
    purposes in this section, we set a relatively small number, `3`, for the k-fold
    cross-validation. Also, we chose a relatively small number, `100`, for the max
    iterations and a relatively large number, 1e-6 or 1/1,000,000, for regularization
    of the `IterativeReweightedLeastSquares` learning algorithm. For the loss function,
    we used a simple zero-one loss function, where it assigns 0s for the correct predictions
    and 1s for the incorrect predictions. This is the cost function that our learning
    algorithm tries to minimize. All of these parameters can be tuned differently.
    You can choose a different loss/cost function, the number of folds to use in k-fold
    cross-validation, and the maximum number of iterations and the regularization
    number for the learning algorithm. You can even use a different learning algorithm
    to fit a logistic regression model, such as `LogisticGradientDescent`, which iteratively
    tries to find the local minimum of a loss function.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于参数，你可以指定k折交叉验证的折数（*k*），具有自定义参数的学习方法（`learner`），你选择的损失/成本函数（`loss`），以及一个知道如何使用学习算法（`fit`）、输入（`x`）和输出（`y`）来拟合模型的功能。为了本节说明的目的，我们为k折交叉验证设置了一个相对较小的数字，`3`。此外，我们为最大迭代次数选择了相对较小的数字，`100`，以及相对较大的数字，1e-6或1/1,000,000，用于`IterativeReweightedLeastSquares`学习算法的正则化。对于损失函数，我们使用了一个简单的零一损失函数，其中对于正确预测分配0，对于错误预测分配1。这是我们学习算法试图最小化的成本函数。所有这些参数都可以进行不同的调整。你可以选择不同的损失/成本函数，k折交叉验证中使用的折数，以及学习算法的最大迭代次数和正则化数字。你甚至可以使用不同的学习算法来拟合逻辑回归模型，例如`LogisticGradientDescent`，它迭代地尝试找到一个损失函数的局部最小值。
- en: 'We can apply this same approach to train the Naive Bayes classifier with a
    k-fold cross-validation. The code to run k-fold cross-validation with the Naive
    Bayes learning algorithm is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种方法应用于使用k折交叉验证训练朴素贝叶斯分类器。使用朴素贝叶斯学习算法运行k折交叉验证的代码如下：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The only difference between the previous code for the logistic regression model
    and this code is the model and the learning algorithm we chose. Instead of `LogisticRegression`and `IterativeReweightedLeastSquares`,
    we used `NaiveBayes` as a model and `NaiveBayesLearning` as a learning algorithm
    to train our Naive Bayes classifier. Since all of our input values are binary
    (either 0 or 1), we used `BernoulliDistribution` for our Naive Bayes classifier
    model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 之前用于逻辑回归模型的代码与这段代码之间的唯一区别是我们选择的不同模型和学习算法。我们不是使用`LogisticRegression`和`IterativeReweightedLeastSquares`，而是使用了`NaiveBayes`作为模型，并使用`NaiveBayesLearning`作为学习算法来训练我们的朴素贝叶斯分类器。由于我们的所有输入值都是二元的（要么是0，要么是1），我们为我们的朴素贝叶斯分类器模型使用了`BernoulliDistribution`。
- en: 'The full code to train and validate a classification model with k-fold cross
    validation can be found in the following repository: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/Modeling.cs). When
    you run this code, you should see an output that looks like the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证具有k折交叉验证的分类模型的完整代码可以在以下仓库中找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/Modeling.cs)。当你运行此代码时，你应该会看到一个类似以下输出的结果：
- en: '![](img/00028.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00028.jpeg)'
- en: We will take a closer look at what these numbers represent in the following
    section where we discuss model validation methods. In order to try different ML
    models, simply modify lines 68–88 in the code. You can replace these with the
    logistic regression model code that we discussed previously or you can also try
    fitting a different learning algorithm of your choice.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地探讨这些数字代表什么，其中我们将讨论模型验证方法。为了尝试不同的机器学习模型，只需修改代码中的第68-88行。你可以用我们之前讨论过的逻辑回归模型代码替换这些行，或者你也可以尝试拟合你选择的不同学习算法。
- en: Classification model validations
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型验证
- en: We built our very first ML models in C# using the Accord.NET framework in the
    previous section. However, we are not quite done yet. If we look at the previous
    console output more closely, there is one thing that is quite concerning. The
    training error is around 0.03, but the validation error is about 0.26\. This means
    that our classification model predicted correctly 87 out of 100 times in the training
    set, but the model predictions in the validation or test set were correct only
    74 times out of 100\. This is a typical example of overfitting, where the model
    fits so closely to the train set that its predictions for the unforeseen dataset
    are unreliable and unpredictable. If we were to take this model and put it in
    the production spam filtering system, the model performance in practice for filtering
    spam emails would be unreliable and would be different from what we saw in the
    training set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中使用了C#和Accord.NET框架构建了我们非常第一个机器学习模型。然而，我们还没有完成。如果我们更仔细地查看之前的控制台输出，有一件事相当令人担忧。训练错误率大约是0.03，但验证错误率大约是0.26。这意味着我们的分类模型在训练集中正确预测了100次中的87次，但在验证或测试集中的模型预测只有100次中的74次是正确的。这是一个典型的过拟合例子，其中模型与训练集拟合得太紧密，以至于其对未预见数据集的预测是不可靠的和不可预测的。如果我们将这个模型用于生产中的垃圾邮件过滤系统，实际过滤垃圾邮件的性能将是不可靠的，并且与我们在训练集中看到的不同。
- en: 'Overfitting typically happens because the model is too complex for the given
    dataset or too many parameters were used to fit the model. The overfitting problem
    with the Naive Bayes classifier model we built in the last section is most likely
    due to the complexity and the number of features we used to train the model. If
    you look at the console output at the end of the last section again, you can see
    that the number of features used to train our Naive Bayes model was 2,212\. This
    is way too many features, considering that we only have about 4,200 email records
    in our sample set and only about two thirds of them (or about 3,000 records) were
    used to train our model (this is because we used 3-fold cross-validation and only
    two of those three folds were used as a training set in each iteration). To fix
    this overfitting issue, we will have to reduce the number of features we use to
    train a model. In order to do this, we can filter out those terms that occur not
    so often. The code to do this is in lines 48–53 of the full code in the previous
    section, which looks like the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合通常是因为模型对于给定的数据集来说过于复杂，或者使用了过多的参数来拟合模型。我们在上一节中构建的朴素贝叶斯分类器模型中存在的过拟合问题很可能是由于模型的复杂性和我们用来训练模型的特征数量。如果你再次查看上一节末尾的控制台输出，你可以看到我们用来训练朴素贝叶斯模型的特征数量是2,212。考虑到我们的样本集中只有大约4,200封电子邮件记录，而且其中只有大约三分之二（或者说大约3,000条记录）被用来训练我们的模型（这是因为我们使用了三折交叉验证，并且每次迭代中只有其中的两个折被用作训练集），这实在太多了。为了修复这个过拟合问题，我们将不得不减少我们用来训练模型的特征数量。为了做到这一点，我们可以过滤掉那些出现频率不高的术语。执行此操作的代码位于上一节完整代码的第48-53行，如下所示：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see from this code, the Naive Bayes classifier model that we built
    in the previous section used all the words that appeared in the spam emails at
    least once. If you look at the word frequencies in spam emails, there are about
    1,400 words that only occur once (take a look at the `spam-frequencies.csv` file
    that was created in the data analysis step). Intuitively, those words with a low
    number of occurrences would only create noise, not much information for our models
    to learn. This immediately tells us how much noise our model would have been exposed
    to when we initially built our classification model in the previous section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在上一节中构建的朴素贝叶斯分类器模型使用了在垃圾邮件中至少出现一次的所有单词。如果你查看垃圾邮件中的单词频率，大约有1,400个单词只出现了一次（查看在数据分析步骤中创建的`spam-frequencies.csv`文件）。直观上看，这些出现次数低的单词只会产生噪声，而不是为我们的模型提供很多学习信息。这立即告诉我们，当我们最初在上一节中构建我们的分类模型时，我们的模型会暴露于多少噪声。
- en: Now that we know the cause of this overfitting issue, let's fix it. Let's experiment
    with different thresholds for selecting features. We have tried 5, 10, 15, 20,
    and 25 for the minimum number of occurrences in spam emails (that is, we set `minNumOccurrences`
    to 5, 10, 15, and so on) and trained Naive Bayes classifiers with these thresholds.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了这个过拟合问题的原因，让我们来修复它。让我们尝试使用不同的阈值来选择特征。我们已经尝试了5、10、15、20和25作为垃圾邮件中最低出现次数（即我们将`minNumOccurrences`设置为5、10、15等等）并使用这些阈值训练了朴素贝叶斯分类器。
- en: 'First, the Naive Bayes classifier results with a minimum of five occurrences
    looks like the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，具有至少五次出现的朴素贝叶斯分类器结果如下：
- en: '![](img/00029.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00029.jpeg)'
- en: 'The Naive Bayes classifier results with a minimum of 10 occurrences looks like
    the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 具有至少10次出现的朴素贝叶斯分类器结果如下：
- en: '![](img/00030.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00030.jpeg)'
- en: 'The Naive Bayes classifier results with a minimum of 15 occurrences looks like
    the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 具有至少15次出现的朴素贝叶斯分类器结果如下：
- en: '![](img/00031.jpeg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00031.jpeg)'
- en: 'Lastly, the Naive Bayes classifier results with a minimum of 20 occurrences
    looks like the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，具有至少20次出现的朴素贝叶斯分类器结果如下：
- en: '![](img/00032.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00032.jpeg)'
- en: As you can see from these experiment results, as we increase the minimum number
    of word occurrences and reduce the number of features being used to train the
    model accordingly, the gap between `training error` and `validation error` decreases
    and the training errors start to look more similar to the validation errors. As
    we resolve the overfitting issues, we can be more confident in how the model will
    behave for the unforeseen data and in production systems. We ran the same experiment
    with the logistic regression classification model and the results are similar
    to what we have found with the Naive Bayes classifiers. The experiment results
    for the logistic regression model are shown in the following outputs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验结果中可以看出，随着我们增加最小单词出现次数并相应地减少用于训练模型的特征数量，`训练错误`和`验证错误`之间的差距减小，训练错误开始看起来更接近验证错误。当我们解决了过拟合问题，我们可以对模型在不可预见的数据和在生产系统中的表现更有信心。我们使用逻辑回归分类模型进行了相同的实验，结果与朴素贝叶斯分类器发现的结果相似。逻辑回归模型的实验结果如下所示。
- en: 'First, the logistic regression classifier results with a minimum of five occurrences
    looks like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，具有至少五次出现的逻辑回归分类器结果如下：
- en: '![](img/00029.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00029.jpeg)'
- en: 'The logistic regression classifier results with a minimum of ten occurrences
    looks like the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 具有至少十次出现的逻辑回归分类器结果如下：
- en: '![](img/00030.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00030.jpeg)'
- en: 'The logistic regression classifier results with a minimum of 15 occurrences
    looks like the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 具有至少15次出现的逻辑回归分类器结果如下：
- en: '![](img/00033.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00033.jpeg)'
- en: 'The logistic regression classifier results with a minimum of 20 occurrences
    looks like the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 具有至少20次出现的逻辑回归分类器结果如下：
- en: '![](img/00034.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00034.jpeg)'
- en: 'Now that we have covered how we can handle overfitting issues, there are a
    few more model performance metrics we want to look at:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了如何处理过拟合问题，还有一些模型性能指标我们想要查看：
- en: '**Confusion matrix**: Confusion matrix is a table that tells us the overall
    performance of a prediction model. Each column represents each of the actual classes
    and each row represents each of the predicted classes. In the case of a binary
    classification problem, the confusion matrix will be a 2 x 2 matrix, where the
    first row represents negative predictions and the second row represents positive
    predictions. The first column represents actual negatives and the second column
    represents actual positives. The following table illustrates what each of the
    cells in the confusion matrix for a binary classification problem represents:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：混淆矩阵是一个表格，它告诉我们预测模型的总体性能。每一列代表每个实际类别，每一行代表每个预测类别。在二元分类问题的案例中，混淆矩阵将是一个2
    x 2的矩阵，其中第一行代表负预测，第二行代表正预测。第一列代表实际负值，第二列代表实际正值。以下表格说明了二元分类问题的混淆矩阵中每个单元格代表的内容：'
- en: '![](img/00035.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00035.jpeg)'
- en: '**True Negative** (**TN**) is when the model predicted class 0 correctly; **False
    Negative** (**FN**) is when the model prediction is **0**, but the actual class
    is **1**; **False Positive** (**FP**) is when the model prediction is class **1**,
    but the actual class is **0**; and **True Positive** (**TP**) is when the model
    predicted class **1** correctly. As you can see from the table, a confusion matrix
    describes the overall model performance. In our example, if we look at the last
    console output in the previous screenshots where it shows the console output of
    our logistic regression classification model, we can see that the number of TNs
    is `2847`, the number of FNs is `606`, the number of FPs is `102`, and the number
    of TPs is `772`. With this information, we can further calculate the **true positive
    rates** (**TPR**), **true negative rates** (**TNR**), **false positive rates**
    (**FPR**), and **false negative rates** (**FNR**) as follows:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真负**（**TN**）是指模型正确预测了类别0；**假负**（**FN**）是指模型预测为**0**，但实际类别是**1**；**假正**（**FP**）是指模型预测为类别**1**，但实际类别是**0**；而**真正**（**TP**）是指模型正确预测了类别**1**。从表中可以看出，混淆矩阵描述了整体模型性能。在我们的例子中，如果我们查看之前截图中的最后一个控制台输出，其中显示了我们的逻辑回归分类模型的控制台输出，我们可以看到TNs的数量为`2847`，FNs的数量为`606`，FPs的数量为`102`，TPs的数量为`772`。有了这些信息，我们可以进一步计算**真正正率**（**TPR**）、**真正负率**（**TNR**）、**假正率**（**FPR**）和**假负率**（**FNR**）如下：'
- en: '![](img/00036.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00036.jpeg)'
- en: Using the preceding example, the true positive rate in our example is 0.56,
    the TNR is 0.97, the FPR is 0.03, and the FNR is 0.44.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的例子，我们例子中的真正正率（TPR）为0.56，真正负率（TNR）为0.97，假正率（FPR）为0.03，假负率（FNR）为0.44。
- en: '**Accuracy**: Accuracy is the proportion of correct predictions. Using the
    same notations from the previous example confusion matrix, the accuracy can be
    calculated as follows:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**: 准确率是指正确预测的比例。使用之前例子混淆矩阵中的相同符号，准确率可以计算如下：'
- en: '![](img/00037.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00037.jpeg)'
- en: 'Accuracy is a frequently used model performance metric, but sometimes it is
    not a good representation of the overall model performance. For instance, if the
    sample set is largely unbalanced, and if, say, there are five spam emails and
    95 hams in our sample set, then a simple classifier that classifies every email
    as ham will have to be 95% accurate. However, it will never catch spam emails.
    This is the reason why we need to look at confusion matrixes and other performance
    metrics, such as precision and recall rates:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是一个常用的模型性能指标，但有时它并不能很好地代表整体模型性能。例如，如果样本集大部分不平衡，比如说在我们的样本集中有五个垃圾邮件和95封正常邮件，那么一个简单地将所有邮件分类为正常邮件的分类器将不得不达到95%的准确率。然而，它永远不会捕获垃圾邮件。这就是为什么我们需要查看混淆矩阵和其他性能指标，如精确率和召回率：
- en: '**Precision rate**: Precision rate is the proportion of the number of correct
    positive predictions over the total number of positive predictions. Using the
    same notation as before, we can calculate the precision rate as follows:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**: 精确率是指正确预测的正例数量与总预测正例数量的比例。使用与之前相同的符号，我们可以计算精确率如下：'
- en: '![](img/00038.jpeg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00038.jpeg)'
- en: If you look at the last console output in the previous screenshots of our logistic
    regression classification model results, the precision rate was calculated by
    dividing the number of TPs in the confusion matrix, 772, by the sum of TPs, 772,
    and FPs, 102, from the confusion matrix, and the result was 0.88.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看之前截图中的最后一个控制台输出，我们的逻辑回归分类模型结果中的精确率是通过将混淆矩阵中的TPs数量，即772，除以TPs和FPs的总和，即772和102，得到的，结果为0.88。
- en: '**Recall rate**: Recall rate is the proportion of the number of correct positive
    predictions over the total number of actual positive cases. This is a way of telling
    us how many of the actual positive cases are retrieved by this model. Using the
    same notation as before, we can compute the recall rate as follows:'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**: 召回率是指正确预测的正例数量与实际正例总数量的比例。这是告诉我们模型检索了多少实际正例的一种方式。使用与之前相同的符号，我们可以计算召回率如下：'
- en: '![](img/00039.jpeg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00039.jpeg)'
- en: If you look at the last console output in the previous screenshots for our logistic
    regression classification mode results, the recall rate was calculated by dividing
    the number of TPs in the confusion matrix, 772, by the sum of TPs, 772, and FNs,
    606, from the confusion matrix, and the result was 0.56.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看上一张截图中的最后一个控制台输出，即我们的逻辑回归分类模型结果，召回率是通过将混淆矩阵中TP（真阳性）的数量，772，除以TP（真阳性）和FN（假阴性）的总和，772和606，得到的，结果是0.56。
- en: With these performance metrics, it is the data scientist's duty to choose the
    optimal model. There will always be a trade-off between precision and recall rates.
    A model with a higher precision rate than others will have a lower recall rate.
    In the case of our spam filtering problem, if you believe correctly filtering
    out spam emails is more important and that you can sacrifice some of the spam
    emails going through your users' Inboxes, then you might want to optimize for
    precision. On the other hand, if you believe filtering out as many spam emails
    as possible is more important, even though you might end up filtering out some
    non-spam emails as well, then you might want to optimize for recall. Choosing
    the right model is not an easy decision and thinking through the requirements
    and success criteria will be essential in making the right choice.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些性能指标，数据科学家的责任是选择最优模型。精确率和召回率之间总会存在权衡。一个精确率高于其他模型的模型将具有较低的召回率。在我们的垃圾邮件过滤问题中，如果你认为正确过滤掉垃圾邮件更重要，并且你可以牺牲一些通过用户收件箱的垃圾邮件，那么你可能希望优化精确率。另一方面，如果你认为过滤掉尽可能多的垃圾邮件更重要，即使你可能会过滤掉一些非垃圾邮件，那么你可能希望优化召回率。选择正确的模型不是一个容易的决定，思考需求和成功标准对于做出正确的选择至关重要。
- en: 'In summary, the following are the code we can use to compute performance metrics
    from the cross-validation result and confusion matrix:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，以下是我们可以从交叉验证结果和混淆矩阵中计算性能指标的代码：
- en: '**Training versus validation (test) errors**: Used to identify overfitting
    issues (lines 48–52):'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练与验证（测试）错误**：用于识别过拟合问题（第48-52行）：'
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Confusion matrix**: True Positives versus False Positives and True Negatives
    versus False Negatives (lines 95–108):'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：真阳性与假阳性，以及真阴性与假阴性（第95-108行）：'
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Accuracy versus precision versus recall**: Used to measure the correctness
    of ML models (lines 122–130):'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率与精确率与召回率**：用于衡量机器学习模型的正确性（第122-130行）：'
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built our very first ML models in C# that can be used for
    spam email filtering. We first defined and clearly stated what we were trying
    to solve and what the success criteria would be. Then, we extracted the relevant
    information from the raw email data and transformed it into a format that we could
    use for the data analysis, feature engineering, and ML model building steps. In
    the data analysis step, we learned how to apply one-hot encoding and built a matrix
    representation of words used in subject lines. We also identified a data issue
    from our data analysis process and learned how we often iterate back and forth
    between the data preparation and analysis steps. Then, we further improved our
    feature set by filtering out stop words and using a `regex` to split by non-alphanumeric
    or non-alphabetical words. With this feature set, we built our very first classification
    models using the logistic regression and Naive Bayes classifier algorithms, briefly
    covered the danger of overfitting, and learned how to evaluate and compare model
    performance by looking at accuracy, precision, and recall rates. Lastly, we also
    learned the trade-off between precision and recall and how to choose a model based
    on these metrics and business requirements.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用C#构建了我们第一个机器学习模型，它可以用于垃圾邮件过滤。我们首先定义并清楚地说明了我们试图解决的问题以及成功标准。然后，我们从原始电子邮件数据中提取相关信息，并将其转换成我们可以用于数据分析、特征工程和机器学习模型构建步骤的格式。在数据分析步骤中，我们学习了如何应用独热编码，并构建了用于主题行中使用的单词的矩阵表示。我们还从我们的数据分析过程中识别出一个数据问题，并学习了我们通常如何在数据准备和分析步骤之间来回迭代。然后，我们通过过滤掉停用词和使用正则表达式来分割非字母数字或非字母词来进一步改进我们的特征集。有了这个特征集，我们使用逻辑回归和朴素贝叶斯分类器算法构建了我们第一个分类模型，简要介绍了过拟合的危险，并学习了如何通过查看准确率、精确率和召回率来评估和比较模型性能。最后，我们还学习了精确率和召回率之间的权衡，以及如何根据这些指标和业务需求来选择模型。
- en: In the next chapter, we are going to further expand our knowledge and skills
    in building classification models using a text dataset. We will start looking
    at a dataset where we have more than two classes by using Twitter sentiment data.
    We are going to learn the difference between the binary classification model and
    the multi-class classification model. We will also discuss some other NLP techniques
    for feature engineering and how to build a multi-class classification model using
    the random forest algorithm.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进一步扩展我们在使用文本数据集构建分类模型方面的知识和技能。我们将从分析一个包含超过两个类别的数据集开始，使用Twitter情感数据。我们将学习二分类模型和多分类模型之间的区别。我们还将讨论一些用于特征工程的NLP技术，以及如何使用随机森林算法构建多分类分类模型。
