- en: Spam Email Filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to start building real-world **machine learning**
    (**ML**) models in C# using the two packages we installed in [Chapter 1](part0020.html#J2B80-5ebdf09927b7492888e31e8436526470), *Basics
    of Machine Learning Modeling*; Accord.NET for ML and Deedle for data manipulation.
    In this chapter, we are going to build a classification model for spam email filtering.
    We will be working with a raw email dataset that contains both spam and ham (non-spam)
    emails and use it to train our ML model. We are going to start following the steps
    for developing ML models that we discussed in the previous chapter. This will
    help us understand the workflow and approaches in ML modeling better and make
    them second nature. While we work to build a spam email classification model,
    we will also discuss feature engineering techniques for text datasets and basic
    model validation methods for classification models, and compare the logistic regression
    classifier and Naive Bayes classifier for spam email filtering. Familiarizing
    ourselves with these model-building steps, basic text feature engineering techniques,
    and basic classification model validation methods will lay the groundwork for
    more advanced feature engineering techniques using **natural language processing**
    (**NLP**) and for building multi-class classification models in [Chapter 3](part0036.html#12AK80-5ebdf09927b7492888e31e8436526470),
    *Twitter Sentiment Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition for the spam email filtering project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Email data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering for email data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression versus Naive Bayes for spam email filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification model validations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem definition for the spam email filtering project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by defining the problem that we are going to solve in this chapter.
    You are probably familiar with what spam emails are already; spam email filtering
    is an essential feature for email services such as Gmail, Yahoo Mail, and Outlook.
    Spam emails can be annoying for users, but they bring more issues and risks with
    them. For example, a spam email can be designed to solicit credit card numbers
    or bank account information, which can be used for credit card fraud or money
    laundering. A spam email can also be used to obtain personal data, such as a social
    security number or user IDs and passwords, which then can be used for identity
    theft and various other crimes. Having spam email filtering technology in place
    is an essential step for email services to save users from being exposed to such
    crimes. However, having the right spam email filtering solution is difficult.
    You want to filter out suspicious emails, but at the same time, you do not want
    to filter too much so that non-spam emails go into the spam folder and never get
    looked at by users. To solve this problem, we are going to have our ML models
    learn from the raw email dataset and classify suspicious emails as spam using
    the subject line. We are going to look at two performance metrics to measure our
    success: precision and recall. We will discuss these metrics in detail in the
    following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our problem definition:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem? We need a spam email filtering solution to prevent our
    users from being victims of fraudulent activities and to improve user experience
    at the same time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it a problem? Having the right balance between filtering suspicious emails
    and not filtering too much, so that non-spam emails still get the Inbox, is difficult.
    We are going to rely on ML models to learn how to classify such suspicious emails
    statistically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some of the approaches to solving this problem? We will build a classification
    model that flags potential spam emails based on the subject lines of emails. We
    will use precision and recall rates as a way to balance the amount of emails being
    filtered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the success criteria? We want high recall rates (the percentage of
    actual spam emails retrieved over the total number of spam emails) without sacrificing
    too much for precision rates (the percentage of correctly classified spam emails
    among those predicted as spam).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have clearly stated and defined the problem that we are going to
    solve with ML, we need the data. No data, no ML. Typically, you need to take an
    extra step prior to the data preparation step to collect and gather the data that
    you need, but in this book we are going to use a pre-compiled and labeled dataset
    that is publicly available. In this chapter, we are going to use the CSDMC2010
    SPAM corpus dataset ([http://csmining.org/index.php/spam-email-datasets-.html](http://csmining.org/index.php/spam-email-datasets-.html))
    to train and test our models. You can follow the link and download the compressed
    data at the bottom of the web page. When you have downloaded and decompressed
    the data, you will see two folders named `TESTING` and `TRAINING`, and a text
    file named `SPAMTrain.label`. The `SPAMTrain.label` file has encoded labels for
    each email in the `TRAINING` folder—`0` stands for spam and `1` stands for ham
    (non-spam). We will use this text file with the email data in the `TRAINING` folder
    to build spam email classification models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the data and put it in a place where you can load
    it from, you need to prepare it for future feature engineering and model building
    steps. What we have now is a raw dataset that contains a number of EML files that
    contain information about individual emails and a text file that contains labeling
    information. To make this raw dataset usable for building spam email classification
    models using the email subject lines, we need to do the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Extract subject lines from EML files**: The first step to prepare our data
    for future tasks is to extract the subject and body from individual EML files.
    We are going to use a package called `EAGetMail` to load and extract information
    from EML files. You can install this package using the Package Manager in Visual
    Studio. Take a look at lines 4 to 6 of the code to install the package. Using
    the `EAGetMail` package, you can easily load and extract the subject and body
    contents from the EML files (lines 24–30). Once you have extracted the subject
    and body from an email, you need to append each line of data as a row to a Deedle
    data frame. Take a look at the `ParseEmails` function from line 18 in the following
    code to see how to create a Deedle data frame, where each row contains each email''s
    index number, subject line, and body content.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Combine the extracted data with the labels**: After extracting the subject
    and body contents from individual EML files, there is one more thing we need to
    do. We need to map the encoded labels (0 for spam versus 1 for ham) to each row
    of the data frame that we created in the previous step. If you open the `SPAMTrain.label`
    file with any text editor, you can see that the encoded label is in the first
    column and the corresponding email file name is in the second column, separated
    by a space. Using Deedle frame''s `ReadCsv` function, you can easily load this
    label data into a data frame by specifying a space as a separator (see line 50
    in the code). Once you have loaded this labeled data into a data frame, you can
    simply add the first column of this data frame to the other data frame we created
    in the previous step using the `AddColumn` function of Deedle''s frame. Take a
    look at lines 49-52 of the following code to see how we can combine the labeling
    information with the extracted email data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Export this merged data as a CSV file**: Now that we have one data frame
    that contains both email and labeling data, it is time to export this data frame
    into a CSV file for future usage. As shown in line 54 in the following code, it
    takes one line to export the data frame into a CSV file. Using Deedle frame''s
    `SaveCsv` function, you can easily save the data frame as a CSV file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code for this data preparation step is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You will need to replace `<path-to-data-directory>` in line 44 with the actual
    path where you have your data stored before you run this code. Once you run this
    code, a file named `transformed.csv` should be created and it will contain four
    columns (`emailNum`, `subject`, `body`, and `is_ham`). We will use this output
    data as an input to the following steps for building ML models for the spam email
    filtering project. However, feel free to be creative and play around with the
    Deedle framework and `EAGetMail` package to tweak and prepare this data in a different
    way. The code we presented here is one way to prepare this raw email data for
    future usage and some of the information you can extract from the raw email data.
    Using the `EAGetMail` package, you can extract other features, such as the sender's
    email addresses and attachments in the emails, and these extra features can potentially
    help improve your spam email classification models.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this data preparation step can also be found in the following repository: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/EmailParser.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/EmailParser.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Email data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the data preparation step, we transformed the raw dataset into a more readable
    and usable dataset. We now have one file to look at to figure out which emails
    are spam and which emails are not. Also, we can easily find out the subject lines
    for spam emails and non-spam emails. With this transformed data, let's start looking
    at what the data actually looks like and see if we can find any patterns or issues
    within the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are dealing with text data, the first thing we want to look at is
    how the word distributions differ between spam and non-spam emails. In order to
    do this, we need to transform the data output from the previous step into a matrix
    representation of word occurrences. Let''s work through this step by step, taking
    the first three subject lines from our data as an example. The first three subject
    lines we have are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00017.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we transform this data such that each column corresponds to each word in
    each subject line and encode the value of each cell as `1`, if the given subject
    line has the word, and `0` if not, then the resulting matrix looks something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00018.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This specific way of encoding is called **one-hot encoding**, where we only
    care about whether the specific word occurred in the subject line or not and we
    do not care about the actual number of occurrences of each word in the subject
    line. In the aforementioned case, we also took out all the punctuation marks,
    such as colons, question marks, and exclamation points. To do this programmatically,
    we can use a `regex` to split each subject line into words that only contain alpha-numeric
    characters and then build a data frame with one-hot encoding. The code to do this
    encoding step looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Having this one-hot encoded matrix representation of words makes our data analysis
    process much easier. For example, if we want to take a look at the top ten frequently
    occurring words in spam emails, we can simply sum the values in each column of
    the one-hot encoded word matrix for spam emails and take the ten words with the
    highest summed values. This is exactly what we do in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, we use the `Sum` method of Deedle''s data frame
    to sum the values in each column and sort in reverse order. We do this once for
    spam emails and again for ham emails. Then, we use the `Take` method to get the
    top ten words that appear the most frequently in spam and ham emails. When you
    run this code, it will generate two CSV files: `ham-frequencies.csv` and `spam-frequencies.csv`.
    These two files contain information about the number of word occurrences in spam
    and ham emails, which we are going to use later for the feature engineering and
    model building steps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now visualize some of the data for further analysis. First, take a look
    at the following plot for the top ten frequently appearing terms in ham emails
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00019.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A bar plot for the top ten frequently appearing terms in ham emails
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from this bar chart, there are more ham emails than spam emails
    in the dataset, as in the real world. We typically get more ham emails than spam
    emails in our inbox. We used the following code to generate this bar chart to
    visualize the distribution of ham and spam emails in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `DataBarBox` class in the Accord.NET framework, we can easily visualize
    data in bar charts. Let''s now visualize the top ten frequently occurring terms
    in ham and spam emails. You can use the following code to generate bar charts
    for the top ten terms in ham and spam emails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we used `DataBarBox` class to display bar charts. When you run this
    code, you will see the following plot for the top ten frequently appearing terms
    in ham emails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00020.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A plot for the top ten frequently appearing terms in ham emails
  prefs: []
  type: TYPE_NORMAL
- en: 'The bar plot for the top ten frequently occurring terms in spam emails looks
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A bar plot for the top ten frequently occurring terms in spam emails
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the word distribution in spam emails is quite different from non-spam
    emails. For example, if you look at the chart on the right, the words **spam**
    and **hibody** appear frequently in spam emails, but not so much in non-spam emails.
    However, there is something that does not make much sense. If you look closely,
    the two words **trial** and **version** appear in all of the spam and ham emails,
    which is very unlikely to be true. If you open some of the raw EML files in a
    text editor, you can easily find out that not all of the emails contain those
    two words in their subject lines. So, what is happening? Did our data get polluted
    by our previous data preparation or data analysis steps?
  prefs: []
  type: TYPE_NORMAL
- en: 'Further research suggests that one of the packages that we used caused this
    issue. The `EAGetMail` package, which we used to load and extract email contents,
    automatically appends `(Trial Version)` to the end of the subject lines when we
    use their trial version. Now that we know the root cause of this data issue, we
    need to go back and fix it. One solution is to go back to the data preparation
    step and update our `ParseEmails` function with the following code, which simply
    drops the appended `(Trial Version)` flag from the subject lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: After updating this code and running the previous data preparation and analysis
    code again, the bar charts for word distribution make much more sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following bar plot shows the top ten frequently occurring terms in ham
    emails after fixing and removing `(Trial Version)` flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00022.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following bar plot shows the top ten frequently occurring terms in spam
    emails after fixing and removing `(Trial Version)` flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is a good example of the importance of a data analysis step when building
    ML models. Iterating between the data preparation and data analysis steps is very
    common, as we typically find issues with the data in the analysis step and often
    we can improve the data quality by updating some of the code used in the data
    preparation step. Now that we have clean data with a matrix representation of
    words used in subject lines, it is time to start working on the actual features
    that we will use for building ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for email data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We briefly looked at word distributions for spam and ham emails in the previous
    step and there are a couple things that we noticed. First, a large number of the
    most frequently occurring words are commonly used words with out much meaning.
    For example, words like *to*, *the*, *for*, and *a* are commonly used words and
    our ML algorithms would not learn much from these words. These type of words are
    called **stop words** and are often ignored or dropped from the feature set. We
    will use NLTK''s list of stop words to filter out commonly used words from our
    feature set. You can download the NLTK list of stop words from here: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/stopwords.txt](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/stopwords.txt).
    One way to filter out these stop words is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After filtering out these stop words, the new top ten frequently occurring
    terms for non-spam emails are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00024.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The top ten frequently occurring terms for spam emails, after filtering out
    stop words, look as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from these bar charts, filtering out these stop words from the
    feature set made more meaningful words come to the top of the frequently appearing
    word lists. However, there is one more thing we can notice here. Numbers seem
    to come up as some of the top frequently occurring words. For example, the numbers
    **3** and **2** made it to the top ten frequently appearing words in ham emails.
    Numbers **80** and **70** made it to the top ten frequently appearing words in
    spam emails. However, it is hard to establish whether or not those numbers would
    contribute much in training ML models to classify an email as a spam or ham. There
    are multiple ways to filter out these numbers from the feature set, but we will
    show you one way to do it here. We updated the `regex` we used in the previous
    step to match words that contain alphabetical characters only, not alphanumeric
    characters. The following code shows how we updated the `CreateWordVec` function
    to filter out the numbers from the feature set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we filter out those numbers from the feature set, the word distributions
    for ham emails looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the word distributions for spam emails, after filtering out the numbers
    from the feature set, looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00027.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from these bar charts, we have more meaningful words on the top
    lists and there seems to be a greater distinction between the word distributions
    for spam and ham emails. Those words that frequently appear in spam emails do
    not seem to appear much in ham emails and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for the data analysis and feature engineering step can be found
    in the following repo: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/DataAnalyzer.cs).
    Once you run this code, it will generate bar charts that show word distributions
    in spam and ham emails and two CSV files—one for the list of words in ham emails
    with the corresponding counts of occurrences and another for the list of words
    in spam emails with the corresponding counts of occurrences. We are going to use
    this term frequency output for feature selection processes when we build classification
    models for spam email filtering in the following model building section.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression versus Naive Bayes for email spam filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have come a long way to finally build our very first ML models in C#. In
    this section, we are going to train logistic regression and Naive Bayes classifiers
    to classify emails into spam and ham. We are going to run cross-validations with
    those two learning algorithms to estimate and get a better understanding of how
    our classification models will perform in practice. As discussed briefly in the
    previous chapter, in k-fold cross-validation, the training set is divided into
    *k* equally sized subsets and one of those *k* subsets is held out as a validation
    set, and the rest of the *k-1* subsets are used to train a model. It then repeats
    this process *k* times, where different subsets or folds are used in each iteration
    as a validation set for testing, and the corresponding *k* validation results
    are then averaged to report a single estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first look at how we can instantiate a cross-validation algorithm with
    logistic regression in C# using the Accord.NET framework. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a deeper look at this code. We can create a new `CrossValidation`
    algorithm using the static `Create` function by supplying the type of model to
    train, the type of learning algorithm to fit the model, the type of input data,
    and the type of output data. For this example, we created a new `CrossValidation`
    algorithm with `LogisticRegression` as the model, `IterativeReweightedLeastSquares`
    as the learning algorithm, a double array as the type of input, and an integer
    as the type of output (each label). You can experiment with different learning
    algorithms to train a logistic regression model. In Accord.NET, you have the option
    to choose the stochastic gradient descent algorithm (`LogisticGradientDescent`)
    as a learning algorithm to fit a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: For the parameters, you can specify the number of folds for the k-fold cross-validation
    (*k*), the learning method with custom parameters (`learner`), the loss/cost function
    of your choice (`loss`), and a function that knows how to fit a model using the
    learning algorithm (`fit`), the input (`x`), and the output (`y`). For illustration
    purposes in this section, we set a relatively small number, `3`, for the k-fold
    cross-validation. Also, we chose a relatively small number, `100`, for the max
    iterations and a relatively large number, 1e-6 or 1/1,000,000, for regularization
    of the `IterativeReweightedLeastSquares` learning algorithm. For the loss function,
    we used a simple zero-one loss function, where it assigns 0s for the correct predictions
    and 1s for the incorrect predictions. This is the cost function that our learning
    algorithm tries to minimize. All of these parameters can be tuned differently.
    You can choose a different loss/cost function, the number of folds to use in k-fold
    cross-validation, and the maximum number of iterations and the regularization
    number for the learning algorithm. You can even use a different learning algorithm
    to fit a logistic regression model, such as `LogisticGradientDescent`, which iteratively
    tries to find the local minimum of a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply this same approach to train the Naive Bayes classifier with a
    k-fold cross-validation. The code to run k-fold cross-validation with the Naive
    Bayes learning algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The only difference between the previous code for the logistic regression model
    and this code is the model and the learning algorithm we chose. Instead of `LogisticRegression`and `IterativeReweightedLeastSquares`,
    we used `NaiveBayes` as a model and `NaiveBayesLearning` as a learning algorithm
    to train our Naive Bayes classifier. Since all of our input values are binary
    (either 0 or 1), we used `BernoulliDistribution` for our Naive Bayes classifier
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code to train and validate a classification model with k-fold cross
    validation can be found in the following repository: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.2/Modeling.cs). When
    you run this code, you should see an output that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We will take a closer look at what these numbers represent in the following
    section where we discuss model validation methods. In order to try different ML
    models, simply modify lines 68–88 in the code. You can replace these with the
    logistic regression model code that we discussed previously or you can also try
    fitting a different learning algorithm of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Classification model validations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We built our very first ML models in C# using the Accord.NET framework in the
    previous section. However, we are not quite done yet. If we look at the previous
    console output more closely, there is one thing that is quite concerning. The
    training error is around 0.03, but the validation error is about 0.26\. This means
    that our classification model predicted correctly 87 out of 100 times in the training
    set, but the model predictions in the validation or test set were correct only
    74 times out of 100\. This is a typical example of overfitting, where the model
    fits so closely to the train set that its predictions for the unforeseen dataset
    are unreliable and unpredictable. If we were to take this model and put it in
    the production spam filtering system, the model performance in practice for filtering
    spam emails would be unreliable and would be different from what we saw in the
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting typically happens because the model is too complex for the given
    dataset or too many parameters were used to fit the model. The overfitting problem
    with the Naive Bayes classifier model we built in the last section is most likely
    due to the complexity and the number of features we used to train the model. If
    you look at the console output at the end of the last section again, you can see
    that the number of features used to train our Naive Bayes model was 2,212\. This
    is way too many features, considering that we only have about 4,200 email records
    in our sample set and only about two thirds of them (or about 3,000 records) were
    used to train our model (this is because we used 3-fold cross-validation and only
    two of those three folds were used as a training set in each iteration). To fix
    this overfitting issue, we will have to reduce the number of features we use to
    train a model. In order to do this, we can filter out those terms that occur not
    so often. The code to do this is in lines 48–53 of the full code in the previous
    section, which looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, the Naive Bayes classifier model that we built
    in the previous section used all the words that appeared in the spam emails at
    least once. If you look at the word frequencies in spam emails, there are about
    1,400 words that only occur once (take a look at the `spam-frequencies.csv` file
    that was created in the data analysis step). Intuitively, those words with a low
    number of occurrences would only create noise, not much information for our models
    to learn. This immediately tells us how much noise our model would have been exposed
    to when we initially built our classification model in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the cause of this overfitting issue, let's fix it. Let's experiment
    with different thresholds for selecting features. We have tried 5, 10, 15, 20,
    and 25 for the minimum number of occurrences in spam emails (that is, we set `minNumOccurrences`
    to 5, 10, 15, and so on) and trained Naive Bayes classifiers with these thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the Naive Bayes classifier results with a minimum of five occurrences
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Naive Bayes classifier results with a minimum of 10 occurrences looks like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Naive Bayes classifier results with a minimum of 15 occurrences looks like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, the Naive Bayes classifier results with a minimum of 20 occurrences
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from these experiment results, as we increase the minimum number
    of word occurrences and reduce the number of features being used to train the
    model accordingly, the gap between `training error` and `validation error` decreases
    and the training errors start to look more similar to the validation errors. As
    we resolve the overfitting issues, we can be more confident in how the model will
    behave for the unforeseen data and in production systems. We ran the same experiment
    with the logistic regression classification model and the results are similar
    to what we have found with the Naive Bayes classifiers. The experiment results
    for the logistic regression model are shown in the following outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the logistic regression classifier results with a minimum of five occurrences
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic regression classifier results with a minimum of ten occurrences
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic regression classifier results with a minimum of 15 occurrences
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic regression classifier results with a minimum of 20 occurrences
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have covered how we can handle overfitting issues, there are a
    few more model performance metrics we want to look at:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: Confusion matrix is a table that tells us the overall
    performance of a prediction model. Each column represents each of the actual classes
    and each row represents each of the predicted classes. In the case of a binary
    classification problem, the confusion matrix will be a 2 x 2 matrix, where the
    first row represents negative predictions and the second row represents positive
    predictions. The first column represents actual negatives and the second column
    represents actual positives. The following table illustrates what each of the
    cells in the confusion matrix for a binary classification problem represents:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00035.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**True Negative** (**TN**) is when the model predicted class 0 correctly; **False
    Negative** (**FN**) is when the model prediction is **0**, but the actual class
    is **1**; **False Positive** (**FP**) is when the model prediction is class **1**,
    but the actual class is **0**; and **True Positive** (**TP**) is when the model
    predicted class **1** correctly. As you can see from the table, a confusion matrix
    describes the overall model performance. In our example, if we look at the last
    console output in the previous screenshots where it shows the console output of
    our logistic regression classification model, we can see that the number of TNs
    is `2847`, the number of FNs is `606`, the number of FPs is `102`, and the number
    of TPs is `772`. With this information, we can further calculate the **true positive
    rates** (**TPR**), **true negative rates** (**TNR**), **false positive rates**
    (**FPR**), and **false negative rates** (**FNR**) as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using the preceding example, the true positive rate in our example is 0.56,
    the TNR is 0.97, the FPR is 0.03, and the FNR is 0.44.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Accuracy is the proportion of correct predictions. Using the
    same notations from the previous example confusion matrix, the accuracy can be
    calculated as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Accuracy is a frequently used model performance metric, but sometimes it is
    not a good representation of the overall model performance. For instance, if the
    sample set is largely unbalanced, and if, say, there are five spam emails and
    95 hams in our sample set, then a simple classifier that classifies every email
    as ham will have to be 95% accurate. However, it will never catch spam emails.
    This is the reason why we need to look at confusion matrixes and other performance
    metrics, such as precision and recall rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision rate**: Precision rate is the proportion of the number of correct
    positive predictions over the total number of positive predictions. Using the
    same notation as before, we can calculate the precision rate as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you look at the last console output in the previous screenshots of our logistic
    regression classification model results, the precision rate was calculated by
    dividing the number of TPs in the confusion matrix, 772, by the sum of TPs, 772,
    and FPs, 102, from the confusion matrix, and the result was 0.88.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall rate**: Recall rate is the proportion of the number of correct positive
    predictions over the total number of actual positive cases. This is a way of telling
    us how many of the actual positive cases are retrieved by this model. Using the
    same notation as before, we can compute the recall rate as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you look at the last console output in the previous screenshots for our logistic
    regression classification mode results, the recall rate was calculated by dividing
    the number of TPs in the confusion matrix, 772, by the sum of TPs, 772, and FNs,
    606, from the confusion matrix, and the result was 0.56.
  prefs: []
  type: TYPE_NORMAL
- en: With these performance metrics, it is the data scientist's duty to choose the
    optimal model. There will always be a trade-off between precision and recall rates.
    A model with a higher precision rate than others will have a lower recall rate.
    In the case of our spam filtering problem, if you believe correctly filtering
    out spam emails is more important and that you can sacrifice some of the spam
    emails going through your users' Inboxes, then you might want to optimize for
    precision. On the other hand, if you believe filtering out as many spam emails
    as possible is more important, even though you might end up filtering out some
    non-spam emails as well, then you might want to optimize for recall. Choosing
    the right model is not an easy decision and thinking through the requirements
    and success criteria will be essential in making the right choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the following are the code we can use to compute performance metrics
    from the cross-validation result and confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training versus validation (test) errors**: Used to identify overfitting
    issues (lines 48–52):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Confusion matrix**: True Positives versus False Positives and True Negatives
    versus False Negatives (lines 95–108):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Accuracy versus precision versus recall**: Used to measure the correctness
    of ML models (lines 122–130):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built our very first ML models in C# that can be used for
    spam email filtering. We first defined and clearly stated what we were trying
    to solve and what the success criteria would be. Then, we extracted the relevant
    information from the raw email data and transformed it into a format that we could
    use for the data analysis, feature engineering, and ML model building steps. In
    the data analysis step, we learned how to apply one-hot encoding and built a matrix
    representation of words used in subject lines. We also identified a data issue
    from our data analysis process and learned how we often iterate back and forth
    between the data preparation and analysis steps. Then, we further improved our
    feature set by filtering out stop words and using a `regex` to split by non-alphanumeric
    or non-alphabetical words. With this feature set, we built our very first classification
    models using the logistic regression and Naive Bayes classifier algorithms, briefly
    covered the danger of overfitting, and learned how to evaluate and compare model
    performance by looking at accuracy, precision, and recall rates. Lastly, we also
    learned the trade-off between precision and recall and how to choose a model based
    on these metrics and business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to further expand our knowledge and skills
    in building classification models using a text dataset. We will start looking
    at a dataset where we have more than two classes by using Twitter sentiment data.
    We are going to learn the difference between the binary classification model and
    the multi-class classification model. We will also discuss some other NLP techniques
    for feature engineering and how to build a multi-class classification model using
    the random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
