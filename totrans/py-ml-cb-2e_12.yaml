- en: Reinforcement Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Weather forecasting with MDP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing a financial portfolio using DP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the shortest path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding the discount factor using Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a deep Q-learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing an AI-based dynamic modeling system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep reinforcement learning with Double Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-Network algorithm with dueling Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you will need the following files (available
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`MarkovChain.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KPDP.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DijkstraNX.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FrozenQlearning.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FrozenDeepQLearning.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dqn_cartpole.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DoubleDQNCartpole.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DuelingDQNCartpole.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning represents a family of algorithms that are able to learn
    and adapt to environmental changes. It is based on the concept of receiving external
    stimuli based on the choices of the algorithm. A correct choice will result in
    a reward, while a wrong choice will lead to a penalty. The goal of the system
    is to achieve the best possible result.
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, the correct output is clearly specified (learning with
    a teacher). But it is not always possible to do so. Often, we only have qualitative
    information. The information that's available is called a **reinforcement signal**.
    In these cases, the system does not provide any information on how to update the
    agent's behavior (for example, weights). You cannot define a cost function or
    a gradient. The goal of the system is to create the smart agents that are able
    to learn from their experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see a flowchart that displays the reinforcement
    learning interaction with the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7ba0357-9922-4ceb-8752-3787f92d9767.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the steps to follow to correctly apply a reinforcement learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the agent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observation of the environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selection of the optimal strategy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execution of actions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculation of the corresponding reward (or penalty)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development of updating strategies (if necessary)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repetition of steps 2 to 5 iteratively until the agent learns the optimal strategies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reinforcement learning tries to maximize the rewards that are received for the
    execution of the action or set of actions that allow a goal to be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: Weather forecasting with MDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid load problems and computational difficulties, the agent-environment
    interaction is considered a **Markov decision process** (**MDP**). An MDP is a
    discrete time stochastic control process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic processes** are mathematical models that are used to study the
    evolution of phenomena following random or probabilistic laws. It is known that
    in all natural phenomena, both by their very nature and by observational errors,
    a random or accidental component is present.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This component causes the following: at every instance of *t*, the result of
    the observation of the phenomenon is a random number or random variable, *st*.
    It is not possible to predict with certainty what the result will be; you can
    only state that it will take one of several possible values, each of which has
    a given probability.'
  prefs: []
  type: TYPE_NORMAL
- en: A stochastic process is called **Markovian** when, having chosen a certain instance
    of *t* for observation, the evolution of the process, starting with *t*, depends
    only on *t* and does not depend in any way on the previous instances. Thus, a
    process is Markovian when, given the moment of observation, only this instance
    determines the future evolution of the process, while this evolution does not
    depend on the past.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we want to build a statistical model to predict the weather.
    To simplify the model, we will assume that there are only two states: sunny and
    rainy. Let''s further assume that we have made some calculations and discovered
    that tomorrow''s time is somehow based on today''s time.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can perform weather forecasting with MDP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `MarkovChain.py` file that is already provided for you as a
    reference. To start, we import the `numpy`, `time`, and `matplotlib.pyplot` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set the seed of a random number generator and the state of the weather:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have to define the possible transitions of weather conditions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we insert the following check to verify that we did not make mistakes
    in defining the transition matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set the initial condition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now predict the weather conditions for each of the days set by the `NumberDays`
    variable. To do this, we will use a `while` loop, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It consists of a control condition and a loop body. At the entrance of the cycle
    and every time that all the instructions contained in the body are executed, the
    validity of the control condition is verified. The cycle ends when the condition,
    consisting of a Boolean expression, returns `false`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have generated forecasts for the next 200 days. Let''s plot
    the chart using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the weather conditions for the next 200 days, starting
    from the sunny condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08afee2a-205a-413c-b485-7eebe23692ea.png)'
  prefs: []
  type: TYPE_IMG
- en: At first sight, it seems that sunny days prevail over the rainy ones.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Markov chain is a mathematical model of a random phenomenon that evolves over
    time in such a way that the past influences the future only through the present.
    In other words, a stochastic model describes a sequence of possible events where
    the probability of each event depends only on the state that was attained in the
    previous event. So, Markov chains have the property of memorylessness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The structure of a Markov chain is therefore completely represented by the
    following transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d8d7be0-63a8-4818-ae7b-717417ce5420.png)'
  prefs: []
  type: TYPE_IMG
- en: The properties of transition probability matrices derive directly from the nature
    of the elements that compose them.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very intuitive alternative to the description of a Markov chain through a
    transition matrix is associating an oriented graph (transition diagram) to a Markov
    chain. The transition matrix and transition diagram provide the same information
    regarding the same Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt Publishing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*INTRODUCTION TO MARKOV MODELS* (from Clemson University): [http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf](http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Markov Decision Processes* (from Carnegie Mellon University): [http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf](http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing a financial portfolio using DP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The management of financial portfolios is an activity that aims to combine financial
    products in a manner that best represents the investor's needs. This requires
    an overall assessment of various characteristics, such as risk appetite, expected
    returns, and investor consumption, as well as an estimate of future returns and
    risk. **Dynamic programming** (**DP**) represents a set of algorithms that can
    be used to calculate an optimal policy given a perfect model of the environment
    in the form of an MDP. The fundamental idea of DP, as well as reinforcement learning
    in general, is the use of state values and actions to look for good policies.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we will address the **knapsack problem**: a thief goes into
    a house and wants to steal valuables. They put them in their knapsack, but they
    are limited by the weight. Each object has its own value and weight. They must
    choose the objects that are of value, but that do not have excessive weight. The
    thief must not exceed the weight limit in the knapsack, but, at the same time,
    they must optimize their gain.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can optimize a financial portfolio using DP:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `KPDP.py` file that is already provided for you as a reference.
    This algorithm starts with the definition of a `KnapSackTable()` function that
    will choose the optimal combination of the objects respecting the two constraints
    imposed by the problem: the total weight of the objects equal to 10, and the maximum
    value of the chosen objects, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set an iterative loop on all objects and on all weight values, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can memorize the result that was obtained, which represents the maximum
    value of the objects that can be carried in the knapsack, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The procedure we''ve followed so far does not indicate which subset provides
    the optimal solution. We must extract this information using a set procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If the current element is the same as the previous one, we will move on to
    the next one, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If it is not the same, then the current object will be included in the knapsack,
    and this item will be printed, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the total included weight is printed, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we have defined the function that allows us to build the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to define the input variables and pass them to the function, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we need to extract the weight and variable values from the objects.
    We put them in a separate array to better understand the steps, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the total weight that can be carried by the knapsack and the number
    of available items is set, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print out the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The DP algorithm allowed us to obtain the optimal solution, saving on computational
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider, for example, the problem of finding the best path that joins two locations.
    The principle of optimality states that each sub path included in it, between
    any intermediate location and the final location, must in turn be optimal. Based
    on this principle, DP solves a problem by taking one decision at a time. At every
    step, the best policy for the future is determined, regardless of the past choices
    (it is a Markov process), assuming that the latter choices are also optimal.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DP is a technique for solving recursive problems more efficiently. Why is this
    the case? Oftentimes, in recursive procedures, we solve sub problems repeatedly.
    In DP, this does not happen: we memorize the solution of these sub problems so
    that we do not have to solve them again. This is called **memoization**. If the
    value of a variable at a given step depends on the results of previous calculations,
    and if the same calculations are repeated over and over, then it is convenient
    to store the intermediate results so as to avoid repeating computationally expensive
    calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Dynamic Programming* (from Stanford University): [https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf](https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *The Knapsack Problem* (from Eindhoven University): [http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf](http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Memoization* (from Radford University): [https://www.radford.edu/~nokie/classes/360/dp-memoized.html](https://www.radford.edu/~nokie/classes/360/dp-memoized.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the shortest path
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a weighted graph and a designated vertex *X*, we will often need to find
    the path from *X* to each of the other vertices in the graph. Identifying a path
    connecting two or more nodes of a graph appears as a sub problem of many other
    problems of discrete optimization and has, in addition, numerous applications
    in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will find the shortest path between two points using the
    **Dijkstra** algorithm. We will also use the `networkx` package to represent graphs
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can find the shortest path:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `DijkstraNX.py` file that is already provided for you as a
    reference. First, we import the libraries we will use here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a graph object is created and the vertices are added:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Subsequently, the weighted edges are added:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have drawn the graph by adding labels to the edges with the
    indication of weight:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To do this, the `draw_networkx_edge_labels` function was used. The following
    diagram shows the results of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afea934e-5e66-439a-b69e-b2c5676c5619.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the shortest path from one to four nodes has been calculated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `shortest_path` function computes the shortest paths and path lengths between
    nodes in the graph. The following are the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the length of the shortest paths has been calculated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As we can verify, we have obtained the same result.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Dijkstra algorithm is able to solve the problem of finding the shortest
    path from the source, *s,* to all of the nodes. The algorithm maintains a label
    *d(i)* to the nodes representing an upper bound on the length of the shortest
    path of the node i.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each step, the algorithm partitions the nodes in *V* into two sets: the
    set of permanently labeled nodes and the set of nodes that are still temporarily
    labeled. The distance of permanently labeled nodes represents the shortest path
    distance from the source to these nodes, whereas the temporary labels contain
    a value that can be greater than or equal to the shortest path length.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic idea of the algorithm is to start from the source and try to permanently
    label the successor nodes. At the beginning, the algorithm places the value of
    the source distance to zero and initializes the other distances to an arbitrarily
    high value (by convention, we will set the initial value of the distances: *d[i]
    = + ∞, ∀i ∈* *V*).'
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration, the node label i is the value of the minimum distance along
    a path from the source that contains, apart from i, only permanently labeled nodes.
    The algorithm selects the node whose label has the lowest value among those labeled
    temporarily, labels it permanently, and updates all the labels of the nodes adjacent
    to it. The algorithm terminates when all the nodes have been permanently labeled.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out *Solving Shortest Path Problem: Dijkstra''s Algorithm* (from Illinois
    University): [http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf](http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out *Graph Theory Tutorials* (from the University of Tennessee at Martin):
    [https://primes.utm.edu/graph/index.html](https://primes.utm.edu/graph/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding the discount factor using Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Q-learning** is one of the most used reinforcement learning algorithms. This
    is due to its ability to compare the expected utility of the available actions
    without requiring an environment model. Thanks to this technique, it is possible
    to find an optimal action for every given state in a finished MDP.'
  prefs: []
  type: TYPE_NORMAL
- en: A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the Q function (the
    action-value function), which represents the maximum discounted future reward
    when we perform actions, *a*, in the state, *s*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will deal with the problem of controlling a character's movement
    in a grid world by offering a first solution based on Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can decide on the discount factor using Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `FrozenQlearning.py` file that is already provided for you
    as reference. Let''s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will move on and create the environment by calling the `make` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This method creates the environment that our agent will run in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s initialize the parameters, starting with `QTable`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define some parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Here, `alpha` is the learning rate, `gamma` is the discount factor, and `NumEpisodes`
    is the number of episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create a list to contain the total rewards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, after setting the parameters, it is possible to start the Q-learning
    cycle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: At the end of each episode, the list of rewards is enriched with a new value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the final Q-Table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21b062e8-2b5a-4052-8cf2-9d02539736a6.png)'
  prefs: []
  type: TYPE_IMG
- en: To improve the result, the retuning of the configuration parameters is required.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `FrozenLake` environment is a 4 × 4 grid that contains four possible areas:
    **Safe** (**S**), **Frozen** (**F**), **Hole** (**H**), and **Goal** (**G**).
    The agent controls the movement of a character in a grid world, and moves around
    the grid until it reaches the goal or the hole. Some tiles of the grid are walkable,
    and others lead to the agent falling into the water. If it falls into the hole,
    it has to start from the beginning and is rewarded with the value 0\. Additionally,
    the direction in which the agent will move is uncertain and only partially depends
    on the chosen direction. If the agent finds a walkable path to a goal tile, it
    is rewarded. The agent has four possible moves: up, down, left, and right. The
    process continues until it learns from every mistake and reaches the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Q-learning estimates the function value *q (s, a)* incrementally, updating
    the value of the state-action pair at each step of the environment, following
    the logic of updating the general formula for estimating the values for the temporal
    difference methods. Q-learning has off-policy characteristics; that is, while
    the policy is improved according to the values estimated by *q (s, a)*, the value
    function updates the estimates following a strictly greedy secondary policy: given
    a state, the chosen action is always the one that maximizes the *max q (s, a)*
    value. However, the π policy has an important role in estimating values, because
    through it the state-action pairs to be visited and updated are determined.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Reinforcement Learning: A Tutorial* (from University of Toronto):
    [http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf](http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out the official site of the `gym` library: [https://gym.openai.com/](https://gym.openai.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out the *FrozenLake-v0* environment: [https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the deep Q-learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep Q-learning** represents an evolution of the basic Q-learning method.
    The state-action is replaced by a neural network, with the aim of approximating
    the optimal value function. Compared to the Q-learning approaches, where it was
    used to structure the network in order to request both input and action and providing
    its expected return, deep Q-learning revolutionizes the structure to request only
    the state of the environment and supply as many status-action values as there
    are actions that can be performed in the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the deep Q-learning approaches to controls a character's
    movement in a grid world. In this recipe, the `keras-rl` library will be used;
    to learn about it further, refer to the *Developing AI-based dynamic modeling
    system* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can implement a deep Q-learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `FrozenDeepQLearning.py` file that is already provided for
    you as a reference. Let''s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will define the environment and set the seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract the actions that are available to the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `Actions` variable now contains all the actions that are available in the
    selected environment. Gym will not always tell you the meaning of those actions,
    but only about which ones are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will build a simple neural network model using the `keras` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, the neural network model is ready to use, so let's configure and compile
    our agent. One problem with using the DQN is that the neural network that was
    used in the algorithm tends to forget previous experiences because it overwrites
    them with new experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we need a list of previous experiences and observations to reform the model
    with previous experiences. For this reason, a memory variable is defined that
    will contain the previous experiences, and a policy will be set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to define the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s proceed to compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will evaluate our algorithm for 20 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Our agent is now able to identify the path that allows them to reach the goal.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the `Q` function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions, *a*, in the state, *s*. DQN represents an evolution of the basic Q-learning
    method, where the state-action is replaced by a neural network, with the aim of
    approximating the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym is a library that helps us implement algorithms based on reinforcement
    learning. It includes a growing collection of benchmark issues that expose a common
    interface, and a website where people can share their results and compare algorithm
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym focuses on the episodic setting of reinforced learning. In other
    words, the agent's experience is divided into a series of episodes. The initial
    state of the agent is randomly sampled by a distribution, and the interaction
    proceeds until the environment reaches a terminal state. This procedure is repeated
    for each episode, with the aim of maximizing the total reward expectation per
    episode and achieving a high level of performance in the fewest possible episodes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Learning 2048 with Deep Reinforcement Learning* (from the University
    of Waterloo): [https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf](https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Deep RL with Q-Functions* (from UC Berkeley): [http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing an AI-based dynamic modeling system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **Segway** is a personal transport device that exploits an innovative combination
    of computer science, electronics, and mechanics. It functions as an extension
    of the body; as with a partner in a dance, it is able to anticipate every move.
    The operating principle is based on the **reverse pendulum** system. The reverse
    pendulum system is an example that's commonly found in textbooks on control and
    research literature. Its popularity derives in part from the fact that it is unstable
    without control and has a non-linear dynamic, but, above all, because it has several
    practical applications, such as controlling a rocket's take-off or a Segway.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will analyze the functioning of a physical system that's
    made by connecting a rigid rod to a cart, modeling the system using different
    approaches. The rod is connected through a pivot that's hinged on the carriage
    and is free to rotate around it. This mechanical system, which is called the reverse
    pendulum, is a classic problem in control theory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can develop an AI-based dynamic modeling system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `dqn_cartpole.py` file that is already provided for you as
    a reference. Let''s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define and load the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To set the `seed` value, the NumPy library''s `random.seed()` function is used,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract the actions that are available to the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build a simple neural network model using the `keras` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'A `memory` variable and a `policy` will be set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to define the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move on to compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Saving the weight of a network or an entire structure takes place in an `HDF5`
    file, an efficient and flexible storage system that supports complex multidimensional
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will evaluate our algorithm for 10 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used the `keras–rl` package to develop an AI-based dynamic
    modeling system. This package implements some deep reinforcement learning algorithms
    in Python, and integrates seamlessly with Keras' in-depth learning library.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, `keras-rl` works immediately with OpenAI Gym. OpenAI Gym includes
    a growing collection of benchmark issues that shows a common interface and a website
    where people can share their results and compare algorithm performance. This library
    will be adequately addressed in the next chapter—for now, we will limit ourselves
    to using it.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These choices do not limit the use of the `keras-rl` package, in the sense that
    the uses of `keras-rl` can be easily adapted to our needs. You can use the built-in
    Keras callbacks and metrics, or define others. For this reason, it is easy to
    implement your own environments, and even algorithms, simply by extending some
    simple abstract classes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out *Tutorial: Deep Reinforcement Learning* (from Google DeepMind): [https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf](https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Deep Reinforcement Learning* (by Xu Wang): [https://pure.tue.nl/ws/files/46933213/844320-1.pdf](https://pure.tue.nl/ws/files/46933213/844320-1.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep reinforcement learning with double Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Q-learning algorithm, the future maximum approximated action value is
    evaluated using the same Q function as the current stock selection policy. In
    some cases, this can overestimate the action values, slowing down learning. A
    variation called **Double Q-learning** was proposed by DeepMind researchers in
    the following paper: *Deep reinforcement learning with Double Q-learning*, H van
    Hasselt, A Guez, and D Silver, March, 2016, at the Thirtieth AAAI Conference on
    Artificial Intelligence. As a solution to this problem, the authors proposed to
    modify the Bellman update.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will control an inverted pendulum system using the Double
    Q-learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can perform deep reinforcement learning with Double Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `DoubleDQNCartpole.py` file that is already provided for you
    as a reference. Let''s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define and load the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'To set the `seed` value, the NumPy library''s `random.seed()` function is used,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract the actions that are available to the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build a simple neural network model using the `keras` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'A `memory` variable and a `policy` will be set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to define the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: To enable the double network, we have to set the `enable_double_dqn` option
    to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Saving the weight of a network or an entire structure takes place in an `HDF5`
    file, an efficient and flexible storage system that supports complex multidimensional
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will evaluate our algorithm for 10 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The overestimation of the action value is due to the maximum operator that is
    used in the Bellman equation. The max operator uses the same value for both selecting
    and evaluating an action. Now, if we select the best action as the one that has
    the maximum value, we will end up selecting a sub-optimal action (which assumes
    the maximum value by mistake) instead of the optimal action. We can solve this
    problem by having two separate Q functions, each of which learns independently.
    A Q1 function is used to select an action, and the other Q2 function is used to
    evaluate an action. To do this, simply change the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Essentially, the following two networks are used:'
  prefs: []
  type: TYPE_NORMAL
- en: The DQN network to select what is the best action to take for the next state
    (the action with the highest Q value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target network, to calculate the target Q value of taking that action at
    the next state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Deep Reinforcement Learning with Double Q-learning*: [https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-network algorithm with dueling Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To improve convergence speed by making our network''s architecture closer represent
    one of the last challenges of reinforcement learning, a definite improvement in
    the performance of a DQN model has been proposed by Wang and others in the following
    paper: *Dueling network architectures for deep reinforcement learning, Z *Wang,
    T Schaul, M Hessel, H van Hasselt, M Lanctot, and N de Freitas, 2015, arXiv preprint
    arXiv:1511.06581.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will control an inverted pendulum system using the dueling
    Q-learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can perform deep Q-network algorithm with dueling Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `DuelingDQNCartpole.py` file that is already provided for you
    as a reference. Let''s start by importing the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define and load the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'To set the `seed` value, the NumPy library''s `random.seed()` function is used,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract the actions, available to the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build a simple neural network model using the Keras library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'A `memory` variable and a `policy` will be set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We just have to define the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: To enable the dueling network, we have to specify the `dueling_type` to one
    of the following:`'avg'`, `'max'`, or `'naive'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on to compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the training, it is necessary to save the obtained weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Saving the weight of a network or an entire structure takes place in an `HDF5`
    file, an efficient and flexible storage system that supports complex multidimensional
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will evaluate our algorithm for 10 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In reinforcement learning, the function Q and the value function play a fundamental
    role:'
  prefs: []
  type: TYPE_NORMAL
- en: The Q function specifies how good an agent is to perform an action in the *s*
    state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value function specifies how good it is for an agent to be in a state, *s*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To introduce a further improvement in the performance of a DQN, we introduce
    a new function called an `advantage` function, which can be defined as the difference
    between the `value` function and the `benefit` function. The `benefit` function
    specifies how good an agent is at performing an action compared to other actions.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the `value` function specifies the goodness of a state and the `advantage`
    function specifies the goodness of an action. Then, the combination of these two
    functions tells us how good it is for an agent to perform an action in a state
    that is actually our `Q` function. So, we can define our `Q` function as the sum
    of a `value` function and an `advantage` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dueling DQN is essentially a DQN, in which the fully connected final layer
    is divided into two streams:'
  prefs: []
  type: TYPE_NORMAL
- en: One calculates the `value` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other calculates the `advantage` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the two streams are combined using the aggregate level for obtaining
    the `Q` function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approximation of the `value` function via the neural network is anything
    but stable. To achieve convergence, the basic algorithm should be modified by
    introducing techniques to avoid oscillations and divergences.
  prefs: []
  type: TYPE_NORMAL
- en: The most important technique is called `experience replay`. During the episodes,
    at each step, the agent's experience is stored in a dataset, called `replay memory`.
    In the internal cycle of the algorithm, instead of performing the training on
    the network based on the only transition just performed, a subset of transitions
    is selected randomly from the replay memory, and the training takes place according
    to the loss calculated on the subset of transitions.
  prefs: []
  type: TYPE_NORMAL
- en: The experience of the `replay` technique, that is, randomly selecting transitions
    from `replay memory`, eliminates the problem of correlation between consecutive
    transitions and reduces variance among different updates.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out *Keras Reinforcement Learning Projects*, Giuseppe Ciaburro, Packt
    Publishing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Dueling Network Architectures for Deep Reinforcement Learning* for
    more information: [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
