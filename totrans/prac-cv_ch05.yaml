- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed the importance and applications of features.
    We now understand that the better the features are, the more accurate the results
    are going to be. In recent periods, the features have become more precise and
    as such better accuracy has been achieved. This is due to a new kind of feature
    extractor called **Convolutional Neural Networks** (**CNNs**) and they have shown
    remarkable accuracy in complex tasks, such as object detection in challenging
    domains, and classifying images with high accuracy, and are now quite ubiquitous
    in applications ranging from smartphone photo enhancements to satellite image
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will begin with an introduction to neural nets and continue
    into an explanation of CNNs and how to implement them. After this chapter, you
    will be able to write your own CNN from scratch for applications like image classification.
    The chapter includes:'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and libraries used in the various sections of the chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to neural networks with an explanation on simple neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN explanation and various components involved in it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of creating CNN for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description of transfer learning and statistics on various deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets and libraries used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using Keras to write neural nets with TensorFlow
    as backend. A detailed installation procedure is explained in [Chapter 2](prac-cv_ch02.html),
    *Libraries, Development Platforms and Datasets*. To check if you have Keras installed,
    in shell run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will print the Keras version as well as which backend you are using. If
    you have TensorFlow installed and Keras is using TensorFlow, it will print `using
    Tensorflow backend`. If you have an older version of Keras and TensorFlow, there
    might be some issues, so please install or upgrade to the latest versions. We
    will also be using other libraries like `NumPy` and `OpenCV`.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the `Fashion–MNIST` dataset by Zalando SE which is available
    at [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).
    This can be downloaded directly with Keras and there is no requirement for a separate
    download. `Fashion-MNIST` is MIT License (MIT) Copyright © [2017] Zalando SE.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks have been here for quite some time, with initial papers arriving
    more than a few decades ago. The recent popularity is due to the availability of
    better software for algorithms and proper hardware to run them. Initially, neural
    networks were motivated by how humans perceive the world and were modeled according
    to biological neuron functions. This was continuously modified over the course
    of time and has since been evolving to get a better performance.
  prefs: []
  type: TYPE_NORMAL
- en: A simple neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A simple neural net consists of a node which takes in an input or a list of
    inputs and performs a transformation. An example is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e84b36da-bb46-4778-a759-b4597fc8ad3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Mathematically, it takes the inputs *x* and applies a transformation *W* to
    get the output *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1375032a-e775-4d91-8855-4957b474767a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The input *x* can be a vector or multi-dimensional array. Based on the transformation
    matrix *W* we get the output *y* to be a vector or multi-dimensional array. This
    structure is further modified by also including the non-linear transformation
    *F*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa279536-9ab4-46aa-89b1-db651e0d0e18.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, output *y* is not linearly dependent on input *x*, and as a result, the
    change in *x* does not proportionally change *y*. More often, these non-linear
    transformations consist of clipping all negative values after applying the transformation
    matrix *W* to the inputs *x*. A neuron consists of this complete operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'These networks are stacked in layered structures, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f851f33-3a9c-4dfd-8e6a-6fde0f3f2d53.png)'
  prefs: []
  type: TYPE_IMG
- en: These networks are also termed **feedforward networks**, as there are no loops
    and input flows through the network in one direction, like in a **Directed Acyclic
    Graph** (**DAG**). In these networks, the parameters are termed as **weights**,
    which perform a transformation on the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Using machine learning approaches to learn these weights, we can get an optimal
    network that performs the desired operation with good accuracy. For this, the
    requirement is to have a dataset of labeled inputs; for example, for a given input
    *x* we already know the output value *y*. During the learning of the weights of
    neural networks, also termed as **training**, for this dataset the input is passed
    through the network layer by layer. At each layer, the input from the previous
    layer is transformed according to the layer's properties. The final output is
    our prediction for *y* and we can measure how far our prediction of *y* is from
    the actual value. Once we have this measure, termed as a **loss**, we can then
    use it to update the weights using a derivative-based approach called **gradient
    descent**. Each weight is updated according to the change in the loss with respect
    to weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will describe a simple example of a neural network using `NumPy` library. In
    this example, we consider the input `x` a vector of size `1000` and we want to
    compute an output of size `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a neural network that takes in this input *x* and applies a non-linear
    transformation with the weight matrix *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1d7976b-6804-4da5-a6f0-6e94abf54729.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To learn these weights `w`, we will follow the gradient descent method. For
    each input we will compute the gradient of loss with respect to `w` and update
    the weights as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This step is iterated repeatedly over our labeled dataset until our loss does
    not change significantly or the loss values start following some repetition. The
    `loss` function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, we can see the values of `loss` decreasing and
    settling down. The parameters here are learning rate and initial `w` values. A
    good choice for these values may cause the loss to decrease faster and settle
    early; however, a bad choice will lead to no decrease in loss, or sometimes an
    increase in loss over several iterations.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how to build a simple neural network. You can
    use this code and modify or add complex structures to play with it. Before we
    go further, to the explanation for CNNs, in the next section we will briefly revisit
    the convolution operation, which was explained in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the convolution operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Extending our discussion on filters from [Chapter 3](prac-cv_ch03.html), *Image
    Filtering and Transformations in OpenCV*, the convolution operation is taking
    a dot product of a shifted kernel matrix with a given input image. This process
    is explained in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5464aeef-7bde-4eeb-ad80-1cbf646c553c.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the previous figure, a kernel is a small two-dimensional array that
    computes dot product with the input image (on the left) to create a block of the
    output image (on the right).
  prefs: []
  type: TYPE_NORMAL
- en: 'In convolution, the output image is generated by taking a dot product between
    an **Input** image and a **Kernel** matrix. This is then shifted along the image
    and after each shift, corresponding values of the output are generated using a
    dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a28c429-8d56-4c7d-9e9e-fa7282c399d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we saw in the previous chapter, we can perform a convolution operation using
    OpenCV as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we assume a kernel with equal values whose sum is 1 and is used to perform
    convolution on a grayscale image. In [Chapter 3](prac-cv_ch03.html), *Image Filtering
    and Transformations in OpenCV*, this was termed as **smoothing operation** because
    if we have a noisy grayscale image, the output will look smoother.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, to perform a smoothing operation we already know the kernel values.
    If we know the kernels for extracting more complex features, we can do better
    inference from images. However, manually setting the values is unfeasible when
    we have to perform tasks like image classification and object detection. In such
    cases, models such as CNNs extract good features and perform better than other
    previous methods. In the next section, we will define a structure that will learn
    these kernel matrix values and compute richer features for a wide variety of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolutional Neural Networks, also known as **ConvNets**, use this convolution
    property in a neural network to compute better features, which can then be used
    to classify images or detect objects. As shown in the previous section, convolution
    consists of kernels which compute an output by sliding and taking a dot product
    with the input image. In a simple neural network, the neurons of a layer are connected
    to all the neurons of the next layer, but CNNs consist of convolution layers which
    have the property of the receptive field. Only a small portion of a previous layer''s
    neurons are connected to the neurons of the current layer. As a result, small
    region features are computed through every layer as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e160bc89-12ef-48f4-9c38-cb960631651f.png)'
  prefs: []
  type: TYPE_IMG
- en: As we have seen in a simple neural network, the neuron takes an input from one
    or more of previous neurons' output to perform a non-linear transformation. In
    CNNs, this is further combined with the convolution approach. We assume a set
    of kernels with varied values called **weights**. Each of these kernels is convolved
    with an input to create a response matrix. There is then a non-linear transformation
    called **activation** for each of the values in the convolved output. The output
    from each of the kernels after activation is stacked to create the output of our
    operation such that for *K* kernels the output is of *K x H[o] x W[o]* size, with
    *H[o]* and *W[o]* as the height and width of our output. This makes one layer
    of CNN.
  prefs: []
  type: TYPE_NORMAL
- en: The output from the previous layer is then used again as the input for the next
    layer with another set of kernels *K[2]*, and a new response is computed by first
    convolving each of the kernels and then taking the non-linear transformation of
    the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, CNNs are composed of the following types of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these layers will be explained in the following sections. In recent
    developments, some more components have been added to CNNs, but the preceding
    components still remain important.
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key component of a CNN is the convolution layer, which performs a dot product
    of a kernel matrix with part of an image and generates an output. This is followed
    by shifting and repeating the same operation over a complete image and is termed **convolution**.
    The region of the input which is taken for the dot product is called the **receptive
    field** of the convolution layer. In each convolution layer, there is a set of
    kernels and they are collectively termed** filters**.
  prefs: []
  type: TYPE_NORMAL
- en: The input for a convolution layer is an n-dimensional array, meaning the input
    is an image of the form *Width x Height x Depth*. For example, if we have a grayscale
    image of the size 32 x 32, width and height, then the input is 32 x 32 x 1 where
    depth is the number of color channels in this case, and is represented by the
    third dimension. Similarly, for a colored image of size 512, the input is 512
    x 512 x 3\. All the kernels in filters also have the same depth as the input.
  prefs: []
  type: TYPE_NORMAL
- en: The parameters for the layer are the number of filters, filter size, strides,
    and padding value. Of these, *filter* values are the only learnable parameters.
    *Strides* refer to the amount of shift in pixels for a kernel. With a stride of
    1, the kernel is moved left by 1 pixel and the dot product is taken with the corresponding
    input region. With a stride of 2, the kernel is moved by 2 pixels and the same
    operation takes place. On each input, at the boundary, the kernel can only overlap
    a certain region inside the image. Boundaries are therefore padded with zeros
    for the kernel to capture the complete image region. The padding value sets the
    way to pad the image boundary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The size of the output depends on these parameter values. We can use Keras
    to write CNN and perform operations on images. An example of writing one convolution
    layer is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create an example model to see the properties of convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'When you execute codes, please ignore the warnings shown, such as: Your CPU
    supports instructions that this TensorFlow binary was not compiled to use: SSE4.1
    SSE4.2 AVX AVX2 FMA.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On executing this code, we can see our model and output after each layer as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have set the input to be of the shape 512 x 512 x 3, and for convolution,
    we use 32 filters with the size 5 x 5\. The stride values we have set to 1, and
    using the same padding for the edges, we make sure a kernel captures all of the
    images. We will not use bias for this example. The output after convolution is
    of the shape (None, 512, 512, 32) of the shape (samples, width, height, filters).
    For the discussion, we will ignore the sample's value. The width and height of
    the output is 512 with the depth as 32\. The number of filters we used to set
    the output's depth value. The total number of parameters for this layer is 5 x
    5 x 3 x 32 (kernel_size * number of filters) which is 2400.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try another run. Now, we set strides to 2 in the previous code. On execution,
    we get the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the convolution output shape (width, height) is reduced to half
    of the input size. This is due to the stride option that we have chosen. Using
    strides 2, it will skip one pixel, making the output half of the input. Let''s
    increase the stride to 4\. The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the output shape (width and height) is reduced to one-fourth
    of the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we set our strides to 1, and padding to valid, the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, even if we set strides to 1, the output shape (width and height) is reduced
    to `508`. This is due to the lack of a padding set, and the kernel cannot be applied
    to the edges of the input.
  prefs: []
  type: TYPE_NORMAL
- en: We can compute the output shape as ![](img/d47fc610-6adc-4264-b6b8-6b2e4c0dc9b0.png),
    where *I* is input size, *K* is kernel size, *P* is padding used and *S* is stride
    value. If we use the same padding, the *P* value is![](img/535f27e6-8f4e-439e-ad0d-5b6a3548f75b.png). Otherwise,
    if we use the valid padding then *P* value is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have set another parameter `use_bias=False`. On setting this as true,
    it will add a constant value to each kernel and for a convolution layer, the bias
    parameter is the same as the number of filters used. So, if we set `use_bias to
    True`, with strides 1 and the same padding, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The total number of parameters is increased by `32`, which is the number of
    filters used in this layer. We have seen how to design the convolution layer and
    what happens when we use different parameters for the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: The crucial thing is, what are the kernel values in the filters that will give
    the desired output? To have good performance, we would like to get the output
    to consist of high-quality features from the input. Setting values for the filters
    manually is not feasible as the number of filters grows quite large and the combinations
    of these values are practically infinite. We learn the filter values using optimization
    techniques which use a dataset of inputs and targets and tries to predict as close
    to the targets as possible. The optimization then updates the weights after each
    iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The activation layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we saw in the case of the simple neural network, the weighted output is
    passed through a non-linear transformation. This non-linear layer is often referred
    to as the activation layer. Some common types of activation are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sigmoid: ![](img/c032bfe3-45d3-42d2-9f86-a8c3dd69f4f4.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ReLU: ![](img/297ac62e-1aa0-4803-9caf-b31c9efd7c3e.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tanh: ![](img/ba5882e8-f64b-4322-bf56-3a05f3bcb3e9.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU:![](img/ae3fbb33-c7d3-4f9e-9a27-32fccc3fcbfd.png) where ![](img/f88cbe14-ce23-4efa-afe8-0f1a41ea67f2.png)is
    a small positive float
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax: ![](img/182b2e91-8e68-4000-a462-795997e94cd1.png) this is often used
    to represent probability for a class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The most common choice of activation function is **Rectified Linear Unit** (**ReLU**)
    and this performs well in the majority of cases. In our previous code we can add
    an activation layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of executing the code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see from our formulation of activation functions, it does not contain
    any trainable parameters. In Keras, the activation layer can also be added to
    the convolution layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pooling takes in a region of inputs and either a max or an average value of
    that region is produced as output. In effect, it reduces the size of an input
    by sampling in the local region. This layer is inserted between 2 to 3 convolution
    layers to reduce the resolution of the output, thereby reducing the requirement
    for parameters. A visual representation of the pooling operation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ccdb84b-1c02-406e-b2f9-a1bf17bb7fe8.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous figure, the input is a two-dimensional array of 2 x 2 size and
    after pooling operation the output is of size 1 x 1\. This can be generated by
    taking average of the values in the previous array or the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show how the output shape changes after this operation, we can use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, we used a convolution layer and added a pooling operation
    to it. When we execute it, the expected output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set the pooling parameter to `(2,2)`, representing the width and height
    of the pooling operation. The depth for pooling will be set according to the depth
    of the input to the pooling layer. The resulting output is of half the shape in
    terms of width and height; however, there is no change in depth size.
  prefs: []
  type: TYPE_NORMAL
- en: The fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a simple neural network layer where each neuron in the current layer
    is connected to all the neurons in the previous layer. This is often referred
    to as `Dense` or `Linear` in various deep learning libraries. In Keras, this can
    be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As we execute this code, we can see the output shapes, as well as the number
    of trainable parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The total parameters for this layer are given by ![](img/0835407b-ec13-4fac-97be-9671f468a0e5.png) where* I[s]* is
    input shape and *O[s]* is output shape. In our example, we used an input of shape
    `512` and an output of shape `32` and get a total of 16416 parameters with bias.
    This is quite large compared to a similar convolution layer block, therefore in
    recent models, there has been a trend towards using more convolution blocks rather
    than fully connected blocks. Nonetheless, this layer still plays a major role
    in designing simple convolution neural net blocks.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw what CNNs are and what their components are. However,
    we haven't seen a way to set parameter values. Additionally, we have not seen
    several other layer structures, such as Batch Normalization and Dropout. These
    other layers also play major roles in designing CNN models.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is applied to normalize the output from the input layer with mean 0 and
    variance 1 as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccb54ecc-c993-40e5-a529-c29c0e34177d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This layer also has learnable parameters (which are optional in most of the
    deep learning libraries) to squash output in a given range:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b89d853-6cac-4370-b86a-83af9f4c1efc.png)'
  prefs: []
  type: TYPE_IMG
- en: Here γ and β are learnable parameters. Batch Normalization improves training
    by faster convergence as well as acting as a kind of regularization. However,
    since there are learnable parameters, the effect of normalization is different
    in training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the important layers that helps prevent overfitting is Dropout. It randomly
    drops, with some probability, the neurons from the previous layer to be used as
    input to the next layer. This acts like we are training an ensemble of neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will see how to implement a model in Keras and
    perform the learning of parameters, which we skipped in this section.
  prefs: []
  type: TYPE_NORMAL
- en: CNN in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now start with our implementation of a convolutional neural net in Keras.
    For our example case, we will train a network to classify `Fashion-MNIST`. This
    is a dataset of grayscale images of fashion products, of the size 28 x 28\. The
    total number of images is 70,000, with 60,000 as training and 10,000 as a test.
    There are ten categories in this dataset, which are t-shirt, trousers, pullover,
    dress, coat, sandal, shirt, sneakers, bag, and ankle boots. Labels for each are
    marked with a category number from 0-9.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load this dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code block doesn''t output a visualization of the dataset, so
    following image is to show what dataset we will be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe0bbbb7-e19c-4bcf-a570-783f0b8959a8.png)'
  prefs: []
  type: TYPE_IMG
- en: It will split the data into the train and test sets with both inputs `x` as
    well as the label `y`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution layer is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The pooling layer is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The overall output layer is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code block,  created model can be seen as, where each
    row is a layer type arranged sequentially with input layer on the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Fashion-MNIST classifier training code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the section, we will see a classifier model on `Fashion-MNIST` dataset.
    This will take in input grayscale image and outputs one of the pre-defined 10
    classes. In the following steps, we will build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the relevant libraries and modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the input height and width parameters to be used throughout, as well
    as other parameters. Here, an epoch defines one iteration over all of the data.
    So, the number of `epochs` means the total number of iterations over all of the
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s download and prepare the dataset for training and validation. There
    is already an inbuilt function to do this in Keras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build the model using the wrapper convolution function defined earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up the `optimizer`, `loss` function, and `metrics` to evaluate our
    predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is optional if we would like to save our model after every epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s begin training our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The previous piece of code will run for a while if you are using the only CPU.
    After 10 epochs, it will say `val_acc= 0.92` (approximately). This means our trained
    model can perform with about 92% accuracy on unseen `Fashion-MNIST` data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once all epoch training finishes, final evaluation is computed as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Analysis of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Research on different kinds of CNNs is still ongoing, and year after year we
    see improvements in accuracy for models on complex datasets. These improvements
    are in terms of both model structure and how to train these models effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Popular CNN architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the recent few years, the following have become popular in various practical
    applications. In this section, we will see some of the popular architectures and
    how to load them in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: VGGNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was introduced in 2014 by Karen Simonyan and Andrew Zisserman, in the paper
    *Very Deep Convolution Networks for Large-Scale Image Recognition,* [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).
  prefs: []
  type: TYPE_NORMAL
- en: 'This was one of the initial papers that improved the performance of object
    classification models and was one of the top performing models in the **Imagenet
    Large Scale Visual Recognition Challenge** (**ILSVRC**) 2014, the dataset for
    this was introduced in [Chapter 2](prac-cv_ch02.html), *Libraries, Development
    Platform, and Datasets*. The performance gain was about 4% from the previous best
    model, and as a result, it became quite popular. There were several versions of
    the model but the most popular are VGG16 and VGG19\. We can see a pretrained VGG16
    model in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'On execution, we can see the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Since the total number of parameters is quite large, training such a model from
    scratch will also require a huge amount of data of the order of a few hundred
    thousand.
  prefs: []
  type: TYPE_NORMAL
- en: Inception models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These were successful in using parallel structures in the convolution network,
    which further increased the performance of models in the same competition. It
    was proposed and refined by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
    Jonathon Shlens, Zbigniew Wojna in the paper *Rethinking the Inception Architecture
    for Computer Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567).
    The model structure for inception-v3 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aeee76d5-2e68-41dc-9df7-335e88166a31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use this model in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: On execution, it will print out the model structure.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Extending more on parallel structure, Kaiming He, Xiangyu Zhang, Shaoqing Ren,
    Jian Sun. introduced *Deep Residual Learning for Image Recognition*  [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385) that
    uses skip connection. The basic block of ResNet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e122e62-bd2e-4b66-be3f-1a11c5028ec9.png)'
  prefs: []
  type: TYPE_IMG
- en: These blocks are repeated and stacked over to create a large network with a
    depth of 18 for 18 blocks, 50 for 50 blocks, and so on. They have shown remarkable
    performance both in terms of accuracy and computation time. In the following code,
    we will see how to use this to predict the top-5 probable categories from an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input to the model is the following image of a train locomotive engine,
    which is any normal smartphone photo. We want to see if the pretrained ResNet-50
    model can do close to ground truth predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31524fda-12fd-45ee-a27a-0e18c8547a6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s load the required imports as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We now begin creating a model to detect object in previously shown figure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we do is setup loading the ResNet-50 pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to preprocess the image for a specific input type for ResNet. In this
    case, the input is the mean, normalized to the size (1, 224, 224, 3):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and load the image and apply preprocessing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now load the model and pass the processed input through the trained
    model. This also computes the runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We have got predictions, but these are just probability values and not class
    names. We will now print class names corresponding to only the top-5 probable
    predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`n04310018` and `steam_locomotive` are the class index and names. The value
    denoted after that is the probability for prediction. So, the pretrained model
    is 89% probable that the input image is a steam locomotive. This is quite impressive
    since the input image is of a locomotive which is not in service and has probably
    never been seen by the model during training.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section we saw three different model types, but in deep learning
    models, we are not limited to these. Every year there are better performing model
    architectures being published. However, the performance of these models totally
    depends on training data and their performance is due to the millions of images
    they are trained on. Getting such large datasets and training them for task specific
    purposes is not cost effective, as well as being time consuming. Nonetheless,
    the models can be used in various domains by doing a special type of training
    called **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: In transfer learning, we fix a part of a model from the input to a given layer
    (also known as **freezing a model**), such that the pretrained weights will help
    in computing rich features from the image. The remaining part is trained on task
    specific datasets. As a result, the remaining part of the model learns better
    features even with small datasets. The choice of how much of a model to freeze
    depends on available datasets and repeated experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, we will show a comparison of previous models, to understand better
    which model to use. The first plot is the number of parameters in each of the
    models. As the newer models were released, they became more efficient in terms
    of number of parameters to train:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fe55971-191c-4f37-a853-645e24a8f535.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, we show the comparison of accuracy for the ILSVRC challenge across different
    years. This shows that the model gets better with less parameters and better model
    structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6786a26f-d30e-43ad-8906-7f530bb70d5f.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we saw that even lacking of large dataset for a particular
    task, we can still achieve good performance by transferring the learning from
    model trained on other similar dataset. In most practical applications, we use
    models trained on ImageNet dataset, however choice of model is, decided by the
    user, based on criteria such as more accuracy model or faster model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we had an introduction to CNNs and their basic components.
    We also saw how to train a model from scratch on an example dataset. Later, we
    learnt to use pretrained models to perform prediction and also transfer learning
    to re-utilize trained models for our tasks.
  prefs: []
  type: TYPE_NORMAL
- en: These trained models and CNNs are not only used for image classification but
    also on more complex tasks like object detection and segmentation, as we will
    see in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
