- en: Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In the previous chapter, we discussed the importance and applications of features.
    We now understand that the better the features are, the more accurate the results
    are going to be. In recent periods, the features have become more precise and
    as such better accuracy has been achieved. This is due to a new kind of feature
    extractor called **Convolutional Neural Networks** (**CNNs**) and they have shown
    remarkable accuracy in complex tasks, such as object detection in challenging
    domains, and classifying images with high accuracy, and are now quite ubiquitous
    in applications ranging from smartphone photo enhancements to satellite image
    analysis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了特征的重要性及其应用。我们现在明白，特征越好，结果越准确。在最近的一段时间里，特征变得更加精确，因此实现了更高的准确性。这得益于一种新的特征提取器，称为
    **卷积神经网络**（**CNNs**），它们在复杂任务中表现出显著的准确性，例如在具有挑战性的领域进行目标检测，以及以高精度对图像进行分类，现在在从智能手机照片增强到卫星图像分析的应用中相当普遍。
- en: 'In this chapter, we will begin with an introduction to neural nets and continue
    into an explanation of CNNs and how to implement them. After this chapter, you
    will be able to write your own CNN from scratch for applications like image classification.
    The chapter includes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从神经网络介绍开始，然后继续解释 CNNs 以及如何实现它们。在本章之后，你将能够从头开始编写自己的 CNN，用于图像分类等应用。本章包括：
- en: Datasets and libraries used in the various sections of the chapter
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章各节使用的数据集和库
- en: Introduction to neural networks with an explanation on simple neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单神经网络介绍
- en: CNN explanation and various components involved in it
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 解释及其涉及的各种组件
- en: An example of creating CNN for image classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于图像分类的 CNN 的示例
- en: Description of transfer learning and statistics on various deep learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移学习和各种深度学习模型的统计数据描述
- en: Datasets and libraries used
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的数据集和库
- en: 'In this chapter, we will be using Keras to write neural nets with TensorFlow
    as backend. A detailed installation procedure is explained in [Chapter 2](prac-cv_ch02.html),
    *Libraries, Development Platforms and Datasets*. To check if you have Keras installed,
    in shell run:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Keras 以 TensorFlow 作为后端来编写神经网络。详细的安装过程在 [第 2 章](prac-cv_ch02.html)，*库、开发平台和数据集*
    中有解释。要检查你是否已安装 Keras，请在 shell 中运行：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will print the Keras version as well as which backend you are using. If
    you have TensorFlow installed and Keras is using TensorFlow, it will print `using
    Tensorflow backend`. If you have an older version of Keras and TensorFlow, there
    might be some issues, so please install or upgrade to the latest versions. We
    will also be using other libraries like `NumPy` and `OpenCV`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出 Keras 版本以及你正在使用的后端。如果你已经安装了 TensorFlow 并且 Keras 正在使用 TensorFlow，它将打印 `using
    Tensorflow backend`。如果你安装了较旧的 Keras 和 TensorFlow 版本，可能会出现一些问题，因此请安装或升级到最新版本。我们还将使用其他库，如
    `NumPy` 和 `OpenCV`。
- en: We will be using the `Fashion–MNIST` dataset by Zalando SE which is available
    at [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).
    This can be downloaded directly with Keras and there is no requirement for a separate
    download. `Fashion-MNIST` is MIT License (MIT) Copyright © [2017] Zalando SE.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Zalando SE 的 `Fashion–MNIST` 数据集，该数据集可在 [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
    获取。这可以直接使用 Keras 下载，无需单独下载。`Fashion-MNIST` 是 MIT 许可证（MIT）版权所有 © [2017] Zalando
    SE。
- en: Introduction to neural networks
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络介绍
- en: Neural networks have been here for quite some time, with initial papers arriving
    more than a few decades ago. The recent popularity is due to the availability of
    better software for algorithms and proper hardware to run them. Initially, neural
    networks were motivated by how humans perceive the world and were modeled according
    to biological neuron functions. This was continuously modified over the course
    of time and has since been evolving to get a better performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已经存在一段时间了，最初的论文发表超过了几十年。最近的热度是由于更好的算法软件和适当的硬件来运行它们。最初，神经网络受到人类感知世界的方式的启发，并按照生物神经元功能进行建模。随着时间的推移，这不断被修改，并因此不断进化以获得更好的性能。
- en: A simple neural network
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单神经网络
- en: 'A simple neural net consists of a node which takes in an input or a list of
    inputs and performs a transformation. An example is shown in the following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的神经网络由一个节点组成，该节点接收输入或输入列表并执行转换。以下图示了一个示例：
- en: '![](img/e84b36da-bb46-4778-a759-b4597fc8ad3d.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e84b36da-bb46-4778-a759-b4597fc8ad3d.png)'
- en: 'Mathematically, it takes the inputs *x* and applies a transformation *W* to
    get the output *y*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它对输入 *x* 应用一个变换 *W* 来得到输出 *y*：
- en: '![](img/1375032a-e775-4d91-8855-4957b474767a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1375032a-e775-4d91-8855-4957b474767a.png)'
- en: 'The input *x* can be a vector or multi-dimensional array. Based on the transformation
    matrix *W* we get the output *y* to be a vector or multi-dimensional array. This
    structure is further modified by also including the non-linear transformation
    *F*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 *x* 可以是一个向量或多维数组。根据变换矩阵 *W*，我们得到输出 *y* 也是一个向量或多维数组。这种结构通过包括非线性变换 *F* 进一步修改：
- en: '![](img/fa279536-9ab4-46aa-89b1-db651e0d0e18.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fa279536-9ab4-46aa-89b1-db651e0d0e18.png)'
- en: Now, output *y* is not linearly dependent on input *x*, and as a result, the
    change in *x* does not proportionally change *y*. More often, these non-linear
    transformations consist of clipping all negative values after applying the transformation
    matrix *W* to the inputs *x*. A neuron consists of this complete operation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，输出 *y* 与输入 *x* 的线性依赖性不再，因此，*x* 的变化不会成比例地改变 *y*。更常见的是，这些非线性变换包括在将变换矩阵 *W*
    应用到输入 *x* 之后剪切所有负值。一个神经元由这个完整操作组成。
- en: 'These networks are stacked in layered structures, as shown in the following
    figure:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络以分层结构堆叠，如下图所示：
- en: '![](img/2f851f33-3a9c-4dfd-8e6a-6fde0f3f2d53.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f851f33-3a9c-4dfd-8e6a-6fde0f3f2d53.png)'
- en: These networks are also termed **feedforward networks**, as there are no loops
    and input flows through the network in one direction, like in a **Directed Acyclic
    Graph** (**DAG**). In these networks, the parameters are termed as **weights**,
    which perform a transformation on the inputs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络也被称为**前馈网络**，因为没有循环，输入以一个方向通过网络，就像在**有向无环图**（**DAG**）中一样。在这些网络中，参数被称为**权重**，它们对输入进行转换。
- en: Using machine learning approaches to learn these weights, we can get an optimal
    network that performs the desired operation with good accuracy. For this, the
    requirement is to have a dataset of labeled inputs; for example, for a given input
    *x* we already know the output value *y*. During the learning of the weights of
    neural networks, also termed as **training**, for this dataset the input is passed
    through the network layer by layer. At each layer, the input from the previous
    layer is transformed according to the layer's properties. The final output is
    our prediction for *y* and we can measure how far our prediction of *y* is from
    the actual value. Once we have this measure, termed as a **loss**, we can then
    use it to update the weights using a derivative-based approach called **gradient
    descent**. Each weight is updated according to the change in the loss with respect
    to weights.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习方法来学习这些权重，我们可以得到一个执行所需操作且精度良好的最优网络。为此，需要有一个标记输入的数据集；例如，对于一个给定的输入 *x*，我们已知输出值
    *y*。在神经网络权重的学习过程中，也称为**训练**，对于这个数据集，输入会逐层通过网络。在每一层，前一层输入根据该层的特性进行转换。最终输出是我们对 *y*
    的预测，我们可以测量我们的 *y* 预测与实际值之间的差距。一旦我们有了这个度量，称为**损失**，我们就可以使用基于导数的**梯度下降**方法来更新权重。每个权重根据损失相对于权重的变化进行更新。
- en: 'We will describe a simple example of a neural network using `NumPy` library. In
    this example, we consider the input `x` a vector of size `1000` and we want to
    compute an output of size `2`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `NumPy` 库描述一个简单的神经网络示例。在这个例子中，我们将输入 `x` 视为一个大小为 `1000` 的向量，并希望计算一个大小为
    `2` 的输出：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We create a neural network that takes in this input *x* and applies a non-linear
    transformation with the weight matrix *W*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个神经网络，它接受这个输入 *x* 并使用权重矩阵 *W* 应用非线性变换：
- en: '![](img/a1d7976b-6804-4da5-a6f0-6e94abf54729.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a1d7976b-6804-4da5-a6f0-6e94abf54729.png)'
- en: 'An example is shown as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To learn these weights `w`, we will follow the gradient descent method. For
    each input we will compute the gradient of loss with respect to `w` and update
    the weights as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习这些权重 `w`，我们将遵循梯度下降方法。对于每个输入，我们将计算损失相对于 `w` 的梯度，并按以下方式更新权重：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This step is iterated repeatedly over our labeled dataset until our loss does
    not change significantly or the loss values start following some repetition. The
    `loss` function is defined as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤会反复在我们的标记数据集上迭代，直到我们的损失没有显著变化或损失值开始遵循某种重复。`损失`函数定义为：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The overall code is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 整体代码如下：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: On running the previous code, we can see the values of `loss` decreasing and
    settling down. The parameters here are learning rate and initial `w` values. A
    good choice for these values may cause the loss to decrease faster and settle
    early; however, a bad choice will lead to no decrease in loss, or sometimes an
    increase in loss over several iterations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们可以看到`loss`的值逐渐降低并稳定下来。这里的参数是学习率和初始`w`值。这些值的良好选择可能会导致损失更快地减少并提前稳定；然而，不良的选择可能会导致损失没有减少，有时在多次迭代后损失会增加。
- en: In this section, we have seen how to build a simple neural network. You can
    use this code and modify or add complex structures to play with it. Before we
    go further, to the explanation for CNNs, in the next section we will briefly revisit
    the convolution operation, which was explained in the previous chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何构建一个简单的神经网络。你可以使用这段代码，对其进行修改或添加复杂结构来玩玩。在我们进一步深入之前，为了解释CNN，在下一节中我们将简要回顾卷积操作，这在上一章中已经解释过了。
- en: Revisiting the convolution operation
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾卷积操作
- en: 'Extending our discussion on filters from [Chapter 3](prac-cv_ch03.html), *Image
    Filtering and Transformations in OpenCV*, the convolution operation is taking
    a dot product of a shifted kernel matrix with a given input image. This process
    is explained in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](prac-cv_ch03.html) *OpenCV中的图像滤波和变换*中，我们讨论了滤波器之后，卷积操作是将一个平移的核矩阵与给定的输入图像进行点积。这个过程在下面的图中解释：
- en: '![](img/5464aeef-7bde-4eeb-ad80-1cbf646c553c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5464aeef-7bde-4eeb-ad80-1cbf646c553c.png)'
- en: As shown in the previous figure, a kernel is a small two-dimensional array that
    computes dot product with the input image (on the left) to create a block of the
    output image (on the right).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，核是一个小的二维数组，它与输入图像（在左侧）进行点积，以创建输出图像的块（在右侧）。
- en: 'In convolution, the output image is generated by taking a dot product between
    an **Input** image and a **Kernel** matrix. This is then shifted along the image
    and after each shift, corresponding values of the output are generated using a
    dot product:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积中，输出图像是通过将**输入**图像与**核**矩阵之间的点积生成的。然后它沿着图像进行平移，并在每次平移后，使用点积生成输出对应值：
- en: '![](img/8a28c429-8d56-4c7d-9e9e-fa7282c399d8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a28c429-8d56-4c7d-9e9e-fa7282c399d8.png)'
- en: 'As we saw in the previous chapter, we can perform a convolution operation using
    OpenCV as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章中看到的，我们可以使用以下方式使用OpenCV执行卷积操作：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we assume a kernel with equal values whose sum is 1 and is used to perform
    convolution on a grayscale image. In [Chapter 3](prac-cv_ch03.html), *Image Filtering
    and Transformations in OpenCV*, this was termed as **smoothing operation** because
    if we have a noisy grayscale image, the output will look smoother.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们假设一个具有相等值且总和为1的核，用于对灰度图像进行卷积。在[第3章](prac-cv_ch03.html) *OpenCV中的图像滤波和变换*中，这被称为**平滑操作**，因为如果我们有一个有噪声的灰度图像，输出看起来会更平滑。
- en: In this case, to perform a smoothing operation we already know the kernel values.
    If we know the kernels for extracting more complex features, we can do better
    inference from images. However, manually setting the values is unfeasible when
    we have to perform tasks like image classification and object detection. In such
    cases, models such as CNNs extract good features and perform better than other
    previous methods. In the next section, we will define a structure that will learn
    these kernel matrix values and compute richer features for a wide variety of applications.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，为了执行平滑操作，我们已经知道了核的值。如果我们知道用于提取更复杂特征的核，我们可以从图像中做出更好的推理。然而，当我们必须执行图像分类和目标检测等任务时，手动设置值是不切实际的。在这种情况下，如CNN之类的模型提取良好的特征，并且比以前的方法表现更好。在下一节中，我们将定义一个结构，该结构将学习这些核矩阵值，并为各种应用计算更丰富的特征。
- en: Convolutional Neural Networks
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'Convolutional Neural Networks, also known as **ConvNets**, use this convolution
    property in a neural network to compute better features, which can then be used
    to classify images or detect objects. As shown in the previous section, convolution
    consists of kernels which compute an output by sliding and taking a dot product
    with the input image. In a simple neural network, the neurons of a layer are connected
    to all the neurons of the next layer, but CNNs consist of convolution layers which
    have the property of the receptive field. Only a small portion of a previous layer''s
    neurons are connected to the neurons of the current layer. As a result, small
    region features are computed through every layer as shown in the following figure:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络，也称为**ConvNets**，在神经网络中使用这种卷积属性来计算更好的特征，这些特征可以用于分类图像或检测对象。如前所述，卷积由核组成，通过滑动和与输入图像进行点积运算来计算输出。在简单的神经网络中，一层的神经元连接到下一层的所有神经元，但
    CNN 由具有感受野属性的卷积层组成。只有前一层的少量神经元连接到当前层的神经元。因此，如图所示，通过每一层计算小的区域特征：
- en: '![](img/e160bc89-12ef-48f4-9c38-cb960631651f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e160bc89-12ef-48f4-9c38-cb960631651f.png)'
- en: As we have seen in a simple neural network, the neuron takes an input from one
    or more of previous neurons' output to perform a non-linear transformation. In
    CNNs, this is further combined with the convolution approach. We assume a set
    of kernels with varied values called **weights**. Each of these kernels is convolved
    with an input to create a response matrix. There is then a non-linear transformation
    called **activation** for each of the values in the convolved output. The output
    from each of the kernels after activation is stacked to create the output of our
    operation such that for *K* kernels the output is of *K x H[o] x W[o]* size, with
    *H[o]* and *W[o]* as the height and width of our output. This makes one layer
    of CNN.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在简单的神经网络中看到的，神经元从先前神经元的一个或多个输出中获取输入以执行非线性变换。在 CNN 中，这进一步结合了卷积方法。我们假设一组具有不同值的核，称为**权重**。每个这样的核都与输入进行卷积以创建响应矩阵。然后对卷积输出的每个值进行称为**激活**的非线性变换。激活后的每个核的输出被堆叠以创建我们操作的输出，这样对于
    *K* 个核，输出的大小为 *K x H[o] x W[o]*，其中 *H[o]* 和 *W[o]* 分别是输出的高度和宽度。这构成了 CNN 的一层。
- en: The output from the previous layer is then used again as the input for the next
    layer with another set of kernels *K[2]*, and a new response is computed by first
    convolving each of the kernels and then taking the non-linear transformation of
    the response.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前一层的输出随后再次用作下一层的输入，并使用另一组核 *K[2]*，通过首先卷积每个核然后对响应进行非线性变换来计算新的响应。
- en: 'In general, CNNs are composed of the following types of layers:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN 由以下类型的层组成：
- en: Convolutional layers
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Fully connected layers
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: Activation layers
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活层
- en: Pooling layers
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Each of these layers will be explained in the following sections. In recent
    developments, some more components have been added to CNNs, but the preceding
    components still remain important.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层将在以下章节中逐一解释。在最近的发展中，CNN 中增加了一些更多组件，但前面的组件仍然非常重要。
- en: The convolution layer
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: A key component of a CNN is the convolution layer, which performs a dot product
    of a kernel matrix with part of an image and generates an output. This is followed
    by shifting and repeating the same operation over a complete image and is termed **convolution**.
    The region of the input which is taken for the dot product is called the **receptive
    field** of the convolution layer. In each convolution layer, there is a set of
    kernels and they are collectively termed** filters**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的一个关键组件是卷积层，它通过将核矩阵与图像的一部分进行点积运算并生成输出。随后是对整个图像进行平移和重复相同的操作，这被称为**卷积**。用于点积运算的输入区域被称为卷积层的**感受野**。在每一层卷积中，都有一组核，它们共同被称为**过滤器**。
- en: The input for a convolution layer is an n-dimensional array, meaning the input
    is an image of the form *Width x Height x Depth*. For example, if we have a grayscale
    image of the size 32 x 32, width and height, then the input is 32 x 32 x 1 where
    depth is the number of color channels in this case, and is represented by the
    third dimension. Similarly, for a colored image of size 512, the input is 512
    x 512 x 3\. All the kernels in filters also have the same depth as the input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输入是一个n维数组，这意味着输入是一个形式为*宽度 x 高度 x 深度*的图像。例如，如果我们有一个32 x 32大小的灰度图像，宽度和高度，那么输入是32
    x 32 x 1，其中深度是这种情况下的颜色通道数，由第三维表示。同样，对于512 x 512大小的彩色图像，输入是512 x 512 x 3。过滤器中的所有内核也具有与输入相同的深度。
- en: The parameters for the layer are the number of filters, filter size, strides,
    and padding value. Of these, *filter* values are the only learnable parameters.
    *Strides* refer to the amount of shift in pixels for a kernel. With a stride of
    1, the kernel is moved left by 1 pixel and the dot product is taken with the corresponding
    input region. With a stride of 2, the kernel is moved by 2 pixels and the same
    operation takes place. On each input, at the boundary, the kernel can only overlap
    a certain region inside the image. Boundaries are therefore padded with zeros
    for the kernel to capture the complete image region. The padding value sets the
    way to pad the image boundary.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 层的参数包括滤波器的数量、滤波器大小、步长和填充值。在这些参数中，*滤波器*值是唯一可学习的参数。*步长*指的是内核在像素中的移动量。步长为1时，内核向左移动1个像素，并与相应的输入区域进行点积。步长为2时，内核移动2个像素，并执行相同的操作。在每个输入的边界上，内核只能与图像内部的一定区域重叠。因此，边界用零填充，以便内核捕获完整的图像区域。填充值设置了填充图像边界的方式。
- en: 'The size of the output depends on these parameter values. We can use Keras
    to write CNN and perform operations on images. An example of writing one convolution
    layer is:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的大小取决于这些参数值。我们可以使用Keras编写CNN并在图像上执行操作。编写一个卷积层的例子如下：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s create an example model to see the properties of convolution layer:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个示例模型来查看卷积层的特性：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When you execute codes, please ignore the warnings shown, such as: Your CPU
    supports instructions that this TensorFlow binary was not compiled to use: SSE4.1
    SSE4.2 AVX AVX2 FMA.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行代码时，请忽略显示的警告，例如：您的CPU支持此TensorFlow二进制未编译以使用的指令：SSE4.1 SSE4.2 AVX AVX2 FMA。
- en: 'On executing this code, we can see our model and output after each layer as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们可以看到每个层后的我们的模型和输出：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we have set the input to be of the shape 512 x 512 x 3, and for convolution,
    we use 32 filters with the size 5 x 5\. The stride values we have set to 1, and
    using the same padding for the edges, we make sure a kernel captures all of the
    images. We will not use bias for this example. The output after convolution is
    of the shape (None, 512, 512, 32) of the shape (samples, width, height, filters).
    For the discussion, we will ignore the sample's value. The width and height of
    the output is 512 with the depth as 32\. The number of filters we used to set
    the output's depth value. The total number of parameters for this layer is 5 x
    5 x 3 x 32 (kernel_size * number of filters) which is 2400.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将输入设置为512 x 512 x 3的形状，对于卷积，我们使用32个大小为5 x 5的滤波器。我们设置的步长值为1，并使用相同的填充来确保内核捕获所有图像。在这个例子中，我们不会使用偏差。卷积后的输出形状为(None,
    512, 512, 32)，形状为(samples, 宽度, 高度, 滤波器)。在讨论中，我们将忽略样本值。输出的宽度和高度为512，深度为32。我们使用的滤波器数量设置了输出深度的值。此层的总参数数量为5
    x 5 x 3 x 32（内核大小乘以滤波器数量），即2400。
- en: 'Let''s try another run. Now, we set strides to 2 in the previous code. On execution,
    we get the output as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试再次运行。现在，我们在之前的代码中将步长设置为2。执行后，我们得到以下输出：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see, the convolution output shape (width, height) is reduced to half
    of the input size. This is due to the stride option that we have chosen. Using
    strides 2, it will skip one pixel, making the output half of the input. Let''s
    increase the stride to 4\. The output will be:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，卷积输出的形状（宽度，高度）减少到输入大小的一半。这是由于我们选择的步长选项。使用步长2，它会跳过一个像素，使得输出是输入的一半。让我们将步长增加到4。输出将是：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we can see that the output shape (width and height) is reduced to one-fourth
    of the input.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到输出形状（宽度和高度）减少到输入的四分之一。
- en: 'If we set our strides to 1, and padding to valid, the output is:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将步长设置为1，并将填充设置为valid，输出将是：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, even if we set strides to 1, the output shape (width and height) is reduced
    to `508`. This is due to the lack of a padding set, and the kernel cannot be applied
    to the edges of the input.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，即使我们将步长设置为 1，输出形状（宽度和高度）也减少到 `508`。这是由于没有设置填充，内核无法应用于输入的边缘。
- en: We can compute the output shape as ![](img/d47fc610-6adc-4264-b6b8-6b2e4c0dc9b0.png),
    where *I* is input size, *K* is kernel size, *P* is padding used and *S* is stride
    value. If we use the same padding, the *P* value is![](img/535f27e6-8f4e-439e-ad0d-5b6a3548f75b.png). Otherwise,
    if we use the valid padding then *P* value is zero.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算输出形状为 ![](img/d47fc610-6adc-4264-b6b8-6b2e4c0dc9b0.png)，其中 *I* 是输入大小，*K*
    是内核大小，*P* 是使用的填充，*S* 是步长值。如果我们使用相同的填充，则 *P* 值为![](img/535f27e6-8f4e-439e-ad0d-5b6a3548f75b.png)。否则，如果我们使用有效填充，则
    *P* 值为零。
- en: 'Here, we have set another parameter `use_bias=False`. On setting this as true,
    it will add a constant value to each kernel and for a convolution layer, the bias
    parameter is the same as the number of filters used. So, if we set `use_bias to
    True`, with strides 1 and the same padding, we get:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了另一个参数 `use_bias=False`。将其设置为 true 时，它将为每个内核添加一个常数值，对于卷积层，偏置参数与使用的滤波器数量相同。因此，如果我们设置
    `use_bias` 为 True，步长为 1 且填充相同，我们得到：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The total number of parameters is increased by `32`, which is the number of
    filters used in this layer. We have seen how to design the convolution layer and
    what happens when we use different parameters for the convolution layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 参数总数增加了 `32`，这是该层使用的滤波器数量。我们已经看到了如何设计卷积层，以及当我们为卷积层使用不同的参数时会发生什么。
- en: The crucial thing is, what are the kernel values in the filters that will give
    the desired output? To have good performance, we would like to get the output
    to consist of high-quality features from the input. Setting values for the filters
    manually is not feasible as the number of filters grows quite large and the combinations
    of these values are practically infinite. We learn the filter values using optimization
    techniques which use a dataset of inputs and targets and tries to predict as close
    to the targets as possible. The optimization then updates the weights after each
    iteration.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 关键问题是，哪些滤波器中的核值将给出所需的输出？为了获得良好的性能，我们希望输出由输入中的高质量特征组成。由于滤波器的数量相当大，并且这些值的组合实际上无限，因此手动设置滤波器值是不可行的。我们使用优化技术来学习滤波器值，这些技术使用输入和目标的数据集，并试图尽可能接近目标。优化然后在每次迭代后更新权重。
- en: The activation layer
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活层
- en: 'As we saw in the case of the simple neural network, the weighted output is
    passed through a non-linear transformation. This non-linear layer is often referred
    to as the activation layer. Some common types of activation are:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在简单神经网络案例中看到的那样，加权输出通过非线性变换。这个非线性层通常被称为激活层。一些常见的激活类型包括：
- en: 'Sigmoid: ![](img/c032bfe3-45d3-42d2-9f86-a8c3dd69f4f4.png)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sigmoid: ![](img/c032bfe3-45d3-42d2-9f86-a8c3dd69f4f4.png)'
- en: 'ReLU: ![](img/297ac62e-1aa0-4803-9caf-b31c9efd7c3e.png)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ReLU: ![](img/297ac62e-1aa0-4803-9caf-b31c9efd7c3e.png)'
- en: 'Tanh: ![](img/ba5882e8-f64b-4322-bf56-3a05f3bcb3e9.png)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tanh: ![](img/ba5882e8-f64b-4322-bf56-3a05f3bcb3e9.png)'
- en: Leaky ReLU:![](img/ae3fbb33-c7d3-4f9e-9a27-32fccc3fcbfd.png) where ![](img/f88cbe14-ce23-4efa-afe8-0f1a41ea67f2.png)is
    a small positive float
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Leaky ReLU: ![](img/ae3fbb33-c7d3-4f9e-9a27-32fccc3fcbfd.png) 其中 ![](img/f88cbe14-ce23-4efa-afe8-0f1a41ea67f2.png)是一个小的正浮点数'
- en: Softmax: ![](img/182b2e91-8e68-4000-a462-795997e94cd1.png) this is often used
    to represent probability for a class
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax: ![](img/182b2e91-8e68-4000-a462-795997e94cd1.png) 这通常用于表示一个类别的概率
- en: 'The most common choice of activation function is **Rectified Linear Unit** (**ReLU**)
    and this performs well in the majority of cases. In our previous code we can add
    an activation layer as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的激活函数选择是 **Rectified Linear Unit** (**ReLU**)，它在大多数情况下表现良好。在我们的前一个代码中，我们可以添加一个激活层，如下所示：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of executing the code is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码的输出如下：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As we can see from our formulation of activation functions, it does not contain
    any trainable parameters. In Keras, the activation layer can also be added to
    the convolution layer as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们激活函数的公式中可以看出，它不包含任何可训练的参数。在 Keras 中，激活层也可以添加到卷积层，如下所示：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The pooling layer
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: 'Pooling takes in a region of inputs and either a max or an average value of
    that region is produced as output. In effect, it reduces the size of an input
    by sampling in the local region. This layer is inserted between 2 to 3 convolution
    layers to reduce the resolution of the output, thereby reducing the requirement
    for parameters. A visual representation of the pooling operation is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 池化接收一个输入区域，并产生该区域的最大值或平均值作为输出。实际上，它通过在局部区域采样来减少输入的大小。此层通常插入在2到3个卷积层之间，以减少输出分辨率，从而减少参数需求。池化操作的视觉表示如下：
- en: '![](img/2ccdb84b-1c02-406e-b2f9-a1bf17bb7fe8.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ccdb84b-1c02-406e-b2f9-a1bf17bb7fe8.png)'
- en: In the previous figure, the input is a two-dimensional array of 2 x 2 size and
    after pooling operation the output is of size 1 x 1\. This can be generated by
    taking average of the values in the previous array or the maximum value.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中，输入是一个2 x 2大小的二维数组，经过池化操作后输出大小为1 x 1。这可以通过取前一个数组中值的平均值或最大值来生成。
- en: 'To show how the output shape changes after this operation, we can use the following
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示此操作后输出形状的变化，我们可以使用以下代码：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the previous code, we used a convolution layer and added a pooling operation
    to it. When we execute it, the expected output is:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们使用了一个卷积层，并对其添加了池化操作。当我们执行它时，预期的输出是：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we set the pooling parameter to `(2,2)`, representing the width and height
    of the pooling operation. The depth for pooling will be set according to the depth
    of the input to the pooling layer. The resulting output is of half the shape in
    terms of width and height; however, there is no change in depth size.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将池化参数设置为`(2,2)`，表示池化操作的宽度和高度。池化的深度将根据池化层的输入深度设置。结果输出在宽度和高度方面是原来的一半；然而，深度大小没有变化。
- en: The fully connected layer
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全连接层
- en: 'This is a simple neural network layer where each neuron in the current layer
    is connected to all the neurons in the previous layer. This is often referred
    to as `Dense` or `Linear` in various deep learning libraries. In Keras, this can
    be implemented as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的神经网络层，其中当前层的每个神经元都与前一层的所有神经元相连。这通常在各种深度学习库中被称为`Dense`或`Linear`。在Keras中，可以这样实现：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As we execute this code, we can see the output shapes, as well as the number
    of trainable parameters, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行此代码时，我们可以看到输出形状以及可训练参数的数量，如下所示：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The total parameters for this layer are given by ![](img/0835407b-ec13-4fac-97be-9671f468a0e5.png) where* I[s]* is
    input shape and *O[s]* is output shape. In our example, we used an input of shape
    `512` and an output of shape `32` and get a total of 16416 parameters with bias.
    This is quite large compared to a similar convolution layer block, therefore in
    recent models, there has been a trend towards using more convolution blocks rather
    than fully connected blocks. Nonetheless, this layer still plays a major role
    in designing simple convolution neural net blocks.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此层的总参数数由以下公式给出：![](img/0835407b-ec13-4fac-97be-9671f468a0e5.png)，其中*I[s]*是输入形状，*O[s]*是输出形状。在我们的例子中，我们使用了形状为`512`的输入和形状为`32`的输出，并得到带有偏置的总共16416个参数。这与类似的卷积层块相比相当大，因此，在最近的模型中，有一种趋势是使用更多的卷积块而不是完全连接块。尽管如此，此层在设计简单的卷积神经网络块中仍然发挥着重要作用。
- en: In this section, we saw what CNNs are and what their components are. However,
    we haven't seen a way to set parameter values. Additionally, we have not seen
    several other layer structures, such as Batch Normalization and Dropout. These
    other layers also play major roles in designing CNN models.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了CNN是什么以及它们的组成部分。然而，我们还没有看到设置参数值的方法。此外，我们还没有看到其他几个层结构，例如批标准化和Dropout。这些其他层在CNN模型设计中也扮演着重要角色。
- en: Batch Normalization
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批标准化
- en: 'This is applied to normalize the output from the input layer with mean 0 and
    variance 1 as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将应用于将输入层的输出标准化为均值为0和方差为1：
- en: '![](img/ccb54ecc-c993-40e5-a529-c29c0e34177d.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb54ecc-c993-40e5-a529-c29c0e34177d.png)'
- en: 'This layer also has learnable parameters (which are optional in most of the
    deep learning libraries) to squash output in a given range:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此层还具有可学习的参数（在大多数深度学习库中是可选的），用于将输出压缩到给定范围内：
- en: '![](img/7b89d853-6cac-4370-b86a-83af9f4c1efc.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b89d853-6cac-4370-b86a-83af9f4c1efc.png)'
- en: Here γ and β are learnable parameters. Batch Normalization improves training
    by faster convergence as well as acting as a kind of regularization. However,
    since there are learnable parameters, the effect of normalization is different
    in training and testing.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里 γ 和 β 是可学习的参数。批量归一化通过加快收敛速度以及作为一种正则化手段来提高训练效果。然而，由于存在可学习参数，归一化的效果在训练和测试中是不同的。
- en: Dropout
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: One of the important layers that helps prevent overfitting is Dropout. It randomly
    drops, with some probability, the neurons from the previous layer to be used as
    input to the next layer. This acts like we are training an ensemble of neural
    networks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有助于防止过拟合的一个重要层是 Dropout。它以一定的概率随机丢弃前一层中的神经元，作为下一层的输入。这就像我们在训练一个神经网络集成。
- en: In the following section, we will see how to implement a model in Keras and
    perform the learning of parameters, which we skipped in this section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分，我们将看到如何在 Keras 中实现模型并执行参数学习，这是我们本节中跳过的内容。
- en: CNN in practice
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 实践
- en: We will now start with our implementation of a convolutional neural net in Keras.
    For our example case, we will train a network to classify `Fashion-MNIST`. This
    is a dataset of grayscale images of fashion products, of the size 28 x 28\. The
    total number of images is 70,000, with 60,000 as training and 10,000 as a test.
    There are ten categories in this dataset, which are t-shirt, trousers, pullover,
    dress, coat, sandal, shirt, sneakers, bag, and ankle boots. Labels for each are
    marked with a category number from 0-9.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将开始使用 Keras 实现卷积神经网络。在我们的示例案例中，我们将训练一个网络来分类 `Fashion-MNIST`。这是一个包含时尚产品灰度图像的数据集，尺寸为
    28 x 28。总共有 70,000 张图像，其中 60,000 张用于训练，10,000 张用于测试。该数据集中有十个类别，分别是 T恤、裤子、套头衫、连衣裙、外套、凉鞋、衬衫、运动鞋、包和踝靴。每个类别的标签用
    0-9 的类别数字标记。
- en: 'We can load this dataset as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下加载这个数据集：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The previous code block doesn''t output a visualization of the dataset, so
    following image is to show what dataset we will be using:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码块没有输出数据集的可视化，所以以下图像将展示我们将使用的数据集：
- en: '![](img/fe0bbbb7-e19c-4bcf-a570-783f0b8959a8.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fe0bbbb7-e19c-4bcf-a570-783f0b8959a8.png)'
- en: It will split the data into the train and test sets with both inputs `x` as
    well as the label `y`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它将数据分割成训练集和测试集，包括输入 `x` 以及标签 `y`。
- en: 'The convolution layer is written as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层编写如下：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The pooling layer is written as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层编写如下：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The overall output layer is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 整体输出层如下：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The complete model is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的模型如下：
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'On running the previous code block,  created model can be seen as, where each
    row is a layer type arranged sequentially with input layer on the top:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行之前的代码块后，可以看到创建的模型，其中每一行是按顺序排列的层类型，输入层在顶部：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Fashion-MNIST classifier training code
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`Fashion-MNIST` 分类器训练代码'
- en: 'In the section, we will see a classifier model on `Fashion-MNIST` dataset.
    This will take in input grayscale image and outputs one of the pre-defined 10
    classes. In the following steps, we will build the model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到在 `Fashion-MNIST` 数据集上的分类器模型。它将接受输入灰度图像并输出预定义的 10 个类别之一。在以下步骤中，我们将构建模型：
- en: 'First, we import the relevant libraries and modules:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入相关的库和模块：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We define the input height and width parameters to be used throughout, as well
    as other parameters. Here, an epoch defines one iteration over all of the data.
    So, the number of `epochs` means the total number of iterations over all of the
    data:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了在整个过程中使用的输入高度和宽度参数，以及其他参数。在这里，一个 epoch 定义了对所有数据的单次迭代。因此，`epochs` 的数量意味着对所有数据的总迭代次数：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s download and prepare the dataset for training and validation. There
    is already an inbuilt function to do this in Keras:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们下载并准备用于训练和验证的数据集。在 Keras 中已经内置了一个函数来做这件事：
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will build the model using the wrapper convolution function defined earlier:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用之前定义的包装卷积函数来构建模型：
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s set up the `optimizer`, `loss` function, and `metrics` to evaluate our
    predictions:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置 `optimizer`、`loss` 函数和 `metrics` 来评估我们的预测：
- en: '[PRE31]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is optional if we would like to save our model after every epoch:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们希望在每轮迭代后保存我们的模型，这是可选的：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s begin training our model:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始训练我们的模型：
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The previous piece of code will run for a while if you are using the only CPU.
    After 10 epochs, it will say `val_acc= 0.92` (approximately). This means our trained
    model can perform with about 92% accuracy on unseen `Fashion-MNIST` data.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你只使用CPU，前一段代码将运行一段时间。经过10个epoch后，它将显示`val_acc= 0.92`（大约）。这意味着我们的训练模型可以在未见过的`Fashion-MNIST`数据上以大约92%的准确率执行。
- en: 'Once all epoch training finishes, final evaluation is computed as:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有epoch训练完成，最终评估将计算如下：
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Analysis of CNNs
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN分析
- en: Research on different kinds of CNNs is still ongoing, and year after year we
    see improvements in accuracy for models on complex datasets. These improvements
    are in terms of both model structure and how to train these models effectively.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的CNN的研究仍在进行中，年复一年，我们在复杂数据集上的模型准确率都有所提高。这些改进既包括模型结构，也包括如何有效地训练这些模型。
- en: Popular CNN architectures
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流行CNN架构
- en: In the recent few years, the following have become popular in various practical
    applications. In this section, we will see some of the popular architectures and
    how to load them in Keras.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近几年，以下内容在各种实际应用中变得流行。在本节中，我们将看到一些流行的架构以及如何在Keras中加载它们。
- en: VGGNet
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VGGNet
- en: This was introduced in 2014 by Karen Simonyan and Andrew Zisserman, in the paper
    *Very Deep Convolution Networks for Large-Scale Image Recognition,* [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由Karen Simonyan和Andrew Zisserman在2014年提出的，论文名为*Very Deep Convolutional Networks
    for Large-Scale Image Recognition*，[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)。
- en: 'This was one of the initial papers that improved the performance of object
    classification models and was one of the top performing models in the **Imagenet
    Large Scale Visual Recognition Challenge** (**ILSVRC**) 2014, the dataset for
    this was introduced in [Chapter 2](prac-cv_ch02.html), *Libraries, Development
    Platform, and Datasets*. The performance gain was about 4% from the previous best
    model, and as a result, it became quite popular. There were several versions of
    the model but the most popular are VGG16 and VGG19\. We can see a pretrained VGG16
    model in Keras:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇最初改进了物体分类模型性能的论文之一，也是2014年**ImageNet大规模视觉识别挑战赛**（**ILSVRC**）中的顶级性能模型之一。该数据集在[第2章](prac-cv_ch02.html)中介绍，*库、开发平台和数据集*。性能提升大约为前一个最佳模型的4%，因此它变得相当流行。该模型有多个版本，但最受欢迎的是VGG16和VGG19。我们可以在Keras中看到一个预训练的VGG16模型：
- en: '[PRE35]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'On execution, we can see the output as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行过程中，我们可以看到以下输出：
- en: '[PRE36]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Since the total number of parameters is quite large, training such a model from
    scratch will also require a huge amount of data of the order of a few hundred
    thousand.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于参数总数相当大，从头开始训练这样的模型也需要大约几万的数据量。
- en: Inception models
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception模型
- en: 'These were successful in using parallel structures in the convolution network,
    which further increased the performance of models in the same competition. It
    was proposed and refined by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
    Jonathon Shlens, Zbigniew Wojna in the paper *Rethinking the Inception Architecture
    for Computer Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567).
    The model structure for inception-v3 is as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在卷积网络中使用并行结构方面取得了成功，进一步提高了同一竞赛中模型的性能。它是由Christian Szegedy、Vincent Vanhoucke、Sergey
    Ioffe、Jonathon Shlens和Zbigniew Wojna在论文*Rethinking the Inception Architecture
    for Computer Vision*中提出和完善的，[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)。inception-v3的模型结构如下：
- en: '![](img/aeee76d5-2e68-41dc-9df7-335e88166a31.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aeee76d5-2e68-41dc-9df7-335e88166a31.png)'
- en: 'We can use this model in Keras:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Keras中使用此模型：
- en: '[PRE37]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: On execution, it will print out the model structure.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行过程中，它将打印出模型结构。
- en: ResNet model
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet模型
- en: 'Extending more on parallel structure, Kaiming He, Xiangyu Zhang, Shaoqing Ren,
    Jian Sun. introduced *Deep Residual Learning for Image Recognition*  [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385) that
    uses skip connection. The basic block of ResNet is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行结构上进一步扩展，Kaiming He、Xiangyu Zhang、Shaoqing Ren和Jian Sun介绍了使用跳过连接的*Deep Residual
    Learning for Image Recognition*，[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)。ResNet的基本块如下：
- en: '![](img/1e122e62-bd2e-4b66-be3f-1a11c5028ec9.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e122e62-bd2e-4b66-be3f-1a11c5028ec9.png)'
- en: These blocks are repeated and stacked over to create a large network with a
    depth of 18 for 18 blocks, 50 for 50 blocks, and so on. They have shown remarkable
    performance both in terms of accuracy and computation time. In the following code,
    we will see how to use this to predict the top-5 probable categories from an image.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块被重复并堆叠起来，以创建一个深度为18的大型网络，18块为18块，50块为50块，依此类推。它们在准确性和计算时间方面都表现出卓越的性能。在下面的代码中，我们将看到如何使用这种方法从图像中预测前5个可能的类别。
- en: 'The input to the model is the following image of a train locomotive engine,
    which is any normal smartphone photo. We want to see if the pretrained ResNet-50
    model can do close to ground truth predictions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输入是以下火车机车图像，这是一张普通的智能手机照片。我们想看看预训练的ResNet-50模型能否接近真实预测：
- en: '![](img/31524fda-12fd-45ee-a27a-0e18c8547a6d.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/31524fda-12fd-45ee-a27a-0e18c8547a6d.png)'
- en: 'Let''s load the required imports as:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载所需的导入：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We now begin creating a model to detect object in previously shown figure:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在开始创建一个模型来检测先前显示的图像中的对象：
- en: 'The first thing we do is setup loading the ResNet-50 pretrained model:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先做的是设置加载ResNet-50预训练模型：
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We need to preprocess the image for a specific input type for ResNet. In this
    case, the input is the mean, normalized to the size (1, 224, 224, 3):'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要预处理图像以适应ResNet的特定输入类型。在这种情况下，输入是平均值，归一化到大小（1, 224, 224, 3）：
- en: '[PRE40]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s go ahead and load the image and apply preprocessing:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续加载图像并应用预处理：
- en: '[PRE41]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We will now load the model and pass the processed input through the trained
    model. This also computes the runtime:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将加载模型并将处理后的输入通过训练模型。这也计算了运行时间：
- en: '[PRE42]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We have got predictions, but these are just probability values and not class
    names. We will now print class names corresponding to only the top-5 probable
    predictions:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们得到了预测，但这些只是概率值，而不是类别名称。现在我们将打印出与仅前5个可能预测相对应的类别名称：
- en: '[PRE43]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output for this is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出的结果如下：
- en: '[PRE44]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`n04310018` and `steam_locomotive` are the class index and names. The value
    denoted after that is the probability for prediction. So, the pretrained model
    is 89% probable that the input image is a steam locomotive. This is quite impressive
    since the input image is of a locomotive which is not in service and has probably
    never been seen by the model during training.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`n04310018`和`steam_locomotive`是类别索引和名称。之后的值是预测的概率。因此，预训练模型有89%的概率认为输入图像是蒸汽机车。这相当令人印象深刻，因为输入图像是一张机车，它已经不再使用，并且可能在训练期间从未被模型看到。'
- en: Transfer learning
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: In the previous section we saw three different model types, but in deep learning
    models, we are not limited to these. Every year there are better performing model
    architectures being published. However, the performance of these models totally
    depends on training data and their performance is due to the millions of images
    they are trained on. Getting such large datasets and training them for task specific
    purposes is not cost effective, as well as being time consuming. Nonetheless,
    the models can be used in various domains by doing a special type of training
    called **transfer learning**.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了三种不同的模型类型，但在深度学习模型中，我们并不局限于这些。每年都有更好的模型架构被发布。然而，这些模型的性能完全取决于训练数据，它们的性能归因于它们在训练中使用的数百万张图像。获取如此大的数据集并针对特定任务进行训练既不经济，也耗时。尽管如此，通过进行一种特殊的训练类型，即**迁移学习**，这些模型可以在各种领域中使用。
- en: In transfer learning, we fix a part of a model from the input to a given layer
    (also known as **freezing a model**), such that the pretrained weights will help
    in computing rich features from the image. The remaining part is trained on task
    specific datasets. As a result, the remaining part of the model learns better
    features even with small datasets. The choice of how much of a model to freeze
    depends on available datasets and repeated experimentation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们固定模型从输入到给定层的一部分（也称为**冻结模型**），这样预训练的权重将有助于从图像中计算丰富的特征。剩余部分在特定任务数据集上训练。因此，即使数据集很小，模型的剩余部分也能学习到更好的特征。模型冻结多少取决于可用的数据集和重复实验。
- en: 'Further, we will show a comparison of previous models, to understand better
    which model to use. The first plot is the number of parameters in each of the
    models. As the newer models were released, they became more efficient in terms
    of number of parameters to train:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将展示先前模型的比较，以更好地了解使用哪种模型。第一个图是每个模型中的参数数量。随着新模型的发布，它们在训练参数数量方面变得更加高效：
- en: '![](img/2fe55971-191c-4f37-a853-645e24a8f535.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fe55971-191c-4f37-a853-645e24a8f535.png)'
- en: 'Also, we show the comparison of accuracy for the ILSVRC challenge across different
    years. This shows that the model gets better with less parameters and better model
    structures:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还展示了不同年份ILSVRC挑战赛中准确率的比较。这表明，随着参数的减少和模型结构的优化，模型会变得更好：
- en: '![](img/6786a26f-d30e-43ad-8906-7f530bb70d5f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6786a26f-d30e-43ad-8906-7f530bb70d5f.png)'
- en: In this section, we saw that even lacking of large dataset for a particular
    task, we can still achieve good performance by transferring the learning from
    model trained on other similar dataset. In most practical applications, we use
    models trained on ImageNet dataset, however choice of model is, decided by the
    user, based on criteria such as more accuracy model or faster model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解到，即使对于特定任务缺乏大量数据集，我们仍然可以通过从训练在其他相似数据集上的模型中迁移学习来实现良好的性能。在大多数实际应用中，我们使用在ImageNet数据集上训练的模型，然而模型的选择是由用户根据诸如更高精度模型或更快模型等标准来决定的。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we had an introduction to CNNs and their basic components.
    We also saw how to train a model from scratch on an example dataset. Later, we
    learnt to use pretrained models to perform prediction and also transfer learning
    to re-utilize trained models for our tasks.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了卷积神经网络（CNNs）及其基本组件。我们还看到了如何在一个示例数据集上从头开始训练模型。随后，我们学习了如何使用预训练模型进行预测，以及如何进行迁移学习以重新利用训练好的模型来完成我们的任务。
- en: These trained models and CNNs are not only used for image classification but
    also on more complex tasks like object detection and segmentation, as we will
    see in upcoming chapters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练好的模型和CNN不仅用于图像分类，还用于更复杂如目标检测和分割等任务，我们将在接下来的章节中看到。
