- en: Chapter 5. May JS Be with You! Control Your Browser with Motion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine how exciting it would be to be able to control your browser using neither
    the keyboard nor mouse. There are many fields in computer science that tend to
    create a good human interface. One of those fields is Computer Vision. It provides
    outstanding methods that can help you to create something useful rapidly and you
    do not even need devices such as Kinect! The human interface in Computer Vision
    highly depends on object tracking. In the previous chapter, we already saw some
    object tracking examples, such as Camshift. Here, we will introduce more algorithms
    to play with. First, we will start with the basic tracking algorithms, which do
    not have any assumptions about an object from a previous frame. Next, we will
    move on to **Head-coupled perspective**; this is a technique that uses the head
    (or eye) position to simulate a 3D environment on a screen. This will be covered
    by the headtrackr library, which we have seen in the previous chapter ([https://github.com/auduno/headtrackr](https://github.com/auduno/headtrackr)).
    Finally, we will move on to the optical flow algorithms, with the help of which
    you can track many different objects in your application and even create programs
    which can be controlled by gestures. We will create an interesting example that
    uses that type of control. Here, we will introduce a new library ([https://github.com/anvaka/oflow](https://github.com/anvaka/oflow)),
    which provides an excellent way to generate the optical flow of an image. To track
    multiple points at once, we will use the JSFeat ([http://inspirit.github.io/jsfeat/](http://inspirit.github.io/jsfeat/))
    library. Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic tracking with tracking.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling objects with head motion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optical flow for motion estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic tracking with tracking.js
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will refresh our knowledge about object detection and create
    a sample project, which will show how object detection can be presented as object
    tracking. This is a relatively simple topic but, in some cases, it can outperform
    other methods. Remember that object detection deals with detecting instances of
    objects, while tracking deals with locating moving objects over time. If you have
    only one unique object and you assume it will still be unique on the next frames,
    then you can calculate its location over time. In that case, we do not need to
    worry about tracking techniques because the tracking can be done using object
    detection. Here, we will focus mostly on the tracking.js library ([http://trackingjs.com](http://trackingjs.com))
    since it provides the easiest way to do that.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an object tracking application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already mentioned the idea of using object detection as tracking. The idea
    is using a unique object on a scene. This can be a colored ball, your head, hand,
    or anything that has something special and can help to distinguish it from other
    objects in a scene. When you have that object, you just detect it on a frame and
    calculate its centroid to get its position.
  prefs: []
  type: TYPE_NORMAL
- en: To explore this concept, we will use the tracking.js library. We will draw a
    small ball with its center at the centroid of the detected object.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will place the necessary tags for a video and the ball scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to get the ball''s context and its parameters to be able to draw on
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The object we want to track is just a simple colored object, so we will use
    `ColorTracker`. To remove noise, we set the minimum dimensions of a detected object
    to `20`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When we detect an object, we need to clear the context that contains the ball.
    In addition, we take the first detected object and use it to move the ball to
    a new position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `move` function is defined in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we take the center of a detected rectangle and draw a ball using
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the tracker, we just initiate it using the `track` function on the
    `<video>` tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we did not place the code for displaying the tracked object on a video
    because we already covered it in [Chapter 3](cv-web_ch03.html#aid-MSDG1 "Chapter 3. Easy
    Object Detection for Everyone"), *Easy Object Detection for Everyone*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![An example of an object tracking application](img/image00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image shows the detection of a bird in different frames and the
    estimated positions of the corresponding ball below the detections. As we can
    see, our main assumption about the object's (bird) uniqueness is observed. The
    only question is about the tracking—the bird's head is not perfectly detected.
  prefs: []
  type: TYPE_NORMAL
- en: We saw the basic object detection, which is one step closer to tracking but
    it is not 100 percent. What we need to do is to remember the old coordinates of
    a previous object position and compute the motion vector. Let's move on!
  prefs: []
  type: TYPE_NORMAL
- en: Controlling objects with the head motion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a human interface in Computer Vision is not an easy task. One of the
    exciting fields is Head-coupled perspective. This technique is used for rendering
    the scene on the screen, which responds naturally to changes in the head position
    of a viewer relative to the display. Simply put, the technology creates a 3D display
    without using any additional devices except the camera.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we saw how to track a head with the headtrackr library.
    It was done using the Camshift algorithm. In this section, we will explain the
    background of the function for Head-coupled perspective and how to use it in your
    projects to create an amazing human interface. To present a scene, the headtrackr
    library uses one of the most popular JavaScript libraries for 3D modeling—three.js
    ([http://threejs.org](http://threejs.org)). We will begin with an explanation
    of the core function and then see an example of its usage.
  prefs: []
  type: TYPE_NORMAL
- en: The Head-coupled perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, the headtrackr library works with three.js. The three.js
    library provides a clear API and exceptional functionality for 3D modeling. If
    you want, you can switch to another library, but in that case, you will need to
    rewrite some code from the headtrackr library.
  prefs: []
  type: TYPE_NORMAL
- en: The headtrackr library provides a good explanation of the whole algorithm; you
    can refer to it at [http://auduno.com/post/25125149521/head-tracking-with-webrtc](http://auduno.com/post/25125149521/head-tracking-with-webrtc).
    To help you better understand the whole process, and in case you want to modify
    the functionality of the head tracking or use other libraries for the 3D modeling,
    here we will focus on the code of the core function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function itself is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To change the perspective of a scene appropriately, we need to know the movement
    of a head in three directions: *X*, *Y*, and *Z*. To do this, we need to assume
    some scene attributes. The core assumption this method makes is that, at the first
    initialization of the algorithm, the distance between the user who sits in front
    of the screen and the camera is 60cm. In addition to this, the method defines
    the screen height, which is 20cm by default. Using these parameters, we can find
    the **Field of View** (**fov**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Head-coupled perspective](img/image00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the scene, we will create a camera that will represent a user (your head);
    we will call it **camera on a scene**. Do not confuse it with the camera that
    is used to capture your face, for example, the laptop's camera.
  prefs: []
  type: TYPE_NORMAL
- en: The bigger the fov angle, the more objects fit on a screen and the further they
    appear to be. The Fov is computed in the first frame where a head is detected,
    taking into account previously mentioned assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several parameters that the headtrackr function uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**camera**: This is the `PerspectiveCamera` object from the three.js library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scaling**: This is the vertical size of a screen in a 3D model. Basically,
    it scales the whole scene by the constant you define.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fixedPosition**: This is the initial position of a scene camera.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**lookAt**: This is the position of the object you look at, which should be
    THREE.Vector3 which contains the 3 coordinates of an object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**params**: This includes a `screenHeight` field in centimeters. This is an
    optional parameter and it defines the height of your monitor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During the algorithm initialization, first, we set the scene camera position
    and the position of the object that this camera should point at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the screen width and height using the camera aspect ratio and
    scaling parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a head position in each frame, we need to add a listener to `headtrackingEvent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The headtrackr library returns an estimated position of a head in each frame.
    The event contains the `x`, `y`, and `z` fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our camera represents our head position, to update its parameters, we
    need to change the position of the camera with respect to event data; do not forget
    about scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You need to keep an object in the center of the screen. To do so, the method
    sets the view offset using the `setViewOffset` method. The first two parameters
    define the size of the whole view, the last four parameters are the parameters
    of a view rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The last attribute we want to update is the field of view, for which we use
    the `atan2` function. It returns the result from `-PI` to `PI` in radians; we
    need to convert it to degrees and multiply it by `2`, since we use only half of
    a screen in our computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we update the camera parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we saw, all we need to do is to work with the scene camera. If you want modify
    the code or use another library, it should not be a problem for you now.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling a simple box
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example which is provided by the headtrackr library uses an old version
    of three.js but the good point is that it can be applied to new versions too!
    We will follow the cube example, which is available at [http://threejs.org/examples/#canvas_geometry_cube](http://threejs.org/examples/#canvas_geometry_cube).
    You can copy and paste the whole code from there; we will do only basic modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you need to update the scripts section and add the headtrackr library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To track your face, you will need to define the video and canvas tags, which
    will be used by the headtrackr library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Far plane of a scene camera in the three.js example is too close for us, we
    better set it a bit further away:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialization of the tracking process is done by the function that we reviewed
    in the previous section; take a look at the third parameter of a function below
    - camera position. The cube location in the example is `[0, 150, 0]` and its dimensions
    are 200 pixels. We set the camera initialization position at the cube plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a tracker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous section, we reviewed the parameters that can be used while
    you track a face. Now, let''s see what you can use for the head position estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cameraOffset**: This is the distance from your laptop camera to the center
    of a screen, which is 11.5cm (half of the height of a regular laptop screen) by
    default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fov**: This is the horizontal field of view used by the camera in degrees.
    By default, the algorithm automatically estimates this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we get the video and canvas on the JavaScript side. Then, we get the initialization
    and start the tracker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a rough sketch of what you will see while using the application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Controlling a simple box](img/image00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Keep in mind that when you move your head to the left, the camera will display
    the movement to the right. You can move your head in any direction, the only issue
    with that method is that it does not calculate the position of your eyes and because
    of this, the image will not be as perfect as a 3D model. This can be solved using
    more advanced techniques that involve tracking eyes, but in that case, the performance
    will not be in real-time for JavaScript applications.
  prefs: []
  type: TYPE_NORMAL
- en: Optical flow for motion estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw how to track different objects in a scene and how to make a human interface
    using them, but we did not see a more generalized approach. When an object changes
    its position, it moves through a scene and it is interesting to estimate the overall
    movement of the scene. Here, we will introduce the concept of optical flow, and
    will see how to use it for object tracking. In the first part, we will focus on
    the theory and then present two wonderful examples of the optical flow usage.
    Finally, we will create a simple gesture application.
  prefs: []
  type: TYPE_NORMAL
- en: The Lucas-Kanade optical flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many definitions of optical flow, the main one is: it is the change
    in structured intensities of an image due to relative motion between the eyeball
    (camera) and the scene ([http://www.scholarpedia.org/article/Optic_flow](http://www.scholarpedia.org/article/Optic_flow)).
    According to another definition, it is the distribution of the apparent velocities
    of objects in an image ([http://www.mathworks.com/discovery/optical-flow.html](http://www.mathworks.com/discovery/optical-flow.html)).
    To get the idea, look at the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Lucas-Kanade optical flow](img/image00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we are just moving a box and the arrows in the third picture show this
    movement. Simply put, the optical flow shows the displacement of objects. It can
    be used not only for object tracking, but also for video compression and stabilization.
    Furthermore, you can get a structure of a scene using the optical flow. For example,
    if you record a still environment with a moving camera, the objects that are closer
    to the camera change their destination faster than objects that are far from the
    camera.
  prefs: []
  type: TYPE_NORMAL
- en: The optical flow can be computed in many ways. The basic assumption of optical
    flow algorithms is that the object intensities of neighboring frames do not change
    rapidly. The most popular method is the Lucas-Kanade method. In addition to the
    previous assumption, it states that the displacement of objects in nearby frames
    is not large. Moreover, the method takes an *NxN* patch, typically 3 x 3, around
    each pixel and it assumes that the motion of all these pixels is the same. Using
    these assumptions and the knowledge of changes in pixel intensities (gradient)
    around each pixel of the patch, the algorithm calculates its displacement. The
    changes in intensities are computed in *x* and *y* dimensions and, in time. Here,
    by time, we mean the difference between the previous and a current frame.
  prefs: []
  type: TYPE_NORMAL
- en: It's only a 3x 3 patch? What should we do with the fast moving objects? This
    problem can be solved using image pyramids. In this case, we will downsample an
    image and look for the same movement as that of a 3 x 3 patch, but at a lower
    resolution.
  prefs: []
  type: TYPE_NORMAL
- en: The other improvement is the iterative Lucas-Kanade method. After getting a
    flow vector for each pixel, we move pixels by those vectors and try to match the
    previous and current images. In an ideal situation, those images would be matched,
    but with real videos there might be errors due to changes in pixels brightness.
    To avoid an error, we reiterate the process before we get a small error or exceed
    the maximum number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the theoretical part, now let's move to the two amazing libraries
    that can provide the implementation of the optical flow. They can be used for
    different purposes. The core of both libraries is the Lucas-Kanade method.
  prefs: []
  type: TYPE_NORMAL
- en: Optical flow map with oflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start with a small library—oflow ([https://github.com/anvaka/oflow](https://github.com/anvaka/oflow)).
    This is a simple library that just calculates displacement vectors of each patch
    and returns the overall movement of a scene. We will use this movement to control
    the ball that we already used in this chapter. Unfortunately, the library does
    not use an image pyramid to calculate optical flow and because of that, it is
    better suited for getting the whole scene movement than object tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the library in our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As we did previously, we create a video input and a ball canvas. In addition
    to this, we add a canvas for displaying the optical flow map with `id=''flow''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define an object that will be used for optical flow calculation. You
    can create it not only for a video as shown here, but also for a web camera (`WebFlow`)
    and a canvas (`CanvasFlow`). The `zoneSize` variable defines the half dimension
    of a patch, which is set to `8` by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a short example of what we receive in the end—the video on the left
    and the directions of optical flow on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow map with oflow](img/image00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Each arrow shows the movement direction of a patch, of course, there is some
    noise, but most of the directions show the correct result. How do we receive the
    result?
  prefs: []
  type: TYPE_NORMAL
- en: 'After the computation is done for each frame, we need to handle the result.
    In the following, we receive directions for each patch. Then, we draw arrows that
    point to the direction of a patch displacement; we multiply that displacement
    by four so we can see the result in a better manner. You can choose any other
    multiplier, since it is used only for displaying the optical flow and not for
    the actual calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the overall displacement, the library just adds together all the
    vectors. Using the common vector, we draw a ball on its context. If the ball exceeds
    the screen dimensions, we draw it on the opposite side. The overall direction
    is returned using the `u` and `v` fields of the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To start the computational process, just call the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After several frames of the video, we get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow map with oflow](img/image00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is where the ball placement is in the first and last frames, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Optical flow map with oflow](img/image00135.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using this library, it is easy to create a simple game that is controlled by
    gestures, for example. The library usage is not limited to that and you can create
    something different very fast.
  prefs: []
  type: TYPE_NORMAL
- en: Track points with JSFeat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The JSFeat library extends the functionality of optical flow and it can even
    track image points. You can use those points to track objects and control your
    browser. The implementation of optical flow in JSFeat library uses the iterative
    Lucas-Kanade method with pyramids and the result it provides is very smooth.
  prefs: []
  type: TYPE_NORMAL
- en: 'In JSFeat, to work with a video, we need to include an additional JavaScript
    file, which is provided by this library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a video to be processed and a canvas for displaying the content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a lot of variables that need to be defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The last row, from left to right shows: current image pyramid, image pyramid
    from the previous level, number of points that are tracked, and status of points.
    The status indicates whether a point has its representation on a new frame; if
    there is no such point, then the method assumes that the tracking of this point
    was lost and it is removed from the tracking process. The last two variables contain
    the point coordinates of the previous and current frames.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We do not provide functions to select original points here, but you can see
    them in the JSFeat example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html](https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function initializes all necessary variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To start getting video frames, we need to call the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'It uses the `process` function to work with each video frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We copy the points and pyramid variables from the previous frame with the `curr_`
    prefix to variables with the `prev_` prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute an image pyramid for the current frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To call a function for optical flow, we introduce four more variables that
    this function needs. The first variable is `win_size`, which is the size of a
    patch for searching the flow vector; the `max_iter` variable is the maximum number
    of iterations; the `eps` variable is the algorithm, which stops updating a point
    when the movement is less than `eps`; and the `min_eigen_threshold` variable,
    which is the threshold for removing bad points. The `process` function computes
    new point coordinates on a new frame. After this, we call the `prune_oflow_points`
    function. If you continue a video, you can probably loose some points on a future
    frame. In that case, they will not be tracked anymore and will be removed from
    the `curr_xy` variable by this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result we received:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Track points with JSFeat](img/image00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, all points were successfully tracked; this is an ideal example,
    where the object was moving smoothly. In many cases, points can be tracked correctly,
    especially if a video is not edited.
  prefs: []
  type: TYPE_NORMAL
- en: The functionality of the library provides an excellent opportunity to track
    an object by defining its unique points, for example, you can predefine those
    points using FAST corner detection from [Chapter 3](cv-web_ch03.html#aid-MSDG1
    "Chapter 3. Easy Object Detection for Everyone"), *Easy Object Detection for Everyone*.
    In addition to that, you can stabilize a video in real time and do other amazing
    things with it.
  prefs: []
  type: TYPE_NORMAL
- en: Zooming with gestures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What if we want to extend the functionality a bit? Suppose we want to add a
    simple zooming feature to our website. We can use the optical flow for that.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we create our content tag we want to zoom with the following style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To use a webcam with JSFeat, run the following function from compatibility.js,
    which simply initializes your camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'For zooming, we need only two points. So, after receiving the result from the
    optical flow algorithm, we check whether there are two points and if so, we call
    the zoom method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The method itself is very simple. We save the original size and change it based
    on the information we receive from the points of optical flow. We check the distance
    between two points and if it changed, we change the `<div>` content with respect
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of zooming, where we used the Canny edge detector in addition
    to the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Zooming with gestures](img/image00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There is no function to find your fingers on a video, so you need to select
    them before using the zoom function. If you want to find them by yourself, it
    is all in your hands! Probably, you could create a new era in web browser user
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the last chapter of this book. Here, we saw examples that aggregate
    many techniques from the previous chapters. We covered object detection and tracking
    by color, which you can use to create your first tracking application very quickly.
    We explored the power of Head-coupled perspective, which is a new way to present
    the content on your websites in a fresh manner or create funny browser games with
    a human interface. In addition to that, the optical flow provides many extensions
    to this field too. It provides you with an excellent way to track points and objects.
    Moreover, now you can create a simple application that uses gestures to zoom the
    web content. The usage of optical flow is not limited to that and is very flexible,
    and it can be combined with many techniques.
  prefs: []
  type: TYPE_NORMAL
