- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Engineering for Numerical and Image Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, when we design large-scale machine learning systems, the types
    of data we get require more processing than just visualization. This visualization
    is only for the design and development of machine learning systems. During deployment,
    we can monitor the data, as we discussed in the previous chapters, but we need
    to make sure that we use optimized data for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this chapter, we’ll focus on feature engineering – finding the
    right features that describe our data closer to the problem domain rather than
    closer to the data itself. Feature engineering is a process where we extract and
    transform variables from raw data so that we can use them for predictions, classifications,
    and other machine learning tasks. The goal of feature engineering is to analyze
    and prepare the data for different machine learning tasks, such as making predictions
    or classifications.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll focus on the feature engineering process for numerical
    and image data. We’ll start by going through the typical methods, such as **principal
    component analysis** (**PCA**), which we used previously for visualization. Then,
    we’ll cover more advanced methods, such as **t-student distribution stochastic
    network embedding** (**t-SNE**) and **independent component analysis** (**ICA**).
    What we’ll end up with is the use of autoencoders as a dimensionality reduction
    technique for both numerical and image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering process fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA and similar methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders for numerical and image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is the process of transforming raw data into numerical values
    that can be used in machine learning algorithms. For example, we can transform
    raw data about software defects (for example, their description, the characteristics
    of the module they come from, and so on) into a table of numerical values that
    we can use for machine learning. The raw numerical values, as we saw in the previous
    chapter, are the result of quantifying entities that we use as sources of data.
    They are the results of applying measurement instruments to the data. Therefore,
    by definition, they are closer to the problem domain rather than the solution
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: The features, on the other hand, quantify the raw data and contain only the
    information that is important for the machine learning task at hand. We use these
    features to make sure that we find the patterns in the data during training that
    we can then use during deployment. If we look at this process from the perspective
    of measurement theory, this process changes the abstraction level of the data.
    If we look at this process from a statistical perspective, this is the process
    of removing noise and reducing the dimensions of the data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll focus on the process of reducing the dimensions of the
    data and denoising the image data using advanced methods such as autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7**.1* presents where feature extraction is placed in a typical machine
    learning pipeline. This pipeline was presented in [*Chapter 2*](B19548_02.xhtml#_idTextAnchor023):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Feature engineering in a typical machine learning pipeline](img/B19548_07_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Feature engineering in a typical machine learning pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure shows that the features are as close to the clean and validated
    data as possible, so we need to rely on the techniques from the previous chapters
    to visualize the data and reduce the noise. The next activity, after feature engineering,
    is modeling the data, as presented in *Figure 7**.2*. This figure shows a somewhat
    simplified view of the entire pipeline. This was also presented in [*Chapter 2*](B19548_02.xhtml#_idTextAnchor023):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – A typical machine learning pipeline. A somewhat simplified view
    from Chapter 2](img/B19548_07_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – A typical machine learning pipeline. A somewhat simplified view
    from [*Chapter 2*](B19548_02.xhtml#_idTextAnchor023)
  prefs: []
  type: TYPE_NORMAL
- en: We covered modeling previously, so let’s dive deeper into the feature extraction
    process. Since numerical and image data are somewhat similar from this perspective,
    we’ll discuss them together in this chapter. Text data is different and therefore
    we have devoted the next chapter to it.
  prefs: []
  type: TYPE_NORMAL
- en: My first best practice in this chapter, however, is related to the link between
    feature extraction and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #39'
  prefs: []
  type: TYPE_NORMAL
- en: Use feature engineering techniques if the data is complex, but the task is simple
    – for example, creating a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: If the data is complex and the task is complex, try to use complex but capable
    models, such as the transformer models presented later in this book. An example
    of such a task can be code completion when the model finished creating a piece
    of a program that a programmer started to write. Simplifying complex data for
    simpler models allows us to increase the explainability of the trained models
    because we, as AI engineers, are more involved in the process through data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for numerical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll introduce feature engineering for numerical data by using the same technique
    that we used previously but for visualizing data – PCA.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PCA is used to transform a set of variables into components that are supposed
    to be independent of one another. The first component should explain the variability
    of the data or be correlated with most of the variables. *Figure 7**.3* illustrates
    such a transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Graphical illustration of the PCA transformation from two dimensions
    to two dimensions](img/B19548_07_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Graphical illustration of the PCA transformation from two dimensions
    to two dimensions
  prefs: []
  type: TYPE_NORMAL
- en: This figure contains two axes – the blue ones, which are the original coordinates,
    and the orange ones, which are the imaginary axes and provide the coordinates
    for the principal components. The transformation does not change the values of
    the *x* and *y* axes and instead finds such a transformation that the axes align
    with the data points. Here, we can see that the transformed *Y* axis aligns better
    with the data points than the original *Y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s execute a bit of code that can read the data and make such a PCA
    transformation. In this example, the data has six dimensions – that is, six variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code fragment reads the data and shows that it has six dimensions.
    Now, let’s create the PCA transformation. First, we must remove the dependent
    variable in our dataset – `Defect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must import the PCA transformation and execute it. We want to find
    a transformation from the five variables (six minus the `Defect` variable) to
    three dimensions. The number of dimensions is completely arbitrary, but since
    we used two in the previous chapters, let’s use more this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The resulting DataFrame – `dfDataAnt13PCA` – contains the values of the transformed
    variables. They are as independent from one another as possible (linearly independent).
  prefs: []
  type: TYPE_NORMAL
- en: I would like to emphasize the general scheme of how we work with this kind of
    data transformation because that is a relatively standard way of doing things.
  prefs: []
  type: TYPE_NORMAL
- en: First, we instantiate the transformation module and provide the arguments. In
    most cases, the arguments are plenty, but there is one, `n_components`, that describes
    how many components we want to have.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we use the `fit_transform()` function to train the classifier and transform
    it into these components. We use these two operations together, simply because
    these transformations are data-specific. There is no need to train the transformation
    on one data and apply it to another one.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we can also do with PCA, which we cannot do with other types of transformations,
    is check how much variability each component explains – that is, how well the
    components align with the data. We can do this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fragment results in the diagram presented in *Figure 7**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Variability explained by the principal components](img/B19548_07_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Variability explained by the principal components
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows that the first component is the most important one – that
    is, it explains the largest amount of variability. This variability can be seen
    as the amount of information that the data contains. In the case of this dataset,
    the first component explains about 80% of the variability and the second one almost
    20%. This means that our dataset has one dominating dimension and some dispersion
    of the data along a second dimension. The third dimension is almost non-existent.
  prefs: []
  type: TYPE_NORMAL
- en: This is where my next best practice comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #40'
  prefs: []
  type: TYPE_NORMAL
- en: Use PCA if the data is somehow linearly separable and on similar scales.
  prefs: []
  type: TYPE_NORMAL
- en: If the data is linear, or multilinear, PCA makes a large difference for training
    the model. However, if the data is not linear, use a more complex model, such
    as t-SNE.
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a transformation, PCA works well when data is linearly separable to some
    extent. In practice, this means that the coordinate system can be positioned in
    such a way that most of the data is on one of its axes. However, not all data
    is like that. One example of data that is not like that is data that can be visualized
    as a circle – it is equally distributed along both axes.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the dimensions of non-linear data, we can use another technique –
    t-SNE. This kind of dimensionality reduction technique is based on extracting
    the activation values of a neural network, which is trained to fit the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code fragment creates such a t-SNE transformation of the data.
    It follows the same schema that was described for the PCA and it also reduces
    the dimensions to three:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame – `dfDataAnt13TSNE` – contains the transformed data.
    Unfortunately, the t-SNE transformation does not allow us to get the value of
    the explained variability, simply because this concept does not exist for such
    a transformation. However, we can visualize it. The following figure presents
    a 3D projection of the three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Visualization of the t-SNE components. Green dots represent
    defect-free components and red dots represent components with defects](img/B19548_07_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Visualization of the t-SNE components. Green dots represent defect-free
    components and red dots represent components with defects
  prefs: []
  type: TYPE_NORMAL
- en: Here is my next best practice for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #41'
  prefs: []
  type: TYPE_NORMAL
- en: Use t-SNE if you do not know the properties of the data and the dataset is large
    (more than 1,000 data points).
  prefs: []
  type: TYPE_NORMAL
- en: t-SNE is a very good and robust transformation. It works particularly well for
    large datasets – that is, those that consist of hundreds of data points. One of
    the challenges, however, is that there is no interpretation of the components
    that t-SNE delivers. We should also know that the best results from t-SNE require
    hyperparameters to be tuned carefully.
  prefs: []
  type: TYPE_NORMAL
- en: ICA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use another kind of transformation here – ICA. This transformation works
    in such a way that it finds the least correlated data points and separates them.
    It’s been historically used in the medical domain to remove disturbances and artifacts
    from high-frequency **electroencephalography** (**EEG**) signals. An example of
    such a disturbance is the 50 - Hz electrical power signal.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it can be used for any kind of data. The following code fragment illustrates
    how ICA can be used for the same dataset that we used in the previous transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'ICA needs to result in fewer components than the original data, although we
    only used three in the preceding code fragment. The visualization of these components
    is presented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Visualization of the dataset transformed using ICA](img/B19548_07_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Visualization of the dataset transformed using ICA
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.6*, green components are the ones without defects and the red
    ones contain defects.
  prefs: []
  type: TYPE_NORMAL
- en: Locally linear embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A technique that is somewhat in between t-SNE and PCA (or ICA) is known as **locally
    linear embedding** (**LLE**). This technique assumes that neighboring nodes are
    placed close to one another on some kind of virtual plane. The algorithm trains
    a neural network in such a way that it preserves the distances between the neighboring
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code fragment illustrates how to use the LLE technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This fragment results in a similar DataFrame to the previous algorithms. Here
    is the visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Visualization of the LLE components](img/B19548_07_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Visualization of the LLE components
  prefs: []
  type: TYPE_NORMAL
- en: All the techniques we’ve discussed so far are flexible and allow us to indicate
    how many components we need in the transformed data. However, sometimes, the problem
    is that we do not know how many components we need.
  prefs: []
  type: TYPE_NORMAL
- en: Linear discriminant analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear discriminant analysis** (**LDA**) is a technique that results in as
    many components as we have in our dataset. This means that the number of columns
    in our dataset is the same as the number of components that LDA provides. This,
    in turn, means that we need to define one of the variables as the dependent one
    for the algorithm to work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The LDA algorithm finds a projection of the dataset on a lower dimensional
    space in such a way that it separates the data in the classes of the dependent
    variable. Therefore, we need one. The following code fragment illustrates the
    use of LDA on our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The resulting DataFrame contains only one component as we have only one dependent
    variable in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, a new technique for feature extraction has been gaining popularity
    – autoencoders. Autoencoders are special kinds of neural networks that are designed
    to transform data from one type of data into another. Usually, they are used to
    recreate the input data in a slightly modified form. For example, they can be
    used to remove noise from images or change images to use different styles of brushes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoencoders are quite generic and can be used for other kinds of data, which
    we’ll learn about in the remainder of this chapter (for example, for image data).
    *Figure 7**.8* presents the conceptual model of autoencoders. They consist of
    two parts – an encoder and a decoder. The role of the encoder is to transform
    the input data – an image in this example – into an abstract representation. This
    abstract representation is stored in a specific layer (or layers), which is called
    the bottleneck. The role of the bottleneck is to store such properties of the
    input data that allow the decoder to recreate the data. The role of the decoder
    is to take the abstract representation of the data from the bottleneck layer and
    re-create the input data as best as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Conceptual visualization of an autoencoder. Here, the input
    data is in the form of an image](img/B19548_07_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Conceptual visualization of an autoencoder. Here, the input data
    is in the form of an image
  prefs: []
  type: TYPE_NORMAL
- en: Since autoencoders are trained to recreate the data as best as possible, the
    bottleneck values are generally believed to be a good internal representation
    of the input data. It is such a good representation that it allows us to discriminate
    between different input data points.
  prefs: []
  type: TYPE_NORMAL
- en: The bottleneck values are also very flexible. As opposed to the techniques presented
    previously, there is no limit on how many features we can extract. If we need
    to, we can even extract more features than we have columns in our dataset, although
    it does not make much sense.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s construct a pipeline for extracting features from an autoencoder that’s
    designed to learn the representation of the defect data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code fragment illustrates reading the dataset and removing the
    defect column from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the removal of the column, we need to scale the data so that
    the autoencoder has a good chance of recognizing small patterns in all columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create the encoder part of our autoencoder, which is shown in the
    following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates two levels of the autoencoder since our data is
    quite simple. Now, the interesting part is the bottleneck, which can be created
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the bottleneck is very narrow – only three neurons – as the dataset
    is rather small and it is not very complex. In the next part, when we use the
    autoencoder for images, we will see that the bottleneck can be much wider. The
    general idea is that wider bottlenecks allow us to capture more complex dependencies
    in the data. For example, for color images, we need more neurons as we need to
    capture colors, while for grayscale images, we need narrower bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can create the decoder part of the autoencoder by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the construction process is to put these three parts together
    – the encoder, the bottleneck, and the decoder. We can use the following code
    for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have constructed our autoencoder. We’ve defined its layers
    and the bottleneck. Now, the autoencoder must be trained to understand how to
    represent our data. We can do this using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that we use the same data as input and as validation since we need
    to train the encoder to re-create the same data as accurately as possible, given
    the size of the bottleneck. After training the encoder model, we can use it to
    extract the bottleneck values from the model. We can do this by defining a submodel
    and using it for input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The outcome of executing this code is a vector of three values – the bottleneck
    values of the autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: My next best practice in this chapter is related to the use of autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #42'
  prefs: []
  type: TYPE_NORMAL
- en: Use autoencoders for numerical data when the dataset is really large since autoencoders
    are complex and require a lot of data for training.
  prefs: []
  type: TYPE_NORMAL
- en: Since the quality of the features is a function of how well the autoencoder
    is trained, we need to make sure that the training dataset is large. Therefore,
    autoencoders are often used for image data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most prominent feature extraction methods for image data is the use
    of **convolutional neural networks** (**CNNs**) and extracting embeddings from
    these networks. In recent years, a new type of this kind of neural network was
    introduced – autoencoders. Although we can use autoencoders for all kinds of data,
    they are particularly well-suited for images. So, let’s construct an autoencoder
    for the MNIST dataset and extract bottleneck values from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to download the MNIST dataset using the following code fragment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can construct the encoder part by using the following code. Please
    note that there is one extra layer in the encoder part. The goal of this layer
    is to transform a two-dimensional image into a one-dimensional input array – to
    flatten the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can construct our bottleneck. In this case, the bottleneck can be much
    wider as the images are more complex (and there are more of them) than the array
    of numerical values of modules, which we used in the previous autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder part is very similar to the previous example, with one extra layer
    that re-creates the image from its flat representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can compile and train the autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can extract the bottleneck values from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, the resulting array of values is much larger – it has 32 values, the same
    number of neurons that we have in our bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: The number of neurons in the bottleneck is essentially arbitrary. Here’s a best
    practice for selecting the number of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #43'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a small number of neurons in the bottleneck – usually one third of
    the number of columns. If the autoencoder does not learn, increase the number
    gradually.
  prefs: []
  type: TYPE_NORMAL
- en: There is no specific reason why I chose 1/3rd of the number of columns, just
    experience. You can start from the opposite direction – make the bottleneck layer
    as wide as the input – and decrease gradually. However, having the same number
    of features as the number of columns is not why we use feature extraction in the
    first place.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, our focus was on feature extraction techniques. We explored
    how we can use dimensionality reduction techniques and autoencoders to reduce
    the number of features in order to make machine learning models more effective.
  prefs: []
  type: TYPE_NORMAL
- en: However, numerical and image data are only two examples of data. In the next
    chapter, we continue with the feature engineering methods, but for textual data,
    which is more common in contemporary software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Zheng, A. and A. Casari, Feature engineering for machine learning: principles
    and techniques for data scientists. 2018: O’Reilly* *Media, Inc*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Heaton, J. An empirical analysis of feature engineering for predictive modeling.
    In SoutheastCon 2016\.* *2016\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Staron, M. and W. Meding, Software Development Measurement Programs. Springer.
    https://doi.org/10.1007/978-3-319-91836-5\. Vol. 10\.* *2018\. 3281333.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Abran, A., Software metrics and software metrology. 2010: John Wiley &* *Sons.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meng, Q., et al. Relational autoencoder for feature extraction. In 2017 International
    joint conference on neural networks (IJCNN).* *2017\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Masci, J., et al. Stacked convolutional auto-encoders for hierarchical feature
    extraction. In Artificial Neural Networks and Machine Learning, ICANN 2011: 21st
    International Conference on Artificial Neural Networks, Espoo, Finland, June 14-17,
    2011, Proceedings, Part I 21\.* *2011\. Springer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rumelhart, D.E., G.E. Hinton, and R.J. Williams, Learning representations
    by back-propagating errors. nature, 1986\. 323(6088):* *p. 533-536.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mosin, V., et al., Comparing autoencoder-based approaches for anomaly detection
    in highway driving scenario images. SN Applied Sciences, 2022\. 4(12):* *p. 334.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
