<html><head></head><body>
		<div id="_idContainer057">
			<h1 id="_idParaDest-59" class="chapter-number"><a id="_idTextAnchor079"/><a id="_idTextAnchor080"/><a id="_idTextAnchor081"/>3</h1>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor082"/>Undersampling Methods</h1>
			<p>Sometimes, you have so much data that adding more data by oversampling only makes things worse. Don‚Äôt worry, as we have a strategy for those situations as well. It‚Äôs called undersampling, or downsampling. In this chapter, you will learn about the concept of undersampling, including when to use it and the various techniques to perform it. You will also see how to use these techniques via the <strong class="source-inline">imbalanced-learn</strong> library APIs and compare their performance with some classical machine <span class="No-Break">learning models.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Introducing undersampling</span></li>
				<li>When to avoid undersampling in the <span class="No-Break">majority class</span></li>
				<li>Removing <span class="No-Break">examples uniformly</span></li>
				<li>Strategies for removing <span class="No-Break">noisy observations</span></li>
				<li>Strategies for removing <span class="No-Break">easy observations</span></li>
			</ul>
			<p>By the end of this chapter, you‚Äôll have mastered various undersampling techniques for imbalanced datasets and will be able to confidently apply them with the <strong class="source-inline">imbalanced-learn</strong> library to build better machine <span class="No-Break">learning models.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor083"/>Technical requirements</h1>
			<p>This chapter will make use of common libraries such as <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">seaborn</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">scikit-learn</strong>, and <strong class="source-inline">imbalanced-learn</strong>. The code and notebooks for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03">https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter03</a>. To run the notebook, there are two options: you can click the <strong class="bold">Open in Colab</strong> icon at the top of the chapter‚Äôs notebook, or you can launch it directly from <a href="https://colab.research.google.com">https://colab.research.google.com</a> using the GitHub URL of <span class="No-Break">the notebook.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor084"/>Introducing undersampling</h1>
			<p class="author-quote">Two households, both alike in dignity,</p>
			<p class="author-quote">In fair Verona, where we lay our scene,</p>
			<p class="author-quote">From ancient grudge break to new mutiny,</p>
			<p class="author-quote">Where civil blood makes civil hands unclean.</p>
			<p>‚Äì Opening lines of <em class="italic">Romeo and Juliet</em>, <span class="No-Break">by Shakespeare</span></p>
			<p>Let‚Äôs look at a<a id="_idIndexMarker177"/> scenario inspired by Shakespeare‚Äôs play <em class="italic">Romeo and Juliet</em>. Imagine a town with two warring communities (viz., the Montagues and Capulets). They have been enemies for generations. The Montagues are in the minority and the Capulets are in the majority in the town. The Montagues are super rich and powerful. The Capulets are not that well off. This creates a complex situation in the town. There are regular riots in the town because of this rivalry. One day, the Montagues win the king‚Äôs favor and conspire to eliminate some Capulets to bring their numbers down. The idea is that if fewer Capulets are in the town, the Montagues will no longer be in the minority. The king agrees to the plan as he hopes for peace after its execution. We will use this story in this chapter to illustrate various <span class="No-Break">undersampling algorithms.</span></p>
			<p>Sometimes, it is not sufficient to oversample the minority class. With oversampling, you can run into problems such as overfitting and longer training time. To solve these problems and to approach the issue of class imbalance differently, people have thought of the opposite of oversampling‚Äîthat is, <strong class="bold">undersampling</strong>. This is also often referred to in the literature<a id="_idIndexMarker178"/> as <strong class="bold">downsampling</strong> or <strong class="bold">negative downsampling</strong> to denote<a id="_idIndexMarker179"/> that the negative class (that is, the majority class) is <span class="No-Break">being undersampled.</span></p>
			<p>Undersampling techniques reduce the number of samples in the majority class(es). This method has two obvious advantages <span class="No-Break">over oversampling:</span></p>
			<ul>
				<li><strong class="bold">The data size remains in check</strong>: Even if data <a id="_idIndexMarker180"/>imbalance is not a concern, dealing with massive datasets‚Äîranging from terabytes to petabytes‚Äîoften necessitates data reduction for practical training. The sheer volume can make training impractical both in terms of time and computational costs. Cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud charge for compute units in addition to storage, making large-scale training expensive. Given that you‚Äôre likely to use only a fraction of the available training data anyway, it‚Äôs crucial to be strategic about which data to retain and which to discard. Undersampling becomes not just a method for balancing classes but also a cost-effective strategy for efficient training, potentially reducing training time from days <span class="No-Break">to hours.</span></li>
				<li><strong class="bold">There is a smaller chance of overfitting</strong>: By using undersampling techniques, the number of majority class instances can be reduced, allowing the model to focus more on the minority class instances. This, in turn, improves the model‚Äôs ability to generalize across both classes. As a result, the model becomes less likely to overfit to the majority class and is better equipped to handle new, unseen data, thus reducing the likelihood of overfitting. We are going to discuss the various undersampling<a id="_idIndexMarker181"/> methods in <span class="No-Break">this chapter.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em> shows the general idea behind <span class="No-Break">undersampling graphically.</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B17259_03_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 ‚Äì General idea behind undersampling showing (a) imbalanced data with two classes and (b) data after undersampling</p>
			<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1(a)</em>, we show the<a id="_idIndexMarker182"/> original data containing many data points from the circle class. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1(b)</em>, we show the resampled data after removing some data points from the <span class="No-Break">circle class.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor085"/>When to avoid undersampling the majority class</h1>
			<p>Undersampling is not <a id="_idIndexMarker183"/>a panacea and may not always work. It depends on the dataset and model <span class="No-Break">under consideration:</span></p>
			<ul>
				<li><strong class="bold">Too little training data for all the classes</strong>: If the dataset is already small, undersampling the majority class can lead to a significant loss of information. In such cases, it is advisable to try gathering more data or exploring other techniques, such as oversampling the minority class to balance the <span class="No-Break">class distribution.</span></li>
				<li><strong class="bold">Majority class equally important or more important than minority class</strong>: In specific scenarios, such as the spam filtering example mentioned in <a href="B17259_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Data Imbalance in Machine Learning</em>, it is crucial to maintain high accuracy in identifying the majority class instances. In such situations, undersampling the majority class might reduce the model‚Äôs ability to accurately classify majority class instances, leading to a higher false positive rate. Instead, alternative methods, such as cost-sensitive learning or adjusting the decision threshold (both of these are discussed in <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Cost-Sensitive Learning</em>), can <span class="No-Break">be considered.</span></li>
				<li><strong class="bold">When undersampling harms model performance or causes overfitting of the model</strong>: Undersampling the majority class might decrease overall model performance, as it discards potentially valuable information. Some of the undersampling methods discard the examples near the decision boundary, which can also alter the decision boundary. Also, by reducing the size of the majority class, undersampling can cause underfitting, where the model becomes too simple to capture the underlying trends in the limited training data and performs poorly on new, unseen data. There is some risk of overfitting as well when using undersampling techniques if the model memorizes the reduced dataset. In such cases, exploring other techniques such as ensemble methods (<a href="B17259_04.xhtml#_idTextAnchor120"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Ensemble Methods</em>), hybrid methods that combine oversampling and undersampling (discussed toward the end of this chapter), or using different algorithms less prone to overfitting might <span class="No-Break">be better.</span></li>
			</ul>
			<p class="callout-heading">üöÄ Undersampling techniques in production at Meta, Microsoft, and Uber</p>
			<p class="callout">The main challenge in tasks such as ad click prediction is handling massive and imbalanced datasets. For instance, a single day of Facebook ads can contain hundreds of millions of instances, with an <a id="_idIndexMarker184"/>average <strong class="bold">ClickThrough Rate</strong> (<strong class="bold">CTR</strong>) of just 0.1%. To address this, Meta employs two specialized techniques, as detailed in the paper <em class="italic">Practical Lessons from Predicting Clicks on Ads at Facebook</em> [1]. The first is uniform subsampling, which uniformly reduces the training data volume and has shown that using just 10% of the data results in only a 1% reduction in model performance. The second is negative downsampling, which specifically targets negative (‚Äúno-click‚Äù) examples and uses an optimal downsampling rate <span class="No-Break">of 0.025.</span></p>
			<p class="callout">Similarly, Microsoft and Uber have very similar approaches to tackling these challenges. To estimate the CTR of sponsored ads on Bing search [2], Microsoft uses a 50% negative downsampling rate for non-click cases, effectively halving the training time while maintaining similar performance metrics. Uber Eats also employs negative downsampling to reduce the training data in order to train models that predict whether to send push notifications to customers about new restaurants [3]. In addition, they remove the least important features when building the final version of <span class="No-Break">the model.</span></p>
			<p>Let‚Äôs look at one of <a id="_idIndexMarker185"/>the ways of classifying undersampling <span class="No-Break">methods<a id="_idTextAnchor086"/> next.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor087"/>Fixed versus cleaning undersampling</h2>
			<p>Undersampling<a id="_idIndexMarker186"/> methods can be divided into<a id="_idIndexMarker187"/> two categories based on how data points get removed from the majority class. These categories are fixed methods and <span class="No-Break">cleaning methods.</span></p>
			<p>In <strong class="bold">fixed methods</strong>, the <a id="_idIndexMarker188"/>number of examples in the majority class is reduced to a fixed number. Usually, we reduce the number of majority class samples to the size of the minority class. For example, if there are 100 million samples in the majority class and 10 million samples in the minority class, you will be left with only 10 million samples of both classes after applying the fixed method. Some such methods are random undersampling and instance <span class="No-Break">hardness-based undersampling.</span></p>
			<p>In <strong class="bold">cleaning methods</strong>, the <a id="_idIndexMarker189"/>number of samples of the majority class is reduced based on some pre-determined criteria, independent of the absolute number of examples. Once this criterion is met, the algorithm doesn‚Äôt care about the size of the majority or <span class="No-Break">minority class.</span></p>
			<p><em class="italic">Table 3.1</em> summarizes <a id="_idIndexMarker190"/>the key differences between <a id="_idIndexMarker191"/>the two methods in a <span class="No-Break">tabular format:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold">Fixed </strong><span class="No-Break"><strong class="bold">undersampling methods</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Cleaning </strong><span class="No-Break"><strong class="bold">undersampling methods</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Key idea</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Selects a specific number of majority class instances <span class="No-Break">to remove</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Identifies and removes noisy, redundant, or misclassified majority class instances aiming to improve decision boundaries <span class="No-Break">between classes</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Relationship <span class="No-Break">between instances</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Doesn‚Äôt consider relationships <span class="No-Break">between instances</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Evaluates relationships <span class="No-Break">between instances</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Performance and ease <span class="No-Break">of implementation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Faster and easier <span class="No-Break">to implement</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Sometimes, may have a better model performance and generalization than fixed <span class="No-Break">undersampling methods</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Examples</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Random undersampling</span></p>
							<p>Instance <span class="No-Break">hardness-based undersampling</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Tomek links</span></p>
							<p>Neighborhood <span class="No-Break">cleaning rule</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Figure">Table 3.1 ‚Äì Fixed versus cleaning undersampling methods</p>
			<p><a id="_idTextAnchor088"/>Let‚Äôs create an imbalanced dataset using the <strong class="source-inline">make_classification</strong> API from <strong class="source-inline">sklearn</strong>. We will apply various undersampling techniques throughout this chapter to balance <span class="No-Break">this dataset:</span></p>
			<pre class="source-code">
from collections import Counter
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=10000, n_features=2,
¬†¬†¬†¬†n_redundant=0, n_classes=2, flip_y=0,
¬†¬†¬†¬†n_clusters_per_class=2, class_sep=0.79,
¬†¬†¬†¬†weights=[0.99], random_state=81)</pre>			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> shows what the dataset looks like on a 2D plot. For the complete notebook code, please refer to<a id="_idIndexMarker192"/> the<a id="_idIndexMarker193"/> GitHub repository of <span class="No-Break">this chapter.</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B17259_03_02.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 ‚Äì Plotting a dataset with an imbalance ratio of 1:99</p>
			<p class="callout-heading">Model calibration and threshold adjustment</p>
			<p class="callout">After applying undersampling techniques, you may want to recalibrate the model‚Äôs probability scores. Why? As undersampling alters the original distribution of the classes, the model‚Äôs confidence estimates are biased [4] and may no longer accurately reflect the true likelihood of each class in the real-world scenario. Failing to recalibrate can lead to misleading or suboptimal decision-making when the model is deployed. Therefore, recalibrating the model‚Äôs probability scores ensures that the model not only classifies instances correctly but also estimates the probabilities in a manner that is consistent with the actual class distribution, enhancing its reliability. For a deeper understanding of this process, especially how to recalibrate model scores to account for the effects of downsampling, please refer to <a href="B17259_10.xhtml#_idTextAnchor279"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <span class="No-Break"><em class="italic">Model Calibration</em></span><span class="No-Break">.</span></p>
			<p class="callout">In the context of imbalanced datasets, threshold adjustment techniques can be a critical complement to undersampling methods. Whether or not we end up applying any sampling techniques, adjusting the threshold to determine the correct class label can be crucial for correctly interpreting the model‚Äôs performance. For a more in-depth understanding of <a id="_idIndexMarker194"/>various <a id="_idIndexMarker195"/>threshold adjustment techniques, you can refer to <a href="B17259_05.xhtml#_idTextAnchor151"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Cost-Sensitive Learning</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor089"/>Undersampling approaches</h2>
			<p>Let‚Äôs look at a <a id="_idIndexMarker196"/>second way to categorize undersampling algorithms. There are a few ways the king can eliminate <span class="No-Break">some Capulets:</span></p>
			<ul>
				<li>He can eliminate the Capulets uniformly from the whole town, thereby removing a few Capulets from all areas of <span class="No-Break">the town</span></li>
				<li>Alternatively, the king can remove the Capulets who live near the houses of <span class="No-Break">the Montagues</span></li>
				<li>Lastly, he can remove the Capulets who live far away from the houses of <span class="No-Break">the Montagues</span></li>
			</ul>
			<p>These are the three major approaches used in undersampling techniques. We either remove the majority samples uniformly, remove the majority samples near the minority samples, or remove the majority samples far from the minority samples. We can also combine the last two approaches by removing some nearby and some far away samples. The following diagram gives the classification of <span class="No-Break">these methods:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B17259_03_03.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 ‚Äì Categorization of undersampling techniques</p>
			<p>The following figure<a id="_idIndexMarker197"/> illustrates the difference between the two criteria. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4(a)</em>, we show the original dataset. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4(b)</em>, we show the same dataset after removing the examples close to the decision boundary. Notice how examples closer to the class boundary <span class="No-Break">are removed.</span></p>
			<p>Majority class examples far from the minority class may not effectively help models establish a decision boundary. Hence, such majority class examples away from the decision boundary can be removed. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4(c)</em>, we show the dataset after removing examples far away from the boundary. The examples far from the decision boundary can be considered <span class="No-Break">easy-to-classify examples.</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B17259_03_04.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 ‚Äì Difference between two general approaches to undersampling</p>
			<p>Having discussed <a id="_idIndexMarker198"/>various ways to classify the various undersampling techniques, let‚Äôs now look at them in <span class="No-Break">more deta<a id="_idTextAnchor090"/>il.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor091"/>Removing examples uniformly</h1>
			<p>There are two major ways of removing the majority class examples uniformly from the data. The first way is to remove the examples randomly, and the other way involves using clustering techniques. Let‚Äôs discuss both of these methods <span class="No-Break">in deta<a id="_idTextAnchor092"/>il.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor093"/>Random UnderSampling</h2>
			<p>The first technique<a id="_idIndexMarker199"/> the king might think of is to pick Capulets randomly and remove them from the town. This is a na√Øve approach. It might work, and the king might be able to bring peace to the town. But the king might cause unforeseen damage by picking up some influential Capulets. However, it is an excellent place to start our discussion. This technique can be considered a close cousin of random oversampling. In <strong class="bold">Random UnderSampling</strong> (<strong class="bold">RUS</strong>), as the name suggests, we randomly extract observations from the majority class until the classes are balanced. This technique inevitably leads to data loss, might harm the underlying structure of the data, and thus performs <span class="No-Break">poorly sometimes.</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B17259_03_05.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 ‚Äì Comic explaining the main idea behind the RUS method</p>
			<p>The following is<a id="_idIndexMarker200"/> the code sample for <span class="No-Break">using RUS:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(sampling_strategy=1.0,¬†random_state=42)
X_res, y_res = rus.fit_resample(X, y)</pre>			<p>The <strong class="source-inline">sampling_strategy</strong> value can be used to specify the desired ratio of minority and majority classes, the default being that they will be made equal in number. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6</em> shows the application of the <strong class="source-inline">RandomUnderSampler</strong> technique, where the right plot shows that most of the negative class samples <span class="No-Break">got dropped:</span></p>
			<p class="IMG---Figure">  </p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B17259_03_06.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 ‚Äì Plotting datasets before and after undersampling using RandomUnderSampler</p>
			<p>Next, we transition to <a id="_idIndexMarker201"/>a smarter technique that forms groups among majority <span class="No-Break">class examp<a id="_idTextAnchor094"/>les.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor095"/>ClusterCentroids</h2>
			<p>The second <a id="_idIndexMarker202"/>technique the king might follow to carry out uniform undersampling is to divide the Capulet population into groups based on location. Then, keep one Capulet from each group and remove other Capulets from the group. This method of undersampling is called the <strong class="bold">ClusterCentroids</strong> method. If there are <em class="italic">N</em> items in the minority class, we create <em class="italic">N</em> clusters from the points of the majority class. For example, this can be done using the K-means algorithm. K-means is a clustering algorithm that groups nearby points into different clusters and assigns centroids to <span class="No-Break">each group.</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B17259_03_07.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 ‚Äì Comic illustrating the main idea behind the ClusterCentroids method</p>
			<p>In the ClusterCentroids technique, we first apply the K-means algorithm to all of the majority class<a id="_idIndexMarker203"/> data. Then, for each cluster, we keep the centroid and remove all other examples within that cluster. It‚Äôs worth noting that the centroid might not even be a part of the original data, which is an important aspect of <span class="No-Break">this method.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em>, we show the working of ClusterCentroids. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8(a)</em>, we start with an imbalanced dataset. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8(b)</em>, we calculate the centroids for the three clusters. These centroids are shown as stars in the diagram. Finally, we remove all majority class samples except the centroids from the dataset in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.8(c)</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B17259_03_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 ‚Äì Illustrating how the ClusterCentroids method works</p>
			<p>Here is the code for <span class="No-Break">using ClusterCentroids:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import ClusterCentroids
cc = ClusterCentroids(random_state=42)
X_res, y_res = cc.fit_resample(X,y)
print('Resampled dataset shape %s' % Counter(y_res))</pre>			<p>The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 100, 1: 100})</pre>			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.9</em> shows the<a id="_idIndexMarker204"/> application of the ClusterCentroids technique, where the right plot shows that most of the negative class samples <span class="No-Break">got dropped.</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B17259_03_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 ‚Äì Plotting datasets before and after undersampling using ClusterCentroids</p>
			<p>One thing to note is that ClusterCentroids can be computationally expensive because it uses the K-means algorithm by default, which can be slow. We recommend exploring various parameters in the ClusterCentroids method, such as the estimator, which specifies the clustering method to be used. For example, K-means can be replaced <a id="_idIndexMarker205"/>with MiniBatchKMeans, a faster variant of the K-means <span class="No-Break">clustering algorithm.</span></p>
			<p>In the following section, we will attempt to eliminate the majority class examples in a more <span class="No-Break">strategic m<a id="_idTextAnchor096"/>anner.</span></p>
			<h1 id="_idParaDest-69">Strategies for removing noisy observa<a id="_idTextAnchor097"/>tions</h1>
			<p>The king might<a id="_idIndexMarker206"/> decide to look at the friendships and locations of the citizens before removing anyone. The king might decide to remove the Capulets who are rich and live near the Montagues. This could bring peace to the city by separating the feuding clans. Let‚Äôs look at some strategies to do that with <span class="No-Break">our data.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor098"/>ENN, RENN, and AllKNN</h2>
			<p>The king can<a id="_idIndexMarker207"/> remove <a id="_idIndexMarker208"/>the<a id="_idIndexMarker209"/> Capulets<a id="_idIndexMarker210"/> based<a id="_idIndexMarker211"/> on their neighbors. For example, if one or more of the three closest neighbors of a Capulet is a Montague, the king can remove the Capulet. This technique is<a id="_idIndexMarker212"/> called <strong class="bold">Edited Nearest Neighbors</strong> (<strong class="bold">ENN</strong>) [5]. ENN removes the examples near the decision boundary to increase the separation between classes. We fit a KNN to the whole dataset and remove the examples whose neighbors don‚Äôt belong to the same class. The <strong class="source-inline">imbalanced-learn</strong> library gives us options to decide which classes we would like to resample and what kind of class arrangement the neighbors of the sample <span class="No-Break">should have.</span></p>
			<p>There are two different criteria that we can follow for excluding <span class="No-Break">the samples:</span></p>
			<ul>
				<li>We can choose to exclude samples whose one or more neighbors are not from the same class <span class="No-Break">as themselves</span></li>
				<li>We can decide to exclude samples whose majority of neighbors are not from the same class <span class="No-Break">as themselves</span></li>
			</ul>
			<p>In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.10</em>, we show the working of the ENN algorithm. Here, we remove the majority samples that have one or more minority neighbors. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.10(a)</em>, we show the original dataset. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.10(b)</em>, we highlight the majority class samples that have one or<a id="_idIndexMarker213"/> more<a id="_idIndexMarker214"/> minority<a id="_idIndexMarker215"/> class<a id="_idIndexMarker216"/> nearest<a id="_idIndexMarker217"/> neighbors. The highlighted majority class samples are shown as solid boxes, and their neighbors are shown by creating curves <span class="No-Break">around them.</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B17259_03_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 ‚Äì Illustrating how the ENN method works</p>
			<p>Here is the code for <span class="No-Break">using ENN:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import EditedNearestNeighbours
enn = EditedNearestNeighbours(
¬†¬†¬†¬†sampling_strategy='auto', n_neighbors=200, kind_sel='all')
X_res, y_res = enn.fit_resample(X, y)
print('Resampled dataset shape %s' % Counter(y_res))</pre>			<p>The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 7852, 1: 100})</pre>			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B17259_03_11.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 ‚Äì Plotting datasets before and after undersampling using ENN</p>
			<p>Here, <strong class="source-inline">n_neighbors</strong> is <a id="_idIndexMarker218"/>the <a id="_idIndexMarker219"/>size<a id="_idIndexMarker220"/> of<a id="_idIndexMarker221"/> the<a id="_idIndexMarker222"/> neighborhood to consider to compute the <span class="No-Break">nearest neighbors.</span></p>
			<p>There are two variants of ENN that we won‚Äôt dive into, but you can explore them if you are interested: <strong class="bold">RENN</strong> (<strong class="source-inline">imblearn.under_sampling.RepeatedEditedNearestNeighbours</strong>) and <strong class="bold">AllKNN</strong> (<strong class="source-inline">imblearn.under_sampling.AllKNN</strong>). In RENN [6], we repeat the process followed in ENN until there are no more examples that can be removed or the maximum number of cycle counts has been reached. This algorithm also removes the noisy data. It is stronger in removing the boundary examples as the algorithm is repeated several times (<span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B17259_03_12.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 ‚Äì Plotting datasets before and after undersampling using RENN</p>
			<p>In<a id="_idIndexMarker223"/> the <strong class="bold">AllKNN</strong> method [6], we <a id="_idIndexMarker224"/>repeat <strong class="bold">ENN</strong> but <a id="_idIndexMarker225"/>with<a id="_idIndexMarker226"/> the <a id="_idIndexMarker227"/>number of neig<a id="_idTextAnchor099"/>hbors going from 1 <span class="No-Break">to K.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor100"/>Tomek links</h2>
			<p>In 1976, Ivan Tomek <a id="_idIndexMarker228"/>proposed the idea<a id="_idIndexMarker229"/> of <strong class="bold">Tomek links</strong> [7]. Two examples are said to form Tomek links if they belong to two different classes, and there is no third point with a shorter distance to them than the distance between the two points. The intuition behind Tomek links is that ‚Äú<em class="italic">if two points are from different classes, they should not be nearest to each other.</em>‚Äù These points are part of the noise, and we can eliminate the majority member or both points to reduce noise. This is as if the king decides to remove the Capulets whose best friends <span class="No-Break">are Montagues.</span></p>
			<p>We can use the <strong class="source-inline">TomekLinks</strong> API <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import TomekLinks
tklinks = TomekLinks(sampling_strategy='auto')
X_res, y_res = tklinks.fit_resample(X, y)
print('Resampled dataset shape %s' % Counter(y_res))</pre>			<p>The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 9875, 1: 100})</pre>			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B17259_03_13.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 ‚Äì Plotting datasets before and after undersampling using TomekLinks</p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14</em> shows the <a id="_idIndexMarker230"/>working of the Tomek links<a id="_idIndexMarker231"/> algorithm. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14(a)</em>, we have the original dataset. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14(b),</em> we find and highlight the Tomek links. Notice that the points in these links are close to each other. In <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.14(c)</em>, we show the dataset after removing the majority class samples (depicted as circles) that belong to the Tomek links. Notice the two circles present in part <em class="italic">(b)</em> but missing in part <em class="italic">(c)</em>. Similarly, we show the dataset after removing all the points in Tomek links in part <em class="italic">(d)</em> of <span class="No-Break">the diagram.</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B17259_03_14.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 ‚Äì Illustrating how the TomekLinks algorithm works</p>
			<p>Tomek links is a resource-intensive method due to its requirement of calculating pairwise distances between all examples. As stated in <em class="italic">A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data</em> [8], performing this process on a reduced dataset would be more computationally efficient when dealing with large amounts <span class="No-Break">of data.</span></p>
			<p>In the next <a id="_idIndexMarker232"/>method, we will try to remove majority <a id="_idIndexMarker233"/>class examples from the perspective of minority class examples. Can we remove the nearest neighbors of the minority class that b<a id="_idTextAnchor101"/>elong to the <span class="No-Break">majority class?</span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor102"/>Neighborhood Cleaning Rule</h2>
			<p>Apart from removing <a id="_idIndexMarker234"/>the<a id="_idIndexMarker235"/> Capulets whose one or more nearest neighbors are Montagues, the king might decide to look at the nearest neighbors of Montagues and remove the Capulets who might come up as one of the nearest neighbors for a Montague. In <a id="_idIndexMarker236"/>the <strong class="bold">Neighborhood Cleaning Rule</strong> (<strong class="bold">NCR</strong>) [9], we apply an ENN algorithm, train a KNN on the remaining data, and then remove all the majority class samples that are the nearest neighbors of a <span class="No-Break">minority sample.</span></p>
			<p>Here is the code for <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">NeighourhoodCleaningRule</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import NeighbourhoodCleaningRule
ncr = NeighbourhoodCleaningRule(
¬†¬†¬†¬†sampling_strategy='auto', n_neighbors=200, threshold_cleaning=0.5)
X_res, y_res = ncr.fit_resample(X, y)
print('Resampled data<a id="_idTextAnchor103"/>set shape %s' % Counter(y_res))</pre>			<p>The following is <a id="_idIndexMarker237"/><span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker238"/></span><span class="No-Break"> output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 6710, 1: 100})</pre>			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17259_03_15.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 ‚Äì Plotting datasets before and after undersampling using NCR</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor104"/>Instance hardness threshold</h2>
			<p>The king might<a id="_idIndexMarker239"/> ask a<a id="_idIndexMarker240"/> minister, ‚Äú<em class="italic">Which Capulets have mixed well with Montagues?</em>‚Äù The minister, based on their knowledge of the town, will give a list of those Capulets. Then, the king will remove the Capulets whose names are on the list. This method of using another model to identify noisy samples is known as the <strong class="bold">instance hardness threshold</strong>. In this method, we train a classification model on the data, such as a decision tree, random forest, or <span class="No-Break">linear SVM.</span></p>
			<p>In addition to predicting the class of an instance, these classifiers can return their class probabilities. Class probabilities show the confidence the model has in classifying the instances. With the instance hardness threshold method [10], we remove the majority class samples that received low probability estimates (referred to as the ‚Äúhard instances‚Äù). These instances are considered ‚Äúhard to classify‚Äù due to class overlap, a principal contributor to <span class="No-Break">instance hardness.</span></p>
			<p>The <strong class="source-inline">imbalanced-learn</strong> library provides an API for utilizing <strong class="source-inline">InstanceHardnessThreshold</strong>, where we can specify the estimator used to estimate the hardness of the examples. In this case, we use <strong class="source-inline">LogisticRegression</strong> as<a id="_idIndexMarker241"/> <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker242"/></span><span class="No-Break"> estimator:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import InstanceHardnessThreshold
nm = InstanceHardnessThreshold(
¬†¬†¬†¬†sampling_strategy='auto', estimator=LogisticRegression())
X_res, y_res = nm.fit_resample(X, y)
print('Resampled dataset shape %s' % Counter(y_res))</pre>			<p>The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 100, 1: 100})</pre>			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17259_03_16.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 ‚Äì Plotting datasets before and after undersampling using InstanceHardnessThreshold</p>
			<p>Since classification models need to draw a decision boundary between the majority and minority classes, the majority class examples that are too far away from the minority class examples may <a id="_idIndexMarker243"/>not help the <a id="_idIndexMarker244"/>model decide this decision boundary. Considering this, we will look at methods in the next section that will remo<a id="_idTextAnchor105"/>ve such easy majority <span class="No-Break">class examples.</span></p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor106"/>Strategies for removing easy observations</h1>
			<p>The reverse of the strategy to<a id="_idIndexMarker245"/> remove the rich and famous Capulets is to remove the poor and weak Capulets. This section will discuss the techniques for <strong class="bold">removing the majority samples far away from the minority samples</strong>. Instead of removing the samples from the boundary between the two classes, we use them for training a model. This way, we can train a model to better discriminate between the classes. However, one downside is that these algorithms risk retaining noisy data points, which could then be used to train the model, potentially introdu<a id="_idTextAnchor107"/>cing noise into the <span class="No-Break">predictive system.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor108"/>Condensed Nearest Neighbors</h2>
			<p><strong class="bold">Condensed Nearest Neighbors</strong> (<strong class="bold">CNNeighbors</strong>) [11] is an <a id="_idIndexMarker246"/>algorithm that works <span class="No-Break">as follows:</span></p>
			<ol>
				<li>We add all minority samples to a set and one randomly selected majority sample. Let‚Äôs call this <span class="No-Break">set </span><span class="No-Break"><strong class="source-inline">C</strong></span><span class="No-Break">.</span></li>
				<li>We train a KNN model with <em class="italic">k = 1</em> on <span class="No-Break">set </span><span class="No-Break"><strong class="source-inline">C</strong></span><span class="No-Break">.</span></li>
				<li>Now, we repeat the following four steps for each of the remaining <span class="No-Break">majority samples:</span><ol><li class="upper-roman">We consider one majority sample; let‚Äôs call <span class="No-Break">it </span><span class="No-Break"><strong class="source-inline">e</strong></span><span class="No-Break">.</span></li><li class="upper-roman">We try to predict the class of <strong class="source-inline">e</strong> <span class="No-Break">using KNN.</span></li><li class="upper-roman">If the predicted class matches the original class, we remove the sample. The intuition is that there is little to learn from <strong class="source-inline">e</strong> as even a <em class="italic">1-NN</em> classifier can <span class="No-Break">learn it.</span></li><li class="upper-roman">Otherwise, we add the sample to our set <strong class="source-inline">C</strong> and train the <em class="italic">1-NN</em> on <span class="No-Break"><strong class="source-inline">C</strong></span><span class="No-Break"> again.</span></li></ol></li>
			</ol>
			<p>This method <a id="_idIndexMarker247"/>removes the easy-to-classify samples from the <span class="No-Break">majority class.</span></p>
			<p>The code to use <strong class="source-inline">CondensedNearestNeighbour</strong> is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import CondensedNearestNeighbour
cnn = CondensedNearestNeighbour(random_state=42)
X_res, y_res = cnn.fit_resample(X, y)
print('Resam<a id="_idTextAnchor109"/>pled dataset shape %s' % Counter(y_res))</pre>			<p>The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 198, 1: 100})</pre>			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17259_03_17.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 ‚Äì Plotting datasets before and after undersampling using CNNeighbors</p>
			<p>However, the CNNeighbors method can be computationally expensive, as it evaluates each majority class example using the KNN algorithm. This makes the CNNeighbors method unsuitable for big <span class="No-Break">data applications.</span></p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor110"/>One-sided selection</h2>
			<p>The king might <a id="_idIndexMarker248"/>decide to remove some rich<a id="_idIndexMarker249"/> and many poor Capulets. This way, only the middle-class Capulets will stay in the town. In one-sided selection [12], we do just that. This method is a combination of CNNeighbors and Tomek links. We first resample using a CNNeighbors. Then, we remove the Tomek links from the resampled data. It reduces both noisy and <span class="No-Break">easy-to-identify samples.</span></p>
			<p>Here is the code for <strong class="source-inline">OneSidedSelection</strong>. When we don‚Äôt provide the <strong class="source-inline">n_neighbors</strong> parameter, the default value of <strong class="source-inline">1</strong> <span class="No-Break">is taken:</span></p>
			<pre class="source-code">
from imblearn.under_sampling import OneSidedSelection
oss = OneSidedSelection(random_state=0, n_seeds_S=10)
X_res, y_res = oss.fit_resample(X, y)
print('Resa<a id="_idTextAnchor111"/>mpled dataset shape %s' % Counter(y_res))</pre>			<p>The following is <span class="No-Break">the output:</span></p>
			<pre class="source-code">
Resampled dataset shape Counter({0: 4276, 1: 100})</pre>			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17259_03_18.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 ‚Äì Plotting datasets before and after undersampling using OneSidedSelection</p>
			<p>Here, <strong class="source-inline">n_seeds_S</strong> is the number of minority class samples used as seeds in the method, and it can<a id="_idIndexMarker250"/> significantly impact the method‚Äôs<a id="_idIndexMarker251"/> performance. It is advisable to tune <span class="No-Break">this parameter.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor112"/>Combining undersampling and oversampling</h2>
			<p>You might wonder<a id="_idIndexMarker252"/> whether we can combine <a id="_idIndexMarker253"/>undersampling techniques with oversampling techniques to produce even better results. The answer is yes. Oversampling methods increase the number of samples of the minority class but also usually increase the noise in the data. Some undersampling techniques can help us remove the noise, for example, ENN, Tomek links, NCR, and instance hardness. We can combine these methods with SMOTE to produce good results. The combination of SMOTE with ENN [13] and Tomek links [14] has been well researched. Also, the <strong class="source-inline">imbalanced-learn</strong> library sup<a id="_idTextAnchor113"/>ports both of them: <strong class="source-inline">SMOTEENN</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">SMOTETomek</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor114"/>Model performance comparison</h2>
			<p>Let‚Äôs explore how <a id="_idIndexMarker254"/>some popular models perform using the various undersampling techniques we‚Äôve discussed. We use two datasets for this comparison: one synthetic dataset and one real-world dataset called <strong class="source-inline">thyroid_sick</strong> from the <strong class="source-inline">imbalanced-learn</strong> library. We‚Äôll evaluate the performance of 11 different undersampling techniques against a baseline of no sampling, using both logistic regression and random forest models. <em class="italic">Figures 3.19</em> to <em class="italic">3.22</em> show the average precision values for models trained using these <span class="No-Break">various methods.</span></p>
			<p>You can find the <a id="_idIndexMarker255"/>notebook in the GitHub <a id="_idIndexMarker256"/>repository of <span class="No-Break">the chapter.</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B17259_03_19.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19 ‚Äì Average precision when using various methods on the thyroid_sick dataset using random forest</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17259_03_20.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20 ‚Äì Average precision when using various methods on synthetic data using random forest</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17259_03_21.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21 ‚Äì Average precision when using various methods on the thyroid_sick dataset using logistic regression</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B17259_03_22.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22 ‚Äì Average precision when using various methods on synthetic data using logistic regression</p>
			<p>Here are some <span class="No-Break">more </span><span class="No-Break"><a id="_idIndexMarker257"/></span><span class="No-Break">observations:</span></p>
			<ul>
				<li>The effectiveness of undersampling techniques can vary significantly depending on the dataset and <span class="No-Break">its characteristics</span></li>
				<li>No single technique dominates across all datasets, emphasizing the need for empirical testing to choose the best method for your <span class="No-Break">specific problem</span></li>
			</ul>
			<p>So, which method will work best for your data? There is no easy answer to this question. The key here is to develop an intuition about the inner workings of these methods and have a pipeline that can help you test <span class="No-Break">different techniques.</span></p>
			<p>However, certain techniques can be time-consuming. In our testing on a dataset with a million examples, methods such as <strong class="source-inline">CondensedNearestNeighbor</strong>, <strong class="source-inline">ClusterCentroids</strong>, and <strong class="source-inline">ALLKNN</strong> took longer than others. If you‚Äôre dealing with large amounts of data, planning to scale in the future, or are pressed for time, you may want to avoid these methods or tune their parameters. Techniques such as <strong class="source-inline">RandomUnderSampler</strong> and <strong class="source-inline">InstanceHardnessThreshold</strong> are m<a id="_idTextAnchor115"/>ore suitable for rapid <a id="_idIndexMarker258"/><span class="No-Break">iterative development.</span></p>
			<p>That brings us to the end of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor116"/>Summary</h1>
			<p>In this chapter, we discussed undersampling, an approach to address the class imbalance in datasets by reducing the number of samples in the majority class. We reviewed the advantages of undersampling, such as keeping the data size in check and reducing the chances of overfitting. Undersampling methods can be categorized into fixed methods, which reduce the number of majority class samples to a fixed size, and cleaning methods, which reduce majority class samples based on <span class="No-Break">predetermined criteria.</span></p>
			<p>We went over various undersampling techniques, including random undersampling, instance hardness-based undersampling, ClusterCentroids, ENN, Tomek links, NCR, instance hardness, CNNeighbors, one-sided selection, and combinations of undersampling and oversampling techniques, such as <strong class="source-inline">SMOTEENN</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">SMOTETomek</strong></span><span class="No-Break">.</span></p>
			<p>We concluded with a performance comparison of various undersampling techniques from the <strong class="source-inline">imbalanced-learn</strong> library on logistic regression and random forest models, using a few datasets, and benchmarked their performance <span class="No-Break">and effectiveness.</span></p>
			<p>Once you identify that your dataset is imbalanced and could potentially benefit from applying undersampling techniques, go ahead and experiment with the various methods discussed in this chapter. Evaluate their effectiveness using the appropriate metrics, such as PR-AUC, to find the most suitable approach for improving your <span class="No-Break">model‚Äôs performance.</span></p>
			<p>In the next chapter, we will go over various <span class="No-Break">ensemble-based techniques.</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor117"/>Exercises</h1>
			<ol>
				<li>Explore the various undersampling APIs available from the <strong class="source-inline">imbalanced-learn</strong> library <span class="No-Break">at </span><a href="https://imbalanced-learn.org/stable/references/under_sampling.html"><span class="No-Break">https://imbalanced-learn.org/stable/references/under_sampling.html</span></a><span class="No-Break">.</span></li>
				<li>Explore the <strong class="source-inline">NearMiss</strong> undersampling technique, available through the <strong class="source-inline">imblearn.under_sampling.NearMiss</strong> API. Which class of methods does it belong to? Apply the <strong class="source-inline">NearMiss</strong> method to the dataset that we used in <span class="No-Break">the chapter.</span></li>
				<li>Try all the undersampling methods discussed in this chapter on the <strong class="source-inline">us_crime</strong> dataset from UCI. You can find this dataset in the <strong class="source-inline">fetch_datasets</strong> API of the <strong class="source-inline">imbalanced-learn</strong> library. Find the undersampling method with the highest <strong class="source-inline">f1-score</strong> metr<a id="_idTextAnchor118"/>ic for <strong class="source-inline">LogisticRegression</strong> and <span class="No-Break"><strong class="source-inline">XGBoost</strong></span><span class="No-Break"> models.</span></li>
				<li>Can you identify an undersampling method of your own? (Hint: think about combining the various approaches to undersampling in <span class="No-Break">new ways.)</span></li>
			</ol>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor119"/>References</h1>
			<ol>
				<li value="1">X. He et al., ‚Äú<em class="italic">Practical Lessons from Predicting Clicks on Ads at Facebook</em>,‚Äù in Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, New York NY USA: ACM, Aug. 2014, pp. 1‚Äì9. <span class="No-Break">doi: </span><span class="No-Break">10.1145/2648584.2648589</span><span class="No-Break">.</span></li>
				<li>X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, ‚Äú<em class="italic">Model Ensemble for Click Prediction in Bing Search Ads</em>,‚Äù in Proceedings of the 26th International Conference on World Wide Web Companion - WWW ‚Äô17 Companion, Perth, Australia: ACM Press, 2017, pp. 689‚Äì698. <span class="No-Break">doi: </span><span class="No-Break">10.1145/3041021.3054192</span><span class="No-Break">.</span></li>
				<li><em class="italic">How Uber Optimizes the Timing of Push Notifications using ML and Linear </em><span class="No-Break"><em class="italic">Programming</em></span><span class="No-Break">: </span><a href="https://www.uber.com/blog/how-uber-optimizes-push-notifications-using-ml/"><span class="No-Break">https://www.uber.com/blog/how-uber-optimizes-push-notifications-using-ml/</span></a><span class="No-Break">.</span></li>
				<li>A. D. Pozzolo, O. Caelen, R. A. Johnson, and G. Bontempi, ‚Äú<em class="italic">Calibrating Probability with Undersampling for Unbalanced Classification</em>,‚Äù in 2015 IEEE Symposium Series on Computational Intelligence, Cape Town, South Africa: IEEE, Dec. 2015, pp. 159‚Äì166. <span class="No-Break">doi: </span><span class="No-Break">10.1109/SSCI.2015.33</span><span class="No-Break">.</span></li>
				<li>(Introducing the ENN method) D. L. Wilson, ‚Äú<em class="italic">Asymptotic Properties of Nearest Neighbor Rules Using Edited Data</em>,‚Äù IEEE Trans. Syst., Man, Cybern., vol. SMC-2, no. 3, pp. 408‚Äì421, Jul. 1972, <span class="No-Break">doi: </span><span class="No-Break">10.1109/TSMC.1972.4309137</span><span class="No-Break">.</span></li>
				<li>(Introducing the RENN and AllKNN methods) ‚Äú<em class="italic">An Experiment with the Edited Nearest-Neighbor Rule</em>,‚Äù IEEE Trans. Syst., Man, Cybern., vol. SMC-6, no. 6, pp. 448‚Äì452, Jun. 1976, <span class="No-Break">doi: </span><span class="No-Break">10.1109/TSMC.1976.4309523</span><span class="No-Break">.</span></li>
				<li>I. Tomek, ‚Äú<em class="italic">Two Modifications of CNN</em>,‚Äù IEEE Trans. Syst., Man, Cybern., vol. SMC-6, no. 11, pp. 769‚Äì772, Nov. 1976, <span class="No-Break">doi: </span><span class="No-Break">10.1109/TSMC.1976.4309452</span><span class="No-Break">.</span></li>
				<li>G. E. A. P. A. Batista, R. C. Prati, and M. C. Monard, ‚Äú<em class="italic">A study of the behavior of several methods for balancing machine learning training data</em>,‚Äù SIGKDD Explor. Newsl., vol. 6, no. 1, pp. 20‚Äì29, Jun. 2004, <span class="No-Break">doi: </span><span class="No-Break">10.1145/1007730.1007735</span><span class="No-Break">.</span></li>
				<li>(Introducing the neighborhood cleaning rule method) J. Laurikkala, ‚Äú<em class="italic">Improving Identification of Difficult Small Classes by Balancing Class Distribution</em>,‚Äù in Artificial Intelligence in Medicine, S. Quaglini, P. Barahona, and S. Andreassen, Eds., in Lecture Notes in Computer Science, vol. 2101. Berlin, Heidelberg: Springer Berlin Heidelberg, 2001, pp. 63‚Äì66. <span class="No-Break">doi: </span><span class="No-Break">10.1007/3-540-48229-6_9</span><span class="No-Break">.</span></li>
				<li>(Introducing the instance hardness threshold technique) M. R. Smith, T. Martinez, and C. Giraud-Carrier, ‚Äú<em class="italic">An instance level analysis of data complexity</em>,‚Äù Mach Learn, vol. 95, no. 2, pp. 225‚Äì256, May 2014, <span class="No-Break">doi: </span><span class="No-Break">10.1007/s10994-013-5422-z</span><span class="No-Break">.</span></li>
				<li>P. Hart, ‚Äú<em class="italic">The condensed nearest neighbor rule (corresp.)</em>,‚Äù IEEE transactions on information theory, vol. 14, no. 3, pp. 515‚Äì516, <span class="No-Break">1968, </span><span class="No-Break">https://citeseerx.ist.psu.edu/document?</span>‚Ä®<span class="No-Break">repid=rep1&amp;type=pdf&amp;doi=7c3771fd6829630cf450af853 df728ecd8da4ab2</span><span class="No-Break">.</span></li>
				<li>(Introducing the one-sided selection method) M. Kubat and S. Matwin, ‚Äú<em class="italic">Addressing The Curse Of Imbalanced Training Sets: </em><span class="No-Break"><em class="italic">One-sided Selection</em></span><span class="No-Break">‚Äù.</span></li>
				<li>(Application of <strong class="source-inline">SMOTEENN</strong> and <strong class="source-inline">SMOTETomek</strong> methods) Gustavo EAPA Batista, Ronaldo C Prati, and Maria Carolina Monard. <em class="italic">A study of the behavior of several methods for balancing machine learning training data</em>. ACM SIGKDD explorations newsletter, <span class="No-Break">6(1):20‚Äì29, 2004.</span></li>
				<li>(Application of the <strong class="source-inline">SMOTETomek</strong> method) Gustavo EAPA Batista, Ana LC Bazzan, and Maria Carolina Monard. <em class="italic">Balancing training data for automated annotation of keywords: a case study</em>. In WOB, <span class="No-Break">10‚Äì18. 2003.</span></li>
			</ol>
		</div>
	</body></html>