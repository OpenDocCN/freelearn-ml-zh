["```py\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense \n    ```", "```py\n    def build_generator(latent_dim):\n        model = Sequential([\n            Dense(128, activation='relu', input_dim=latent_dim),  # maps from latent space\n            Dense(256, activation='relu'),  # expands representation\n            Dense(784, activation='tanh')  # produces 28x28 image\n        ])\n        return model \n    ```", "```py\n    def build_discriminator(input_shape):\n        model = Sequential([\n            Dense(256, activation='relu', input_shape=(input_shape,)),  # processes input image\n            Dense(128, activation='relu'),  # reduces dimensionality\n            Dense(1, activation='sigmoid')  # classifies input as real or generated\n        ])\n        return model\n    Define the latent dimension and initialize the models:\n    latent_dim = 100  # size of input noise vector\n    generator = build_generator(latent_dim)\n    discriminator = build_discriminator(784)  # for 28x28 images \n    ```", "```py\n    from tensorflow.keras.layers import Input, Lambda\n    from tensorflow.keras import Model, backend as K\n    def sampling(args):\n        z_mean, z_log_var = args\n        batch = K.shape(z_mean)[0]\n        dim = K.int_shape(z_mean)[1]\n        epsilon = K.random_normal(shape=(batch, dim))\n        return z_mean + K.exp(0.5 * z_log_var) * epsilon \n    ```", "```py\n    inputs = Input(shape=(784,)) # Encoder\n    h = Dense(256, activation='relu')(inputs)\n    z_mean = Dense(2)(h)\n    z_log_var = Dense(2)(h)\n    z = Lambda(sampling, output_shape=(2,))([z_mean, z_log_var]) \n    ```", "```py\n    decoder_h = Dense(256, activation='relu') # Decoder\n    decoder_mean = Dense(784, activation='sigmoid')\n    h_decoded = decoder_h(z)\n    x_decoded_mean = decoder_mean(h_decoded)\n    vae = Model(inputs, x_decoded_mean) \n    ```", "```py\nfrom tensorflow.keras.layers import LSTM\nsequence_length = 10\nnum_features = 5\nmodel = Sequential([\n    LSTM(50, activation='relu', input_shape=(sequence_length, num_features)),\n    Dense(1)\n])\nmodel.compile(optimizer='adam', loss='mse') \n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef generate_synthetic_data(num_samples, sequence_length, num_features):\n    X = []\n    y = []\n    for i in range(num_samples):\n        base = np.array([np.sin(x) for x in range(sequence_length)]) + np.random.normal(0, 0.1, sequence_length)\n        features = np.tile(base, (num_features, 1)).T + np.random.normal(0, 0.1, (sequence_length, num_features))\n        target = np.sum(base) + np.random.normal(0, 0.1)\n        X.append(features)\n        y.append(target)\n    return np.array(X), np.array(y)\nX_train, y_train = generate_synthetic_data(100, sequence_length, num_features)\nmodel.fit(X_train, y_train, epochs=10, verbose=1) \n```", "```py\nX_test, y_test = generate_synthetic_data(10, sequence_length, num_features)\ny_pred = model.predict(X_test)\nplt.plot(y_test, label='Actual')\nplt.plot(y_pred, label='Predicted')\nplt.xlabel('Days')\nplt.ylabel('Sales Prediction')\nplt.title('Actual vs Predicted Sales Over Time')\nplt.legend()\nplt.show() \n```", "```py\nfrom tensorflow.keras.layers import Embedding, LayerNormalization, MultiHeadAttention\ndef simplified_gpt_model(vocab_size=10000, embed_dim=256, max_length=40, num_heads=4, ff_dim=512)\n    inputs = Input(shape=(max_length,))  # input layer for sequences of tokens\n    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs) # convert token indices to vectors\n    # self-Attention layer with multiple heads\n    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(embedding_layer, embedding_layer)\n    # normalization and residual connection for the attention output\n    attn_output = LayerNormalization(epsilon=1e-6)(attn_output + embedding_layer)\n    # feed Forward network to processes attention output\n    ff_network = Dense(ff_dim, activation=\"relu\")(attn_output)\n    ff_network_output = Dense(embed_dim)(ff_network)\n    # second normalization and residual connection\n    sequence_output = LayerNormalization(epsilon=1e-6)(ff_network_output + attn_output)\n    # output layer to predict the next token in the sequence\n    outputs = Dense(vocab_size, activation=\"softmax\")(sequence_output)\n    model = Model(inputs=inputs, outputs=outputs)\n    return model\ngpt_model = simplified_gpt_model()\ngpt_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') \n```", "```py\nA, B = np.array([2, 2]), np.array([3, 0])\ncos_sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\nangle = np.arccos(cos_sim)\nplt.figure(figsize=(6,6))\nfor vector, color, label in zip([A, B], ['red', 'green'], ['Vector A', 'Vector B']):\n    plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, color=color, label=label)\nplt.plot(0.5 * np.cos(np.linspace(0, angle, 100)), 0.5 * np.sin(np.linspace(0, angle, 100)), color='blue', label=f'Cosine Similarity = {cos_sim:.2f}')\nplt.axis([-1, 4, -1, 4])\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.title('Cosine Similarity between Vectors')\nplt.legend()\nplt.show() \n```", "```py\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\nsentence_1 = \"He turned on the light to read.\"\nsentence_2 = \"The light fabric was perfect for summer.\"\ntokens_1 = tokenizer(sentence_1, return_tensors=\"tf\")\ntokens_2 = tokenizer(sentence_2, return_tensors=\"tf\")\noutputs_1 = model(tokens_1)\noutputs_2 = model(tokens_2)\nlight_index_1 = tokens_1['input_ids'][0].numpy().tolist().index(tokenizer.convert_tokens_to_ids('light'))\nlight_index_2 = tokens_2['input_ids'][0].numpy().tolist().index(tokenizer.convert_tokens_to_ids('light'))\nembedding_1 = outputs_1.last_hidden_state[0, light_index_1]\nembedding_2 = outputs_2.last_hidden_state[0, light_index_2]\ncosine_similarity = tf.keras.losses.cosine_similarity(embedding_1, embedding_2, axis=0)\ncosine_similarity = -cosine_similarity.numpy()\nprint(\"Cosine similarity between 'light' embeddings in the two sentences:\", cosine_similarity) \n```", "```py\nCosine similarity between 'light' embeddings in the two sentences: 0.47577658 \n```", "```py\n!conda install pydot\n!conda install graphviz\nfrom tensorflow.keras.utils import plot_model\nmodel = Sequential([\n    LSTM(64, return_sequences=True, input_shape=(10, 128)),\n    LSTM(64),\n    Dense(10, activation='softmax')\n])\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nplot_model(model, show_shapes=True, show_layer_names=True) \n```", "```py\nlogits = np.array([2, 1, 0.1, 5, 3])\nlabels = ['mat' , 'tree', 'ball', 'bed', 'table']\ndef softmax_temperature_adjusted(logits, temperature):\n    exp_logits = np.exp(logits / temperature)\n    return exp_logits / np.sum(exp_logits)\ntemperatures = [0.5, 1, 2, 4]\nplt.figure(figsize=(12, 8))\nfor T in temperatures:\n    probabilities = softmax_temperature_adjusted(logits, T)\n    plt.plot(labels, probabilities, marker='o', label=f'Temperature = {T}')\nplt.title('Effect of Temperature on Prediction Distribution')\nplt.ylabel('Probability')\nplt.xlabel('Words')\nplt.legend()\nplt.grid(True)\nplt.show() \n```", "```py\nfrom transformers import pipeline\ngenerator = pipeline('text-generation', model='gpt2')\nprompt = \"Write a compelling product description for eco-friendly kitchenware emphasizing sustainability:\"\ncompletion = generator(prompt, max_length=100, num_return_sequences=1, temperature=0.7)\nprint(\"Generated text using HuggingFace's GPT model:\", completion[0]['generated_text']) \n```", "```py\n\"Your kitchen will save you time and energy. It will save you money. It will make you feel good. It will help you get the most out of your kitchen and minimize waste. It will help you get good results. You'll save money and your home will be healthier.\" \n```", "```py\nfrom openai import OpenAI\nclient = OpenAI(api_key = 'XXX') #insert your api key here\ncompletion = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\",  \"content\": \"Write a compelling product description for eco-friendly kitchenware emphasizing sustainability:\"}],\n    max_tokens=100, n=1, temperature=0.7)\nprint(completion.choices[0].message) \n```", "```py\n\"Introducing our premium range of eco-friendly kitchenware, designed for the conscious home cook. Every item in this collection is expertly created from sustainable materials, ensuring minimal impact on our environment. From bamboo cutting boards to recycled stainless-steel knives, each piece combines functionality with eco-friendly design. Our eco-friendly kitchenware is not only durable and high-performing, but also promotes a healthier lifestyle and a greener planet.\" \n```", "```py\ndef generate_response(prompt, model=\"gpt-4\", max_tokens=100, temperature=0.7, n=1):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=max_tokens,\n        n=n,\n        temperature=temperature)\nreturn response.choices[0].message.content \n```", "```py\npoor_product_description = \"Talk about bamboo cutlery.\"\ngenerate_response(poor_product_description) \n```", "```py\nBamboo cutlery is a type of eating utensil made from bamboo, a fast-growing renewable resource. This form of cutlery includes items such as forks, knives, spoons, and chopsticks. They are an eco-friendly alternative to plastic disposable cutlery due to their reusable nature and biodegradable properties. Bamboo cutlery is lightweight and durable, making it a popular choice for camping, picnics, and travel. \n```", "```py\ngood_product_description = \"Write a captivating description for a bamboo cutlery set designed for eco-conscious consumers, emphasizing its sustainability and style.\"\ngenerate_response(good_product_description) \n```", "```py\nDiscover the perfect harmony of sophistication and sustainability with our Bamboo Cutlery Set. Designed for the eco-conscious consumer, this set is not only a statement of your commitment to the environment but also a reflection of your exquisite taste. Each piece is crafted from 100% organic bamboo, a renewable resource that grows significantly faster than traditional hardwoods. This set boasts a sleek, minimalist design that perfectly complements any table setting, from casual picnic to elegant dinner parties. \n```", "```py\npoor_blog_title = \"Write a title about kitchenware benefits.\"\ngenerate_response(poor_blog_title) \n```", "```py\nExploring the Numerous Benefits of High-Quality Kitchenware \n```", "```py\ngood_blog_title = \"Create an intriguing title for a blog post highlighting the top five benefits of biodegradable kitchenware for sustainable living.\"\ngenerate_response(good_blog_title) \n```", "```py\nUnlocking Sustainable Living: Top 5 Benefits of Biodegradable Kitchenware \n```", "```py\npoor_social_media_caption = \"Make an Instagram post for kitchenware.\"\ngenerate_response(poor_social_media_caption) \n```", "```py\n'Spice up your kitchen game with our premium-quality kitchenware! 🍳🥄 From non-stick pans to sharp, durable knives, we have everything you need. Cook like a pro and serve in style! Get yours today at our store. 🍽️👩🍳👨🍳 \n```", "```py\ngood_social_media_caption = \"Create an engaging and witty Instagram caption for our latest eco-friendly kitchenware line, focusing on reducing plastic waste.\"\ngenerate_response(good_social_media_caption) \n```", "```py\nKiss plastic goodbye👋and say hello to our chic, eco-friendly kitchenware!🍃Because every meal should be a feast for the eyes and gentle on the earth. #SustainableGourmet\" \n```", "```py\ndef generate_slogan(prompt, temperature=1.0, top_p=1.0):\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=15,\n        temperature=temperature,\n        top_p=top_p,        n=3)\n    return (response.choices[0].message.content, response.choices[1].message.content, response.choices[2].message.content) \n```", "```py\ngenerate_slogan(prompt) \n```", "```py\n('\"Dress with Purpose: Eco-Elegance Never Goes Out of Style.\"',\n '\"Style with Substance, Fashion with Consciousness!\"',\n '\"Dressing the world, Preserving the planet.\"') \n```", "```py\n    generate_slogan(prompt, temperature=1.8) \n    ```", "```py\n('\"Drape Yourself in Earth Love: Stylishly Sustainable Fashion Favorites\"',\n '\"Wear the change, leave no trace.\"',\n '\"Leave no carbon footprints, only stylish one - Wear Eco in Style\"') \n```", "```py\ngenerate_slogan(prompt, temperature=0.3) \n```", "```py\n('\"Style with Sustainability: Dress Green, Live Clean!\"',\n '\"Style with Sustainability: Fashion for a Better Tomorrow\"',\n '\"Style with Sustainability: Fashion for a Future\"') \n```", "```py\n    generate_slogan(prompt, top_p=0.4) \n    ```", "```py\n('\"Style with Sustainability: Fashion for a Better Future\"',\n '\"Style with Sustainability: Fashion for a Greener Tomorrow\"',\n '\"Style with Sustainability: Fashion for a Greener Tomorrow\"') \n```"]