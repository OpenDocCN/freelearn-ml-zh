- en: Chapter 7. Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 深度学习
- en: In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, we discussed
    different supervised classification techniques that are general and can be used
    in a wide range of applications. In the area of supervised non-linear techniques,
    especially in computer-vision, deep learning and its variants are having a remarkable
    impact. We find that deep learning and associated methodologies can be applied
    to image-recognition, image and object annotation, movie descriptions, and even
    areas such as text classification, language modeling, translations, and so on.
    (*References* [1, 2, 3, 4, and 5])
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《实际应用中的监督学习》*的[第2章](ch02.html "第2章. 实际应用中的监督学习")中，我们讨论了不同的一般监督分类技术，这些技术可以广泛应用于各种应用。在监督非线性技术的领域，特别是在计算机视觉中，深度学习及其变体正产生显著影响。我们发现深度学习及其相关方法可以应用于图像识别、图像和对象标注、电影描述，甚至包括文本分类、语言建模、翻译等领域。（*参考文献*
    [1, 2, 3, 4, 和 5]）
- en: To set the stage for deep learning, we will start with describing what neurons
    are and how they can be arranged to build multi-layer neural networks, present
    the core elements of these networks, and explain how they work. We will then discuss
    the issues and problems associated with neural networks that gave rise to advances
    and structural changes in deep learning. We will learn about some building blocks
    of deep learning such as Restricted Boltzmann Machines and Autoencoders. We will
    then explore deep learning through different variations in supervised and unsupervised
    learning. Next, we will take a tour of Convolutional Neural Networks (CNN) and
    by means of a use case, illustrate how they work by deconstructing an application
    of CNNs in the area of computer-vision. We will introduce Recurrent Neural Networks
    (RNN) and its variants and how they are used in the text/sequence mining fields.
    We will finally present a case study using real-life data of MNIST images and
    use it to compare/contrast different techniques. We will use DeepLearning4J as
    our Java toolkit for performing these experiments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为深度学习做准备，我们将首先描述神经元是什么以及它们如何被排列来构建多层神经网络，展示这些网络的核心元素，并解释它们的工作原理。然后，我们将讨论与神经网络相关的问题和挑战，这些挑战导致了深度学习在技术和结构上的进步。我们将了解深度学习的一些构建块，如受限玻尔兹曼机和自编码器。然后，我们将通过监督学习和无监督学习的不同变体来探索深度学习。接下来，我们将游览卷积神经网络（CNN），并通过一个用例，通过分解CNN在计算机视觉领域的应用来展示它们的工作原理。我们将介绍循环神经网络（RNN）及其变体，以及它们在文本/序列挖掘领域的应用。最后，我们将通过使用MNIST图像的真实数据案例研究来比较/对比不同的技术。我们将使用DeepLearning4J作为我们的Java工具包来执行这些实验。
- en: Multi-layer feed-forward neural network
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层前馈神经网络
- en: Historically, artificial neural networks have been largely identified by multi-layer
    feed-forward perceptrons, and so we will begin with a discussion of the primitive
    elements of the structure of such networks, how to train them, the problem of
    overfitting, and techniques to address it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，人工神经网络主要被识别为多层前馈感知器，因此我们将从讨论这类网络的结构的基本元素开始，包括如何训练它们、过拟合问题以及解决该问题的技术。
- en: Inputs, neurons, activation function, and mathematical notation
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入、神经元、激活函数和数学符号
- en: A single neuron or perceptron is the same as the unit described in the Linear
    Regression topic in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*.
    In this chapter, the data instance vector will be represented by *x* and has *d*
    dimensions, and each dimension can be represented as ![Inputs, neurons, activation
    function, and mathematical notation](img/B05137_07_003.jpg). The weights associated
    with each dimension are represented as a weight vector *w* that has *d* dimensions,
    and each dimension can be represented as ![Inputs, neurons, activation function,
    and mathematical notation](img/B05137_07_005.jpg). Each neuron has an extra input
    *b*, known as the bias, associated with it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元或感知器与[第2章](ch02.html "第2章. 实际应用中的监督学习")中线性回归主题所描述的单位相同，即*《实际应用中的监督学习》*。在本章中，数据实例向量将由*x*表示，具有*d*个维度，每个维度可以表示为![输入、神经元、激活函数和数学符号](img/B05137_07_003.jpg)。与每个维度相关联的权重表示为一个具有*d*个维度的权重向量*w*，每个维度可以表示为![输入、神经元、激活函数和数学符号](img/B05137_07_005.jpg)。每个神经元都有一个额外的输入*b*，称为偏置。
- en: 'Neuron pre-activation performs the linear transformation of inputs given by:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元预激活执行由以下给出的输入线性变换：
- en: '![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_007.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![输入、神经元、激活函数和数学符号](img/B05137_07_007.jpg)'
- en: 'The activation function is given by ![Inputs, neurons, activation function,
    and mathematical notation](img/B05137_07_008.jpg), which transforms the neuron
    input ![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_009.jpg)
    as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数由![输入、神经元、激活函数和数学符号](img/B05137_07_008.jpg)给出，它将神经元输入![输入、神经元、激活函数和数学符号](img/B05137_07_009.jpg)转换为以下形式：
- en: '![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_010.jpg)![Inputs,
    neurons, activation function, and mathematical notation](img/B05137_07_011.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![输入、神经元、激活函数和数学符号](img/B05137_07_010.jpg)![输入、神经元、激活函数和数学符号](img/B05137_07_011.jpg)'
- en: Figure 1\. Perceptron with inputs, weights, and bias feeding to generate outputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 具有输入、权重和偏置的感知器生成输出。
- en: Multi-layered neural network
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层神经网络
- en: Multi-layered neural networks are the first step to understanding deep learning
    networks as the fundamental concepts and primitives of multi-layered nets form
    the basis of all deep neural nets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络是理解深度学习网络的第一步，因为多层网络的基本概念和原语构成了所有深度神经网络的基础。
- en: Structure and mathematical notations
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构和数学符号
- en: We introduce the generic structure of neural networks in this section. Most
    neural nets are variants of the structure outlined here. We also present the relevant
    notation that we will use in the rest of the chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了神经网络的一般结构。大多数神经网络都是这里概述的结构变体。我们还展示了本章其余部分我们将使用的相关符号。
- en: '![Structure and mathematical notations](img/B05137_07_012.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_012.jpg)'
- en: Figure 2\. Multilayer neural network showing an input layer, two hidden layers,
    and an output layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 展示输入层、两个隐藏层和输出层的多层神经网络。
- en: The most common supervised learning algorithms pertaining to neural networks
    use multi-layered perceptrons. The Input Layer consists of several neurons, each
    connected independently to the input, with its own set of weights and bias. In
    addition to the Input Layer, there are one or more layers of neurons known as
    Hidden Layers. The input layer neurons are connected to every neuron in the first
    hidden layer, that layer is similarly connected to the next hidden layer, and
    so on, resulting in a fully connected network. The layer of neurons connected
    to the last hidden layer is called the Output Layer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络相关的最常见监督学习算法使用多层感知器。输入层由几个神经元组成，每个神经元独立连接到输入，并有自己的权重和偏置集。除了输入层之外，还有一层或更多被称为隐藏层的神经元。输入层神经元连接到第一隐藏层中的每个神经元，该层同样连接到下一隐藏层，依此类推，形成一个全连接网络。连接到最后隐藏层的神经元层被称为输出层。
- en: 'Each hidden layer is represented by ![Structure and mathematical notations](img/B05137_07_013.jpg)
    where *k* is the layer. The pre-activation for layer *0 < k * *< l* is given by:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏层由![结构和数学符号](img/B05137_07_013.jpg)表示，其中*k*是层。对于*0 < k * *< l*层的预激活由以下给出：
- en: '![Structure and mathematical notations](img/B05137_07_016.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_016.jpg)'
- en: 'The hidden layer activation for ![Structure and mathematical notations](img/B05137_07_017.jpg):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![结构和数学符号](img/B05137_07_017.jpg)的隐藏层激活：'
- en: '![Structure and mathematical notations](img/B05137_07_018.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_018.jpg)'
- en: 'The final output layer activation is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出层激活为：
- en: '![Structure and mathematical notations](img/B05137_07_019.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_019.jpg)'
- en: The output is generally one class per neuron and it is tuned in such a way that
    only one neuron activates and all others have 0 as the output. A softmax function
    with ![Structure and mathematical notations](img/B05137_07_020.jpg) is used for
    giving the result.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 输出通常是每个神经元一个类别，并且调整方式使得只有一个神经元激活，其他所有神经元的输出都是0。使用![结构和数学符号](img/B05137_07_020.jpg)的softmax函数来给出结果。
- en: Activation functions in NN
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络中的激活函数
- en: Some of the most well-known activation functions that are used in neural networks
    are given in the following sections and they are used because the derivatives
    needed in learning can be expressed in terms of the function itself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中给出了神经网络中使用的一些最著名的激活函数，它们之所以被使用，是因为学习过程中所需的导数可以用函数本身来表示。
- en: Sigmoid function
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'Sigmoid activation functions are given by the following equation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数由以下方程给出：
- en: '![Sigmoid function](img/B05137_07_021.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Sigmoid函数](img/B05137_07_021.jpg)'
- en: It can be seen as a bounded, strictly increasing and positive transformation
    function that squashes the values between 0 and 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以被视为一个有界、严格递增且正的变换函数，将值压缩在0和1之间。
- en: Hyperbolic tangent ("tanh") function
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双曲正切("tanh")函数
- en: 'The Tanh function is given by the following equation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh函数由以下方程给出：
- en: '![Hyperbolic tangent ("tanh") function](img/B05137_07_022.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![双曲正切("tanh")函数](img/B05137_07_022.jpg)'
- en: It can be seen as bounded, strictly increasing, but as a positive or negative
    transformation function that squashes the values between -1 and 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以被视为有界、严格递增的，但作为一个将值压缩在-1和1之间的正或负变换函数。
- en: Training neural network
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: In this section, we will discuss the key elements of training neural networks
    from input training sets, in much the same fashion as we did in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*. The dataset is denoted by *D* and
    consists of individual data instances. The instances are normally represented
    as the set ![Training neural network](img/B05137_07_024.jpg). The labels for each
    instance are represented as the set ![Training neural network](img/B05137_07_025.jpg).
    The entire labeled dataset with numeric or real-valued features is represented
    as paired elements in a set as given by ![Training neural network](img/B05137_07_026.jpg).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论从输入训练集训练神经网络的要素，这与我们在[第2章](ch02.html "第2章. 实际应用世界监督学习")中讨论的方式非常相似，即*实际应用世界监督学习*。数据集用
    *D* 表示，由单个数据实例组成。实例通常表示为![训练神经网络](img/B05137_07_024.jpg)的集合。每个实例的标签表示为![训练神经网络](img/B05137_07_025.jpg)的集合。整个带标签的数据集，具有数值或实值特征，表示为集合中的配对元素，如![训练神经网络](img/B05137_07_026.jpg)所示。
- en: Empirical risk minimization
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经验风险最小化
- en: Empirical risk minimization is a general machine learning concept that is used
    in many classifications or supervised learning. The main idea behind this technique
    is to convert a training or learning problem into an optimization problem (*References*
    [13]).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经验风险最小化是一个通用的机器学习概念，在许多分类或监督学习中得到应用。这种技术背后的主要思想是将训练或学习问题转化为一个优化问题（*参考文献* [13]）。
- en: Given the parameters for a neural network as **?** = ({**W**¹, **W**², … **W**
    *^l* ^(+1)}, {**b**¹, **b**², …**b** *^L* ^(+1)}) the training problem can be
    seen as finding the best parameters (*?*)such that
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定神经网络的参数为 **?** = ({**W**¹, **W**², … **W** *^l* ^(+1)}, {**b**¹, **b**², …**b**
    *^L* ^(+1)})，训练问题可以看作是寻找最佳参数 (*?*)，使得
- en: '![Empirical risk minimization](img/B05137_07_030.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![经验风险最小化](img/B05137_07_030.jpg)'
- en: 'Where ![Empirical risk minimization](img/B05137_07_031.jpg) Stochastic gradient
    descent (SGD) discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach
    to Real-World Supervised Learning"), *Practical Approach to Real-World Supervised
    Learning* and [Chapter 5](ch05.html "Chapter 5. Real-Time Stream Machine Learning"),
    *Real-time Stream Machine Learning,* is commonly used as the optimization procedure.
    The SGD applied to training neural networks is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![经验风险最小化](img/B05137_07_031.jpg) 随机梯度下降（SGD）在第2章（ch02.html "第2章. 实际应用世界监督学习"）和第5章（ch05.html
    "第5章. 实时流机器学习"）中讨论，*实际应用世界监督学习*和*实时流机器学习*，通常用作优化过程。应用于训练神经网络的SGD如下：
- en: initialize **?** = ({**W**¹, **W**², … **W***^l* ^(+1)}, {**b**¹, **b**², …**b***^L*
    ^(+1)})
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 **?** = ({**W**¹, **W**², … **W***^l* ^(+1)}, {**b**¹, **b**², …**b***^L*
    ^(+1)})
- en: for i=1 to *N* epochs
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 i=1 到 *N* 个epoch
- en: for each training sample (**x**^t, *y*^t) ![Empirical risk minimization](img/B05137_07_036.jpg)//
    find the gradient of function 2 ?= ?+ a? //move in direction
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练样本（**x**^t，*y*^t）![经验风险最小化](img/B05137_07_036.jpg)//找到函数2的梯度 ?= ?+ a?
    //沿方向移动
- en: The learning rate used here (a) will impact the algorithm convergence by reducing
    the oscillation near the optimum; choosing the right value of a is often a hyper
    parameter search that needs the validation techniques described in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的学习率（a）将通过减少接近最优解的振荡来影响算法的收敛；选择正确的a值通常是一个超参数搜索，需要使用[第2章](ch02.html "第2章.
    实际应用世界监督学习")中描述的验证技术。
- en: Thus, to learn the parameters of a neural network, we need to choose a way to
    do parameter initialization, select a loss function ![Empirical risk minimization](img/B05137_07_040.jpg),
    compute the parameter gradients ![Empirical risk minimization](img/B05137_07_041.jpg),
    propagate the losses back, select the regularization/penalty function O(*?*),
    and compute the gradient of regularization ![Empirical risk minimization](img/B05137_07_043.jpg).
    In the next few sections, we will describe this step by step.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了学习神经网络的参数，我们需要选择一种参数初始化的方法，选择一个损失函数 ![经验风险最小化](img/B05137_07_040.jpg)，计算参数梯度
    ![经验风险最小化](img/B05137_07_041.jpg)，将损失反向传播，选择正则化/惩罚函数 O(*?*)，并计算正则化的梯度 ![经验风险最小化](img/B05137_07_043.jpg)。在接下来的几节中，我们将逐步描述这一过程。
- en: Parameter initialization
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数初始化
- en: The parameters of neural networks are the weights and biases of each layer from
    the input layer, through hidden layers, to the output layer. There has been much
    research in this area as the optimization depends on the start or initialization.
    Biases are generally set to value 0\. The weight initialization depends on the
    activation functions as some, such as tanh, value 0, cannot be used. Generally,
    the way to initialize the weights of each layer is by random initialization using
    a symmetric function with a user-defined boundary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的参数是输入层、通过隐藏层到输出层的每一层的权重和偏置。在这个领域已经进行了很多研究，因为优化取决于起始或初始化。偏置通常设置为0。权重初始化取决于激活函数，因为一些函数，如tanh，其值0不能使用。通常，初始化每一层权重的办法是使用具有用户定义边界的对称函数进行随机初始化。
- en: Loss function
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function's main role is to maximize how well the predicted output label
    matches the class of the input data vector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的主要作用是最大化预测输出标签与输入数据向量类别的匹配程度。
- en: 'Thus, maximization ![Loss function](img/B05137_07_044.jpg) is equivalent to
    minimizing the negative of the log-likelihood or cross-entropy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最大化 ![损失函数](img/B05137_07_044.jpg) 等同于最小化负对数似然或交叉熵：
- en: '![Loss function](img/B05137_07_045.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/B05137_07_045.jpg)'
- en: Gradients
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 梯度
- en: We will describe gradients at the output layer and the hidden layer without
    going into the derivation as it is beyond the scope of this book. Interested readers
    can see the derivation in the text by Rumelhart, Hinton and Williams (*References*
    [6]).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述输出层和隐藏层的梯度，而不涉及推导，因为这超出了本书的范围。感兴趣的读者可以在Rumelhart、Hinton和Williams的文本中看到推导（*参考文献*
    [6]）。
- en: Gradient at the output layer
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 输出层的梯度
- en: 'Gradient at the output layer can be calculated as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的梯度可以计算为：
- en: '![Gradient at the output layer](img/B05137_07_046.jpg)![Gradient at the output
    layer](img/B05137_07_047.jpg)![Gradient at the output layer](img/B05137_07_048.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![输出层的梯度](img/B05137_07_046.jpg)![输出层的梯度](img/B05137_07_047.jpg)![输出层的梯度](img/B05137_07_048.jpg)'
- en: Where *e(y)* is called the "one hot vector" where only one value in the vector
    is 1 corresponding to the right class *y* and the rest are 0.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *e(y)* 被称为“独热向量”，其中向量的一个值是1，对应正确的类别 *y*，其余都是0。
- en: 'The gradient at the output layer pre-activation can be calculated similarly:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层预激活的梯度可以类似地计算：
- en: '![Gradient at the output layer](img/B05137_07_051.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![输出层的梯度](img/B05137_07_051.jpg)'
- en: = – (**e**(y) – **f**(**x**))
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: = – (**e**(y) – **f**(**x**))
- en: Gradient at the Hidden Layer
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 隐藏层梯度
- en: A hidden layer gradient is computed using the chain rule of partial differentiation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层梯度是使用偏微分链式法则计算的。
- en: Gradient at the hidden layer ![Gradient at the Hidden Layer](img/B05137_07_053.jpg)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层梯度 ![隐藏层的梯度](img/B05137_07_053.jpg)
- en: '![Gradient at the Hidden Layer](img/B05137_07_054.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏层的梯度](img/B05137_07_054.jpg)'
- en: 'Gradient at the hidden layer pre-activation can be shown as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层预激活的梯度可以表示为：
- en: '![Gradient at the Hidden Layer](img/B05137_07_055.jpg)![Gradient at the Hidden
    Layer](img/B05137_07_056.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏层的梯度](img/B05137_07_055.jpg)![隐藏层的梯度](img/B05137_07_056.jpg)'
- en: Since the hidden layer pre-activation needs partial derivatives of the activation
    functions as shown previously (*g'*(*a*^k**x**[j])), some of the well-known activation
    functions described previously have partial derivatives in terms of the equation
    itself, which makes computation very easy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于隐藏层预激活需要如前所述的激活函数的偏导数（*g'*(*a*^k**x**[j]）），因此之前描述的一些著名激活函数在方程本身中有偏导数，这使得计算非常容易。
- en: For example, the partial derivative of the sigmoid function is *g'(a) = g(a)(*1
    *– g(a))* and, for the tanh function, it is 1 – *g*(a)².
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，sigmoid 函数的偏导数为 *g'(a) = g(a)(*1 *– g(a))*，而对于 tanh 函数，它是 1 – *g*(a)²。
- en: Parameter gradient
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 参数梯度
- en: 'The loss gradient of parameters must be computed using gradients of weights
    and biases. Gradient of weights can be shown as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的损失梯度必须使用权重和偏差的梯度来计算。权重的梯度可以表示为：
- en: '![Parameter gradient](img/B05137_07_060.jpg)![Parameter gradient](img/B05137_07_061.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参数梯度](img/B05137_07_060.jpg)![参数梯度](img/B05137_07_061.jpg)'
- en: 'Gradient of biases can be shown as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差的梯度可以表示为：
- en: '![Parameter gradient](img/B05137_07_062.jpg)![Parameter gradient](img/B05137_07_063.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参数梯度](img/B05137_07_062.jpg)![参数梯度](img/B05137_07_063.jpg)'
- en: Feed forward and backpropagation
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 前向传播和反向传播
- en: The aim of neural network training is to adjust the weights and biases at each
    layer so that, based on the feedback from the output layer and the loss function
    that estimates the difference between the predicted output and the actual output,
    that difference is minimized.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练的目的是调整每一层的权重和偏差，以便基于输出层的反馈和损失函数（该函数估计预测输出与实际输出之间的差异），最小化这种差异。
- en: 'The neural network algorithm based on initial weights and biases can be seen
    as forwarding the computations layer by layer as shown in the acyclic flow graph
    with one hidden layer to demonstrate the flow:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于初始权重和偏差的神经网络算法可以看作是按层逐层前向计算，如图所示的单隐藏层无环流程图，以演示流程：
- en: '![Feed forward and backpropagation](img/B05137_07_064.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播和反向传播](img/B05137_07_064.jpg)'
- en: 'Figure 3: Neural network flow as a graph in feed forward.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：前向传播中神经网络作为图的流程。
- en: 'From the input vector and pre-initialized values of weights and biases, each
    subsequent element is computed: the pre-activation, hidden layer output, final
    layer pre-activation, final layer output, and loss function with respect to the
    actual label. In backward propagation, the flow is exactly reversed, from the
    loss at the output down to the weights and biases of the first layer, as shown
    in the following figure:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入向量和预初始化的权重和偏差值开始，计算后续的每个元素：预激活、隐藏层输出、最终层预激活、最终层输出以及相对于实际标签的损失函数。在反向传播中，流向正好相反，从输出层的损失到第一层的权重和偏差，如下图所示：
- en: '![Feed forward and backpropagation](img/B05137_07_065.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播和反向传播](img/B05137_07_065.jpg)'
- en: 'Figure 4: Neural network flow as a graph in back propagation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：反向传播中神经网络作为图的流程。
- en: How does it work?
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'The backpropagation algorithm (*References* [6 and 7]) in its entirety can
    be summarized as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法（*参考文献* [6 和 7]）整体上可以总结如下：
- en: 'Compute the output gradient before activation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 计算激活前的输出梯度：
- en: '![How does it work?](img/B05137_07_066.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_066.jpg)'
- en: 'For hidden layers *k=l+1 to 1*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层 *k=l+1 到 1*：
- en: 'Compute the gradient of hidden layer parameters:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 计算隐藏层参数的梯度：
- en: '![How does it work?](img/B05137_07_068.jpg)![How does it work?](img/B05137_07_069.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_068.jpg)![它是如何工作的？](img/B05137_07_069.jpg)'
- en: 'Compute the gradient of the hidden layer below the current:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 计算当前隐藏层以下的隐藏层梯度：
- en: '![How does it work?](img/B05137_07_070.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_070.jpg)'
- en: 'Compute the gradient of the layer before activation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 计算激活前的层梯度：
- en: '![How does it work?](img/B05137_07_071.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_071.jpg)'
- en: Regularization
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正则化
- en: In the empirical risk minimization objective defined previously, regularization
    is used to address the over-fitting problem in machine learning as introduced
    in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*. The well-known
    regularization functions are given as follows.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前定义的经验风险最小化目标中，正则化用于解决机器学习中的过拟合问题，如[第二章](ch02.html "第二章. 实际应用中的监督学习")《实际应用中的监督学习》中所述。以下给出了众所周知的正则化函数。
- en: L2 regularization
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: L2 正则化
- en: 'This is applied only to the weights and not to the biases and is given for
    layers connecting (*i,j*) components as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这只应用于权重，而不应用于偏差，并且对于连接 (*i,j*) 元件的层给出如下：
- en: '![L2 regularization](img/B05137_07_073.jpg)![L2 regularization](img/B05137_07_074.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![L2 正则化](img/B05137_07_073.jpg)![L2 正则化](img/B05137_07_074.jpg)'
- en: Also, the gradient of the regularizer can be computed as ![L2 regularization](img/B05137_07_075.jpg).
    They are often interpreted as the "Gaussian Prior" over the weight distribution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正则化器的梯度可以计算为 ![L2 正则化](img/B05137_07_075.jpg)。它们通常被解释为权重分布上的“高斯先验”。
- en: L1 regularization
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: L1 正则化
- en: 'This is again applied only to the weights and not to the biases and is given
    for layers connecting *(i,j)* components as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这只应用于权重，而不应用于偏差，并且对于连接 *(i,j)* 组件的层给出如下：
- en: '![L1 regularization](img/B05137_07_076.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![L1 正则化](img/B05137_07_076.jpg)'
- en: And the gradient of this regularizer can be computed as ![L1 regularization](img/B05137_07_077.jpg).
    It is often interpreted as the "Laplacian Prior" over the weight distribution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个正则化器的梯度可以计算为 ![L1 正则化](img/B05137_07_077.jpg)。它通常被解释为权重分布上的“拉普拉斯先验”。
- en: Limitations of neural networks
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的局限性
- en: In this section, we will discuss in detail the issues faced by neural networks,
    which will become the stepping stone for building deep learning networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细讨论神经网络面临的问题，这将成为构建深度学习网络的垫脚石。
- en: Vanishing gradients, local optimum, and slow training
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度、局部最优和慢速训练
- en: 'One of the major issues with neural networks is the problem of "vanishing gradient"
    (*References* [8]). We will try to give a simple explanation of the issue rather
    than exploring the mathematical derivations in depth. We will choose the sigmoid
    activation function and a two-layer neural network, as shown in the following
    figure, to demonstrate the issue:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的主要问题之一是“消失梯度”问题 (*参考文献* [8])。我们将尝试给出对该问题的简单解释，而不是深入探讨数学推导。我们将选择 sigmoid
    激活函数和两层神经网络，如图所示，以演示该问题：
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_078.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度、局部最优和慢速训练](img/B05137_07_078.jpg)'
- en: 'Figure 5: Vanishing Gradient issue.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：消失梯度问题。
- en: As we saw in the activation function description, the sigmoid function squashes
    the output between the range 0 and 1\. The derivative of the sigmoid function
    *g'(a) = g(a)(*1 *– g(a))* has a range between 0 and 0.25\. The goal of learning
    is to minimize the output loss, that is, ![Vanishing gradients, local optimum,
    and slow training](img/B05137_07_079.jpg). In general, the output error does not
    go to 0, so maximum iterations; a user-specified parameter determines the quality
    of learning and backpropagation of the errors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在激活函数描述中看到的，sigmoid 函数将输出压缩在 0 和 1 之间。sigmoid 函数的导数 *g'(a) = g(a)(*1 *–
    g(a))* 的范围在 0 和 0.25 之间。学习的目标是使输出损失最小化，即 ![消失梯度、局部最优和慢速训练](img/B05137_07_079.jpg)。一般来说，输出误差不会降到
    0，所以最大迭代次数；一个用户指定的参数决定了学习的质量和误差的反向传播。
- en: 'Simplifying to illustrate the effect of output error on the input weight layer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 简化以说明输出误差对输入权重层的影响：
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_080.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度、局部最优和慢速训练](img/B05137_07_080.jpg)'
- en: 'Each of the transformations, for instance, from output to hidden, involves
    multiplication of two terms, both less than 1:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 每个转换，例如，从输出到隐藏，涉及两个小于 1 的项的乘法：
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_081.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度、局部最优和慢速训练](img/B05137_07_081.jpg)'
- en: Thus, the value becomes so small when it reaches the input layer that the propagation
    of the gradient has almost vanished. This is known as the vanishing gradient problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当它达到输入层时，值变得如此之小，以至于梯度的传播几乎消失。这被称为消失梯度问题。
- en: A paradoxical situation arises when you need to add more layers to make features
    more interesting in the hidden layers. But adding more layers also increases the
    errors. As you add more layers, the input layers become "slow to train," which
    causes the output layers to be more inaccurate as they are dependent on the input
    layers; further, and for the same number of iterations, the errors increase with
    the increase in the number of layers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要添加更多层来使隐藏层中的特征更加有趣时，就会出现一个矛盾的情况。但添加更多层也会增加错误。随着你添加更多层，输入层变得“训练缓慢”，这导致输出层更加不准确，因为它们依赖于输入层；进一步地，对于相同的迭代次数，随着层数的增加，错误也会增加。
- en: With a fixed number of maximum iterations, more layers and slow propagation
    of errors can lead to a "local optimum."
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定最大迭代次数的情况下，更多的层和缓慢的误差传播可能导致“局部最优”。
- en: Another issue with basic neural networks is the number of parameters. Finding
    effective size and weights for each hidden layer and bias becomes more challenging
    with the increase in the number of layers. If we increase the number of layers,
    the parameters increase in polynomials. Fitting the parameters for the data requires
    a large number of data samples. This can result in the problem discussed before,
    that is, overfitting.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will start learning about the building blocks of
    deep learning that help overcome these issues.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning includes architectures and techniques for supervised and unsupervised
    learning with the capacity to internalize the abstract structure of high-dimensional
    data using networks composed of building blocks to create discriminative or generative
    models. These techniques have proved enormously successful in recent years and
    any reader interested in mastering them must become familiar with the basic building
    blocks of deep learning first and understand the various types of networks in
    use by practitioners. Hands-on experience building and tuning deep neural networks
    is invaluable if you intend to get a deeper understanding of the subject. Deep
    learning, in various domains such as image classification and text learning, incorporates
    feature generation in its structures thus making the task of mining the features
    redundant in many applications. The following sections provide a guide to the
    concepts, building blocks, techniques for composing architectures, and training
    deep networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks for deep learning
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following sections, we introduce the most important components used in
    deep learning, including Restricted Boltzmann machines, Autoencoders, and Denoising
    Autoencoders, how they work, and their advantages and limitations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear activation function
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Reclin function is given by the equation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '*g*(*a*) *= reclin* (*a*) *= max* (0*,* *a*)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen as having a lower bound of 0 and no upper bound, strictly increasing,
    and a positive transformation function that just does linear transformation of
    positives.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: It is easier to see that the rectified linear unit or ReLu has a derivative
    of 1 or identity for values greater than 0\. This acts as a significant benefit
    as the derivatives are not squashed and do not have diminishing values when chained.
    One of the issues with ReLu is that the value is 0 for negative inputs and the
    corresponding neurons act as "dead", especially when a large negative value is
    learned for the bias term. ReLu cannot recover from this as the input and derivative
    are both 0\. This is generally solved by having a "leaky ReLu". These functions
    have a small value for negative inputs and are given by ![Rectified linear activation
    function](img/B05137_07_083.jpg) where ? = 0.01, typically.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines (RBM) is an unsupervised learning neural network
    (*References* [11]). The idea of RBM is to extract "more meaningful features"
    from labeled or unlabeled data. It is also meant to "learn" from the large quantity
    of unlabeled data available in many domains when getting access to labeled data
    is costly or difficult.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机（RBM）是一种无监督学习神经网络（*参考文献* [11]）。RBM的想法是从标记或未标记数据中提取“更有意义的特征”。它还旨在在获取标记数据成本高昂或困难时，从许多领域中的大量未标记数据中“学习”。
- en: Definition and mathematical notation
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义和数学符号
- en: In its basic form RBM assumes inputs to be binary values 0 or 1 in each dimension.
    RBMs are undirected graphical models having two layers, a visible layer represented
    as *x* and a hidden layer *h,* and connections *W*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在其基本形式中，RBM假设每个维度上的输入是二进制值0或1。RBM是无向图模型，具有两层，一个表示为*x*的可见层和一个隐藏层*h*，以及连接*W*。
- en: 'RBM defines a distribution over the visible layer that involves the latent
    variables from the hidden layer. First an energy function is defined to capture
    the relationship between the visible and the hidden layers in vector form as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: RBM定义了一个涉及隐藏层潜在变量的可见层分布。首先定义一个能量函数来捕捉可见层和隐藏层之间在向量形式上的关系：
- en: '![Definition and mathematical notation](img/B05137_07_088.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_088.jpg)'
- en: 'In scalar form the energy function can be defined as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在标量形式中，能量函数可以定义为：
- en: '![Definition and mathematical notation](img/B05137_07_089.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_089.jpg)'
- en: The probability of the distribution is given by ![Definition and mathematical
    notation](img/B05137_07_090.jpg) where *Z* is called the "partitioning function",
    which is an enumeration over all the values of *x and h*, which are binary, resulting
    in exponential terms and thus making it intractable!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的概率由![分布的定义和数学符号](img/B05137_07_090.jpg)给出，其中*Z*被称为“配分函数”，它是对所有* x 和 h*值的枚举，它们是二进制值，导致指数项，因此使其难以处理！
- en: '![Definition and mathematical notation](img/B05137_07_093.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_093.jpg)'
- en: 'Figure 6: Connection between the visible layer and hidden layer.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：可见层和隐藏层之间的连接。
- en: 'The Markov network view of the same in scalar form can be represented using
    all the pairwise factors, as shown in the following figure. This also makes it
    clear why it is called a "restricted" Boltzmann machine as there is no connection
    among units within a given hidden layer or in the visible layers:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 相同内容的马尔可夫网络视图可以用所有成对因子表示，如下图所示。这也清楚地说明了为什么它被称为“受限”玻尔兹曼机，因为给定隐藏层或可见层内的单元之间没有连接：
- en: '![Definition and mathematical notation](img/B05137_07_094.jpg)![Definition
    and mathematical notation](img/B05137_07_095.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_094.jpg)![分布的定义和数学符号](img/B05137_07_095.jpg)'
- en: 'Figure 7: Input and hidden layers as scalars'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：输入和隐藏层作为标量
- en: We have seen that the whole probability distribution function ![Definition and
    mathematical notation](img/B05137_07_090.jpg) is intractable. We will now derive
    the basic conditional probability distributions for *x, h*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，整个概率分布函数![分布的定义和数学符号](img/B05137_07_090.jpg)难以处理。现在我们将推导出*x, h*的基本条件概率分布。
- en: Conditional distribution
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 条件分布
- en: 'Although computing the whole *p(x, h)* is intractable, the conditional distribution
    of *p(x|h)* or *p(h|x)* can be easily defined and shown to be a Bernoulli distribution
    and tractable:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算整个*p(x, h)*是难以处理的，但*p(x|h)*或*p(h|x)*的条件分布可以很容易地定义并证明是伯努利分布且可处理：
- en: '![Conditional distribution](img/B05137_07_100.jpg)![Conditional distribution](img/B05137_07_101.jpg)![Conditional
    distribution](img/B05137_07_102.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![条件分布](img/B05137_07_100.jpg)![条件分布](img/B05137_07_101.jpg)![条件分布](img/B05137_07_102.jpg)'
- en: 'Similarly, being symmetric and undirected:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，由于对称和无向性：
- en: '![Conditional distribution](img/B05137_07_103.jpg)![Conditional distribution](img/B05137_07_104.jpg)![Conditional
    distribution](img/B05137_07_105.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![条件分布](img/B05137_07_103.jpg)![条件分布](img/B05137_07_104.jpg)![条件分布](img/B05137_07_105.jpg)'
- en: Free energy in RBM
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RBM中的自由能
- en: 'The distribution of input or the observed variable is:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输入或观察变量的分布是：
- en: '![Free energy in RBM](img/B05137_07_106.jpg)![Free energy in RBM](img/B05137_07_107.jpg)![Free
    energy in RBM](img/B05137_07_108.jpg)![Free energy in RBM](img/B05137_07_109.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![RBM中的自由能](img/B05137_07_106.jpg)![RBM中的自由能](img/B05137_07_107.jpg)![RBM中的自由能](img/B05137_07_108.jpg)![RBM中的自由能](img/B05137_07_109.jpg)'
- en: The function *F(x)* is called free energy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*F(x)*被称为自由能。
- en: Training the RBM
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练RBM
- en: 'RBMs are trained using the optimization objective of minimizing the average
    negative log-likelihood over the entire training data. This can be represented
    as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the RBM](img/B05137_07_111.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'The optimization is carried out by using stochastic gradient descent:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the RBM](img/B05137_07_112.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: The term ![Training the RBM](img/B05137_07_113.jpg) is called the "positive
    phase" and the term ![Training the RBM](img/B05137_07_114.jpg) is called the "negative
    phase" because of how they affect the probability distributions—the positive phase,
    because it increases the probability of training data by reducing the free energy,
    and the negative phase, as it decreases the probability of samples generated by
    the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown that the overall gradient is difficult to compute analytically
    because of the "negative phase", as it is computing the expectation over all possible
    configurations of the input data under the distribution formed by the model and
    making it intractable!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: To make the computation tractable, estimation is carried out using a fixed number
    of model samples and they are referred to as "negative particles" denoted by *N*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient can be now written as the approximation:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the RBM](img/B05137_07_116.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Where particles ![Training the RBM](img/B05137_07_117.jpg) are sampled using
    some sampling techniques such as the Monte Carlo method.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Sampling in RBM
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gibbs sampling is often the technique used to generate samples and learn the
    probability of *p(x,h)* in terms of *p(x|h)* and *p (h|x)*, which are relatively
    easy to compute, as shown previously.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Gibbs sampling for joint sampling of N random variables ![Sampling in RBM](img/B05137_07_119.jpg)
    is done using N sampling sub-steps of the form ![Sampling in RBM](img/B05137_07_120.jpg)
    where *S* *[-i]* contains samples up to and excluding step *S* *[i]*. Graphically,
    this can be shown as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling in RBM](img/B05137_07_123.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Graphical representation of sampling done between hidden and input
    layers.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: As ![Sampling in RBM](img/B05137_07_124.jpg) it can be shown that the sampling
    represents the actual distribution *p(x,h)*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive divergence
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contrastive divergence (CD) is a trick used to expedite the Gibbs sampling process
    described previously so it stops at step *k* of the process rather than continuing
    for a long time to guarantee convergence. It has been seen that even *k=1* is
    reasonable and gives good performance (*References* [10]).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These are the inputs to the algorithm:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Training dataset
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of steps for Gibbs sampling, *k*
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate a
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is the set of updated parameters
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The complete training pseudo-code using CD with the free energy function and
    partial derivatives can be given as:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'For each instance in training **x**^t:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a negative particle ![How does it work?](img/B05137_07_117.jpg) using
    *k* steps of Gibbs Sampling.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the parameters:'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_130.jpg)![How does it work?](img/B05137_07_131.jpg)![How
    does it work?](img/B05137_07_132.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Persistent contrastive divergence
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Persistent contrastive divergence is another trick used to compute the joint
    probability *p(x,h)*. In this method, there is a single chain that does not reinitialize
    after every observed sample to find the negative particle ![Persistent contrastive
    divergence](img/B05137_07_117.jpg). It persists its state and parameters are updated
    just through running these k states by using the particle from the previous step.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An autoencoder is another form of unsupervised learning technique in neural
    networks. It is very similar to the feed-forward neural network described at the
    start with the only difference being it doesn't generate a class at output, but
    tries to replicate the input at the output layer (*References* [12 and 23]). The
    goal is to have hidden layer(s) capture the latent or hidden information of the
    input as features that can be useful in unsupervised or supervised learning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Definition and mathematical notations
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A single hidden layer example of an Autoencoder is shown in the following figure:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_133.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Autoencoder flow between layers'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The input layer and the output layer have the same number of neurons similar
    as feed-forward, corresponding to the input vector, *x*. Each hidden layer can
    have greater, equal, or fewer neurons than the input or output layer and an activation
    function that does a non-linear transformation of the signal. It can be seen as
    using the unsupervised or latent hidden structure to "compress" the data effectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder or input transformation of the data by the hidden layer is given
    by:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_135.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'And the decoder or output transformation of the data by the output layer is
    given by:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_136.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'Generally, a sigmoid function with linear transformation of signals as described
    in the neural network section is popularly used in the layers:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_137.jpg) and ![Definition
    and mathematical notations](img/B05137_07_138.jpg))'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Loss function
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The job of the loss function is to reduce the training error as before so that
    an optimization process such as a stochastic gradient function can be used.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of binary valued input, the loss function is generally the average
    cross-entropy given by:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_139.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'It can be easily verified that, when the input signal and output signal match
    either 0 or 1, the error is 0\. Similarly, for real-valued input, a squared error
    is used:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_140.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'The gradient of the loss function that is needed for the stochastic gradient
    procedure is similar to the feed-forward neural network and can be shown through
    derivation for both real-valued and binary as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_141.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Parameter gradients are obtained by back-propagating the ![Loss function](img/B05137_07_142.jpg)
    exactly as in the neural network.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Autoencoders
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Autoencoders have some known drawbacks that have been addressed by specialized
    architectures that we will discuss in the sections to follow. These limitations
    are:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the Autoencoder is equal to the number of neurons in the input,
    there is a chance that the weights learned by the Autoencoders are just the identity
    vectors and that the whole representation simply passes on the inputs exactly
    as outputs with zero loss. Thus, they emulate "rote learning" or "memorization"
    without any generalization.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the Autoencoder is greater than the number of neurons in the
    input, the configuration is called an "overcomplete" hidden layer and can have
    similar problems to the ones mentioned previously. Some of the units can be turned
    off and others can become identity making it just the copy unit.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the Autoencoder is less than the number of neurons in the input,
    known as "undercomplete", the latent structure in the data or important hidden
    components can be discovered.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Autoencoder
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned previously, when the Autoencoder has a hidden layer size greater
    than or equal to that of the input, it is not guaranteed to learn the weights
    and can become simply a unit switch to copy input to output. This issue is addressed
    by the Denoising Autoencoder. Here there is another layer added between input
    and the hidden layer. This layer adds some noise to the input using either a well-known
    distribution ![Denoising Autoencoder](img/B05137_07_143.jpg) or using stochastic
    noise such as turning a bit to 0 in binary input. This "noisy" input then goes
    through learning from the hidden layer to the output layer exactly like the Autoencoder.
    The loss function of the Denoising Autoencoder compares the output with the actual
    input. Thus, the added noise and the larger hidden layer enable either learning
    latent structures or adding/removing redundancy to produce the exact signal at
    the output. This architecture—where non-zero features at the noisy layer generate
    features at the hidden layer that are themselves transformed by the activation
    layer as the signal advances forward—lends a robustness and implicit structure
    to the learning press (*References* [15]).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoder](img/B05137_07_144.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Denoising Autoencoder'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised pre-training and supervised fine-tuning
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the issues section on neural networks, the issue with over-training
    arises especially in deep learning as the number of layers, and hence parameters,
    is large. One way to account for over-fitting is to do data-specific regularization.
    In this section, we will describe the "unsupervised pre-training" method done
    in the hidden layers to overcome the issue of over-fitting. Note that this is
    generally the "initialization process" used in many deep learning algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm of unsupervised pre-training works in a layer-wise greedy fashion.
    As shown in the following figure, one layer of a visible and hidden structure
    is considered at a given time. The weights of this layer are learned for a few
    iterations using unsupervised techniques such as RBM, described previously. The
    output of the hidden layer is then used as a "visible" or "input" layer and the
    training proceeds to the next, and so on.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Each learning of layers can be thought of as a "feature extraction or feature
    generation" process. The real data inputs when transformed form higher-level features
    at a given layer and then are further combined to form much higher-level features,
    and so on.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised pre-training and supervised fine-tuning](img/B05137_07_145.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Layer wise incremental learning through unsupervised learning.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Once all the hidden layer parameters are learned in pre-training using unsupervised
    techniques as described previously, a supervised fine-tuning process follows.
    In the supervised fine-tuning process, a final output layer is added and, just
    like in a neural network, training is done with forward and backward propagation.
    The idea is that most weights or parameters are almost fully tuned and only need
    a small change for producing a discriminative class mapping at the output.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised pre-training and supervised fine-tuning](img/B05137_07_146.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Final tuning or supervised learning.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Deep feed-forward NN
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deep feed-forward neural network involves using the stages pre-training, and
    fine-tuning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the unsupervised learning technique used—RBM, Autoencoders, or
    Denoising Autoencoders—different algorithms are formed: Stacked RBM, Stacked Autoencoders,
    and Stacked Denoising Autoencoders, respectively.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Input and outputs
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given an architecture for the deep feed-forward neural net, these are the inputs
    for training the network:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Number of layers *L*
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset without labels *D*
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset with labels *D*
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of training iterations *n*
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The generalized learning/training algorithm for all three is given as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'For layers *l=1 to L* (Pre-Training):'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset without Labels ![How does it work?](img/B05137_07_151.jpg)
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform Step-wise Layer Unsupervised Learning (RBM, Autoencoders, or Denoising
    Autoencoders)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finalize the parameters **W**^l, **b**^l from the preceding step
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the output layer *(L+1)* perform random initialization of parameters **W***^L*^(+1),
    **b***^L* ^(+1).
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For layers *l=1 to L+1* (Fine-Tuning):'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset with Labels ![How does it work?](img/B05137_07_156.jpg).
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the pre-initialized weights from 1\. (**W**^l, **b**^l).
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform forward-backpropagation for *n* iterations.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Autoencoders
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep Autoencoders have many layers of hidden units, which shrink to a very small
    dimension and then symmetrically grow to the input size.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Autoencoders](img/B05137_07_158.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Deep Autoencoders'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind Deep Autoencoders is to create features that capture latent
    complex structures of input using deep networks and at the same time overcome
    the issue of gradients and underfitting due to the deep structure. It was shown
    that this methodology generated better features and performed better than PCA
    on many datasets (*References* [13]).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Autoencoders use the concept of pre-training, encoders/decoders, and fine-tuning
    to perform unsupervised learning:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'In the pre-training phase, the RBM methodology is used to learn greedy stepwise
    parameters of the encoders, as shown in the following figure, for initialization:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Autoencoders](img/B05137_07_159.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Stepwise learning in RBM'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: In the unfolding phase the same parameters are symmetrically applied to the
    decoder network for initialization.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Finally, fine-tuning backpropagation is used to adjust the parameters across
    the entire network.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Deep Belief Networks
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep Belief Networks (DBNs) are the origin of the concept of unsupervised pre-training
    (*References* [9]). Unsupervised pre-training originated from DBNs and then was
    found to be equally useful and effective in the feed-forward supervised deep networks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks are not supervised feed-forward networks, but a generative
    model to generate data samples.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input layer is the instance of data, represented by one neuron for each
    input feature. The output of a DBN is a reconstruction of the input from a hierarchy
    of learned features of increasingly greater abstraction.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How a DBN learns the joint distribution of the input data is explained here
    using a three-layer DBN architecture as an example.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_160.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Deep belief network'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: The three-hidden-layered DBN as shown has a first layer of undirected RBM connected
    to a two-layered Bayesian network. The Bayesian network with a sigmoid activation
    function is called a sigmoid Bayesian network (SBN).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a generative model is to learn the joint distribution as given by
    *p*(**x**,**h**^((1)),**h**^((2)),**h**^((3)))
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(**x**,**h**^((1)),**h**^((2)),**h**^((3))) = *p*(**h**²),**h**^((3)))*p*(**h**^((1))|**h**^((2)))
    *p*(**x**|**h**^((1)))'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'RBM computation as seen before gives us:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_163.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: 'The Bayesian Network in the next two layers is:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_164.jpg)![How does it work?](img/B05137_07_165.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'For binary data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_166.jpg)![How does it work?](img/B05137_07_166.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: Deep learning with dropouts
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another technique used to overcome the "overfitting" issues mentioned in deep
    neural networks is using the dropout technique to learn the parameters. In the
    next sections, we will define, illustrate, and explain how deep learning with
    dropouts works.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Definition and mathematical notation
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea behind dropouts is to "cripple" the deep neural network structure by
    stochastically removing some of the hidden units as shown in the following figure
    after the parameters are learned. The units are set to 0 with the dropout probability
    generally set as *p=0.5*
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: The idea is similar to adding noise to the input, but done in all the hidden
    layers. When certain features (or a combination of features) are removed stochastically,
    the neural network has to learn latent features in a more robust way, without
    the interdependence of some features.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_169.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Deep learning with dropout indicated by dropping certain units with
    dark shading.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Each hidden layer is represented by *h*^k*(x)* where *k* is the layer. The
    pre-activation for layer *0<k<l* is given by:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_016.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer activation for *1< k < l*. Binary masks are represented by
    **m**^k at each hidden layer:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_172.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: 'The final output layer activation is:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_019.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Inputs and outputs
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For training with dropouts, inputs are:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training dataset
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout probability *p* (typically 0.5)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is a trained deep neural net that can be applied for predictive use.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will now describe the different parts of how deep learning with dropouts
    works.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Learning Training and testing with dropouts
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The backward propagation learning of weights and biases from the output loss
    function using gradients is very similar to traditional neural network learning.
    The only difference is that masks are applied appropriately as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the output gradient before activation:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_066.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'For hidden layers *k=l+1 to 1*:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the gradient of hidden layer parameters:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_068.jpg)![Learning
    Training and testing with dropouts](img/B05137_07_069.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: '**h**^(k-1) computation has taken into account the binary mask **m**^(k-1)
    applied.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the gradient of the hidden layer below the current:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_070.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: 'Compute the gradient of the layer below before activation:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_177.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: When testing the model, we cannot use the binary mask as it is stochastic; the
    "expectation" value of the mask is used. If the dropout probability is *p=0.5*,
    the same value 0.5 is used as the expectation for the unit at test or model application
    time.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Sparse coding
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sparse coding is another neural network used for unsupervised learning and feature
    generation (*References* [22]). It works on the principle of finding latent structures
    in high dimensions that capture the patterns, thus performing feature extraction
    in addition to unsupervised learning.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, for every input **x**^((t)) a latent representation **h**^((t)) is
    learned, which has a sparse representation (most values are 0 in the vector).
    This is done by optimization using the following objective function:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparse coding](img/B05137_07_180.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: Where the first term ![Sparse coding](img/B05137_07_181.jpg) is to control the
    reconstruction error and the second term, which uses a regularizer ?, is for sparsity
    control. The matrix **D** is also known as a Dictionary as it has equivalence
    to words in a dictionary and **h**^((t)) is similar to word frequency; together
    they capture the impact of words in extracting patterns when performing text mining.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional Neural Networks or CNNs have become prominent and are widely used
    in the computer vision domain. Computer vision involves processing images/videos
    for capturing knowledge and patterns. Annotating images, classifying images/videos,
    correcting them, story-telling or describing images, and so on, are some of the
    broad applications in computer visi [16].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision problems most generally have to deal with unstructured data
    that can be described as:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Inputs that are 2D images with single or multiple color channels or 3D videos
    that are high-dimensional vectors.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The features in these 2D or 3D representations have a well-known spatial topology,
    a hierarchical structure, and some repetitive elements that can be exploited.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The images/videos have a large number of transformations or variants based on
    factors such as illumination, noise, and so on. The same person or car can look
    different based on several factors.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will describe some building blocks used in CNNs. We will use simple
    images such as the letter X of the alphabet to explain the concept and mathematics
    involved. For example, even though the same character X is represented in different
    ways in the following figure due to translation, scaling, or distortion, the human
    eye can easily read it as X, but it becomes tricky for the computer to see the
    pattern. The images are shown with the author''s permission (*References* [19]):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional Neural Network](img/B05137_07_184.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Image of character X represented in different ways.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates how a simple grayscale image of X has common
    features such as a diagonal from top left, a diagonal from top right, and left
    and right intersecting diagonals repeated and combined to form a larger X:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional Neural Network](img/B05137_07_185.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Common features represented in the image of character X.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Local connectivity
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the simple concept of dividing the whole image into "patches" or "recipient
    fields" and giving each patch to the hidden layers. As shown in the figure, instead
    of 9 X 9 pixels of the complete sample image, a 3 X 3 patch of pixels from the
    top left goes to the first hidden unit, the overlapping second patch goes to second,
    and so on.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Since the fully connected hidden layer would have a huge number of parameters,
    having smaller patches completely reduces the parameter or high-dimensional space
    problem!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![Local connectivity](img/B05137_07_186.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Concept of patches on the whole image.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Parameter sharing
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The concept of parameter sharing is to construct a weight matrix that can be
    reused over different patches or recipient fields as constructed in the preceding
    figure in the local sharing. As shown in the following figure, the Feature map
    with same parameters **W**[1,1] and **W**[1,4] creates two different feature maps,
    Feature Map 1 and 4, both capturing the same features, that is, diagonal edges
    on either side. Thus, feature maps capture "similar regions" in the images and
    further reduce the dimensionality of the input space.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![Parameter sharing](img/B05137_07_187.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Discrete convolution
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will explain the steps in discrete convolution, taking a simple contrived
    example with simplified mathematics to illustrate the operation.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the kernel representing the diagonal feature is scanned over the entire
    image as a patch of 3 X 3\. If this kernel lands on the self-same feature in the
    input image and we have to compute the center value through what we call the convolution
    operator, we get the exact value of 1 because of the matching as shown:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_190.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Discrete convolution step.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire image when run through this kernel and convolution operator gives
    a matrix of values as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_191.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Transformation of the character image after a kernel and convolution
    operator.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how the left diagonal feature gets highlighted by running this scan.
    Similarly, by running other kernels, as shown in the following figure, we can
    get a "stack of filtered images":'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_192.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Different features run through the kernel giving a stack of images.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cell in the filtered images can be given as:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_193.jpg)![Discrete convolution](img/B05137_07_194.jpg)![Discrete
    convolution](img/B05137_07_195.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: Pooling or subsampling
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pooling or subsampling works on the stack of filtered images to further shrink
    the image or compress it, while keeping the pattern as-is. The main steps carried
    out in pooling are:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Pick a window size (for example, 2 X 2) and a stride size (for example, 2).
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the window over all the filtered images at stride.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each window, pick the "maximum" value.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Pooling or subsampling](img/B05137_07_196.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: Max pooling, done using a window size of 2 X 2 and stride of 2,
    computes cell values with maximum for first as 1.0, 0.33 for next, and so on.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: 'Pooling also plays an important part where the same features if moved or scaled
    can still be detected due to the use of maximum. The same set of stacked filtered
    images gets transformed into pooled images as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling or subsampling](img/B05137_07_197.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: Transformation showing how a stack of filtered images is converted
    to pooled images.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Normalization using ReLU
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed in the building blocks of deep learning, ReLUs remove the negative
    by squashing it to 0 and keep the positives as-is. They also play an important
    role in gradient computation in the backpropagation, removing the vanishing gradient
    issue of vanishing gradient.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalization using ReLU](img/B05137_07_198.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: Transformation using ReLu.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: CNN Layers
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will put together the building blocks discussed earlier
    to form the complete picture of CNNs. Combining the layers of convolution, ReLU,
    and pooling to form a connected network yielding shrunken images with patterns
    captured in the final output, we obtain the next composite building block, as
    shown in the following figure:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_199.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Basic Unit of CNN showing a combination of Convolution, ReLu, and
    Pooling.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, these layers can be combined or "deep-stacked", as shown in the following
    figure, to form a complex network that gives a small pool of images as output:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_200.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: Deep-stacking the basic units repeatedly to form CNN layers.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: The output layer is a fully connected network as shown, which uses a voting
    technique and learns the weights for the desired output. The fully connected output
    layer can be stacked too.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_201.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: Fully connected layer as output of CNN.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the final CNNs can be completely illustrated as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_202.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: CNNS with all layers showing inputs and outputs.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: As before, gradient descent is selected as the learning technique using the
    loss functions to compute the difference and propagate the error backwards.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'CNN''s can be used in other domains such as voice pattern recognition, text
    mining, and so on, if the mapping of the data to the "image" can be successfully
    done and "local spatial" patterns exist. The following figure shows one of the
    ways of mapping sound and text to images for CNN usage:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_203.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: Illustration of mapping between temporal data, such as voice to
    spatial data, to an image.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normal deep networks are used when you have finite inputs and there is no interdependence
    between the input examples or instances. When there are variable length inputs
    and there are temporal dependencies between them, that is, sequence related data,
    neural networks must be modified to handle such data. Recurrent Neural Networks
    (RNN) are examples of neural networks that are used widely to solve such problems,
    and we will discuss them in the following sections. RNNs are used in many sequence-related
    problems such as text mining, language modeling, bioinformatics data modeling,
    and so on, to name a few areas that fit this meta-level description (*References*
    [18 and 21]).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Structure of Recurrent Neural Networks
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will describe the simplest unit of the RNN first and then shown how it is
    combined to understand it functionally and mathematically and illustrate how different
    components interact and work.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_204.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: 'Figure 32: Difference between an artificial neuron and a neuron with feedback.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the basic input, a neuron with activation, and its output at
    a given time *t*:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_206.jpg)![Structure
    of Recurrent Neural Networks](img/B05137_07_207.jpg)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
- en: 'A neuron with feedback keeps a matrix **W**[R] to incorporate previous output
    at time *t-1* and the equations are:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_210.jpg)![Structure
    of Recurrent Neural Networks](img/B05137_07_207.jpg)![Structure of Recurrent Neural
    Networks](img/B05137_07_211.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
- en: 'Figure 33: Chain of neurons with feedbacks connected together.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: The basic RNN stacks the structure of hidden units as shown with feedback connected
    from the previous layer. At activation at time *t*, it depends not only on **x**^((t))
    as input, but also on the previous unit given by **W**[R]**h**^((t-1)). The weights
    in the feedback connection of RNN are generally the same across all the units,
    **W**[R]. Also, instead of emitting output at the very end of the feed-forward
    neural network, each unit continuously emits an output that can be used in the
    loss function calculation.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Learning and associated problems in RNNs
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Working with RNNs presents some challenges that are specific to them but there
    are common problems that are also encountered in other types of neural net.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: The gradient used from the output loss function at any time *t* of the unit
    has dependency going back to the first unit or *t=0*, as shown in the following
    figure. This is because the partial derivative at the unit is dependent on the
    previous unit, since:![Learning and associated problems in RNNs](img/B05137_07_210.jpg)
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT) is the term used to illustrate the process.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Learning and associated problems in RNNs](img/B05137_07_215.jpg)'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 34: Backpropagation through time.'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to what we saw in the section on feed-forward neural networks, the cases
    of exploding and vanishing gradient become more pronounced in RNNs due to the
    connectivity of units as discussed previously.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the solutions for exploding gradients are:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Truncated BPTT is a small change to the BPTT process. Instead of propagating
    the learning back to time *t=0*, it is truncated to a fixed time backward to *t=k*.
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient Clipping to cut the gradient above a threshold when it shoots up.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Learning Rate. The learning rate adjusts itself based on the feedback
    and values.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the solutions for vanishing gradients are:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using ReLU as the activation function; hence the gradient will be 1.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Learning Rate. The learning rate adjusts itself based on the feedback
    and values.
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using extensions such as Long Short Term Memory (LSTM) and Gated Recurrent Units
    (GRUs), which we will describe next.
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many applications of RNNs, for example, in next letter predictions,
    next word predictions, language translation, and so on.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning and associated problems in RNNs](img/B05137_07_218.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
- en: 'Figure 35: Showing some applications in next letter/word predictions using
    RNN structures.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Long Short Term Memory
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the neural network architectures or modifications to RNNs that addresses
    the issue of vanishing gradient is known as long short term memory or LSTM. We
    will explain some building blocks of LSTM and then put it together for our readers.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'The first modification to RNN is to change the feedback learning matrix to
    1, that is, **W**[R] = 1, as shown in the following figure:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_220.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
- en: 'Figure 36: Building blocks of LSTM where the feedback matrix is set to 1.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: This will ensure the inputs from older cell or memory units are passed as-is
    to the next unit. Hence some modifications are needed.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: The output gate, as shown in the following figure, combines two computations.
    The first is the output from the individual unit, passed through an activation
    function, and the second is the output of the older unit that has been passed
    through a sigmoid using scaling.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_221.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
- en: 'Figure 37: Building block Output Gate for LSTM.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the output gate at the unit is given by:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_222.jpg)![Long Short Term Memory](img/B05137_07_223.jpg)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: 'The forget gate is between the two memory units. It generates 0 or 1 based
    on learned weights and transformations. The forget gate is shown in the following
    figure:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_224.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
- en: 'Figure 38: Building block Forget Gate addition to LSTM.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, ![Long Short Term Memory](img/B05137_07_225.jpg) can be seen
    as the representation of the forget gate. Next, the input gate and the new gate
    are combined, as shown in the following figure:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_226.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: 'Figure 39: Building blocks New Gate and Input Gate added to complete LSTM.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: The new memory generation unit uses the current input *x*[t] and the old state
    *h*[t-1] through an activation function and generates a new memory *C*[t]. The
    input gate combines the input and the old state and determines whether the new
    memory or the input should be preserved.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the update equation looks like this:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_230.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
- en: Gated Recurrent Units
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Gated Recurrent Units (GRUs) are simplified LSTMs with modifications. Many
    of the gates are simplified by using one "update" unit as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '![Gated Recurrent Units](img/B05137_07_231.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
- en: 'Figure 40: GRUs with Update unit.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes made to the equations are:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![Gated Recurrent Units](img/B05137_07_232.jpg)![Gated Recurrent Units](img/B05137_07_233.jpg)![Gated
    Recurrent Units](img/B05137_07_234.jpg)![Gated Recurrent Units](img/B05137_07_235.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
- en: Case study
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several benchmarks exist for image classification. We will use the MNIST image
    database for this case study. When we used MNIST in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), Unsupervised Machine Learning Techniques with clustering
    and outlier detection techniques, each pixel was considered a feature. In addition
    to learning from the pixel values as in previous experiments, with deep learning
    techniques we will also be learning new features from the structure of the training
    dataset. The deep learning algorithms will be trained on 60,000 images and tested
    on a 10,000-image test dataset.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Tools and software
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we introduce the open-source Java framework for deep learning
    called DeepLearning4J (DL4J). DL4J has libraries implementing a host of deep learning
    techniques and they can be used on distributed CPUs and GPUs.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepLearning4J: [https://deeplearning4j.org/index.html](https://deeplearning4j.org/index.html)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate the use of some DL4J libraries in learning from the MNIST
    training images and apply the learned models to classify the images in the test
    set.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classification is a particularly attractive test-bed to evaluate deep
    learning networks. We have previously encountered the MNIST database, which consists
    of greyscale images of handwritten digits. This time, we will show how both unsupervised
    and supervised deep learning techniques can be used to learn from the same dataset.
    The MNIST dataset has 28-by-28 pixel images in a single channel. These images
    are categorized into 10 labels representing the digits 0 to 9\. The goal is to
    train on 60,000 data points and test our deep learning classification algorithm
    on the remaining 10,000 images.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This includes supervised and unsupervised methods applied to a classification
    problem in which there are 10 possible output classes. Some techniques use an
    initial pre-training stage, which is unsupervised in nature, as we have seen in
    the preceding sections.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transfor
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset is available at:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: '[https://yann.lecun.com/exdb/mnist](https://yann.lecun.com/exdb/mnist)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments in this case study, the MNIST dataset has been standardized
    such that pixel values in the range 0 to 255 have been normalized to values from
    0.0 to 1.0\. The exception is in the experiment using stacked RBMs, where the
    training and test data have been binarized, that is, set to 1 if the standardized
    value is greater than or equal to 0.3 and 0 otherwise. Each of the 10 classes
    is equally represented in both the training set and the test set. In addition,
    examples are shuffled using a random number generator seed supplied by the user.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input data features are the greyscale values of the pixels in each image.
    This is the raw data and we will be using the deep learning algorithms to learn
    higher-level features out of the raw pixel values. The dataset has been prepared
    such that there are an equal number of examples of each class in both the training
    and the test sets.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will perform different experiments starting with simple MLP, Convolutional
    Networks, Variational Autoencoders, Stacked RBMS, and DBNs. We will walk through
    important parts of code that highlight the network structure or specialized tunings,
    give parameters to help readers, reproduce the experiments, and give the results
    for each type of network.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Basic data handling
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following snippet of code shows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: How to generically read data from a CSV with a structure enforced by delimiters.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: How to iterate the data and get records.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'How to shuffle data in memory and create training/testing or validation sets:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'DL4J has a specific MNIST wrapper for handling the data that we have used,
    as shown in the following snippet:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Multi-layer perceptron
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the first experiment, we will use a basic multi-layer perceptron with an
    input layer, one hidden layer, and an output layer. A detailed list of parameters
    that are used in the code is given here:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for MLP
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | m | 1 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | rate | 0.0015 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.98 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.005 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| **Number of rows in input** | numRows | 28 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| **Number of columns in input** | numColumns | 28 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 output size, Layer 1 input size** | outputLayer0, inputLayer1 |
    500 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
- en: '| **Layer 1 output size, Layer 2 input size** | outputLayer1, inputLayer2 |
    300 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
- en: '| **Layer 2 output size, Layer 3 input size** | outputLayer2, inputLayer3 |
    100 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
- en: '| **Layer 3 output size,** | outputNum | 10 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: Code for MLP
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the listing that follows, we can see how we first configure the MLP by passing
    in the hyperarameters using the Builder pattern.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Training, evaluation, and testing the MLP are shown in the following snippet.
    Notice the code that initializes the visualization backend enabling you to monitor
    the model training in your browser, particularly the model score (the training
    error after each iteration) and updates to parameters:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following plots show the training error against training iteration for
    the MLP model. This curve should decrease with iterations:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '![Code for MLP](img/B05137_07_236.jpg)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
- en: 'Figure 41: Training error as measured with number of iterations of training
    for the MLP model.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we see the distribution of parameters in Layer 0 of
    the MLP as well as the distribution of updates to the parameters. These histograms
    should have an approximately Gaussian (Normal) shape, which indicates good convergence.
    For more on how to use charts to tune your model, see the DL4J Visualization page
    ([https://deeplearning4j.org/visualization](https://deeplearning4j.org/visualization)):'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '![Code for MLP](img/B05137_07_237.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
- en: 'Figure 42: Histograms showing Layer parameters and update distribution.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Network
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second experiment, we configured a Convolutional Network (ConvNet) using
    the built-in MultiLayerConfiguration. The architecture of the network consists
    of a total of five layers, as can be seen from the following code snippet. Following
    the input layer, two convolution layers with 5-by-5 filters alternating with Max
    pooling layers are followed by a fully connected dense layer using the ReLu activation
    layer, ending with Softmax activation in the final output layer. The optimization
    algorithm used is Stochastic Gradient Descent, and the loss function is Negative
    Log Likelihood.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: The various configuration parameters (or hyper-parameters) for the ConvNet are
    given in the table.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for ConvNet
  id: totrans-490
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| **Seed** | seed | 123 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| **Input size** | numRows, numColumns | 28, 28 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| **Number of epochs** | numEpochs | 10 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | iterations | 1 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.005 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | learningRate | 0.1 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.9 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| **Convolution filter size** | xsize, ysize | 5, 5 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| **Convolution layers stride size** | x, y | 1, 1 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| **Number of input channels** | numChannels | 1 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| **Subsampling layer stride size** | sx, sy | 2, 2 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 output size** | nOut0 | 20 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| **Layer 2 output size** | nOut1 | 50 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| **Layer 4 output size** | nOut2 | 500 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| **Layer 5 output size** | outputNum | 10 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: Code for CNN
  id: totrans-508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you can see, configuring multi-layer neural networks with the DL4J API is
    similar whether you are building MLPs or CNNs. Algorithm-specific configuration
    is simply done in the definition of each layer.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Variational Autoencoder
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the third experiment, we configure a Variational Autoencoder as the classifier.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for the Variational Autoencoder
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The parameters used to configure the VAE are shown in the table.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Values |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| **Seed for RNG** | rngSeed | 12345 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | Iterations | 1 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | learningRate | 0.001 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| **RMS decay** | rmsDecay | 0.95 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.0001 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| **Output layer size** | outputNum | 10 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| **VAE encoder layers size** | vaeEncoder1, vaeEncoder2 | 256, 256 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| **VAE decoder layers size** | vaeDecoder1, vaeDecoder2 | 256, 256 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| **Size of latent variable space** | latentVarSpaceSize | 128 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: Code for Variational Autoencoder
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have configured two layers each of encoders and decoders and are reconstructing
    the input using a Bernoulli distribution.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: DBN
  id: totrans-529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The parameters used in DBN are shown in the following table:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: '| **Input data size** | numRows, numColumns | 28, 28 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
- en: '| **Seed for RNG** | seed | 123 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
- en: '| **Number of training iterations** | iterations | 1 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.5 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 (input)****Layer 0 (output)****Layer 1 (input, output)****Layer
    2 (input, output)****Layer 3 (input, output)** | numRows * numColumnsnOut0nIn1,
    nOut1nIn2, nOut2nIn3, outputNum | 28 * 28500500, 250250, 200200, 10 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
- en: Configuring the DBN using the DL4J API is shown in the example used in this
    case study. The code for the configuration of the network is shown here.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameter search using Arbiter
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DeepLearning4J provides a framework for fine-tuning hyper-parameters by taking
    the burden of hand-tuning away from the modeler; instead, it allows the specification
    of the parameter space to search. In the following example code snippet, the configuration
    is specified using a MultiLayerSpace instead of a MutiLayerConfiguration object,
    in which the ranges for the hyper-parameters are specified by means of ParameterSpace
    objects in the Arbiter DL4J package for the parameters to be tuned:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Results and analysis
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The results of evaluating the performance of the four networks on the test
    data are given in the following table:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '|   | MLP | ConvNet | VAE | DBN |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '| **Accuracy** | 0.9807 | 0.9893 | 0.9743 | 0.7506 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '| **Precision** | 0.9806 | 0.9893 | 0.9742 | 0.7498 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '| **Recall** | 0.9805 | 0.9891 | 0.9741 | 0.7454 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| **F1 score** | 0.9806 | 0.9892 | 0.9741 | 0.7476 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: The goal of the experiments was not to match benchmark results in each of the
    neural network structures, but to give a comprehensive architecture implementation
    in the code with detailed parameters for the readers to explore.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the hyper-parameters in deep learning Networks is quite a challenge and
    though Arbiter and online resources such as gitter ( [https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j))
    help with DL4J, the time and cost of running the hyper-parameter search is quite
    high as compared to other classification techniques including SVMs.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: 'The benchmark results on the MNIST dataset and corresponding papers are available
    here:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As seen from the benchmark result, Linear 1 Layer NN gets an error rate of 12%
    and adding more layers reduces it to about 2\. This shows the non-linear nature
    of the data and the need for a complex algorithm to fit the patterns.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: As compared to the benchmark best result on neural networks ranging from a 2.5%
    to 1.6% error rate, our results are very much comparable with the 2% error rate.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: Most of the benchmark results show Convolutional Network architectures having
    error rates in the range of 1.1% to 0.5% and our hyper-parameter search has matched
    the best of those models with an error rate of just under 1.1%.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: Our results for DBN fall far short of the benchmarks at just over 25%. There
    is no reason to doubt that further tuning can improve performance bringing it
    to the range of 3-5%.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-560
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The history of Deep Learning is intimately tied to the limitations of earlier
    attempts at using neural networks in machine learning and AI, and how these limitations
    were overcome with newer techniques, technological improvements, and the availability
    of vast amounts of data.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron is the basic neural network. Multi-layer networks are used in
    supervised learning and are built by connecting several hidden layers of neurons
    to propagate activations forward and using backpropagation to reduce the training
    error. Several activation functions are used, most commonly, the sigmoid and tanh
    functions.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: The problems of neural networks are vanishing or exploding gradients, slow training,
    and the trap of local minima.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning successfully addresses these problems with the help of several
    effective techniques that can be used for unsupervised as well as supervised learning.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: Among the building blocks of deep learning networks are Restricted Boltzmann
    Machines (RBM), Autoencoders, and Denoising Autoencoders. RBMs are two-layered
    undirected networks that are able to extract high-level features from their input.
    Contrastive divergence is used to speed up the training. Autoencoders are also
    deep learning networks used in unsupervised learning—they attempt to replicate
    the input by first encoding learned features in the encoding layer and then reconstructing
    the input via a set of decoding layers. Denoising Autoencoders address some limitations
    of Autoencoders, which can sometimes cause them to trivially learn the identity
    function.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning networks are often pretrained in an unsupervised fashion and then
    their parameters are fine-tuned via supervised fine-tuning. Stacked RBMs or Autoencoders
    are used in the pretraining phase and the fine-tuning is typically accomplished
    with a softmax activation in the output layer in the case of classification.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Deep Autoencoders are good at learning complex latent structures in data and
    are used in unsupervised learning by employing pre-training and fine-tuning with
    Autoencoder building blocks. Deep Belief Networks (DBN) are generative models
    that can be used to create more samples. It is constructed using a directed Bayesian
    network with an undirected RBM layer on top. Overfitting in deep learning networks
    can be addressed by learning with dropouts, where some nodes in the network are
    randomly "turned off".
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) have a number of applications in computer
    vision. CNNs can learn patterns in the data translation-invariant and robust to
    linear scaling in the data. They reduce the dimensionality of the data using convolution
    filters and pooling layers and can achieve very effective results in classification
    tasks. A use case involving the classification of digital images is presented.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: When the data arrives as sequences and there are temporal relationships among
    data, Recurrent Neural Networks (RNN) are used for modeling. RNNs use feedback
    from previous layers and emit output continually. The problem of vanishing and
    exploding gradients recurs in RNNs, and are addressed by several modifications
    to the architecture, such as Long Short Term Memory (LSTM) and Gated Recurrent
    Networks (GRU).
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter's case study, we present the experiments done with various deep
    learning networks to learn from MNIST handwritten digit image datasets. Results
    using MLP, ConvNet, Variational Autoencoder, and Stacked RBM are presented.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: We think that deep neural networks are able to approximate a significant and
    representative sub-set of key structures that the underlying data is based on.
    In addition, the hierarchic structures of the data can be easily captured with
    the help of different hidden layers. Finally, the invariance against rotation,
    translation, and the scale of images, for instance, is the last key elements of
    the performance of deep neural networks. The invariance allows us to reduce the
    number of possible states to be captured by the neural network (*References* [19]).
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Behnke, S. (2001). Learning iterative image reconstruction in the neural abstraction
    pyramid. International Journal of Computational Intelligence and Applications,
    1(4), 427–438\.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Behnke, S. (2002). Learning face localization using hierarchical recurrent networks.
    In Proceedings of the 12^(th) international conference on artificial neural networks
    (pp. 1319–1324).
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Behnke, S. (2003). Discovering hierarchical speech features using convolutional
    non-negative matrix factorization. In Proceedings of the international joint conference
    on neural networks, vol. 4 (pp. 2758–2763).
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Behnke, S. (2003). LNCS, Lecture notes in computer science: Vol. 2766\. Hierarchical
    neural networks for image interpretation. Springer. Behnke, S. (2005). Face localization
    and tracking in the neural abstraction pyramid. Neural Computing and Applications,
    14(2), 97–103.'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Casey, M. P. (1996). The dynamics of discrete-time computation, with application
    to recurrent neural networks and finite state machine extraction. Neural Computation,
    8(6), 1135–1178.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal
    representations by error propagation. In Rumelhart, D. E. and McClelland, J. L.,
    editors, Parallel Distributed Processing, volume 1, pages 318–362\. MIT Press.
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goller, C.; Küchler, A (1996). ""Learning task-dependent distributed representations
    by backpropagation through structure"". Neural Networks, IEEE. doi:10.1109/ICNN.1996.548916
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hochreiter, Sepp. The vanishing gradient problem during learning recurrent
    neural nets and problem solutions. International Journal of Uncertainty, Fuzziness
    and Knowledge-Based Systems, 6(02): 107–116, 1998.'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. E. Hinton, S. Osindero, and Y. The (2006). "A fast learning algorithm for
    deep belief nets," Neural Comput., vol. 18, pp. 1527–1554\.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. E. Hinton (2002). "Training products of experts by minimizing contrastive
    divergence," Neural Comput., vol. 14, pp. 1771–1800\.
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. E. Hinton and R. R. Salakhutdinov (2006). "Reducing the dimensionality of
    data with neural networks," Science, vol. 313, no. 5786, pp. 504–507.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton, G. E., & Zemel, R. S. (1994). Autoencoders, minimum description length,
    and Helmholtz free energy. Advances in Neural Information Processing Systems,
    6, 3–10.
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. (2007). "Greedy layer-wise
    training of deep networks," in Advances in Neural Information Processing Systems
    19 (NIPS'06) pp. 153–160\.
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio (2007). "An
    empirical evaluation of deep architectures on problems with many factors of variation,"
    in Proc. 24^(th) Int. Conf. Machine Learning (ICML'07) pp. 473–480.
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol (2008), "Extracting
    and composing robust features with denoising autoencoders," in Proc. ^(25)th Int.
    Conf. Machine Learning (ICML'08), pp. 1096–1103.
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F.-J. Huang and Y. LeCun (2006). "Large-scale learning with SVM and convolutional
    nets for generic object categorization," in Proc. Computer Vision and Pattern
    Recognition Conf. (CVPR'06).
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. A. Gers, N. N. Schraudolph, and J. Schmidhuber (2003). Learning precise timing
    with LSTM recurrent networks. The Journal of Machine Learning Research.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kyunghyun Cho et. al (2014). Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation. [https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf).
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://brohrer.github.io/how_convolutional_neural_networks_work.html](https://brohrer.github.io/how_convolutional_neural_networks_work.html)'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Henry W. Lin, Max Tegmark, David Rolnick (2016). Why does deep and cheap learning
    work so well? [https://arxiv.org/abs/1608.08225](https://arxiv.org/abs/1608.08225)
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mike Schuster and Kuldip K. Paliwal (1997). Bidirectional Recurrent Neural Networks,
    Trans. on Signal Processing.
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: H Lee, A Battle, R Raina, AY Ng (2007). Efficient sparse coding algorithms,
    In Advances in Neural Information Processing Systems
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bengio Y. (2009). Learning deep architectures for AI, Foundations and Trends
    in Machine Learning 1(2) pages 1-127.
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
