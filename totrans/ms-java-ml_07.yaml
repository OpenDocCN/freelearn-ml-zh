- en: Chapter 7. Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, we discussed
    different supervised classification techniques that are general and can be used
    in a wide range of applications. In the area of supervised non-linear techniques,
    especially in computer-vision, deep learning and its variants are having a remarkable
    impact. We find that deep learning and associated methodologies can be applied
    to image-recognition, image and object annotation, movie descriptions, and even
    areas such as text classification, language modeling, translations, and so on.
    (*References* [1, 2, 3, 4, and 5])
  prefs: []
  type: TYPE_NORMAL
- en: To set the stage for deep learning, we will start with describing what neurons
    are and how they can be arranged to build multi-layer neural networks, present
    the core elements of these networks, and explain how they work. We will then discuss
    the issues and problems associated with neural networks that gave rise to advances
    and structural changes in deep learning. We will learn about some building blocks
    of deep learning such as Restricted Boltzmann Machines and Autoencoders. We will
    then explore deep learning through different variations in supervised and unsupervised
    learning. Next, we will take a tour of Convolutional Neural Networks (CNN) and
    by means of a use case, illustrate how they work by deconstructing an application
    of CNNs in the area of computer-vision. We will introduce Recurrent Neural Networks
    (RNN) and its variants and how they are used in the text/sequence mining fields.
    We will finally present a case study using real-life data of MNIST images and
    use it to compare/contrast different techniques. We will use DeepLearning4J as
    our Java toolkit for performing these experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer feed-forward neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historically, artificial neural networks have been largely identified by multi-layer
    feed-forward perceptrons, and so we will begin with a discussion of the primitive
    elements of the structure of such networks, how to train them, the problem of
    overfitting, and techniques to address it.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs, neurons, activation function, and mathematical notation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A single neuron or perceptron is the same as the unit described in the Linear
    Regression topic in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*.
    In this chapter, the data instance vector will be represented by *x* and has *d*
    dimensions, and each dimension can be represented as ![Inputs, neurons, activation
    function, and mathematical notation](img/B05137_07_003.jpg). The weights associated
    with each dimension are represented as a weight vector *w* that has *d* dimensions,
    and each dimension can be represented as ![Inputs, neurons, activation function,
    and mathematical notation](img/B05137_07_005.jpg). Each neuron has an extra input
    *b*, known as the bias, associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neuron pre-activation performs the linear transformation of inputs given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The activation function is given by ![Inputs, neurons, activation function,
    and mathematical notation](img/B05137_07_008.jpg), which transforms the neuron
    input ![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_009.jpg)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_010.jpg)![Inputs,
    neurons, activation function, and mathematical notation](img/B05137_07_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Perceptron with inputs, weights, and bias feeding to generate outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layered neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multi-layered neural networks are the first step to understanding deep learning
    networks as the fundamental concepts and primitives of multi-layered nets form
    the basis of all deep neural nets.
  prefs: []
  type: TYPE_NORMAL
- en: Structure and mathematical notations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduce the generic structure of neural networks in this section. Most
    neural nets are variants of the structure outlined here. We also present the relevant
    notation that we will use in the rest of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure and mathematical notations](img/B05137_07_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Multilayer neural network showing an input layer, two hidden layers,
    and an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The most common supervised learning algorithms pertaining to neural networks
    use multi-layered perceptrons. The Input Layer consists of several neurons, each
    connected independently to the input, with its own set of weights and bias. In
    addition to the Input Layer, there are one or more layers of neurons known as
    Hidden Layers. The input layer neurons are connected to every neuron in the first
    hidden layer, that layer is similarly connected to the next hidden layer, and
    so on, resulting in a fully connected network. The layer of neurons connected
    to the last hidden layer is called the Output Layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each hidden layer is represented by ![Structure and mathematical notations](img/B05137_07_013.jpg)
    where *k* is the layer. The pre-activation for layer *0 < k * *< l* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure and mathematical notations](img/B05137_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer activation for ![Structure and mathematical notations](img/B05137_07_017.jpg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure and mathematical notations](img/B05137_07_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final output layer activation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure and mathematical notations](img/B05137_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output is generally one class per neuron and it is tuned in such a way that
    only one neuron activates and all others have 0 as the output. A softmax function
    with ![Structure and mathematical notations](img/B05137_07_020.jpg) is used for
    giving the result.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions in NN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some of the most well-known activation functions that are used in neural networks
    are given in the following sections and they are used because the derivatives
    needed in learning can be expressed in terms of the function itself.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sigmoid activation functions are given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sigmoid function](img/B05137_07_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It can be seen as a bounded, strictly increasing and positive transformation
    function that squashes the values between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic tangent ("tanh") function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Tanh function is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hyperbolic tangent ("tanh") function](img/B05137_07_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It can be seen as bounded, strictly increasing, but as a positive or negative
    transformation function that squashes the values between -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Training neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will discuss the key elements of training neural networks
    from input training sets, in much the same fashion as we did in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*. The dataset is denoted by *D* and
    consists of individual data instances. The instances are normally represented
    as the set ![Training neural network](img/B05137_07_024.jpg). The labels for each
    instance are represented as the set ![Training neural network](img/B05137_07_025.jpg).
    The entire labeled dataset with numeric or real-valued features is represented
    as paired elements in a set as given by ![Training neural network](img/B05137_07_026.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Empirical risk minimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Empirical risk minimization is a general machine learning concept that is used
    in many classifications or supervised learning. The main idea behind this technique
    is to convert a training or learning problem into an optimization problem (*References*
    [13]).
  prefs: []
  type: TYPE_NORMAL
- en: Given the parameters for a neural network as **?** = ({**W**¹, **W**², … **W**
    *^l* ^(+1)}, {**b**¹, **b**², …**b** *^L* ^(+1)}) the training problem can be
    seen as finding the best parameters (*?*)such that
  prefs: []
  type: TYPE_NORMAL
- en: '![Empirical risk minimization](img/B05137_07_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![Empirical risk minimization](img/B05137_07_031.jpg) Stochastic gradient
    descent (SGD) discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach
    to Real-World Supervised Learning"), *Practical Approach to Real-World Supervised
    Learning* and [Chapter 5](ch05.html "Chapter 5. Real-Time Stream Machine Learning"),
    *Real-time Stream Machine Learning,* is commonly used as the optimization procedure.
    The SGD applied to training neural networks is:'
  prefs: []
  type: TYPE_NORMAL
- en: initialize **?** = ({**W**¹, **W**², … **W***^l* ^(+1)}, {**b**¹, **b**², …**b***^L*
    ^(+1)})
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for i=1 to *N* epochs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: for each training sample (**x**^t, *y*^t) ![Empirical risk minimization](img/B05137_07_036.jpg)//
    find the gradient of function 2 ?= ?+ a? //move in direction
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The learning rate used here (a) will impact the algorithm convergence by reducing
    the oscillation near the optimum; choosing the right value of a is often a hyper
    parameter search that needs the validation techniques described in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to learn the parameters of a neural network, we need to choose a way to
    do parameter initialization, select a loss function ![Empirical risk minimization](img/B05137_07_040.jpg),
    compute the parameter gradients ![Empirical risk minimization](img/B05137_07_041.jpg),
    propagate the losses back, select the regularization/penalty function O(*?*),
    and compute the gradient of regularization ![Empirical risk minimization](img/B05137_07_043.jpg).
    In the next few sections, we will describe this step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter initialization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The parameters of neural networks are the weights and biases of each layer from
    the input layer, through hidden layers, to the output layer. There has been much
    research in this area as the optimization depends on the start or initialization.
    Biases are generally set to value 0\. The weight initialization depends on the
    activation functions as some, such as tanh, value 0, cannot be used. Generally,
    the way to initialize the weights of each layer is by random initialization using
    a symmetric function with a user-defined boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The loss function's main role is to maximize how well the predicted output label
    matches the class of the input data vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, maximization ![Loss function](img/B05137_07_044.jpg) is equivalent to
    minimizing the negative of the log-likelihood or cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_045.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gradients
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will describe gradients at the output layer and the hidden layer without
    going into the derivation as it is beyond the scope of this book. Interested readers
    can see the derivation in the text by Rumelhart, Hinton and Williams (*References*
    [6]).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient at the output layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Gradient at the output layer can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient at the output layer](img/B05137_07_046.jpg)![Gradient at the output
    layer](img/B05137_07_047.jpg)![Gradient at the output layer](img/B05137_07_048.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *e(y)* is called the "one hot vector" where only one value in the vector
    is 1 corresponding to the right class *y* and the rest are 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient at the output layer pre-activation can be calculated similarly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient at the output layer](img/B05137_07_051.jpg)'
  prefs: []
  type: TYPE_IMG
- en: = – (**e**(y) – **f**(**x**))
  prefs: []
  type: TYPE_NORMAL
- en: Gradient at the Hidden Layer
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A hidden layer gradient is computed using the chain rule of partial differentiation.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient at the hidden layer ![Gradient at the Hidden Layer](img/B05137_07_053.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient at the Hidden Layer](img/B05137_07_054.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Gradient at the hidden layer pre-activation can be shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient at the Hidden Layer](img/B05137_07_055.jpg)![Gradient at the Hidden
    Layer](img/B05137_07_056.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the hidden layer pre-activation needs partial derivatives of the activation
    functions as shown previously (*g'*(*a*^k**x**[j])), some of the well-known activation
    functions described previously have partial derivatives in terms of the equation
    itself, which makes computation very easy.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the partial derivative of the sigmoid function is *g'(a) = g(a)(*1
    *– g(a))* and, for the tanh function, it is 1 – *g*(a)².
  prefs: []
  type: TYPE_NORMAL
- en: Parameter gradient
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The loss gradient of parameters must be computed using gradients of weights
    and biases. Gradient of weights can be shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parameter gradient](img/B05137_07_060.jpg)![Parameter gradient](img/B05137_07_061.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Gradient of biases can be shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parameter gradient](img/B05137_07_062.jpg)![Parameter gradient](img/B05137_07_063.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Feed forward and backpropagation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The aim of neural network training is to adjust the weights and biases at each
    layer so that, based on the feedback from the output layer and the loss function
    that estimates the difference between the predicted output and the actual output,
    that difference is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network algorithm based on initial weights and biases can be seen
    as forwarding the computations layer by layer as shown in the acyclic flow graph
    with one hidden layer to demonstrate the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed forward and backpropagation](img/B05137_07_064.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Neural network flow as a graph in feed forward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the input vector and pre-initialized values of weights and biases, each
    subsequent element is computed: the pre-activation, hidden layer output, final
    layer pre-activation, final layer output, and loss function with respect to the
    actual label. In backward propagation, the flow is exactly reversed, from the
    loss at the output down to the weights and biases of the first layer, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feed forward and backpropagation](img/B05137_07_065.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Neural network flow as a graph in back propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The backpropagation algorithm (*References* [6 and 7]) in its entirety can
    be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the output gradient before activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_066.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For hidden layers *k=l+1 to 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the gradient of hidden layer parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_068.jpg)![How does it work?](img/B05137_07_069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute the gradient of the hidden layer below the current:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_070.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute the gradient of the layer before activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_071.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Regularization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the empirical risk minimization objective defined previously, regularization
    is used to address the over-fitting problem in machine learning as introduced
    in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*. The well-known
    regularization functions are given as follows.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is applied only to the weights and not to the biases and is given for
    layers connecting (*i,j*) components as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L2 regularization](img/B05137_07_073.jpg)![L2 regularization](img/B05137_07_074.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also, the gradient of the regularizer can be computed as ![L2 regularization](img/B05137_07_075.jpg).
    They are often interpreted as the "Gaussian Prior" over the weight distribution.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is again applied only to the weights and not to the biases and is given
    for layers connecting *(i,j)* components as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![L1 regularization](img/B05137_07_076.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And the gradient of this regularizer can be computed as ![L1 regularization](img/B05137_07_077.jpg).
    It is often interpreted as the "Laplacian Prior" over the weight distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss in detail the issues faced by neural networks,
    which will become the stepping stone for building deep learning networks.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing gradients, local optimum, and slow training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the major issues with neural networks is the problem of "vanishing gradient"
    (*References* [8]). We will try to give a simple explanation of the issue rather
    than exploring the mathematical derivations in depth. We will choose the sigmoid
    activation function and a two-layer neural network, as shown in the following
    figure, to demonstrate the issue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_078.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Vanishing Gradient issue.'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in the activation function description, the sigmoid function squashes
    the output between the range 0 and 1\. The derivative of the sigmoid function
    *g'(a) = g(a)(*1 *– g(a))* has a range between 0 and 0.25\. The goal of learning
    is to minimize the output loss, that is, ![Vanishing gradients, local optimum,
    and slow training](img/B05137_07_079.jpg). In general, the output error does not
    go to 0, so maximum iterations; a user-specified parameter determines the quality
    of learning and backpropagation of the errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplifying to illustrate the effect of output error on the input weight layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_080.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each of the transformations, for instance, from output to hidden, involves
    multiplication of two terms, both less than 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_081.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the value becomes so small when it reaches the input layer that the propagation
    of the gradient has almost vanished. This is known as the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: A paradoxical situation arises when you need to add more layers to make features
    more interesting in the hidden layers. But adding more layers also increases the
    errors. As you add more layers, the input layers become "slow to train," which
    causes the output layers to be more inaccurate as they are dependent on the input
    layers; further, and for the same number of iterations, the errors increase with
    the increase in the number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: With a fixed number of maximum iterations, more layers and slow propagation
    of errors can lead to a "local optimum."
  prefs: []
  type: TYPE_NORMAL
- en: Another issue with basic neural networks is the number of parameters. Finding
    effective size and weights for each hidden layer and bias becomes more challenging
    with the increase in the number of layers. If we increase the number of layers,
    the parameters increase in polynomials. Fitting the parameters for the data requires
    a large number of data samples. This can result in the problem discussed before,
    that is, overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will start learning about the building blocks of
    deep learning that help overcome these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning includes architectures and techniques for supervised and unsupervised
    learning with the capacity to internalize the abstract structure of high-dimensional
    data using networks composed of building blocks to create discriminative or generative
    models. These techniques have proved enormously successful in recent years and
    any reader interested in mastering them must become familiar with the basic building
    blocks of deep learning first and understand the various types of networks in
    use by practitioners. Hands-on experience building and tuning deep neural networks
    is invaluable if you intend to get a deeper understanding of the subject. Deep
    learning, in various domains such as image classification and text learning, incorporates
    feature generation in its structures thus making the task of mining the features
    redundant in many applications. The following sections provide a guide to the
    concepts, building blocks, techniques for composing architectures, and training
    deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks for deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following sections, we introduce the most important components used in
    deep learning, including Restricted Boltzmann machines, Autoencoders, and Denoising
    Autoencoders, how they work, and their advantages and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear activation function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Reclin function is given by the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g*(*a*) *= reclin* (*a*) *= max* (0*,* *a*)'
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen as having a lower bound of 0 and no upper bound, strictly increasing,
    and a positive transformation function that just does linear transformation of
    positives.
  prefs: []
  type: TYPE_NORMAL
- en: It is easier to see that the rectified linear unit or ReLu has a derivative
    of 1 or identity for values greater than 0\. This acts as a significant benefit
    as the derivatives are not squashed and do not have diminishing values when chained.
    One of the issues with ReLu is that the value is 0 for negative inputs and the
    corresponding neurons act as "dead", especially when a large negative value is
    learned for the bias term. ReLu cannot recover from this as the input and derivative
    are both 0\. This is generally solved by having a "leaky ReLu". These functions
    have a small value for negative inputs and are given by ![Rectified linear activation
    function](img/B05137_07_083.jpg) where ? = 0.01, typically.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines (RBM) is an unsupervised learning neural network
    (*References* [11]). The idea of RBM is to extract "more meaningful features"
    from labeled or unlabeled data. It is also meant to "learn" from the large quantity
    of unlabeled data available in many domains when getting access to labeled data
    is costly or difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Definition and mathematical notation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In its basic form RBM assumes inputs to be binary values 0 or 1 in each dimension.
    RBMs are undirected graphical models having two layers, a visible layer represented
    as *x* and a hidden layer *h,* and connections *W*.
  prefs: []
  type: TYPE_NORMAL
- en: 'RBM defines a distribution over the visible layer that involves the latent
    variables from the hidden layer. First an energy function is defined to capture
    the relationship between the visible and the hidden layers in vector form as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_088.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In scalar form the energy function can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_089.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The probability of the distribution is given by ![Definition and mathematical
    notation](img/B05137_07_090.jpg) where *Z* is called the "partitioning function",
    which is an enumeration over all the values of *x and h*, which are binary, resulting
    in exponential terms and thus making it intractable!
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_093.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Connection between the visible layer and hidden layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Markov network view of the same in scalar form can be represented using
    all the pairwise factors, as shown in the following figure. This also makes it
    clear why it is called a "restricted" Boltzmann machine as there is no connection
    among units within a given hidden layer or in the visible layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_094.jpg)![Definition
    and mathematical notation](img/B05137_07_095.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Input and hidden layers as scalars'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that the whole probability distribution function ![Definition and
    mathematical notation](img/B05137_07_090.jpg) is intractable. We will now derive
    the basic conditional probability distributions for *x, h*.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional distribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although computing the whole *p(x, h)* is intractable, the conditional distribution
    of *p(x|h)* or *p(h|x)* can be easily defined and shown to be a Bernoulli distribution
    and tractable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional distribution](img/B05137_07_100.jpg)![Conditional distribution](img/B05137_07_101.jpg)![Conditional
    distribution](img/B05137_07_102.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, being symmetric and undirected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional distribution](img/B05137_07_103.jpg)![Conditional distribution](img/B05137_07_104.jpg)![Conditional
    distribution](img/B05137_07_105.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Free energy in RBM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The distribution of input or the observed variable is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Free energy in RBM](img/B05137_07_106.jpg)![Free energy in RBM](img/B05137_07_107.jpg)![Free
    energy in RBM](img/B05137_07_108.jpg)![Free energy in RBM](img/B05137_07_109.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The function *F(x)* is called free energy.
  prefs: []
  type: TYPE_NORMAL
- en: Training the RBM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'RBMs are trained using the optimization objective of minimizing the average
    negative log-likelihood over the entire training data. This can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the RBM](img/B05137_07_111.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The optimization is carried out by using stochastic gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the RBM](img/B05137_07_112.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The term ![Training the RBM](img/B05137_07_113.jpg) is called the "positive
    phase" and the term ![Training the RBM](img/B05137_07_114.jpg) is called the "negative
    phase" because of how they affect the probability distributions—the positive phase,
    because it increases the probability of training data by reducing the free energy,
    and the negative phase, as it decreases the probability of samples generated by
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown that the overall gradient is difficult to compute analytically
    because of the "negative phase", as it is computing the expectation over all possible
    configurations of the input data under the distribution formed by the model and
    making it intractable!
  prefs: []
  type: TYPE_NORMAL
- en: To make the computation tractable, estimation is carried out using a fixed number
    of model samples and they are referred to as "negative particles" denoted by *N*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient can be now written as the approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the RBM](img/B05137_07_116.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where particles ![Training the RBM](img/B05137_07_117.jpg) are sampled using
    some sampling techniques such as the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling in RBM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gibbs sampling is often the technique used to generate samples and learn the
    probability of *p(x,h)* in terms of *p(x|h)* and *p (h|x)*, which are relatively
    easy to compute, as shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gibbs sampling for joint sampling of N random variables ![Sampling in RBM](img/B05137_07_119.jpg)
    is done using N sampling sub-steps of the form ![Sampling in RBM](img/B05137_07_120.jpg)
    where *S* *[-i]* contains samples up to and excluding step *S* *[i]*. Graphically,
    this can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling in RBM](img/B05137_07_123.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Graphical representation of sampling done between hidden and input
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: As ![Sampling in RBM](img/B05137_07_124.jpg) it can be shown that the sampling
    represents the actual distribution *p(x,h)*.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive divergence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Contrastive divergence (CD) is a trick used to expedite the Gibbs sampling process
    described previously so it stops at step *k* of the process rather than continuing
    for a long time to guarantee convergence. It has been seen that even *k=1* is
    reasonable and gives good performance (*References* [10]).
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These are the inputs to the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of steps for Gibbs sampling, *k*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate a
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is the set of updated parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The complete training pseudo-code using CD with the free energy function and
    partial derivatives can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each instance in training **x**^t:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a negative particle ![How does it work?](img/B05137_07_117.jpg) using
    *k* steps of Gibbs Sampling.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the parameters:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_130.jpg)![How does it work?](img/B05137_07_131.jpg)![How
    does it work?](img/B05137_07_132.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Persistent contrastive divergence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Persistent contrastive divergence is another trick used to compute the joint
    probability *p(x,h)*. In this method, there is a single chain that does not reinitialize
    after every observed sample to find the negative particle ![Persistent contrastive
    divergence](img/B05137_07_117.jpg). It persists its state and parameters are updated
    just through running these k states by using the particle from the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An autoencoder is another form of unsupervised learning technique in neural
    networks. It is very similar to the feed-forward neural network described at the
    start with the only difference being it doesn't generate a class at output, but
    tries to replicate the input at the output layer (*References* [12 and 23]). The
    goal is to have hidden layer(s) capture the latent or hidden information of the
    input as features that can be useful in unsupervised or supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Definition and mathematical notations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A single hidden layer example of an Autoencoder is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_133.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Autoencoder flow between layers'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer and the output layer have the same number of neurons similar
    as feed-forward, corresponding to the input vector, *x*. Each hidden layer can
    have greater, equal, or fewer neurons than the input or output layer and an activation
    function that does a non-linear transformation of the signal. It can be seen as
    using the unsupervised or latent hidden structure to "compress" the data effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder or input transformation of the data by the hidden layer is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_135.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the decoder or output transformation of the data by the output layer is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_136.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally, a sigmoid function with linear transformation of signals as described
    in the neural network section is popularly used in the layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notations](img/B05137_07_137.jpg) and ![Definition
    and mathematical notations](img/B05137_07_138.jpg))'
  prefs: []
  type: TYPE_IMG
- en: Loss function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The job of the loss function is to reduce the training error as before so that
    an optimization process such as a stochastic gradient function can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of binary valued input, the loss function is generally the average
    cross-entropy given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_139.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It can be easily verified that, when the input signal and output signal match
    either 0 or 1, the error is 0\. Similarly, for real-valued input, a squared error
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_140.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradient of the loss function that is needed for the stochastic gradient
    procedure is similar to the feed-forward neural network and can be shown through
    derivation for both real-valued and binary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss function](img/B05137_07_141.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Parameter gradients are obtained by back-propagating the ![Loss function](img/B05137_07_142.jpg)
    exactly as in the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Autoencoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Autoencoders have some known drawbacks that have been addressed by specialized
    architectures that we will discuss in the sections to follow. These limitations
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the Autoencoder is equal to the number of neurons in the input,
    there is a chance that the weights learned by the Autoencoders are just the identity
    vectors and that the whole representation simply passes on the inputs exactly
    as outputs with zero loss. Thus, they emulate "rote learning" or "memorization"
    without any generalization.
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the Autoencoder is greater than the number of neurons in the
    input, the configuration is called an "overcomplete" hidden layer and can have
    similar problems to the ones mentioned previously. Some of the units can be turned
    off and others can become identity making it just the copy unit.
  prefs: []
  type: TYPE_NORMAL
- en: When the size of the Autoencoder is less than the number of neurons in the input,
    known as "undercomplete", the latent structure in the data or important hidden
    components can be discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned previously, when the Autoencoder has a hidden layer size greater
    than or equal to that of the input, it is not guaranteed to learn the weights
    and can become simply a unit switch to copy input to output. This issue is addressed
    by the Denoising Autoencoder. Here there is another layer added between input
    and the hidden layer. This layer adds some noise to the input using either a well-known
    distribution ![Denoising Autoencoder](img/B05137_07_143.jpg) or using stochastic
    noise such as turning a bit to 0 in binary input. This "noisy" input then goes
    through learning from the hidden layer to the output layer exactly like the Autoencoder.
    The loss function of the Denoising Autoencoder compares the output with the actual
    input. Thus, the added noise and the larger hidden layer enable either learning
    latent structures or adding/removing redundancy to produce the exact signal at
    the output. This architecture—where non-zero features at the noisy layer generate
    features at the hidden layer that are themselves transformed by the activation
    layer as the signal advances forward—lends a robustness and implicit structure
    to the learning press (*References* [15]).
  prefs: []
  type: TYPE_NORMAL
- en: '![Denoising Autoencoder](img/B05137_07_144.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Denoising Autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised pre-training and supervised fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in the issues section on neural networks, the issue with over-training
    arises especially in deep learning as the number of layers, and hence parameters,
    is large. One way to account for over-fitting is to do data-specific regularization.
    In this section, we will describe the "unsupervised pre-training" method done
    in the hidden layers to overcome the issue of over-fitting. Note that this is
    generally the "initialization process" used in many deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm of unsupervised pre-training works in a layer-wise greedy fashion.
    As shown in the following figure, one layer of a visible and hidden structure
    is considered at a given time. The weights of this layer are learned for a few
    iterations using unsupervised techniques such as RBM, described previously. The
    output of the hidden layer is then used as a "visible" or "input" layer and the
    training proceeds to the next, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Each learning of layers can be thought of as a "feature extraction or feature
    generation" process. The real data inputs when transformed form higher-level features
    at a given layer and then are further combined to form much higher-level features,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised pre-training and supervised fine-tuning](img/B05137_07_145.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Layer wise incremental learning through unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Once all the hidden layer parameters are learned in pre-training using unsupervised
    techniques as described previously, a supervised fine-tuning process follows.
    In the supervised fine-tuning process, a final output layer is added and, just
    like in a neural network, training is done with forward and backward propagation.
    The idea is that most weights or parameters are almost fully tuned and only need
    a small change for producing a discriminative class mapping at the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised pre-training and supervised fine-tuning](img/B05137_07_146.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Final tuning or supervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep feed-forward NN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A deep feed-forward neural network involves using the stages pre-training, and
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the unsupervised learning technique used—RBM, Autoencoders, or
    Denoising Autoencoders—different algorithms are formed: Stacked RBM, Stacked Autoencoders,
    and Stacked Denoising Autoencoders, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Input and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given an architecture for the deep feed-forward neural net, these are the inputs
    for training the network:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of layers *L*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset without labels *D*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset with labels *D*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of training iterations *n*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The generalized learning/training algorithm for all three is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For layers *l=1 to L* (Pre-Training):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset without Labels ![How does it work?](img/B05137_07_151.jpg)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform Step-wise Layer Unsupervised Learning (RBM, Autoencoders, or Denoising
    Autoencoders)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finalize the parameters **W**^l, **b**^l from the preceding step
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For the output layer *(L+1)* perform random initialization of parameters **W***^L*^(+1),
    **b***^L* ^(+1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For layers *l=1 to L+1* (Fine-Tuning):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset with Labels ![How does it work?](img/B05137_07_156.jpg).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the pre-initialized weights from 1\. (**W**^l, **b**^l).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform forward-backpropagation for *n* iterations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Autoencoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep Autoencoders have many layers of hidden units, which shrink to a very small
    dimension and then symmetrically grow to the input size.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Autoencoders](img/B05137_07_158.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Deep Autoencoders'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind Deep Autoencoders is to create features that capture latent
    complex structures of input using deep networks and at the same time overcome
    the issue of gradients and underfitting due to the deep structure. It was shown
    that this methodology generated better features and performed better than PCA
    on many datasets (*References* [13]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Autoencoders use the concept of pre-training, encoders/decoders, and fine-tuning
    to perform unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the pre-training phase, the RBM methodology is used to learn greedy stepwise
    parameters of the encoders, as shown in the following figure, for initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep Autoencoders](img/B05137_07_159.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Stepwise learning in RBM'
  prefs: []
  type: TYPE_NORMAL
- en: In the unfolding phase the same parameters are symmetrically applied to the
    decoder network for initialization.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, fine-tuning backpropagation is used to adjust the parameters across
    the entire network.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Belief Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep Belief Networks (DBNs) are the origin of the concept of unsupervised pre-training
    (*References* [9]). Unsupervised pre-training originated from DBNs and then was
    found to be equally useful and effective in the feed-forward supervised deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks are not supervised feed-forward networks, but a generative
    model to generate data samples.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input layer is the instance of data, represented by one neuron for each
    input feature. The output of a DBN is a reconstruction of the input from a hierarchy
    of learned features of increasingly greater abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: How a DBN learns the joint distribution of the input data is explained here
    using a three-layer DBN architecture as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_160.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Deep belief network'
  prefs: []
  type: TYPE_NORMAL
- en: The three-hidden-layered DBN as shown has a first layer of undirected RBM connected
    to a two-layered Bayesian network. The Bayesian network with a sigmoid activation
    function is called a sigmoid Bayesian network (SBN).
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a generative model is to learn the joint distribution as given by
    *p*(**x**,**h**^((1)),**h**^((2)),**h**^((3)))
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(**x**,**h**^((1)),**h**^((2)),**h**^((3))) = *p*(**h**²),**h**^((3)))*p*(**h**^((1))|**h**^((2)))
    *p*(**x**|**h**^((1)))'
  prefs: []
  type: TYPE_NORMAL
- en: 'RBM computation as seen before gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_163.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The Bayesian Network in the next two layers is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_164.jpg)![How does it work?](img/B05137_07_165.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For binary data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_07_166.jpg)![How does it work?](img/B05137_07_166.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning with dropouts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another technique used to overcome the "overfitting" issues mentioned in deep
    neural networks is using the dropout technique to learn the parameters. In the
    next sections, we will define, illustrate, and explain how deep learning with
    dropouts works.
  prefs: []
  type: TYPE_NORMAL
- en: Definition and mathematical notation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The idea behind dropouts is to "cripple" the deep neural network structure by
    stochastically removing some of the hidden units as shown in the following figure
    after the parameters are learned. The units are set to 0 with the dropout probability
    generally set as *p=0.5*
  prefs: []
  type: TYPE_NORMAL
- en: The idea is similar to adding noise to the input, but done in all the hidden
    layers. When certain features (or a combination of features) are removed stochastically,
    the neural network has to learn latent features in a more robust way, without
    the interdependence of some features.
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_169.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Deep learning with dropout indicated by dropping certain units with
    dark shading.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each hidden layer is represented by *h*^k*(x)* where *k* is the layer. The
    pre-activation for layer *0<k<l* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The hidden layer activation for *1< k < l*. Binary masks are represented by
    **m**^k at each hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_172.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final output layer activation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Definition and mathematical notation](img/B05137_07_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inputs and outputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For training with dropouts, inputs are:'
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout probability *p* (typically 0.5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is a trained deep neural net that can be applied for predictive use.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will now describe the different parts of how deep learning with dropouts
    works.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Training and testing with dropouts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The backward propagation learning of weights and biases from the output loss
    function using gradients is very similar to traditional neural network learning.
    The only difference is that masks are applied appropriately as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the output gradient before activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_066.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For hidden layers *k=l+1 to 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the gradient of hidden layer parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_068.jpg)![Learning
    Training and testing with dropouts](img/B05137_07_069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**h**^(k-1) computation has taken into account the binary mask **m**^(k-1)
    applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the gradient of the hidden layer below the current:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_070.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Compute the gradient of the layer below before activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning Training and testing with dropouts](img/B05137_07_177.jpg)'
  prefs: []
  type: TYPE_IMG
- en: When testing the model, we cannot use the binary mask as it is stochastic; the
    "expectation" value of the mask is used. If the dropout probability is *p=0.5*,
    the same value 0.5 is used as the expectation for the unit at test or model application
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse coding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sparse coding is another neural network used for unsupervised learning and feature
    generation (*References* [22]). It works on the principle of finding latent structures
    in high dimensions that capture the patterns, thus performing feature extraction
    in addition to unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, for every input **x**^((t)) a latent representation **h**^((t)) is
    learned, which has a sparse representation (most values are 0 in the vector).
    This is done by optimization using the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparse coding](img/B05137_07_180.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where the first term ![Sparse coding](img/B05137_07_181.jpg) is to control the
    reconstruction error and the second term, which uses a regularizer ?, is for sparsity
    control. The matrix **D** is also known as a Dictionary as it has equivalence
    to words in a dictionary and **h**^((t)) is similar to word frequency; together
    they capture the impact of words in extracting patterns when performing text mining.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional Neural Networks or CNNs have become prominent and are widely used
    in the computer vision domain. Computer vision involves processing images/videos
    for capturing knowledge and patterns. Annotating images, classifying images/videos,
    correcting them, story-telling or describing images, and so on, are some of the
    broad applications in computer visi [16].
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer vision problems most generally have to deal with unstructured data
    that can be described as:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs that are 2D images with single or multiple color channels or 3D videos
    that are high-dimensional vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The features in these 2D or 3D representations have a well-known spatial topology,
    a hierarchical structure, and some repetitive elements that can be exploited.
  prefs: []
  type: TYPE_NORMAL
- en: The images/videos have a large number of transformations or variants based on
    factors such as illumination, noise, and so on. The same person or car can look
    different based on several factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will describe some building blocks used in CNNs. We will use simple
    images such as the letter X of the alphabet to explain the concept and mathematics
    involved. For example, even though the same character X is represented in different
    ways in the following figure due to translation, scaling, or distortion, the human
    eye can easily read it as X, but it becomes tricky for the computer to see the
    pattern. The images are shown with the author''s permission (*References* [19]):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional Neural Network](img/B05137_07_184.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Image of character X represented in different ways.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates how a simple grayscale image of X has common
    features such as a diagonal from top left, a diagonal from top right, and left
    and right intersecting diagonals repeated and combined to form a larger X:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Convolutional Neural Network](img/B05137_07_185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Common features represented in the image of character X.'
  prefs: []
  type: TYPE_NORMAL
- en: Local connectivity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the simple concept of dividing the whole image into "patches" or "recipient
    fields" and giving each patch to the hidden layers. As shown in the figure, instead
    of 9 X 9 pixels of the complete sample image, a 3 X 3 patch of pixels from the
    top left goes to the first hidden unit, the overlapping second patch goes to second,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Since the fully connected hidden layer would have a huge number of parameters,
    having smaller patches completely reduces the parameter or high-dimensional space
    problem!
  prefs: []
  type: TYPE_NORMAL
- en: '![Local connectivity](img/B05137_07_186.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Concept of patches on the whole image.'
  prefs: []
  type: TYPE_NORMAL
- en: Parameter sharing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The concept of parameter sharing is to construct a weight matrix that can be
    reused over different patches or recipient fields as constructed in the preceding
    figure in the local sharing. As shown in the following figure, the Feature map
    with same parameters **W**[1,1] and **W**[1,4] creates two different feature maps,
    Feature Map 1 and 4, both capturing the same features, that is, diagonal edges
    on either side. Thus, feature maps capture "similar regions" in the images and
    further reduce the dimensionality of the input space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Parameter sharing](img/B05137_07_187.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Discrete convolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will explain the steps in discrete convolution, taking a simple contrived
    example with simplified mathematics to illustrate the operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the kernel representing the diagonal feature is scanned over the entire
    image as a patch of 3 X 3\. If this kernel lands on the self-same feature in the
    input image and we have to compute the center value through what we call the convolution
    operator, we get the exact value of 1 because of the matching as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_190.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Discrete convolution step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire image when run through this kernel and convolution operator gives
    a matrix of values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_191.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Transformation of the character image after a kernel and convolution
    operator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how the left diagonal feature gets highlighted by running this scan.
    Similarly, by running other kernels, as shown in the following figure, we can
    get a "stack of filtered images":'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_192.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Different features run through the kernel giving a stack of images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each cell in the filtered images can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete convolution](img/B05137_07_193.jpg)![Discrete convolution](img/B05137_07_194.jpg)![Discrete
    convolution](img/B05137_07_195.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Pooling or subsampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pooling or subsampling works on the stack of filtered images to further shrink
    the image or compress it, while keeping the pattern as-is. The main steps carried
    out in pooling are:'
  prefs: []
  type: TYPE_NORMAL
- en: Pick a window size (for example, 2 X 2) and a stride size (for example, 2).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the window over all the filtered images at stride.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each window, pick the "maximum" value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Pooling or subsampling](img/B05137_07_196.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: Max pooling, done using a window size of 2 X 2 and stride of 2,
    computes cell values with maximum for first as 1.0, 0.33 for next, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pooling also plays an important part where the same features if moved or scaled
    can still be detected due to the use of maximum. The same set of stacked filtered
    images gets transformed into pooled images as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pooling or subsampling](img/B05137_07_197.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: Transformation showing how a stack of filtered images is converted
    to pooled images.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalization using ReLU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we discussed in the building blocks of deep learning, ReLUs remove the negative
    by squashing it to 0 and keep the positives as-is. They also play an important
    role in gradient computation in the backpropagation, removing the vanishing gradient
    issue of vanishing gradient.
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalization using ReLU](img/B05137_07_198.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: Transformation using ReLu.'
  prefs: []
  type: TYPE_NORMAL
- en: CNN Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will put together the building blocks discussed earlier
    to form the complete picture of CNNs. Combining the layers of convolution, ReLU,
    and pooling to form a connected network yielding shrunken images with patterns
    captured in the final output, we obtain the next composite building block, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_199.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Basic Unit of CNN showing a combination of Convolution, ReLu, and
    Pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, these layers can be combined or "deep-stacked", as shown in the following
    figure, to form a complex network that gives a small pool of images as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_200.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: Deep-stacking the basic units repeatedly to form CNN layers.'
  prefs: []
  type: TYPE_NORMAL
- en: The output layer is a fully connected network as shown, which uses a voting
    technique and learns the weights for the desired output. The fully connected output
    layer can be stacked too.
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_201.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: Fully connected layer as output of CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the final CNNs can be completely illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_202.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: CNNS with all layers showing inputs and outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: As before, gradient descent is selected as the learning technique using the
    loss functions to compute the difference and propagate the error backwards.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNN''s can be used in other domains such as voice pattern recognition, text
    mining, and so on, if the mapping of the data to the "image" can be successfully
    done and "local spatial" patterns exist. The following figure shows one of the
    ways of mapping sound and text to images for CNN usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CNN Layers](img/B05137_07_203.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: Illustration of mapping between temporal data, such as voice to
    spatial data, to an image.'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normal deep networks are used when you have finite inputs and there is no interdependence
    between the input examples or instances. When there are variable length inputs
    and there are temporal dependencies between them, that is, sequence related data,
    neural networks must be modified to handle such data. Recurrent Neural Networks
    (RNN) are examples of neural networks that are used widely to solve such problems,
    and we will discuss them in the following sections. RNNs are used in many sequence-related
    problems such as text mining, language modeling, bioinformatics data modeling,
    and so on, to name a few areas that fit this meta-level description (*References*
    [18 and 21]).
  prefs: []
  type: TYPE_NORMAL
- en: Structure of Recurrent Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will describe the simplest unit of the RNN first and then shown how it is
    combined to understand it functionally and mathematically and illustrate how different
    components interact and work.
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_204.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 32: Difference between an artificial neuron and a neuron with feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the basic input, a neuron with activation, and its output at
    a given time *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_206.jpg)![Structure
    of Recurrent Neural Networks](img/B05137_07_207.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A neuron with feedback keeps a matrix **W**[R] to incorporate previous output
    at time *t-1* and the equations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_210.jpg)![Structure
    of Recurrent Neural Networks](img/B05137_07_207.jpg)![Structure of Recurrent Neural
    Networks](img/B05137_07_211.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 33: Chain of neurons with feedbacks connected together.'
  prefs: []
  type: TYPE_NORMAL
- en: The basic RNN stacks the structure of hidden units as shown with feedback connected
    from the previous layer. At activation at time *t*, it depends not only on **x**^((t))
    as input, but also on the previous unit given by **W**[R]**h**^((t-1)). The weights
    in the feedback connection of RNN are generally the same across all the units,
    **W**[R]. Also, instead of emitting output at the very end of the feed-forward
    neural network, each unit continuously emits an output that can be used in the
    loss function calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Learning and associated problems in RNNs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Working with RNNs presents some challenges that are specific to them but there
    are common problems that are also encountered in other types of neural net.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient used from the output loss function at any time *t* of the unit
    has dependency going back to the first unit or *t=0*, as shown in the following
    figure. This is because the partial derivative at the unit is dependent on the
    previous unit, since:![Learning and associated problems in RNNs](img/B05137_07_210.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT) is the term used to illustrate the process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Learning and associated problems in RNNs](img/B05137_07_215.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 34: Backpropagation through time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar to what we saw in the section on feed-forward neural networks, the cases
    of exploding and vanishing gradient become more pronounced in RNNs due to the
    connectivity of units as discussed previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the solutions for exploding gradients are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Truncated BPTT is a small change to the BPTT process. Instead of propagating
    the learning back to time *t=0*, it is truncated to a fixed time backward to *t=k*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient Clipping to cut the gradient above a threshold when it shoots up.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Learning Rate. The learning rate adjusts itself based on the feedback
    and values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the solutions for vanishing gradients are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using ReLU as the activation function; hence the gradient will be 1.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Learning Rate. The learning rate adjusts itself based on the feedback
    and values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using extensions such as Long Short Term Memory (LSTM) and Gated Recurrent Units
    (GRUs), which we will describe next.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There are many applications of RNNs, for example, in next letter predictions,
    next word predictions, language translation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning and associated problems in RNNs](img/B05137_07_218.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 35: Showing some applications in next letter/word predictions using
    RNN structures.'
  prefs: []
  type: TYPE_NORMAL
- en: Long Short Term Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the neural network architectures or modifications to RNNs that addresses
    the issue of vanishing gradient is known as long short term memory or LSTM. We
    will explain some building blocks of LSTM and then put it together for our readers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first modification to RNN is to change the feedback learning matrix to
    1, that is, **W**[R] = 1, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_220.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 36: Building blocks of LSTM where the feedback matrix is set to 1.'
  prefs: []
  type: TYPE_NORMAL
- en: This will ensure the inputs from older cell or memory units are passed as-is
    to the next unit. Hence some modifications are needed.
  prefs: []
  type: TYPE_NORMAL
- en: The output gate, as shown in the following figure, combines two computations.
    The first is the output from the individual unit, passed through an activation
    function, and the second is the output of the older unit that has been passed
    through a sigmoid using scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_221.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 37: Building block Output Gate for LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the output gate at the unit is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_222.jpg)![Long Short Term Memory](img/B05137_07_223.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The forget gate is between the two memory units. It generates 0 or 1 based
    on learned weights and transformations. The forget gate is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_224.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 38: Building block Forget Gate addition to LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, ![Long Short Term Memory](img/B05137_07_225.jpg) can be seen
    as the representation of the forget gate. Next, the input gate and the new gate
    are combined, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_226.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 39: Building blocks New Gate and Input Gate added to complete LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: The new memory generation unit uses the current input *x*[t] and the old state
    *h*[t-1] through an activation function and generates a new memory *C*[t]. The
    input gate combines the input and the old state and determines whether the new
    memory or the input should be preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the update equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Long Short Term Memory](img/B05137_07_230.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gated Recurrent Units
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Gated Recurrent Units (GRUs) are simplified LSTMs with modifications. Many
    of the gates are simplified by using one "update" unit as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gated Recurrent Units](img/B05137_07_231.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 40: GRUs with Update unit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The changes made to the equations are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gated Recurrent Units](img/B05137_07_232.jpg)![Gated Recurrent Units](img/B05137_07_233.jpg)![Gated
    Recurrent Units](img/B05137_07_234.jpg)![Gated Recurrent Units](img/B05137_07_235.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several benchmarks exist for image classification. We will use the MNIST image
    database for this case study. When we used MNIST in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), Unsupervised Machine Learning Techniques with clustering
    and outlier detection techniques, each pixel was considered a feature. In addition
    to learning from the pixel values as in previous experiments, with deep learning
    techniques we will also be learning new features from the structure of the training
    dataset. The deep learning algorithms will be trained on 60,000 images and tested
    on a 10,000-image test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tools and software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we introduce the open-source Java framework for deep learning
    called DeepLearning4J (DL4J). DL4J has libraries implementing a host of deep learning
    techniques and they can be used on distributed CPUs and GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepLearning4J: [https://deeplearning4j.org/index.html](https://deeplearning4j.org/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: We will illustrate the use of some DL4J libraries in learning from the MNIST
    training images and apply the learned models to classify the images in the test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image classification is a particularly attractive test-bed to evaluate deep
    learning networks. We have previously encountered the MNIST database, which consists
    of greyscale images of handwritten digits. This time, we will show how both unsupervised
    and supervised deep learning techniques can be used to learn from the same dataset.
    The MNIST dataset has 28-by-28 pixel images in a single channel. These images
    are categorized into 10 labels representing the digits 0 to 9\. The goal is to
    train on 60,000 data points and test our deep learning classification algorithm
    on the remaining 10,000 images.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This includes supervised and unsupervised methods applied to a classification
    problem in which there are 10 possible output classes. Some techniques use an
    initial pre-training stage, which is unsupervised in nature, as we have seen in
    the preceding sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transfor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset is available at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://yann.lecun.com/exdb/mnist](https://yann.lecun.com/exdb/mnist)'
  prefs: []
  type: TYPE_NORMAL
- en: In the experiments in this case study, the MNIST dataset has been standardized
    such that pixel values in the range 0 to 255 have been normalized to values from
    0.0 to 1.0\. The exception is in the experiment using stacked RBMs, where the
    training and test data have been binarized, that is, set to 1 if the standardized
    value is greater than or equal to 0.3 and 0 otherwise. Each of the 10 classes
    is equally represented in both the training set and the test set. In addition,
    examples are shuffled using a random number generator seed supplied by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input data features are the greyscale values of the pixels in each image.
    This is the raw data and we will be using the deep learning algorithms to learn
    higher-level features out of the raw pixel values. The dataset has been prepared
    such that there are an equal number of examples of each class in both the training
    and the test sets.
  prefs: []
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will perform different experiments starting with simple MLP, Convolutional
    Networks, Variational Autoencoders, Stacked RBMS, and DBNs. We will walk through
    important parts of code that highlight the network structure or specialized tunings,
    give parameters to help readers, reproduce the experiments, and give the results
    for each type of network.
  prefs: []
  type: TYPE_NORMAL
- en: Basic data handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following snippet of code shows:'
  prefs: []
  type: TYPE_NORMAL
- en: How to generically read data from a CSV with a structure enforced by delimiters.
  prefs: []
  type: TYPE_NORMAL
- en: How to iterate the data and get records.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to shuffle data in memory and create training/testing or validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'DL4J has a specific MNIST wrapper for handling the data that we have used,
    as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Multi-layer perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the first experiment, we will use a basic multi-layer perceptron with an
    input layer, one hidden layer, and an output layer. A detailed list of parameters
    that are used in the code is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for MLP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | m | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | rate | 0.0015 |'
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.005 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of rows in input** | numRows | 28 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of columns in input** | numColumns | 28 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 output size, Layer 1 input size** | outputLayer0, inputLayer1 |
    500 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 1 output size, Layer 2 input size** | outputLayer1, inputLayer2 |
    300 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 2 output size, Layer 3 input size** | outputLayer2, inputLayer3 |
    100 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 3 output size,** | outputNum | 10 |'
  prefs: []
  type: TYPE_TB
- en: Code for MLP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the listing that follows, we can see how we first configure the MLP by passing
    in the hyperarameters using the Builder pattern.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Training, evaluation, and testing the MLP are shown in the following snippet.
    Notice the code that initializes the visualization backend enabling you to monitor
    the model training in your browser, particularly the model score (the training
    error after each iteration) and updates to parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plots show the training error against training iteration for
    the MLP model. This curve should decrease with iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Code for MLP](img/B05137_07_236.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 41: Training error as measured with number of iterations of training
    for the MLP model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we see the distribution of parameters in Layer 0 of
    the MLP as well as the distribution of updates to the parameters. These histograms
    should have an approximately Gaussian (Normal) shape, which indicates good convergence.
    For more on how to use charts to tune your model, see the DL4J Visualization page
    ([https://deeplearning4j.org/visualization](https://deeplearning4j.org/visualization)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Code for MLP](img/B05137_07_237.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 42: Histograms showing Layer parameters and update distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second experiment, we configured a Convolutional Network (ConvNet) using
    the built-in MultiLayerConfiguration. The architecture of the network consists
    of a total of five layers, as can be seen from the following code snippet. Following
    the input layer, two convolution layers with 5-by-5 filters alternating with Max
    pooling layers are followed by a fully connected dense layer using the ReLu activation
    layer, ending with Softmax activation in the final output layer. The optimization
    algorithm used is Stochastic Gradient Descent, and the loss function is Negative
    Log Likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: The various configuration parameters (or hyper-parameters) for the ConvNet are
    given in the table.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for ConvNet
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Seed** | seed | 123 |'
  prefs: []
  type: TYPE_TB
- en: '| **Input size** | numRows, numColumns | 28, 28 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of epochs** | numEpochs | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | iterations | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.005 |'
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | learningRate | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| **Convolution filter size** | xsize, ysize | 5, 5 |'
  prefs: []
  type: TYPE_TB
- en: '| **Convolution layers stride size** | x, y | 1, 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of input channels** | numChannels | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Subsampling layer stride size** | sx, sy | 2, 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 output size** | nOut0 | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 2 output size** | nOut1 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 4 output size** | nOut2 | 500 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 5 output size** | outputNum | 10 |'
  prefs: []
  type: TYPE_TB
- en: Code for CNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you can see, configuring multi-layer neural networks with the DL4J API is
    similar whether you are building MLPs or CNNs. Algorithm-specific configuration
    is simply done in the definition of each layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Variational Autoencoder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the third experiment, we configure a Variational Autoencoder as the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for the Variational Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The parameters used to configure the VAE are shown in the table.
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Seed for RNG** | rngSeed | 12345 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | Iterations | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | learningRate | 0.001 |'
  prefs: []
  type: TYPE_TB
- en: '| **RMS decay** | rmsDecay | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.0001 |'
  prefs: []
  type: TYPE_TB
- en: '| **Output layer size** | outputNum | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| **VAE encoder layers size** | vaeEncoder1, vaeEncoder2 | 256, 256 |'
  prefs: []
  type: TYPE_TB
- en: '| **VAE decoder layers size** | vaeDecoder1, vaeDecoder2 | 256, 256 |'
  prefs: []
  type: TYPE_TB
- en: '| **Size of latent variable space** | latentVarSpaceSize | 128 |'
  prefs: []
  type: TYPE_TB
- en: Code for Variational Autoencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We have configured two layers each of encoders and decoders and are reconstructing
    the input using a Bernoulli distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: DBN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The parameters used in DBN are shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Input data size** | numRows, numColumns | 28, 28 |'
  prefs: []
  type: TYPE_TB
- en: '| **Seed for RNG** | seed | 123 |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of training iterations** | iterations | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 (input)****Layer 0 (output)****Layer 1 (input, output)****Layer
    2 (input, output)****Layer 3 (input, output)** | numRows * numColumnsnOut0nIn1,
    nOut1nIn2, nOut2nIn3, outputNum | 28 * 28500500, 250250, 200200, 10 |'
  prefs: []
  type: TYPE_TB
- en: Configuring the DBN using the DL4J API is shown in the example used in this
    case study. The code for the configuration of the network is shown here.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Parameter search using Arbiter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DeepLearning4J provides a framework for fine-tuning hyper-parameters by taking
    the burden of hand-tuning away from the modeler; instead, it allows the specification
    of the parameter space to search. In the following example code snippet, the configuration
    is specified using a MultiLayerSpace instead of a MutiLayerConfiguration object,
    in which the ranges for the hyper-parameters are specified by means of ParameterSpace
    objects in the Arbiter DL4J package for the parameters to be tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Results and analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The results of evaluating the performance of the four networks on the test
    data are given in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | MLP | ConvNet | VAE | DBN |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Accuracy** | 0.9807 | 0.9893 | 0.9743 | 0.7506 |'
  prefs: []
  type: TYPE_TB
- en: '| **Precision** | 0.9806 | 0.9893 | 0.9742 | 0.7498 |'
  prefs: []
  type: TYPE_TB
- en: '| **Recall** | 0.9805 | 0.9891 | 0.9741 | 0.7454 |'
  prefs: []
  type: TYPE_TB
- en: '| **F1 score** | 0.9806 | 0.9892 | 0.9741 | 0.7476 |'
  prefs: []
  type: TYPE_TB
- en: The goal of the experiments was not to match benchmark results in each of the
    neural network structures, but to give a comprehensive architecture implementation
    in the code with detailed parameters for the readers to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the hyper-parameters in deep learning Networks is quite a challenge and
    though Arbiter and online resources such as gitter ( [https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j))
    help with DL4J, the time and cost of running the hyper-parameter search is quite
    high as compared to other classification techniques including SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benchmark results on the MNIST dataset and corresponding papers are available
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As seen from the benchmark result, Linear 1 Layer NN gets an error rate of 12%
    and adding more layers reduces it to about 2\. This shows the non-linear nature
    of the data and the need for a complex algorithm to fit the patterns.
  prefs: []
  type: TYPE_NORMAL
- en: As compared to the benchmark best result on neural networks ranging from a 2.5%
    to 1.6% error rate, our results are very much comparable with the 2% error rate.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the benchmark results show Convolutional Network architectures having
    error rates in the range of 1.1% to 0.5% and our hyper-parameter search has matched
    the best of those models with an error rate of just under 1.1%.
  prefs: []
  type: TYPE_NORMAL
- en: Our results for DBN fall far short of the benchmarks at just over 25%. There
    is no reason to doubt that further tuning can improve performance bringing it
    to the range of 3-5%.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The history of Deep Learning is intimately tied to the limitations of earlier
    attempts at using neural networks in machine learning and AI, and how these limitations
    were overcome with newer techniques, technological improvements, and the availability
    of vast amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron is the basic neural network. Multi-layer networks are used in
    supervised learning and are built by connecting several hidden layers of neurons
    to propagate activations forward and using backpropagation to reduce the training
    error. Several activation functions are used, most commonly, the sigmoid and tanh
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: The problems of neural networks are vanishing or exploding gradients, slow training,
    and the trap of local minima.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning successfully addresses these problems with the help of several
    effective techniques that can be used for unsupervised as well as supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Among the building blocks of deep learning networks are Restricted Boltzmann
    Machines (RBM), Autoencoders, and Denoising Autoencoders. RBMs are two-layered
    undirected networks that are able to extract high-level features from their input.
    Contrastive divergence is used to speed up the training. Autoencoders are also
    deep learning networks used in unsupervised learning—they attempt to replicate
    the input by first encoding learned features in the encoding layer and then reconstructing
    the input via a set of decoding layers. Denoising Autoencoders address some limitations
    of Autoencoders, which can sometimes cause them to trivially learn the identity
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning networks are often pretrained in an unsupervised fashion and then
    their parameters are fine-tuned via supervised fine-tuning. Stacked RBMs or Autoencoders
    are used in the pretraining phase and the fine-tuning is typically accomplished
    with a softmax activation in the output layer in the case of classification.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Autoencoders are good at learning complex latent structures in data and
    are used in unsupervised learning by employing pre-training and fine-tuning with
    Autoencoder building blocks. Deep Belief Networks (DBN) are generative models
    that can be used to create more samples. It is constructed using a directed Bayesian
    network with an undirected RBM layer on top. Overfitting in deep learning networks
    can be addressed by learning with dropouts, where some nodes in the network are
    randomly "turned off".
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNNs) have a number of applications in computer
    vision. CNNs can learn patterns in the data translation-invariant and robust to
    linear scaling in the data. They reduce the dimensionality of the data using convolution
    filters and pooling layers and can achieve very effective results in classification
    tasks. A use case involving the classification of digital images is presented.
  prefs: []
  type: TYPE_NORMAL
- en: When the data arrives as sequences and there are temporal relationships among
    data, Recurrent Neural Networks (RNN) are used for modeling. RNNs use feedback
    from previous layers and emit output continually. The problem of vanishing and
    exploding gradients recurs in RNNs, and are addressed by several modifications
    to the architecture, such as Long Short Term Memory (LSTM) and Gated Recurrent
    Networks (GRU).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter's case study, we present the experiments done with various deep
    learning networks to learn from MNIST handwritten digit image datasets. Results
    using MLP, ConvNet, Variational Autoencoder, and Stacked RBM are presented.
  prefs: []
  type: TYPE_NORMAL
- en: We think that deep neural networks are able to approximate a significant and
    representative sub-set of key structures that the underlying data is based on.
    In addition, the hierarchic structures of the data can be easily captured with
    the help of different hidden layers. Finally, the invariance against rotation,
    translation, and the scale of images, for instance, is the last key elements of
    the performance of deep neural networks. The invariance allows us to reduce the
    number of possible states to be captured by the neural network (*References* [19]).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Behnke, S. (2001). Learning iterative image reconstruction in the neural abstraction
    pyramid. International Journal of Computational Intelligence and Applications,
    1(4), 427–438\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Behnke, S. (2002). Learning face localization using hierarchical recurrent networks.
    In Proceedings of the 12^(th) international conference on artificial neural networks
    (pp. 1319–1324).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Behnke, S. (2003). Discovering hierarchical speech features using convolutional
    non-negative matrix factorization. In Proceedings of the international joint conference
    on neural networks, vol. 4 (pp. 2758–2763).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Behnke, S. (2003). LNCS, Lecture notes in computer science: Vol. 2766\. Hierarchical
    neural networks for image interpretation. Springer. Behnke, S. (2005). Face localization
    and tracking in the neural abstraction pyramid. Neural Computing and Applications,
    14(2), 97–103.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Casey, M. P. (1996). The dynamics of discrete-time computation, with application
    to recurrent neural networks and finite state machine extraction. Neural Computation,
    8(6), 1135–1178.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal
    representations by error propagation. In Rumelhart, D. E. and McClelland, J. L.,
    editors, Parallel Distributed Processing, volume 1, pages 318–362\. MIT Press.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Goller, C.; Küchler, A (1996). ""Learning task-dependent distributed representations
    by backpropagation through structure"". Neural Networks, IEEE. doi:10.1109/ICNN.1996.548916
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hochreiter, Sepp. The vanishing gradient problem during learning recurrent
    neural nets and problem solutions. International Journal of Uncertainty, Fuzziness
    and Knowledge-Based Systems, 6(02): 107–116, 1998.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. E. Hinton, S. Osindero, and Y. The (2006). "A fast learning algorithm for
    deep belief nets," Neural Comput., vol. 18, pp. 1527–1554\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. E. Hinton (2002). "Training products of experts by minimizing contrastive
    divergence," Neural Comput., vol. 14, pp. 1771–1800\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. E. Hinton and R. R. Salakhutdinov (2006). "Reducing the dimensionality of
    data with neural networks," Science, vol. 313, no. 5786, pp. 504–507.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton, G. E., & Zemel, R. S. (1994). Autoencoders, minimum description length,
    and Helmholtz free energy. Advances in Neural Information Processing Systems,
    6, 3–10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. (2007). "Greedy layer-wise
    training of deep networks," in Advances in Neural Information Processing Systems
    19 (NIPS'06) pp. 153–160\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio (2007). "An
    empirical evaluation of deep architectures on problems with many factors of variation,"
    in Proc. 24^(th) Int. Conf. Machine Learning (ICML'07) pp. 473–480.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol (2008), "Extracting
    and composing robust features with denoising autoencoders," in Proc. ^(25)th Int.
    Conf. Machine Learning (ICML'08), pp. 1096–1103.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F.-J. Huang and Y. LeCun (2006). "Large-scale learning with SVM and convolutional
    nets for generic object categorization," in Proc. Computer Vision and Pattern
    Recognition Conf. (CVPR'06).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. A. Gers, N. N. Schraudolph, and J. Schmidhuber (2003). Learning precise timing
    with LSTM recurrent networks. The Journal of Machine Learning Research.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kyunghyun Cho et. al (2014). Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation. [https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://brohrer.github.io/how_convolutional_neural_networks_work.html](https://brohrer.github.io/how_convolutional_neural_networks_work.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Henry W. Lin, Max Tegmark, David Rolnick (2016). Why does deep and cheap learning
    work so well? [https://arxiv.org/abs/1608.08225](https://arxiv.org/abs/1608.08225)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mike Schuster and Kuldip K. Paliwal (1997). Bidirectional Recurrent Neural Networks,
    Trans. on Signal Processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: H Lee, A Battle, R Raina, AY Ng (2007). Efficient sparse coding algorithms,
    In Advances in Neural Information Processing Systems
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bengio Y. (2009). Learning deep architectures for AI, Foundations and Trends
    in Machine Learning 1(2) pages 1-127.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
