- en: Chapter 7. Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 深度学习
- en: In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, we discussed
    different supervised classification techniques that are general and can be used
    in a wide range of applications. In the area of supervised non-linear techniques,
    especially in computer-vision, deep learning and its variants are having a remarkable
    impact. We find that deep learning and associated methodologies can be applied
    to image-recognition, image and object annotation, movie descriptions, and even
    areas such as text classification, language modeling, translations, and so on.
    (*References* [1, 2, 3, 4, and 5])
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《实际应用中的监督学习》*的[第2章](ch02.html "第2章. 实际应用中的监督学习")中，我们讨论了不同的一般监督分类技术，这些技术可以广泛应用于各种应用。在监督非线性技术的领域，特别是在计算机视觉中，深度学习及其变体正产生显著影响。我们发现深度学习及其相关方法可以应用于图像识别、图像和对象标注、电影描述，甚至包括文本分类、语言建模、翻译等领域。（*参考文献*
    [1, 2, 3, 4, 和 5]）
- en: To set the stage for deep learning, we will start with describing what neurons
    are and how they can be arranged to build multi-layer neural networks, present
    the core elements of these networks, and explain how they work. We will then discuss
    the issues and problems associated with neural networks that gave rise to advances
    and structural changes in deep learning. We will learn about some building blocks
    of deep learning such as Restricted Boltzmann Machines and Autoencoders. We will
    then explore deep learning through different variations in supervised and unsupervised
    learning. Next, we will take a tour of Convolutional Neural Networks (CNN) and
    by means of a use case, illustrate how they work by deconstructing an application
    of CNNs in the area of computer-vision. We will introduce Recurrent Neural Networks
    (RNN) and its variants and how they are used in the text/sequence mining fields.
    We will finally present a case study using real-life data of MNIST images and
    use it to compare/contrast different techniques. We will use DeepLearning4J as
    our Java toolkit for performing these experiments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为深度学习做准备，我们将首先描述神经元是什么以及它们如何被排列来构建多层神经网络，展示这些网络的核心元素，并解释它们的工作原理。然后，我们将讨论与神经网络相关的问题和挑战，这些挑战导致了深度学习在技术和结构上的进步。我们将了解深度学习的一些构建块，如受限玻尔兹曼机和自编码器。然后，我们将通过监督学习和无监督学习的不同变体来探索深度学习。接下来，我们将游览卷积神经网络（CNN），并通过一个用例，通过分解CNN在计算机视觉领域的应用来展示它们的工作原理。我们将介绍循环神经网络（RNN）及其变体，以及它们在文本/序列挖掘领域的应用。最后，我们将通过使用MNIST图像的真实数据案例研究来比较/对比不同的技术。我们将使用DeepLearning4J作为我们的Java工具包来执行这些实验。
- en: Multi-layer feed-forward neural network
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层前馈神经网络
- en: Historically, artificial neural networks have been largely identified by multi-layer
    feed-forward perceptrons, and so we will begin with a discussion of the primitive
    elements of the structure of such networks, how to train them, the problem of
    overfitting, and techniques to address it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，人工神经网络主要被识别为多层前馈感知器，因此我们将从讨论这类网络的结构的基本元素开始，包括如何训练它们、过拟合问题以及解决该问题的技术。
- en: Inputs, neurons, activation function, and mathematical notation
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入、神经元、激活函数和数学符号
- en: A single neuron or perceptron is the same as the unit described in the Linear
    Regression topic in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*.
    In this chapter, the data instance vector will be represented by *x* and has *d*
    dimensions, and each dimension can be represented as ![Inputs, neurons, activation
    function, and mathematical notation](img/B05137_07_003.jpg). The weights associated
    with each dimension are represented as a weight vector *w* that has *d* dimensions,
    and each dimension can be represented as ![Inputs, neurons, activation function,
    and mathematical notation](img/B05137_07_005.jpg). Each neuron has an extra input
    *b*, known as the bias, associated with it.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元或感知器与[第2章](ch02.html "第2章. 实际应用中的监督学习")中线性回归主题所描述的单位相同，即*《实际应用中的监督学习》*。在本章中，数据实例向量将由*x*表示，具有*d*个维度，每个维度可以表示为![输入、神经元、激活函数和数学符号](img/B05137_07_003.jpg)。与每个维度相关联的权重表示为一个具有*d*个维度的权重向量*w*，每个维度可以表示为![输入、神经元、激活函数和数学符号](img/B05137_07_005.jpg)。每个神经元都有一个额外的输入*b*，称为偏置。
- en: 'Neuron pre-activation performs the linear transformation of inputs given by:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元预激活执行由以下给出的输入线性变换：
- en: '![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_007.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![输入、神经元、激活函数和数学符号](img/B05137_07_007.jpg)'
- en: 'The activation function is given by ![Inputs, neurons, activation function,
    and mathematical notation](img/B05137_07_008.jpg), which transforms the neuron
    input ![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_009.jpg)
    as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数由![输入、神经元、激活函数和数学符号](img/B05137_07_008.jpg)给出，它将神经元输入![输入、神经元、激活函数和数学符号](img/B05137_07_009.jpg)转换为以下形式：
- en: '![Inputs, neurons, activation function, and mathematical notation](img/B05137_07_010.jpg)![Inputs,
    neurons, activation function, and mathematical notation](img/B05137_07_011.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![输入、神经元、激活函数和数学符号](img/B05137_07_010.jpg)![输入、神经元、激活函数和数学符号](img/B05137_07_011.jpg)'
- en: Figure 1\. Perceptron with inputs, weights, and bias feeding to generate outputs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 具有输入、权重和偏置的感知器生成输出。
- en: Multi-layered neural network
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层神经网络
- en: Multi-layered neural networks are the first step to understanding deep learning
    networks as the fundamental concepts and primitives of multi-layered nets form
    the basis of all deep neural nets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络是理解深度学习网络的第一步，因为多层网络的基本概念和原语构成了所有深度神经网络的基础。
- en: Structure and mathematical notations
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结构和数学符号
- en: We introduce the generic structure of neural networks in this section. Most
    neural nets are variants of the structure outlined here. We also present the relevant
    notation that we will use in the rest of the chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了神经网络的一般结构。大多数神经网络都是这里概述的结构变体。我们还展示了本章其余部分我们将使用的相关符号。
- en: '![Structure and mathematical notations](img/B05137_07_012.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_012.jpg)'
- en: Figure 2\. Multilayer neural network showing an input layer, two hidden layers,
    and an output layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 展示输入层、两个隐藏层和输出层的多层神经网络。
- en: The most common supervised learning algorithms pertaining to neural networks
    use multi-layered perceptrons. The Input Layer consists of several neurons, each
    connected independently to the input, with its own set of weights and bias. In
    addition to the Input Layer, there are one or more layers of neurons known as
    Hidden Layers. The input layer neurons are connected to every neuron in the first
    hidden layer, that layer is similarly connected to the next hidden layer, and
    so on, resulting in a fully connected network. The layer of neurons connected
    to the last hidden layer is called the Output Layer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络相关的最常见监督学习算法使用多层感知器。输入层由几个神经元组成，每个神经元独立连接到输入，并有自己的权重和偏置集。除了输入层之外，还有一层或更多被称为隐藏层的神经元。输入层神经元连接到第一隐藏层中的每个神经元，该层同样连接到下一隐藏层，依此类推，形成一个全连接网络。连接到最后隐藏层的神经元层被称为输出层。
- en: 'Each hidden layer is represented by ![Structure and mathematical notations](img/B05137_07_013.jpg)
    where *k* is the layer. The pre-activation for layer *0 < k * *< l* is given by:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏层由![结构和数学符号](img/B05137_07_013.jpg)表示，其中*k*是层。对于*0 < k * *< l*层的预激活由以下给出：
- en: '![Structure and mathematical notations](img/B05137_07_016.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_016.jpg)'
- en: 'The hidden layer activation for ![Structure and mathematical notations](img/B05137_07_017.jpg):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![结构和数学符号](img/B05137_07_017.jpg)的隐藏层激活：'
- en: '![Structure and mathematical notations](img/B05137_07_018.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_018.jpg)'
- en: 'The final output layer activation is:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出层激活为：
- en: '![Structure and mathematical notations](img/B05137_07_019.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![结构和数学符号](img/B05137_07_019.jpg)'
- en: The output is generally one class per neuron and it is tuned in such a way that
    only one neuron activates and all others have 0 as the output. A softmax function
    with ![Structure and mathematical notations](img/B05137_07_020.jpg) is used for
    giving the result.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 输出通常是每个神经元一个类别，并且调整方式使得只有一个神经元激活，其他所有神经元的输出都是0。使用![结构和数学符号](img/B05137_07_020.jpg)的softmax函数来给出结果。
- en: Activation functions in NN
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络中的激活函数
- en: Some of the most well-known activation functions that are used in neural networks
    are given in the following sections and they are used because the derivatives
    needed in learning can be expressed in terms of the function itself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中给出了神经网络中使用的一些最著名的激活函数，它们之所以被使用，是因为学习过程中所需的导数可以用函数本身来表示。
- en: Sigmoid function
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'Sigmoid activation functions are given by the following equation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数由以下方程给出：
- en: '![Sigmoid function](img/B05137_07_021.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Sigmoid函数](img/B05137_07_021.jpg)'
- en: It can be seen as a bounded, strictly increasing and positive transformation
    function that squashes the values between 0 and 1.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以被视为一个有界、严格递增且正的变换函数，将值压缩在0和1之间。
- en: Hyperbolic tangent ("tanh") function
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 双曲正切("tanh")函数
- en: 'The Tanh function is given by the following equation:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh函数由以下方程给出：
- en: '![Hyperbolic tangent ("tanh") function](img/B05137_07_022.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![双曲正切("tanh")函数](img/B05137_07_022.jpg)'
- en: It can be seen as bounded, strictly increasing, but as a positive or negative
    transformation function that squashes the values between -1 and 1.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以被视为有界、严格递增的，但作为一个将值压缩在-1和1之间的正或负变换函数。
- en: Training neural network
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: In this section, we will discuss the key elements of training neural networks
    from input training sets, in much the same fashion as we did in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*. The dataset is denoted by *D* and
    consists of individual data instances. The instances are normally represented
    as the set ![Training neural network](img/B05137_07_024.jpg). The labels for each
    instance are represented as the set ![Training neural network](img/B05137_07_025.jpg).
    The entire labeled dataset with numeric or real-valued features is represented
    as paired elements in a set as given by ![Training neural network](img/B05137_07_026.jpg).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论从输入训练集训练神经网络的要素，这与我们在[第2章](ch02.html "第2章. 实际应用世界监督学习")中讨论的方式非常相似，即*实际应用世界监督学习*。数据集用
    *D* 表示，由单个数据实例组成。实例通常表示为![训练神经网络](img/B05137_07_024.jpg)的集合。每个实例的标签表示为![训练神经网络](img/B05137_07_025.jpg)的集合。整个带标签的数据集，具有数值或实值特征，表示为集合中的配对元素，如![训练神经网络](img/B05137_07_026.jpg)所示。
- en: Empirical risk minimization
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经验风险最小化
- en: Empirical risk minimization is a general machine learning concept that is used
    in many classifications or supervised learning. The main idea behind this technique
    is to convert a training or learning problem into an optimization problem (*References*
    [13]).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经验风险最小化是一个通用的机器学习概念，在许多分类或监督学习中得到应用。这种技术背后的主要思想是将训练或学习问题转化为一个优化问题（*参考文献* [13]）。
- en: Given the parameters for a neural network as **?** = ({**W**¹, **W**², … **W**
    *^l* ^(+1)}, {**b**¹, **b**², …**b** *^L* ^(+1)}) the training problem can be
    seen as finding the best parameters (*?*)such that
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定神经网络的参数为 **?** = ({**W**¹, **W**², … **W** *^l* ^(+1)}, {**b**¹, **b**², …**b**
    *^L* ^(+1)})，训练问题可以看作是寻找最佳参数 (*?*)，使得
- en: '![Empirical risk minimization](img/B05137_07_030.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![经验风险最小化](img/B05137_07_030.jpg)'
- en: 'Where ![Empirical risk minimization](img/B05137_07_031.jpg) Stochastic gradient
    descent (SGD) discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach
    to Real-World Supervised Learning"), *Practical Approach to Real-World Supervised
    Learning* and [Chapter 5](ch05.html "Chapter 5. Real-Time Stream Machine Learning"),
    *Real-time Stream Machine Learning,* is commonly used as the optimization procedure.
    The SGD applied to training neural networks is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![经验风险最小化](img/B05137_07_031.jpg) 随机梯度下降（SGD）在第2章（ch02.html "第2章. 实际应用世界监督学习"）和第5章（ch05.html
    "第5章. 实时流机器学习"）中讨论，*实际应用世界监督学习*和*实时流机器学习*，通常用作优化过程。应用于训练神经网络的SGD如下：
- en: initialize **?** = ({**W**¹, **W**², … **W***^l* ^(+1)}, {**b**¹, **b**², …**b***^L*
    ^(+1)})
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 **?** = ({**W**¹, **W**², … **W***^l* ^(+1)}, {**b**¹, **b**², …**b***^L*
    ^(+1)})
- en: for i=1 to *N* epochs
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 i=1 到 *N* 个epoch
- en: for each training sample (**x**^t, *y*^t) ![Empirical risk minimization](img/B05137_07_036.jpg)//
    find the gradient of function 2 ?= ?+ a? //move in direction
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练样本（**x**^t，*y*^t）![经验风险最小化](img/B05137_07_036.jpg)//找到函数2的梯度 ?= ?+ a?
    //沿方向移动
- en: The learning rate used here (a) will impact the algorithm convergence by reducing
    the oscillation near the optimum; choosing the right value of a is often a hyper
    parameter search that needs the validation techniques described in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的学习率（a）将通过减少接近最优解的振荡来影响算法的收敛；选择正确的a值通常是一个超参数搜索，需要使用[第2章](ch02.html "第2章.
    实际应用世界监督学习")中描述的验证技术。
- en: Thus, to learn the parameters of a neural network, we need to choose a way to
    do parameter initialization, select a loss function ![Empirical risk minimization](img/B05137_07_040.jpg),
    compute the parameter gradients ![Empirical risk minimization](img/B05137_07_041.jpg),
    propagate the losses back, select the regularization/penalty function O(*?*),
    and compute the gradient of regularization ![Empirical risk minimization](img/B05137_07_043.jpg).
    In the next few sections, we will describe this step by step.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了学习神经网络的参数，我们需要选择一种参数初始化的方法，选择一个损失函数 ![经验风险最小化](img/B05137_07_040.jpg)，计算参数梯度
    ![经验风险最小化](img/B05137_07_041.jpg)，将损失反向传播，选择正则化/惩罚函数 O(*?*)，并计算正则化的梯度 ![经验风险最小化](img/B05137_07_043.jpg)。在接下来的几节中，我们将逐步描述这一过程。
- en: Parameter initialization
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数初始化
- en: The parameters of neural networks are the weights and biases of each layer from
    the input layer, through hidden layers, to the output layer. There has been much
    research in this area as the optimization depends on the start or initialization.
    Biases are generally set to value 0\. The weight initialization depends on the
    activation functions as some, such as tanh, value 0, cannot be used. Generally,
    the way to initialize the weights of each layer is by random initialization using
    a symmetric function with a user-defined boundary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的参数是输入层、通过隐藏层到输出层的每一层的权重和偏置。在这个领域已经进行了很多研究，因为优化取决于起始或初始化。偏置通常设置为0。权重初始化取决于激活函数，因为一些函数，如tanh，其值0不能使用。通常，初始化每一层权重的办法是使用具有用户定义边界的对称函数进行随机初始化。
- en: Loss function
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function's main role is to maximize how well the predicted output label
    matches the class of the input data vector.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的主要作用是最大化预测输出标签与输入数据向量类别的匹配程度。
- en: 'Thus, maximization ![Loss function](img/B05137_07_044.jpg) is equivalent to
    minimizing the negative of the log-likelihood or cross-entropy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最大化 ![损失函数](img/B05137_07_044.jpg) 等同于最小化负对数似然或交叉熵：
- en: '![Loss function](img/B05137_07_045.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/B05137_07_045.jpg)'
- en: Gradients
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 梯度
- en: We will describe gradients at the output layer and the hidden layer without
    going into the derivation as it is beyond the scope of this book. Interested readers
    can see the derivation in the text by Rumelhart, Hinton and Williams (*References*
    [6]).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述输出层和隐藏层的梯度，而不涉及推导，因为这超出了本书的范围。感兴趣的读者可以在Rumelhart、Hinton和Williams的文本中看到推导（*参考文献*
    [6]）。
- en: Gradient at the output layer
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 输出层的梯度
- en: 'Gradient at the output layer can be calculated as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的梯度可以计算为：
- en: '![Gradient at the output layer](img/B05137_07_046.jpg)![Gradient at the output
    layer](img/B05137_07_047.jpg)![Gradient at the output layer](img/B05137_07_048.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![输出层的梯度](img/B05137_07_046.jpg)![输出层的梯度](img/B05137_07_047.jpg)![输出层的梯度](img/B05137_07_048.jpg)'
- en: Where *e(y)* is called the "one hot vector" where only one value in the vector
    is 1 corresponding to the right class *y* and the rest are 0.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *e(y)* 被称为“独热向量”，其中向量的一个值是1，对应正确的类别 *y*，其余都是0。
- en: 'The gradient at the output layer pre-activation can be calculated similarly:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层预激活的梯度可以类似地计算：
- en: '![Gradient at the output layer](img/B05137_07_051.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![输出层的梯度](img/B05137_07_051.jpg)'
- en: = – (**e**(y) – **f**(**x**))
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: = – (**e**(y) – **f**(**x**))
- en: Gradient at the Hidden Layer
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 隐藏层梯度
- en: A hidden layer gradient is computed using the chain rule of partial differentiation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层梯度是使用偏微分链式法则计算的。
- en: Gradient at the hidden layer ![Gradient at the Hidden Layer](img/B05137_07_053.jpg)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层梯度 ![隐藏层的梯度](img/B05137_07_053.jpg)
- en: '![Gradient at the Hidden Layer](img/B05137_07_054.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏层的梯度](img/B05137_07_054.jpg)'
- en: 'Gradient at the hidden layer pre-activation can be shown as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层预激活的梯度可以表示为：
- en: '![Gradient at the Hidden Layer](img/B05137_07_055.jpg)![Gradient at the Hidden
    Layer](img/B05137_07_056.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![隐藏层的梯度](img/B05137_07_055.jpg)![隐藏层的梯度](img/B05137_07_056.jpg)'
- en: Since the hidden layer pre-activation needs partial derivatives of the activation
    functions as shown previously (*g'*(*a*^k**x**[j])), some of the well-known activation
    functions described previously have partial derivatives in terms of the equation
    itself, which makes computation very easy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于隐藏层预激活需要如前所述的激活函数的偏导数（*g'*(*a*^k**x**[j]）），因此之前描述的一些著名激活函数在方程本身中有偏导数，这使得计算非常容易。
- en: For example, the partial derivative of the sigmoid function is *g'(a) = g(a)(*1
    *– g(a))* and, for the tanh function, it is 1 – *g*(a)².
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，sigmoid 函数的偏导数为 *g'(a) = g(a)(*1 *– g(a))*，而对于 tanh 函数，它是 1 – *g*(a)²。
- en: Parameter gradient
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 参数梯度
- en: 'The loss gradient of parameters must be computed using gradients of weights
    and biases. Gradient of weights can be shown as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的损失梯度必须使用权重和偏差的梯度来计算。权重的梯度可以表示为：
- en: '![Parameter gradient](img/B05137_07_060.jpg)![Parameter gradient](img/B05137_07_061.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参数梯度](img/B05137_07_060.jpg)![参数梯度](img/B05137_07_061.jpg)'
- en: 'Gradient of biases can be shown as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差的梯度可以表示为：
- en: '![Parameter gradient](img/B05137_07_062.jpg)![Parameter gradient](img/B05137_07_063.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参数梯度](img/B05137_07_062.jpg)![参数梯度](img/B05137_07_063.jpg)'
- en: Feed forward and backpropagation
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 前向传播和反向传播
- en: The aim of neural network training is to adjust the weights and biases at each
    layer so that, based on the feedback from the output layer and the loss function
    that estimates the difference between the predicted output and the actual output,
    that difference is minimized.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练的目的是调整每一层的权重和偏差，以便基于输出层的反馈和损失函数（该函数估计预测输出与实际输出之间的差异），最小化这种差异。
- en: 'The neural network algorithm based on initial weights and biases can be seen
    as forwarding the computations layer by layer as shown in the acyclic flow graph
    with one hidden layer to demonstrate the flow:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于初始权重和偏差的神经网络算法可以看作是按层逐层前向计算，如图所示的单隐藏层无环流程图，以演示流程：
- en: '![Feed forward and backpropagation](img/B05137_07_064.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播和反向传播](img/B05137_07_064.jpg)'
- en: 'Figure 3: Neural network flow as a graph in feed forward.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：前向传播中神经网络作为图的流程。
- en: 'From the input vector and pre-initialized values of weights and biases, each
    subsequent element is computed: the pre-activation, hidden layer output, final
    layer pre-activation, final layer output, and loss function with respect to the
    actual label. In backward propagation, the flow is exactly reversed, from the
    loss at the output down to the weights and biases of the first layer, as shown
    in the following figure:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入向量和预初始化的权重和偏差值开始，计算后续的每个元素：预激活、隐藏层输出、最终层预激活、最终层输出以及相对于实际标签的损失函数。在反向传播中，流向正好相反，从输出层的损失到第一层的权重和偏差，如下图所示：
- en: '![Feed forward and backpropagation](img/B05137_07_065.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播和反向传播](img/B05137_07_065.jpg)'
- en: 'Figure 4: Neural network flow as a graph in back propagation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：反向传播中神经网络作为图的流程。
- en: How does it work?
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'The backpropagation algorithm (*References* [6 and 7]) in its entirety can
    be summarized as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法（*参考文献* [6 和 7]）整体上可以总结如下：
- en: 'Compute the output gradient before activation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 计算激活前的输出梯度：
- en: '![How does it work?](img/B05137_07_066.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_066.jpg)'
- en: 'For hidden layers *k=l+1 to 1*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层 *k=l+1 到 1*：
- en: 'Compute the gradient of hidden layer parameters:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 计算隐藏层参数的梯度：
- en: '![How does it work?](img/B05137_07_068.jpg)![How does it work?](img/B05137_07_069.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_068.jpg)![它是如何工作的？](img/B05137_07_069.jpg)'
- en: 'Compute the gradient of the hidden layer below the current:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 计算当前隐藏层以下的隐藏层梯度：
- en: '![How does it work?](img/B05137_07_070.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_070.jpg)'
- en: 'Compute the gradient of the layer before activation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 计算激活前的层梯度：
- en: '![How does it work?](img/B05137_07_071.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_071.jpg)'
- en: Regularization
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正则化
- en: In the empirical risk minimization objective defined previously, regularization
    is used to address the over-fitting problem in machine learning as introduced
    in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*. The well-known
    regularization functions are given as follows.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前定义的经验风险最小化目标中，正则化用于解决机器学习中的过拟合问题，如[第二章](ch02.html "第二章. 实际应用中的监督学习")《实际应用中的监督学习》中所述。以下给出了众所周知的正则化函数。
- en: L2 regularization
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: L2 正则化
- en: 'This is applied only to the weights and not to the biases and is given for
    layers connecting (*i,j*) components as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这只应用于权重，而不应用于偏差，并且对于连接 (*i,j*) 元件的层给出如下：
- en: '![L2 regularization](img/B05137_07_073.jpg)![L2 regularization](img/B05137_07_074.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![L2 正则化](img/B05137_07_073.jpg)![L2 正则化](img/B05137_07_074.jpg)'
- en: Also, the gradient of the regularizer can be computed as ![L2 regularization](img/B05137_07_075.jpg).
    They are often interpreted as the "Gaussian Prior" over the weight distribution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正则化器的梯度可以计算为 ![L2 正则化](img/B05137_07_075.jpg)。它们通常被解释为权重分布上的“高斯先验”。
- en: L1 regularization
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: L1 正则化
- en: 'This is again applied only to the weights and not to the biases and is given
    for layers connecting *(i,j)* components as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这只应用于权重，而不应用于偏差，并且对于连接 *(i,j)* 组件的层给出如下：
- en: '![L1 regularization](img/B05137_07_076.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![L1 正则化](img/B05137_07_076.jpg)'
- en: And the gradient of this regularizer can be computed as ![L1 regularization](img/B05137_07_077.jpg).
    It is often interpreted as the "Laplacian Prior" over the weight distribution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 而这个正则化器的梯度可以计算为 ![L1 正则化](img/B05137_07_077.jpg)。它通常被解释为权重分布上的“拉普拉斯先验”。
- en: Limitations of neural networks
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的局限性
- en: In this section, we will discuss in detail the issues faced by neural networks,
    which will become the stepping stone for building deep learning networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细讨论神经网络面临的问题，这将成为构建深度学习网络的垫脚石。
- en: Vanishing gradients, local optimum, and slow training
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度、局部最优和慢速训练
- en: 'One of the major issues with neural networks is the problem of "vanishing gradient"
    (*References* [8]). We will try to give a simple explanation of the issue rather
    than exploring the mathematical derivations in depth. We will choose the sigmoid
    activation function and a two-layer neural network, as shown in the following
    figure, to demonstrate the issue:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的主要问题之一是“消失梯度”问题 (*参考文献* [8])。我们将尝试给出对该问题的简单解释，而不是深入探讨数学推导。我们将选择 sigmoid
    激活函数和两层神经网络，如图所示，以演示该问题：
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_078.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度、局部最优和慢速训练](img/B05137_07_078.jpg)'
- en: 'Figure 5: Vanishing Gradient issue.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：消失梯度问题。
- en: As we saw in the activation function description, the sigmoid function squashes
    the output between the range 0 and 1\. The derivative of the sigmoid function
    *g'(a) = g(a)(*1 *– g(a))* has a range between 0 and 0.25\. The goal of learning
    is to minimize the output loss, that is, ![Vanishing gradients, local optimum,
    and slow training](img/B05137_07_079.jpg). In general, the output error does not
    go to 0, so maximum iterations; a user-specified parameter determines the quality
    of learning and backpropagation of the errors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在激活函数描述中看到的，sigmoid 函数将输出压缩在 0 和 1 之间。sigmoid 函数的导数 *g'(a) = g(a)(*1 *–
    g(a))* 的范围在 0 和 0.25 之间。学习的目标是使输出损失最小化，即 ![消失梯度、局部最优和慢速训练](img/B05137_07_079.jpg)。一般来说，输出误差不会降到
    0，所以最大迭代次数；一个用户指定的参数决定了学习的质量和误差的反向传播。
- en: 'Simplifying to illustrate the effect of output error on the input weight layer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 简化以说明输出误差对输入权重层的影响：
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_080.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度、局部最优和慢速训练](img/B05137_07_080.jpg)'
- en: 'Each of the transformations, for instance, from output to hidden, involves
    multiplication of two terms, both less than 1:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 每个转换，例如，从输出到隐藏，涉及两个小于 1 的项的乘法：
- en: '![Vanishing gradients, local optimum, and slow training](img/B05137_07_081.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![消失梯度、局部最优和慢速训练](img/B05137_07_081.jpg)'
- en: Thus, the value becomes so small when it reaches the input layer that the propagation
    of the gradient has almost vanished. This is known as the vanishing gradient problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当它达到输入层时，值变得如此之小，以至于梯度的传播几乎消失。这被称为消失梯度问题。
- en: A paradoxical situation arises when you need to add more layers to make features
    more interesting in the hidden layers. But adding more layers also increases the
    errors. As you add more layers, the input layers become "slow to train," which
    causes the output layers to be more inaccurate as they are dependent on the input
    layers; further, and for the same number of iterations, the errors increase with
    the increase in the number of layers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要添加更多层来使隐藏层中的特征更加有趣时，就会出现一个矛盾的情况。但添加更多层也会增加错误。随着你添加更多层，输入层变得“训练缓慢”，这导致输出层更加不准确，因为它们依赖于输入层；进一步地，对于相同的迭代次数，随着层数的增加，错误也会增加。
- en: With a fixed number of maximum iterations, more layers and slow propagation
    of errors can lead to a "local optimum."
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定最大迭代次数的情况下，更多的层和缓慢的误差传播可能导致“局部最优”。
- en: Another issue with basic neural networks is the number of parameters. Finding
    effective size and weights for each hidden layer and bias becomes more challenging
    with the increase in the number of layers. If we increase the number of layers,
    the parameters increase in polynomials. Fitting the parameters for the data requires
    a large number of data samples. This can result in the problem discussed before,
    that is, overfitting.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基本神经网络的一个问题是参数的数量。随着层数的增加，找到每个隐藏层和偏置的有效大小和权重变得更加具有挑战性。如果我们增加层数，参数将以多项式增长。为数据拟合参数需要大量的数据样本。这可能导致之前讨论过的问题，即过拟合。
- en: In the next few sections, we will start learning about the building blocks of
    deep learning that help overcome these issues.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将开始学习帮助克服这些问题的深度学习构建块。
- en: Deep learning
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: Deep learning includes architectures and techniques for supervised and unsupervised
    learning with the capacity to internalize the abstract structure of high-dimensional
    data using networks composed of building blocks to create discriminative or generative
    models. These techniques have proved enormously successful in recent years and
    any reader interested in mastering them must become familiar with the basic building
    blocks of deep learning first and understand the various types of networks in
    use by practitioners. Hands-on experience building and tuning deep neural networks
    is invaluable if you intend to get a deeper understanding of the subject. Deep
    learning, in various domains such as image classification and text learning, incorporates
    feature generation in its structures thus making the task of mining the features
    redundant in many applications. The following sections provide a guide to the
    concepts, building blocks, techniques for composing architectures, and training
    deep networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习包括用于监督学习和无监督学习的架构和技术，它能够使用由构建块组成的网络来内化高维数据的抽象结构，以创建判别性或生成模型。这些技术在近年来证明取得了巨大成功，任何希望掌握这些技术的读者都必须首先熟悉深度学习的基本构建块，并了解从业者使用的各种网络类型。如果你打算更深入地了解该主题，那么实际构建和调整深度神经网络的经验是无价的。在图像分类和文本学习等各个领域，深度学习在其结构中融合了特征生成，从而使得在许多应用中挖掘特征的任务变得多余。以下章节提供了概念、构建块、架构组合技术和训练深度网络的指南。
- en: Building blocks for deep learning
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的构建块
- en: In the following sections, we introduce the most important components used in
    deep learning, including Restricted Boltzmann machines, Autoencoders, and Denoising
    Autoencoders, how they work, and their advantages and limitations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将介绍深度学习中最重要的组件，包括受限玻尔兹曼机、自编码器和去噪自编码器，它们的工作原理以及它们的优缺点。
- en: Rectified linear activation function
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩形线性激活函数
- en: 'The Reclin function is given by the equation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Reclin函数由以下方程给出：
- en: '*g*(*a*) *= reclin* (*a*) *= max* (0*,* *a*)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*g*(*a*) *= reclin* (*a*) *= max* (0*,* *a*)'
- en: It can be seen as having a lower bound of 0 and no upper bound, strictly increasing,
    and a positive transformation function that just does linear transformation of
    positives.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以看作是下限为0且没有上限，严格递增，并且仅对正数进行线性变换的正变换函数。
- en: It is easier to see that the rectified linear unit or ReLu has a derivative
    of 1 or identity for values greater than 0\. This acts as a significant benefit
    as the derivatives are not squashed and do not have diminishing values when chained.
    One of the issues with ReLu is that the value is 0 for negative inputs and the
    corresponding neurons act as "dead", especially when a large negative value is
    learned for the bias term. ReLu cannot recover from this as the input and derivative
    are both 0\. This is generally solved by having a "leaky ReLu". These functions
    have a small value for negative inputs and are given by ![Rectified linear activation
    function](img/B05137_07_083.jpg) where ? = 0.01, typically.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 可以更容易地看出，修正线性单元或ReLu对于大于0的值具有1或恒等导数。这作为一个显著的好处，因为导数没有被压缩，并且在链式操作中不具有递减值。ReLu的一个问题是对于负输入值，其值为0，相应的神经元充当“死亡”状态，尤其是在偏置项学习到较大的负值时。ReLu无法从这种情况中恢复，因为输入和导数都是0。这通常通过具有“泄漏ReLu”来解决。这些函数对于负输入值具有较小的值，并由以下公式给出
    ![矩形线性激活函数](img/B05137_07_083.jpg)，其中 ? = 0.01，通常是。
- en: Restricted Boltzmann Machines
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: Restricted Boltzmann Machines (RBM) is an unsupervised learning neural network
    (*References* [11]). The idea of RBM is to extract "more meaningful features"
    from labeled or unlabeled data. It is also meant to "learn" from the large quantity
    of unlabeled data available in many domains when getting access to labeled data
    is costly or difficult.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机（RBM）是一种无监督学习神经网络（*参考文献* [11]）。RBM的想法是从标记或未标记数据中提取“更有意义的特征”。它还旨在在获取标记数据成本高昂或困难时，从许多领域中的大量未标记数据中“学习”。
- en: Definition and mathematical notation
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义和数学符号
- en: In its basic form RBM assumes inputs to be binary values 0 or 1 in each dimension.
    RBMs are undirected graphical models having two layers, a visible layer represented
    as *x* and a hidden layer *h,* and connections *W*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在其基本形式中，RBM假设每个维度上的输入是二进制值0或1。RBM是无向图模型，具有两层，一个表示为*x*的可见层和一个隐藏层*h*，以及连接*W*。
- en: 'RBM defines a distribution over the visible layer that involves the latent
    variables from the hidden layer. First an energy function is defined to capture
    the relationship between the visible and the hidden layers in vector form as:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: RBM定义了一个涉及隐藏层潜在变量的可见层分布。首先定义一个能量函数来捕捉可见层和隐藏层之间在向量形式上的关系：
- en: '![Definition and mathematical notation](img/B05137_07_088.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_088.jpg)'
- en: 'In scalar form the energy function can be defined as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在标量形式中，能量函数可以定义为：
- en: '![Definition and mathematical notation](img/B05137_07_089.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_089.jpg)'
- en: The probability of the distribution is given by ![Definition and mathematical
    notation](img/B05137_07_090.jpg) where *Z* is called the "partitioning function",
    which is an enumeration over all the values of *x and h*, which are binary, resulting
    in exponential terms and thus making it intractable!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的概率由![分布的定义和数学符号](img/B05137_07_090.jpg)给出，其中*Z*被称为“配分函数”，它是对所有* x 和 h*值的枚举，它们是二进制值，导致指数项，因此使其难以处理！
- en: '![Definition and mathematical notation](img/B05137_07_093.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_093.jpg)'
- en: 'Figure 6: Connection between the visible layer and hidden layer.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：可见层和隐藏层之间的连接。
- en: 'The Markov network view of the same in scalar form can be represented using
    all the pairwise factors, as shown in the following figure. This also makes it
    clear why it is called a "restricted" Boltzmann machine as there is no connection
    among units within a given hidden layer or in the visible layers:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 相同内容的马尔可夫网络视图可以用所有成对因子表示，如下图所示。这也清楚地说明了为什么它被称为“受限”玻尔兹曼机，因为给定隐藏层或可见层内的单元之间没有连接：
- en: '![Definition and mathematical notation](img/B05137_07_094.jpg)![Definition
    and mathematical notation](img/B05137_07_095.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![分布的定义和数学符号](img/B05137_07_094.jpg)![分布的定义和数学符号](img/B05137_07_095.jpg)'
- en: 'Figure 7: Input and hidden layers as scalars'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：输入和隐藏层作为标量
- en: We have seen that the whole probability distribution function ![Definition and
    mathematical notation](img/B05137_07_090.jpg) is intractable. We will now derive
    the basic conditional probability distributions for *x, h*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，整个概率分布函数![分布的定义和数学符号](img/B05137_07_090.jpg)难以处理。现在我们将推导出*x, h*的基本条件概率分布。
- en: Conditional distribution
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 条件分布
- en: 'Although computing the whole *p(x, h)* is intractable, the conditional distribution
    of *p(x|h)* or *p(h|x)* can be easily defined and shown to be a Bernoulli distribution
    and tractable:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算整个*p(x, h)*是难以处理的，但*p(x|h)*或*p(h|x)*的条件分布可以很容易地定义并证明是伯努利分布且可处理：
- en: '![Conditional distribution](img/B05137_07_100.jpg)![Conditional distribution](img/B05137_07_101.jpg)![Conditional
    distribution](img/B05137_07_102.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![条件分布](img/B05137_07_100.jpg)![条件分布](img/B05137_07_101.jpg)![条件分布](img/B05137_07_102.jpg)'
- en: 'Similarly, being symmetric and undirected:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，由于对称和无向性：
- en: '![Conditional distribution](img/B05137_07_103.jpg)![Conditional distribution](img/B05137_07_104.jpg)![Conditional
    distribution](img/B05137_07_105.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![条件分布](img/B05137_07_103.jpg)![条件分布](img/B05137_07_104.jpg)![条件分布](img/B05137_07_105.jpg)'
- en: Free energy in RBM
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RBM中的自由能
- en: 'The distribution of input or the observed variable is:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 输入或观察变量的分布是：
- en: '![Free energy in RBM](img/B05137_07_106.jpg)![Free energy in RBM](img/B05137_07_107.jpg)![Free
    energy in RBM](img/B05137_07_108.jpg)![Free energy in RBM](img/B05137_07_109.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![RBM中的自由能](img/B05137_07_106.jpg)![RBM中的自由能](img/B05137_07_107.jpg)![RBM中的自由能](img/B05137_07_108.jpg)![RBM中的自由能](img/B05137_07_109.jpg)'
- en: The function *F(x)* is called free energy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*F(x)*被称为自由能。
- en: Training the RBM
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练RBM
- en: 'RBMs are trained using the optimization objective of minimizing the average
    negative log-likelihood over the entire training data. This can be represented
    as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是通过在整个训练数据上最小化平均负对数似然度来训练的。这可以表示为：
- en: '![Training the RBM](img/B05137_07_111.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![训练 RBM](img/B05137_07_111.jpg)'
- en: 'The optimization is carried out by using stochastic gradient descent:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 优化是通过使用随机梯度下降来进行的：
- en: '![Training the RBM](img/B05137_07_112.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![训练 RBM](img/B05137_07_112.jpg)'
- en: The term ![Training the RBM](img/B05137_07_113.jpg) is called the "positive
    phase" and the term ![Training the RBM](img/B05137_07_114.jpg) is called the "negative
    phase" because of how they affect the probability distributions—the positive phase,
    because it increases the probability of training data by reducing the free energy,
    and the negative phase, as it decreases the probability of samples generated by
    the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 术语![训练 RBM](img/B05137_07_113.jpg)被称为“正相”，而术语![训练 RBM](img/B05137_07_114.jpg)被称为“负相”，因为它们对概率分布的影响——正相，因为它通过减少自由能来增加训练数据的概率，而负相，因为它减少了模型生成的样本的概率。
- en: It has been shown that the overall gradient is difficult to compute analytically
    because of the "negative phase", as it is computing the expectation over all possible
    configurations of the input data under the distribution formed by the model and
    making it intractable!
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，由于“负相”，整体梯度难以从理论上计算，因为它是在模型形成的分布下计算输入数据的所有可能配置的期望，这使得它变得难以处理！
- en: To make the computation tractable, estimation is carried out using a fixed number
    of model samples and they are referred to as "negative particles" denoted by *N*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使计算可行，使用固定数量的模型样本进行估计，并将它们称为“负粒子”，用*N*表示。
- en: 'The gradient can be now written as the approximation:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度现在可以写成近似形式：
- en: '![Training the RBM](img/B05137_07_116.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![训练 RBM](img/B05137_07_116.jpg)'
- en: Where particles ![Training the RBM](img/B05137_07_117.jpg) are sampled using
    some sampling techniques such as the Monte Carlo method.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中粒子![训练 RBM](img/B05137_07_117.jpg)是通过一些采样技术（如蒙特卡洛方法）采样的。
- en: Sampling in RBM
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RBM 中的采样
- en: Gibbs sampling is often the technique used to generate samples and learn the
    probability of *p(x,h)* in terms of *p(x|h)* and *p (h|x)*, which are relatively
    easy to compute, as shown previously.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 吉布斯抽样通常用于生成样本并学习*p(x,h)*的概率，这是基于*p(x|h)*和*p(h|x)*，它们相对容易计算，如前所述。
- en: 'Gibbs sampling for joint sampling of N random variables ![Sampling in RBM](img/B05137_07_119.jpg)
    is done using N sampling sub-steps of the form ![Sampling in RBM](img/B05137_07_120.jpg)
    where *S* *[-i]* contains samples up to and excluding step *S* *[i]*. Graphically,
    this can be shown as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对N个随机变量进行联合采样的吉布斯抽样![RBM 中的采样](img/B05137_07_119.jpg)是通过N个形式![RBM 中的采样](img/B05137_07_120.jpg)的采样子步骤来完成的，其中*S*
    *[-i]*包含从步骤*S* *[i]*到但不包括步骤*S* *[i]*的样本。从图形上看，可以表示如下：
- en: '![Sampling in RBM](img/B05137_07_123.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![RBM 中的采样](img/B05137_07_123.jpg)'
- en: 'Figure 8: Graphical representation of sampling done between hidden and input
    layers.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：隐藏层和输入层之间采样的图形表示。
- en: As ![Sampling in RBM](img/B05137_07_124.jpg) it can be shown that the sampling
    represents the actual distribution *p(x,h)*.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如![RBM 中的采样](img/B05137_07_124.jpg)所示，采样表示实际的分布*p(x,h)*。
- en: Contrastive divergence
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对比散度
- en: Contrastive divergence (CD) is a trick used to expedite the Gibbs sampling process
    described previously so it stops at step *k* of the process rather than continuing
    for a long time to guarantee convergence. It has been seen that even *k=1* is
    reasonable and gives good performance (*References* [10]).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对比散度（CD）是用于加速先前描述的吉布斯抽样过程的一个技巧，它停止在过程的步骤*k*而不是长时间继续以保证收敛。已经看到，即使*k=1*也是合理的，并且给出了良好的性能（*参考文献*
    [10]）。
- en: Inputs and outputs
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'These are the inputs to the algorithm:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是算法的输入：
- en: Training dataset
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集
- en: Number of steps for Gibbs sampling, *k*
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉布斯抽样的步数，*k*
- en: Learning rate a
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率a
- en: The output is the set of updated parameters
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是更新后的参数集
- en: How does it work?
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'The complete training pseudo-code using CD with the free energy function and
    partial derivatives can be given as:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自由能函数和偏导数的CD的完整训练伪代码可以表示为：
- en: 'For each instance in training **x**^t:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练中的每个实例**x**^t：
- en: Generate a negative particle ![How does it work?](img/B05137_07_117.jpg) using
    *k* steps of Gibbs Sampling.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用* k* 步吉布斯抽样生成一个负粒子![如何工作？](img/B05137_07_117.jpg)。
- en: 'Update the parameters:'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新参数：
- en: '![How does it work?](img/B05137_07_130.jpg)![How does it work?](img/B05137_07_131.jpg)![How
    does it work?](img/B05137_07_132.jpg)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_130.jpg)![它是如何工作的？](img/B05137_07_131.jpg)![它是如何工作的？](img/B05137_07_132.jpg)'
- en: Persistent contrastive divergence
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 持续对比散度
- en: Persistent contrastive divergence is another trick used to compute the joint
    probability *p(x,h)*. In this method, there is a single chain that does not reinitialize
    after every observed sample to find the negative particle ![Persistent contrastive
    divergence](img/B05137_07_117.jpg). It persists its state and parameters are updated
    just through running these k states by using the particle from the previous step.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 持续对比散度是另一种用于计算联合概率 *p(x,h)* 的技巧。在此方法中，有一个单链，在观察每个样本后不会重新初始化，以找到负粒子 ![持续对比散度](img/B05137_07_117.jpg)。它保持其状态，并且参数仅通过运行这些k状态并通过使用前一步的粒子来更新。
- en: Autoencoders
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自编码器
- en: An autoencoder is another form of unsupervised learning technique in neural
    networks. It is very similar to the feed-forward neural network described at the
    start with the only difference being it doesn't generate a class at output, but
    tries to replicate the input at the output layer (*References* [12 and 23]). The
    goal is to have hidden layer(s) capture the latent or hidden information of the
    input as features that can be useful in unsupervised or supervised learning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是神经网络中另一种无监督学习技术。它与开头描述的前馈神经网络非常相似，唯一的区别是它不生成输出层的类别，而是在输出层尝试复制输入。(*参考文献[12和23]*)
    目标是让隐藏层捕获输入的潜在或隐藏信息作为特征，这些特征在无监督或监督学习中可能是有用的。
- en: Definition and mathematical notations
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义和数学符号
- en: 'A single hidden layer example of an Autoencoder is shown in the following figure:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图中展示了自编码器的单个隐藏层示例：
- en: '![Definition and mathematical notations](img/B05137_07_133.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_133.jpg)'
- en: 'Figure 9: Autoencoder flow between layers'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：层之间的自编码器流程
- en: The input layer and the output layer have the same number of neurons similar
    as feed-forward, corresponding to the input vector, *x*. Each hidden layer can
    have greater, equal, or fewer neurons than the input or output layer and an activation
    function that does a non-linear transformation of the signal. It can be seen as
    using the unsupervised or latent hidden structure to "compress" the data effectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层具有与前馈网络相同的神经元数量，对应于输入向量 *x*。每个隐藏层可以比输入层或输出层有更多的神经元，也可以相等或更少，并且有一个执行信号的非线性变换的激活函数。它可以看作是使用无监督或潜在隐藏结构来“压缩”数据。
- en: 'The encoder or input transformation of the data by the hidden layer is given
    by:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层通过数据编码器或输入变换如下所示：
- en: '![Definition and mathematical notations](img/B05137_07_135.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_135.jpg)'
- en: 'And the decoder or output transformation of the data by the output layer is
    given by:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 并且数据通过输出层的解码器或输出变换如下所示：
- en: '![Definition and mathematical notations](img/B05137_07_136.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_136.jpg)'
- en: 'Generally, a sigmoid function with linear transformation of signals as described
    in the neural network section is popularly used in the layers:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在层中使用描述在神经网络部分的信号线性变换的Sigmoid函数是流行的：
- en: '![Definition and mathematical notations](img/B05137_07_137.jpg) and ![Definition
    and mathematical notations](img/B05137_07_138.jpg))'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_137.jpg) 和 ![定义和数学符号](img/B05137_07_138.jpg))'
- en: Loss function
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 损失函数
- en: The job of the loss function is to reduce the training error as before so that
    an optimization process such as a stochastic gradient function can be used.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的工作是减少训练误差，就像之前一样，以便可以使用如随机梯度函数之类的优化过程。
- en: 'In the case of binary valued input, the loss function is generally the average
    cross-entropy given by:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元值输入的情况下，损失函数通常是平均交叉熵，如下所示：
- en: '![Loss function](img/B05137_07_139.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/B05137_07_139.jpg)'
- en: 'It can be easily verified that, when the input signal and output signal match
    either 0 or 1, the error is 0\. Similarly, for real-valued input, a squared error
    is used:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 可以很容易地验证，当输入信号和输出信号匹配0或1时，误差为0。同样，对于实值输入，使用平方误差：
- en: '![Loss function](img/B05137_07_140.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/B05137_07_140.jpg)'
- en: 'The gradient of the loss function that is needed for the stochastic gradient
    procedure is similar to the feed-forward neural network and can be shown through
    derivation for both real-valued and binary as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 用于随机梯度过程的损失函数的梯度与前馈神经网络相似，可以通过推导在实值和二进制中展示如下：
- en: '![Loss function](img/B05137_07_141.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![损失函数](img/B05137_07_141.jpg)'
- en: Parameter gradients are obtained by back-propagating the ![Loss function](img/B05137_07_142.jpg)
    exactly as in the neural network.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向传播![损失函数](img/B05137_07_142.jpg)来获得参数梯度，这与神经网络中的操作完全相同。
- en: Limitations of Autoencoders
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动编码器的局限性
- en: 'Autoencoders have some known drawbacks that have been addressed by specialized
    architectures that we will discuss in the sections to follow. These limitations
    are:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器存在一些已知的缺点，这些问题将在后续章节中讨论。这些局限性包括：
- en: When the size of the Autoencoder is equal to the number of neurons in the input,
    there is a chance that the weights learned by the Autoencoders are just the identity
    vectors and that the whole representation simply passes on the inputs exactly
    as outputs with zero loss. Thus, they emulate "rote learning" or "memorization"
    without any generalization.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动编码器的大小等于输入层中的神经元数量时，存在一种可能性，即自动编码器学习到的权重只是身份向量，整个表示仅仅将输入原封不动地传递为输出，损失为零。因此，它们模拟了“死记硬背”或“记忆”而没有任何泛化。
- en: When the size of the Autoencoder is greater than the number of neurons in the
    input, the configuration is called an "overcomplete" hidden layer and can have
    similar problems to the ones mentioned previously. Some of the units can be turned
    off and others can become identity making it just the copy unit.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动编码器的大小大于输入层中的神经元数量时，这种配置称为“过完备”隐藏层，可能存在与之前提到的问题相似的问题。一些单元可能被关闭，而其他单元可能成为身份，使其仅成为复制单元。
- en: When the size of the Autoencoder is less than the number of neurons in the input,
    known as "undercomplete", the latent structure in the data or important hidden
    components can be discovered.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动编码器的大小小于输入层中的神经元数量，称为“欠完备”时，可以在数据中发现潜在结构或重要的隐藏组件。
- en: Denoising Autoencoder
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 去噪自动编码器
- en: As mentioned previously, when the Autoencoder has a hidden layer size greater
    than or equal to that of the input, it is not guaranteed to learn the weights
    and can become simply a unit switch to copy input to output. This issue is addressed
    by the Denoising Autoencoder. Here there is another layer added between input
    and the hidden layer. This layer adds some noise to the input using either a well-known
    distribution ![Denoising Autoencoder](img/B05137_07_143.jpg) or using stochastic
    noise such as turning a bit to 0 in binary input. This "noisy" input then goes
    through learning from the hidden layer to the output layer exactly like the Autoencoder.
    The loss function of the Denoising Autoencoder compares the output with the actual
    input. Thus, the added noise and the larger hidden layer enable either learning
    latent structures or adding/removing redundancy to produce the exact signal at
    the output. This architecture—where non-zero features at the noisy layer generate
    features at the hidden layer that are themselves transformed by the activation
    layer as the signal advances forward—lends a robustness and implicit structure
    to the learning press (*References* [15]).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当自动编码器的隐藏层大小大于或等于输入层时，并不能保证学习到权重，它可能仅仅是一个将输入复制到输出的单元开关。这个问题通过去噪自动编码器得到解决。在这里，在输入和隐藏层之间增加了一个额外的层。这个层使用已知的分布![去噪自动编码器](img/B05137_07_143.jpg)或使用随机噪声（例如将二进制输入中的一个位转换为0）向输入添加一些噪声。这个“噪声”输入随后通过从隐藏层到输出层的精确学习过程，就像自动编码器一样。去噪自动编码器的损失函数将输出与实际输入进行比较。因此，添加的噪声和更大的隐藏层使得学习潜在结构或添加/删除冗余以在输出端产生精确信号成为可能。这种架构——其中噪声层中的非零特征在信号向前推进时由激活层转换成隐藏层中的特征——为学习过程提供了鲁棒性和隐式结构（*参考文献*
    [15]）。
- en: '![Denoising Autoencoder](img/B05137_07_144.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自动编码器](img/B05137_07_144.jpg)'
- en: 'Figure 10: Denoising Autoencoder'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：去噪自动编码器
- en: Unsupervised pre-training and supervised fine-tuning
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督预训练和监督微调
- en: As we discussed in the issues section on neural networks, the issue with over-training
    arises especially in deep learning as the number of layers, and hence parameters,
    is large. One way to account for over-fitting is to do data-specific regularization.
    In this section, we will describe the "unsupervised pre-training" method done
    in the hidden layers to overcome the issue of over-fitting. Note that this is
    generally the "initialization process" used in many deep learning algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在神经网络问题部分所讨论的，过拟合问题在深度学习中尤为突出，因为层数和参数数量都很大。解决过拟合的一种方法是对数据进行特定正则化。在本节中，我们将描述在隐藏层中进行的“无监督预训练”方法，以克服过拟合问题。请注意，这通常是许多深度学习算法中使用的“初始化过程”。
- en: The algorithm of unsupervised pre-training works in a layer-wise greedy fashion.
    As shown in the following figure, one layer of a visible and hidden structure
    is considered at a given time. The weights of this layer are learned for a few
    iterations using unsupervised techniques such as RBM, described previously. The
    output of the hidden layer is then used as a "visible" or "input" layer and the
    training proceeds to the next, and so on.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督预训练算法以层级的贪婪方式工作。如图所示，在给定时间考虑一个可见和隐藏结构的一层。使用之前描述的无监督技术（如RBM）对该层的权重进行几次迭代学习。然后，隐藏层的输出被用作“可见”或“输入”层，训练继续进行到下一层，依此类推。
- en: Each learning of layers can be thought of as a "feature extraction or feature
    generation" process. The real data inputs when transformed form higher-level features
    at a given layer and then are further combined to form much higher-level features,
    and so on.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每层的学习可以被视为一个“特征提取或特征生成”过程。当实际数据输入被转换时，在给定层形成高级特征，然后进一步组合形成更高层次的特征，依此类推。
- en: '![Unsupervised pre-training and supervised fine-tuning](img/B05137_07_145.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![无监督预训练和监督微调](img/B05137_07_145.jpg)'
- en: 'Figure 11: Layer wise incremental learning through unsupervised learning.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：通过无监督学习逐层增量学习。
- en: Once all the hidden layer parameters are learned in pre-training using unsupervised
    techniques as described previously, a supervised fine-tuning process follows.
    In the supervised fine-tuning process, a final output layer is added and, just
    like in a neural network, training is done with forward and backward propagation.
    The idea is that most weights or parameters are almost fully tuned and only need
    a small change for producing a discriminative class mapping at the output.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在预训练中使用之前描述的无监督技术学习所有隐藏层参数，接下来将进行监督微调过程。在监督微调过程中，添加一个最终输出层，就像在神经网络中一样，通过前向和反向传播进行训练。其想法是大多数权重或参数几乎完全调整，只需要微小变化就能在输出产生判别性类别映射。
- en: '![Unsupervised pre-training and supervised fine-tuning](img/B05137_07_146.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![无监督预训练和监督微调](img/B05137_07_146.jpg)'
- en: 'Figure 12: Final tuning or supervised learning.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：最终调整或监督学习。
- en: Deep feed-forward NN
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度前馈神经网络
- en: A deep feed-forward neural network involves using the stages pre-training, and
    fine-tuning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈神经网络涉及使用预训练和微调阶段。
- en: 'Depending on the unsupervised learning technique used—RBM, Autoencoders, or
    Denoising Autoencoders—different algorithms are formed: Stacked RBM, Stacked Autoencoders,
    and Stacked Denoising Autoencoders, respectively.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的无监督学习技术——RBM、自编码器或降噪自编码器——形成不同的算法：堆叠RBM、堆叠自编码器和堆叠降噪自编码器，分别。
- en: Input and outputs
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Given an architecture for the deep feed-forward neural net, these are the inputs
    for training the network:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 给定深度前馈神经网络的架构，以下是为训练网络提供的输入：
- en: Number of layers *L*
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数数量 *L*
- en: Dataset without labels *D*
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有标签的数据集 *D*
- en: Dataset with labels *D*
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有标签的数据集 *D*
- en: Number of training iterations *n*
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练迭代次数 *n*
- en: How does it work?
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'The generalized learning/training algorithm for all three is given as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种的通用学习/训练算法如下所示：
- en: 'For layers *l=1 to L* (Pre-Training):'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于层 *l=1 到 L*（预训练）：
- en: Dataset without Labels ![How does it work?](img/B05137_07_151.jpg)
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有标签的数据集 ![如何工作？](img/B05137_07_151.jpg)
- en: Perform Step-wise Layer Unsupervised Learning (RBM, Autoencoders, or Denoising
    Autoencoders)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行逐步层无监督学习（RBM、自编码器或降噪自编码器）
- en: Finalize the parameters **W**^l, **b**^l from the preceding step
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前一步骤中最终确定参数 **W**^l, **b**^l
- en: For the output layer *(L+1)* perform random initialization of parameters **W***^L*^(+1),
    **b***^L* ^(+1).
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于输出层 *(L+1)* 执行参数 **W***^L*^(+1), **b***^L* ^(+1) 的随机初始化。
- en: 'For layers *l=1 to L+1* (Fine-Tuning):'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于层 *l=1 到 L+1*（微调）：
- en: Dataset with Labels ![How does it work?](img/B05137_07_156.jpg).
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带标签的数据集 ![它是如何工作的？](img/B05137_07_156.jpg)。
- en: Use the pre-initialized weights from 1\. (**W**^l, **b**^l).
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从1.（**W**^l, **b**^l）预初始化的权重。
- en: Perform forward-backpropagation for *n* iterations.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行 *n* 次前向-反向传播。
- en: Deep Autoencoders
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度自编码器
- en: Deep Autoencoders have many layers of hidden units, which shrink to a very small
    dimension and then symmetrically grow to the input size.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器具有许多隐藏单元层，这些层缩小到一个非常小的维度，然后对称地增长到输入大小。
- en: '![Deep Autoencoders](img/B05137_07_158.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![深度自编码器](img/B05137_07_158.jpg)'
- en: 'Figure 13: Deep Autoencoders'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：深度自编码器
- en: The idea behind Deep Autoencoders is to create features that capture latent
    complex structures of input using deep networks and at the same time overcome
    the issue of gradients and underfitting due to the deep structure. It was shown
    that this methodology generated better features and performed better than PCA
    on many datasets (*References* [13]).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器背后的想法是使用深度网络创建能够捕捉输入的潜在复杂结构的特征，同时克服由于深度结构导致的梯度消失和欠拟合问题。已经证明，这种方法生成的特征在许多数据集上比PCA表现更好（*参考文献*
    [13]）。
- en: 'Deep Autoencoders use the concept of pre-training, encoders/decoders, and fine-tuning
    to perform unsupervised learning:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器使用预训练、编码器/解码器和微调的概念来执行无监督学习：
- en: 'In the pre-training phase, the RBM methodology is used to learn greedy stepwise
    parameters of the encoders, as shown in the following figure, for initialization:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，使用RBM方法学习编码器的贪婪逐步参数，如图所示，用于初始化：
- en: '![Deep Autoencoders](img/B05137_07_159.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![深度自编码器](img/B05137_07_159.jpg)'
- en: 'Figure 14: Stepwise learning in RBM'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：RBM的逐步学习
- en: In the unfolding phase the same parameters are symmetrically applied to the
    decoder network for initialization.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在展开阶段，相同的参数对称地应用于解码器网络进行初始化。
- en: Finally, fine-tuning backpropagation is used to adjust the parameters across
    the entire network.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用全网络的反向传播微调来调整参数。
- en: Deep Belief Networks
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: Deep Belief Networks (DBNs) are the origin of the concept of unsupervised pre-training
    (*References* [9]). Unsupervised pre-training originated from DBNs and then was
    found to be equally useful and effective in the feed-forward supervised deep networks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 深度信念网络（DBNs）是无监督预训练概念的起源（*参考文献* [9]）。无监督预训练起源于DBNs，后来发现它在前馈监督深度网络中也同样有用和有效。
- en: Deep belief networks are not supervised feed-forward networks, but a generative
    model to generate data samples.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 深度信念网络不是监督的前馈网络，而是一个生成模型，用于生成数据样本。
- en: Inputs and outputs
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: The input layer is the instance of data, represented by one neuron for each
    input feature. The output of a DBN is a reconstruction of the input from a hierarchy
    of learned features of increasingly greater abstraction.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层是数据的实例，每个输入特征用一个神经元表示。DBN的输出是从学习到的特征层次中重建的输入，这些特征具有越来越大的抽象性。
- en: How does it work?
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: How a DBN learns the joint distribution of the input data is explained here
    using a three-layer DBN architecture as an example.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用三层深度信念网络（DBN）架构来解释DBN学习输入数据的联合分布。
- en: '![How does it work?](img/B05137_07_160.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_160.jpg)'
- en: 'Figure 15: Deep belief network'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：深度信念网络
- en: The three-hidden-layered DBN as shown has a first layer of undirected RBM connected
    to a two-layered Bayesian network. The Bayesian network with a sigmoid activation
    function is called a sigmoid Bayesian network (SBN).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，具有三个隐藏层的DBN包含一个无向的RBM层，该层连接到两层贝叶斯网络。具有sigmoid激活函数的贝叶斯网络被称为sigmoid贝叶斯网络（SBN）。
- en: The goal of a generative model is to learn the joint distribution as given by
    *p*(**x**,**h**^((1)),**h**^((2)),**h**^((3)))
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的目标是学习由 *p*(**x**,**h**^((1)),**h**^((2)),**h**^((3))) 给出的联合分布
- en: '*p*(**x**,**h**^((1)),**h**^((2)),**h**^((3))) = *p*(**h**²),**h**^((3)))*p*(**h**^((1))|**h**^((2)))
    *p*(**x**|**h**^((1)))'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(**x**,**h**^((1)),**h**^((2)),**h**^((3))) = *p*(**h**²),**h**^((3)))*p*(**h**^((1))|**h**^((2)))
    *p*(**x**|**h**^((1)))'
- en: 'RBM computation as seen before gives us:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的RBM计算给我们：
- en: '![How does it work?](img/B05137_07_163.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_163.jpg)'
- en: 'The Bayesian Network in the next two layers is:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 下一两层中的贝叶斯网络是：
- en: '![How does it work?](img/B05137_07_164.jpg)![How does it work?](img/B05137_07_165.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_07_164.jpg)![它是如何工作的？](img/B05137_07_165.jpg)'
- en: 'For binary data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二进制数据：
- en: '![How does it work?](img/B05137_07_166.jpg)![How does it work?](img/B05137_07_166.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_07_166.jpg)![如何工作？](img/B05137_07_166.jpg)'
- en: Deep learning with dropouts
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 具有丢弃法的深度学习
- en: Another technique used to overcome the "overfitting" issues mentioned in deep
    neural networks is using the dropout technique to learn the parameters. In the
    next sections, we will define, illustrate, and explain how deep learning with
    dropouts works.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于克服深度神经网络中提到的“过拟合”问题的技术是使用丢弃技术来学习参数。在接下来的几节中，我们将定义、说明并解释具有丢弃法的深度学习是如何工作的。
- en: Definition and mathematical notation
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义和数学符号
- en: The idea behind dropouts is to "cripple" the deep neural network structure by
    stochastically removing some of the hidden units as shown in the following figure
    after the parameters are learned. The units are set to 0 with the dropout probability
    generally set as *p=0.5*
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃背后的想法是通过随机移除一些隐藏单元来“削弱”深度神经网络结构，如图所示，在参数学习之后。这些单元被设置为0，丢弃概率通常设置为 *p=0.5*
- en: The idea is similar to adding noise to the input, but done in all the hidden
    layers. When certain features (or a combination of features) are removed stochastically,
    the neural network has to learn latent features in a more robust way, without
    the interdependence of some features.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 灵感类似于向输入添加噪声，但这是在所有隐藏层中完成的。当随机移除某些特征（或特征的组合）时，神经网络必须以更稳健的方式学习潜在特征，而不依赖于某些特征之间的相互依赖性。
- en: '![Definition and mathematical notation](img/B05137_07_169.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_169.jpg)'
- en: 'Figure 16: Deep learning with dropout indicated by dropping certain units with
    dark shading.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：通过用深色阴影表示丢弃某些单元来指示具有丢弃法的深度学习。
- en: 'Each hidden layer is represented by *h*^k*(x)* where *k* is the layer. The
    pre-activation for layer *0<k<l* is given by:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 每个隐藏层由 *h*^k*(x)* 表示，其中 *k* 是层。对于 *0<k<l* 层的预激活由以下给出：
- en: '![Definition and mathematical notation](img/B05137_07_016.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_016.jpg)'
- en: 'The hidden layer activation for *1< k < l*. Binary masks are represented by
    **m**^k at each hidden layer:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *1< k < l* 的隐藏层激活。二进制掩码在每个隐藏层由 **m**^k 表示：
- en: '![Definition and mathematical notation](img/B05137_07_172.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_172.jpg)'
- en: 'The final output layer activation is:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出层激活为：
- en: '![Definition and mathematical notation](img/B05137_07_019.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![定义和数学符号](img/B05137_07_019.jpg)'
- en: Inputs and outputs
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'For training with dropouts, inputs are:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用丢弃法的训练，输入如下：
- en: Network architecture
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络架构
- en: Training dataset
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集
- en: Dropout probability *p* (typically 0.5)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃概率 *p*（通常为0.5）
- en: The output is a trained deep neural net that can be applied for predictive use.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个经过训练的深度神经网络，可用于预测用途。
- en: How does it work?
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: We will now describe the different parts of how deep learning with dropouts
    works.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将描述具有丢弃法的深度学习的工作原理的不同部分。
- en: Learning Training and testing with dropouts
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用丢弃法的训练和测试
- en: 'The backward propagation learning of weights and biases from the output loss
    function using gradients is very similar to traditional neural network learning.
    The only difference is that masks are applied appropriately as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度从输出损失函数反向传播学习权重和偏置与传统的神经网络学习非常相似。唯一的区别是应用了适当的掩码，如下所示：
- en: 'Compute the output gradient before activation:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在激活之前计算输出梯度：
- en: '![Learning Training and testing with dropouts](img/B05137_07_066.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![使用丢弃法的训练和测试](img/B05137_07_066.jpg)'
- en: 'For hidden layers *k=l+1 to 1*:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于隐藏层 *k=l+1 到 1*：
- en: 'Compute the gradient of hidden layer parameters:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 计算隐藏层参数的梯度：
- en: '![Learning Training and testing with dropouts](img/B05137_07_068.jpg)![Learning
    Training and testing with dropouts](img/B05137_07_069.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![使用丢弃法的训练和测试](img/B05137_07_068.jpg)![使用丢弃法的训练和测试](img/B05137_07_069.jpg)'
- en: '**h**^(k-1) computation has taken into account the binary mask **m**^(k-1)
    applied.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**h**^(k-1) 的计算已考虑应用二进制掩码 **m**^(k-1)。'
- en: 'Compute the gradient of the hidden layer below the current:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在激活之前计算当前以下隐藏层的梯度：
- en: '![Learning Training and testing with dropouts](img/B05137_07_070.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![使用丢弃法的训练和测试](img/B05137_07_070.jpg)'
- en: 'Compute the gradient of the layer below before activation:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在激活之前计算以下层的梯度：
- en: '![Learning Training and testing with dropouts](img/B05137_07_177.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![使用丢弃法的训练和测试](img/B05137_07_177.jpg)'
- en: When testing the model, we cannot use the binary mask as it is stochastic; the
    "expectation" value of the mask is used. If the dropout probability is *p=0.5*,
    the same value 0.5 is used as the expectation for the unit at test or model application
    time.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试模型时，我们不能使用二值掩码，因为它是不确定的；使用掩码的“期望”值。如果dropout概率是*p=0.5*，则在测试或模型应用时间点的单元期望值使用相同的值0.5。
- en: Sparse coding
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏编码
- en: Sparse coding is another neural network used for unsupervised learning and feature
    generation (*References* [22]). It works on the principle of finding latent structures
    in high dimensions that capture the patterns, thus performing feature extraction
    in addition to unsupervised learning.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏编码是另一种用于无监督学习和特征生成的神经网络（*参考文献* [22]）。它基于在高维空间中寻找捕获模式的潜在结构的原则，从而在无监督学习之外执行特征提取。
- en: 'Formally, for every input **x**^((t)) a latent representation **h**^((t)) is
    learned, which has a sparse representation (most values are 0 in the vector).
    This is done by optimization using the following objective function:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，对于每个输入**x**^((t))，学习一个潜在表示**h**^((t))，它具有稀疏表示（向量中的大多数值都是0）。这是通过以下目标函数的优化来完成的：
- en: '![Sparse coding](img/B05137_07_180.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏编码](img/B05137_07_180.jpg)'
- en: Where the first term ![Sparse coding](img/B05137_07_181.jpg) is to control the
    reconstruction error and the second term, which uses a regularizer ?, is for sparsity
    control. The matrix **D** is also known as a Dictionary as it has equivalence
    to words in a dictionary and **h**^((t)) is similar to word frequency; together
    they capture the impact of words in extracting patterns when performing text mining.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一个项![稀疏编码](img/B05137_07_181.jpg)用于控制重建误差，第二个项，使用正则化器？用于稀疏控制。矩阵**D**也被称为字典，因为它与字典中的单词具有等价性，**h**^((t))类似于词频；它们共同捕捉到在执行文本挖掘时，单词在提取模式中的影响。
- en: Convolutional Neural Network
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Convolutional Neural Networks or CNNs have become prominent and are widely used
    in the computer vision domain. Computer vision involves processing images/videos
    for capturing knowledge and patterns. Annotating images, classifying images/videos,
    correcting them, story-telling or describing images, and so on, are some of the
    broad applications in computer visi [16].
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络或CNNs已成为突出且在计算机视觉领域得到广泛应用。计算机视觉涉及处理图像/视频以捕获知识和模式。标注图像、分类图像/视频、纠正它们、讲故事或描述图像等，是计算机视觉中的一些广泛应用[16]。
- en: 'Computer vision problems most generally have to deal with unstructured data
    that can be described as:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉问题通常需要处理无结构数据，这些数据可以描述为：
- en: Inputs that are 2D images with single or multiple color channels or 3D videos
    that are high-dimensional vectors.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是具有单色或多色通道的2D图像或高维向量的3D视频。
- en: The features in these 2D or 3D representations have a well-known spatial topology,
    a hierarchical structure, and some repetitive elements that can be exploited.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这些2D或3D表示中的特征具有众所周知的空间拓扑结构、层次结构和一些可利用的重复元素。
- en: The images/videos have a large number of transformations or variants based on
    factors such as illumination, noise, and so on. The same person or car can look
    different based on several factors.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图像/视频基于诸如光照、噪声等因素具有大量变换或变体。同一个人或汽车可以根据几个因素看起来不同。
- en: 'Next, we will describe some building blocks used in CNNs. We will use simple
    images such as the letter X of the alphabet to explain the concept and mathematics
    involved. For example, even though the same character X is represented in different
    ways in the following figure due to translation, scaling, or distortion, the human
    eye can easily read it as X, but it becomes tricky for the computer to see the
    pattern. The images are shown with the author''s permission (*References* [19]):'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述在CNNs中使用的某些构建块。我们将使用简单的图像，如字母表中的X，来解释涉及的概念和数学。例如，尽管由于平移、缩放或扭曲，以下图中相同的字符X以不同的方式表示，但人眼可以轻松地将其识别为X，但对于计算机来说，识别模式变得复杂。图像显示已获得作者的许可（*参考文献*
    [19]）：
- en: '![Convolutional Neural Network](img/B05137_07_184.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络](img/B05137_07_184.jpg)'
- en: 'Figure 17: Image of character X represented in different ways.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：以不同方式表示的字符X的图像。
- en: 'The following figure illustrates how a simple grayscale image of X has common
    features such as a diagonal from top left, a diagonal from top right, and left
    and right intersecting diagonals repeated and combined to form a larger X:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图示说明了如何将一个简单的灰度图像X具有的共同特征，如从左上角到右上角的斜线，以及左和右交叉的斜线重复并组合成更大的X：
- en: '![Convolutional Neural Network](img/B05137_07_185.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络](img/B05137_07_185.jpg)'
- en: 'Figure 18: Common features represented in the image of character X.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：字符X图像中表示的常见特征。
- en: Local connectivity
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 局部连通性
- en: This is the simple concept of dividing the whole image into "patches" or "recipient
    fields" and giving each patch to the hidden layers. As shown in the figure, instead
    of 9 X 9 pixels of the complete sample image, a 3 X 3 patch of pixels from the
    top left goes to the first hidden unit, the overlapping second patch goes to second,
    and so on.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是将整个图像划分为“补丁”或“接收场”并将每个补丁分配给隐藏层的简单概念。如图所示，而不是完整的样本图像的9 X 9像素，左上角的3 X 3像素补丁被送到第一个隐藏单元，重叠的第二补丁被送到第二个，依此类推。
- en: Since the fully connected hidden layer would have a huge number of parameters,
    having smaller patches completely reduces the parameter or high-dimensional space
    problem!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于完全连接的隐藏层会有大量的参数，拥有较小的补丁完全减少了参数或高维空间问题！
- en: '![Local connectivity](img/B05137_07_186.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![局部连通性](img/B05137_07_186.jpg)'
- en: 'Figure 19: Concept of patches on the whole image.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：整个图像上的补丁概念。
- en: Parameter sharing
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 参数共享
- en: The concept of parameter sharing is to construct a weight matrix that can be
    reused over different patches or recipient fields as constructed in the preceding
    figure in the local sharing. As shown in the following figure, the Feature map
    with same parameters **W**[1,1] and **W**[1,4] creates two different feature maps,
    Feature Map 1 and 4, both capturing the same features, that is, diagonal edges
    on either side. Thus, feature maps capture "similar regions" in the images and
    further reduce the dimensionality of the input space.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 参数共享的概念是构建一个权重矩阵，可以在不同的补丁或接收场中重复使用，如前图所示在局部共享中构建。如图所示，具有相同参数**W**[1,1]和**W**[1,4]的特征图创建出两个不同的特征图，特征图1和4，都捕捉到相同的特征，即两侧的对角线边缘。因此，特征图捕捉图像中的“相似区域”并进一步降低输入空间的维度。
- en: '![Parameter sharing](img/B05137_07_187.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![参数共享](img/B05137_07_187.jpg)'
- en: Discrete convolution
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离散卷积
- en: We will explain the steps in discrete convolution, taking a simple contrived
    example with simplified mathematics to illustrate the operation.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释离散卷积的步骤，通过一个简单的、经过简化的数学示例来说明这个操作。
- en: 'Suppose the kernel representing the diagonal feature is scanned over the entire
    image as a patch of 3 X 3\. If this kernel lands on the self-same feature in the
    input image and we have to compute the center value through what we call the convolution
    operator, we get the exact value of 1 because of the matching as shown:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 假设代表对角线特征的核在整个图像上作为一个3 X 3的补丁进行扫描。如果这个核落在输入图像中的相同特征上，并且我们必须通过我们所说的卷积算子来计算中心值，由于匹配，我们得到精确的值为1，如下所示：
- en: '![Discrete convolution](img/B05137_07_190.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![离散卷积](img/B05137_07_190.jpg)'
- en: 'Figure 21: Discrete convolution step.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：离散卷积步骤。
- en: 'The entire image when run through this kernel and convolution operator gives
    a matrix of values as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 当整个图像通过这个核和卷积算子运行时，会得到如下值的矩阵：
- en: '![Discrete convolution](img/B05137_07_191.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![离散卷积](img/B05137_07_191.jpg)'
- en: 'Figure 22: Transformation of the character image after a kernel and convolution
    operator.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：经过核和卷积算子变换后的字符图像。
- en: 'We can see how the left diagonal feature gets highlighted by running this scan.
    Similarly, by running other kernels, as shown in the following figure, we can
    get a "stack of filtered images":'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到如何通过这个扫描突出显示左对角线特征。同样，通过运行其他核，如图所示，我们可以得到一个“过滤图像堆叠”：
- en: '![Discrete convolution](img/B05137_07_192.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![离散卷积](img/B05137_07_192.jpg)'
- en: 'Figure 23: Different features run through the kernel giving a stack of images.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：不同的特征通过核运行，生成一系列图像。
- en: 'Each cell in the filtered images can be given as:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后的图像中的每个单元格可以表示为：
- en: '![Discrete convolution](img/B05137_07_193.jpg)![Discrete convolution](img/B05137_07_194.jpg)![Discrete
    convolution](img/B05137_07_195.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![离散卷积](img/B05137_07_193.jpg)![离散卷积](img/B05137_07_194.jpg)![离散卷积](img/B05137_07_195.jpg)'
- en: Pooling or subsampling
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 池化或子采样
- en: 'Pooling or subsampling works on the stack of filtered images to further shrink
    the image or compress it, while keeping the pattern as-is. The main steps carried
    out in pooling are:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 池化或子采样在过滤图像堆栈上工作，进一步缩小图像或压缩它，同时保持模式不变。池化中执行的主要步骤如下：
- en: Pick a window size (for example, 2 X 2) and a stride size (for example, 2).
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择窗口大小（例如，2 X 2）和步长大小（例如，2）。
- en: Move the window over all the filtered images at stride.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在步长中移动窗口，覆盖所有过滤图像。
- en: At each window, pick the "maximum" value.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个窗口中，选择“最大”值。
- en: '![Pooling or subsampling](img/B05137_07_196.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![池化或子采样](img/B05137_07_196.jpg)'
- en: 'Figure 24: Max pooling, done using a window size of 2 X 2 and stride of 2,
    computes cell values with maximum for first as 1.0, 0.33 for next, and so on.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：使用2 X 2窗口大小和2步长进行的最大池化，计算第一个单元格的最大值为1.0，下一个为0.33，依此类推。
- en: 'Pooling also plays an important part where the same features if moved or scaled
    can still be detected due to the use of maximum. The same set of stacked filtered
    images gets transformed into pooled images as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 池化也扮演着重要角色，由于使用了最大值，即使特征被移动或缩放，也可以检测到相同的特征。同一组堆叠的过滤图像被转换成池化图像，如下所示：
- en: '![Pooling or subsampling](img/B05137_07_197.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![池化或子采样](img/B05137_07_197.jpg)'
- en: 'Figure 25: Transformation showing how a stack of filtered images is converted
    to pooled images.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：展示如何将一叠过滤图像转换为池化图像的转换。
- en: Normalization using ReLU
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用ReLU进行归一化
- en: As we discussed in the building blocks of deep learning, ReLUs remove the negative
    by squashing it to 0 and keep the positives as-is. They also play an important
    role in gradient computation in the backpropagation, removing the vanishing gradient
    issue of vanishing gradient.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在深度学习的基石中讨论的那样，ReLU通过将其压缩到0来移除负值，并保持正值不变。它们在反向传播中的梯度计算中也扮演着重要的角色，解决了梯度消失的问题。
- en: '![Normalization using ReLU](img/B05137_07_198.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![使用ReLU进行归一化](img/B05137_07_198.jpg)'
- en: 'Figure 26: Transformation using ReLu.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图26：使用ReLU的转换。
- en: CNN Layers
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN层
- en: 'In this section, we will put together the building blocks discussed earlier
    to form the complete picture of CNNs. Combining the layers of convolution, ReLU,
    and pooling to form a connected network yielding shrunken images with patterns
    captured in the final output, we obtain the next composite building block, as
    shown in the following figure:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将把之前讨论的构建块组合起来，形成CNN的完整图景。通过结合卷积层、ReLU和池化层形成一个连接的网络，生成缩小后的图像，并在最终输出中捕获模式，我们得到下一个复合构建块，如图所示：
- en: '![CNN Layers](img/B05137_07_199.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![CNN层](img/B05137_07_199.jpg)'
- en: 'Figure 27: Basic Unit of CNN showing a combination of Convolution, ReLu, and
    Pooling.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图27：CNN的基本单元，展示了卷积、ReLU和池化的组合。
- en: 'Thus, these layers can be combined or "deep-stacked", as shown in the following
    figure, to form a complex network that gives a small pool of images as output:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些层可以组合或“深度堆叠”，如图所示，形成一个复杂的网络，输出一个小图像池：
- en: '![CNN Layers](img/B05137_07_200.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![CNN层](img/B05137_07_200.jpg)'
- en: 'Figure 28: Deep-stacking the basic units repeatedly to form CNN layers.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图28：通过重复堆叠基本单元来形成CNN层。
- en: The output layer is a fully connected network as shown, which uses a voting
    technique and learns the weights for the desired output. The fully connected output
    layer can be stacked too.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层是一个如所示的全连接网络，它使用投票技术并学习所需输出的权重。全连接输出层也可以堆叠。
- en: '![CNN Layers](img/B05137_07_201.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![CNN层](img/B05137_07_201.jpg)'
- en: 'Figure 29: Fully connected layer as output of CNN.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图29：CNN的输出层为全连接层。
- en: 'Thus, the final CNNs can be completely illustrated as follows:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的CNN可以完全如图所示：
- en: '![CNN Layers](img/B05137_07_202.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![CNN层](img/B05137_07_202.jpg)'
- en: 'Figure 30: CNNS with all layers showing inputs and outputs.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图30：显示所有层的CNNS输入和输出的图。
- en: As before, gradient descent is selected as the learning technique using the
    loss functions to compute the difference and propagate the error backwards.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，梯度下降被选为学习技术，使用损失函数来计算差异并反向传播错误。
- en: 'CNN''s can be used in other domains such as voice pattern recognition, text
    mining, and so on, if the mapping of the data to the "image" can be successfully
    done and "local spatial" patterns exist. The following figure shows one of the
    ways of mapping sound and text to images for CNN usage:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以将数据映射到“图像”并且存在“局部空间”模式，CNN可以用于其他领域，如声音模式识别、文本挖掘等。以下图示了将声音和文本映射到图像以供CNN使用的一种方法：
- en: '![CNN Layers](img/B05137_07_203.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![CNN层](img/B05137_07_203.jpg)'
- en: 'Figure 31: Illustration of mapping between temporal data, such as voice to
    spatial data, to an image.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图31：时间数据（如声音）映射到空间数据（如图像）的映射示意图。
- en: Recurrent Neural Networks
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Normal deep networks are used when you have finite inputs and there is no interdependence
    between the input examples or instances. When there are variable length inputs
    and there are temporal dependencies between them, that is, sequence related data,
    neural networks must be modified to handle such data. Recurrent Neural Networks
    (RNN) are examples of neural networks that are used widely to solve such problems,
    and we will discuss them in the following sections. RNNs are used in many sequence-related
    problems such as text mining, language modeling, bioinformatics data modeling,
    and so on, to name a few areas that fit this meta-level description (*References*
    [18 and 21]).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有有限输入且输入示例或实例之间没有相互依赖时，使用正常的深度网络。当存在可变长度输入且它们之间存在时间依赖性时，即序列相关数据，神经网络必须修改以处理此类数据。循环神经网络（RNN）是广泛用于解决此类问题的神经网络的例子，我们将在以下章节中讨论它们。RNN用于许多与序列相关的问题，如文本挖掘、语言建模、生物信息学数据建模等，仅举一些符合这一元级描述的领域（*参考文献*
    [18和21]）。
- en: Structure of Recurrent Neural Networks
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 循环神经网络的结构
- en: We will describe the simplest unit of the RNN first and then shown how it is
    combined to understand it functionally and mathematically and illustrate how different
    components interact and work.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先描述RNN的最简单单元，然后展示它是如何组合起来以理解其功能和数学原理，并说明不同组件如何相互作用和工作。
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_204.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络的结构](img/B05137_07_204.jpg)'
- en: 'Figure 32: Difference between an artificial neuron and a neuron with feedback.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图32：人工神经元与具有反馈的神经元的区别。
- en: 'Let''s consider the basic input, a neuron with activation, and its output at
    a given time *t*:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑基本输入，一个具有激活的神经元及其在给定时间 *t* 的输出：
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_206.jpg)![Structure
    of Recurrent Neural Networks](img/B05137_07_207.jpg)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络的结构](img/B05137_07_206.jpg)![循环神经网络的结构](img/B05137_07_207.jpg)'
- en: 'A neuron with feedback keeps a matrix **W**[R] to incorporate previous output
    at time *t-1* and the equations are:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有反馈的神经元保持一个矩阵 **W**[R]，以包含时间 *t-1* 的前一个输出，方程如下：
- en: '![Structure of Recurrent Neural Networks](img/B05137_07_210.jpg)![Structure
    of Recurrent Neural Networks](img/B05137_07_207.jpg)![Structure of Recurrent Neural
    Networks](img/B05137_07_211.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络的结构](img/B05137_07_210.jpg)![循环神经网络的结构](img/B05137_07_207.jpg)![循环神经网络的结构](img/B05137_07_211.jpg)'
- en: 'Figure 33: Chain of neurons with feedbacks connected together.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图33：具有反馈连接的神经元链。
- en: The basic RNN stacks the structure of hidden units as shown with feedback connected
    from the previous layer. At activation at time *t*, it depends not only on **x**^((t))
    as input, but also on the previous unit given by **W**[R]**h**^((t-1)). The weights
    in the feedback connection of RNN are generally the same across all the units,
    **W**[R]. Also, instead of emitting output at the very end of the feed-forward
    neural network, each unit continuously emits an output that can be used in the
    loss function calculation.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 基本RNN堆叠隐藏单元的结构，如图所示，从前一层反馈连接。在时间 *t* 的激活不仅依赖于输入 **x**^((t))，还依赖于由 **W**[R]**h**^((t-1))
    给出的前一个单元。RNN反馈连接中的权重通常在所有单元中相同，**W**[R]。此外，RNN不是在前馈神经网络的末端发出输出，而是每个单元持续发出输出，这些输出可用于损失函数的计算。
- en: Learning and associated problems in RNNs
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RNN中的学习和相关问题
- en: Working with RNNs presents some challenges that are specific to them but there
    are common problems that are also encountered in other types of neural net.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 与RNN一起工作会带来一些特定于它们的挑战，但也有在其他类型神经网络中也遇到的一些共同问题。
- en: The gradient used from the output loss function at any time *t* of the unit
    has dependency going back to the first unit or *t=0*, as shown in the following
    figure. This is because the partial derivative at the unit is dependent on the
    previous unit, since:![Learning and associated problems in RNNs](img/B05137_07_210.jpg)
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单元在任何时间 *t* 的输出损失函数中使用的梯度具有回溯到第一个单元或 *t=0* 的依赖性，如图所示。这是因为单元的偏导数依赖于前一个单元，因为：![RNN中的学习和相关问题](img/B05137_07_210.jpg)
- en: Backpropagation through time (BPTT) is the term used to illustrate the process.
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）是用于说明该过程的术语。
- en: '![Learning and associated problems in RNNs](img/B05137_07_215.jpg)'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![RNN中的学习和相关问题](img/B05137_07_215.jpg)'
- en: 'Figure 34: Backpropagation through time.'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图34：时间反向传播。
- en: Similar to what we saw in the section on feed-forward neural networks, the cases
    of exploding and vanishing gradient become more pronounced in RNNs due to the
    connectivity of units as discussed previously.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们在前馈神经网络部分看到的情况类似，由于单元的连接性，梯度爆炸和消失的情况在RNN中变得更加明显。
- en: 'Some of the solutions for exploding gradients are:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决梯度爆炸的一些方法包括：
- en: Truncated BPTT is a small change to the BPTT process. Instead of propagating
    the learning back to time *t=0*, it is truncated to a fixed time backward to *t=k*.
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截断BPTT是对BPTT过程的小幅修改。不是将学习传播回时间*t=0*，而是截断到固定时间向后到*t=k*。
- en: Gradient Clipping to cut the gradient above a threshold when it shoots up.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度裁剪，当梯度急剧上升时，将其裁剪到阈值以上。
- en: Adaptive Learning Rate. The learning rate adjusts itself based on the feedback
    and values.
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应学习率。学习率根据反馈和值进行调整。
- en: 'Some of the solutions for vanishing gradients are:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决梯度消失的一些方法包括：
- en: Using ReLU as the activation function; hence the gradient will be 1.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ReLU作为激活函数；因此梯度将为1。
- en: Adaptive Learning Rate. The learning rate adjusts itself based on the feedback
    and values.
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应学习率。学习率根据反馈和值进行调整。
- en: Using extensions such as Long Short Term Memory (LSTM) and Gated Recurrent Units
    (GRUs), which we will describe next.
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用扩展，如长短期记忆（LSTM）和门控循环单元（GRU），我们将在下一部分进行描述。
- en: There are many applications of RNNs, for example, in next letter predictions,
    next word predictions, language translation, and so on.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: RNN有许多应用，例如在下一个字母预测、下一个单词预测、语言翻译等方面。
- en: '![Learning and associated problems in RNNs](img/B05137_07_218.jpg)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![RNN中的学习和相关问题](img/B05137_07_218.jpg)'
- en: 'Figure 35: Showing some applications in next letter/word predictions using
    RNN structures.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 图35：展示使用RNN结构在下一个字母/单词预测中的应用。
- en: Long Short Term Memory
  id: totrans-409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: One of the neural network architectures or modifications to RNNs that addresses
    the issue of vanishing gradient is known as long short term memory or LSTM. We
    will explain some building blocks of LSTM and then put it together for our readers.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 解决RNN中梯度消失问题的一种神经网络架构或修改，被称为长短期记忆或LSTM。我们将解释LSTM的一些构建块，然后将其组合起来供读者参考。
- en: 'The first modification to RNN is to change the feedback learning matrix to
    1, that is, **W**[R] = 1, as shown in the following figure:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的第一个修改是将反馈学习矩阵改为1，即**W**[R] = 1，如图所示：
- en: '![Long Short Term Memory](img/B05137_07_220.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/B05137_07_220.jpg)'
- en: 'Figure 36: Building blocks of LSTM where the feedback matrix is set to 1.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图36：LSTM的构建块，其中反馈矩阵设置为1。
- en: This will ensure the inputs from older cell or memory units are passed as-is
    to the next unit. Hence some modifications are needed.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保从旧细胞或记忆单元的输入直接传递到下一个单元。因此需要一些修改。
- en: The output gate, as shown in the following figure, combines two computations.
    The first is the output from the individual unit, passed through an activation
    function, and the second is the output of the older unit that has been passed
    through a sigmoid using scaling.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 输出门，如图所示，结合了两个计算。第一个是从单个单元输出的，通过激活函数传递，第二个是经过sigmoid缩放的老单元的输出。
- en: '![Long Short Term Memory](img/B05137_07_221.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/B05137_07_221.jpg)'
- en: 'Figure 37: Building block Output Gate for LSTM.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图37：LSTM的构建块输出门。
- en: 'Mathematically, the output gate at the unit is given by:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，单元的输出门由以下公式给出：
- en: '![Long Short Term Memory](img/B05137_07_222.jpg)![Long Short Term Memory](img/B05137_07_223.jpg)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/B05137_07_222.jpg)![长短期记忆](img/B05137_07_223.jpg)'
- en: 'The forget gate is between the two memory units. It generates 0 or 1 based
    on learned weights and transformations. The forget gate is shown in the following
    figure:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门位于两个记忆单元之间。它根据学习权重和变换生成0或1。遗忘门如图所示：
- en: '![Long Short Term Memory](img/B05137_07_224.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/B05137_07_224.jpg)'
- en: 'Figure 38: Building block Forget Gate addition to LSTM.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图38：LSTM中添加的遗忘门构建块。
- en: 'Mathematically, ![Long Short Term Memory](img/B05137_07_225.jpg) can be seen
    as the representation of the forget gate. Next, the input gate and the new gate
    are combined, as shown in the following figure:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，![长短期记忆](img/B05137_07_225.jpg)可以看作是遗忘门的表示。接下来，输入门和新门结合，如图所示：
- en: '![Long Short Term Memory](img/B05137_07_226.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/B05137_07_226.jpg)'
- en: 'Figure 39: Building blocks New Gate and Input Gate added to complete LSTM.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图39：添加了新门和输入门以完成LSTM。
- en: The new memory generation unit uses the current input *x*[t] and the old state
    *h*[t-1] through an activation function and generates a new memory *C*[t]. The
    input gate combines the input and the old state and determines whether the new
    memory or the input should be preserved.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 新的记忆生成单元通过激活函数使用当前输入 *x*[t] 和旧状态 *h*[t-1]，生成新的记忆 *C*[t]。输入门结合输入和旧状态，并确定是否保留新的记忆或输入。
- en: 'Thus, the update equation looks like this:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更新方程看起来是这样的：
- en: '![Long Short Term Memory](img/B05137_07_230.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![长短期记忆](img/B05137_07_230.jpg)'
- en: Gated Recurrent Units
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: 'Gated Recurrent Units (GRUs) are simplified LSTMs with modifications. Many
    of the gates are simplified by using one "update" unit as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 门控循环单元 (GRUs) 是经过修改的简化LSTMs。许多门通过使用一个“更新”单元进行了简化，如下所示：
- en: '![Gated Recurrent Units](img/B05137_07_231.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![门控循环单元](img/B05137_07_231.jpg)'
- en: 'Figure 40: GRUs with Update unit.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图40：带有更新单元的GRUs。
- en: 'The changes made to the equations are:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 对方程所做的更改如下：
- en: '![Gated Recurrent Units](img/B05137_07_232.jpg)![Gated Recurrent Units](img/B05137_07_233.jpg)![Gated
    Recurrent Units](img/B05137_07_234.jpg)![Gated Recurrent Units](img/B05137_07_235.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![门控循环单元](img/B05137_07_232.jpg)![门控循环单元](img/B05137_07_233.jpg)![门控循环单元](img/B05137_07_234.jpg)![门控循环单元](img/B05137_07_235.jpg)'
- en: Case study
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: Several benchmarks exist for image classification. We will use the MNIST image
    database for this case study. When we used MNIST in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), Unsupervised Machine Learning Techniques with clustering
    and outlier detection techniques, each pixel was considered a feature. In addition
    to learning from the pixel values as in previous experiments, with deep learning
    techniques we will also be learning new features from the structure of the training
    dataset. The deep learning algorithms will be trained on 60,000 images and tested
    on a 10,000-image test dataset.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类，存在几个基准测试。我们将使用MNIST图像数据库进行本案例研究。当我们使用MNIST在[第3章](ch03.html "第3章。无监督机器学习技术")中时，无监督机器学习技术，包括聚类和异常检测技术，每个像素都被视为一个特征。除了像以前实验中那样从像素值中学习外，我们还将使用深度学习技术从训练数据集的结构中学习新特征。深度学习算法将在60,000张图像上进行训练，并在10,000张图像的测试数据集上进行测试。
- en: Tools and software
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具和软件
- en: In this chapter, we introduce the open-source Java framework for deep learning
    called DeepLearning4J (DL4J). DL4J has libraries implementing a host of deep learning
    techniques and they can be used on distributed CPUs and GPUs.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了名为DeepLearning4J (DL4J)的开源Java深度学习框架。DL4J包含实现众多深度学习技术的库，它们可以在分布式CPU和GPU上使用。
- en: 'DeepLearning4J: [https://deeplearning4j.org/index.html](https://deeplearning4j.org/index.html)'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeepLearning4J: [https://deeplearning4j.org/index.html](https://deeplearning4j.org/index.html)'
- en: We will illustrate the use of some DL4J libraries in learning from the MNIST
    training images and apply the learned models to classify the images in the test
    set.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何使用一些DL4J库从MNIST训练图像中学习，并将学到的模型应用于测试集中的图像分类。
- en: Business problem
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业问题
- en: Image classification is a particularly attractive test-bed to evaluate deep
    learning networks. We have previously encountered the MNIST database, which consists
    of greyscale images of handwritten digits. This time, we will show how both unsupervised
    and supervised deep learning techniques can be used to learn from the same dataset.
    The MNIST dataset has 28-by-28 pixel images in a single channel. These images
    are categorized into 10 labels representing the digits 0 to 9\. The goal is to
    train on 60,000 data points and test our deep learning classification algorithm
    on the remaining 10,000 images.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是评估深度学习网络的特别有吸引力的测试平台。我们之前遇到了MNIST数据库，它由手写数字的灰度图像组成。这次，我们将展示如何使用无监督和监督的深度学习技术从同一数据集中学习。MNIST数据集包含28x28像素的单通道图像。这些图像被分类为10个标签，代表数字0到9。目标是训练60,000个数据点，并在剩余的10,000张图像上测试我们的深度学习分类算法。
- en: Machine learning mapping
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: This includes supervised and unsupervised methods applied to a classification
    problem in which there are 10 possible output classes. Some techniques use an
    initial pre-training stage, which is unsupervised in nature, as we have seen in
    the preceding sections.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括应用于有10个可能输出类别的分类问题的监督和无监督方法。一些技术使用一个初始的预训练阶段，这个阶段本质上是无监督的，正如我们在前面的章节中看到的。
- en: Data sampling and transfor
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采样和转换
- en: 'The dataset is available at:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可在以下位置获取：
- en: '[https://yann.lecun.com/exdb/mnist](https://yann.lecun.com/exdb/mnist)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://yann.lecun.com/exdb/mnist](https://yann.lecun.com/exdb/mnist)'
- en: In the experiments in this case study, the MNIST dataset has been standardized
    such that pixel values in the range 0 to 255 have been normalized to values from
    0.0 to 1.0\. The exception is in the experiment using stacked RBMs, where the
    training and test data have been binarized, that is, set to 1 if the standardized
    value is greater than or equal to 0.3 and 0 otherwise. Each of the 10 classes
    is equally represented in both the training set and the test set. In addition,
    examples are shuffled using a random number generator seed supplied by the user.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究的实验中，MNIST数据集已经被标准化，使得像素值在0到255的范围内被归一化到0.0到1.0。例外的是在堆叠RBM的实验中，训练数据和测试数据已经被二值化，即如果标准化值大于或等于0.3则设置为1，否则为0。10个类别在训练集和测试集中都有相同的代表性。此外，使用用户提供的随机数生成器种子对示例进行随机排序。
- en: Feature analysis
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征分析
- en: The input data features are the greyscale values of the pixels in each image.
    This is the raw data and we will be using the deep learning algorithms to learn
    higher-level features out of the raw pixel values. The dataset has been prepared
    such that there are an equal number of examples of each class in both the training
    and the test sets.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据特征是每张图像中像素的灰度值。这是原始数据，我们将使用深度学习算法从原始像素值中学习高级特征。数据集已经被准备，使得训练集和测试集中每个类别的示例数量相等。
- en: Models, results, and evaluation
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型、结果和评估
- en: We will perform different experiments starting with simple MLP, Convolutional
    Networks, Variational Autoencoders, Stacked RBMS, and DBNs. We will walk through
    important parts of code that highlight the network structure or specialized tunings,
    give parameters to help readers, reproduce the experiments, and give the results
    for each type of network.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从简单的MLP、卷积网络、变分自编码器、堆叠RBM和DBN开始进行不同的实验。我们将逐步讲解代码中的重要部分，突出网络结构或专门的调整，提供参数以帮助读者重现实验，并给出每种网络类型的结果。
- en: Basic data handling
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基本数据处理
- en: 'The following snippet of code shows:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段显示：
- en: How to generically read data from a CSV with a structure enforced by delimiters.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用分隔符强制结构的CSV读取数据。
- en: How to iterate the data and get records.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如何迭代数据并获取记录。
- en: 'How to shuffle data in memory and create training/testing or validation sets:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在内存中打乱数据并创建训练/测试或验证集：
- en: '[PRE0]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'DL4J has a specific MNIST wrapper for handling the data that we have used,
    as shown in the following snippet:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J有一个特定的MNIST包装器来处理我们所使用的数据，如下面的代码片段所示：
- en: '[PRE1]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Multi-layer perceptron
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'In the first experiment, we will use a basic multi-layer perceptron with an
    input layer, one hidden layer, and an output layer. A detailed list of parameters
    that are used in the code is given here:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个实验中，我们将使用一个基本的多层感知器，包含输入层、一个隐藏层和一个输出层。以下是代码中使用的参数的详细列表：
- en: Parameters used for MLP
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用于MLP的参数
- en: '| Parameter | Variable | Value |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 变量 | 值 |'
- en: '| --- | --- | --- |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Number of iterations** | m | 1 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| **迭代次数** | m | 1 |'
- en: '| **Learning rate** | rate | 0.0015 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| **学习率** | rate | 0.0015 |'
- en: '| **Momentum** | momentum | 0.98 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| **动量** | momentum | 0.98 |'
- en: '| **L2 regularization** | regularization | 0.005 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| **L2正则化** | 正则化 | 0.005 |'
- en: '| **Number of rows in input** | numRows | 28 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| **输入行数** | numRows | 28 |'
- en: '| **Number of columns in input** | numColumns | 28 |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| **输入列数** | numColumns | 28 |'
- en: '| **Layer 0 output size, Layer 1 input size** | outputLayer0, inputLayer1 |
    500 |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| **第0层输出大小，第1层输入大小** | outputLayer0, inputLayer1 | 500 |'
- en: '| **Layer 1 output size, Layer 2 input size** | outputLayer1, inputLayer2 |
    300 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| **第1层输出大小，第2层输入大小** | outputLayer1, inputLayer2 | 300 |'
- en: '| **Layer 2 output size, Layer 3 input size** | outputLayer2, inputLayer3 |
    100 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| **第2层输出大小，第3层输入大小** | outputLayer2, inputLayer3 | 100 |'
- en: '| **Layer 3 output size,** | outputNum | 10 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| **第3层输出大小** | outputNum | 10 |'
- en: Code for MLP
  id: totrans-476
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MLP代码
- en: In the listing that follows, we can see how we first configure the MLP by passing
    in the hyperarameters using the Builder pattern.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的列表中，我们可以看到我们首先通过Builder模式传递超参数来配置MLP。
- en: '[PRE2]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Training, evaluation, and testing the MLP are shown in the following snippet.
    Notice the code that initializes the visualization backend enabling you to monitor
    the model training in your browser, particularly the model score (the training
    error after each iteration) and updates to parameters:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following plots show the training error against training iteration for
    the MLP model. This curve should decrease with iterations:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '![Code for MLP](img/B05137_07_236.jpg)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
- en: 'Figure 41: Training error as measured with number of iterations of training
    for the MLP model.'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we see the distribution of parameters in Layer 0 of
    the MLP as well as the distribution of updates to the parameters. These histograms
    should have an approximately Gaussian (Normal) shape, which indicates good convergence.
    For more on how to use charts to tune your model, see the DL4J Visualization page
    ([https://deeplearning4j.org/visualization](https://deeplearning4j.org/visualization)):'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '![Code for MLP](img/B05137_07_237.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
- en: 'Figure 42: Histograms showing Layer parameters and update distribution.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Network
  id: totrans-487
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the second experiment, we configured a Convolutional Network (ConvNet) using
    the built-in MultiLayerConfiguration. The architecture of the network consists
    of a total of five layers, as can be seen from the following code snippet. Following
    the input layer, two convolution layers with 5-by-5 filters alternating with Max
    pooling layers are followed by a fully connected dense layer using the ReLu activation
    layer, ending with Softmax activation in the final output layer. The optimization
    algorithm used is Stochastic Gradient Descent, and the loss function is Negative
    Log Likelihood.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: The various configuration parameters (or hyper-parameters) for the ConvNet are
    given in the table.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for ConvNet
  id: totrans-490
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Value |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| **Seed** | seed | 123 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| **Input size** | numRows, numColumns | 28, 28 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| **Number of epochs** | numEpochs | 10 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| **Number of iterations** | iterations | 1 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| **L2 regularization** | regularization | 0.005 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| **Learning rate** | learningRate | 0.1 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| **Momentum** | momentum | 0.9 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| **Convolution filter size** | xsize, ysize | 5, 5 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| **Convolution layers stride size** | x, y | 1, 1 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| **Number of input channels** | numChannels | 1 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| **Subsampling layer stride size** | sx, sy | 2, 2 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| **Layer 0 output size** | nOut0 | 20 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| **Layer 2 output size** | nOut1 | 50 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| **Layer 4 output size** | nOut2 | 500 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| **Layer 5 output size** | outputNum | 10 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: Code for CNN
  id: totrans-508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you can see, configuring multi-layer neural networks with the DL4J API is
    similar whether you are building MLPs or CNNs. Algorithm-specific configuration
    is simply done in the definition of each layer.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Variational Autoencoder
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the third experiment, we configure a Variational Autoencoder as the classifier.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Parameters used for the Variational Autoencoder
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The parameters used to configure the VAE are shown in the table.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Variable | Values |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 变量 | 值 |'
- en: '| --- | --- | --- |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Seed for RNG** | rngSeed | 12345 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| **RNG种子** | rngSeed | 12345 |'
- en: '| **Number of iterations** | Iterations | 1 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| **迭代次数** | Iterations | 1 |'
- en: '| **Learning rate** | learningRate | 0.001 |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| **学习率** | learningRate | 0.001 |'
- en: '| **RMS decay** | rmsDecay | 0.95 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| **RMS衰减** | rmsDecay | 0.95 |'
- en: '| **L2 regularization** | regularization | 0.0001 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| **L2正则化** | regularization | 0.0001 |'
- en: '| **Output layer size** | outputNum | 10 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| **输出层大小** | outputNum | 10 |'
- en: '| **VAE encoder layers size** | vaeEncoder1, vaeEncoder2 | 256, 256 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| **VAE编码器层大小** | vaeEncoder1, vaeEncoder2 | 256, 256 |'
- en: '| **VAE decoder layers size** | vaeDecoder1, vaeDecoder2 | 256, 256 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| **VAE解码器层大小** | vaeDecoder1, vaeDecoder2 | 256, 256 |'
- en: '| **Size of latent variable space** | latentVarSpaceSize | 128 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| **潜在变量空间大小** | latentVarSpaceSize | 128 |'
- en: Code for Variational Autoencoder
  id: totrans-526
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变分自动编码器的代码
- en: We have configured two layers each of encoders and decoders and are reconstructing
    the input using a Bernoulli distribution.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已配置了编码器和解码器各两层，并使用伯努利分布重构输入。
- en: '[PRE5]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: DBN
  id: totrans-529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBN
- en: 'The parameters used in DBN are shown in the following table:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: DBN中使用的参数如下表所示：
- en: '| Parameter | Variable | Value |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 变量 | 值 |'
- en: '| --- | --- | --- |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Input data size** | numRows, numColumns | 28, 28 |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| **输入数据大小** | numRows, numColumns | 28, 28 |'
- en: '| **Seed for RNG** | seed | 123 |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| **RNG种子** | seed | 123 |'
- en: '| **Number of training iterations** | iterations | 1 |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| **训练迭代次数** | iterations | 1 |'
- en: '| **Momentum** | momentum | 0.5 |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| **动量** | momentum | 0.5 |'
- en: '| **Layer 0 (input)****Layer 0 (output)****Layer 1 (input, output)****Layer
    2 (input, output)****Layer 3 (input, output)** | numRows * numColumnsnOut0nIn1,
    nOut1nIn2, nOut2nIn3, outputNum | 28 * 28500500, 250250, 200200, 10 |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| **层0（输入）****层0（输出）****层1（输入，输出）****层2（输入，输出）****层3（输入，输出）** | numRows * numColumnsnOut0nIn1,
    nOut1nIn2, nOut2nIn3, outputNum | 28 * 28500500, 250250, 200200, 10 |'
- en: Configuring the DBN using the DL4J API is shown in the example used in this
    case study. The code for the configuration of the network is shown here.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DL4J API配置DBN的示例在此案例研究中使用。网络配置的代码在此处显示。
- en: '[PRE6]'
  id: totrans-539
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Parameter search using Arbiter
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Arbiter进行参数搜索
- en: 'DeepLearning4J provides a framework for fine-tuning hyper-parameters by taking
    the burden of hand-tuning away from the modeler; instead, it allows the specification
    of the parameter space to search. In the following example code snippet, the configuration
    is specified using a MultiLayerSpace instead of a MutiLayerConfiguration object,
    in which the ranges for the hyper-parameters are specified by means of ParameterSpace
    objects in the Arbiter DL4J package for the parameters to be tuned:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLearning4J提供了一个框架，通过移除模型师手动调整的负担来微调超参数；相反，它允许指定搜索的参数空间。在下面的示例代码片段中，配置是通过使用MultiLayerSpace而不是MutiLayerConfiguration对象来指定的，其中超参数的范围是通过Arbiter
    DL4J包中的ParameterSpace对象来指定的，用于调整的参数：
- en: '[PRE7]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Results and analysis
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结果和分析
- en: 'The results of evaluating the performance of the four networks on the test
    data are given in the following table:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下表格中给出了评估四个网络在测试数据上性能的结果：
- en: '|   | MLP | ConvNet | VAE | DBN |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '|   | MLP | ConvNet | VAE | DBN |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **Accuracy** | 0.9807 | 0.9893 | 0.9743 | 0.7506 |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| **准确率** | 0.9807 | 0.9893 | 0.9743 | 0.7506 |'
- en: '| **Precision** | 0.9806 | 0.9893 | 0.9742 | 0.7498 |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| **精度** | 0.9806 | 0.9893 | 0.9742 | 0.7498 |'
- en: '| **Recall** | 0.9805 | 0.9891 | 0.9741 | 0.7454 |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| **召回率** | 0.9805 | 0.9891 | 0.9741 | 0.7454 |'
- en: '| **F1 score** | 0.9806 | 0.9892 | 0.9741 | 0.7476 |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| **F1分数** | 0.9806 | 0.9892 | 0.9741 | 0.7476 |'
- en: The goal of the experiments was not to match benchmark results in each of the
    neural network structures, but to give a comprehensive architecture implementation
    in the code with detailed parameters for the readers to explore.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的目标不是在每个神经网络结构中匹配基准结果，而是向读者提供一个综合的架构实现，并在代码中提供详细的参数供他们探索。
- en: Tuning the hyper-parameters in deep learning Networks is quite a challenge and
    though Arbiter and online resources such as gitter ( [https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j))
    help with DL4J, the time and cost of running the hyper-parameter search is quite
    high as compared to other classification techniques including SVMs.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习网络中调整超参数是一项相当大的挑战，尽管Arbiter和在线资源如gitter（[https://gitter.im/deeplearning4j/deeplearning4j](https://gitter.im/deeplearning4j/deeplearning4j)）有助于DL4J，但与包括SVM在内的其他分类技术相比，超参数搜索的时间和成本相当高。
- en: 'The benchmark results on the MNIST dataset and corresponding papers are available
    here:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集上的基准结果和相应的论文在此处可用：
- en: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
- en: '[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)'
- en: As seen from the benchmark result, Linear 1 Layer NN gets an error rate of 12%
    and adding more layers reduces it to about 2\. This shows the non-linear nature
    of the data and the need for a complex algorithm to fit the patterns.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 从基准测试结果来看，线性1层神经网络（NN）的错误率为12%，增加更多层可以将其降低到大约2%。这表明了数据的非线性性质以及需要复杂算法来拟合模式的需求。
- en: As compared to the benchmark best result on neural networks ranging from a 2.5%
    to 1.6% error rate, our results are very much comparable with the 2% error rate.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 与基准测试中神经网络的最佳结果相比，其错误率从2.5%到1.6%，我们的结果与2%的错误率非常相似。
- en: Most of the benchmark results show Convolutional Network architectures having
    error rates in the range of 1.1% to 0.5% and our hyper-parameter search has matched
    the best of those models with an error rate of just under 1.1%.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基准测试结果都显示卷积网络架构的错误率在1.1%到0.5%之间，我们的超参数搜索已经将那些模型中的最佳模型与错误率略低于1.1%相匹配。
- en: Our results for DBN fall far short of the benchmarks at just over 25%. There
    is no reason to doubt that further tuning can improve performance bringing it
    to the range of 3-5%.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对深度信念网络（DBN）的结果远远低于基准测试，仅为25%多。没有理由怀疑进一步的调整不能提高性能，将其提升到3-5%的范围内。
- en: Summary
  id: totrans-560
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The history of Deep Learning is intimately tied to the limitations of earlier
    attempts at using neural networks in machine learning and AI, and how these limitations
    were overcome with newer techniques, technological improvements, and the availability
    of vast amounts of data.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的历史与早期在机器学习和人工智能中使用神经网络尝试的局限性紧密相连，以及这些局限性是如何通过新技术、技术改进和大量数据的可用性而被克服的。
- en: The perceptron is the basic neural network. Multi-layer networks are used in
    supervised learning and are built by connecting several hidden layers of neurons
    to propagate activations forward and using backpropagation to reduce the training
    error. Several activation functions are used, most commonly, the sigmoid and tanh
    functions.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是基本的人工神经网络。多层网络用于监督学习，通过连接几个隐藏层的神经元来传播激活，并使用反向传播来减少训练误差。常用的激活函数包括sigmoid和tanh函数。
- en: The problems of neural networks are vanishing or exploding gradients, slow training,
    and the trap of local minima.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的问题包括梯度消失或爆炸、训练缓慢和陷入局部最小值陷阱。
- en: Deep learning successfully addresses these problems with the help of several
    effective techniques that can be used for unsupervised as well as supervised learning.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通过几种有效的技术成功地解决了这些问题，这些技术可以用于无监督学习以及监督学习。
- en: Among the building blocks of deep learning networks are Restricted Boltzmann
    Machines (RBM), Autoencoders, and Denoising Autoencoders. RBMs are two-layered
    undirected networks that are able to extract high-level features from their input.
    Contrastive divergence is used to speed up the training. Autoencoders are also
    deep learning networks used in unsupervised learning—they attempt to replicate
    the input by first encoding learned features in the encoding layer and then reconstructing
    the input via a set of decoding layers. Denoising Autoencoders address some limitations
    of Autoencoders, which can sometimes cause them to trivially learn the identity
    function.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络的基本构建块包括受限玻尔兹曼机（RBM）、自编码器和去噪自编码器。RBM是两层无向网络，能够从其输入中提取高级特征。对比散度用于加速训练。自编码器也是用于无监督学习的深度学习网络，它们通过首先在编码层中编码学习到的特征，然后通过一组解码层重建输入来尝试复制输入。去噪自编码器解决了自编码器的一些局限性，有时会导致它们简单地学习恒等函数。
- en: Deep learning networks are often pretrained in an unsupervised fashion and then
    their parameters are fine-tuned via supervised fine-tuning. Stacked RBMs or Autoencoders
    are used in the pretraining phase and the fine-tuning is typically accomplished
    with a softmax activation in the output layer in the case of classification.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习网络通常以无监督的方式预训练，然后通过监督微调来调整其参数。在预训练阶段使用堆叠的RBM或自编码器，而在分类的情况下，微调通常通过输出层的softmax激活来完成。
- en: Deep Autoencoders are good at learning complex latent structures in data and
    are used in unsupervised learning by employing pre-training and fine-tuning with
    Autoencoder building blocks. Deep Belief Networks (DBN) are generative models
    that can be used to create more samples. It is constructed using a directed Bayesian
    network with an undirected RBM layer on top. Overfitting in deep learning networks
    can be addressed by learning with dropouts, where some nodes in the network are
    randomly "turned off".
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自编码器擅长学习数据中的复杂潜在结构，并通过使用自编码器构建块进行预训练和微调在无监督学习中使用。深度信念网络（DBN）是生成模型，可用于创建更多样本。它使用一个带有顶部无向RBM层的有向贝叶斯网络构建。通过使用带有随机“关闭”一些网络节点的dropout进行学习，可以解决深度学习网络中的过拟合问题。
- en: Convolutional Neural Networks (CNNs) have a number of applications in computer
    vision. CNNs can learn patterns in the data translation-invariant and robust to
    linear scaling in the data. They reduce the dimensionality of the data using convolution
    filters and pooling layers and can achieve very effective results in classification
    tasks. A use case involving the classification of digital images is presented.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）在计算机视觉中有许多应用。CNN可以学习数据中的模式，这些模式对数据的平移不变性和线性缩放具有鲁棒性。它们使用卷积滤波器和池化层来降低数据的维度，并在分类任务中实现非常有效的结果。一个涉及数字图像分类的用例被提出。
- en: When the data arrives as sequences and there are temporal relationships among
    data, Recurrent Neural Networks (RNN) are used for modeling. RNNs use feedback
    from previous layers and emit output continually. The problem of vanishing and
    exploding gradients recurs in RNNs, and are addressed by several modifications
    to the architecture, such as Long Short Term Memory (LSTM) and Gated Recurrent
    Networks (GRU).
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据以序列形式到达且数据之间存在时间关系时，使用循环神经网络（RNN）进行建模。RNN使用来自先前层的反馈并持续输出。在RNN中，梯度消失和梯度爆炸的问题会再次出现，并通过修改架构的几种方法来解决，例如长短期记忆（LSTM）和门控循环网络（GRU）。
- en: In this chapter's case study, we present the experiments done with various deep
    learning networks to learn from MNIST handwritten digit image datasets. Results
    using MLP, ConvNet, Variational Autoencoder, and Stacked RBM are presented.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的案例研究中，我们展示了使用各种深度学习网络从MNIST手写数字图像数据集中学习的实验。展示了使用MLP、ConvNet、变分自编码器和堆叠RBM的结果。
- en: We think that deep neural networks are able to approximate a significant and
    representative sub-set of key structures that the underlying data is based on.
    In addition, the hierarchic structures of the data can be easily captured with
    the help of different hidden layers. Finally, the invariance against rotation,
    translation, and the scale of images, for instance, is the last key elements of
    the performance of deep neural networks. The invariance allows us to reduce the
    number of possible states to be captured by the neural network (*References* [19]).
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为，深度神经网络能够逼近底层数据所基于的关键结构的显著和代表性子集。此外，数据的层次结构可以通过不同的隐藏层轻松捕获。最后，图像的旋转、平移和尺度的不变性是深度神经网络性能的最后关键要素。这种不变性使我们能够减少神经网络需要捕获的可能状态的数量（*参考文献*
    [19]）。
- en: References
  id: totrans-572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Behnke, S. (2001). Learning iterative image reconstruction in the neural abstraction
    pyramid. International Journal of Computational Intelligence and Applications,
    1(4), 427–438\.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Behnke, S. (2001). 在神经抽象金字塔中学习迭代图像重建。国际计算智能与应用杂志，1(4)，427–438。
- en: Behnke, S. (2002). Learning face localization using hierarchical recurrent networks.
    In Proceedings of the 12^(th) international conference on artificial neural networks
    (pp. 1319–1324).
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Behnke, S. (2002). 使用分层循环神经网络进行人脸定位。载于第12届国际人工神经网络会议论文集（第1319–1324页）。
- en: Behnke, S. (2003). Discovering hierarchical speech features using convolutional
    non-negative matrix factorization. In Proceedings of the international joint conference
    on neural networks, vol. 4 (pp. 2758–2763).
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Behnke, S. (2003). 使用卷积非负矩阵分解发现层次语音特征。载于国际神经网络联合会议论文集，第4卷（第2758–2763页）。
- en: 'Behnke, S. (2003). LNCS, Lecture notes in computer science: Vol. 2766\. Hierarchical
    neural networks for image interpretation. Springer. Behnke, S. (2005). Face localization
    and tracking in the neural abstraction pyramid. Neural Computing and Applications,
    14(2), 97–103.'
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Behnke, S. (2003). LNCS，计算机科学讲义：第2766卷。用于图像解释的层次神经网络。Springer。Behnke, S. (2005).
    神经抽象金字塔中的人脸定位和跟踪。神经计算与应用，14(2)，97–103。
- en: Casey, M. P. (1996). The dynamics of discrete-time computation, with application
    to recurrent neural networks and finite state machine extraction. Neural Computation,
    8(6), 1135–1178.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Casey, M. P. (1996). 离散时间计算的动力学，及其在循环神经网络和有限状态机提取中的应用。神经计算杂志，第8卷，第6期，第1135–1178页。
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning internal
    representations by error propagation. In Rumelhart, D. E. and McClelland, J. L.,
    editors, Parallel Distributed Processing, volume 1, pages 318–362\. MIT Press.
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., 和 Williams, R. J. (1986). 通过误差传播学习内部表示。在Rumelhart,
    D. E. 和 McClelland, J. L. 编著的《并行分布式处理》第1卷，第318–362页。麻省理工学院出版社。
- en: Goller, C.; Küchler, A (1996). ""Learning task-dependent distributed representations
    by backpropagation through structure"". Neural Networks, IEEE. doi:10.1109/ICNN.1996.548916
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goller, C.; Küchler, A (1996). ""通过结构反向传播进行任务相关分布式表示的学习"". 神经网络，IEEE。doi:10.1109/ICNN.1996.548916
- en: 'Hochreiter, Sepp. The vanishing gradient problem during learning recurrent
    neural nets and problem solutions. International Journal of Uncertainty, Fuzziness
    and Knowledge-Based Systems, 6(02): 107–116, 1998.'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hochreiter, Sepp. 在学习循环神经网络时梯度消失问题及其解决方案。国际不确定性、模糊性和基于知识的系统杂志，第6卷，第2期，第107–116页，1998年。
- en: G. E. Hinton, S. Osindero, and Y. The (2006). "A fast learning algorithm for
    deep belief nets," Neural Comput., vol. 18, pp. 1527–1554\.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. E. Hinton, S. Osindero, 和 Y. The (2006). "深度信念网的快速学习算法," 神经计算杂志，第18卷，第1527–1554页。
- en: G. E. Hinton (2002). "Training products of experts by minimizing contrastive
    divergence," Neural Comput., vol. 14, pp. 1771–1800\.
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. E. Hinton (2002). "通过最小化对比散度训练专家乘积," 神经计算杂志，第14卷，第1771–1800页。
- en: G. E. Hinton and R. R. Salakhutdinov (2006). "Reducing the dimensionality of
    data with neural networks," Science, vol. 313, no. 5786, pp. 504–507.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. E. Hinton 和 R. R. Salakhutdinov (2006). "使用神经网络降低数据维度," 科学，第313卷，第5786号，第504–507页。
- en: Hinton, G. E., & Zemel, R. S. (1994). Autoencoders, minimum description length,
    and Helmholtz free energy. Advances in Neural Information Processing Systems,
    6, 3–10.
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, G. E., 和 Zemel, R. S. (1994). 自编码器、最小描述长度和亥姆霍兹自由能。神经信息处理系统进展，第6卷，第3–10页。
- en: Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. (2007). "Greedy layer-wise
    training of deep networks," in Advances in Neural Information Processing Systems
    19 (NIPS'06) pp. 153–160\.
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Bengio, P. Lamblin, D. Popovici, 和 H. Larochelle. (2007). "深度网络的贪婪层叠训练,"
    在《神经信息处理系统进展》第19卷（NIPS'06）第153–160页。
- en: H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio (2007). "An
    empirical evaluation of deep architectures on problems with many factors of variation,"
    in Proc. 24^(th) Int. Conf. Machine Learning (ICML'07) pp. 473–480.
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H. Larochelle, D. Erhan, A. Courville, J. Bergstra, 和 Y. Bengio (2007). "在具有许多变化因素的问题上对深度架构的经验评估,"
    在第24届国际机器学习会议（ICML'07）论文集，第473–480页。
- en: P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol (2008), "Extracting
    and composing robust features with denoising autoencoders," in Proc. ^(25)th Int.
    Conf. Machine Learning (ICML'08), pp. 1096–1103.
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Vincent, H. Larochelle, Y. Bengio, 和 P.-A. Manzagol (2008), "使用降噪自编码器提取和组合鲁棒特征,"
    在第25届国际机器学习会议（ICML'08）论文集，第1096–1103页。
- en: F.-J. Huang and Y. LeCun (2006). "Large-scale learning with SVM and convolutional
    nets for generic object categorization," in Proc. Computer Vision and Pattern
    Recognition Conf. (CVPR'06).
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F.-J. Huang 和 Y. LeCun (2006). "使用SVM和卷积网进行通用对象分类的大规模学习," 在计算机视觉和模式识别会议（CVPR'06）论文集。
- en: F. A. Gers, N. N. Schraudolph, and J. Schmidhuber (2003). Learning precise timing
    with LSTM recurrent networks. The Journal of Machine Learning Research.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F. A. Gers, N. N. Schraudolph, 和 J. Schmidhuber (2003). 使用LSTM循环网络学习精确的时间。机器学习研究杂志。
- en: Kyunghyun Cho et. al (2014). Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation. [https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf).
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kyunghyun Cho 等人 (2014). 使用RNN编码器-解码器学习短语表示以进行统计机器翻译。[https://arxiv.org/pdf/1406.1078.pdf](https://arxiv.org/pdf/1406.1078.pdf)。
- en: '[https://brohrer.github.io/how_convolutional_neural_networks_work.html](https://brohrer.github.io/how_convolutional_neural_networks_work.html)'
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://brohrer.github.io/how_convolutional_neural_networks_work.html](https://brohrer.github.io/how_convolutional_neural_networks_work.html)'
- en: Henry W. Lin, Max Tegmark, David Rolnick (2016). Why does deep and cheap learning
    work so well? [https://arxiv.org/abs/1608.08225](https://arxiv.org/abs/1608.08225)
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Henry W. Lin, Max Tegmark, David Rolnick (2016). 为什么深度学习和低成本学习工作得如此之好？[https://arxiv.org/abs/1608.08225](https://arxiv.org/abs/1608.08225)。
- en: Mike Schuster and Kuldip K. Paliwal (1997). Bidirectional Recurrent Neural Networks,
    Trans. on Signal Processing.
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mike Schuster 和 Kuldip K. Paliwal (1997). 双向循环神经网络，信号处理杂志。
- en: H Lee, A Battle, R Raina, AY Ng (2007). Efficient sparse coding algorithms,
    In Advances in Neural Information Processing Systems
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H Lee, A Battle, R Raina, AY Ng (2007). 高效稀疏编码算法，载于《神经信息处理系统进展》
- en: Bengio Y. (2009). Learning deep architectures for AI, Foundations and Trends
    in Machine Learning 1(2) pages 1-127.
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bengio Y. (2009). 为人工智能学习深度架构，载于《机器学习基础与趋势》第1卷第2期，第1-127页。
