["```py\njava -jar elki-bundle-0.7.1.jar\n```", "```py\nString filePath = \"/Users/bostjan/Dropbox/ML Java Book/book/datasets/chap07/claims.csv\"; \n\nCSVLoader loader = new CSVLoader(); \nloader.setFieldSeparator(\",\"); \nloader.setSource(new File(filePath)); \nInstances data = loader.getDataSet(); \n```", "```py\nNumericToNominal toNominal = new NumericToNominal(); \ntoNominal.setInputFormat(data); \ndata = Filter.useFilter(data, toNominal); \n```", "```py\nint CLASS_INDEX = 15; \ndata.setClassIndex(CLASS_INDEX); \n```", "```py\nRemove remove = new Remove(); \nremove.setInputFormat(data); \nremove.setOptions(new String[]{\"-R\", \"\"+POLICY_INDEX}); \ndata = Filter.useFilter(data, remove); \n```", "```py\nArrayList<Classifier>models = new ArrayList<Classifier>(); \nmodels.add(new J48()); \nmodels.add(new RandomForest()); \nmodels.add(new NaiveBayes()); \nmodels.add(new AdaBoostM1()); \nmodels.add(new Logistic()); \n```", "```py\nint FOLDS = 3; \nEvaluation eval = new Evaluation(data); \n\nfor(Classifier model : models){ \n  eval.crossValidateModel(model, data, FOLDS,  \n  new Random(1), new String[] {}); \n  System.out.println(model.getClass().getName() + \"\\n\"+ \n    \"\\tRecall:    \"+eval.recall(FRAUD) + \"\\n\"+ \n    \"\\tPrecision: \"+eval.precision(FRAUD) + \"\\n\"+ \n    \"\\tF-measure: \"+eval.fMeasure(FRAUD)); \n} \n```", "```py\n    weka.classifiers.trees.J48\n      Recall:    0.03358613217768147\n      Precision: 0.9117647058823529\n      F-measure: 0.06478578892371996\n    ...\n    weka.classifiers.functions.Logistic\n      Recall:    0.037486457204767065\n      Precision: 0.2521865889212828\n      F-measure: 0.06527070364082249\n\n```", "```py\nStratifiedRemoveFolds kFold = new StratifiedRemoveFolds(); \nkFold.setInputFormat(data); \n\ndouble measures[][] = new double[models.size()][3]; \n\nfor(int k = 1; k <= FOLDS; k++){ \n\n  // Split data to test and train folds \n  kFold.setOptions(new String[]{ \n    \"-N\", \"\"+FOLDS, \"-F\", \"\"+k, \"-S\", \"1\"}); \n  Instances test = Filter.useFilter(data, kFold); \n\n  kFold.setOptions(new String[]{ \n    \"-N\", \"\"+FOLDS, \"-F\", \"\"+k, \"-S\", \"1\", \"-V\"}); \n    // select inverse \"-V\" \n  Instances train = Filter.useFilter(data, kFold); \n```", "```py\nResample resample = new Resample(); \nresample.setInputFormat(data); \nresample.setOptions(new String[]{\"-Z\", \"100\", \"-B\", \"1\"}); //with \n   replacement \nInstances balancedTrain = Filter.useFilter(train, resample); \n```", "```py\nfor(ListIterator<Classifier>it = models.listIterator(); \n   it.hasNext();){ \n  Classifier model = it.next(); \n  model.buildClassifier(balancedTrain); \n  eval = new Evaluation(balancedTrain); \n  eval.evaluateModel(model, test); \n\n// save results for average \n  measures[it.previousIndex()][0] += eval.recall(FRAUD); \n  measures[it.previousIndex()][1] += eval.precision(FRAUD); \n measures[it.previousIndex()][2] += eval.fMeasure(FRAUD); \n} \n```", "```py\n// calculate average \nfor(int i = 0; i < models.size(); i++){ \n  measures[i][0] /= 1.0 * FOLDS; \n  measures[i][1] /= 1.0 * FOLDS; \n  measures[i][2] /= 1.0 * FOLDS; \n} \n\n// output results and select best model \nClassifier bestModel = null; double bestScore = -1; \nfor(ListIterator<Classifier> it = models.listIterator(); \n   it.hasNext();){ \n  Classifier model = it.next(); \n  double fMeasure = measures[it.previousIndex()][2]; \n  System.out.println( \n    model.getClass().getName() + \"\\n\"+ \n    \"\\tRecall:    \"+measures[it.previousIndex()][0] + \"\\n\"+ \n    \"\\tPrecision: \"+measures[it.previousIndex()][1] + \"\\n\"+ \n    \"\\tF-measure: \"+fMeasure); \n  if(fMeasure > bestScore){ \n    bestScore = fMeasure; \n    bestModel = model; \n\n  } \n} \nSystem.out.println(\"Best model:\"+bestModel.getClass().getName()); \n```", "```py\n    weka.classifiers.trees.J48\n      Recall:    0.44204845100610574\n      Precision: 0.14570766048577555\n      F-measure: 0.21912423640160392\n    ...\n    weka.classifiers.functions.Logistic\n      Recall:    0.7670657247204478\n      Precision: 0.13507459756495374\n      F-measure: 0.22969038530557626\n    Best model: weka.classifiers.functions.Logistic\n\n```", "```py\nFile filename = new File(\"data/spot_num.txt\");\nCSVFormat format = new CSVFormat('.', ' ');\nVersatileDataSource source = new CSVDataSource(filename, true, format);\nVersatileMLDataSet data = new VersatileMLDataSet(source);\ndata.getNormHelper().setFormat(format);\nColumnDefinition columnSSN = data.defineSourceColumn(\"SSN\", ColumnType.continuous);\nColumnDefinition columnDEV = data.defineSourceColumn(\"DEV\", ColumnType.continuous);\ndata.analyze();\ndata.defineInput(columnSSN);\ndata.defineInput(columnDEV);\ndata.defineOutput(columnSSN);\n```", "```py\nEncogModel model = new EncogModel(data);\nmodel.selectMethod(data, MLMethodFactory.TYPE_FEEDFORWARD);\n\nmodel.setReport(new ConsoleStatusReportable());\ndata.normalize();\n\n// Set time series.\ndata.setLeadWindowSize(1);\ndata.setLagWindowSize(WINDOW_SIZE);\nmodel.holdBackValidation(0.3, false, 1001);\nmodel.selectTrainingType(data);\n```", "```py\nMLRegression bestMethod = (MLRegression) model.crossvalidate(5, false);\n```", "```py\nSystem.out.println(\"Training error: \" + model.calculateError(bestMethod, model.getTrainingDataset()));\nSystem.out.println(\"Validation error: \" + model.calculateError(bestMethod, model.getValidationDataset()));\n\nNormalizationHelper helper = data.getNormHelper();\nSystem.out.println(helper.toString());\n\n// Display the final model.\nSystem.out.println(\"Final model: \" + bestMethod);\n```", "```py\nwhile (csv.next() && stopAfter > 0) {\n                StringBuilder result = new StringBuilder();\n\n                line[0] = csv.get(2);// ssn\n                line[1] = csv.get(3);// dev\n                helper.normalizeInputVector(line, slice, false);\n\n                if (window.isReady()) {\n                    window.copyWindow(input.getData(), 0);\n                    String correct = csv.get(2); // trying to predict SSN.\n                    MLData output = bestMethod.compute(input);\n                    String predicted = helper\n                            .denormalizeOutputVectorToString(output)[0];\n\n                    result.append(Arrays.toString(line));\n                    result.append(\" -> predicted: \");\n                    result.append(predicted);\n                    result.append(\"(correct: \");\n                    result.append(correct);\n                    result.append(\")\");\n\n                    System.out.println(result.toString());\n                }\n\n                window.add(slice);\n\n                stopAfter--;\n            }\n```", "```py\nString filePath = \"chap07/ydata/A1Benchmark/real\"; \nList<List<Double>> rawData = new ArrayList<List<Double>>(); \n```", "```py\ndouble max = Double.MIN_VALUE; \ndouble min = Double.MAX_VALUE; \n\nfor(int i = 1; i<= 67; i++){ \n  List<Double> sample = new ArrayList<Double>(); \n  BufferedReader reader = new BufferedReader(new \n     FileReader(filePath+i+\".csv\")); \n\n  boolean isAnomaly = false; \n  reader.readLine(); \n  while(reader.ready()){ \n    String line[] = reader.readLine().split(\",\"); \n    double value = Double.parseDouble(line[1]); \n    sample.add(value); \n\n    max = Math.max(max, value); \n    min = Double.min(min, value); \n\n    if(line[2] == \"1\") \n      isAnomaly = true; \n\n  } \n  System.out.println(isAnomaly); \n  reader.close(); \n\n  rawData.add(sample); \n} \n```", "```py\nint WIN_SIZE = 500; \nint HIST_BINS = 20; \nint current = 0; \n\nList<double[]> dataHist = new ArrayList<double[]>(); \nfor(List<Double> sample : rawData){ \n  double[] histogram = new double[HIST_BINS]; \n  for(double value : sample){ \n    int bin = toBin(normalize(value, min, max), HIST_BINS); \n    histogram[bin]++; \n    current++; \n    if(current == WIN_SIZE){ \n      current = 0; \n      dataHist.add(histogram); \n      histogram = new double[HIST_BINS]; \n    } \n  } \n  dataHist.add(histogram); \n} \n```", "```py\nArrayList<Attribute> attributes = new ArrayList<Attribute>(); \nfor(int i = 0; i<HIST_BINS; i++){ \n  attributes.add(new Attribute(\"Hist-\"+i)); \n} \nInstances dataset = new Instances(\"My dataset\", attributes, \n   dataHist.size()); \nfor(double[] histogram: dataHist){ \n  dataset.add(new Instance(1.0, histogram)); \n} \n```", "```py\n// split data to train and test \nInstances trainData = dataset.testCV(2, 0); \nInstances testData = dataset.testCV(2, 1); \n```", "```py\nimport weka.filters.unsupervised.attribute.LOF; \n```", "```py\nLOF lof = new LOF(); \nlof.setInputFormat(trainData); \nlof.setOptions(new String[]{\"-min\", \"3\", \"-max\", \"3\"}); \n```", "```py\nfor(Instance inst : trainData){ \n  lof.input(inst); \n} \nlof.batchFinished(); \n```", "```py\nInstances testDataLofScore = Filter.useFilter(testData, lof); \n\nfor(Instance inst : testDataLofScore){ \n  System.out.println(inst.value(inst.numAttributes()-1)); \n} \n```", "```py\n    1.306740014927325\n    1.318239332210458\n    1.0294812291949587\n    1.1715039094530768\n\n```"]