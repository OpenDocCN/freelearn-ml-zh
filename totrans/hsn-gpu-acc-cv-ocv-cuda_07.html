<html><head></head><body><div><div><h1 class="header-title">Object Detection and Tracking Using OpenCV and CUDA</h1>
                
            
            
                
<p>The last chapter described basic computer vision operations using OpenCV and CUDA. In this chapter, we will see how to use these basic operations along with OpenCV and CUDA to develop complex computer vision applications. We will use the example of object detection and tracking to demonstrate this concept. Object detection and tracking is a very active area of research in computer vision. It deals with identifying the location of an object in an image and tracking it in a sequence of frames. Many algorithms are proposed for this task based on color, shape, and the other salient features of an image. In this chapter, these algorithms are implemented using OpenCV and CUDA. We start with an explanation of detecting an object based on color, then describe the methods to detect an object with a particular shape. All objects have salient features that can be used to detect and track objects. This chapter describes the implementation of different feature detection algorithms and how they can be used to detect objects. The last part of the chapter will demonstrate the use of a background subtraction technique that separates the foreground from the background for object detection and tracking.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introduction to object detection and tracking</li>
<li>Object detection and tracking based on color</li>
<li>Object detection and tracking based on a shape</li>
<li>Feature-based object detection</li>
<li>Object detection using Haar cascade</li>
<li>Background subtraction methods</li>
</ul>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter requires a good understanding of image processing and computer vision. It also requires some basic knowledge of algorithms used for object detection and tracking. It needs familiarity with the basic C or C++ programming language, CUDA, and all the codes explained in previous chapters. All the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a>. The code can be executed on any operating system, though it has only been tested on Ubuntu 16.04. </p>
<p>Check out the following video to see the code in action:<br/>
<a href="http://bit.ly/2PSRqkU">http://bit.ly/2PSRqkU</a></p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Introduction to object detection and tracking</h1>
                
            
            
                
<p>Object detection and tracking is an active research topic in the field of computer vision that makes efforts to detect, recognize, and track objects through a series of frames. It has been found that object detection and tracking in the video sequence is a challenging task and a very time-consuming process. Object detection is the first step in building a larger computer vision system. A large amount of information can be derived from the detected object, as follows:</p>
<ul>
<li>The detected object can be classified into a particular class</li>
<li>It can be tracked in an image sequence</li>
<li>More information about the scene or other object inferences can be derived from the detected object</li>
</ul>
<p>Object tracking is defined as the task of detecting objects in every frame of the video and establishing the correspondence between the detected objects from one frame to the other. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Applications of object detection and tracking</h1>
                
            
            
                
<p>Object detection and tracking can be used to develop video surveillance systems to track suspicious activities, events, and persons. It can be used for developing an intelligent traffic system to track vehicles and detect traffic rule violations. Object detection is essential in autonomous vehicles to give them information about the surroundings and planning for their navigation. It is also useful for pedestrian detection or vehicle detection in automatic driver assistance systems. It can be used in the medical field for applications like breast cancer detection or brain tumor detection and so on. It can be used for face and hand gesture recognition. It has a wide application in industrial assembly and quality control in production lines.  It is vital for image retrieval from search engines and for photo management.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Challenges in object detection</h1>
                
            
            
                
<p>Object detection is a challenging task because images in real life are affected by noise, illumination variation, dynamic backgrounds, shadowing effect, camera jitter, and motion blur.  Object detection is difficult when an object to be detected is rotated, scaled, or under occlusion. Many applications require detecting more than one object class. If a large number of classes are being detected then the processing speed becomes an important issue along with the kinds of classes that the system can handle without accuracy loss.</p>
<p>There are many algorithms that overcome some of these challenges. They are discussed in this chapter. The chapter does not describe the algorithms in detail, but more focus is given on how it can be implemented using CUDA and OpenCV.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Object detection and tracking based on color</h1>
                
            
            
                
<p>An object has many global features like color and shape, which describe the object as a whole. These features can be utilized for the detection of an object and tracking it in a sequence of frames. In this section, we will use color as a feature to detect an object with a particular color. This method is useful when an object to be detected is of a specific color and this color is different to the color of the background. If the object and background have the same color, then this method for detection will fail. In this section, we will try to detect any object with a blue color from a webcam stream using OpenCV and CUDA.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Blue object detection and tracking</h1>
                
            
            
                
<p>The first question that should come to your mind is which color space should be used for segmenting blue color. A <strong>Red Green Blue</strong> (<strong>RGB</strong>) color space does not separate color information from intensity information. The color spaces that separate color information from intensity, like <strong>Hue Saturation Value</strong> (HSV) and <strong>YCrCb</strong> (where Yâ€² is the luma component and CB and CR are the blue-difference and red-difference chroma components), are ideal for this kind of task. Every color has a specific range in the hue channel that can be utilized for detection of that color. The boilerplate code for starting the webcam, capturing frames, and uploading on-device memory for a GPU operation is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>using namespace cv;<br/>using namespace std;<br/><br/>int main()<br/>{<br/>  VideoCapture cap(0); //capture the video from web cam<br/>  // if webcam is not available then exit the program<br/>  if ( !cap.isOpened() ) <br/>  {<br/>    cout &lt;&lt; "Cannot open the web cam" &lt;&lt; endl;<br/>    return -1;<br/>  }<br/>  while (true)<br/>  {<br/>    Mat frame;<br/>    // read a new frame from webcam<br/>    bool flag = cap.read(frame); <br/>    if (!flag) <br/>    {<br/>      cout &lt;&lt; "Cannot read a frame from webcam" &lt;&lt; endl;<br/>      break;<br/>    }<br/>  <br/>    cuda::GpuMat d_frame, d_frame_hsv,d_intermediate,d_result;<br/>    cuda::GpuMat d_frame_shsv[3];<br/>    cuda::GpuMat d_thresc[3];<br/>    Mat h_result;<br/>    d_frame.upload(frame);<br/><br/>    d_result.download(h_result);<br/>    imshow("Thresholded Image", h_result); <br/>    imshow("Original", frame); <br/><br/>    if (waitKey(1) == 'q') <br/>    {<br/>      break; <br/>    }<br/>  }<br/>  return 0;<br/>}<br/>}</pre>
<p>To detect the blue color, we need to find a range for blue color in the HSV color space. If a range is accurate then the detection will be accurate. The range of blue color for three channels, hue, saturation, and value, is as follows:</p>
<pre>lower_range = [110,50,50]<br/>upper_range = [130,255,255]</pre>
<p>This range will be used to threshold an image in a particular channel to create a mask for the blue color. If this mask is again ANDed with the original frame, then only a blue object will be there in the resultant image. The code for this is as follows:</p>
<pre>//Transform image to HSV<br/>cuda::cvtColor(d_frame, d_frame_hsv, COLOR_BGR2HSV);<br/><br/>//Split HSV 3 channels<br/>cuda::split(d_frame_hsv, d_frame_shsv);<br/><br/>//Threshold HSV channels for blue color according to range<br/>cuda::threshold(d_frame_shsv[0], d_thresc[0], 110, 130, THRESH_BINARY);<br/>cuda::threshold(d_frame_shsv[1], d_thresc[1], 50, 255, THRESH_BINARY);<br/>cuda::threshold(d_frame_shsv[2], d_thresc[2], 50, 255, THRESH_BINARY);<br/><br/>//Bitwise AND the channels<br/>cv::cuda::bitwise_and(d_thresc[0], d_thresc[1],d_intermediate);<br/>cv::cuda::bitwise_and(d_intermediate, d_thresc[2], d_result);</pre>
<p>The frame from the webcam is converted to an HSV color space. The blue color has a different range in three channels, so each channel has to be thresholded individually. The channels are split using the <kbd>split</kbd> method and thresholded using the <kbd>threshold</kbd> function. The minimum and maximum ranges for each channel are used as lower and upper thresholds. The channel value inside this range will be converted to white and others are converted to black. These three thresholded channels are logically ANDed to get a final mask for a blue color. This mask can be used to detect and track an object with a blue color from a video.</p>
<p>The output of two frames, one without the blue object and the other with the blue object, is as follows:</p>
<div><img class="alignnone size-full wp-image-454 image-border" src="img/32bdf79b-65c0-4e98-a877-663ff2d52588.png" style="" width="683" height="557"/></div>
<p>As can be seen from the result, when a frame does not contain any blue object, the mask is almost black; whereas in the frame below, when the blue object comes into frame, that part turns white. This method will only work when the background does not contain the color of an object.  </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Object detection and tracking based on shape</h1>
                
            
            
                
<p>The shape of an object can also be utilized as a global feature to detect an object with a distinct shape. This shape can be a straight line, polygons, circles, or any other irregular shapes. Object boundaries, edges, and contours can be utilized to detect an object with a particular shape. In this section, we will use the Canny edge detection algorithm and Hough transform to detect two regular shapes, which are a line and a circle.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Canny edge detection</h1>
                
            
            
                
<p>In the last chapter, we saw various high pass filters, which can be used as edge detectors. In this section, the Canny edge detection algorithm, which combines Gaussian filtering, gradient finding, non-maximum suppression, and hysteresis thresholding, is implemented using OpenCV and CUDA.  High pass filters, as explained in the last chapter, are very sensitive to noise. In Canny edge detection, Gaussian smoothing is done before detecting edges, which makes it less sensitive to noises. It also has a non-maximum suppression stage after detecting edges to remove unnecessary edges from the result.</p>
<p>Canny edge detection is a computationally intensive task, which is hard to use in real-time applications. The CUDA version of the algorithm can be used to accelerate it. The code for implementing a Canny edge detection algorithm is described below: </p>
<pre>#include &lt;cmath&gt;<br/>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>using namespace std;<br/>using namespace cv;<br/>using namespace cv::cuda;<br/><br/><br/>int main()<br/>{<br/>  Mat h_image = imread("images/drawing.JPG",0);<br/>  if (h_image.empty())<br/>  {<br/>    cout &lt;&lt; "can not open image"&lt;&lt; endl;<br/>    return -1;<br/>  }<br/>  GpuMat d_edge,d_image;<br/>  Mat h_edge;<br/>  d_image.upload(h_image);<br/>  cv::Ptr&lt;cv::cuda::CannyEdgeDetector&gt; Canny_edge = cv::cuda::createCannyEdgeDetector(2.0, 100.0, 3, false);<br/>  Canny_edge-&gt;detect(d_image, d_edge);<br/>  d_edge.download(h_edge);<br/>  imshow("source", h_image);<br/>  imshow("detected edges", h_edge);<br/>  waitKey(0);<br/><br/>  return 0;<br/>}</pre>
<p>OpenCV and CUDA provides the <kbd>createCannyEdgeDetector</kbd> class for Canny edge detection. The object of this class is created, and many arguments can be passed while creating it.  The first and second arguments are the low and high thresholds for hysteresis thresholding. If the intensity gradient at a point is greater then the maximum threshold, then it is categorized as an edge point. If the gradient is less than the low threshold, then the point is not an edge point. If the gradient is in between thresholds, then whether the point is an edge or not is decided based on connectivity. The third argument is the aperture size for the edge detector. The final argument is the Boolean argument, which indicates whether to use <kbd>L2_norm</kbd> or <kbd>L1_norm</kbd> for gradient magnitude calculation.  <kbd>L2_norm</kbd> is computationally expensive but it is more accurate. The true value indicates the use of <kbd>L2_norm</kbd>.  The output of the code is shown below:</p>
<div><img class="alignnone size-full wp-image-455 image-border" src="img/9330eca7-5962-4191-8fc5-82c8628a870f.png" style="" width="519" height="324"/></div>
<p>You can play around with the values of the lower and upper thresholds to detect edges more accurately for a given image. Edge detection is a very important preprocessing step for many computer vision applications and Canny edge detection is widely used for that.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Straight line detection using Hough transform</h1>
                
            
            
                
<p>The detection of straight lines is important in many computer vision applications, like lane detection. It can also be used to detect lines that are part of other regular shapes. Hough transform is a popular feature extraction technique used in computer vision to detect straight lines.  We will not go into detail about how Hough transform detects lines, but we will see how it can be implemented in OpenCV and CUDA. The code for implementing Hough transform for line detection is as follows:</p>
<pre>#include &lt;cmath&gt;<br/>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/><br/>using namespace std;<br/>using namespace cv;<br/>using namespace cv::cuda;<br/><br/><br/>int main()<br/>{<br/>  Mat h_image = imread("images/drawing.JPG",0);<br/>  if (h_image.empty())<br/>  {<br/>    cout &lt;&lt; "can not open image"&lt;&lt; endl;<br/>    return -1;<br/>  }<br/><br/>  Mat h_edge;<br/>  cv::Canny(h_image, h_edge, 100, 200, 3);<br/><br/>  Mat h_imagec;<br/>  cv::cvtColor(h_edge, h_imagec, COLOR_GRAY2BGR);<br/>  Mat h_imageg = h_imagec.clone();<br/>  GpuMat d_edge, d_lines;<br/>  d_edge.upload(h_edge);<br/>  {<br/>    const int64 start = getTickCount();<br/>    Ptr&lt;cuda::HoughSegmentDetector&gt; hough = cuda::createHoughSegmentDetector(1.0f, (float) (CV_PI / 180.0f), 50, 5);<br/>    hough-&gt;detect(d_edge, d_lines);<br/><br/>    const double time_elapsed = (getTickCount() - start) / getTickFrequency();<br/>    cout &lt;&lt; "GPU Time : " &lt;&lt; time_elapsed * 1000 &lt;&lt; " ms" &lt;&lt; endl;<br/>    cout &lt;&lt; "GPU FPS : " &lt;&lt; (1/time_elapsed) &lt;&lt; endl;<br/>  }<br/>  vector&lt;Vec4i&gt; lines_g;<br/>  if (!d_lines.empty())<br/>  {<br/>    lines_g.resize(d_lines.cols);<br/>    Mat h_lines(1, d_lines.cols, CV_32SC4, &amp;lines_g[0]);<br/>    d_lines.download(h_lines);<br/>  }<br/>  for (size_t i = 0; i &lt; lines_g.size(); ++i)<br/>  {<br/>    Vec4i line_point = lines_g[i];<br/>    line(h_imageg, Point(line_point[0], line_point[1]), Point(line_point[2], line_point[3]), Scalar(0, 0, 255), 2, LINE_AA);<br/>  }<br/><br/>  imshow("source", h_image);<br/>  imshow("detected lines [GPU]", h_imageg);<br/>  waitKey(0);<br/>  return 0;<br/>}</pre>
<p>OpenCV provides the <kbd>createHoughSegmentDetector</kbd> class for implementing Hough transform. It needs an edge map of an image as input. So edges are detected from an image using a Canny edge detector.  The output of the Canny edge detector is uploaded to the device memory for GPU computation. The edges can also be computed on GPU as discussed in the last section.</p>
<p>The object of <kbd>createHoughSegmentDetector</kbd> is created. It requires many arguments. The first argument indicates the resolution of parameter <kbd>r</kbd> used in Hough transform, which is taken as 1 pixel normally. The second argument is the resolution of parameter theta in radians, which is taken as 1 radian or pi/180.  The third argument is the minimum number of points that are needed to form a line, which is taken as 50 pixels. The final argument is the maximum gap between two points to be considered as the same line, which is taken as 5 pixels.</p>
<p>The detect method of the created object is used to detect straight lines. It needs two arguments. The first argument is the image on which the edges are to be detected, and the second argument is the array in which detected line points will be stored. The array contains the starting and ending (x,y) points of the detected lines.  This array is iterated using the <kbd>for</kbd> loop to draw individual lines on an image using the line function from OpenCV.  The final image is displayed using the <kbd>imshow</kbd> function.</p>
<p>Hough transform is a mathematically intensive step. Just to show an advantage of CUDA, we will implement the same algorithm for CPU and compare the performance of it with a CUDA implementation. The CPU code for Hough transform is as follows:</p>
<pre>Mat h_imagec; <br/>vector&lt;Vec4i&gt; h_lines;<br/>{<br/>  const int64 start = getTickCount();<br/>  HoughLinesP(h_edge, h_lines, 1, CV_PI / 180, 50, 60, 5);<br/>  const double time_elapsed = (getTickCount() - start) / getTickFrequency();<br/>  cout &lt;&lt; "CPU Time : " &lt;&lt; time_elapsed * 1000 &lt;&lt; " ms" &lt;&lt; endl;<br/>  cout &lt;&lt; "CPU FPS : " &lt;&lt; (1/time_elapsed) &lt;&lt; endl;<br/>}<br/><br/>for (size_t i = 0; i &lt; h_lines.size(); ++i)<br/>{<br/>  Vec4i line_point = h_lines[i];<br/>  line(h_imagec, Point(line_point[0], line_point[1]), Point(line_point[2], line_point[3]), Scalar(0, 0, 255), 2, LINE_AA);<br/>}<br/>imshow("detected lines [CPU]", h_imagec);</pre>
<p>The <kbd>HoughLinesP</kbd> function is used for detecting lines on a CPU using probabilistic Hough transform. The first two arguments are the source image and the array to store output line points. The third and fourth arguments are a resolution for <kbd>r</kbd> and theta. The fifth argument is the threshold that indicates the minimum number of intersection points for a line. The sixth argument indicates the minimum number of points needed to form a line. The last argument indicates the maximum gap between points to be considered on the same line. </p>
<p>The array returned by the function is iterated using the <kbd>for</kbd> loop for displaying detected lines on the original image. The output for both the GPU and CPU function is as  follows:</p>
<div><img class="alignnone size-full wp-image-456 image-border" src="img/7942ad86-ce16-4db4-9fcb-31724c0d1985.png" style="" width="661" height="288"/></div>
<p>The comparison between the performance of the GPU and CPU code for the Hough transform is shown in the following screenshot:</p>
<div><img class="alignnone size-full wp-image-457 image-border" src="img/62f0224e-327a-4efc-a17f-bb9679864639.png" style="" width="734" height="108"/></div>
<p>It takes around 4 ms for a single image to process on the CPU and 1.5 ms on the GPU, which is equivalent to 248 FPS on the CPU, and 632 FPS on the GPU, which is almost 2.5 times an improvement on the GPU.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Circle detection </h1>
                
            
            
                
<p>Hough transform can also be used for circle detection. It can be used in many applications, like ball detection and tracking and coin detection, and so on, where objects are circular. OpenCV and CUDA provide a class to implement this. The code for coin detection using Hough transform is as follows:</p>
<pre>#include "opencv2/opencv.hpp"<br/>#include &lt;iostream&gt;<br/><br/>using namespace cv;<br/>using namespace std;<br/><br/>int main(int argc, char** argv)<br/>{<br/>  Mat h_image = imread("images/eight.tif", IMREAD_COLOR);<br/>  Mat h_gray;<br/>  cvtColor(h_image, h_gray, COLOR_BGR2GRAY);<br/>  cuda::GpuMat d_gray,d_result;<br/>  std::vector&lt;cv::Vec3f&gt; d_Circles;<br/>cv::Ptr&lt;cv::cuda::HoughCirclesDetector&gt; detector = cv::cuda::createHoughCirclesDetector(1, 100, 122, 50, 1, max(h_image.size().width, h_image.size().height));<br/>  d_gray.upload(h_gray);<br/>  detector-&gt;detect(d_gray, d_result);<br/>  d_Circles.resize(d_result.size().width);<br/>  if (!d_Circles.empty())<br/>    d_result.row(0).download(cv::Mat(d_Circles).reshape(3, 1));<br/><br/>  cout&lt;&lt;"No of circles: " &lt;&lt;d_Circles.size() &lt;&lt;endl;<br/>  for( size_t i = 0; i &lt; d_Circles.size(); i++ )<br/>  {<br/>    Vec3i cir = d_Circles[i];<br/>    circle( h_image, Point(cir[0], cir[1]), cir[2], Scalar(255,0,0), 2, LINE_AA);<br/>  }<br/>  imshow("detected circles", h_image);<br/>  waitKey(0);<br/><br/>  return 0;<br/>}</pre>
<p>There is a <kbd>createHoughCirclesDetector</kbd> class for detecting the circular object. The object of that class is created. Many arguments can be provided while creating an object of this class. The first argument is <kbd>dp</kbd> that signifies an inverse ratio of the accumulator resolution to the image resolution, which is mostly taken as 1. The second argument is the minimum distance between the centers of the detected circle. The third argument is a Canny threshold and the fourth argument is the accumulator threshold. The fifth and sixth arguments are the minimum and maximum radiuses of the circles to be detected.</p>
<p>The minimum distance between the centers of the circle is taken as <kbd>100</kbd> pixels. You can play around with this value. If this is decreased, then many circles are detected falsely on the original image, while if it is increased then some true circles may be missed. The last two arguments, which are the minimum and maximum radiuses, can be taken as <kbd>0</kbd> if you don't know the exact dimension. In the preceding code, it is taken as <kbd>1</kbd> and maximum dimension of an image to detect all circles in an image. The output of the program is as follows:</p>
<div><img class="alignnone size-full wp-image-458 image-border" src="img/b7076793-815c-4a5f-8693-ba885a00feb0.png" style="" width="581" height="247"/></div>
<p>The Hough transform is very sensitive to Gaussian and salt-pepper noise. So, sometimes it is better to preprocess the image with Gaussian and median filters before applying Hough transform. It will give more accurate results.</p>
<p>To summarize, we have used the Hough line and circle transforms to detect objects with regular shapes. Contours and convexity can also be used for shape detection. The functions for this are available in OpenCV, but they are not available with CUDA implementation. You will have to develop your own versions of these functions.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Key-point detectors and descriptors</h1>
                
            
            
                
<p>Till this point, we have used global features like color and shape to detect an object. These features are easy to compute, are quick, and require a small amount of memory, but they can only be used when some information regarding the object is already available. If that is not the case then local features are used, which require more computation and memory, but they are more accurate. In this section, various algorithms that find local features are explained. They are also called key point detectors. Key-points are the points that characterize the image and can be used to define an object accurately. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Features from Accelerated Segment Test (FAST) feature detector</h1>
                
            
            
                
<p>The FAST algorithm is used to detect corner points as key-points from an image. It detects the corners by applying a segment test to every pixel. It considers a circle of 16 pixels around the pixel. If there are <em>n</em> continuous points in a circle of radius 16, which have the intensity of pixel greater than <em>Ip +t</em> or less than <em>Ip- t,</em> then that pixel is considered a corner. <em>Ip</em> is intensity at pixel <em>p,</em> and <em>t</em> is the selected threshold.</p>
<p>Sometimes instead of checking all points in the radius, a few selected  points are checked for intensity values to determine corner points. It accelerates the performance of the FAST algorithm. FAST provides corner points that can be utilized as key-points to detect an object. It is rotation-invariant, as corners of an object will remain the same even if the object is rotated. FAST is not scale-invariant, as the increase in dimension may result in a smooth transition of intensity values rather than a sharp transition at corners.</p>
<p>OpenCV and CUDA provide an efficient way of implementing the FAST algorithm. The program to detect key-points using the FAST algorithm is shown below:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/> <br/>using namespace cv;<br/>using namespace std;<br/> <br/>int main()<br/>{<br/>  Mat h_image = imread( "images/drawing.JPG", 0 );<br/> <br/>  //Detect the key-points using FAST Detector<br/>  cv::Ptr&lt;cv::cuda::FastFeatureDetector&gt; detector = cv::cuda::FastFeatureDetector::create(100,true,2);<br/>  std::vector&lt;cv::key point&gt; key-points;<br/>  cv::cuda::GpuMat d_image;<br/>  d_image.upload(h_image);<br/>  detector-&gt;detect(d_image, key-points);<br/>  cv::drawkey-points(h_image,key-points,h_image);<br/>  //Show detected key-points<br/>  imshow("Final Result", h_image );<br/>  waitKey(0);<br/>  return 0;<br/>}</pre>
<p>OpenCV and CUDA provide a <kbd>FastFeatureDetector</kbd> class for implementing the FAST algorithm. The object of this class is created using the create method of the class. It needs three arguments. The first argument is the intensity threshold to be used for the FAST algorithm. The second argument specifies whether to use non-maximum suppression or not. It is a Boolean value, which can be specified as <kbd>true</kbd> or <kbd>false</kbd>. The third argument indicates which FAST method is used for calculating the neighborhood. Three methods, <kbd>cv2.FAST_FEATURE_DETECTOR_TYPE_5_8</kbd>,   <kbd>cv2.FAST_FEATURE_DETECTOR_TYPE_7_12</kbd>, and <kbd>cv2.FAST_FEATURE_DETECTOR_TYPE_9_16</kbd> , are available, which can be specified as flags <kbd>0</kbd>, <kbd>1</kbd>, or <kbd>2</kbd>. </p>
<p>The detect method of the created object is used to detect key-points. It needs an input image and vector to store key-points as an argument. The calculated key-points can be drawn on an original image using the <kbd>drawkey-points</kbd> function. It requires source image, the vector of the key-points and the destination image as an argument.</p>
<p>The intensity threshold can be changed to detect the different number of key-points. If the threshold is low, then more key-points will pass the segment test and will be categorized as key-points. As this threshold is increased, the number of key-points detected will gradually decrease. In the same way, if non-maximum suppression is false then more than one key point is detected at a single corner point. The output of the code is as follows:</p>
<div><img class="alignnone size-full wp-image-459 image-border" src="img/2b5da030-d05b-4e4b-aa85-80d08b02719f.png" style="" width="486" height="562"/></div>
<p>As can be seen from the output, as the threshold increases from 10 to 50 and 100, the number of key-points decreases. These key-points can be used to detect an object in a query image.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Oriented FAST and Rotated BRIEF (ORB) feature detection</h1>
                
            
            
                
<p>ORB is a very efficient feature detection and description algorithm. It is a combination of the FAST algorithm for feature detection and the <strong>Binary Robust Independent Elementary Features</strong> (<strong>BRIEF</strong>) algorithm for feature description. It provides an efficient alternative to the SURF and SIFT algorithms, which are widely used for object detection. As they are patented, their use should be paid for. ORB matches the performance of SIFT and SURF at no cost. </p>
<p>OpenCV and CUDA provide an easy API for implementing the ORB algorithm. The code for implementing the ORB algorithm is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/> <br/>using namespace cv;<br/>using namespace std;<br/> <br/>int main()<br/>{<br/>  Mat h_image = imread( "images/drawing.JPG", 0 );<br/>  cv::Ptr&lt;cv::cuda::ORB&gt; detector = cv::cuda::ORB::create();<br/>  std::vector&lt;cv::key point&gt; key-points;<br/>  cv::cuda::GpuMat d_image;<br/>  d_image.upload(h_image);<br/>  detector-&gt;detect(d_image, key-points);<br/>  cv::drawkey-points(h_image,key-points,h_image);<br/>  imshow("Final Result", h_image );<br/>  waitKey(0);<br/>  return 0;<br/>}</pre>
<p>The object of the <kbd>ORB</kbd> class is created using the create method. All the arguments to this method are optional so we are using the default values for it. The detect method of the created object is used to detect key-points from an image. It requires an input image and the vector of key-points in which the output will be stored as arguments. The detected key-points are drawn on the image using the <kbd>drawkey-points</kbd> function. The output of the preceding code is as follows:</p>
<div><img class="alignnone size-full wp-image-460 image-border" src="img/979882c6-5901-4d4b-b18d-5dca9b669df7.png" style="" width="495" height="285"/></div>
<p>The <kbd>ORB</kbd> class also provides a method to calculate descriptors for all the key-points. These descriptors can accurately describe the object and can be used to detect objects from an image. These descriptors can also be used to classify an object.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Speeded up robust feature detection and matching</h1>
                
            
            
                
<p>SURF approximates Laplacian of Gaussian with computation based on a simple two-dimensional box filter as described in the last chapter.  The convolution with the box filter can be easily calculated with the help of integral images, which improves the performance of the algorithm. SURF relies on the determinant of the Hessian matrix for both scale and location. The approximated determinant of Hessian can be expressed as:</p>
<p><img class="fm-editor-equation" src="img/894254f9-4c9b-40da-b0d0-9c0636194147.png" style="width:17.33em;height:2.00em;" width="2080" height="240"/></p>
<p>Where <em>w</em> is a relative weight for the filter response and used to balance the expression for the determinant. The <em>Dx</em>, <em>Dy</em> are the result of the Laplacian operator in <em>X</em>- and <em>Y</em>-direction.</p>
<p>SURF uses wavelet responses in horizontal and vertical directions, using an integral image approach for orientation assignment.  Adequate Gaussian weights are also applied to it. The dominant orientation is estimated by calculating the sum of all responses within a sliding orientation window of angle 60 degrees.</p>
<p>For feature description, SURF uses Haar wavelet responses in horizontal and vertical directions. This is computed for all subregions in an image, resulting in a SURF feature descriptor with a total of 64 dimensions. The lower the dimensions, the higher the speed of computation and matching will be. For more precision, the SURF feature descriptor has an extended 128-dimension version. SURF is rotation-invariant and scale-invariant. </p>
<p>SURF has a higher processing speed than SIFT because it uses a 64-directional feature vector compared to SIFT, which uses a 128-dimensional feature vector. SURF is good at handling images with blurring and rotation, but not good at handling a viewpoint change and illumination change.</p>
<p>OpenCV and CUDA provide an API to calculate SURF key-points and descriptors. We will also see how these can be used to detect an object in the query image. The code for SURF feature detection and matching is as follows:</p>
<pre>#include &lt;stdio.h&gt;<br/>#include &lt;iostream&gt;<br/>#include "opencv2/opencv.hpp"<br/>#include "opencv2/features2d.hpp"<br/>#include "opencv2/xfeatures2d.hpp"<br/>#include "opencv2/xfeatures2d/nonfree.hpp"<br/>#include "opencv2/xfeatures2d/cuda.hpp"<br/><br/>using namespace cv;<br/>using namespace cv::xfeatures2d;<br/>using namespace std;<br/><br/>int main( int argc, char** argv )<br/>{<br/>  Mat h_object_image = imread( "images/object1.jpg", 0 ); <br/>  Mat h_scene_image = imread( "images/scene1.jpg", 0 );<br/>  cuda::GpuMat d_object_image;<br/>  cuda::GpuMat d_scene_image;<br/>  cuda::GpuMat d_key-points_scene, d_key-points_object; <br/>  vector&lt; key point &gt; h_key-points_scene, h_key-points_object;<br/>  cuda::GpuMat d_descriptors_scene, d_descriptors_object;<br/>  d_object_image.upload(h_object_image);<br/>  d_scene_image.upload(h_scene_image);<br/>  cuda::SURF_CUDA surf(150);<br/>  surf( d_object_image, cuda::GpuMat(), d_key-points_object, d_descriptors_object );<br/>surf( d_scene_image, cuda::GpuMat(), d_key-points_scene, d_descriptors_scene );<br/><br/>Ptr&lt; cuda::DescriptorMatcher &gt; matcher = cuda::DescriptorMatcher::createBFMatcher();<br/>vector&lt; vector&lt; DMatch&gt; &gt; d_matches;<br/>matcher-&gt;knnMatch(d_descriptors_object, d_descriptors_scene, d_matches, 3);<br/>surf.downloadkey-points(d_key-points_scene, h_key-points_scene);<br/>surf.downloadkey-points(d_key-points_object, h_key-points_object);<br/>std::vector&lt; DMatch &gt; good_matches;<br/>for (int k = 0; k &lt; std::min(h_key-points_object.size()-1, d_matches.size()); k++)<br/>{<br/>  if ( (d_matches[k][0].distance &lt; 0.75*(d_matches[k][1].distance)) &amp;&amp;<br/>      ((int)d_matches[k].size() &lt;= 2 &amp;&amp; (int)d_matches[k].size()&gt;0) )<br/>  {<br/>    good_matches.push_back(d_matches[k][0]);<br/>  }<br/>}<br/>std::cout &lt;&lt; "size:" &lt;&lt;good_matches.size();<br/>Mat h_image_result;<br/>drawMatches( h_object_image, h_key-points_object, h_scene_image, h_key-points_scene,<br/>      good_matches, h_image_result, Scalar::all(-1), Scalar::all(-1),<br/>      vector&lt;char&gt;(), DrawMatchesFlags::DEFAULT );<br/>imshow("Good Matches &amp; Object detection", h_image_result);<br/>waitKey(0);<br/>return 0;<br/>}</pre>
<p>Two images are read from the disk. The first image contains the object to be detected. The second image is the query image in which the object is to be searched. We will calculate SURF features from both the images, and then these features will be matched for detecting an object from a query image. </p>
<p>OpenCV provides the <kbd>SURF_CUDA</kbd> class for calculating SURF features. The object of this class is created. It requires the Hessian threshold as an argument. It is taken as <kbd>150</kbd>. This threshold determines how large the output from the Hessian determinant calculation must be in order for a point to be considered as a key point. A larger threshold value will result in fewer but more salient interest points, and a smaller value will result in more numerous but less salient points. It can be chosen according to the application.</p>
<p>This <kbd>surf</kbd> object is used to calculate key-points and descriptors from both the object and query image. The image, data type of the image, vectors to store key-points, and descriptors are passed as an argument. To match the object in the query image, descriptors from both the images need to be matched. OpenCV provides the different type of matching algorithms for this purpose, like the Brute-Force matcher and the <strong>Fast Library for Approximate Nearest Neighbors</strong> (<strong>FLANN</strong>) matcher. </p>
<p>The Brute-Force matcher is used in the program; it is a simple method. It takes the descriptor of one feature in an object that is matched with all other features in the query image, using some distance calculation. It returns the best match key point, or best <kbd>k</kbd> matches using the nearest neighbor algorithm when using the <kbd>knnMatch</kbd> method of the <kbd>matcher</kbd> class. The <kbd>knnMatch</kbd> method requires two sets of descriptors along with the number of nearest neighbors. It is taken as <kbd>3</kbd> in the code.  </p>
<p>The good, matching key-points are extracted from the matching points returned by the <kbd>knnMatch</kbd> method. These good matches are found out by using the ratio test method, described in the original paper. These good matches are used to detect an object from a scene.</p>
<p>The <kbd>drawMatches</kbd> function is used to draw a line between matched good points from both the images. It requires many arguments. The first argument is the source image, the second image is the key-points of the source image, the third argument is the second image, the fourth argument is the key-points of the second image, and the fifth argument is the output image. The sixth argument is the color of the lines and key-points. It is taken as <kbd>Scalar::all(-1)</kbd> , which indicates that a color will be taken randomly.  The seventh argument is the color of the key-points, which do not have any matches. It is also taken as <kbd>Scalar::all(-1)</kbd> , which indicates that a color will be taken randomly. The last two arguments specify a mask to draw matches and flag settings. The empty mask is taken so that all matches are drawn.</p>
<p>These matches can be used to draw a bounding box around the detected object, which will localize the object from the scene. The code for drawing a bounding box is as follows:</p>
<pre>std::vector&lt;Point2f&gt; object;<br/>std::vector&lt;Point2f&gt; scene;<br/>for (int i = 0; i &lt; good_matches.size(); i++) {<br/>  object.push_back(h_key-points_object[good_matches[i].queryIdx].pt);<br/>  scene.push_back(h_key-points_scene[good_matches[i].trainIdx].pt);<br/>}<br/>Mat Homo = findHomography(object, scene, RANSAC);<br/>std::vector&lt;Point2f&gt; corners(4);<br/>std::vector&lt;Point2f&gt; scene_corners(4);<br/>corners[0] = Point(0, 0);<br/>corners[1] = Point(h_object_image.cols, 0);<br/>corners[2] = Point(h_object_image.cols, h_object_image.rows);<br/>corners[3] = Point(0, h_object_image.rows);<br/>perspectiveTransform(corners, scene_corners, Homo);<br/>line(h_image_result, scene_corners[0] + Point2f(h_object_image.cols, 0),scene_corners[1] + Point2f(h_object_image.cols, 0), Scalar(255, 0, 0), 4);<br/>line(h_image_result, scene_corners[1] + Point2f(h_object_image.cols, 0),scene_corners[2] + Point2f(h_object_image.cols, 0),Scalar(255, 0, 0), 4);<br/>line(h_image_result, scene_corners[2] + Point2f(h_object_image.cols, 0),scene_corners[3] + Point2f(h_object_image.cols, 0),Scalar(255, 0, 0), 4);<br/>line(h_image_result, scene_corners[3] + Point2f(h_object_image.cols, 0),scene_corners[0] + Point2f(h_object_image.cols, 0),Scalar(255, 0, 0), 4);</pre>
<p>OpenCV provides the <kbd>findHomography</kbd> function to search for the position, orientation, and scale of the object in the scene, based on the good matches. The first two arguments are good, matching key-points from the object and scene images. The <strong>random sample consensus</strong> (<strong>RANSAC</strong>) method is passed as one of the arguments that is used to find the best translation matrix.  </p>
<p>After finding this translation matrix, the <kbd>perspectiveTransform</kbd> function is used to find an object. It requires four corner points and a translation matrix as an argument. These transformed points are used to draw a bounding box around the detected object. The output of the SURF program for finding features and matching objects is as follows:</p>
<div><img class="alignnone size-full wp-image-461 image-border" src="img/cbe37af6-d0c7-44ba-a414-69bebff1e47b.png" style="" width="936" height="464"/></div>
<p>The figure contains the object image, query image, and detected image. As can be seen from the preceding image, SURF can accurately determine the position of the object even if the object is rotated. Though sometimes it may detect the wrong features. The Hessian threshold and test ratio can be varied to find the optimum matches.</p>
<p>So to summarize, in this section we have seen FAST, ORB, and SURF key point detection algorithms. We have also seen how these points can be used to match and localize an object in an image by using SURF features as an example. You can try using FAST and ORB features to do the same. In the next section, we discuss Haar cascades in detail for detecting faces and eyes from an image.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Object detection using Haar cascades</h1>
                
            
            
                
<p>A Haar cascade uses rectangular features to detect an object. It uses rectangles of different sizes to calculate different line and edge features. The rectangle contains some black and white regions, as shown in the following figure, and they are centered at different positions in an image:</p>
<div><img class="alignnone size-full wp-image-462 image-border" src="img/21388154-44a4-49a5-b2e8-1a85342c94ce.png" style="" width="438" height="342"/></div>
<p>The idea behind the Haar-like feature selection algorithm is to compute the difference between the sum of white pixels and the sum of black pixels inside the rectangle.</p>
<p>The main advantage of this method is the fast sum computation using the integral image. This makes a Haar cascade ideal for real-time object detection. It requires less time for processing an image than algorithms like SURF described previously. This algorithm can also be implemented on embedded systems, like Raspberry Pi, because it is less computationally intensive and has less memory footprint. It is called Haar-like because it is based on the same principle as Haar wavelets. Haar cascades are widely used in human body detection, along with its parts like face and eye detection. It can also be used for expression analysis. The Haar cascade can be utilized for detecting vehicles like a car.</p>
<p>In this section, the Haar cascade is described for detecting faces and eyes from an image and webcam. Haar cascade is a machine learning algorithm, which needs to be trained to do a particular task. It is difficult to train a Haar cascade from scratch for a particular application, so OpenCV provides some trained XML files, which can be used to detect objects. These XML files are provided in the <kbd>opencv\data\haarcascades_cuda</kbd> folder of an OpenCV or CUDA installation.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Face detection using Haar cascades</h1>
                
            
            
                
<p>We will use the Haar cascade for detecting faces from an image and live webcam in this section. The code for detecting faces from an image using the Haar cascade is as follows: </p>
<pre>#include "opencv2/objdetect/objdetect.hpp"<br/>#include "opencv2/highgui/highgui.hpp"<br/>#include "opencv2/imgproc/imgproc.hpp"<br/>#include "opencv2/cudaobjdetect.hpp" <br/>#include &lt;iostream&gt;<br/>#include &lt;stdio.h&gt;<br/> <br/>using namespace std;<br/>using namespace cv;<br/> <br/>int main( )<br/>{<br/>  Mat h_image;<br/>  h_image = imread("images/lena_color_512.tif", 0); <br/>  Ptr&lt;cuda::CascadeClassifier&gt; cascade = cuda::CascadeClassifier::create("haarcascade_frontalface_alt2.xml");<br/>  cuda::GpuMat d_image;<br/>  cuda::GpuMat d_buf;<br/>  d_image.upload(h_image);<br/>  cascade-&gt;detectMultiScale(d_image, d_buf);<br/>  std::vector&lt;Rect&gt; detections;<br/>  cascade-&gt;convert(d_buf, detections);<br/>  if (detections.empty())<br/>    std::cout &lt;&lt; "No detection." &lt;&lt; std::endl;<br/>  cvtColor(h_image,h_image,COLOR_GRAY2BGR);<br/>  for(int i = 0; i &lt; detections.size(); ++i)<br/>  {<br/>    rectangle(h_image, detections[i], Scalar(0,255,255), 5);<br/>  }<br/>  imshow("Result image", h_image);<br/>  waitKey(0); <br/>  return 0;<br/>}</pre>
<p>OpenCV and CUDA provide the <kbd>CascadeClassifier</kbd> class that can be used for implementing the Haar cascade. The create method is used to create an object of that class. It requires the filename of the trained XML file to be loaded. There is the <kbd>detectMultiScale</kbd> method of the created object, which detects an object at multiple scales from an image. It requires an image file and a <kbd>Gpumat</kbd> array to store the output results as arguments. This <kbd>gpumat</kbd> vector is converted to a standard rectangle vector by using the convert method of the <kbd>CascadeClassifier</kbd> object. This converted vector contains the coordinates for drawing a rectangle on the detected object.</p>
<p>The <kbd>detectMultiScale</kbd> function has many parameters that can be modified before invoking the function. These include <kbd>scaleFactor</kbd> that is used to specify how much the image size will be reduced at each image scale, and  <kbd>minNeighbors</kbd>  specifies the value of the minimum neighbors each rectangle should have for retention; <kbd>minSize</kbd> specifies the minimum object size and <kbd>maxSize</kbd> the maximum object size. All these parameters have their default values, so in normal scenarios there is no need for modification. If we want to change them, then we can use the following code before invoking the <kbd>detectMultiscale</kbd> function:</p>
<pre>cascade-&gt;setMinNeighbors(0);<br/>cascade-&gt;setScaleFactor(1.01);</pre>
<p>This first function will set minimum neighbors to <kbd>0</kbd> and the second function will reduce the image size by a factor of <kbd>1.01</kbd> after every scale. The scale factor is very important for detecting objects with a different size. If it is large then the algorithm will take less time to complete, but some faces might not be detected. If it is small then the algorithm will take more time to complete, and it will be more accurate. The output of the preceding code is as follows:</p>
<div><img class="alignnone size-full wp-image-463 image-border" src="img/aff00cf8-0f15-4aba-ae6e-620cb2744731.png" style="" width="491" height="240"/></div>


            

            
        
    </div></div>
<div><div><h1 class="header-title">From video</h1>
                
            
            
                
<p>The same concept of the Haar cascade can be used to detect faces from videos. The code for detecting a face is included inside the <kbd>while</kbd> loop so that the face is detected in every frame of a video. The code for face detection from a webcam is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;opencv2/opencv.hpp&gt;<br/>using namespace cv;<br/>using namespace std;<br/><br/>int main()<br/>{<br/>  VideoCapture cap(0);<br/>  if (!cap.isOpened()) {<br/>    cerr &lt;&lt; "Can not open video source";<br/>    return -1;<br/>  }<br/>  std::vector&lt;cv::Rect&gt; h_found;<br/>  cv::Ptr&lt;cv::cuda::CascadeClassifier&gt; cascade = cv::cuda::CascadeClassifier::create("haarcascade_frontalface_alt2.xml");<br/>  cv::cuda::GpuMat d_frame, d_gray, d_found;<br/>  while(1)<br/>  {<br/>    Mat frame;<br/>    if ( !cap.read(frame) ) {<br/>      cerr &lt;&lt; "Can not read frame from webcam";<br/>      return -1;<br/>    }<br/>    d_frame.upload(frame);<br/>    cv::cuda::cvtColor(d_frame, d_gray, cv::COLOR_BGR2GRAY);<br/><br/>    cascade-&gt;detectMultiScale(d_gray, d_found);<br/>    cascade-&gt;convert(d_found, h_found);<br/>       <br/>    for(int i = 0; i &lt; h_found.size(); ++i)<br/>    {<br/>      rectangle(frame, h_found[i], Scalar(0,255,255), 5);<br/>    }<br/><br/>    imshow("Result", frame);<br/>    if (waitKey(1) == 'q') {<br/>      break;<br/>    }<br/>  }<br/><br/>  return 0;<br/>}</pre>
<p>The webcam is initialized and frames from the webcam are captured one by one. This frame is uploaded to the device memory for processing on GPU. The object of the  <kbd>CascadeClassifier</kbd> class is created by using the <kbd>create</kbd> method of the class. The XML file for face detection is provided as an argument while creating an object. Inside the <kbd>while</kbd> loop, the <kbd>detectMultiscale</kbd> method is applied to every frame so that faces of different sizes can be detected in each frame. The detected location is converted to a rectangle vector using the <kbd>convert</kbd> method. Then this vector is iterated using the <kbd>for</kbd> loop so that the bounding box can be drawn using the <kbd>rectangle</kbd> function on all the detected faces. The output of the program is as follows: </p>
<div><img class="alignnone size-full wp-image-464 image-border" src="img/4605a82b-4693-4f9a-a19a-6226b654cca4.png" style="" width="638" height="509"/></div>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Eye detection using Haar cascades</h1>
                
            
            
                
<p>This section will describe the use of Haar cascades in detecting the eyes of humans. The XML file for a trained Haar cascade for eye detection is provided in the OpenCV installation directory. This file is used to detect eyes. The code for it is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;stdio.h&gt;<br/> #include &lt;opencv2/opencv.hpp&gt;<br/><br/>using namespace std;<br/>using namespace cv;<br/> <br/>int main( )<br/>{<br/>  Mat h_image;<br/>  h_image = imread("images/lena_color_512.tif", 0); <br/>  Ptr&lt;cuda::CascadeClassifier&gt; cascade = cuda::CascadeClassifier::create("haarcascade_eye.xml");<br/>  cuda::GpuMat d_image;<br/>  cuda::GpuMat d_buf;<br/>  d_image.upload(h_image);<br/>  cascade-&gt;setScaleFactor(1.02);<br/>  cascade-&gt;detectMultiScale(d_image, d_buf);<br/>  std::vector&lt;Rect&gt; detections;<br/>  cascade-&gt;convert(d_buf, detections);<br/>  if (detections.empty())<br/>    std::cout &lt;&lt; "No detection." &lt;&lt; std::endl;<br/>    cvtColor(h_image,h_image,COLOR_GRAY2BGR);<br/>    for(int i = 0; i &lt; detections.size(); ++i)<br/>    {<br/>      rectangle(h_image, detections[i], Scalar(0,255,255), 5);<br/>    }<br/><br/>    imshow("Result image", h_image);<br/>     <br/>    waitKey(0); <br/>    return 0;<br/>  }<br/>}</pre>
<p>The code is similar to the code for face detection. This is the advantage of using Haar cascades. If an XML file for a trained Haar cascade on a given object is available then the same code will work on all applications. Just the name of the XML file needs to change when creating an object of the <kbd>CascadeClassifier</kbd> class. In the preceding code, <kbd>haarcascade_eye.xml</kbd> , which is a trained XML file for eye detection, is used. The other code is self-explanatory. The scale factor is set at <kbd>1.02</kbd> so that the image size will be reduced by <kbd>1.02</kbd> at every scale.</p>
<p>The output of the eye detection program is as follows:</p>
<div><img class="alignnone size-full wp-image-561 image-border" src="img/ed98b373-902a-491b-adfb-2cfd7d5aff16.png" style="" width="475" height="247"/></div>
<p>The sizes of eyes are different because of the viewpoint taken to capture the image, but still the Haar cascade is able to localize both eyes efficiently. The performance of the code can also be measured to see how quickly it can work. </p>
<p>To summarize, in this section we demonstrated the use of Haar cascades for face and eye detection. It is very simple to implement once the trained file is available, and it is a very powerful algorithm. It is widely used in embedded or mobile environments where memory and processing power are limited.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Object tracking using background subtraction</h1>
                
            
            
                
<p>Background subtraction is the process of separating out foreground objects from the background in a sequence of video frames. It is widely used in object detection and tracking applications to remove the background part. Background subtraction is performed in four steps:</p>
<ol>
<li>Image preprocessing</li>
<li>Modeling of background</li>
<li>Detection of foreground</li>
<li>Data validation</li>
</ol>
<p>Image preprocessing is always performed to remove any kind of noise present in the image. The second step is to model the background so that it can be separated from the foreground. In some applications, the first frame of the video is taken as the background and it is not updated. The absolute difference between each frame and the first frame is taken to separate foreground from background.</p>
<p>In other techniques, the background is modeled by taking an average or median of all the frames that have been seen by the algorithm, and that background is separated from the foreground. It will be more robust for illumination changes and will produce a more dynamic background than the first method. Even more statistically intensive models, like Gaussian models and support vector models that use a history of frames, can be used for modeling a background. </p>
<p>The third step is to segregate the foreground from the modeled background by taking the absolute difference between the current frame and the background. This absolute difference is compared with the set threshold, and if it is greater than the threshold then the objects are considered moving, and if it is less than the threshold then the objects are stationary.  </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Mixture of Gaussian (MoG) method</h1>
                
            
            
                
<p>MoG is a widely used background subtraction method used for separating foregrounds from  backgrounds, based on Gaussian mixtures. The background is continuously updated from the sequence of frames. A mixture of K Gaussian distribution is used to categorize pixels as being foreground or background. The time sequence of the frame is also weighted to improve background modeling. The intensities that are continuously changing are categorized as foreground, and the intensities that are static are categorized as background.</p>
<p>OpenCV and CUDA provide an easy API to implement MoG for background subtraction. The code for this is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;string&gt;<br/>#include "opencv2/opencv.hpp"<br/>using namespace std;<br/>using namespace cv;<br/>using namespace cv::cuda;<br/>int main()<br/>{<br/>  VideoCapture cap("abc.avi");<br/>  if (!cap.isOpened())<br/>  {<br/>    cerr &lt;&lt; "can not open camera or video file" &lt;&lt; endl;<br/>    return -1;<br/>  }<br/>  Mat frame;<br/>  cap.read(frame);<br/>  GpuMat d_frame;<br/>  d_frame.upload(frame);<br/>  Ptr&lt;BackgroundSubtractor&gt; mog = cuda::createBackgroundSubtractorMOG();<br/>  GpuMat d_fgmask,d_fgimage,d_bgimage;<br/>  Mat h_fgmask,h_fgimage,h_bgimage;<br/>  mog-&gt;apply(d_frame, d_fgmask, 0.01);<br/>  while(1)<br/>  {<br/>    cap.read(frame);<br/>    if (frame.empty())<br/>      break;<br/>    d_frame.upload(frame);<br/>    int64 start = cv::getTickCount();<br/>    mog-&gt;apply(d_frame, d_fgmask, 0.01);<br/>    mog-&gt;getBackgroundImage(d_bgimage);<br/>    double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/>    std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/>    d_fgimage.create(d_frame.size(), d_frame.type());<br/>    d_fgimage.setTo(Scalar::all(0));<br/>    d_frame.copyTo(d_fgimage, d_fgmask);<br/>    d_fgmask.download(h_fgmask);<br/>    d_fgimage.download(h_fgimage);<br/>    d_bgimage.download(h_bgimage);<br/>    imshow("image", frame);<br/>    imshow("foreground mask", h_fgmask);<br/>    imshow("foreground image", h_fgimage);<br/>    imshow("mean background image", h_bgimage);<br/>    if (waitKey(1) == 'q')<br/>      break;<br/>  }<br/><br/>  return 0;<br/>}</pre>
<p>The <kbd>createBackgroundSubtractorMOG</kbd> class is used to create an object for MoG implementation. It can be provided with some optional arguments while creating an object. The parameters include <kbd>history</kbd>, <kbd>nmixtures</kbd>, <kbd>backgroundRatio</kbd>, and <kbd>noiseSigma</kbd>. The <kbd>history</kbd> parameter signifies the number of previous frames used for modeling the background. Its default value is 200. The <kbd>nmixture</kbd> parameter specifies the number of Gaussian mixtures used to segregate pixels. Its default value is 5. You can play around with these values according to an application.</p>
<p>The <kbd>apply</kbd> method of the created object is used to create a foreground mask from the first frame. It requires an input image and an image array to store the foreground mask and learning rate as the input. This foreground mask and the background image are continuously updated after every frame inside the <kbd>while</kbd> loop. The <kbd>getBackgroundImage</kbd> function is used to fetch the current background model.</p>
<p>The foreground mask is used to create a foreground image that indicates which objects are currently moving. It is basically logical and operates between the original frame and foreground mask.  The foreground mask, foreground image, and the modeled background are downloaded to host memory after every frame for displaying on the screen.</p>
<p>The MoG model is applied to a video from the PETS 2009 dataset, which is widely used for pedestrian detection. It has a static background and persons are moving around in the video. The output of two different frames from the video is as follows:</p>
<div><img class="alignnone size-full wp-image-465 image-border" src="img/506e1519-220b-44a8-8b2d-6d17caefa52d.png" style="" width="1192" height="505"/></div>
<p>As can be seen,  MoG models the background very efficiently. Only the persons who are moving are present in the foreground mask and foreground image. This foreground image can be used for further processing of the detected objects. If a human stops walking, then he will start being part of the background, as can be seen from the result of the second frame. So this algorithm can only be used to detect moving objects. It will not consider static objects. The performance of MoG in terms of frame rate is as follows:</p>
<div><img class="alignnone size-full wp-image-466 image-border" src="img/0fdcd137-6d13-4bf1-9f43-e96a16a60237.png" style="" width="143" height="72"/></div>
<p>The frame rate is updated after every frame. It can be seen that it is around 330 frames per second, which is very high and easily used in real-time applications. OpenCV and CUDA also provide the second version of MoG, which can be invoked by the <kbd>createBackgroundSubtractorMOG2</kbd> class.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">GMG for background subtraction</h1>
                
            
            
                
<p>The name of the algorithm GMG is derived from the initials of the inventors who proposed the algorithm. The algorithm is a combination of background estimation and Bayesian segmentation per pixel. It uses Bayesian inference to separate the background from the foreground. It also uses the  history of frames for modeling the background. It is again weighted based on the time sequence of the frame. The new observation is weighted more than the old observations. </p>
<p>OpenCV and CUDA provide a similar API to MoG for implementation of the GMG algorithm. The code for implementing the GMG algorithm for background subtraction is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;string&gt;<br/>#include "opencv2/opencv.hpp"<br/>#include "opencv2/core.hpp"<br/>#include "opencv2/core/utility.hpp"<br/>#include "opencv2/cudabgsegm.hpp"<br/>#include "opencv2/cudalegacy.hpp"<br/>#include "opencv2/video.hpp"<br/>#include "opencv2/highgui.hpp"<br/><br/>using namespace std;<br/>using namespace cv;<br/>using namespace cv::cuda;<br/><br/>int main()<br/>{<br/>  VideoCapture cap("abc.avi");<br/>  if (!cap.isOpened())<br/>  {<br/>    cerr &lt;&lt; "can not open video file" &lt;&lt; endl;<br/>    return -1;<br/>  }<br/>  Mat frame;<br/>  cap.read(frame);<br/>  GpuMat d_frame;<br/>  d_frame.upload(frame);<br/>  Ptr&lt;BackgroundSubtractor&gt; gmg = cuda::createBackgroundSubtractorGMG(40);<br/>  GpuMat d_fgmask,d_fgimage,d_bgimage;<br/>  Mat h_fgmask,h_fgimage,h_bgimage;<br/>  gmg-&gt;apply(d_frame, d_fgmask);<br/>  while(1)<br/>  {<br/>    cap.read(frame);<br/>    if (frame.empty())<br/>      break;<br/>    d_frame.upload(frame);<br/>    int64 start = cv::getTickCount();<br/>    gmg-&gt;apply(d_frame, d_fgmask, 0.01);<br/>    double fps = cv::getTickFrequency() / (cv::getTickCount() - start);<br/>    std::cout &lt;&lt; "FPS : " &lt;&lt; fps &lt;&lt; std::endl;<br/>    d_fgimage.create(d_frame.size(), d_frame.type());<br/>    d_fgimage.setTo(Scalar::all(0));<br/>    d_frame.copyTo(d_fgimage, d_fgmask);<br/>    d_fgmask.download(h_fgmask);<br/>    d_fgimage.download(h_fgimage);<br/>    imshow("image", frame);<br/>    imshow("foreground mask", h_fgmask);<br/>    imshow("foreground image", h_fgimage);<br/>    if (waitKey(30) == 'q')<br/>      break;<br/>  }<br/>  return 0;<br/>}</pre>
<p>The <kbd>createBackgroundSubtractorGMG</kbd> class is used to create an object for GMG implementation. It can be provided with two arguments while creating an object. The first argument is the number of previous frames used for modeling the background. It is taken as <kbd>40</kbd> in the code above. The second argument is the decision threshold, which is used to categorize pixels as foreground. Its default value is 0.8. </p>
<p>The <kbd>apply</kbd> method of the created object is used on the first frame to create a foreground mask. The foreground mask and foreground image are continuously updated inside the <kbd>while</kbd> loop by using the history of frames. The foreground mask is used to create a foreground image in a similar way as shown for MoG. The output of the GMG algorithm on the same video and two frames is as follows:</p>
<div><img class="alignnone size-full wp-image-467 image-border" src="img/7fc9de7e-58f0-450f-9613-13d086fb19fc.png" style="" width="859" height="486"/></div>
<p>The output of GMG is noisy compared to MoG. The morphological opening and closing operation can be applied to the result of GMG to remove shadowing noise present in the result. The performance of the GMG algorithm in terms of FPS is as follows:</p>
<div><img class="alignnone size-full wp-image-468 image-border" src="img/2e29cf7f-4dac-4595-b16b-f2339c01b284.png" style="" width="131" height="94"/></div>
<p>As it is more computationally intensive than MoG, the FPS rate is less,  but still the FPS performance is 120, which is more than 30 FPS for real-time performance.</p>
<p>To summarize, in this section we have seen two methods for background modeling and background subtraction. MoG is faster and less noisy compared to the GMG algorithm. The GMG algorithm requires morphological operations to remove noise present in the result.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>This chapter described the role of OpenCV and CUDA in real-time object detection and tracking applications. It started with the introduction of object detection and tracking, along with challenges encountered in that process and the applications of it. Different features like color, shape, histograms, and other distinct key-points, like corners, can be used to detect and track objects in an image. Color-based object detection is easier to implement, but it requires that the object should have a distinct color from the background. For shape-based object detection, the Canny edge detection technique has been described to detect edges, and Hough transform has been described for straight line and circle detection. It has many applications, such as land detection, ball tracking, and so on. The color and shape are global features, which are easier to compute and require less memory. They are more susceptible to noise. Other algorithms like FAST, ORB, and SURF have been described in detail, which can be used to detect key-points from the image, and these key-points can be used to describe an image accurately, which in turn can be used to detect an object in the image. ORB is open source and gives a comparable result to SURF at no cost. SURF is patented but it is faster, scale-invariant, and rotation-invariant. The Haar cascade has been described, which is a simple algorithm used to detect objects like faces, eyes, and the human body from an image. It can be used in embedded systems for real-time applications. The last part of the chapter described background subtraction algorithms in detail, such as MoG and GMG, which can separate foregrounds from backgrounds. The output of these algorithms can be used for object detection and tracking. The next chapter will describe how these applications can be deployed on embedded development boards.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>Write an OpenCV code for detecting objects of yellow color from the video.</li>
<li>In which situations will the object detection using color fail?</li>
<li>Why is the Canny edge detection algorithm better than the other edge detection algorithm seen in the last chapter?</li>
<li>What can be done to reduce the noise sensitivity of Hough transform?</li>
<li>What is the importance of the threshold in the FAST key point detector?</li>
<li>What is the importance of the Hessian threshold in the SURF detector?</li>
<li>If the scale factor in a Haar cascade is changed from 1.01 to 1.05 then what effect will it have on output?</li>
<li>Compare the MoG and GMG background subtraction methods. What can be done to remove noise from GMG outputs?</li>
</ol>


            

            
        
    </div></div></body></html>