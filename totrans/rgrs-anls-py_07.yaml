- en: Chapter 7. Online and Batch Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will be presented with best practices when it comes to
    training classifiers on big data. The new approach, exposed in the following pages,
    is both scalable and generic, making it perfect for datasets with a huge number
    of observations. Moreover, this approach can allow you to cope with streaming
    datasets—that is, datasets with observations transmitted on-the-fly and not all
    available at the same time. Furthermore, such an approach enhances precision,
    as more data is fed in during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: With respect to the classic approach seen so far in the book, batch learning,
    this new approach is, not surprisingly, called online learning. The core of online
    learning is the *divide et impera* (divide and conquer) principle whereby each
    step of a mini-batch of the data serves as input to train and improve the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first focus on batch learning and its limitations,
    and then introduce online learning. Finally, we will supply an example of big
    data, showing the benefits of combining online learning and hashing tricks.
  prefs: []
  type: TYPE_NORMAL
- en: Batch learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the dataset is fully available at the beginning of a supervised task, and
    doesn't exceed the quantity of RAM on your machine, you can train the classifier
    or the regression using batch learning. As seen in previous chapters, during training
    the learner scans the full dataset. This also happens when **stochastic gradient
    descent** (SGD)-based methods are used (see [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression* and [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*).
    Let's now compare how much time is needed to train a linear regressor and relate
    its performance with the number of observations in the dataset (that is, the number
    of rows of the feature matrix *X*) and the number of features (that is, the number
    of columns of *X*). In this first experiment, we will use the plain vanilla `LinearRegression()`
    and `SGDRegressor()` classes provided by Scikit-learn, and we will store the actual
    time taken to fit a classifier, without any parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create a function to create fake datasets: it takes as parameters
    the number of training points and the number of features (plus, optionally, the
    noise variance), and returns normalized training and testing feature matrixes
    and labels. Note that all the features in the *X* matrix are numerical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now store the time needed to train and test the learner in all the combinations
    of these configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two classifiers: `LinearRegression()` and `SGDRegressor()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of observations: `1000`, `10000`, and `100000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of features: `10`, `50`, `100`, `500`, and `1000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To average the results, each training operation is performed five times. The
    testing dataset always comprises `1000` observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s finally plot the results. In the following screenshot, the chart on
    the left shows the training time of the `LogisticRegressor` algorithm against
    the number of features, whereas the chart on the right displays the time against
    the number of observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Batch learning](img/00113.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the plots, you can see that the classifier is pretty good on small datasets,
    with a small number of features and observations. While dealing with the largest
    *X* matrix, 1,000 features and 100,000 observations (containing 100 million entries),
    the training time is just above `30` seconds: that''s also the limit above which
    the regressor no longer scales.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see what happens with the testing time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Batch learning](img/00114.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The testing time does scale as a linear function of the number of features,
    and it's independent of it. It seems that applying the linear approach is not
    very much of a problem on big data, fortunately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see what happens with the SGD implementation of linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Batch learning](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The results have drastically changed in comparison with the previous regressor:
    on the biggest matrix, this learner takes around 1.5 seconds. It also seems that
    the time needed to train an SGD regressor is linear with respect to the number
    of features and the number of training points. Let''s now verify how it works
    in testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Batch learning](img/00116.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Applying the SGD-based learner on a test dataset takes about the same time as
    the other implementation. Here, again, there is really no problem when scaling
    the solution on big datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Online mini-batch learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the previous section, we''ve learned an interesting lesson: for big data,
    always use SGD-based learners because they are faster, and they do scale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in this section, let''s consider this regression dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Massive number of observations: 2M'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Large number of features: 100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `X_train` matrix is composed of 200 million elements, and may not completely
    fit in memory (on a machine with 4 GB RAM); the testing set is composed of 10,000
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create the datasets, and print the memory footprint of the biggest
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `X_train` matrix is itself `1.6` GB of data; we can consider it as a starting
    point for big data. Let's now try to classify it using the best model we got from
    the previous section, `SGDRegressor()`. To access its performance, we use MAE,
    the Mean Absolute Error (as for error evaluation, the lower the better).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: On our computer (equipped with Mac OS and 4 GB of RAM), this operation takes
    around 6 seconds, and the final MAE is 10^(-1.24).
  prefs: []
  type: TYPE_NORMAL
- en: Can we do better? Yes, with mini-batches and online learning. Before we see
    these in action, let's introduce how SGD works with mini-batches.
  prefs: []
  type: TYPE_NORMAL
- en: Split the `X_train` matrix in batches of *N* observations. Since we're using
    SGD, if possible it's better to shuffle the observations, as the method is strongly
    driven by the order of the input vectors. At this point, every mini-batch has
    *N* lines and *M* columns (where *M* is the number of features).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train the learner using a mini-batch. SGD coefficients are initialized randomly,
    as shown previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train the learner using another mini-batch. SGD coefficients are initialized
    as the output of the previous step (using the `partial_fit` method).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 3 until you've used all the mini-batches. In each step, the coefficients
    of the SGD model are refined and modified according to the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is clearly a smart approach, and it doesn't take too long to implement.
    You just need to set the initial values of each coefficient for every new batch
    and train the learner on the mini-batch.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in terms of performance, what do we get using online learning?
  prefs: []
  type: TYPE_NORMAL
- en: We have an incremental way to train the model. Since, at every step, we can
    test the model, we can stop at any point we think is good enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don't need to keep the whole `X_train` matrix in memory; we just need to
    keep the mini-batch in RAM. That also means that the consumed RAM is constant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have a way to control the learning: we can have small mini-batches or big
    ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see now how it performs, by changing the batch size (that is, the number
    of observations for each observation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Online mini-batch learning](img/00117.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the end, the final MAE is always the same; that is, batch learning and online
    learning eventually provide the same results when both are trained on the whole
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: We also see that, by using a small mini-batch (1,000 observations), we have
    a working model after just 1 millisecond. Of course, it's not a perfect solution
    since its MAE is just 10^(-0.94), but still we now have a reasonable working model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's compare timings to fully train the model. Using mini-batches, the
    total time is around 1.2 seconds; using the batch it was greater than 5 seconds.
    The MAEs are more or less equal—why such a difference in timings? Because the
    dataset didn't all fit in RAM and the system kept on swapping data with storage
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now focus on mini-batch size: is smaller really always better? Actually,
    it will produce an output earlier, but it will take more time in total.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a plot of the training time and the MAE of the learner, when trained
    with different mini-batch sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Online mini-batch learning](img/00118.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A real example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now combine feature hashing (seen in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*), batch-learning, and SGD.
    From what we''ve seen so far, this should be the best way to deal with big data
    because:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of features is constant (feature hashing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of observations per batch is constant (batch learning).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It allows streaming datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm is stochastic (SGD).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All these points together will ensure a few consequences:'
  prefs: []
  type: TYPE_NORMAL
- en: We can very quickly have a model (after the first mini-batch) that is refined
    with time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAM consumption is constant (since every mini-batch has exactly the same size).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ideally, we can deal with as many observation as we want.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the real-world example, let''s use a textual input: the Twenty Newsgroups
    dataset. This dataset contains 20,000 messages (textual content) extracted from
    20 different newsgroups, each of them on a different topic. The webpage of the
    project is: [https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to classify each document in one of the possible labels (it''s
    a classification task). Let''s first load it, and split it into train and test.
    To make it more real, we''re going to remove headers, footers, and quoted e-mail
    from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now create a function that yields mini-batches of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the core task is simply to classify the document. We first apply feature
    hashing via the `HashingVectorizer` class, whose output feeds a `SGDClassifier`
    (another class with the `partial_fit` method). This fact will ensure an additional
    advantage: since the output of the `HashingVectorizer` is very sparse, a sparse
    representation is used, making the mini-batch size even more compact in memory'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what the best hash size is, we may try a full search with sizes
    of `1000`, `5000`, `10000`, `50000`, and `100000` and then measure the accuracy
    for each learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot our results on a graph representing time and accuracy for
    each size of the hash size. The *X* signs on the graph are the instances (and
    related accuracy) when the classifier outputs a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![A real example](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It appears from the obtained results that using a hash table bigger than 10,000
    elements can allow us to achieve the best performance. In this exercise, the mini-batch
    size was fixed to 1,000 observations; this means that every mini-batch was a matrix
    of 10 M elements, represented in sparse way. It also means that, for every mini-batch,
    the memory used is up to 80 MB of RAM.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming scenario without a test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many real cases, the test dataset is not available. What can we do? The
    best practice would be to:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the data until you reach a specific mini-batch size; let's say 10 observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shuffle the observations, and store eight of them within the train set, and
    two in the test set (for an 80/20 validation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the classifier on the train set and test on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to step 1\. With each mini-batch, the train set will increase by 10
    observations and the test set by 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have just described the classic method, used when data is consistent and
    the dataset is not very large. If the features change throughout the streaming,
    and you need to build a learner that has to adapt to rapid changes of feature
    statistics. then simply don''t use a test set and follow this algorithm. In addition,
    this is the preferred way to learn from big data:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the data till you reach a mini-batch size; let's say 10 observations.
    Don't shuffle and train the learner with all the observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait till you fetch another mini-batch. Test the classifier on those observations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the classifier with the mini-batch you received in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The good thing about this algorithm is that you don't have to keep anything
    but the model and the current mini-batch in memory; these are used first to test
    the learner, and then to update it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've introduced the concepts of batch and online learning,
    which are necessary to be able to process big datasets (big data) in a quick and
    scalable way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore some advanced techniques of machine learning
    that will produce great results for some classes of well-known problems.
  prefs: []
  type: TYPE_NORMAL
