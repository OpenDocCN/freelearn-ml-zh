- en: Chapter 7. Online and Batch Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。在线和批量学习
- en: In this chapter, you will be presented with best practices when it comes to
    training classifiers on big data. The new approach, exposed in the following pages,
    is both scalable and generic, making it perfect for datasets with a huge number
    of observations. Moreover, this approach can allow you to cope with streaming
    datasets—that is, datasets with observations transmitted on-the-fly and not all
    available at the same time. Furthermore, such an approach enhances precision,
    as more data is fed in during the training process.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解在大数据上训练分类器的最佳实践。在接下来的几页中公开的新方法既可扩展又通用，使其非常适合具有大量观测值的数据集。此外，这种方法还可以让您处理流数据集——即观测值在飞行中传输且不是同时可用的数据集。此外，这种方法通过在训练过程中输入更多数据来提高精度。
- en: With respect to the classic approach seen so far in the book, batch learning,
    this new approach is, not surprisingly, called online learning. The core of online
    learning is the *divide et impera* (divide and conquer) principle whereby each
    step of a mini-batch of the data serves as input to train and improve the classifier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与书中迄今为止看到的经典方法（批量学习）相比，这种新的方法不出所料地被称为在线学习。在线学习的核心是 *divide et impera*（分而治之）原则，即数据的小批量中的每一步都作为训练和改进分类器的输入。
- en: In this chapter, we will first focus on batch learning and its limitations,
    and then introduce online learning. Finally, we will supply an example of big
    data, showing the benefits of combining online learning and hashing tricks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先关注批量学习及其局限性，然后介绍在线学习。最后，我们将提供一个大数据的例子，展示结合在线学习和哈希技巧的好处。
- en: Batch learning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量学习
- en: When the dataset is fully available at the beginning of a supervised task, and
    doesn't exceed the quantity of RAM on your machine, you can train the classifier
    or the regression using batch learning. As seen in previous chapters, during training
    the learner scans the full dataset. This also happens when **stochastic gradient
    descent** (SGD)-based methods are used (see [Chapter 2](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 2. Approaching Simple Linear Regression"), *Approaching Simple Linear
    Regression* and [Chapter 3](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 3. Multiple Regression in Action"), *Multiple Regression in Action*).
    Let's now compare how much time is needed to train a linear regressor and relate
    its performance with the number of observations in the dataset (that is, the number
    of rows of the feature matrix *X*) and the number of features (that is, the number
    of columns of *X*). In this first experiment, we will use the plain vanilla `LinearRegression()`
    and `SGDRegressor()` classes provided by Scikit-learn, and we will store the actual
    time taken to fit a classifier, without any parallelization.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当监督任务开始时数据集完全可用，并且不超过您机器上的RAM数量时，您可以使用批量学习来训练分类器或回归。如前几章所见，在学习过程中，学习器会扫描整个数据集。这也发生在使用基于
    **随机梯度下降**（SGD）的方法时（见[第2章](part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6
    "第2章。接近简单线性回归")，*接近简单线性回归*和[第3章](part0023_split_000.html#LTSU2-a2faae6898414df7b4ff4c9a487a20c6
    "第3章。实际中的多重回归")，*实际中的多重回归*)。现在让我们比较训练线性回归器所需的时间，并将其性能与数据集中的观测值数量（即特征矩阵 *X* 的行数）和特征数量（即
    *X* 的列数）联系起来。在这个第一个实验中，我们将使用Scikit-learn提供的普通`LinearRegression()`和`SGDRegressor()`类，并且我们将存储拟合分类器实际花费的时间，而不进行任何并行化。
- en: 'Let''s first create a function to create fake datasets: it takes as parameters
    the number of training points and the number of features (plus, optionally, the
    noise variance), and returns normalized training and testing feature matrixes
    and labels. Note that all the features in the *X* matrix are numerical:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个函数来创建假数据集：它接受训练点的数量和特征的数目（以及可选的噪声方差）作为参数，并返回归一化的训练和测试特征矩阵以及标签。请注意，*X*
    矩阵中的所有特征都是数值型的：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s now store the time needed to train and test the learner in all the combinations
    of these configurations:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们存储训练和测试学习者在所有这些配置组合中所需的时间：
- en: 'Two classifiers: `LinearRegression()` and `SGDRegressor()`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个分类器：`LinearRegression()` 和 `SGDRegressor()`
- en: 'Number of observations: `1000`, `10000`, and `100000`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观测值数量：`1000`，`10000` 和 `100000`
- en: 'Number of features: `10`, `50`, `100`, `500`, and `1000`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征数量：`10`，`50`，`100`，`500` 和 `1000`
- en: 'To average the results, each training operation is performed five times. The
    testing dataset always comprises `1000` observations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s finally plot the results. In the following screenshot, the chart on
    the left shows the training time of the `LogisticRegressor` algorithm against
    the number of features, whereas the chart on the right displays the time against
    the number of observations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Batch learning](img/00113.jpeg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: 'In the plots, you can see that the classifier is pretty good on small datasets,
    with a small number of features and observations. While dealing with the largest
    *X* matrix, 1,000 features and 100,000 observations (containing 100 million entries),
    the training time is just above `30` seconds: that''s also the limit above which
    the regressor no longer scales.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see what happens with the testing time:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Batch learning](img/00114.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: The testing time does scale as a linear function of the number of features,
    and it's independent of it. It seems that applying the linear approach is not
    very much of a problem on big data, fortunately.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now see what happens with the SGD implementation of linear regression:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Batch learning](img/00115.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'The results have drastically changed in comparison with the previous regressor:
    on the biggest matrix, this learner takes around 1.5 seconds. It also seems that
    the time needed to train an SGD regressor is linear with respect to the number
    of features and the number of training points. Let''s now verify how it works
    in testing:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Batch learning](img/00116.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: Applying the SGD-based learner on a test dataset takes about the same time as
    the other implementation. Here, again, there is really no problem when scaling
    the solution on big datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Online mini-batch learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the previous section, we''ve learned an interesting lesson: for big data,
    always use SGD-based learners because they are faster, and they do scale.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in this section, let''s consider this regression dataset:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Massive number of observations: 2M'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Large number of features: 100'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy dataset
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `X_train` matrix is composed of 200 million elements, and may not completely
    fit in memory (on a machine with 4 GB RAM); the testing set is composed of 10,000
    observations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create the datasets, and print the memory footprint of the biggest
    one:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `X_train` matrix is itself `1.6` GB of data; we can consider it as a starting
    point for big data. Let's now try to classify it using the best model we got from
    the previous section, `SGDRegressor()`. To access its performance, we use MAE,
    the Mean Absolute Error (as for error evaluation, the lower the better).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: On our computer (equipped with Mac OS and 4 GB of RAM), this operation takes
    around 6 seconds, and the final MAE is 10^(-1.24).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Can we do better? Yes, with mini-batches and online learning. Before we see
    these in action, let's introduce how SGD works with mini-batches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Split the `X_train` matrix in batches of *N* observations. Since we're using
    SGD, if possible it's better to shuffle the observations, as the method is strongly
    driven by the order of the input vectors. At this point, every mini-batch has
    *N* lines and *M* columns (where *M* is the number of features).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train the learner using a mini-batch. SGD coefficients are initialized randomly,
    as shown previously.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train the learner using another mini-batch. SGD coefficients are initialized
    as the output of the previous step (using the `partial_fit` method).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 3 until you've used all the mini-batches. In each step, the coefficients
    of the SGD model are refined and modified according to the input.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is clearly a smart approach, and it doesn't take too long to implement.
    You just need to set the initial values of each coefficient for every new batch
    and train the learner on the mini-batch.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Now, in terms of performance, what do we get using online learning?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: We have an incremental way to train the model. Since, at every step, we can
    test the model, we can stop at any point we think is good enough.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We don't need to keep the whole `X_train` matrix in memory; we just need to
    keep the mini-batch in RAM. That also means that the consumed RAM is constant.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have a way to control the learning: we can have small mini-batches or big
    ones.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see now how it performs, by changing the batch size (that is, the number
    of observations for each observation):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Online mini-batch learning](img/00117.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: In the end, the final MAE is always the same; that is, batch learning and online
    learning eventually provide the same results when both are trained on the whole
    training set.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We also see that, by using a small mini-batch (1,000 observations), we have
    a working model after just 1 millisecond. Of course, it's not a perfect solution
    since its MAE is just 10^(-0.94), but still we now have a reasonable working model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's compare timings to fully train the model. Using mini-batches, the
    total time is around 1.2 seconds; using the batch it was greater than 5 seconds.
    The MAEs are more or less equal—why such a difference in timings? Because the
    dataset didn't all fit in RAM and the system kept on swapping data with storage
    memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now focus on mini-batch size: is smaller really always better? Actually,
    it will produce an output earlier, but it will take more time in total.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a plot of the training time and the MAE of the learner, when trained
    with different mini-batch sizes:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Online mini-batch learning](img/00118.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: A real example
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now combine feature hashing (seen in [Chapter 5](part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6
    "Chapter 5. Data Preparation"), *Data Preparation*), batch-learning, and SGD.
    From what we''ve seen so far, this should be the best way to deal with big data
    because:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The number of features is constant (feature hashing).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of observations per batch is constant (batch learning).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It allows streaming datasets.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm is stochastic (SGD).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All these points together will ensure a few consequences:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: We can very quickly have a model (after the first mini-batch) that is refined
    with time.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RAM consumption is constant (since every mini-batch has exactly the same size).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ideally, we can deal with as many observation as we want.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the real-world example, let''s use a textual input: the Twenty Newsgroups
    dataset. This dataset contains 20,000 messages (textual content) extracted from
    20 different newsgroups, each of them on a different topic. The webpage of the
    project is: [https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to classify each document in one of the possible labels (it''s
    a classification task). Let''s first load it, and split it into train and test.
    To make it more real, we''re going to remove headers, footers, and quoted e-mail
    from the dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s now create a function that yields mini-batches of the dataset:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, the core task is simply to classify the document. We first apply feature
    hashing via the `HashingVectorizer` class, whose output feeds a `SGDClassifier`
    (another class with the `partial_fit` method). This fact will ensure an additional
    advantage: since the output of the `HashingVectorizer` is very sparse, a sparse
    representation is used, making the mini-batch size even more compact in memory'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what the best hash size is, we may try a full search with sizes
    of `1000`, `5000`, `10000`, `50000`, and `100000` and then measure the accuracy
    for each learner:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we plot our results on a graph representing time and accuracy for
    each size of the hash size. The *X* signs on the graph are the instances (and
    related accuracy) when the classifier outputs a model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![A real example](img/00119.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: It appears from the obtained results that using a hash table bigger than 10,000
    elements can allow us to achieve the best performance. In this exercise, the mini-batch
    size was fixed to 1,000 observations; this means that every mini-batch was a matrix
    of 10 M elements, represented in sparse way. It also means that, for every mini-batch,
    the memory used is up to 80 MB of RAM.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Streaming scenario without a test set
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many real cases, the test dataset is not available. What can we do? The
    best practice would be to:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the data until you reach a specific mini-batch size; let's say 10 observations.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shuffle the observations, and store eight of them within the train set, and
    two in the test set (for an 80/20 validation).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the classifier on the train set and test on the test set.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to step 1\. With each mini-batch, the train set will increase by 10
    observations and the test set by 2.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have just described the classic method, used when data is consistent and
    the dataset is not very large. If the features change throughout the streaming,
    and you need to build a learner that has to adapt to rapid changes of feature
    statistics. then simply don''t use a test set and follow this algorithm. In addition,
    this is the preferred way to learn from big data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚描述了经典方法，该方法在数据一致且数据集不是很大时使用。如果特征在整个流中发生变化，并且你需要构建一个必须适应特征统计快速变化的学习者，那么就简单地不要使用测试集，并遵循此算法。此外，这是从大数据中学习的首选方式：
- en: Fetch the data till you reach a mini-batch size; let's say 10 observations.
    Don't shuffle and train the learner with all the observations.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据，直到达到小批量大小；比如说 10 个观察结果。不要打乱顺序，并使用所有观察结果来训练学习者。
- en: Wait till you fetch another mini-batch. Test the classifier on those observations.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等到你获取到另一个小批量数据。在这些观察结果上测试分类器。
- en: Update the classifier with the mini-batch you received in the previous step.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一步中接收到的那个小批量更新分类器。
- en: Go back to step 2.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到步骤 2。
- en: The good thing about this algorithm is that you don't have to keep anything
    but the model and the current mini-batch in memory; these are used first to test
    the learner, and then to update it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的好处是，你只需要在内存中保留模型和当前的小批量数据；这些首先用于测试学习者，然后用于更新它。
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've introduced the concepts of batch and online learning,
    which are necessary to be able to process big datasets (big data) in a quick and
    scalable way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了批量和在线学习概念，这对于能够快速且可扩展地处理大数据集（大数据）是必要的。
- en: In the next chapter, we will explore some advanced techniques of machine learning
    that will produce great results for some classes of well-known problems.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一些机器学习的先进技术，这些技术将为一些已知问题类别产生很好的结果。
