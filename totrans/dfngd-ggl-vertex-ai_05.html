<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer074">
			<h1 id="_idParaDest-67" class="chapter-number"><a id="_idTextAnchor066"/>5</h1>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>No-Code Options for Building ML Models</h1>
			<p>In recent years, the world of machine learning has undergone a profound transformation, breaking free from the realm of expert data scientists and engineers to empower a broader audience. The rise of no-code machine learning platforms has ushered in a new era, where individuals with diverse skill sets and backgrounds can harness the power of artificial intelligence to solve complex challenges, without writing a single line of code. This democratization of machine learning has not only expedited the development process but has also opened up a myriad of opportunities for businesses and <span class="No-Break">individuals alike.</span></p>
			<p>In this chapter, we will dive into the foundations of no-code machine learning, shedding light on the remarkable tools and services Google Cloud Vertex AI offers. We will explore how users can leverage prebuilt machine learning models, AutoML capabilities, and visual interfaces to construct sophisticated and highly accurate models with ease. From computer vision to natural language processing and tabular data analysis, Google Cloud Vertex AI covers a vast array of use cases, democratizing AI application development <span class="No-Break">for everyone.</span></p>
			<p>The key topics we will cover in this chapter include <span class="No-Break">the following:</span></p>
			<ul>
				<li>What <span class="No-Break">is AutoML?</span></li>
				<li>What is Vertex <span class="No-Break">AI AutoML?</span></li>
				<li>Creating and deploying a model using Vertex <span class="No-Break">AI AutoML</span></li>
				<li>Getting predictions from a deployed Vertex <span class="No-Break">AI model</span></li>
			</ul>
			<p>Let’s first start by looking at the different solutions offered by Google Cloud to facilitate model creation without <span class="No-Break">using code.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>ML modeling options in Google Cloud</h1>
			<p>Google Cloud offers several solutions<a id="_idIndexMarker201"/> within Vertex AI and the broader <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) to build and consume machine learning models. These solutions vary widely in terms of required data science <a id="_idIndexMarker202"/>and coding skills, catering to both advanced ML engineers, relatively less technical business analysts, and everyone in between these two personas. The three main GCP solutions for model creation are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Big Query ML (BQML)</strong>: This is part of the <a id="_idIndexMarker203"/>BigQuery platform and requires only the knowledge of SQL for <a id="_idIndexMarker204"/>someone to train and use a model to generate predictions on structured data. More details about BQML will be covered in <a href="B17792_06.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Low-Code Options for Building </em><span class="No-Break"><em class="italic">ML Models</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Vertex AI AutoML</strong>: This allows users <a id="_idIndexMarker205"/>to build models with <a id="_idIndexMarker206"/>no coding or even SQL knowledge, and it is primarily GUI-based. However, it has APIs that can be accessed programmatically <span class="No-Break">if required.</span></li>
				<li><strong class="bold">Vertex AI custom training</strong>: This option <a id="_idIndexMarker207"/>provides users complete flexibility on their model training and deployment but also requires basic <span class="No-Break">coding ability.</span></li>
			</ul>
			<p>The following table shows a comparison of the different options available in Google Cloud to create machine <span class="No-Break">learning models:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">BQML</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Vertex </strong><span class="No-Break"><strong class="bold">AI AutoML</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Vertex </strong><span class="No-Break"><strong class="bold">AI custom</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Coding <span class="No-Break">requirements</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Very Low</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">None</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">High</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Required ML engineering <span class="No-Break">expertise</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Low</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Low</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium to high, depending on the type <span class="No-Break">of model</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Limits on <span class="No-Break">data size</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Yes. Standard BigQuery quotas and <span class="No-Break">limits apply.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Yes. Dataset limitations vary by the type of dataset being used (see the GCP documentation for <span class="No-Break">current limits).</span></p>
						</td>
						<td class="No-Table-Style">
							<p>No limit is imposed by GCP for datasets that are <span class="No-Break">not managed.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Types of <span class="No-Break">models supported</span></p>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li class="Table-Bullet-style"><span class="No-Break">LINEAR REG</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">LOGISTIC REG</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">KMEANS</span></li>
								<li class="Table-Bullet-style">MATRIX <span class="No-Break">FACTORIZATION</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">PCA</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">AUTOENCODER</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">AUTOML CLASSIFIER</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">AUTOML REGRESSOR</span></li>
								<li class="Table-Bullet-style">BOOSTED <span class="No-Break">TREE CLASSIFIER</span></li>
								<li class="Table-Bullet-style">BOOSTED <span class="No-Break">TREE REGRESSOR</span></li>
								<li class="Table-Bullet-style">RANDOM <span class="No-Break">FOREST CLASSIFIER</span></li>
								<li class="Table-Bullet-style">RANDOM <span class="No-Break">FOREST REGRESSOR</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">DNN CLASSIFIER</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">DNN REGRESSOR</span></li>
								<li class="Table-Bullet-style">DNN LINEAR COMBINED <span class="No-Break">CLASSIFIER</span></li>
								<li class="Table-Bullet-style">DNN LINEAR COMBINED <span class="No-Break">REGRESSOR</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">ARIMA PLUS</span></li>
								<li class="Table-Bullet-style">ARIMA <span class="No-Break">PLUS XREG</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">TENSORFLOW</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">TENSORFLOW LITE</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">ONNX</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">XGBOOST</span></li>
							</ul>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Image</span><span class="No-Break">:</span></p>
							<ul>
								<li class="Table-Bullet-style"><span class="No-Break">Classification</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">Object detection</span></li>
							</ul>
							<p><span class="No-Break">Text</span><span class="No-Break">:</span></p>
							<ul>
								<li class="Table-Bullet-style"><span class="No-Break">Classification</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">Entity extraction</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">Sentiment analysis</span></li>
							</ul>
							<p><span class="No-Break">Video</span><span class="No-Break">:</span></p>
							<ul>
								<li class="Table-Bullet-style"><span class="No-Break">Action recognition</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">Classification</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">Object tracking</span></li>
							</ul>
							<p><span class="No-Break">Tabular</span><span class="No-Break">:</span></p>
							<ul>
								<li class="Table-Bullet-style"><span class="No-Break">Regression</span></li>
								<li class="Table-Bullet-style"><span class="No-Break">Classification</span></li>
								<li class="Table-Bullet-style">Time <span class="No-Break">series forecasting</span></li>
							</ul>
						</td>
						<td class="No-Table-Style">
							<p>Full flexibility to build any type of <span class="No-Break">ML model</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Model <span class="No-Break">development speed</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Fast. Some data preparation is required, but training can be <span class="No-Break">mostly automated.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Fast. Minimal data preparation and fully automated <span class="No-Break">model training.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Slower. More data preparation is required. Significant model design and training <span class="No-Break">management.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Flexibility/control over model <span class="No-Break">generation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Medium</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Low</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">High</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Does the tool support feature <span class="No-Break">engineering?</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 5.1 – ML model creation options in Google Cloud</p>
			<p>As shown in the preceding table, Vertex AI AutoML is the key code-less model creation option available as part of Google Cloud. When AutoML was initially launched as part of GCP, it used to be a <a id="_idIndexMarker208"/>standalone product. Now, it is part of the overall Vertex AI platform, and the legacy AutoML product is on the roadmap to be sunset. Now, let’s understand what AutoML is and how you can use AutoML features available <span class="No-Break">in GCP.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>What is AutoML?</h1>
			<p>AutoML refers to the methodology of <a id="_idIndexMarker209"/>automating the process of building machine learning models, including data preprocessing, feature engineering, model selection, hyperparameter tuning, and model deployment. AutoML aims to make machine learning accessible and more efficient for non-experts, saving time and resources for experts by reducing the amount of manual work involved in building a model. Different types of AutoML products on the market offer different levels of automation. Some just automate the training and hyperparameter portion of it, while some do end-to-end automation by also automating the steps of data preprocessing and <span class="No-Break">feature generation.</span></p>
			<p>AutoML tools allow users to specify their requirements, such as accuracy, interpretability, or training time, and then automatically select and train the best model based on these criteria. It can be used for various types of machine learning tasks, including classification, regression, and time series forecasting on structured and unstructured datasets. AutoML technologies have seen rapid development in recent years and are extremely capable of handling many complex ML use cases now, with minimal human intervention. However, you need to be careful about being overly dependent on AutoML. It is not a substitute for a deep understanding of machine learning and data science but, rather, a tool that can help make these processes more efficient and accessible. AutoML tools can be risky when used by someone who does not understand the fundamentals of machine learning because they can generate seemingly high-performing models, while still suffering from common issues such as data leakages and overfitting. So, now that we have a basic understanding of the concept of AutoML as it relates to model development, let’s look at the AutoML features available in <span class="No-Break">Vertex AI.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Vertex AI AutoML</h1>
			<p>AutoML tools in Google Cloud <a id="_idIndexMarker210"/>have existed a lot longer than Vertex AI, which was launched to primarily unify most of the separate ML offerings existing in GCP. GCP AutoML makes use of models such as NASNet and constantly benefits from the AI research happening in other divisions of the Alphabet teams, such as Google DeepMind. Few of the interesting papers on the topic are <span class="No-Break">listed below:</span></p>
			<ul>
				<li><em class="italic">Learning Transferable Architectures for Scalable Image </em><span class="No-Break"><em class="italic">Recognition</em></span><span class="No-Break">. </span><a href="https://arxiv.org/pdf/1707.07012.pdf"><span class="No-Break">https://arxiv.org/pdf/1707.07012.pdf</span></a></li>
				<li><em class="italic">Regularized Evolution for Image Classifier Architecture </em><span class="No-Break"><em class="italic">Search</em></span><span class="No-Break">. </span><a href="https://arxiv.org/pdf/1802.01548.pdf"><span class="No-Break">https://arxiv.org/pdf/1802.01548.pdf</span></a></li>
				<li><em class="italic">Large-Scale Evolution of Image </em><span class="No-Break"><em class="italic">Classifiers</em></span><span class="No-Break">. </span><a href="https://arxiv.org/pdf/1703.01041.pdf"><span class="No-Break">https://arxiv.org/pdf/1703.01041.pdf</span></a></li>
			</ul>
			<p>The use cases supported by Vertex AI AutoML are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Tabular data:</span><ul><li>Classification (an example is covered in <span class="No-Break">this chapter)</span></li><li><span class="No-Break">Regression</span></li><li>Time <span class="No-Break">series forecasting</span></li></ul></li>
				<li><span class="No-Break">Image data:</span><ul><li>Image classification (an example is covered in <a href="B17792_08.xhtml#_idTextAnchor102"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">)</span></li><li><span class="No-Break">Object </span><span class="No-Break"><a id="_idIndexMarker211"/></span><span class="No-Break">detection</span></li></ul></li>
				<li><span class="No-Break">Natural language:</span><ul><li><span class="No-Break">Text classification</span></li><li><span class="No-Break">Entity extraction</span></li><li><span class="No-Break">Sentiment analysis</span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>How to create a Vertex AI AutoML model using tabular data</h2>
			<p>In the following use case, we <a id="_idIndexMarker212"/>will walk you through the steps of building a classification model, using a public dataset containing hotel reservation data. The <a id="_idIndexMarker213"/>model’s objective will be to predict the probability of a particular hotel reservation being canceled by the customer, helping the hotel to better plan around future room occupancy and possibly allow for overbooking in the hotel on dates where they expect a high number <span class="No-Break">of cancellations:</span></p>
			<ul>
				<li>The hotel reservation dataset can be accessed <span class="No-Break">here: </span><a href="https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand"><span class="No-Break">https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand</span></a></li>
				<li>You can download the data from the GitHub repository accompanying this <span class="No-Break">book: </span><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI</span></a></li>
			</ul>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Importing data to use with Vertex AI AutoML</h1>
			<p>The first step when <a id="_idIndexMarker214"/>planning to use the Vertex AI AutoML feature is to import the data you plan to use to train as Vertex <span class="No-Break">AI datasets:</span></p>
			<ol>
				<li>Navigate to <strong class="bold">Vertex AI</strong> | <strong class="bold">Datasets</strong> within the Google Cloud console, and click <strong class="bold">Create</strong> to start creating a new Vertex <span class="No-Break">AI dataset.</span></li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B17792_05_1.jpg" alt="Figure 5.1 – Creating a Vertex AI dataset" width="974" height="427"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Creating a Vertex AI dataset</p>
			<ol>
				<li value="2">Type in the name of<a id="_idIndexMarker215"/> the dataset, select <strong class="bold">Tabular</strong> as the data type, choose <strong class="bold">Regression/classification</strong>, and then <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CREATE</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B17792_05_2.jpg" alt="Figure 5.2 – Selecting a dataset type and model objective" width="826" height="497"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Selecting a dataset type and model objective</p>
			<ol>
				<li value="3">Upload the file <a id="_idIndexMarker216"/>named <strong class="source-inline">hotel_reservation_data.csv</strong> that you previously downloaded from the <span class="No-Break">GitHub repository.</span></li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B17792_05_3.jpg" alt="" role="presentation" width="724" height="314"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Specifying a data source</p>
			<ol>
				<li value="4">Enter a path to the<a id="_idIndexMarker217"/> GCS location where you would like to store the imported file. If you have not created a GCS bucket before, click on <strong class="bold">Browse</strong> and type in a name for the storage bucket you want to create. In subsequent prompts, pick the <strong class="bold">Location</strong> type as <strong class="bold">Region and Region</strong> for <strong class="source-inline">us-central1</strong>. For all other prompts, you can leave the default options already selected (<span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">It’s important to have consistency in the location of the resources you create on GCP, so throughout this book, we will try to use <strong class="source-inline">us-central1</strong> as <span class="No-Break">the location.</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B17792_05_4.jpg" alt="Figure 5.4 – Selecting cloud storage location" width="408" height="657"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Selecting cloud storage location</p>
			<ol>
				<li value="5">Next, create a<a id="_idIndexMarker218"/> folder within the bucket where you want the imported file to be stored. Click the <strong class="bold">Folder+</strong> sign at the top right and then provide a name for the folder. Then, click <strong class="bold">CREATE</strong>. Finally, highlight the folder that was just created and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Select</strong></span><span class="No-Break">.</span><div id="_idContainer053" class="IMG---Figure"><img src="image/B17792_05_5.jpg" alt="Figure 5.5 – Creating a folder to store the dataset" width="1510" height="1012"/></div></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Creating a folder to store the dataset</p>
			<ol>
				<li value="6">Once the bucket and folder path have been selected, click <strong class="bold">Continue</strong> at the bottom of the screen, which will start the import of the CSV file into the Vertex <span class="No-Break">AI dataset.</span></li>
				<li>Once the data import is<a id="_idIndexMarker219"/> completed, you will be taken to the <strong class="bold">Analyze</strong> tab. If you navigate away from the screen, you can always go back by following the <strong class="bold">Vertex AI</strong> &gt; <strong class="bold">Datasets</strong> &gt; <strong class="source-inline">hotel_cancellation_prediction</strong> or &lt;<em class="italic">whatever name you specified for your dataset</em>&gt; path. Here, all the feature statistics will be blank. To generate these, you can click <strong class="bold">Generate Statistics</strong>, which will start the process of analyzing the feature data and calculating <span class="No-Break">detailed statistics.</span></li>
			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B17792_05_6.jpg" alt="" role="presentation" width="659" height="432"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Generating statistics for the dataset</p>
			<ol>
				<li value="8">Once this process is<a id="_idIndexMarker220"/> completed, you can click on a specific feature to see further details, such as <span class="No-Break">the following:</span><ul><li>The distinct <span class="No-Break">value count</span></li><li>The missing percentage of data in <span class="No-Break">the field</span></li><li>The feature <span class="No-Break">value distribution</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B17792_05_7.jpg" alt="Figure 5.7 – Analyzing key statistics for the dataset" width="470" height="869"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Analyzing key statistics for the dataset</p>
			<p>The above screenshot shows the graphs explaining the distribution of feature/field <span class="No-Break">titled ‘</span><span class="No-Break"><strong class="source-inline">market_segment</strong></span><span class="No-Break">’.</span></p>
			<p>Now let’s look at how <a id="_idIndexMarker221"/>we will train an <strong class="bold">AutoML Classification</strong> model using the dataset <span class="No-Break">discussed above.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Training the AutoML model for tabular/structured data</h2>
			<p>Now, let’s look at how you can<a id="_idIndexMarker222"/> use Vertex AI AutoML to train ML models on <span class="No-Break">tabular data:</span></p>
			<ol>
				<li>Within the <strong class="bold">ANALYZE</strong> tab of the dataset that you want to use to train a model, click <strong class="bold">TRAIN NEW MODEL</strong> | <span class="No-Break"><strong class="bold">Other</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B17792_05_8.jpg" alt="Figure 5.8 – Training a new model" width="897" height="462"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Training a new model</p>
			<ol>
				<li value="2">Since we are trying to classify the reservations based on their cancellation likelihood, pick the model objective as <strong class="bold">Classification</strong>. Pick <strong class="bold">AutoML</strong> as the training method, which uses the codeless automated training option. Then, <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B17792_05_9.jpg" alt="Figure 5.9 – Specifying the model type" width="789" height="539"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Specifying the model type</p>
			<ol>
				<li value="3">On the following <strong class="bold">Model Details</strong> screen, add <span class="No-Break">these details:</span><ul><li>Since it’s the first<a id="_idIndexMarker223"/> time we are training a model on this dataset, pick <span class="No-Break"><strong class="bold">New Model</strong></span><span class="No-Break">.</span></li><li>Type in what you want to call <span class="No-Break">the model.</span></li><li>Select a target column so that AutoML knows which column to use as the prediction target. In our dataset, the column is <span class="No-Break">titled </span><span class="No-Break"><strong class="source-inline">is_canceled</strong></span><span class="No-Break">.</span></li><li>If you want the <strong class="bold">Test</strong> results to be exported to BigQuery for further analysis, select the <strong class="bold">Export test dataset to big query</strong> option, and provide a BigQuery table path where these results need to be stored. Vertex AI will create the table after the <span class="No-Break">training run.</span></li><li>If you want Vertex AI to randomly split data into <strong class="bold">Training</strong>, <strong class="bold">Validation</strong>, and <strong class="bold">Test</strong> datasets, leave the default option, <strong class="bold">Random</strong>, selected. If you want to control the assignments of samples to the <strong class="bold">Training</strong>, <strong class="bold">Validation</strong>, and <strong class="bold">Test</strong> datasets, select the <strong class="bold">Manual</strong> option. In this case, you will need to provide a column where <strong class="bold">Train</strong>/<strong class="bold">Validation</strong>/<strong class="bold">Test</strong> assignments <span class="No-Break">are provided.</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B17792_05_10.jpg" alt="Figure 5.10 – Configuring the model training options" width="922" height="750"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Configuring the model training options</p>
			<ol>
				<li value="4">On the following <strong class="bold">Training Options</strong> screen, verify that only the fields you want to be used for training are included in the current training run. Any unnecessary features should be<a id="_idIndexMarker224"/> removed, by selecting them and then clicking <strong class="bold">Exclude from training</strong> from the <strong class="bold">Inclusivity</strong> dropdown at <span class="No-Break">the top.</span><p class="list-inset">As shown in the following screenshot, remove the following two features from the training dataset we <span class="No-Break">are using:</span></p><ul><li><span class="No-Break"><strong class="source-inline">reservation_status</strong></span></li><li><span class="No-Break"><strong class="source-inline">reservation_status_date</strong></span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B17792_05_11.jpg" alt="Figure 5.11 – Selecting the features to be removed" width="601" height="407"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Selecting the features to be removed</p>
			<p class="list-inset">Also, check that the Vertex AI has defaulted to the correct transformation types for each field. To be used for model training, tabular data must undergo a transformation process that is specific to each data feature. This transformation process indicates the<a id="_idIndexMarker225"/> function of a particular data feature. The supported types of transformations are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Categorical</strong>: When training a model with a categorical feature in Vertex AI, the feature undergoes data transformations that help with the model training process. Vertex AI applies the following transformations to the feature and uses any that provide <span class="No-Break">useful information:</span><ul><li>The categorical string remains unchanged, with no modifications made to case, punctuation, spelling, tense, and <span class="No-Break">other attributes.</span></li><li>The category name is converted into a dictionary lookup index, and an embedding is generated for <span class="No-Break">each index.</span></li><li>Categories that appear less than five times in the training dataset are considered the <strong class="bold">unknown</strong> category. The <strong class="bold">unknown</strong> category is assigned a unique lookup index, and an embedding is generated for this category <span class="No-Break">as well.</span></li></ul></li>
				<li><strong class="bold">Text</strong>: A feature that has undergone a text transformation is treated as freeform text and is usually made up of text tokens. The text is tokenized into words, and 1-grams and 2-grams are generated from those words. Each <em class="italic">n</em>-gram is then converted into a dictionary lookup index, and an embedding is generated for each index. Finally, the embeddings of all the elements are combined into a single embedding using <span class="No-Break">the mean.</span></li>
				<li><strong class="bold">Numeric</strong>: The following data transformations are applied to the feature, and any that provide useful information are used <span class="No-Break">for training:</span><ul><li>Rows with invalid <a id="_idIndexMarker226"/>numerical inputs, such as a string that cannot be parsed to <strong class="source-inline">float32</strong>, are not included in the training and <span class="No-Break">prediction process.</span></li><li>The value is converted <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">float32</strong></span><span class="No-Break">.</span></li><li>The <em class="italic">z</em>-score of the value <span class="No-Break">is calculated.</span></li><li>The value is bucketed based on quantiles, with a bucket size <span class="No-Break">of 100.</span></li><li>The log of (<strong class="source-inline">value+1</strong>) is calculated when the value is greater than or equal to 0. If the value is less than 0, this transformation is not applied, and the value is considered a <span class="No-Break">missing value.</span></li><li>The <em class="italic">z</em>-score of the log of (<strong class="source-inline">value+1</strong>) is calculated when the value is greater than or equal to 0. If the value is less than 0, this transformation is not applied, and the value is considered a <span class="No-Break">missing value.</span></li><li>A Boolean value is assigned to indicate whether the value <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">null</strong></span><span class="No-Break">.</span></li></ul></li>
				<li><strong class="bold">Timestamp</strong>: The following data transformations are applied to the feature, and any that provide useful information are used <span class="No-Break">for training:</span><ul><li>The year, month, day, and weekday of the timestamp are determined and treated as <span class="No-Break">categorical columns</span></li><li>Invalid numerical values, such as values outside the typical timestamp range or extreme values, are not removed or <span class="No-Break">treated differently</span></li><li>The transformations for numerical columns are applied to <span class="No-Break">the feature</span></li><li>Rows with invalid timestamp inputs, such as an invalid timestamp string, are not included in the training and <span class="No-Break">prediction process</span></li></ul></li>
			</ul>
			<ol>
				<li value="5">If you want to feed <a id="_idIndexMarker227"/>the model additional weights to rebalance the dataset, you can provide an additional column that contains weights assigned to each <span class="No-Break">data sample.</span></li>
				<li>As shown in the following screenshot, by default, the optimization objective is set to maximize the AUC <span class="No-Break">ROC curve.</span></li>
			</ol>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B17792_05_12.jpg" alt="Figure 5.12 – Selecting the optimization objective" width="545" height="410"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Selecting the optimization objective</p>
			<p class="list-inset">Although not required, you have the option to change the <span class="No-Break">optimization objective.</span></p>
			<p class="list-inset">For classification problems, there are five <span class="No-Break">different objectives:</span></p>
			<ul>
				<li><strong class="bold">AUC ROC</strong>: This objective in <a id="_idIndexMarker228"/>AutoML maximizes the area under the receiver operating characteristic curve. This is the default selection for binary classification and can be selected to distinguish <span class="No-Break">between classes.</span></li>
				<li><strong class="bold">Log loss</strong>: This aims to minimize the log loss for the model. It is used when the goal is to predict <a id="_idIndexMarker229"/>probabilities as accurately as possible, and it is the only supported objective in AutoML for multi-class <span class="No-Break">classification models.</span></li>
				<li><strong class="bold">AUC PR</strong>: This objective aims to <a id="_idIndexMarker230"/>maximize the area under the precision-recall curve. It optimizes<a id="_idIndexMarker231"/> results for predictions for the less <span class="No-Break">common class.</span></li>
				<li><strong class="bold">Precision at recall</strong>: This aims to<a id="_idIndexMarker232"/> maximize precision at a specific <span class="No-Break">recall value.</span></li>
				<li><strong class="bold">Recall at precision</strong>: This aims to <a id="_idIndexMarker233"/>maximize recall at a specific <span class="No-Break">precision value.</span></li>
			</ul>
			<p class="list-inset">For regression problems, there are three <span class="No-Break">different objectives:</span></p>
			<ul>
				<li><strong class="bold">RMSE</strong>: This objective <a id="_idIndexMarker234"/>aims to minimize the root mean squared error. It captures more extreme <span class="No-Break">values accurately.</span></li>
				<li><strong class="bold">MAE</strong>: This objective aims to <a id="_idIndexMarker235"/>minimize the mean absolute error. It views extreme values as outliers with less impact on <span class="No-Break">the model.</span></li>
				<li><strong class="bold">RMSLE</strong>: This objective aims to <a id="_idIndexMarker236"/>minimize the root mean squared log error. It penalizes errors in relative size rather than absolute value and is especially helpful when both predicted and actual values are <span class="No-Break">quite large.</span></li>
			</ul>
			<ol>
				<li value="7">Provide the maximum time you want the training to run. The minimum needs to be one hour. During the first run, it’s hard to know how long the model will take to reach the highest possible accuracy, and you might have to experiment with this setting a little for new datasets and <span class="No-Break">model types.</span></li>
			</ol>
			<p class="callout-heading">Best practice</p>
			<p class="callout">For most small datasets (&lt; 1 million data samples and &lt; 20 features), two to four hours is a good starting point. If the <strong class="bold">Enable early stopping</strong> option is on, regardless of the number of hours you have budgeted for, the training will stop once AutoML determines that no further improvement in the model objective is being achieved with further rounds <span class="No-Break">of training.</span></p>
			<ol>
				<li value="8">Lastly, as shown in the following screenshot, click <strong class="bold">START TRAINING</strong> to kick off the training process. In a few hours, you will have a shiny new model ready <span class="No-Break">to evaluate.</span></li>
			</ol>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B17792_05_13.jpg" alt="Figure 5.13 – Kicking off model training" width="676" height="221"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Kicking off model training</p>
			<p>Once the model training<a id="_idIndexMarker237"/> is completed, let’s see how you can evaluate the model created <span class="No-Break">by AutoML.</span></p>
			<h3>Evaluating the trained model</h3>
			<p>Every time you<a id="_idIndexMarker238"/> train a machine learning model, it’s crucial to evaluate its performance to determine whether it’s reliable for real-world applications. Model evaluation metrics are calculated based on the model’s performance against a portion of the dataset that was not used during training, referred to as the test dataset. This evaluation provides insight into how the model generalizes to new, unseen data and helps identify any issues or areas <span class="No-Break">for improvement:</span></p>
			<ol>
				<li>Go to <strong class="bold">Model Registry</strong> in the Vertex AI, and as shown in the following screenshot, locate the model you just trained. Click on the model, and then on the next screen, click the version of the model you <span class="No-Break">just trained.</span></li>
			</ol>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B17792_05_14.jpg" alt="Figure 5.14 – Model Registry" width="626" height="611"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Model Registry</p>
			<ol>
				<li value="2">On the <strong class="bold">EVALUATE</strong> tab, you <a id="_idIndexMarker239"/>will see the details of the test results generated by AutoML, by using the test dataset provided <span class="No-Break">during training.</span></li>
			</ol>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B17792_05_15.jpg" alt="Figure 5.15 – The model evaluation metrics" width="1650" height="636"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – The model evaluation metrics</p>
			<p>Here is the key information typically shown in the Vertex AI <strong class="bold">Evaluate</strong> tab depending on the <span class="No-Break">model type:</span></p>
			<ul>
				<li>Confidence threshold: When working with classification models, predictions are given a confidence score to indicate the level of certainty that the predicted class is correct. The score is a numeric assessment that determines how sure the model is that the prediction is accurate. For instance, consider a machine learning model that predicts whether a customer will cancel their <span class="No-Break">hotel reservation:</span><ul><li>To convert the score into <a id="_idIndexMarker240"/>a binary decision, a score threshold is used. The score threshold is the value at which the model says, “<em class="italic">Yes, the confidence score is high enough to conclude that the customer will cancel their reservation</em>” or “<em class="italic">No, the confidence score is not high enough to predict that the customer will cancel their reservation.</em>” The score threshold should be based on a specific use case, and a low score threshold increases the risk <span class="No-Break">of misclassification.</span></li></ul></li>
				<li>Precision and recall: In classification models, precision and recall are essential metrics to assess and summarize how well the model captures information and <span class="No-Break">avoid errors:</span><ul><li>Precision: Precision answers the question, “<em class="italic">Of all the predicted hotel reservation cancellations, how many were actually canceled?</em>” It measures the accuracy of the model’s positive predictions – that is, the percentage of true positives out of all <span class="No-Break">predicted positives.</span></li><li>Recall: Recall answers the question, “<em class="italic">Of all the canceled hotel reservations, how many did the model correctly predict?</em>” It measures the model’s ability to identify true positive cases – that is, the percentage of true positives out of all <span class="No-Break">actual positives.</span></li></ul></li>
			</ul>
			<p>Depending on the use case, you may need to optimize for either precision or recall. For instance, if you want to minimize the number of false negatives (i.e., hotel reservations that were canceled but were not identified by the model), then you should aim for a high recall. If you want to reduce the number of false positives (i.e., hotel reservations that were not canceled but were predicted to be), then you should aim for <span class="No-Break">high precision.</span></p>
			<p>Apart from precision and recall, there are several other classification metrics that are useful to evaluate the performance of a machine learning model. Here are some of the most commonly <span class="No-Break">used metrics:</span></p>
			<ul>
				<li><strong class="bold">AUC PR</strong>: The area under the <strong class="bold">precision-recall</strong> (<strong class="bold">PR</strong>) curve<a id="_idIndexMarker241"/> measures the trade-off between precision <a id="_idIndexMarker242"/>and recalls across various score thresholds. The <strong class="bold">AUC PR</strong> ranges from zero to one, where a higher value indicates a <span class="No-Break">higher-quality model.</span></li>
				<li><strong class="bold">AUC ROC</strong>: The <a id="_idIndexMarker243"/>area under the <strong class="bold">receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) curve measures the model’s<a id="_idIndexMarker244"/> performance across all possible score thresholds. The AUC ROC also ranges<a id="_idIndexMarker245"/> from zero to one, where a higher value indicates a <span class="No-Break">higher-quality model.</span></li>
				<li><strong class="bold">Accuracy</strong>: The fraction of classification predictions produced by the model that was correct. This is a simple and intuitive metric that provides an overall measure of the <span class="No-Break">model’s performance.</span></li>
				<li><strong class="bold">Log loss</strong>: Log loss, also known as cross-entropy, serves as a crucial metric in assessing the alignment between a model’s predictions and the actual target values. This metric quantifies the efficacy of the model’s performance by measuring how closely its predictions match the real-world outcomes. With a scale spanning from zero to infinity, a lower log loss signifies a higher-quality model, showcasing its ability to make more accurate and <span class="No-Break">confident predictions.</span></li>
				<li><strong class="bold">F1 score</strong>: When seeking a balance between precision and recall, particularly in scenarios with imbalanced class distributions, the F1 score emerges as a valuable metric. This score represents the harmonic mean of precision and recall, operating on a scale ranging from zero to one. A higher F1 score denotes a model of superior quality, signifying its capability to achieve both precision and recall effectively, even in challenging class <span class="No-Break">distribution scenarios.</span></li>
			</ul>
			<p>By using these metrics, you can gain a more comprehensive understanding of your model’s performance and make more informed decisions about how to <span class="No-Break">improve it.</span></p>
			<p>The <strong class="bold">Evaluate</strong> tab also showcases two additional useful pieces <span class="No-Break">of information:</span></p>
			<ul>
				<li><strong class="bold">Confusion matrix</strong>: A confusion matrix is a visualization tool that shows the frequency with which a model correctly predicts a result, and for the instances when it incorrectly predicts a result, the matrix shows what the model predicted instead. The confusion<a id="_idIndexMarker246"/> matrix is a helpful way to understand where the model may be “confusing” two results, and to diagnose the accuracy and performance of <span class="No-Break">the model.</span></li>
			</ul>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B17792_05_16.jpg" alt="Figure 5.16 – The confusion matrix" width="706" height="564"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – The confusion matrix</p>
			<ul>
				<li><strong class="bold">Feature importance</strong>: Model feature importance is expressed as a percentage for each feature, with a higher percentage indicating a stronger impact on model training. By looking at the feature importance values, we can gain a better understanding of the relative importance of different features in the model and how they contribute to the accuracy of predictions. The following screenshot shows the feature importance graph for the model we <span class="No-Break">just trained.</span></li>
			</ul>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B17792_05_17.jpg" alt="Figure 5.17 – Feature importance" width="618" height="535"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Feature importance</p>
			<p>So, we have trained a model and evaluated its key performance indicators. Now, let’s look at how to deploy it and <a id="_idIndexMarker247"/>use it to <span class="No-Break">generate predictions.</span></p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Generating predictions using the recently trained model</h1>
			<p>Once the AutoML model is trained, you can generate predictions using one of the following <span class="No-Break">two methods:</span></p>
			<ul>
				<li><strong class="bold">Batch predictions</strong>: As the <a id="_idIndexMarker248"/>name suggests, batch <a id="_idIndexMarker249"/>predictions are asynchronous predictions generated for a batch of inputs. This is used when a real-time response is unnecessary and you want to submit a single request to process many data instances. In Vertex AI, a request for batch predictions can be submitted directly to a model residing in the<a id="_idIndexMarker250"/> Vertex <a id="_idIndexMarker251"/>AI Model registry, without the need to deploy it on <span class="No-Break">an endpoint.</span></li>
				<li><strong class="bold">Online predictions</strong>: If you need real-time<a id="_idIndexMarker252"/> inference – for example, when responding to application input – you need to use the Vertex AI online prediction option. To use online prediction, you must first deploy the model to an endpoint. This step provisions infrastructure resources and deploys prediction serving mechanism using the specified model, enabling it to serve predictions with low latency. The steps to deploy the model are shown in the <span class="No-Break">following section.</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">Models deployed on Vertex AI endpoints continuously incur costs, regardless of their usage. This could add up quickly, depending on the type of underlying VM types, especially if you are <span class="No-Break">using GPUs.</span></p>
			<p>In the next section, we will deploy the ML model we trained on Vertex AI to generate <span class="No-Break">online predictions.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Deploying a model in Vertex AI</h1>
			<p>Now, let us walk you<a id="_idIndexMarker253"/> through the steps of deploying the trained model on Vertex AI to enable <span class="No-Break">real-time predictions:</span></p>
			<ol>
				<li>Go to <strong class="bold">Model Registry</strong>, click on the model and then the model version you want to deploy, and on the <strong class="bold">DEPLOY &amp; TEST</strong> tab, click <strong class="bold">DEPLOY </strong><span class="No-Break"><strong class="bold">TO ENDPOINT</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B17792_05_18.jpg" alt="Figure 5.18 – Initiating model deployment" width="607" height="755"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – Initiating model deployment</p>
			<ol>
				<li value="2">Type in the desired <a id="_idIndexMarker254"/>name of the API endpoint being created and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">CONTINUE</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B17792_05_19.jpg" alt="Figure 5.19 – Creating a model endpoint" width="786" height="443"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.19 – Creating a model endpoint</p>
			<ol>
				<li value="3">You can leave all default options unchanged for quick test deployment, but these are the settings you need <span class="No-Break">to understand:</span><ul><li><strong class="bold">Traffic split</strong>: If multiple versions of the model are deployed on the same API endpoint, this option allows users to define what percentage of total traffic is allocated to a specific version. For example, when deploying a new model, you might want only 2% of the overall incoming data to be routed to the new model so that it can be tested, while continuing to send the rest of the 98% of data<a id="_idIndexMarker255"/> to the existing version of the model. When deploying a model to a new endpoint, you need to leave this value at 100%, since there is no other model to split <span class="No-Break">the workload.</span></li><li><strong class="bold">Minimum number of compute nodes</strong>: This is the bare minimum number of compute nodes always available to handle the inference requests. Even if there are no requests being handled, these nodes will constantly be deployed and <span class="No-Break">incur charges.</span></li><li><strong class="bold">Maximum number of compute nodes</strong>: During autoscaling, as the number of incoming requests increases, Vertex AI will automatically increase the number of deployed nodes up to <span class="No-Break">this number.</span></li><li><strong class="bold">Machine type</strong>: This relates to the configuration of the node supporting the online inference in terms of CPUs <span class="No-Break">and memory.</span></li><li><strong class="bold">Explainability options</strong>: If you want the <strong class="bold">Feature importance</strong> value to be generated with every inference, select <span class="No-Break">this option.</span></li><li>Click <strong class="bold">CONTINUE</strong> once you have made the <span class="No-Break">desired changes.</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B17792_05_20.jpg" alt="Figure 5.20 – Specifying the model deployment configurations" width="423" height="622"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.20 – Specifying the model deployment configurations</p>
			<ol>
				<li value="4">On the <strong class="bold">Model monitoring</strong> page, turn <a id="_idIndexMarker256"/>off monitoring for now. We will cover model monitoring in detail in <a href="B17792_11.xhtml#_idTextAnchor153"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><em class="italic">, MLOps Governance with Vertex AI</em>. Click <strong class="bold">DEPLOY</strong>. This will start the endpoint creation process for new endpoints, followed by the model <span class="No-Break">deployment process.</span></li>
			</ol>
			<p>Once the model <a id="_idIndexMarker257"/>deployment is complete we can use the model to generate real time predictions as <span class="No-Break">shown below.</span></p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Generating predictions</h1>
			<p>Once the model deployment is complete, you will see the endpoint listed in the <strong class="bold">DEPLOY &amp; TEST</strong> tab. Underneath that, there will be a <strong class="bold">Test your model</strong> table, listing all feature values required to <a id="_idIndexMarker258"/>generate predictions. Fields will already have starting values based on the data used for AutoML training, but you can type in different values and click <strong class="bold">Predict</strong> to <span class="No-Break">generate predictions.</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B17792_05_21.jpg" alt="Figure 5.21 – Testing a deployed model" width="1015" height="1423"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.21 – Testing a deployed model</p>
			<p>Once the model churns through the provided feature values, it will return the confidence score <a id="_idIndexMarker259"/>associated with each label. The one with the highest confidence score is the <span class="No-Break">predicted label.</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B17792_05_22.jpg" alt="Figure 5.22 – The prediction result" width="427" height="250"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.22 – The prediction result</p>
			<p>Now let’s look at the options available to developers to use the Vertex AI <span class="No-Break">models programmatically.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Generating predictions programmatically</h2>
			<p>To access the Vertex AI<a id="_idIndexMarker260"/> prediction service, you can work with the Vertex AI SDK for Python, or client libraries available for Python, Java, and Node.js. Check out <em class="italic">Install the Vertex AI client libraries</em> at <a href="https://cloud.google.com/vertex-ai/docs/start/client-libraries">https://cloud.google.com/vertex-ai/docs/start/client-libraries</a> to learn how to install the client library for Java or Node.js. You can find a large number of  Vertex AI  sample notebooks, both community generate and those published by Google Cloud team <span class="No-Break">at </span><a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples"><span class="No-Break">https://github.com/GoogleCloudPlatform/vertex-ai-samples</span></a><span class="No-Break">.</span></p>
			<p>If your preferred programming language doesn’t have a client library, you can make use of the Vertex AI REST <span class="No-Break">API instead.</span></p>
			<h3>Submitting prediction requests using the REST API</h3>
			<p>Although the <strong class="bold">DEPLOY &amp; TEST</strong> tab within <a id="_idIndexMarker261"/>Vertex AI makes it easy to test a model with a few samples, in most typical use cases, the model’s API endpoint will be accessed programmatically by sending the input samples to the API <a id="_idIndexMarker262"/>and receiving a JSON response from it. Vertex AI makes it easier to get started by generating sample code, as shown in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B17792_05_23.jpg" alt="Figure 5.23 – A sample REST request" width="787" height="1015"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.23 – A sample REST request</p>
			<p>We will use GCP’s native Cloud Shell to submit a sample prediction request to the model deployed in Vertex AI. The<a id="_idIndexMarker263"/> first step is to open the cloud shell, as shown in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17792_05_24.jpg" alt="Figure 5.24 – Opening the cloud shell" width="1210" height="437"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.24 – Opening the cloud shell</p>
			<p>Once Cloud Shell is open, upload the JSON file containing the input data samples to the Cloud Shell environment. We will submit this JSON payload as part of the request to <span class="No-Break">the API.</span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B17792_05_25.jpg" alt="Figure 5.25 – Uploading the JSON payload" width="1044" height="548"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.25 – Uploading the JSON payload</p>
			<p>In Cloud Shell, copy and paste the following lines to set the environment variables after replacing <strong class="source-inline">enpoint_id</strong>, <strong class="source-inline">project_id</strong>, and the path to the input JSON file <span class="No-Break">you uploaded.</span></p>
			<p>Note that you need to replace <strong class="source-inline">endpoint-id</strong>, <strong class="source-inline">project-id</strong>, and the local path of the JSON file you just uploaded from your respective <span class="No-Break">GCP environment:</span></p>
			<pre class="source-code">
ENDPOINT_ID="&lt;your-endpoint-id"
PROJECT_ID="&lt;your-project-id&gt;"
INPUT_DATA_FILE="&lt;path-to-input-json-file.json&gt;"</pre>			<p>Now, run the following command in <a id="_idIndexMarker264"/>Cloud Shell (you can copy and paste the command from the <strong class="bold">Sample Request</strong> screen we <span class="No-Break">discussed previously:</span></p>
			<pre class="source-code">
curl \
-X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict \
-d "@${INPUT_DATA_FILE}"</pre>			<p>Once the prediction request is sent to Vertex AI, it will run the data through the model hosted on the endpoint, and if everything goes well, you will receive a JSON response from <span class="No-Break">the API.</span></p>
			<p>Here’s the response received from the Vertex <span class="No-Break">AI endpoint:</span></p>
			<pre class="source-code">
  "predictions": [
    {"scores": [
        0.89472222328186035,
        0.1052777245640755],
      "classes": ["0","1"]},
    {"classes": ["0","1"],
      "scores": [
        0.88255965709686279,
        0.117440328001976]}],
      "deployedModelId": "3558930023110934528",
      "model": "projects/217932574390/locations/us-central1/models/6845069012347387904",
      "modelDisplayName": "hotel_cancellation_prediction,"
      "modelVersionId": "1"}</pre>			<p>The response contains three main parts – <strong class="source-inline">predictions</strong>, <strong class="source-inline">deployedModelId</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">model</strong></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">predictions</strong> section contains the actual predictions made by the model. In this case, since we submitted two samples for prediction, two predictions have been made, each with a set of scores and predicted classes. The scores represent the model’s confidence in each class, and<a id="_idIndexMarker265"/> the classes represent the label or category that the model has predicted for <span class="No-Break">each prediction.</span></p>
			<ul>
				<li>The first prediction has scores of 0.8947 for class <strong class="source-inline">"0"</strong> and 0.1053 for class <strong class="source-inline">"1"</strong>, which suggests that the model is highly confident that the first prediction belongs to <span class="No-Break">class </span><span class="No-Break"><strong class="source-inline">"0"</strong></span></li>
				<li>The second prediction has scores of 0.8826 for class <strong class="source-inline">"0"</strong> and 0.1174 for class <strong class="source-inline">"1"</strong>, which also suggests that the model is confident that the second prediction belongs to class <strong class="source-inline">"0"</strong> but is less certain than it was about the <span class="No-Break">first prediction</span></li>
			</ul>
			<p><strong class="source-inline">deployedModelId</strong> is a unique identifier for the deployed model. This is useful to keep track of different versions of the same model that may be deployed at different times, or in <span class="No-Break">different locations.</span></p>
			<p>The <strong class="source-inline">model</strong> section provides information about the model that was used to make the predictions. It includes the model’s ID, location, and display name, as well as the version of the model that was used for these predictions (in this case, version 1). This information can help debug and troubleshoot if there are issues with the model or <span class="No-Break">its predictions.</span></p>
			<p>In the preceding end-to-end example, we walked you through building an AutoML classification model for tabular/structured data. In the subsequent chapters, we will also provide examples of how to build <a id="_idIndexMarker266"/>AutoML Vision models using image data (<a href="B17792_16.xhtml#_idTextAnchor233"><span class="No-Break"><em class="italic">Chapter 16</em></span></a>) and examples of AutoML NLP models with text data (<a href="B17792_17.xhtml#_idTextAnchor282"><span class="No-Break"><em class="italic">Chapter 17</em></span></a><span class="No-Break">).</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Summary</h1>
			<p>In this chapter, we discussed the transformative influence of no-code platforms in democratizing AI and machine learning, allowing individuals without specialized coding knowledge to develop sophisticated models. You also learned about the no-code machine learning options available through Google Cloud’s Vertex AI toolset, and we explored the steps to build, evaluate, deploy, and generate predictions with an AutoML model. The chapter serves as a practical guide for users interested in leveraging Google Cloud’s machine learning capabilities without <span class="No-Break">writing code.</span></p>
			<p>In the next chapter, we will explore some low-code options to build a machine learning model. Although that would require some additional technical skills compared to the no-code options discussed in this book, it would also give you more fine-grained control over how your model <span class="No-Break">is built.</span></p>
		</div>
	</div>
</div>
</body></html>