<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Important Elements in Machine Learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to discuss some important elements and approaches which span through all machine learning topics and also create a philosophical foundation for many common techniques. First of all, it's useful to understand the mathematical foundation of data formats and prediction functions. In most algorithms, these concepts are treated in different ways, but the goal is always the same. More recent techniques, such as deep learning, extensively use energy/loss functions, just like the one described in this chapter, and even if there are slight differences, a good machine learning result is normally associated with the choice of the best loss function and the usage of the right algorithm to minimize it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data formats</h1>
                </header>
            
            <article>
                
<p>In a supervised learning problem, there will always be a dataset, defined as a finite set of real vectors with <em>m</em> features each:</p>
<div class="CDPAlignCenter CDPAlign"><img height="47" width="262" src="assets/cf7ee9ce-5462-4e32-948e-4b45f311dd60.png"/></div>
<p class="CDPAlignLeft CDPAlign">Considering that our approach is always probabilistic, we need to consider each <em>X</em> as drawn from a statistical multivariate distribution <em>D</em>. For our purposes, it's also useful to add a very important condition upon the whole dataset <em>X</em>: we expect all samples to be <strong>independent and</strong> <strong>identically distributed </strong>(<strong>i.i.d</strong>). This means all variables belong to the same distribution <em>D,</em> and considering an arbitrary subset of <em>m</em> values, it happens that:</p>
<div class="CDPAlignCenter CDPAlign"><img height="56" width="196" src="assets/c9c6791e-c1bf-4105-8c44-fa90d01adc90.png"/></div>
<p class="CDPAlignLeft CDPAlign">The corresponding output values can be both numerical-continuous or categorical. In the first case, the process is called <strong>regression</strong>, while in the second, it is called <strong>classification</strong>. Examples of numerical outputs are:</p>
<div class="CDPAlignCenter CDPAlign"><img height="41" width="307" src="assets/de59db9e-0a89-4320-87b9-18318f790f48.png"/></div>
<p class="CDPAlignLeft CDPAlign">Categorical examples are:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="282" src="assets/294da3eb-db76-4f54-bc9a-eab0f4668bb0.png"/></div>
<p class="CDPAlignLeft CDPAlign">We define generic <strong>regressor</strong>, a vector-valued function which associates an input value to <span>a continuous output</span> and generic <strong>classifier</strong>, a vector-values function whose predicted output is categorical (discrete). If they also depend on an internal parameter vector which determines the actual instance of a generic predictor, the approach is called <strong>parametric learnin</strong><strong>g</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="86" width="328" src="assets/61b89813-adc8-4be6-889d-fbc1cbb395fa.png"/></div>
<p class="CDPAlignLeft CDPAlign">On the other hand, <strong>non-parametric learning</strong> doesn't make initial assumptions about the family of predictors (for example, defining a generic parameterized version of <em>r(...)</em> and <em>c(...))</em>. A very common non-parametric family is called <strong>instance-based learning</strong> and makes real-time predictions (without pre-computing parameter values) based on hypothesis determined only by the training samples (instance set). A simple and widespread approach adopts the concept of neighborhoods (with a fixed radius). In a classification problem, a new sample is automatically surrounded by classified training elements and the output class is determined considering the preponderant one in the neighborhood. In this book, we're going to talk about another very important algorithm family belonging to this class: <strong>kernel-based support vector machines</strong>. More examples can be found in Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson<em>.</em></p>
<p class="CDPAlignLeft CDPAlign">The internal dynamics and the interpretation of all elements are peculiar to each single algorithm, and for this reason, we prefer not to talk now about thresholds or probabilities and try to work with an abstract definition. A generic parametric training process must find the best parameter vector which minimizes the regression/classification error given a specific training dataset and it should also generate a predictor that can correctly generalize when unknown samples are provided.</p>
<p class="CDPAlignLeft CDPAlign">Another interpretation can be expressed in terms of additive noise:</p>
<div class="CDPAlignCenter CDPAlign"><img height="95" width="244" src="assets/9c69d743-ce0e-44c0-9e00-c00f90ddbcb0.png"/></div>
<p class="CDPAlignLeft CDPAlign">For our purposes, we can expect zero-mean and low-variance Gaussian noise added to a perfect prediction. A training task must increase the signal-noise ratio by optimizing the parameters. Of course, whenever such a term <span>doesn't have </span>zero mean (independently from the other <em>X</em> values), probably it means that there's a hidden trend that must be taken into account (maybe a feature that has been prematurely discarded). On the other hand, high noise variance means that <em>X</em> is dirty and its measures are not reliable.</p>
<p class="CDPAlignLeft CDPAlign">Until now we've assumed that both regression and classification operate on <em>m</em>-length vectors but produce a single value or single label (in other words, an input vector is always associated with only one output element). However, there are many strategies to handle multi-label classification and multi-output regression.</p>
<p class="CDPAlignLeft CDPAlign">In unsupervised learning, we normally only have an input set <em>X</em> with <em>m</em>-length vectors, and we define clustering function (with <em>n</em> target clusters) with the following expression:</p>
<div class="CDPAlignCenter CDPAlign"><img height="33" width="241" src="assets/14adc763-f369-4892-a91a-53ebbb98cf2f.png"/></div>
<p class="CDPAlignLeft CDPAlign">In most scikit-learn models, there is an instance variable <kbd>coef_</kbd> which contains all trained parameters. For example, in a single parameter linear regression (we're going to widely discuss it in the next chapters), the output will be:</p>
<pre><strong>&gt;&gt;&gt; model = LinearRegression()<br/>&gt;&gt;&gt; model.fit(X, Y)<br/>&gt;&gt;&gt; model.coef_<br/>array([ 9.10210898])</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiclass strategies</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">When the number of output classes is greater than one, there are two main possibilities to manage a classification problem:</p>
<ul>
<li>One-vs-all</li>
<li>One-vs-one</li>
</ul>
<p>In both cases, the choice is transparent and the output returned to the user will always be the final value or class. However, it's important to understand the different dynamics in order to optimize the model and to always pick the best alternative.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-vs-all</h1>
                </header>
            
            <article>
                
<p>This is probably the most common strategy and is widely adopted by scikit-learn for most of its algorithms. If there are <em>n</em> output classes, <em>n</em> classifiers will be trained in parallel considering there is always a separation between an actual class and the remaining ones. This approach is relatively lightweight (at most, <em>n-1</em> checks are needed to find the right class, so it has an <em>O(n)</em> complexity) and, for this reason, it's normally the default choice and there's no need for further actions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-vs-one</h1>
                </header>
            
            <article>
                
<p>The alternative to one-vs-all is training a model for each pair of classes. The complexity is no longer linear (it's <em>O(n<sup>2</sup>)</em> indeed) and the right class is determined by a majority vote. In general, this choice is more expensive and should be adopted only when a full dataset comparison is not preferable.</p>
<div class="packt_infobox">If you want to learn more about multiclass strategies implemented by scikit-learn, visit <br/>
<a href="http://scikit-learn.org/stable/modules/multiclass.html" target="_blank">http://scikit-learn.org/stable/modules/multiclass.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learnability</h1>
                </header>
            
            <article>
                
<p>A parametric model can be split into two parts: a static structure and a dynamic set of parameters. The former is determined by choice of a specific algorithm and is normally immutable (except in the cases when the model provides some re-modeling functionalities), while the latter is the objective of our optimization. Considering <em>n</em> unbounded parameters, they generate an <em>n</em>-dimensional space (imposing bounds results in a sub-space without relevant changes in our discussion) where each point, together with the immutable part of the estimator function, represents a learning hypothesis <em>H</em> (associated with a specific set of parameters):</p>
<div class="CDPAlignCenter CDPAlign"><img height="43" width="160" src="assets/12d82cf0-4495-47ea-9e82-7e28b2b3cae6.png"/></div>
<p class="CDPAlignLeft CDPAlign">The goal of a parametric learning process is to find the best hypothesis whose corresponding prediction error is minimum and the residual generalization ability is enough to avoid overfitting. In the following figure, there's an example of a dataset whose points must be classified as red (<strong>Class A</strong>) or blue (<strong>Class B</strong>). Three hypotheses are shown: the first one (the middle line starting from left) misclassifies one sample, while the lower and upper ones misclassify 13 and 23 samples respectively: </p>
<div class="CDPAlignCenter CDPAlign"><img height="268" width="442" class="image-border" src="assets/8f0af772-cd16-48a8-8d8b-927248b69f7b.png"/></div>
<p class="CDPAlignLeft CDPAlign"><span>Of course, the first hypothesis is optimal and should be selected; however, it's important to understand an essential concept which can determine a potential overfitting. Think about an <em>n</em>-dimensional binary classification problem. We say that the dataset <em>X</em> is <strong><em>linearly </em><em>separable</em></strong> (without transformations) if there exists a hyperplane which divides the space into two subspaces containing only elements belonging to the same class. Removing the constraint of linearity, we have infinite alternatives using generic hypersurfaces. However, a parametric model adopts only a family of non-periodic and approximate functions whose ability to oscillate and fit the dataset is determined (sometimes in a very complex way) by the number of parameters.</span></p>
<p class="CDPAlignLeft CDPAlign">Consider the example shown in the following figure:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="294" width="471" class="image-border" src="assets/d1c55062-664f-41a9-bef6-0810531d9121.png"/></div>
<p class="mce-root">The blue classifier is linear while the red one is cubic. At a glance, non-linear strategy seems to perform better, because it can capture more expressivity, thanks to its concavities. However, if new samples are added following the trend defined by the last four ones (from the right), they'll be completely misclassified. In fact, while a linear function is globally better but cannot capture the initial oscillation between 0 and 4, a cubic approach can fit this data almost perfectly but, at the same time, loses its ability to keep a global linear trend. Therefore, there are two possibilities:</p>
<ul>
<li>If we expect future data to be exactly distributed as training samples, a more complex model can be a good choice, to capture small variations that a lower-level one will discard. In this case, a linear (or lower-level) model will drive to underfitting, because it won't be able to capture an appropriate level of expressivity.</li>
<li>If we think that future data can be locally distributed differently but keeps a global trend, it's preferable to have a higher residual misclassification error as well as a more precise generalization ability. Using a bigger model focusing only on training data can drive to overfitting.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Underfitting and overfitting</h1>
                </header>
            
            <article>
                
<p>The purpose of a machine learning model is to approximate an unknown function that associates input elements to output ones (for a classifier, we call them classes). However, a training set is normally a representation of a global distribution, but it cannot contain all possible elements; otherwise the problem could be solved with a one-to-one association. In the same way, we don't know the analytic expression of a possible underlying function, therefore, when training, it's necessary to think about fitting the model but keeping it free to generalize when an unknown input is presented. Unfortunately, this ideal condition is not always easy to find and it's important to consider two different dangers:</p>
<ul>
<li><strong>Underfitting</strong>: It means that the model isn't able to capture the dynamics shown by the same training set (probably because its capacity is too limited).</li>
<li><strong>Overfitting</strong>: the model has an excessive capacity and it's not more able to generalize considering the original dynamics provided by the training set. It can associate almost perfectly all the known samples to the corresponding output values, but when an unknown input is presented, the corresponding prediction error can be very high.</li>
</ul>
<p class="mce-root CDPAlignLeft CDPAlign">In the following picture, there are examples of interpolation with low-capacity (underfitting), normal-capacity (normal fitting), and excessive capacity (overfitting):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/bce053b0-b0da-434d-ab6d-7b2c125c14f9.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">It's very important to avoid both underfitting and overfitting. Underfitting is easier to detect considering the prediction error, while overfitting may prove to be more difficult to discover as it could be initially considered the result of a perfect fitting.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Cross-validation and other techniques that we're going to discuss in the next chapters can easily show how our model works with test samples never seen during the training phase. That way, it would be possible to assess the generalization ability in a broader context (remember that we're not working with all possible values, but always with a subset that should reflect the original distribution).</p>
<p class="mce-root CDPAlignLeft CDPAlign">However, a generic rule of thumb says that a residual error is always necessary to guarantee a good generalization ability, while a model that shows a validation accuracy of 99.999... percent on training samples is almost surely overfitted and will likely be unable to predict correctly when never-seen input samples are provided. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Error measures</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">In general, when working with a supervised scenario, we define a non-negative error measure <em>e<sub>m</sub></em> which takes two arguments (expected and predicted output) and allows us to compute a total error value over the whole dataset (made up of <em>n</em> samples):</p>
<div class="CDPAlignCenter CDPAlign"><img height="77" width="350" src="assets/b89bd043-d0fe-429f-a1df-45f1799c6c5e.png"/></div>
<p class="CDPAlignLeft CDPAlign">This value is also implicitly dependent on the specific hypothesis <em>H</em> through the parameter set, therefore optimizing the error implies finding an optimal hypothesis (considering the hardness of many optimization problems, this is not the absolute best one, but an acceptable approximation). In many cases, it's useful to consider the <strong>mean square error</strong> (<strong>MSE</strong>):</p>
<div class="CDPAlignCenter CDPAlign"><img height="63" width="178" src="assets/8f4e6658-1504-46cd-a092-24ca8e396991.png"/></div>
<p class="CDPAlignLeft CDPAlign">Its initial value represents a starting point over the surface of a n-variables function. A generic training algorithm has to find the global minimum or a point quite close to it (there's always a tolerance to avoid an excessive number of iterations and a consequent risk of overfitting). This measure is also called <strong>loss function</strong> because its value must be minimized through an optimization problem. When it's easy to determine an element which must be maximized, the corresponding loss function will be its reciprocal.</p>
<p class="CDPAlignLeft CDPAlign">Another useful loss function is called <strong>zero-one-loss</strong> and it's particularly efficient for binary classifications (also for one-vs-rest multiclass strategy):</p>
<div class="CDPAlignCenter CDPAlign"><img height="58" width="219" src="assets/5e6faa5c-914f-44c6-80e4-330e82586929.png"/></div>
<p class="CDPAlignLeft CDPAlign">This function is implicitly an indicator and can be easily adopted in loss functions based on the probability of misclassification.</p>
<p class="CDPAlignLeft CDPAlign">A helpful interpretation of a generic (and continuous) loss function can be expressed in terms of potential energy:</p>
<div class="CDPAlignCenter CDPAlign"><img height="55" width="199" src="assets/fddbc218-c2e9-4728-9de2-0a94bd59eaee.png"/></div>
<p class="CDPAlignLeft CDPAlign">The predictor is like a ball upon a rough surface: starting from a random point where energy (=error) is usually rather high, it must move until it reaches a stable equilibrium point where its energy (relative to the global minimum) is null. In the following figure, there's a schematic representation of some different situations:</p>
<p class="CDPAlignCenter CDPAlign"><img height="339" width="517" class="image-border" src="assets/8e5b3a8a-8b1d-4fc3-8155-b827d210d6c8.png"/></p>
<p>Just like in the physical situation, the starting point is stable without any external perturbation, so to start the process, it's needed to provide initial kinetic energy. However, if such an energy is strong enough, then after descending over the slope the ball cannot stop in the global minimum. The residual kinetic energy can be enough to overcome the ridge and reach the right valley. If there are not other energy sources, the ball gets trapped in the plain valley and cannot move anymore. There are many techniques that have been engineered to solve this problem and avoid local minima. However, every situation must always be carefully analyzed to understand what level of residual energy (or error) is acceptable, or whether it's better to adopt a different strategy. We're going to discuss some of them in the next chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PAC learning</h1>
                </header>
            
            <article>
                
<p>In many cases machine learning seems to work seamlessly, but is there any way to determine formally the learnability of a concept? In 1984, the computer scientist L. Valiant proposed a mathematical approach to determine whether a problem is learnable by a computer. The name of this technique is <strong>PAC</strong>, or <strong>probably approximately correct</strong>.</p>
<p>The original formulation (you can read it in Valiant L., <em>A Theory of the Learnable</em>, <em>Communications of the ACM</em>, Vol. 27, No. 11 , Nov. 1984) is based on a particular hypothesis, however, without a considerable loss of precision, we can think about a classification problem where an algorithm <em>A</em> has to learn a set of concepts<em>.</em> In particular, a concept is a subset of input patterns <em>X</em> which determine the same output element. Therefore, learning a concept (parametrically) means minimizing the corresponding loss function restricted to a specific class, while learning all possible concepts (belonging to the same universe), means finding the minimum of a global loss function.</p>
<p>However, given a problem, we have many possible (sometimes, theoretically infinite) hypotheses and a probabilistic trade-off is often necessary. For this reason, we accept good approximations with high probability based on a limited number of input elements and produced in polynomial time.</p>
<p>Therefore, an algorithm <em>A</em> can learn the class <em>C</em> of all concepts (making them PAC learnable) if it's able to find a hypothesis <em>H</em> with a procedure <em>O(n<sup>k</sup>)</em> so that <em>A</em>, with a probability <em>p</em>, can classify all patterns correctly with a maximum allowed error <em>m<sub>e</sub></em>. This must be valid for all statistical distributions on <em>X</em> and for a number of training samples which must be greater than or equal to a minimum value depending only on <em>p</em> and <em>m<sub>e</sub></em>.</p>
<p>The constraint to computation complexity is not a secondary matter, in fact, we expect our algorithms to learn efficiently in a reasonable time also when the problem is quite complex. An exponential time could lead to computational explosions when the datasets are too large or the optimization starting point is very far from an acceptable minimum. Moreover, it's important to remember the so-called <strong>curse of dimensionality</strong>, which is an effect that often happens in some models where training or prediction time is proportional (not always linearly) to the dimensions, so when the number of features increases, the performance of the models (that can be reasonable when the input dimensionality is small) gets<span> dramatically reduced</span>. Moreover, in many cases, in order to capture the full expressivity, it's necessary to have a very large dataset and without enough training data, the approximation can become problematic (this is called <strong>Hughes phenomenon</strong>). For these reasons, looking for polynomial-time algorithms is more than a simple effort, because it can determine the success or the failure of a machine learning problem. For these reasons, in the next chapters, we're going to introduce some techniques that can be used to efficiently reduce the dimensionality of a dataset without a problematic loss of information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Statistical learning approaches</h1>
                </header>
            
            <article>
                
<p>Imagine that you need to design a spam-filtering algorithm starting from this initial (over-simplistic) classification based on two parameters:</p>
<table>
<tbody>
<tr>
<td><strong>Parameter</strong></td>
<td><strong>Spam emails (</strong><em>X<sub>1</sub></em><strong>)</strong></td>
<td><strong>Regular emails (</strong><em>X2</em><strong>)</strong></td>
</tr>
<tr>
<td><em>p<sub>1</sub></em> - Contains &gt; 5 blacklisted words</td>
<td>80</td>
<td>20</td>
</tr>
<tr>
<td><em>p<sub>2</sub></em><strong><sub> </sub></strong><span>- </span>Message length &lt; 20 characters</td>
<td>75</td>
<td>25</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We have collected 200 email messages (<em>X)</em> (for simplicity, we consider <em>p<sub>1</sub></em> and <em>p</em><sub><em>2 </em></sub>mutually exclusive) and we need to find a couple of probabilistic hypotheses (expressed in terms of <em>p<sub>1</sub></em> and <em>p<sub>2</sub></em>), to determine:</p>
<div class="CDPAlignCenter CDPAlign"><img height="40" width="139" src="assets/8351e161-8c7e-466e-88d1-9fc05a23d71c.png"/></div>
<p class="CDPAlignLeft CDPAlign">We also assume the conditional independence of both terms (it means that <em>h<sub>p1</sub></em> and <em>h<sub>p2</sub></em> contribute conjunctly to spam in the same way as they were alone).</p>
<p class="CDPAlignLeft CDPAlign">For example, we could think about rules (hypotheses) like: "If there are more than five blacklisted words" or "If the message is less than 20 characters in length" then "the probability of spam is high" (for example, greater than 50 percent). However, without assigning probabilities, it's difficult to generalize when the dataset changes (like in a real world antispam filter). We also want to determine a partitioning threshold (such as green, yellow, and red signals) to help the user in deciding what to keep and what to trash.</p>
<p class="CDPAlignLeft CDPAlign">As the hypotheses are determined through the dataset <em>X</em>, we can also write (in a discrete form):</p>
<div class="CDPAlignCenter CDPAlign"><img height="53" width="267" src="assets/493d60ac-f47e-4c5b-a678-14302e1c18b4.png"/></div>
<p class="CDPAlignLeft CDPAlign">In this example, it's quite easy to determine the value of each term. However, in general, it's necessary to introduce the Bayes formula (which will be discussed in <a href="e8152109-eb49-4f05-bd6b-16b532a68696.xhtml" target="_blank">Chapter 6</a>, <em>Naive Bayes</em>):</p>
<div class="CDPAlignCenter CDPAlign"><img height="56" width="193" src="assets/bb33ae98-e245-45bc-a594-0086a72f5484.png"/></div>
<p class="CDPAlignLeft CDPAlign">The proportionality is necessary to avoid the introduction of the marginal probability <em>P(X)</em>, which acts only as a normalization factor (remember that in a discrete random variable, the sum of all possible probability outcomes must be equal to 1).</p>
<p class="CDPAlignLeft CDPAlign">In the previous equation, the first term is called <strong>a posteriori</strong> (which comes after) probability, because it's determined by a marginal <strong>Apriori</strong> (which comes first) probability multiplied by a factor which is called <strong>likelihood</strong>. To understand the philosophy of such an approach, it's useful to take a simple example: tossing a fair coin. Everybody knows that the marginal probability of each face is equal to 0.5, but who decided that? It's a theoretical consequence of logic and probability axioms (a good physicist would say that it's never 0.5 because of several factors that we simply discard). After tossing the coin 100 times, we observe the outcomes and, surprisingly, we discover that the ratio between heads and tails is slightly different (for example, 0.46). How can we correct our estimation? The term called <strong>likelihood</strong> measures how much our actual experiments confirm the Apriori hypothesis and determines another probability (<strong>a posteriori</strong>) which reflects the actual situation. The likelihood, therefore, helps us in correcting our estimation dynamically, overcoming the problem of a fixed probability.</p>
<p class="CDPAlignLeft CDPAlign">In <a href="e8152109-eb49-4f05-bd6b-16b532a68696.xhtml" target="_blank">Chapter 6</a>, <em>Naive Bayes</em>, dedicated to naive Bayes algorithms, we're going to discuss these topics deeply and implement a few examples with scikit-learn, however, it's useful to introduce here two statistical learning approaches which are very diffused. Refer to <em>Russel S., Norvig P., Artificial Intelligence: A Modern Approach, Pearson </em>for further information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MAP learning</h1>
                </header>
            
            <article>
                
<p>When selecting the right hypothesis, a Bayesian approach is normally one of the best choices, because it takes into account all the factors and, as we'll see, even if it's based on conditional independence, such an approach works perfectly when some factors are partially dependent. However, its complexity (in terms of probabilities) can easily grow because all terms must always be taken into account. For example, a real coin is a very short cylinder, so, in tossing a coin, we should also consider the probability of even. Let's say, it's 0.001. It means that we have three possible outcomes: <em>P(head)</em> = <em>P(tail)</em> = (1.0 - 0.001) / 2.0 and <em>P(even)</em> = 0.001. The latter event is obviously unlikely, but in Bayesian learning it must be considered (even if it'll be squeezed by the strength of the other terms).</p>
<p>An alternative is picking the most probable hypothesis in terms of <strong>a posteriori</strong> probability:</p>
<div class="CDPAlignCenter CDPAlign"><img height="38" width="256" src="assets/1c07ea8b-dbff-4736-9df4-5df7d7303421.png"/></div>
<p>This approach is called <strong>MAP</strong> (<strong>maximum a posteriori</strong>)<em><strong> </strong></em>and it can really simplify the scenario when some hypotheses are quite unlikely (for example, in tossing a coin, a MAP hypothesis will discard <em>P(even)</em>). However, it still does have an important drawback: it depends on Apriori probabilities (remember that maximizing the a posteriori implies considering also the Apriori). As Russel and Norvig (Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson) pointed out, this is often a delicate part of an inferential process, because there's always a theoretical background which can drive to a particular choice and exclude others. In order to rely only on data, it's necessary to have a different approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Maximum-likelihood learning</h1>
                </header>
            
            <article>
                
<p>We have defined likelihood as a filtering term in the Bayes formula. In general, it has the form of:</p>
<div class="CDPAlignCenter CDPAlign"><img height="40" width="150" src="assets/ed45edfa-51f8-45d3-86ce-f7587f981802.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here the first term expresses the actual likelihood of a hypothesis, given a dataset <em>X</em>. As you can imagine, in this formula there are no more Apriori probabilities, so, maximizing it doesn't imply accepting a theoretical preferential hypothesis, nor considering unlikely ones. A very common approach, known as <strong>expectation-maximization</strong> and used in many algorithms (we're going to see an example in logistic regression), is split into two main parts:</p>
<ul>
<li>
<p>Determining a log-likelihood expression based on model parameters (they will be optimized accordingly)</p>
</li>
<li>
<p>Maximizing it until residual error is small enough</p>
</li>
</ul>
<p>A log-likelihood (normally called <strong>L</strong>) is a useful trick that can simplify gradient calculations. A generic likelihood expression is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="53" width="153" src="assets/46a676cb-6f2a-46da-baf9-782ac755edfc.png"/></div>
<p class="CDPAlignLeft CDPAlign">As all parameters are inside <em>h<sub>i</sub></em>, the gradient is a complex expression which isn't very manageable. However our goal is maximizing the likelihood, but it's easier minimizing its reciprocal:</p>
<div class="CDPAlignCenter CDPAlign"><img height="60" width="303" src="assets/88406a0c-88b2-4ae5-8254-65818be8795d.png"/></div>
<p class="CDPAlignLeft CDPAlign">This can be turned into a very simple expression by applying natural logarithm (which is monotonic):</p>
<div class="CDPAlignCenter CDPAlign"><img height="50" width="406" src="assets/51c3835c-1cff-46a8-907d-94789b26a129.png"/></div>
<p class="CDPAlignLeft CDPAlign">The last term is a summation which can be easily derived and used in most of the optimization algorithms. At the end of this process, we can find a set of parameters which provides the maximum likelihood without any strong statement about prior distributions. This approach can seem very technical, but its logic is really simple and intuitive. To understand how it works, I propose a simple exercise, which is part of Gaussian mixture technique discussed also in Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson<em>.</em></p>
<p class="CDPAlignLeft CDPAlign">Let's consider 100 points drawn from a Gaussian distribution with zero mean and a standard deviation equal to 2.0 (quasi-white noise made of independent samples):</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>nb_samples = 100</strong><br/><strong>X_data = np.random.normal(loc=0.0, scale=np.sqrt(2.0), size=nb_samples)</strong></pre>
<p class="CDPAlignLeft CDPAlign">The plot is shown next:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/2fdd0020-4519-4ffd-a8a5-afd1efc8e214.png"/></div>
<p class="mce-root">In this case, there's no need for a deep exploration (we know how they are generated), however, after restricting the hypothesis space to the Gaussian family (the most suitable considering only the graph), we'd like to find the best value for mean and variance. First of all, we need to compute the log-likelihood (which is rather simple thanks to the exponential function):</p>
<div class="CDPAlignCenter CDPAlign"><img height="63" width="365" src="assets/44af75ad-1e1c-4dd7-b53e-95dde3a3e07d.png"/></div>
<p class="mce-root">A simple Python implementation is provided next (for ease of use, there's only a single array which contains both mean (0) and variance (1)):</p>
<pre><strong>def negative_log_likelihood(v):</strong><br/><strong>  l = 0.0</strong><br/><strong>  f1 = 1.0 / np.sqrt(2.0 * np.pi * v[1]) </strong><br/><strong>  f2 = 2.0 * v[1]</strong><br/> <br/><strong>  for x in X_data:</strong><br/><strong>    l += np.log(f1 * np.exp(-np.square(x - v[0]) / f2))</strong><br/> <br/><strong> return -l</strong></pre>
<p class="mce-root">Then we need to find its minimum (in terms of mean and variance) with any of the available methods (gradient descent or another numerical optimization algorithm). For example, using the <kbd>scipy</kbd> minimization function, we can easily get:</p>
<pre><strong>from scipy.optimize import minimize<br/><br/>&gt;&gt;&gt; minimize(fun=negative_log_likelihood, x0=[0.0, 1.0])<br/><br/> fun: 172.33380423827057<br/> hess_inv: array([[ 0.01571807,  0.02658017],<br/>       [ 0.02658017,  0.14686427]])<br/>      jac: array([  0.00000000e+00,  -1.90734863e-06])<br/>  message: 'Optimization terminated successfully.'<br/>     nfev: 52<br/>      nit: 9<br/>     njev: 13<br/>   status: 0<br/>  success: True<br/>        x: array([ 0.04088792,  1.83822255])</strong></pre>
<p class="mce-root">A graph of the negative log-likelihood function is plotted next. The global minimum of this function corresponds to an optimal likelihood given a certain distribution. It doesn't mean that the problem has been completely solved, because the first step of this algorithm is determining an expectation, which must be always realistic. The likelihood function, however, is quite sensitive to wrong distributions because it can easily get close to zero when the probabilities are low. For this reason, <strong>maximum-likelihood</strong> (<strong>ML</strong>) learning is often preferable to MAP learning, which needs Apriori distributions and can fail when they are not selected in the most appropriate way:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="353" width="513" class="image-border" src="assets/0be8899e-8370-4402-be60-3f56a2da2cac.png"/></div>
<p class="mce-root">This approach has been applied to a specific distribution family (which is indeed very easy to manage), but it also works perfectly when the model is more complex. Of course, it's always necessary to have an initial awareness about how the likelihood should be determined because more than one feasible family can generate the same dataset. In all these cases, <strong>Occam's razor</strong> is the best way to proceed: the simplest hypothesis should be considered first. If it doesn't fit, an extra level of complexity can be added to our model. As we'll see, in many situations, the easiest solution is the winning one, and increasing the number of parameters or using a more detailed model can only add noise and a higher possibility of overfitting.</p>
<div class="packt_figref CDPAlignLeft CDPAlign packt_infobox">SciPy (<a href="https://www.scipy.org" target="_blank">https://www.scipy.org</a>) is a set of high-end scientific and data-oriented libraries available for Python. It includes NumPy, Pandas, and many other useful frameworks. If you want to read more about Python scientific computing, refer to Johansson R., <em>Numerical Python</em>, Apress or Landau R. H., Pàez M. J., Bordeianu C. C., <em>Computational Physics. Problem Solving with Python,</em> Wiley-VCH.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elements of information theory</h1>
                </header>
            
            <article>
                
<p>A machine learning problem can also be analyzed in terms of information transfer or exchange. Our dataset is composed of <em>n</em> features, which are considered independent (for simplicity, even if it's often a realistic assumption) drawn from <em>n</em> different statistical distributions. Therefore, there are <em>n</em> probability density functions <em>p<sub>i</sub>(x)</em> which must be approximated through other <em>n</em> <em>q</em><em><sub>i</sub>(x)</em> functions. In any machine learning task, it's very important to understand how two corresponding distributions diverge and what is the amount of information we lose when approximating the original dataset.</p>
<p>The most useful measure is called <strong>entropy</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="48" width="157" src="assets/a0de87df-2154-4f2e-b197-7202471d184a.png"/> </div>
<p class="CDPAlignLeft CDPAlign">This value is proportional to the uncertainty of <em>X</em> and it's measured in <strong>bits</strong> (if the logarithm has another base, this unit can change too). For many purposes, a high entropy is preferable, because it means that a certain feature contains more information. For example, in tossing a coin (two possible outcomes), <em>H(X)</em> = 1 bit, but if the number of outcomes grows, even with the same probability, <em>H(X)</em> also does because of a higher number of different values and therefore increased variability. It's possible to prove that for a Gaussian distribution (using natural logarithm):</p>
<div class="CDPAlignCenter CDPAlign"><img height="46" width="136" src="assets/02490395-4402-4343-b04e-b487a35973c4.png"/></div>
<p class="CDPAlignLeft CDPAlign">So, the entropy is proportional to the variance, which is a measure of the amount of information carried by a single feature. In the next chapter, we're going to discuss a method for feature selection based on variance threshold. Gaussian distributions are very common, so this example can be considered just as a general approach to feature filtering: low variance implies low information level and a model could often discard all those features.</p>
<p class="CDPAlignLeft CDPAlign">In the following figure, there's a plot of <em>H(X)</em> for a Gaussian distribution expressed in <strong>nats</strong> (which is the corresponding unit measure when using natural logarithms):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/da315d54-20ba-49d6-abc8-93340e600d26.png"/></div>
<p class="CDPAlignLeft CDPAlign">For example, if a dataset is made up of some features whose variance (here it's more convenient talking about standard deviation) is bounded between 8 and 10 and a few with <em>STD &lt; 1.5</em>, the latter could be discarded with a limited loss in terms of information. These concepts are very important in real-life problems when large datasets must be cleaned and processed in an efficient way.</p>
<p class="CDPAlignLeft CDPAlign">If we have a target probability distribution <em>p(x)</em>, which is approximated by another distribution <em>q(x)</em>, a useful measure is <strong>cross-entropy</strong> between <em>p</em> and <em>q</em> (we are using the discrete definition as our problems must be solved using numerical computations):</p>
<div class="CDPAlignCenter CDPAlign"><img height="47" width="193" src="assets/f2c2381a-0820-48fc-864c-bb723a5313e6.png"/></div>
<p class="CDPAlignLeft CDPAlign">If the logarithm base is 2, it measures the number of bits requested to decode an event drawn from <em>P</em> when using a code optimized for <em>Q</em>. In many machine learning problems, we have a source distribution and we need to train an estimator to be able to identify correctly the class of a sample. If the error is null, <em>P = Q</em> and the cross-entropy is minimum (corresponding to the entropy <em>H(P)</em>). However, as a null error is almost impossible when working with <em>Q</em>, we need to <em>pay</em> a price of <em>H(P, Q)</em> bits, to determine the right class starting from a prediction. Our goal is often to minimize it, so to reduce this <em>price</em> under a threshold that cannot alter the predicted output if not paid. In other words, think about a binary output and a sigmoid function: we have a threshold of 0.5 (this is the maximum <em>price</em> we can pay) to identify the correct class using a step function (0.6 -&gt; 1, 0.1 -&gt; 0, 0.4999 -&gt; 0, and so on). As we're not able to pay this <em>price</em>, since our classifier doesn't know the original distribution, it's necessary to reduce the cross-entropy under a tolerable noise-robustness threshold (which is always the smallest achievable one).</p>
<p class="CDPAlignLeft CDPAlign">In order to understand how a machine learning approach is performing, it's also useful to introduce a <strong>conditional</strong> entropy or the uncertainty of <em>X</em> given the knowledge of <em>Y</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="66" width="258" src="assets/b6c53585-df26-477f-af85-9e87070e5658.png"/></div>
<p class="CDPAlignLeft CDPAlign">Through this concept, it's possible to introduce the idea of mutual information, which is the amount of information shared by both variables and therefore, the reduction of uncertainty about <em>X</em> provided by the knowledge of <em>Y</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="38" width="164" src="assets/791c1b5b-71b8-4426-8e8f-a8889290990f.png"/></div>
<p class="CDPAlignLeft CDPAlign">Intuitively, when <em>X</em> and <em>Y</em> are independent, they don't share any information. However, in machine learning tasks, there's a very tight dependence between an original feature and its prediction, so we want to maximize the information shared by both distributions. If the conditional entropy is small enough (so <em>Y</em> is able to describe <em>X</em> quite well), the mutual information gets close to the marginal entropy <em>H(X)</em>, which measures the amount of information we want to learn.</p>
<p class="CDPAlignLeft CDPAlign">An interesting learning approach based on the information theory, called <strong>Minimum Description Length</strong> (<strong>MDL</strong>), is discussed in Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson, where I suggest you look for any further information about these topics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Russel S., Norvig P., <em>Artificial Intelligence: A Modern Approach</em>, Pearson</li>
<li>Valiant L., <em>A Theory of the Learnable, Communications of the ACM</em>, Vol. 27, No. 11 (Nov. 1984)</li>
<li>Hastie T., Tibshirani R., Friedman J., <em>The Elements of Statistical Learning: Data Mining, Inference and, Prediction</em>, Springer</li>
<li>Aleksandrov A.D., Kolmogorov A.N, Lavrent'ev M.A., <em>Mathematics: Its contents, Methods, and Meaning</em>, Courier Corporation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have introduced some main concepts about machine learning. We started with some basic mathematical definitions, to have a clear view about data formats, standards, and kind of functions. This notation will be adopted in all the other chapters and it's also the most diffused in technical publications. We discussed how scikit-learn seamlessly works with multi-class problems, and when a strategy is preferable to another.</p>
<p>The next step was the introduction of some fundamental theoretical concepts about learnability. The main questions we tried to answer were: how can we decide if a problem can be learned by an algorithm and what is the maximum precision we can achieve. PAC learning is a generic but powerful definition that can be adopted when defining the boundaries of an algorithm. A PAC learnable problem, in fact, is not only manageable by a suitable algorithm but is also fast enough to be computed in polynomial time. Then we introduced some common statistical learning concepts, in particular, MAP and maximum likelihood learning approaches. The former tries to pick the hypothesis which maximizes the a posteriori probability, while the latter works the likelihood, looking for the hypothesis that best fits the data. This strategy is one of the most diffused in many machine learning problems because it's not affected by Apriori probabilities and it's very easy to implement in many different contexts. We also gave a physical interpretation of a loss function as an energy function. The goal of a training algorithm is to always try to find the global minimum point, which corresponds to the deepest valley in the error surface. At the end of this chapter, there was a brief introduction to information theory and how we can reinterpret our problems in terms of information gain and entropy. Every machine learning approach should work to minimize the amount of information needed to start from prediction and recover original (desired) outcomes.</p>
<p>In the next chapter, we're going to discuss the fundamental concepts of feature engineering, which is the first step in almost every machine learning pipeline. We're going to show how to manage different kinds of data (numerical and categorical) and how it's possible to reduce the dimensionality without a dramatic loss of information.</p>


            </article>

            
        </section>
    </body></html>