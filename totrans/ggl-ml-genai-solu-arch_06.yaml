- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilizing Google Cloud’s High-Level AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have been equipped with an arsenal of information regarding AI/ML
    and Google Cloud, you are ready to start diving in and implementing AI/ML workloads
    in the cloud – that’s exactly what we will do in this chapter! You’re going to
    start by using Google Cloud’s high-level AI/ML APIs, such as the Natural Language
    API and the Vision API, which enable you to implement AI/ML functionality using
    models that are trained and maintained by Google. Then, you’ll use Vertex AI AutoML
    to train your own ML model, all without the need for any AI/ML expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Document AI to extract information from documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Google Cloud Natural Language API to to get sentiment analysis insights
    from textual inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Vertex AI AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites for this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to perform some setup steps to lay the foundations for the activities
    we will be performing in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Cloning this book’s GitHub repository to your local machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout the rest of this book, we will perform a lot of hands-on activities
    that require the use of resources such as code, data, and other files. Many of
    these resources are stored in the GitHub repository associated with this book,
    so the easiest way to access them would be to clone the repository to your local
    machine (that is, your laptop/PC). The exact way to do this will differ based
    on the operating system you use, but the process is generally to open a command
    terminal on your system, navigate to a directory of your choice, and run the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The Google Cloud console
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Prerequisites for using *Google Cloud tools and services* section of
    [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), we discussed that you would need
    to create a Google Cloud account to interact with most Google Cloud services.
    We will use the Google Cloud console to perform many of the activities in this
    chapter. Once you have created a Google Cloud account, you can log into the console
    by navigating to [https://console.cloud.google.com](https://console.cloud.google.com).
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this book, you will need to navigate to different services within
    the Google Cloud console. Whenever you need to do this, you can click on the symbol
    with three horizontal lines next to the Google Cloud logo in the top-left corner
    of the screen to view the menu of Google Cloud services or products (in this context,
    we’ll use the terms “services” and “products” interchangeably throughout this
    book). See *Figure 4**.1* for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Accessing the Google Cloud services menu](img/B18143_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Accessing the Google Cloud services menu'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then scroll through the list of services and select the relevant one.
    Some of the menu items have sub-menus nested under them, as shown in *Figure 4**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Google Cloud services sub-menus](img/B18143_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Google Cloud services sub-menus'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will sometimes use shorthand notation to represent
    the navigation path through the services menus and sub-menus. For example, to
    navigate to the **Credentials** sub-menu item, we will represent this as the **Google
    Cloud services** menu → **APIs & Services** → **Credentials**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at how to access the Google Cloud console, let’s ensure
    that we have a Google Cloud project to use for the activities in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Google Cloud project is an environment that contains all of your resources
    in Google Cloud. Resources include things such as virtual machines, network configurations,
    databases, storage buckets, identity and access controls, and pretty much everything
    that you create on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, when you create your Google Cloud account and log into the console
    for the first time, Google Cloud automatically creates your first project for
    you, with the name **My First Project**. If, for any reason, this has not happened,
    you will need to create one by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **IAM & Admin** → **Create** **a Project**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type a name for your project in the **Project Name** text box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You could also edit the **Project ID** value if you’d like, but it’s more common
    to leave this at the generated value unless you have a specific naming convention
    that you’d like to implement. Note that this cannot be changed after the project
    is created.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the **Location** field, click **Browse** to display potential locations for
    your project. Then, click **Select**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create**. The console will navigate to the **Dashboard** page and your
    project will be created within a few minutes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have our Google Cloud project in place, let’s ensure that our billing
    details are set up for using Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Billing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Google Cloud Billing account contains all of the details that enable Google
    Cloud to bill you for using their services.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t already have a Google Cloud Billing account, then you will need
    to create one by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Billing**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the page that appears, select **Manage Billing Accounts**, then **Add** **billing
    account**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create account**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a name for the Cloud Billing account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Depending on your configuration, you will also need to select one of the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Organization**: If you see an **Organization** dropdown, then you must select
    an organization before you can continue'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Country**: If you are prompted to select a **country**, select the country
    that corresponds with your billing mailing address'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Click **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will then need to fill in all of your billing details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you have entered all of your details, click **Submit and enable billing**.
    To learn more about Google Cloud Billing accounts and concepts, visit [https://cloud.google.com/billing/docs/how-to/create-billing-account](https://cloud.google.com/billing/docs/how-to/create-billing-account).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that our Google Cloud project and Billing account have been set up, we’re
    ready to start using Google Cloud products in our project.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Shell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use Google Cloud Shell to perform some of the activities in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Cloud Shell, as described in the *Interacting with Google Cloud services*
    section of [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059). As a reminder, you
    can access it by clicking the **Cloud Shell** symbol in the top-right corner of
    the screen, as shown in *Figure 4**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Cloud Shell symbol](img/B18143_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Cloud Shell symbol'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve opened Cloud Shell, we’ll perform some basic setup steps to prepare
    our Cloud Shell environment.
  prefs: []
  type: TYPE_NORMAL
- en: Authentication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a Google Cloud service is invoked in some way, it usually wants to know
    who or what (identity) is invoking it. This is for many reasons, such as billing
    and checking whether the invoker is allowed to perform the action that is being
    requested. This process of identification is called “authentication.” There are
    numerous different ways to authenticate with a Google Cloud API. For example,
    when you log into the Google Cloud console, you are authenticating with the console.
    Then, as you navigate and perform actions in the console, your authentication
    details are used to control what kinds of actions you are allowed to perform.
    One of the simplest authentication mechanisms is referred to as **API keys**.
    We’ll explore this next.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud API keys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: API keys are a very basic type of authentication supported by some Google Cloud
    APIs. An API key does not provide any identification or authorization information
    to the called API; they are only generally used for billing purposes (by linking
    to a Google Cloud project) or to track usage against Google Cloud quotas.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use them to access the Google Cloud Natural Language
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an API key
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although we will use Cloud Shell to perform many of the steps in this chapter,
    Google Cloud currently only supports creating API keys in the Google Cloud console.
    To create our API key, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **APIs & Services** → **Credentials**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create credentials**, then **API key**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the generated API key and click **Close**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will need to paste this API key in a later step. If you need to view or
    copy the key again, you can select **Show key** in the **API Keys** section of
    the console (that is, the section you are currently in).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, we have created an API key that we can use in later activities in
    this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enabling the relevant Google Cloud APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we discussed in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), before
    you can use a Google Cloud service API, you need to enable it. Since we are already
    logged into Cloud Shell, we can easily do that directly from here by using the
    `gcloud` command. We will enable the Google Cloud Natural Language API, the Vision
    API, the Document AI API, and the Google Cloud Storage API as we will be using
    all of them in this chapter. To do this, run the following commands in Cloud Shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The responses should be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With the required APIs now enabled, we just have a few more steps to complete
    before we can start using them.
  prefs: []
  type: TYPE_NORMAL
- en: Storing authentication credentials in environment variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will reference our authentication credentials – that is, the API key that
    we created earlier – in many commands in this chapter. To make it easier to reference
    the credentials, we will store them in Linux environment variables in Cloud Shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'To store the API key, run the following command, but replace `<YOUR_API_KEY>`
    with the API key you created and copied earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We’re almost finished with the environment setup steps now. Next, we will clone
    our GitHub repository, after which we’ll be ready to start using Google Cloud’s
    high-level AI services.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a directory and cloning our GitHub repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You already cloned our repository to your local machine. In this section, you
    will also clone it to your Cloud Shell environment. To do this, perform the following
    steps in Cloud Shell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change location to that directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the directory as a local `git` repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Clone the `git` repository that contains the code we will use in the Document
    AI section of this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have all of the required code copied to our Cloud Shell environment,
    it’s time to start executing it!
  prefs: []
  type: TYPE_NORMAL
- en: Detecting text in images with the Cloud Vision API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re going to begin with a very simple example to show you just how easy it
    is to start using AI/ML services on Google Cloud. In just a few short steps, you
    will be able to extract text and associated metadata from an image and save that
    text in a file. This process is called **optical character** **recognition** (**OCR**).
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin this process, download the following image to your computer (the image
    can also be found in the `Chapter``0``4/images` folder in the git repository you
    created above): [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-04/images/poem.png](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-04/images/poem.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The image is shown in *Figure 4**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Sign to be used for OCR](img/B18143_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Sign to be used for OCR'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue with the remaining steps to upload this image to your Cloud Shell
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the symbol with the three dots at the top-right corner of the Cloud Shell
    area (at the bottom of your Google Cloud console screen), then select **Upload**.
    See *Figure 4**.5* for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5: Cloud Shell upload](img/B18143_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Cloud Shell upload'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be presented with a file upload dialogue box (see *Figure 4**.6* for
    reference). Select **Choose files** and browse for the file that you previously
    downloaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6: Cloud Shell upload prompt](img/B18143_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Cloud Shell upload prompt'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, invoke the Cloud Vision API to extract the text and save it in a file
    with the following command (in this case, the filename of the image we’re using
    is `poem.png`). The results are stored in a file named `vision-ocr.json`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the contents of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The file is not conducive to being printed in a book, so we will not display
    it here.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, each word that is extracted is stored with some associated metadata,
    such as the *X* and *Y* coordinates of the corners of the bounding boxes that
    contain that word in the image. This metadata helps us understand where each word
    appeared in the image, which can be useful information for developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you just want to see the detected text, without the other metadata, you
    can use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see the entire body of detected text in the first description, and
    then individual words in the subsequent descriptions. The output will look similar
    to the following (truncated here for brevity):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations – you have just implemented your first AI/ML workload on Google
    Cloud! It really is that simple! Next, we’ll start looking at some slightly more
    advanced use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Using Document AI to extract information from documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may recall, we discussed Document AI at length in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059).
    We talked about how it goes beyond understanding the content of textual inputs
    and also incorporates structure. In this section, we’re going to take a look at
    how Document AI does this, and we’ll see it in action for some real-world use
    cases. We’ll start by covering some important Document AI concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Document AI concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Document AI uses the concept of “processors” to process documents. There are
    three main types of processors that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: General processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom processors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General processors and specialized processors use models that are pre-trained
    by Google, so you can use them without needing to train your own models, and without
    any AI/ML expertise. However, to provide additional flexibility, Google Cloud
    gives you the ability to further train specialized processors with your data to
    improve their accuracy for use cases that are specific to your business. This
    process is referred to as “uptraining.”
  prefs: []
  type: TYPE_NORMAL
- en: General processors include a processor for performing OCR on images of text
    documents and forms. While we performed OCR using the Cloud Vision API in the
    previous section, Document AI provides additional functionality, such as identifying
    structured information within the input documents. This includes understanding
    key-value pairs in input forms, which can be very useful for automated data entry
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This is an important consideration in the Solution Architect role; there are
    often multiple different services that could be used to achieve similar business
    goals, and the Solution Architect’s job is to create or select the most suitable
    solution based on specific business requirements. In the case of selecting between
    the Cloud Vision API and Document AI for OCR use cases, bear in mind that the
    Cloud Vision API generally requires less initial effort to use, but it also provides
    fewer features than Document AI. Therefore, if you have a very simple use case
    that can be satisfied with the Cloud Vision API, then you can select that option,
    whereas more complex use cases would steer you toward using Document AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specialized processors provide models for processing specific types of documents,
    such as US Federal Government forms, identification documents, and invoices. The
    specialized processors are currently categorized into four different types:'
  prefs: []
  type: TYPE_NORMAL
- en: Procurement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lending
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contract
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing (March 2023), Google Cloud recently announced the general
    availability of new functionality in Document AI, which is Document AI Workbench.
    Document AI Workbench enables you to create completely new, custom types of processors,
    beyond what’s provided by Google Cloud, for your specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at how some of the Document AI processors work.
  prefs: []
  type: TYPE_NORMAL
- en: Performing OCR with Document AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to use the simplest use case, OCR, for demonstration purposes, which
    will also allow us to directly contrast this functionality against how we performed
    OCR with the Cloud Vision API.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Document AI OCR processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we can process any documents, we need to create a processor. We’re going
    to perform this action in the Google Cloud console:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **Document AI** → **Processor Gallery**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under **Document OCR**, click **CREATE PROCESSOR**. See *Figure 4**.7* for
    reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7: CREATE PROCESSOR](img/B18143_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: CREATE PROCESSOR'
  prefs: []
  type: TYPE_NORMAL
- en: Input **OCR-Processor** as the name, select the region nearest to you, and click
    **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A note on regions and zones
  prefs: []
  type: TYPE_NORMAL
- en: For this book, whenever you need to select a region, we recommend selecting
    the same region every time since we will build resources throughout this book
    that may need to be used in combination with other resources created in different
    chapters of this book. Generally, when you need to use some cloud resources with
    other cloud resources, it’s often much easier if those resources are in the same
    region. Sometimes, it can be difficult or impossible for resources in one region
    to reference resources in a different region without building a customized solution
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: In some activities in this book, you will also have the option of selecting
    a specific zone within a region. This decision is usually less important for the
    activities in this book because accessing resources across zones within a region
    is generally quite easy. In a production environment, you may want to keep resources
    in the same zone if you have specific business requirements, such as clusters
    of compute instances that need to work very closely together.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t have such specific requirements, then the best practice is to
    distribute your resources across multiple zones to improve workload resilience
    and availability. You can learn more about Google Cloud regions and zones in the
    documentation at the following URL: [https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When your processor gets created, you should automatically be redirected to
    the **PROCESSOR DETAILS** page. If not, you can view the details by selecting
    **My processors** in the menu on the left-hand side of the screen and then selecting
    your newly created processor. The processor details are shown in *Figure 4**.8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.8: OCR processor details](img/B18143_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: OCR processor details'
  prefs: []
  type: TYPE_NORMAL
- en: Take note of your processor ID (highlighted by the red arrow in *Figure 4**.8*)
    as you will need to reference it in the code we will use in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Invoking the OCR processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we’ve created our processor, we can start using it. In this section,
    we will run a simple piece of Python code that uses the Document AI Python client
    library to process a file and extract the necessary information. We’re going to
    use the same file that we used with the Cloud Vision API earlier in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps in Cloud Shell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the latest version of the Document AI client packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take note of your project ID as you will need to reference it in the Python
    code that will be used to interact with Document AI. You can use the following
    command to view it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ensure that you are in your home directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Copy the `docai-ocr.py` file to your home directory from the `git` directory
    we created earlier. We’re going to edit this file outside of the branch because
    we won’t need to merge the updates back to the main branch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this Python file, we’ll begin by importing the required libraries. Then,
    we’ll define some variables (we will need to replace the values of those variables
    with the values of the processor we created in the previous section).
  prefs: []
  type: TYPE_NORMAL
- en: Then, we’ll define a function that creates a Document AI client, reads the input
    file, creates a request object based on the file’s contents, and sends this request
    to be processed by the Document AI service.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the function will print the resulting document text.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform these actions, execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `nano` to open the file for editing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace the variable definitions with the values you saved after creating the
    processor in the Google Cloud console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the `project_id` value with your project ID, which you viewed in the
    `$DEVSHELL_PROJECT_ID` environment variable previously.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify ‘`us'` or `'eu'` as the location, depending on which region you used.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the `processor_id` value with your processor ID, which you can view
    on the `file_path` value with the path to the file we want to process. The Cloud
    Shell upload feature that we used generally stores files in your Cloud Shell home
    directory, which is usually `/home/admin_.`
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, the `mime_type` value is `'image/``png``'`. You can view the list
    of supported MIME types at [https://cloud.google.com/document-ai/docs/file-types](https://cloud.google.com/document-ai/docs/file-types).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: When you have completed all of the edits, press *Ctrl* + *X* to exit `nano`,
    then *Y* to save the file, and then *Enter* to confirm this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we’re ready to execute OCR! The following command will run our Python
    function and save the results in a file named `docai-ocr.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the contents of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note how the output is formatted differently than the results we received when
    we used the Cloud Vision API. This further demonstrates how we can use different
    services to achieve similar results. However, there are subtle differences between
    the services, and we need to pick the best option based on business requirements.
    The Cloud Vision API required much less initial effort to use, but bear in mind
    that Document AI has much more powerful features, such as automated document processing,
    and customized models for specific use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at some of Document AI’s additional features.
  prefs: []
  type: TYPE_NORMAL
- en: Document AI’s response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our OCR example, we focused on the `document.txt` object in the response.
    However, the full response that is returned contains a lot more information that
    we can use in various ways, such as the number of pages, paragraphs, and lines
    in the document, and many other types of metadata. When using form parsers or
    specialized processors, it can even highlight structured data types such as tables
    and key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that in the preceding example, we performed an online
    inference request, in which we got a response in real time for a single document.
    Document AI also allows us to perform batch inference requests if we need to process
    large numbers of documents at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Human in the loop (HITL) AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding use case, the model was able to identify all the words in the
    image. However, in reality, the sources of our images may not always be clearly
    legible. For example, we may need to read information from pictures of road signs
    that could be worn in places. This is something to bear in mind when thinking
    about how accurate you need the results to be. Document AI provides a **HITL**)
    feature to enable you to improve the accuracy of the results by having a human
    review and make updates where necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud Natural Language API to get sentiment analysis insights
    from textual inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) and **natural language understanding**
    (**NLU**) are becoming ever more prominent in our daily lives, and researchers
    continue to find interesting new use cases almost every day. In this chapter,
    we’ll explore the simplest way to get powerful NLP/NLU functionality on Google
    Cloud by using the Google Cloud Natural Language API. In later chapters, we will
    build and use much more complex language use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with the Natural Language API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sentiment analysis allows us to get insights regarding the primary emotional
    tone of a piece of text. This is important for many business use cases, especially
    when it comes to connecting with, and understanding, your customers. For example,
    if you want to understand how customers feel about a new product you’ve released,
    you can analyze various customer interaction channels, such as reviews, social
    media reactions, and customer service center logs, to find out how people are
    reacting to the new product. This enables you to answer questions such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Are they happy with it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a pattern of complaints emerging about a specific feature?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing this kind of analysis manually would not be possible if you have thousands
    of reviews, social media reactions, and service center logs to process.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you can perform sentiment analysis on a piece of text with a simple
    API call to the Google Cloud Natural Language API.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, perform the following steps in Cloud Shell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a JSON file that will contain the piece of text that we want to analyze.
    Just like when we discussed Document AI, we can use the `nano` command to do this,
    which will create and open the file for editing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Paste the following text into the `request.json` file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Press *Ctrl* + *X* to exit `nano`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Y* to save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Enter* to confirm this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can send the request to the Natural Language API’s `analyzeSentiment`
    endpoint. To do that, we will use the following `curl` command. Note that we are
    using the `API_KEY` environment variable that we created earlier in this chapter.
    This will be used to authenticate our request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your response should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first thing we can see in the output is the overall document score, which
    is the score for the entire body of text. After that, we can see the scores for
    the individual sentences that were detected. The score of the sentiment ranges
    between -1.0 (negative) and 1.0 (positive), and the magnitude indicates the overall
    strength of emotion (both positive and negative) within the given text. You can
    learn more about the response fields and scores at [https://cloud.google.com/natural-language/docs/basics#sentiment_analysis_response_fields](https://cloud.google.com/natural-language/docs/basics#sentiment_analysis_response_fields).
  prefs: []
  type: TYPE_NORMAL
- en: In this case, both of the sentences constitute a positive review, so they both
    have high scores, and the overall document score is therefore also high.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If your scores are slightly different than those in the outputs, that’s because
    the models that serve these requests are constantly being updated with new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Retry all of the previous steps in this section, but add the following two
    sentences at the end of the review: “The only bad thing is that it’s just too
    expensive, and that really sucks. Very annoying!”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall review will then look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: “*This is the best soap I’ve ever used! It smells great, my skin feels amazing
    after using it, and my partner loves it too! The only bad thing is that it’s just
    too expensive, and that really sucks.* *Very annoying!*”
  prefs: []
  type: TYPE_NORMAL
- en: 'When you submit the updated request, the response output will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the output, notice that the negative sentences at the end have much lower
    scores, so this brings down the overall score for the entire review. This makes
    sense because the overall sentiment of the review is frustration, even though
    it starts with a positive sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Natural Language API also provides other types of functionality, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Entity analysis**: This involves identifying what kinds of entities (for
    example, people, places, and so on) are present in a piece of text. The supported
    entities are listed at [https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity#type](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity#type).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity sentiment analysis**: A combination of entity analysis and sentiment
    analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntactic analysis**: This involves inspecting the linguistic structure of
    a given piece of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content classification**: This involves categorizing the content of a document
    or piece of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To explore the Natural Language API in more detail, let’s take a look at one
    of its other features content classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying content with the Natural Language API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine that we want to build a search engine that can be used to search
    across large amounts of documents and content objects. One of the first things
    we will need to do is classify our documents and content objects into categories.
    Doing this manually on millions of objects would be impossible, or at least extremely
    laborious and error-prone. This is where the content classification feature of
    the Natural Language API can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate, we’re going to use the text that was produced by our Document
    AI OCR processor in the previous section of this chapter. This will also demonstrate
    an important concept, which is that you can combine multiple AI services to create
    more complex use cases to meet your business needs. In this case, not only can
    we classify regular text inputs, but we can detect text in images and then categorize
    the contents of that text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps in Cloud Shell:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a JSON file that we will use in our request to the API. This will contain
    the text from our OCR processor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Paste the following text into the `classify-request.json` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Press *Ctrl* + *X* to exit `nano`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Y* to save the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Enter* to confirm this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can send the request to the Natural Language API’s `classifyText` endpoint.
    To do that, we will use the following `curl` command. Note that we are using the
    `API_KEY` environment variable that we created earlier in this chapter. This will
    be used to authenticate our request:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your response should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That’s more like it! Note that the response contains entries for multiple potential
    categories, each with different levels of confidence.. This demonstrates two important
    realities for the Solution Architect role:'
  prefs: []
  type: TYPE_NORMAL
- en: While we want to automate everything as much as possible via AI/ML, it’s often
    necessary to have mechanisms for humans to review the model outputs and make corrections
    where needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI/ML workloads often consist of several steps in which data is passed from
    one step to the next, and it’s important to implement data quality checks at each
    stage in the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When using HITL reviews, we wouldn’t need to have a human review every data
    point (that would be impractical, and would negate the benefits of AI/ML), but
    we should have mechanisms that define what kinds of values we expect to see in
    a given use case, if possible, or flag our outputs for human review if our model’s
    confidence levels are below specified thresholds for some data points.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored how to use pre-trained models provided by Google, we’re
    going to look at the next level of complexity when it comes to implementing AI/ML
    workloads on Google Cloud, which is to train our own models in a managed way,
    using AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Using Vertex AI AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), we can use
    AutoML to automate all of the steps in the model training and evaluation process.
    In this section, we’re going to build an AutoML Model with Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the Vertex AutoML pricing at the following link before following the
    instructions in this section: [https://cloud.google.com/vertex-ai/pricing](https://cloud.google.com/vertex-ai/pricing).'
  prefs: []
  type: TYPE_NORMAL
- en: Use case – forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use case we will focus on for this workload is forecasting. After all, forecasting
    is one of the most fundamental business processes that almost all businesses have
    to perform in some form or other. For example, whether you own a global online
    retail company or just a single store in a small town, you will need to estimate
    how much of each product you should purchase each month, or perhaps each day,
    based on the expected customer demand for those items. And, forecasting is not
    just for physical goods. For example, if you owned a consulting or services company,
    you would need to estimate how many people you would need to hire to cater to
    the expected needs of your customers in the coming months. Being able to predict
    the future is a pretty important superpower, and in our case, we’re going to skip
    right the to “get-rich-quick” use case of forecasting stock market performance.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready to train your first ML model? Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Preparation – creating a BigQuery dataset for our prediction outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BigQuery is a useful tool for viewing and performing analytical queries on
    our prediction outputs. For this reason, we will create a BigQuery dataset to
    store the outputs from our AutoML tests. To create a dataset in BigQuery, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **BigQuery**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the top-left corner of the screen, you will see your project’s name. Click
    on the three vertical dots to the right of your project’s name (see *Figure 4**.9*
    for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9: BigQuery project menu](img/B18143_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: BigQuery project menu'
  prefs: []
  type: TYPE_NORMAL
- en: In the menu that’s displayed, select **Create dataset**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give your dataset a name, such as `forecast_test_dataset` (see *Figure* *4**.10*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select your preferred region, then select **Create dataset**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10: Creating a BigQuery dataset](img/B18143_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Creating a BigQuery dataset'
  prefs: []
  type: TYPE_NORMAL
- en: That’s it – we don’t need to do anything else in BigQuery for now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the AutoML workload
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, it’s time to define the AutoML job that’s going to automate all the steps
    in our data science projects. As a reminder, the steps in the model life cycle
    are shown in *Figure 4**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: ML model life cycle managed by AutoML](img/B18143_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: ML model life cycle managed by AutoML'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the first step in the process is to ingest some data. We need
    to create a dataset for this purpose. We’re going to use a public dataset from
    Kaggle, which is a useful resource for practicing ML concepts. Forecasting use
    cases generally require **time series** data, which is a sequence of data that
    is chronologically ordered according to time intervals. For this reason, our dataset
    will need to include a timestamp field. We will dive into how model training and
    deployment work later in this book, but for now, we’ll focus on how easily Vertex
    AI AutoML enables us to train a forecasting model, without the need for much or
    any AI/ML expertise. Note that Vertex AI AutoML can also be used for classification
    and regression use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will use a subset of the *DJIA 30 Stock Time Series* dataset,
    which can be found at [https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231](https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231).
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the data contains the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Date`: In yy-mm-dd format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Open`: The price of the stock at market open (this is NYSE data, so it’s all
    in USD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`High`: The highest price reached in the day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Low Close`: The lowest price reached in the day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Volume`: The number of shares traded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Name`: The stock’s ticker name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the clone of our GitHub repository that you created on your local machine
    earlier in this chapter, you will find a modified version of this file in a directory
    named `data`, which exists within the directory named `Chapter``0``4`. The name
    of the modified file is `automl-forecast-train-1002.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you should find the file at the following path on your local machine
    (the slashes will be reversed if you’re using Microsoft Windows): `[Location in
    which you cloned our` `GitHub repository]/``Chapter-04``/data/automl-forecast-train-1002.csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our dataset in Google Cloud, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **Vertex AI** → **Datasets**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `my-forecasting-dataset`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will also be asked to select a data type and objective. Select **Tabular**,
    and then select **Forecasting**. Your selections should look like what’s shown
    in *Figure 4**.12*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select your preferred region, then select **Create** (note that this must be
    the same region in which you created the BigQuery dataset in the previous section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12: Create dataset](img/B18143_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Create dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add data to the dataset. Select **Upload CSV files from your
    computer**, then click **Select files**. See *Figure 4**.13* for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.13: Add data to your dataset](img/B18143_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Add data to your dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the file from the following path on your local machine (the slashes
    will be reversed if you’re using Microsoft Windows):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the **Cloud Storage path** input field, select **Browse**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re going to create a new Cloud Storage bucket for this use case. To do this,
    click on the symbol in the top-right corner that looks like a bucket with a plus
    sign (![](img/Icon1.png)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give the bucket a unique name and select **Create** (you can leave all of the
    bucket configuration options at their default values).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are prompted to prevent public access, select **Confirm**. This prevents
    any items in your bucket from being made public, which is a best practice unless
    you specifically want to create publicly accessible content. In this example,
    we do not need to create publicly accessible content.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When your bucket has been created, click **Select** at the bottom of the screen.
    With that, your bucket is now the storage location for our training data. The
    resulting screen should look similar to what’s shown in *Figure 4**.14*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.14: Add data to your dataset](img/B18143_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Add data to your dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the file has been uploaded, you should see a message saying that the upload
    has been completed. Now, we’re ready to start training our model!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the details screen for your dataset, select the button that says **Train**
    **new model**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen that appears, select **AutoML (Default)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, enter the details for the AutoML workload, as follows (see *Figure 4**.15*
    for the final configuration details):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset name is automatically selected as the name for the workload. You
    can leave it at the default value, or change the name as you wish.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We’re going to try to predict the volume of the company’s stocks that will
    be sold on each date, so select `[project_name].[dataset_name].[table_name]`,
    where we have the following:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`project_name` is the name of your Google Cloud project (hint: remember seeing
    this in the BigQuery console when you were creating the BigQuery dataset).'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dataset_name` is the name of the BigQuery dataset you created; for example,
    `table_name` is the name of the table that will be created to store the test outputs.
    You can type any name here, and that name will be assigned to the table that will
    be created. *It’s important to note that this table will be created by our AutoML
    job, so it must NOT have been created manually in BigQuery before this point.
    Only the dataset must have been created; not the table within* *that dataset.*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you have entered the dataset details, click **Select**. The dataset path
    will appear in the **BigQuery path** input field:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15: Model details](img/B18143_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: Model details'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Training options** screen that appears, select the checkboxes next
    to the **Close**, **High**, **Low**, and **Open** column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you do that, four menus will appear in blue text near the top of the screen
    (see *Figure 4**.16* for reference). Click on the **Feature type** menu and select
    **Covariate**. This is a required step, and it indicates that these columns in
    the training dataset contain values that change over time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16: Training options](img/B18143_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: Training options'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the `1` in the input field (see *Figure 4**.17* for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17: Compute and pricing budget](img/B18143_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Compute and pricing budget'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Start training**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To monitor our training job, we can select **Training** from the menu on the
    left-hand side of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the status of our job changes to **Finished**, our model has been trained,
    and we can begin using it to get predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Due to the various steps in the AutoML process, and the concept of a “node hour,”
    the job may run for more than 1 hour.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained your first ML model on Google Cloud. Let’s
    take a look at some details regarding our model.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing model details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the status of our model training job changes to **Finished**, we can click
    on the name of the training job and see a lot of useful details regarding our
    model (see *Figure 4**.18* for reference). Along the top of the screen, we can
    see various performance metrics, such as **Mean Absolute Error** (**MAE**), **Mean
    Absolute Percentage Error** (**MAPE**), and others. You can click the question
    mark symbol next to each one to learn more about it. We will also discuss these
    metrics in more detail in later chapters in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Another useful piece of information is **Feature importance**, which shows us
    how much each input feature appears to influence the model outputs. This is very
    important for understanding how our model works.
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of how each of these features may influence the predicted sales
    volume of stocks? For example, when a stock price is low, do people buy more of
    that stock? If this is true, does it make sense that the **Low** feature – which
    represents the stock’s lowest price point each day – would be important in predicting
    how much of that stock would be sold on a given day?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18: Model metrics](img/B18143_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Model metrics'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to our model’s performance metrics, let’s take a look at some of
    the predictions our model made as part of the testing process during our AutoML
    workload execution.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing model predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To take a look at some of the model outputs that were generated by our AutoML
    job, we need to view the BigQuery table that we created to store those outputs.
    To do this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **BigQuery**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the top-left corner of the screen, click on your project’s name, then click
    on your dataset’s name, and then click on the table’s name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will see the schema of the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Preview** tab; a preview of the output data will be displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to the right; you will see columns named **predicted_Volume.value** and
    **predicted_on_Date**. These show us the prediction outputs from our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s important to note that we only allowed our job to run for 1 node hour,
    so our prediction values may not be accurate at this point. An AutoML job would
    usually need to run for a longer time to find the best model. This is something
    to bear in mind from a cost perspective. If your budget allows, try running the
    AutoML job for longer periods, and see how it affects the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use Google Cloud’s high-level AI/ML APIs
    to implement AI/ML functionality by using models that are trained and maintained
    by Google. Then, you moved on to train your own ML model using Vertex AI AutoML.
    All of this was performed with the help of fully managed services on Google Cloud.
    In the next chapter, and beyond, we’re going to dive in deeper, and you will build
    your own models from scratch so that you get to see how each of the steps in the
    model development life cycle works in much more detail.
  prefs: []
  type: TYPE_NORMAL
