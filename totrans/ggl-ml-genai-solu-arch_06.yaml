- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilizing Google Cloud’s High-Level AI Services
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have been equipped with an arsenal of information regarding AI/ML
    and Google Cloud, you are ready to start diving in and implementing AI/ML workloads
    in the cloud – that’s exactly what we will do in this chapter! You’re going to
    start by using Google Cloud’s high-level AI/ML APIs, such as the Natural Language
    API and the Vision API, which enable you to implement AI/ML functionality using
    models that are trained and maintained by Google. Then, you’ll use Vertex AI AutoML
    to train your own ML model, all without the need for any AI/ML expertise.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Using Document AI to extract information from documents
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Google Cloud Natural Language API to to get sentiment analysis insights
    from textual inputs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Vertex AI AutoML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites for this chapter
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to perform some setup steps to lay the foundations for the activities
    we will be performing in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Cloning this book’s GitHub repository to your local machine
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Throughout the rest of this book, we will perform a lot of hands-on activities
    that require the use of resources such as code, data, and other files. Many of
    these resources are stored in the GitHub repository associated with this book,
    so the easiest way to access them would be to clone the repository to your local
    machine (that is, your laptop/PC). The exact way to do this will differ based
    on the operating system you use, but the process is generally to open a command
    terminal on your system, navigate to a directory of your choice, and run the following
    commands:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The Google Cloud console
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Prerequisites for using *Google Cloud tools and services* section of
    [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), we discussed that you would need
    to create a Google Cloud account to interact with most Google Cloud services.
    We will use the Google Cloud console to perform many of the activities in this
    chapter. Once you have created a Google Cloud account, you can log into the console
    by navigating to [https://console.cloud.google.com](https://console.cloud.google.com).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this book, you will need to navigate to different services within
    the Google Cloud console. Whenever you need to do this, you can click on the symbol
    with three horizontal lines next to the Google Cloud logo in the top-left corner
    of the screen to view the menu of Google Cloud services or products (in this context,
    we’ll use the terms “services” and “products” interchangeably throughout this
    book). See *Figure 4**.1* for reference:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Accessing the Google Cloud services menu](img/B18143_04_1.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Accessing the Google Cloud services menu'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'You can then scroll through the list of services and select the relevant one.
    Some of the menu items have sub-menus nested under them, as shown in *Figure 4**.2*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Google Cloud services sub-menus](img/B18143_04_2.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Google Cloud services sub-menus'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will sometimes use shorthand notation to represent
    the navigation path through the services menus and sub-menus. For example, to
    navigate to the **Credentials** sub-menu item, we will represent this as the **Google
    Cloud services** menu → **APIs & Services** → **Credentials**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们有时会使用缩写符号来表示通过服务菜单和子菜单的导航路径。例如，要导航到 **凭证** 子菜单项，我们将表示为 **Google Cloud
    服务** 菜单 → **APIs & Services** → **Credentials**。
- en: Now that we’ve looked at how to access the Google Cloud console, let’s ensure
    that we have a Google Cloud project to use for the activities in this book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何访问 Google Cloud 控制台，让我们确保我们有一个 Google Cloud 项目用于本书的活动。
- en: Google Cloud project
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud 项目
- en: A Google Cloud project is an environment that contains all of your resources
    in Google Cloud. Resources include things such as virtual machines, network configurations,
    databases, storage buckets, identity and access controls, and pretty much everything
    that you create on Google Cloud.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Google Cloud 项目是一个包含您在 Google Cloud 中创建的所有资源的环境。资源包括虚拟机、网络配置、数据库、存储桶、身份和访问控制等几乎所有您在
    Google Cloud 中创建的内容。
- en: 'By default, when you create your Google Cloud account and log into the console
    for the first time, Google Cloud automatically creates your first project for
    you, with the name **My First Project**. If, for any reason, this has not happened,
    you will need to create one by following these steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当您创建 Google Cloud 账户并首次登录控制台时，Google Cloud 会自动为您创建第一个项目，项目名称为 **我的第一个项目**。如果由于任何原因，这还没有发生，您需要按照以下步骤创建一个：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **IAM & Admin** → **Create** **a Project**.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 控制台中，导航到 **Google Cloud 服务** 菜单 → **IAM & Admin** → **创建** **项目**。
- en: Type a name for your project in the **Project Name** text box.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **项目名称** 文本框中为您的项目输入一个名称。
- en: You could also edit the **Project ID** value if you’d like, but it’s more common
    to leave this at the generated value unless you have a specific naming convention
    that you’d like to implement. Note that this cannot be changed after the project
    is created.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以编辑 **项目 ID** 值，如果您想的话，但更常见的是将其保留为生成的值，除非您想实施特定的命名约定。请注意，项目创建后不能更改。
- en: In the **Location** field, click **Browse** to display potential locations for
    your project. Then, click **Select**.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **位置** 字段中，点击 **浏览** 以显示您项目的潜在位置。然后，点击 **选择**。
- en: Click **Create**. The console will navigate to the **Dashboard** page and your
    project will be created within a few minutes.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建**。控制台将导航到 **仪表板** 页面，您的项目将在几分钟内创建完成。
- en: Now that we have our Google Cloud project in place, let’s ensure that our billing
    details are set up for using Google Cloud.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了我们的 Google Cloud 项目，让我们确保我们的计费详细信息已设置好以使用 Google Cloud。
- en: Google Cloud Billing
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud 计费
- en: A Google Cloud Billing account contains all of the details that enable Google
    Cloud to bill you for using their services.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Google Cloud 计费账户包含所有使 Google Cloud 能够对您使用其服务进行计费的相关详细信息。
- en: 'If you don’t already have a Google Cloud Billing account, then you will need
    to create one by following these steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有 Google Cloud 计费账户，那么您需要按照以下步骤创建一个：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Billing**.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 控制台中，导航到 **Google Cloud 服务** 菜单→ **计费**。
- en: On the page that appears, select **Manage Billing Accounts**, then **Add** **billing
    account**.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的页面上，选择 **管理计费账户**，然后 **添加** **计费账户**。
- en: Click **Create account**.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建账户**。
- en: Enter a name for the Cloud Billing account.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为云计费账户输入一个名称。
- en: 'Depending on your configuration, you will also need to select one of the following:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的配置，您还需要选择以下选项之一：
- en: '**Organization**: If you see an **Organization** dropdown, then you must select
    an organization before you can continue'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组织**：如果您看到一个 **组织** 下拉菜单，那么在您继续之前必须选择一个组织'
- en: '**Country**: If you are prompted to select a **country**, select the country
    that corresponds with your billing mailing address'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国家**：如果您被提示选择一个 **国家**，请选择与您的计费邮寄地址相对应的国家'
- en: Click **Continue**.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **继续**。
- en: You will then need to fill in all of your billing details.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您需要填写所有您的计费详细信息。
- en: When you have entered all of your details, click **Submit and enable billing**.
    To learn more about Google Cloud Billing accounts and concepts, visit [https://cloud.google.com/billing/docs/how-to/create-billing-account](https://cloud.google.com/billing/docs/how-to/create-billing-account).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你已输入所有详细信息后，点击**提交并启用计费**。要了解更多关于Google Cloud计费账户和概念的信息，请访问[https://cloud.google.com/billing/docs/how-to/create-billing-account](https://cloud.google.com/billing/docs/how-to/create-billing-account)。
- en: Now that our Google Cloud project and Billing account have been set up, we’re
    ready to start using Google Cloud products in our project.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了Google Cloud项目和计费账户，我们就可以开始在我们的项目中使用Google Cloud产品了。
- en: Google Cloud Shell
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud Shell
- en: We will use Google Cloud Shell to perform some of the activities in this chapter.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Google Cloud Shell来完成本章的一些活动。
- en: 'Open Cloud Shell, as described in the *Interacting with Google Cloud services*
    section of [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059). As a reminder, you
    can access it by clicking the **Cloud Shell** symbol in the top-right corner of
    the screen, as shown in *Figure 4**.3*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Cloud Shell，如[*第3章*](B18143_03.xhtml#_idTextAnchor059)中“与Google Cloud服务交互”部分所述。提醒一下，您可以通过点击屏幕右上角的**Cloud
    Shell**符号来访问它，如图*图4**.3*所示：
- en: '![Figure 4.3: Cloud Shell symbol](img/B18143_04_3.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3：Cloud Shell符号](img/B18143_04_3.jpg)'
- en: 'Figure 4.3: Cloud Shell symbol'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：Cloud Shell符号
- en: Now that we’ve opened Cloud Shell, we’ll perform some basic setup steps to prepare
    our Cloud Shell environment.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经打开了Cloud Shell，我们将执行一些基本的设置步骤来准备我们的Cloud Shell环境。
- en: Authentication
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认证
- en: When a Google Cloud service is invoked in some way, it usually wants to know
    who or what (identity) is invoking it. This is for many reasons, such as billing
    and checking whether the invoker is allowed to perform the action that is being
    requested. This process of identification is called “authentication.” There are
    numerous different ways to authenticate with a Google Cloud API. For example,
    when you log into the Google Cloud console, you are authenticating with the console.
    Then, as you navigate and perform actions in the console, your authentication
    details are used to control what kinds of actions you are allowed to perform.
    One of the simplest authentication mechanisms is referred to as **API keys**.
    We’ll explore this next.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当以某种方式调用Google Cloud服务时，它通常想知道谁或什么（身份）在调用它。这有多种原因，例如计费和检查调用者是否有权执行所请求的操作。这种识别过程被称为“认证”。有无数种不同的方式可以通过Google
    Cloud API进行认证。例如，当您登录Google Cloud控制台时，您正在与控制台进行认证。然后，当您在控制台中导航和执行操作时，您的认证详细信息将用于控制您可以执行的操作类型。其中一种最简单的认证机制被称为**API密钥**。我们将接下来探讨这一点。
- en: Google Cloud API keys
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud API密钥
- en: API keys are a very basic type of authentication supported by some Google Cloud
    APIs. An API key does not provide any identification or authorization information
    to the called API; they are only generally used for billing purposes (by linking
    to a Google Cloud project) or to track usage against Google Cloud quotas.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: API密钥是某些Google Cloud API支持的一种非常基本的认证类型。API密钥不会向被调用的API提供任何标识或授权信息；它们通常仅用于计费目的（通过链接到Google
    Cloud项目）或用于跟踪对Google Cloud配额的使用情况。
- en: In this chapter, we will use them to access the Google Cloud Natural Language
    API.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用它们来访问Google Cloud自然语言API。
- en: Creating an API key
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建API密钥
- en: 'Although we will use Cloud Shell to perform many of the steps in this chapter,
    Google Cloud currently only supports creating API keys in the Google Cloud console.
    To create our API key, perform the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将使用Cloud Shell来完成本章中的许多步骤，但Google Cloud目前仅支持在Google Cloud控制台中创建API密钥。要创建我们的API密钥，请执行以下步骤：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **APIs & Services** → **Credentials**.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单 → **APIs & Services** → **Credentials**。
- en: Select **Create credentials**, then **API key**.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建凭据**，然后**API密钥**。
- en: Copy the generated API key and click **Close**.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制生成的API密钥并点击**关闭**。
- en: You will need to paste this API key in a later step. If you need to view or
    copy the key again, you can select **Show key** in the **API Keys** section of
    the console (that is, the section you are currently in).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将在后续步骤中需要粘贴此API密钥。如果您需要再次查看或复制密钥，您可以在控制台的**API Keys**部分中选择**显示密钥**（即您当前所在的区域）。
- en: With that, we have created an API key that we can use in later activities in
    this chapter.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们就创建了一个可以在本章后续活动中使用的API密钥。
- en: Enabling the relevant Google Cloud APIs
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用相关Google Cloud API
- en: 'As we discussed in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), before
    you can use a Google Cloud service API, you need to enable it. Since we are already
    logged into Cloud Shell, we can easily do that directly from here by using the
    `gcloud` command. We will enable the Google Cloud Natural Language API, the Vision
    API, the Document AI API, and the Google Cloud Storage API as we will be using
    all of them in this chapter. To do this, run the following commands in Cloud Shell:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [*第 3 章*](B18143_03.xhtml#_idTextAnchor059) 中讨论的那样，在您可以使用 Google Cloud
    服务 API 之前，您需要启用它。由于我们已经在 Cloud Shell 中登录，我们可以直接使用 `gcloud` 命令轻松完成此操作。我们将启用 Google
    Cloud Natural Language API、Vision API、Document AI API 和 Google Cloud Storage API，因为在本章中我们将使用所有这些
    API。为此，请在 Cloud Shell 中运行以下命令：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The responses should be similar to the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 响应应类似于以下内容：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With the required APIs now enabled, we just have a few more steps to complete
    before we can start using them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启用了所需的 API，我们只需再完成几个步骤就可以开始使用了。
- en: Storing authentication credentials in environment variables
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在环境变量中存储认证凭据
- en: We will reference our authentication credentials – that is, the API key that
    we created earlier – in many commands in this chapter. To make it easier to reference
    the credentials, we will store them in Linux environment variables in Cloud Shell.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的许多命令中引用我们的认证凭据——即我们之前创建的 API 密钥。为了便于引用凭据，我们将它们存储在 Cloud Shell 的 Linux
    环境变量中。
- en: 'To store the API key, run the following command, but replace `<YOUR_API_KEY>`
    with the API key you created and copied earlier:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要存储 API 密钥，请运行以下命令，但将 `<YOUR_API_KEY>` 替换为您之前创建并复制的 API 密钥：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’re almost finished with the environment setup steps now. Next, we will clone
    our GitHub repository, after which we’ll be ready to start using Google Cloud’s
    high-level AI services.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们几乎完成了环境设置步骤。接下来，我们将克隆我们的 GitHub 仓库，之后我们就可以开始使用 Google Cloud 的高级 AI 服务了。
- en: Creating a directory and cloning our GitHub repository
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建目录并克隆我们的 GitHub 仓库
- en: 'You already cloned our repository to your local machine. In this section, you
    will also clone it to your Cloud Shell environment. To do this, perform the following
    steps in Cloud Shell:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经将我们的仓库克隆到了您的本地机器上。在本节中，您也将将其克隆到您的 Cloud Shell 环境中。为此，请在 Cloud Shell 中执行以下步骤：
- en: 'Create a directory:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个目录：
- en: '[PRE4]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Change location to that directory:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将位置更改为该目录：
- en: '[PRE5]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Initiate the directory as a local `git` repository:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录初始化为本地 `git` 仓库：
- en: '[PRE6]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Clone the `git` repository that contains the code we will use in the Document
    AI section of this chapter:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆包含我们将在此章节的文档 AI 部分使用的代码的 `git` 仓库：
- en: '[PRE7]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have all of the required code copied to our Cloud Shell environment,
    it’s time to start executing it!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有必需的代码复制到我们的 Cloud Shell 环境中，是时候开始执行它了！
- en: Detecting text in images with the Cloud Vision API
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Cloud Vision API 在图像中检测文本
- en: We’re going to begin with a very simple example to show you just how easy it
    is to start using AI/ML services on Google Cloud. In just a few short steps, you
    will be able to extract text and associated metadata from an image and save that
    text in a file. This process is called **optical character** **recognition** (**OCR**).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个非常简单的例子开始，向您展示开始使用 Google Cloud 上的 AI/ML 服务是多么容易。只需几个简单的步骤，您就能从图像中提取文本和相关元数据，并将该文本保存到文件中。这个过程被称为
    **光学字符** **识别** (**OCR**)。
- en: 'To begin this process, download the following image to your computer (the image
    can also be found in the `Chapter``0``4/images` folder in the git repository you
    created above): [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-04/images/poem.png](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-04/images/poem.png).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始此过程，请将以下图像下载到您的计算机上（该图像也可以在您上面创建的 git 仓库的 `Chapter04/images` 文件夹中找到）：[https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-04/images/poem.png](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/blob/main/Chapter-04/images/poem.png)。
- en: 'The image is shown in *Figure 4**.4*:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图像显示在 **图 4.4** 中：
- en: '![Figure 4.4: Sign to be used for OCR](img/B18143_04_4.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4：用于 OCR 的签名](img/B18143_04_4.jpg)'
- en: 'Figure 4.4: Sign to be used for OCR'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：用于 OCR 的签名
- en: 'Continue with the remaining steps to upload this image to your Cloud Shell
    environment:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 继续执行剩余步骤，将此图像上传到您的 Cloud Shell 环境中：
- en: 'Click the symbol with the three dots at the top-right corner of the Cloud Shell
    area (at the bottom of your Google Cloud console screen), then select **Upload**.
    See *Figure 4**.5* for reference:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击云Shell区域右上角的带三个点的符号（位于你的Google Cloud控制台屏幕底部），然后选择**上传**。参见*图4**.5*以供参考：
- en: '![Figure 4.5: Cloud Shell upload](img/B18143_04_5.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5：云Shell上传](img/B18143_04_5.jpg)'
- en: 'Figure 4.5: Cloud Shell upload'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：云Shell上传
- en: 'You will be presented with a file upload dialogue box (see *Figure 4**.6* for
    reference). Select **Choose files** and browse for the file that you previously
    downloaded:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到一个文件上传对话框（参见*图4**.6*以供参考）。选择**选择文件**并浏览你之前下载的文件：
- en: '![Figure 4.6: Cloud Shell upload prompt](img/B18143_04_6.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6：云Shell上传提示](img/B18143_04_6.jpg)'
- en: 'Figure 4.6: Cloud Shell upload prompt'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：云Shell上传提示
- en: 'Now, invoke the Cloud Vision API to extract the text and save it in a file
    with the following command (in this case, the filename of the image we’re using
    is `poem.png`). The results are stored in a file named `vision-ocr.json`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令调用云视觉API提取文本并将其保存到以下文件中（在这种情况下，我们使用的图像文件名为`poem.png`）。结果存储在名为`vision-ocr.json`的文件中：
- en: '[PRE8]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Inspect the contents of the file:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查文件内容：
- en: '[PRE9]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The file is not conducive to being printed in a book, so we will not display
    it here.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件不适合在书中打印，所以我们在此不显示它。
- en: As you can see, each word that is extracted is stored with some associated metadata,
    such as the *X* and *Y* coordinates of the corners of the bounding boxes that
    contain that word in the image. This metadata helps us understand where each word
    appeared in the image, which can be useful information for developers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个提取的单词都存储了一些相关的元数据，例如包含该单词的图像中边界框的*X*和*Y*坐标。这些元数据帮助我们了解每个单词在图像中的位置，这对于开发者来说可能是有用的信息。
- en: 'If you just want to see the detected text, without the other metadata, you
    can use the following command:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你只想查看检测到的文本，而不需要其他元数据，可以使用以下命令：
- en: '[PRE10]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will see the entire body of detected text in the first description, and
    then individual words in the subsequent descriptions. The output will look similar
    to the following (truncated here for brevity):'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将在第一个描述中看到检测到的整个文本正文，然后在随后的描述中看到单个单词。输出将类似于以下内容（此处截断以节省空间）：
- en: '[PRE11]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Congratulations – you have just implemented your first AI/ML workload on Google
    Cloud! It really is that simple! Next, we’ll start looking at some slightly more
    advanced use cases.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你已经在Google Cloud上实现了你的第一个AI/ML工作负载！这真的非常简单！接下来，我们将开始探讨一些稍微复杂一些的应用案例。
- en: Using Document AI to extract information from documents
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文档AI从文档中提取信息
- en: As you may recall, we discussed Document AI at length in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059).
    We talked about how it goes beyond understanding the content of textual inputs
    and also incorporates structure. In this section, we’re going to take a look at
    how Document AI does this, and we’ll see it in action for some real-world use
    cases. We’ll start by covering some important Document AI concepts.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆，我们在[*第3章*](B18143_03.xhtml#_idTextAnchor059)中详细讨论了文档AI。我们讨论了它如何超越理解文本输入的内容，并且还结合了结构。在本节中，我们将探讨文档AI是如何做到这一点的，并将在一些实际应用案例中看到它的实际应用。我们将从介绍一些重要的文档AI概念开始。
- en: Document AI concepts
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档AI概念
- en: 'Document AI uses the concept of “processors” to process documents. There are
    three main types of processors that you can use:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 文档AI使用“处理器”的概念来处理文档。你可以使用以下三种主要类型的处理器：
- en: General processors
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用处理器
- en: Specialized processors
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专用处理器
- en: Custom processors
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义处理器
- en: General processors and specialized processors use models that are pre-trained
    by Google, so you can use them without needing to train your own models, and without
    any AI/ML expertise. However, to provide additional flexibility, Google Cloud
    gives you the ability to further train specialized processors with your data to
    improve their accuracy for use cases that are specific to your business. This
    process is referred to as “uptraining.”
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通用处理器和专用处理器使用由Google预先训练的模型，因此你可以使用它们而无需训练自己的模型，也不需要任何AI/ML专业知识。然而，为了提供额外的灵活性，Google
    Cloud允许你使用自己的数据进一步训练专用处理器，以提高它们针对特定业务用例的准确性。这个过程被称为“再训练”。
- en: General processors include a processor for performing OCR on images of text
    documents and forms. While we performed OCR using the Cloud Vision API in the
    previous section, Document AI provides additional functionality, such as identifying
    structured information within the input documents. This includes understanding
    key-value pairs in input forms, which can be very useful for automated data entry
    use cases.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通用处理器包括对文本文档和表格图像执行OCR的处理器。虽然我们在上一节中使用了云视觉API进行OCR，但文档AI提供了额外的功能，例如识别输入文档中的结构化信息。这包括理解输入表单中的键值对，这对于自动化数据录入用例非常有用。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This is an important consideration in the Solution Architect role; there are
    often multiple different services that could be used to achieve similar business
    goals, and the Solution Architect’s job is to create or select the most suitable
    solution based on specific business requirements. In the case of selecting between
    the Cloud Vision API and Document AI for OCR use cases, bear in mind that the
    Cloud Vision API generally requires less initial effort to use, but it also provides
    fewer features than Document AI. Therefore, if you have a very simple use case
    that can be satisfied with the Cloud Vision API, then you can select that option,
    whereas more complex use cases would steer you toward using Document AI.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决方案架构师的角色中，这是一个重要的考虑因素；通常有多个不同的服务可以实现类似的企业目标，解决方案架构师的工作是根据特定的业务需求创建或选择最合适的解决方案。在比较云视觉API和文档AI用于OCR用例时，请记住，云视觉API通常使用起来需要更少的初始努力，但它提供的功能比文档AI要少。因此，如果你有一个非常简单的用例，可以用云视觉API满足，那么你可以选择该选项，而对于更复杂的用例，则应引导你使用文档AI。
- en: 'Specialized processors provide models for processing specific types of documents,
    such as US Federal Government forms, identification documents, and invoices. The
    specialized processors are currently categorized into four different types:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 专用处理器为处理特定类型的文档提供模型，例如美国联邦政府表格、身份证件和发票。专用处理器目前分为四种不同类型：
- en: Procurement
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采购
- en: Identity
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 身份
- en: Lending
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 借款
- en: Contract
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合同
- en: At the time of writing (March 2023), Google Cloud recently announced the general
    availability of new functionality in Document AI, which is Document AI Workbench.
    Document AI Workbench enables you to create completely new, custom types of processors,
    beyond what’s provided by Google Cloud, for your specific use cases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时（2023年3月），谷歌云最近宣布了文档AI新功能的通用可用性，即文档AI工作台。文档AI工作台允许你为特定的用例创建完全新的、定制的处理器类型，这超出了谷歌云提供的内容。
- en: Let’s take a look at how some of the Document AI processors work.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些文档AI处理器是如何工作的。
- en: Performing OCR with Document AI
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用文档AI进行OCR
- en: We’re going to use the simplest use case, OCR, for demonstration purposes, which
    will also allow us to directly contrast this functionality against how we performed
    OCR with the Cloud Vision API.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用最简单的用例OCR进行演示，这还将使我们能够直接对比这种功能与我们在上一节中使用云视觉API进行OCR的方式。
- en: Creating a Document AI OCR processor
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建文档AI OCR处理器
- en: 'Before we can process any documents, we need to create a processor. We’re going
    to perform this action in the Google Cloud console:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够处理任何文档之前，我们需要创建一个处理器。我们将在这个谷歌云控制台中执行此操作：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **Document AI** → **Processor Gallery**.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在谷歌云控制台中，导航到**谷歌云服务**菜单 → **文档AI** → **处理器库**。
- en: 'Under **Document OCR**, click **CREATE PROCESSOR**. See *Figure 4**.7* for
    reference:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**文档OCR**下点击**创建处理器**。参见*图4**.7*以获取参考：
- en: '![Figure 4.7: CREATE PROCESSOR](img/B18143_04_7.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7：创建处理器](img/B18143_04_7.jpg)'
- en: 'Figure 4.7: CREATE PROCESSOR'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：创建处理器
- en: Input **OCR-Processor** as the name, select the region nearest to you, and click
    **Create**.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**OCR-Processor**作为名称输入，选择离你最近的地域，然后点击**创建**。
- en: A note on regions and zones
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 关于地域和区域的说明
- en: For this book, whenever you need to select a region, we recommend selecting
    the same region every time since we will build resources throughout this book
    that may need to be used in combination with other resources created in different
    chapters of this book. Generally, when you need to use some cloud resources with
    other cloud resources, it’s often much easier if those resources are in the same
    region. Sometimes, it can be difficult or impossible for resources in one region
    to reference resources in a different region without building a customized solution
    to do so.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，每次您需要选择一个区域时，我们建议每次都选择相同的区域，因为我们将在这本书的整个过程中构建资源，这些资源可能需要与其他章节中创建的其他资源一起使用。通常，当您需要使用一些云资源与其他云资源一起使用时，如果这些资源在同一个区域，通常会更简单。有时，可能很难或不可能在没有构建定制解决方案的情况下，让一个区域内的资源引用另一个区域内的资源。
- en: In some activities in this book, you will also have the option of selecting
    a specific zone within a region. This decision is usually less important for the
    activities in this book because accessing resources across zones within a region
    is generally quite easy. In a production environment, you may want to keep resources
    in the same zone if you have specific business requirements, such as clusters
    of compute instances that need to work very closely together.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的一些活动中，您还可以选择在区域内选择一个特定的区域。这个决定对于本书中的活动通常不太重要，因为在一个区域内跨区域访问资源通常相当容易。在生产环境中，如果您有特定的业务需求，例如需要非常紧密协作的计算实例集群，您可能希望将资源保留在同一个区域。
- en: 'If you don’t have such specific requirements, then the best practice is to
    distribute your resources across multiple zones to improve workload resilience
    and availability. You can learn more about Google Cloud regions and zones in the
    documentation at the following URL: [https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有这样的特定要求，那么最佳实践是将您的资源分布在多个区域以提高工作负载的弹性和可用性。您可以在以下 URL 的文档中了解更多关于 Google
    Cloud 区域和区域的信息：[https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones)。
- en: 'When your processor gets created, you should automatically be redirected to
    the **PROCESSOR DETAILS** page. If not, you can view the details by selecting
    **My processors** in the menu on the left-hand side of the screen and then selecting
    your newly created processor. The processor details are shown in *Figure 4**.8*:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您的处理器创建时，您应该自动重定向到 **处理器详细信息** 页面。如果不是，您可以通过在屏幕左侧菜单中选择 **我的处理器** 然后选择您新创建的处理器来查看详细信息。处理器详细信息显示在
    *图 4**.8* 中：
- en: '![Figure 4.8: OCR processor details](img/B18143_04_8.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8：OCR 处理器详细信息](img/B18143_04_8.jpg)'
- en: 'Figure 4.8: OCR processor details'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：OCR 处理器详细信息
- en: Take note of your processor ID (highlighted by the red arrow in *Figure 4**.8*)
    as you will need to reference it in the code we will use in the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 记下您的处理器 ID（由图 4**.8** 中的红色箭头突出显示），您将在下一节中使用的代码中引用它。
- en: Invoking the OCR processor
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调用 OCR 处理器
- en: Now that we’ve created our processor, we can start using it. In this section,
    we will run a simple piece of Python code that uses the Document AI Python client
    library to process a file and extract the necessary information. We’re going to
    use the same file that we used with the Cloud Vision API earlier in this chapter.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的处理器，我们可以开始使用它了。在本节中，我们将运行一段简单的 Python 代码，该代码使用 Document AI Python
    客户端库处理文件并提取必要的信息。我们将使用本章前面与 Cloud Vision API 一起使用的相同文件。
- en: 'Perform the following steps in Cloud Shell:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Cloud Shell 中执行以下步骤：
- en: 'Install the latest version of the Document AI client packages:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Document AI 客户端包的最新版本：
- en: '[PRE12]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Take note of your project ID as you will need to reference it in the Python
    code that will be used to interact with Document AI. You can use the following
    command to view it:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记下您的项目 ID，因为您将在与 Document AI 交互的 Python 代码中引用它。您可以使用以下命令查看它：
- en: '[PRE13]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Ensure that you are in your home directory:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您在您的家目录中：
- en: '[PRE14]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Copy the `docai-ocr.py` file to your home directory from the `git` directory
    we created earlier. We’re going to edit this file outside of the branch because
    we won’t need to merge the updates back to the main branch:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `docai-ocr.py` 文件从我们之前创建的 `git` 目录复制到您的家目录。我们将在这个分支之外编辑这个文件，因为我们不需要将更新合并回主分支：
- en: '[PRE15]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this Python file, we’ll begin by importing the required libraries. Then,
    we’ll define some variables (we will need to replace the values of those variables
    with the values of the processor we created in the previous section).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Python文件中，我们将首先导入所需的库。然后，我们将定义一些变量（我们需要用上一节中创建的处理器的值来替换这些变量的值）。
- en: Then, we’ll define a function that creates a Document AI client, reads the input
    file, creates a request object based on the file’s contents, and sends this request
    to be processed by the Document AI service.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将定义一个函数，该函数创建一个Document AI客户端，读取输入文件，根据文件内容创建一个请求对象，并将此请求发送给Document AI服务进行处理。
- en: Finally, the function will print the resulting document text.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，函数将打印生成的文档文本。
- en: 'To perform these actions, execute the following steps:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这些操作，请执行以下步骤：
- en: 'Use `nano` to open the file for editing:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nano`打开文件进行编辑：
- en: '[PRE16]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Replace the variable definitions with the values you saved after creating the
    processor in the Google Cloud console:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将变量定义替换为在Google Cloud控制台中创建处理器后保存的值：
- en: Replace the `project_id` value with your project ID, which you viewed in the
    `$DEVSHELL_PROJECT_ID` environment variable previously.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`project_id`的值替换为您之前在`$DEVSHELL_PROJECT_ID`环境变量中查看的项目ID。
- en: Specify ‘`us'` or `'eu'` as the location, depending on which region you used.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您使用的区域指定`'us'`或`'eu'`作为位置。
- en: Replace the `processor_id` value with your processor ID, which you can view
    on the `file_path` value with the path to the file we want to process. The Cloud
    Shell upload feature that we used generally stores files in your Cloud Shell home
    directory, which is usually `/home/admin_.`
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`processor_id`的值替换为您可以在包含我们想要处理的文件路径的`file_path`值中查看的处理器ID。我们使用的Cloud Shell上传功能通常将文件存储在您的Cloud
    Shell主目录中，通常是`/home/admin_`。
- en: In this case, the `mime_type` value is `'image/``png``'`. You can view the list
    of supported MIME types at [https://cloud.google.com/document-ai/docs/file-types](https://cloud.google.com/document-ai/docs/file-types).
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，`mime_type`的值是`'image/png'`。您可以在[https://cloud.google.com/document-ai/docs/file-types](https://cloud.google.com/document-ai/docs/file-types)查看支持的MIME类型列表。
- en: When you have completed all of the edits, press *Ctrl* + *X* to exit `nano`,
    then *Y* to save the file, and then *Enter* to confirm this.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您完成所有编辑后，按*Ctrl* + *X*退出`nano`，然后按*Y*保存文件，最后按*Enter*确认。
- en: 'Now, we’re ready to execute OCR! The following command will run our Python
    function and save the results in a file named `docai-ocr.txt`:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备执行OCR！以下命令将运行我们的Python函数，并将结果保存到名为`docai-ocr.txt`的文件中：
- en: '[PRE17]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Inspect the contents of the file:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查文件内容：
- en: '[PRE18]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The response should look like this:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 响应应如下所示：
- en: '[PRE19]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note how the output is formatted differently than the results we received when
    we used the Cloud Vision API. This further demonstrates how we can use different
    services to achieve similar results. However, there are subtle differences between
    the services, and we need to pick the best option based on business requirements.
    The Cloud Vision API required much less initial effort to use, but bear in mind
    that Document AI has much more powerful features, such as automated document processing,
    and customized models for specific use cases.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意输出格式与使用Cloud Vision API时收到的结果不同。这进一步证明了我们可以使用不同的服务来实现类似的结果。然而，这些服务之间存在细微的差异，我们需要根据业务需求选择最佳选项。Cloud
    Vision API的使用初始工作量较小，但请记住，Document AI具有更强大的功能，例如自动化文档处理和针对特定用例的定制模型。
- en: Let’s take a look at some of Document AI’s additional features.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看Document AI的一些附加功能。
- en: Document AI’s response
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Document AI的响应
- en: In our OCR example, we focused on the `document.txt` object in the response.
    However, the full response that is returned contains a lot more information that
    we can use in various ways, such as the number of pages, paragraphs, and lines
    in the document, and many other types of metadata. When using form parsers or
    specialized processors, it can even highlight structured data types such as tables
    and key-value pairs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的OCR示例中，我们关注了响应中的`document.txt`对象。然而，返回的完整响应包含更多信息，我们可以以各种方式使用这些信息，例如文档中的页数、段落和行数，以及许多其他类型的元数据。当使用表单解析器或专用处理器时，甚至可以突出显示结构化数据类型，如表格和键值对。
- en: It should also be noted that in the preceding example, we performed an online
    inference request, in which we got a response in real time for a single document.
    Document AI also allows us to perform batch inference requests if we need to process
    large numbers of documents at a time.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Human in the loop (HITL) AI
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding use case, the model was able to identify all the words in the
    image. However, in reality, the sources of our images may not always be clearly
    legible. For example, we may need to read information from pictures of road signs
    that could be worn in places. This is something to bear in mind when thinking
    about how accurate you need the results to be. Document AI provides a **HITL**)
    feature to enable you to improve the accuracy of the results by having a human
    review and make updates where necessary.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Using the Google Cloud Natural Language API to get sentiment analysis insights
    from textual inputs
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) and **natural language understanding**
    (**NLU**) are becoming ever more prominent in our daily lives, and researchers
    continue to find interesting new use cases almost every day. In this chapter,
    we’ll explore the simplest way to get powerful NLP/NLU functionality on Google
    Cloud by using the Google Cloud Natural Language API. In later chapters, we will
    build and use much more complex language use cases.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis with the Natural Language API
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sentiment analysis allows us to get insights regarding the primary emotional
    tone of a piece of text. This is important for many business use cases, especially
    when it comes to connecting with, and understanding, your customers. For example,
    if you want to understand how customers feel about a new product you’ve released,
    you can analyze various customer interaction channels, such as reviews, social
    media reactions, and customer service center logs, to find out how people are
    reacting to the new product. This enables you to answer questions such as the
    following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Are they happy with it?
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a pattern of complaints emerging about a specific feature?
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing this kind of analysis manually would not be possible if you have thousands
    of reviews, social media reactions, and service center logs to process.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you can perform sentiment analysis on a piece of text with a simple
    API call to the Google Cloud Natural Language API.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, perform the following steps in Cloud Shell:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a JSON file that will contain the piece of text that we want to analyze.
    Just like when we discussed Document AI, we can use the `nano` command to do this,
    which will create and open the file for editing:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Paste the following text into the `request.json` file:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Press *Ctrl* + *X* to exit `nano`.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Y* to save the file.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Enter* to confirm this.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can send the request to the Natural Language API’s `analyzeSentiment`
    endpoint. To do that, we will use the following `curl` command. Note that we are
    using the `API_KEY` environment variable that we created earlier in this chapter.
    This will be used to authenticate our request:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Your response should look like this:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first thing we can see in the output is the overall document score, which
    is the score for the entire body of text. After that, we can see the scores for
    the individual sentences that were detected. The score of the sentiment ranges
    between -1.0 (negative) and 1.0 (positive), and the magnitude indicates the overall
    strength of emotion (both positive and negative) within the given text. You can
    learn more about the response fields and scores at [https://cloud.google.com/natural-language/docs/basics#sentiment_analysis_response_fields](https://cloud.google.com/natural-language/docs/basics#sentiment_analysis_response_fields).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: In this case, both of the sentences constitute a positive review, so they both
    have high scores, and the overall document score is therefore also high.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: If your scores are slightly different than those in the outputs, that’s because
    the models that serve these requests are constantly being updated with new data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Retry all of the previous steps in this section, but add the following two
    sentences at the end of the review: “The only bad thing is that it’s just too
    expensive, and that really sucks. Very annoying!”'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall review will then look as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: “*This is the best soap I’ve ever used! It smells great, my skin feels amazing
    after using it, and my partner loves it too! The only bad thing is that it’s just
    too expensive, and that really sucks.* *Very annoying!*”
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'When you submit the updated request, the response output will look as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the output, notice that the negative sentences at the end have much lower
    scores, so this brings down the overall score for the entire review. This makes
    sense because the overall sentiment of the review is frustration, even though
    it starts with a positive sentiment.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The Natural Language API also provides other types of functionality, such as
    the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '**Entity analysis**: This involves identifying what kinds of entities (for
    example, people, places, and so on) are present in a piece of text. The supported
    entities are listed at [https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity#type](https://cloud.google.com/natural-language/docs/reference/rest/v1/Entity#type).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entity sentiment analysis**: A combination of entity analysis and sentiment
    analysis.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntactic analysis**: This involves inspecting the linguistic structure of
    a given piece of text.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content classification**: This involves categorizing the content of a document
    or piece of text.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To explore the Natural Language API in more detail, let’s take a look at one
    of its other features content classification.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Classifying content with the Natural Language API
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine that we want to build a search engine that can be used to search
    across large amounts of documents and content objects. One of the first things
    we will need to do is classify our documents and content objects into categories.
    Doing this manually on millions of objects would be impossible, or at least extremely
    laborious and error-prone. This is where the content classification feature of
    the Natural Language API can be useful.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate, we’re going to use the text that was produced by our Document
    AI OCR processor in the previous section of this chapter. This will also demonstrate
    an important concept, which is that you can combine multiple AI services to create
    more complex use cases to meet your business needs. In this case, not only can
    we classify regular text inputs, but we can detect text in images and then categorize
    the contents of that text.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps in Cloud Shell:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a JSON file that we will use in our request to the API. This will contain
    the text from our OCR processor:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Paste the following text into the `classify-request.json` file:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Press *Ctrl* + *X* to exit `nano`.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Y* to save the file.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press *Enter* to confirm this.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we can send the request to the Natural Language API’s `classifyText` endpoint.
    To do that, we will use the following `curl` command. Note that we are using the
    `API_KEY` environment variable that we created earlier in this chapter. This will
    be used to authenticate our request:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Your response should look like this:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'That’s more like it! Note that the response contains entries for multiple potential
    categories, each with different levels of confidence.. This demonstrates two important
    realities for the Solution Architect role:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: While we want to automate everything as much as possible via AI/ML, it’s often
    necessary to have mechanisms for humans to review the model outputs and make corrections
    where needed.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI/ML workloads often consist of several steps in which data is passed from
    one step to the next, and it’s important to implement data quality checks at each
    stage in the process.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: When using HITL reviews, we wouldn’t need to have a human review every data
    point (that would be impractical, and would negate the benefits of AI/ML), but
    we should have mechanisms that define what kinds of values we expect to see in
    a given use case, if possible, or flag our outputs for human review if our model’s
    confidence levels are below specified thresholds for some data points.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored how to use pre-trained models provided by Google, we’re
    going to look at the next level of complexity when it comes to implementing AI/ML
    workloads on Google Cloud, which is to train our own models in a managed way,
    using AutoML.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Using Vertex AI AutoML
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), we can use
    AutoML to automate all of the steps in the model training and evaluation process.
    In this section, we’re going to build an AutoML Model with Vertex AI.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the Vertex AutoML pricing at the following link before following the
    instructions in this section: [https://cloud.google.com/vertex-ai/pricing](https://cloud.google.com/vertex-ai/pricing).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Use case – forecasting
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use case we will focus on for this workload is forecasting. After all, forecasting
    is one of the most fundamental business processes that almost all businesses have
    to perform in some form or other. For example, whether you own a global online
    retail company or just a single store in a small town, you will need to estimate
    how much of each product you should purchase each month, or perhaps each day,
    based on the expected customer demand for those items. And, forecasting is not
    just for physical goods. For example, if you owned a consulting or services company,
    you would need to estimate how many people you would need to hire to cater to
    the expected needs of your customers in the coming months. Being able to predict
    the future is a pretty important superpower, and in our case, we’re going to skip
    right the to “get-rich-quick” use case of forecasting stock market performance.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready to train your first ML model? Let’s dive in!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Preparation – creating a BigQuery dataset for our prediction outputs
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BigQuery is a useful tool for viewing and performing analytical queries on
    our prediction outputs. For this reason, we will create a BigQuery dataset to
    store the outputs from our AutoML tests. To create a dataset in BigQuery, perform
    the following steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **BigQuery**.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the top-left corner of the screen, you will see your project’s name. Click
    on the three vertical dots to the right of your project’s name (see *Figure 4**.9*
    for reference):'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9: BigQuery project menu](img/B18143_04_9.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: BigQuery project menu'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: In the menu that’s displayed, select **Create dataset**.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give your dataset a name, such as `forecast_test_dataset` (see *Figure* *4**.10*).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select your preferred region, then select **Create dataset**:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.10: Creating a BigQuery dataset](img/B18143_04_10.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: Creating a BigQuery dataset'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: That’s it – we don’t need to do anything else in BigQuery for now.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the AutoML workload
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, it’s time to define the AutoML job that’s going to automate all the steps
    in our data science projects. As a reminder, the steps in the model life cycle
    are shown in *Figure 4**.11*:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: ML model life cycle managed by AutoML](img/B18143_04_11.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: ML model life cycle managed by AutoML'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the first step in the process is to ingest some data. We need
    to create a dataset for this purpose. We’re going to use a public dataset from
    Kaggle, which is a useful resource for practicing ML concepts. Forecasting use
    cases generally require **time series** data, which is a sequence of data that
    is chronologically ordered according to time intervals. For this reason, our dataset
    will need to include a timestamp field. We will dive into how model training and
    deployment work later in this book, but for now, we’ll focus on how easily Vertex
    AI AutoML enables us to train a forecasting model, without the need for much or
    any AI/ML expertise. Note that Vertex AI AutoML can also be used for classification
    and regression use cases.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will use a subset of the *DJIA 30 Stock Time Series* dataset,
    which can be found at [https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231](https://www.kaggle.com/datasets/szrlee/stock-time-series-20050101-to-20171231).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, the data contains the following fields:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '`Date`: In yy-mm-dd format'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Open`: The price of the stock at market open (this is NYSE data, so it’s all
    in USD)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`High`: The highest price reached in the day'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Low Close`: The lowest price reached in the day'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Volume`: The number of shares traded'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Name`: The stock’s ticker name'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the clone of our GitHub repository that you created on your local machine
    earlier in this chapter, you will find a modified version of this file in a directory
    named `data`, which exists within the directory named `Chapter``0``4`. The name
    of the modified file is `automl-forecast-train-1002.csv`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you should find the file at the following path on your local machine
    (the slashes will be reversed if you’re using Microsoft Windows): `[Location in
    which you cloned our` `GitHub repository]/``Chapter-04``/data/automl-forecast-train-1002.csv`.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'To create our dataset in Google Cloud, perform the following steps:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **Vertex AI** → **Datasets**.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `my-forecasting-dataset`.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will also be asked to select a data type and objective. Select **Tabular**,
    and then select **Forecasting**. Your selections should look like what’s shown
    in *Figure 4**.12*.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select your preferred region, then select **Create** (note that this must be
    the same region in which you created the BigQuery dataset in the previous section):'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12: Create dataset](img/B18143_04_12.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: Create dataset'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add data to the dataset. Select **Upload CSV files from your
    computer**, then click **Select files**. See *Figure 4**.13* for reference:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.13: Add data to your dataset](img/B18143_04_13.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: Add data to your dataset'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the file from the following path on your local machine (the slashes
    will be reversed if you’re using Microsoft Windows):'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the **Cloud Storage path** input field, select **Browse**.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re going to create a new Cloud Storage bucket for this use case. To do this,
    click on the symbol in the top-right corner that looks like a bucket with a plus
    sign (![](img/Icon1.png)).
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give the bucket a unique name and select **Create** (you can leave all of the
    bucket configuration options at their default values).
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are prompted to prevent public access, select **Confirm**. This prevents
    any items in your bucket from being made public, which is a best practice unless
    you specifically want to create publicly accessible content. In this example,
    we do not need to create publicly accessible content.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When your bucket has been created, click **Select** at the bottom of the screen.
    With that, your bucket is now the storage location for our training data. The
    resulting screen should look similar to what’s shown in *Figure 4**.14*:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.14: Add data to your dataset](img/B18143_04_14.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: Add data to your dataset'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Select **Continue**.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the file has been uploaded, you should see a message saying that the upload
    has been completed. Now, we’re ready to start training our model!
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the details screen for your dataset, select the button that says **Train**
    **new model**.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen that appears, select **AutoML (Default)**.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, enter the details for the AutoML workload, as follows (see *Figure 4**.15*
    for the final configuration details):'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset name is automatically selected as the name for the workload. You
    can leave it at the default value, or change the name as you wish.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We’re going to try to predict the volume of the company’s stocks that will
    be sold on each date, so select `[project_name].[dataset_name].[table_name]`,
    where we have the following:'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`project_name` is the name of your Google Cloud project (hint: remember seeing
    this in the BigQuery console when you were creating the BigQuery dataset).'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dataset_name` is the name of the BigQuery dataset you created; for example,
    `table_name` is the name of the table that will be created to store the test outputs.
    You can type any name here, and that name will be assigned to the table that will
    be created. *It’s important to note that this table will be created by our AutoML
    job, so it must NOT have been created manually in BigQuery before this point.
    Only the dataset must have been created; not the table within* *that dataset.*'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you have entered the dataset details, click **Select**. The dataset path
    will appear in the **BigQuery path** input field:'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15: Model details](img/B18143_04_15.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: Model details'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Select **Continue**.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Training options** screen that appears, select the checkboxes next
    to the **Close**, **High**, **Low**, and **Open** column names.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you do that, four menus will appear in blue text near the top of the screen
    (see *Figure 4**.16* for reference). Click on the **Feature type** menu and select
    **Covariate**. This is a required step, and it indicates that these columns in
    the training dataset contain values that change over time:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16: Training options](img/B18143_04_16.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: Training options'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Select **Continue**.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the `1` in the input field (see *Figure 4**.17* for reference):'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17: Compute and pricing budget](img/B18143_04_17.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Compute and pricing budget'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Select **Start training**.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To monitor our training job, we can select **Training** from the menu on the
    left-hand side of the screen.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the status of our job changes to **Finished**, our model has been trained,
    and we can begin using it to get predictions.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Due to the various steps in the AutoML process, and the concept of a “node hour,”
    the job may run for more than 1 hour.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained your first ML model on Google Cloud. Let’s
    take a look at some details regarding our model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Viewing model details
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the status of our model training job changes to **Finished**, we can click
    on the name of the training job and see a lot of useful details regarding our
    model (see *Figure 4**.18* for reference). Along the top of the screen, we can
    see various performance metrics, such as **Mean Absolute Error** (**MAE**), **Mean
    Absolute Percentage Error** (**MAPE**), and others. You can click the question
    mark symbol next to each one to learn more about it. We will also discuss these
    metrics in more detail in later chapters in this book.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Another useful piece of information is **Feature importance**, which shows us
    how much each input feature appears to influence the model outputs. This is very
    important for understanding how our model works.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of how each of these features may influence the predicted sales
    volume of stocks? For example, when a stock price is low, do people buy more of
    that stock? If this is true, does it make sense that the **Low** feature – which
    represents the stock’s lowest price point each day – would be important in predicting
    how much of that stock would be sold on a given day?
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18: Model metrics](img/B18143_04_18.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Model metrics'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: In addition to our model’s performance metrics, let’s take a look at some of
    the predictions our model made as part of the testing process during our AutoML
    workload execution.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Viewing model predictions
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To take a look at some of the model outputs that were generated by our AutoML
    job, we need to view the BigQuery table that we created to store those outputs.
    To do this, perform the following steps:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **BigQuery**.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the top-left corner of the screen, click on your project’s name, then click
    on your dataset’s name, and then click on the table’s name.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will see the schema of the table.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Preview** tab; a preview of the output data will be displayed.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll to the right; you will see columns named **predicted_Volume.value** and
    **predicted_on_Date**. These show us the prediction outputs from our model.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s important to note that we only allowed our job to run for 1 node hour,
    so our prediction values may not be accurate at this point. An AutoML job would
    usually need to run for a longer time to find the best model. This is something
    to bear in mind from a cost perspective. If your budget allows, try running the
    AutoML job for longer periods, and see how it affects the model’s performance.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use Google Cloud’s high-level AI/ML APIs
    to implement AI/ML functionality by using models that are trained and maintained
    by Google. Then, you moved on to train your own ML model using Vertex AI AutoML.
    All of this was performed with the help of fully managed services on Google Cloud.
    In the next chapter, and beyond, we’re going to dive in deeper, and you will build
    your own models from scratch so that you get to see how each of the steps in the
    model development life cycle works in much more detail.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
