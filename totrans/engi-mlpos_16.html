<html><head></head><body>
		<div id="_idContainer201">
			<h1 id="_idParaDest-202"><a id="_idTextAnchor234"/>Chapter 13: Governing the ML System for Continual Learning</h1>
			<p><a id="_idTextAnchor235"/>In this chapter, we will reflect on the need for continual learning in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) solutions. Adaptation is at the core of machine intelligence. The better the adaptation, the better the system. Continual learning focuses on the external environment and adapts to it. Enabling continual learning for an ML system can reap great benefits. We will look at what is needed to successfully govern an ML system as we explore continuous learning and study the governance component of the Explainable Monitoring Framework, which helps us control and govern ML systems to achieve maximum value. </p>
			<p>We will delve into the hands-on implementation of governance by enabling alert and action features. Next, we will look into ways of assuring quality for models and controlling deployments, and we'll learn the best practices to generate model audits and reports. Lastly, we will learn about methods to enable model retraining and maintain CI/CD pipelines.</p>
			<p>Let's start by reflecting on the need for continual learning and go on to explore the following topics in the chapter:</p>
			<ul>
				<li>Understanding the need for continual learning</li>
				<li>Governing an ML system using Explainable Monitoring</li>
				<li>Enabling model retraining</li>
				<li>Maintaining the CI/CD pipeline</li>
			</ul>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor236"/>Understanding the need for continual learning </h1>
			<p>When we got <a id="_idIndexMarker854"/>started in <a href="B16572_01_Final_JM_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Fundamentals of MLOps Workflow</em>, we learned about the reasons AI adoption is stunted in organizations. One of the reasons was the lack of continual learning in ML systems. Yes, continual learning! We will address this challenge in this chapter and make sure we learn how to enable this capability by the end of this chapter. Now, let's look into continual learning.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor237"/>Continual learning </h2>
			<p>Continual learning is built on the principle of continuously learning from data, human experts, and the external <a id="_idIndexMarker855"/>environment. Continual learning enables lifelong learning, with adaptation at its core. It enables ML systems to become intelligent over time to adapt to the task at hand. It does this by monitoring and learning from the environment and the human experts assisting the ML system. Continual learning can be a powerful add-on to an ML system. It can allow you to realize the maximum potential of an AI system over time. Continual learning is highly recommended. Let's have a look at an example:</p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B16572_13_01.jpg" alt="Figure 13.1 – A loan issuing officer – a traditional system versus an ML system assisted by a human &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1 – A loan issuing scenario – a traditional system versus an ML system assisted by a human </p>
			<p>There are several advantages to deploying a model (enabled by continual learning) compared to having a traditional process in an organization that is fully dependent on human employees. For example, in the preceding diagram, we can see the steps of a bank's loan approval process in two cases. The first scenario is driven by human experts (such as in a traditional bank setup) only. The second scenario is where the process is automated or augmented using an ML system to screen applications, negotiate, provide loan application finalization (where a human expert reviews the ML system's decision and approves or rejects it), and approve the loan. The processing time of the traditional setup is 1 week, while the processing time of the ML system (working together with human experts) is 6 hours. </p>
			<p>The ML system is faster and more sustainable for the bank as it is continually learning and improving with a human assistant's help. Human employees have a fixed term of employment in a company or job. When they leave, their domain expertise is gone, and training a new employee or onboarding a new employee for the same task is costly. On the other hand, an ML <a id="_idIndexMarker856"/>model working together with or assisted by human experts that learns continually as time progresses manages to learn with time and retains that knowledge indefinitely (with regards to time). The continual learning that's acquired by the ML system (together with human experts) can be retained forever by the bank compared to the traditional approach, where human employees are constantly changing. Continual learning can unleash great value for an ML system and for the business in the long run. </p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor238"/>The need for continual learning </h2>
			<p>The following <a id="_idIndexMarker857"/>diagram shows some of the reasons why continual learning is needed and how it will enhance your ML system to maximize your business value:</p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B16572_13_02.jpg" alt="Figure 13.2 – Benefits of continual learning &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – Benefits of continual learning </p>
			<p>Let's go through the benefits of <a id="_idIndexMarker858"/>continual learning in detail:</p>
			<ul>
				<li><strong class="bold">Adaptation</strong>: In most straightforward applications, data drift might stay the same as data keeps coming in. However, many applications have dynamically changed data drifts, such as recommendation or anomaly detection systems, where data keeps flowing. In these cases, continually learning is important for adapting and being accurate with predictions. Hence, adapting to the changing nature of data and the environment is important.</li>
				<li><strong class="bold">Scalability</strong>: A white paper published by IDC (<a href="https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf">https://www.seagate.com/files/www-content/our-story/trends/files/idc-seagate-dataage-whitepaper.pdf</a>) suggests that by 2025, the rate of data generation will grow to 160 ZB/year, and that we will not be able to store all of it. The paper predicts that we will only be able to store between 3% and 12%. Data needs to be processed on the fly; otherwise, it will be lost since the storage infrastructure cannot keep up with the data that is produced. The main trick here is to process incoming data once, store only the essential information, and then get rid of the rest.</li>
				<li><strong class="bold">Relevance</strong>: Predictions from ML systems need to be relevant and need to adapt to changing contexts. Continual learning is needed to keep the ML systems highly relevant to and valuable in the changing contexts and environments. </li>
				<li><strong class="bold">Performance</strong>: Continual learning will enable high performance for the ML system, since it powers the ML system to be relevant by adapting to the changing data and environment. In other words, being more relevant will improve the performance of the ML system, for example, in terms of accuracy or other metrics, by providing more meaningful or valuable predictions.</li>
			</ul>
			<p>For these reasons, continual learning is needed in an ML system, so without continual learning, we cannot reach the maximum value an ML system has to offer. In other words, projects are doomed to fail. Continual learning is the key to succeeding in AI projects. An efficient governance strategy as part of Explainable Monitoring can enable continual learning. An important part of continual learning is model retraining, so that we can cope with evolving <a id="_idIndexMarker859"/>data and make relevant decisions. To do this, we can fuse Explainable Monitoring and model retraining to enable continual learning:</p>
			<p><strong class="bold">Explainable Monitoring + Model Retraining = Continual Learning  </strong></p>
			<p>Going ahead we will see continual learning in depth. Now, let's explore how we can bring efficient governance to ML systems. </p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor239"/>Explainable monitoring – governance </h1>
			<p>In this section, we will implement the governance mechanisms that we learned about previously in <a href="B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 11</em></a>, <em class="italic">Key Principles of Monitoring Your ML System</em>, for the business use <a id="_idIndexMarker860"/>case we have been working on. We will delve into three of the components of governing an ML system, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B16572_13_03.jpg" alt="Figure 13.3 – Components of governing your ML system &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 – Components of governing your ML system </p>
			<p>The effectiveness of ML systems results from how they are governed to maximize business value. To have end-to-end trackability and comply with legislation, system governance requires quality assurance and monitoring, model auditing, and reporting. We can regulate and rule ML systems by monitoring and analyzing model outputs. Smart warnings and behavior guide governance to optimize business value. Let's look at how the ML system's governance is orchestrated by warnings and behavior, model quality assurance and control, model auditing, and reports.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor240"/>Alerts and actions</h2>
			<p>Alerts are generated <a id="_idIndexMarker861"/>by performing scheduled checks to detect conditions. Upon meeting a condition, an alert is generated. Based on the alert that's generated, we can perform actions. In this section, we will learn about these elements and how they are orchestrated to govern an ML sys<a id="_idTextAnchor241"/>tem.</p>
			<h3>What is an alert?</h3>
			<p>An alert is a <a id="_idIndexMarker862"/>scheduled task running in the background to monitor an application to check if specific conditions are being detected. An alert is driven by three things:</p>
			<ul>
				<li><strong class="bold">Schedule</strong>: How often should we check for conditions?</li>
				<li><strong class="bold">Conditions</strong>: What needs to be detected?</li>
				<li><strong class="bold">Actions</strong>: What should we do when a condition is detected?</li>
			</ul>
			<p>We can create alerts based on application performance to monitor aspects such as the following: </p>
			<ul>
				<li>Alerts for availability based on a threshold</li>
				<li>Alerts for failed requests based on a threshold</li>
				<li>Alerts for server response time based on a threshold</li>
				<li>Alerts for server exceptions based on a threshold</li>
				<li>Alerts based on a threshold for data drift</li>
				<li>Alerts based on a threshold for model drift</li>
				<li>Alerts based on errors or exceptions</li>
			</ul>
			<p>An important area of governing ML systems is dealing with errors, so let's turn our attention to error handling.</p>
			<h3>Dealing with errors</h3>
			<p>Potential errors are always possible in an application. We can foresee them by addressing all the <a id="_idIndexMarker863"/>possible edge cases for our ML application. Using the framework shown in the following diagram, we can address these errors. The purpose of this framework is to identify edge cases and automated debugging methods to tackle possible errors. This will keep the ML service up and running:</p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B16572_13_04.jpg" alt="Figure 13.4 – Framework for debugging and investigating errors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.4 – Framework for debugging and investigating errors</p>
			<p>As shown in the preceding diagram, we started by identifying resources where errors might be present and choosing a resource to address that error. Upon choosing a resource, we check for errors by checking for high utilization of resources and resource saturation (a resource is saturated when its capacity is fully utilized or its capacity is past a set threshold). In the case of either issue, we investigate the discovery by investigating logs and devising a solution to handle any errors. Eventually, we automate debugging by using premade scripts to handle any issues (blocking the system from functioning optimally), for example, by restarting the resource or reloading a function or file to get the resource up and running in a healthy state. </p>
			<p>By addressing all the possible edge cases and devising automated error handling or debugging, we can make our applications failure-proof to serve our users. Having a failure-proof application enables robustness, making sure the users have a seamless experience and value from using the ML application. Once you have identified an error, address it by investigating or creating an automated debugging process and solving the error. After all, prevention is better than a cure. Hence, checking for all possible edge cases and addressing them beforehand can be rewarding.  </p>
			<p>We can handle <a id="_idIndexMarker864"/>potential errors by using exception handling functionalities. Exception handling is a programming technique that is used for dealing with rare situations that necessitate special care. Exception handling for a wide range of error types is easy to implement in Python. We can use the <strong class="source-inline">try</strong>, <strong class="source-inline">except</strong>, <strong class="source-inline">else</strong>, and <strong class="source-inline">finally</strong> functionalities to handle errors and exceptions, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B16572_13_05.jpg" alt="Figure 13.5 – Handling exceptions and edge cases&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.5 – Handling exceptions and edge cases</p>
			<p>All the statements are executed before an exception is encountered in the <strong class="source-inline">try</strong> clause. The exception(s) that are found in the <strong class="source-inline">try</strong> clause are caught and treated with the <strong class="source-inline">except</strong> block. The <strong class="source-inline">else</strong> block allows you to write parts that can only run if there are no exceptions in the <strong class="source-inline">try</strong> clause. Using <strong class="source-inline">finally</strong>, with or without any previously experienced exceptions, you can run parts of code that should always run.</p>
			<p>Here is a list of some possible common exceptions or errors to look out for:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/012.jpg" alt=""/>
				</div>
			</div>
			<p>These edge cases or <a id="_idIndexMarker865"/>errors are common and can be addressed in the application by using <strong class="source-inline">try</strong> and <strong class="source-inline">exception</strong> techniques. The strategy is to mitigate situations where your ML system will look very basic or naive for the user; for example, a chatbot that sends error messages in the chats. In such cases, the cost of errors is high, and the users will lose trust in the ML system. </p>
			<p>We will implement some custom exception and error handling for the business use case we have been implementing and implement actions based on the alerts that are generated. Let's get started:</p>
			<ol>
				<li>In your Azure DevOps project, go to our <strong class="bold">book repository (previously cloned in your Azure DevOps project)</strong> and go to the folder named <strong class="source-inline">13_Govenance_Continual_Learning</strong>. From there, access the <strong class="source-inline">score.py</strong> file. We will begin by importing the required libraries. This time, we will use the <strong class="source-inline">applicationinsights</strong> library to track custom events or exceptions of <a id="_idIndexMarker866"/>Application Insights that are connected to the endpoint:<p class="source-code">import json</p><p class="source-code">import numpy as np</p><p class="source-code">import os</p><p class="source-code">import pickle</p><p class="source-code">import joblib</p><p class="source-code">import onnxruntime</p><p class="source-code">import logging</p><p class="source-code">import time</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">from applicationinsights import TelemetryClient</p><p class="source-code">from azureml.monitoring import ModelDataCollector</p><p class="source-code">from inference_schema.schema_decorators import input_schema, output_schema</p><p class="source-code">from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType</p><p>As shown in the preceding code, we have imported the <strong class="source-inline">TelemetryClient</strong> function from the <strong class="source-inline">applicationinsights</strong> library. We will use the <strong class="source-inline">TelemetryClient</strong> function to access the Application Insights that are connected to our endpoint. Provide your instrumentation key from Application Insights to the <strong class="source-inline">TelemetryClient</strong> function.</p></li>
				<li>This <strong class="bold">Instrumentation Key</strong> can be accessed from your Application Insights, which should be connected to the ML application, as shown in the following screenshot:<div id="_idContainer188" class="IMG---Figure"><img src="image/B16572_13_06.jpg" alt="Figure 13.6 – Fetching an instrumentation key from Application Insights&#13;&#10;"/></div><p class="figure-caption">Figure 13.6 – Fetching an instrumentation key from Application Insights</p></li>
				<li>After <a id="_idIndexMarker867"/>fetching your <strong class="bold">Instrumentation Key</strong>, provide the <strong class="source-inline">TelemetryClient</strong> function, as shown in the following code. Here, we create a <strong class="source-inline">TelemetryClient</strong> object in the <strong class="source-inline">tc</strong> variable, which is used to track custom events:<p class="source-code">def init():</p><p class="source-code">    global model, scaler, input_name, label_name, inputs_dc, prediction_dc, tc</p><p class="source-code">        <strong class="bold">tc = TelemetryClient('xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx')</strong></p><p class="source-code">    scaler_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'),  </p><p class="source-code">'model-scaler/1/model-scaler.pkl')</p><p class="source-code">    # deserialize the model file back into a sklearn model</p><p class="source-code">        try:</p><p class="source-code">        scaler = joblib.load(scaler_path)</p><p class="source-code">    except Exception as e:</p><p class="source-code">        <strong class="bold">tc.track_event('FileNotFound', {'error_message': str(e)},  </strong></p><p class="source-code"><strong class="bold">                                  {'ErrorCode': 101})</strong></p><p class="source-code">    model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), </p><p class="source-code">'support-vector-classifier/2/svc.onnx')</p><p class="source-code">        try:</p><p class="source-code">        model = onnxruntime.InferenceSession(model_onnx, None)</p><p class="source-code">    except Exception as e:</p><p class="source-code">        <strong class="bold">tc.track_event('FileNotFound', {'error_message': str(e)}, </strong></p><p class="source-code"><strong class="bold">                                                               </strong><strong class="bold">{'ErrorCode': 101})</strong></p><p class="source-code">    input_name = model.get_inputs()[0].name</p><p class="source-code">    label_name = model.get_outputs()[0].name</p><p class="source-code">    # variables to monitor model input and output data</p><p class="source-code">    inputs_dc = ModelDataCollector("Support vector classifier model", designation="inputs", feature_names=["Temperature_C", "Humidity", "Wind_speed_kmph", "Wind_bearing_degrees", "Visibility_km", "Pressure_millibars", "Current_weather_condition"])</p><p class="source-code">    prediction_dc = ModelDataCollector("Support vector classifier model", designation="predictions", feature_names=["Future_weather_condition"])</p><p>Two custom events are tracked in the <strong class="source-inline">init</strong> function to monitor whether a <strong class="source-inline">FileNotFound</strong> error <a id="_idIndexMarker868"/><a id="_idIndexMarker869"/>occurs when we load the <strong class="source-inline">scaler</strong> and <strong class="source-inline">model</strong> artifacts. If a file is not found, the <strong class="source-inline">tc.track_events()</strong> function will log the error message that's generated by the exception and tag the custom code <strong class="source-inline">101</strong>. </p></li>
				<li>Likewise, we will implement some other custom events – that is, <strong class="source-inline">ValueNotFound</strong>, <strong class="source-inline">OutofBoundsException</strong>, and <strong class="source-inline">InferenceError</strong> – in the <strong class="source-inline">run</strong> function:<p class="source-code">@input_schema('data', NumpyParameterType(np.array([[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]])))</p><p class="source-code">@output_schema(NumpyParameterType(np.array([0])))</p><p class="source-code">def run(data):</p><p class="source-code">            try:              </p><p class="source-code">                inputs_dc.collect(data)</p><p class="source-code">            except Exception as e:</p><p class="source-code">                <strong class="bold">tc.track_event('ValueNotFound', {'error_message': str(e)},  </strong></p><p class="source-code"><strong class="bold">{'ErrorCode': 201})</strong></p><p>We use <strong class="source-inline">try</strong> and <strong class="source-inline">except</strong> to collect incoming data using the model data collector function. This collects the incoming data and stores it in the <strong class="source-inline">blob</strong> storage connected to the Azure ML service. If the incoming data contains some anomalous data or a missing value, an exception is raised. We will raise a <strong class="source-inline">ValueNotFound</strong> error using the <strong class="source-inline">track_event</strong> function so that we can log the exception message and custom code (in this case, a random or custom number of 201 is given to track the error). After collecting the incoming data, we will attempt to scale the <a id="_idIndexMarker870"/>data before inference:</p><p class="source-code">            try:</p><p class="source-code">                # scale incoming data</p><p class="source-code">                data = scaler.transform(data)</p><p class="source-code">            except Exception as e: </p><p class="source-code">                <strong class="bold">tc.track_event('ScalingException', {'ScalingError': str(e)}, </strong></p><p class="source-code"><strong class="bold">{'ErrorCode': 301})       </strong></p><p>Scaling data is an important pre-inference step. We need to make sure it is done right, without any errors. <strong class="source-inline">try</strong> and <strong class="source-inline">except</strong> can be handy in this case, since we are trying to scale the data using a <strong class="source-inline">scaler</strong> file that's been loaded in the <strong class="source-inline">init</strong> function. </p><p>If scaling the data is not successful, then an exception is raised. Here, we use the <strong class="source-inline">track_event</strong> function to track the exception on Application Insights. We generate a custom event named <strong class="source-inline">ScalingError</strong> in case an exception is generated. An exception message and an error code of <strong class="source-inline">301</strong> is logged on Application Insights. Likewise, the most important step of dealing with the scoring file – inferencing the model – needs to be done meticulously. </p><p>Now, we will use <strong class="source-inline">try</strong> and <strong class="source-inline">except</strong> again to make sure the inference is successful without any exceptions. Let's see how we can handle exceptions in this case. Note that we are accessing element number <strong class="source-inline">2</strong> for the <strong class="source-inline">model.run</strong> function. This causes an error in the model's inference as we are referring to an incorrect or nonexistent element of the list:</p><p class="source-code">            try:        </p><p class="source-code">                # model inference</p><p class="source-code">                result = model.run([label_name], {input_name: </p><p class="source-code">data.astype(np.float32)})[2]</p><p class="source-code"># this call is saving model output data into Azure Blob</p><p class="source-code">                prediction_dc.collect(result)</p><p class="source-code">                if result == 0:</p><p class="source-code">                    output = "Rain"                 </p><p class="source-code">                else: </p><p class="source-code">                    output = "No Rain"</p><p class="source-code">                return output</p><p class="source-code">            except Exception as e:</p><p class="source-code"><strong class="bold">                    tc.track_event('InferenceException', {'error_message': </strong></p><p class="source-code"><strong class="bold">str(e)}, {'InferenceError': 401})</strong></p><p class="source-code">                    output = 'error'</p><p class="source-code">                    return output </p><p>In case an exception <a id="_idIndexMarker871"/>occurs when the model is being inferenced, we can use the <strong class="source-inline">track_event()</strong> function to generate a custom event called <strong class="source-inline">InferenceError</strong>. This will be logged on Application Insights with an error message and a custom error code of <strong class="source-inline">401</strong>. This way, we can log custom errors and exceptions on Application Insights and generate actions based on these errors and exceptions. </p></li>
			</ol>
			<p>Now, let's look at how to investigate these errors in Application Insights using the error logs and generate actions for it.</p>
			<h3>Setting up actions </h3>
			<p>We can set up <a id="_idIndexMarker872"/>alerts and actions based on the exception events that we created previously (in the <em class="italic">Dealing with errors</em> section). In this section, we will set up an action in the form of an email notification based on an alert that we've generated. Whenever an exception or alert is generated in Application Insights, we will be notified via an email. Then, we can investigate and solve it. </p>
			<p>Let's set up an action (email) upon receiving an alert by going to Application Insights, which should be connected to your ML system endpoint. You can access Application Insights via your Azure ML workspace. Let's get started:</p>
			<ol>
				<li value="1">Go to <strong class="screen-inline">Endpoints</strong> and check for Application Insights. Once you've accessed the Application Insights dashboard, click on <strong class="screen-inline">Transaction search</strong>, as shown in the following screenshot, to check for your custom event logs (for example, inference exception):<div id="_idContainer189" class="IMG---Figure"><img src="image/B16572_13_07.jpg" alt="Figure 13.7 – Checking the custom event logs&#13;&#10;"/></div><p class="figure-caption">Figure 13.7 – Checking the custom event logs</p></li>
				<li>You can check for custom events that have been generated upon exceptions and errors occurs via the logs, and then set up alerts and actions for these custom events. To set up an alert and action, go to the <strong class="bold">Monitoring</strong> &gt; <strong class="bold">Alerts</strong> section and click on <strong class="bold">New alert rule</strong>, as shown in the following screenshot:<div id="_idContainer190" class="IMG---Figure"><img src="image/B16572_13_08.jpg" alt="Figure 13.8 – Setting up a new alert rule&#13;&#10;"/></div><p class="figure-caption">Figure 13.8 – Setting up a new alert rule</p></li>
				<li>Here, you can <a id="_idIndexMarker873"/>create conditions for actions based on alerting. To set up a condition, click on <strong class="bold">Add condition</strong>. You will be presented with a list of signals or log events you can use to make conditions. Select <strong class="bold">InferenceError</strong>, as shown in the following screenshot: <div id="_idContainer191" class="IMG---Figure"><img src="image/B16572_13_09.jpg" alt="Figure 13.9 – Configuring a condition &#13;&#10;"/></div><p class="figure-caption">Figure 13.9 – Configuring a condition </p></li>
				<li>After selecting the <a id="_idIndexMarker874"/>signal or event of your choice, you will get to configure its condition logic, as shown in the following screenshot. Configure the condition by setting up a threshold for it. In this case, we will provide a threshold of <strong class="source-inline">400</strong> as the error raises a value of <strong class="source-inline">401</strong> (since we had provided a custom value of <strong class="source-inline">401</strong> for the <strong class="source-inline">InferenceError</strong> event). When an inference exception occurs, it raises an <strong class="source-inline">InferenceError</strong> with a value above <strong class="source-inline">400</strong> (<strong class="source-inline">401</strong>, to be precise):<div id="_idContainer192" class="IMG---Figure"><img src="image/B16572_13_010.jpg" alt="Figure 13.10 – Configuring the condition logic and threshold&#13;&#10;"/></div><p class="figure-caption">Figure 13.10 – Configuring the condition logic and threshold</p></li>
				<li>After <a id="_idIndexMarker875"/>setting up the threshold, you will be asked to configure other actions, such as running an <strong class="bold">Automation Runbook</strong>, <strong class="bold">Azure Function</strong>, <strong class="bold">Logic App,</strong> or <strong class="bold">Secure Webhook</strong>, as shown in the following screenshot. For now, we will not prompt these actions, but it is good to know that we have them since we can run some scripts or applications as a backup mechanism to automate error debugging:<div id="_idContainer193" class="IMG---Figure"><img src="image/B16572_13_011.jpg" alt="Figure 13.11 – Actions to automate debugging (optional)&#13;&#10;"/></div><p class="figure-caption">Figure 13.11 – Actions to automate debugging (optional)</p><p>This way, we can <a id="_idIndexMarker876"/>automate debugging by having pre-configured scripts or applications set up in case an error occurs or to prevent errors. Prevention is better than a cure, after all!  </p></li>
				<li>Finally, we will create a condition. Click <strong class="bold">Review and create</strong> to create the condition, as shown in the preceding screenshot. Once you have created this condition, you will see it in the <strong class="bold">Create alert rule</strong> panel, as shown in the following screenshot. Next, set up an action by clicking on <strong class="bold">Add action groups</strong> and then <strong class="bold">Create action group</strong>:<div id="_idContainer194" class="IMG---Figure"><img src="image/B16572_13_012.jpg" alt="Figure 13.12 – Creating an action group&#13;&#10;"/></div><p class="figure-caption">Figure 13.12 – Creating an action group</p></li>
				<li>Provide an email <a id="_idIndexMarker877"/>address so that you can receive notifications, as shown in the following screenshot. Here, you can name your notification (in the <strong class="bold">Alert rule name</strong> field) and provide the necessary information to set up an email alert action:<div id="_idContainer195" class="IMG---Figure"><img src="image/B16572_13_013.jpg" alt="Figure 13.13 – Configuring email notifications&#13;&#10;"/></div><p class="figure-caption">Figure 13.13 – Configuring email notifications</p><p>After providing all the <a id="_idIndexMarker878"/>necessary information, including an email, click on the <strong class="bold">Review + Create</strong> button to configure the action (an email based on an error). Finally, provide alert rule details such as <strong class="bold">Alert rule name</strong>, <strong class="bold">Description</strong>, and <strong class="bold">Severity</strong>, as shown in the following screenshot:</p><div id="_idContainer196" class="IMG---Figure"><img src="image/B16572_13_014.jpg" alt="Figure 13.14 – Configuring email notifications&#13;&#10;"/></div><p class="figure-caption">Figure 13.14 – Configuring email notifications</p></li>
				<li>Click on <strong class="bold">Create alert rule</strong> to create an email alert based on the error (for example, <strong class="source-inline">InferenceError</strong>). With that, you have created an alert, so now it's time to test it. Go to the <strong class="source-inline">13_Govenance_Continual_Learning</strong> folder and access the <strong class="source-inline">test_inference.py</strong> script (replace the URL with your endpoint link). Then, run the script by running the following command:<p class="source-code"><strong class="bold">python3 test_inference.py</strong></p></li>
				<li>Running the script will output an error. Stop the script after performing some inferences. Within 5-10 minutes <a id="_idIndexMarker879"/>of the error, you will be notified of the error via email, as shown in the following screenshot:   </li>
			</ol>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B16572_13_015.jpg" alt="Figure 13.15 – Email notification of an error in production &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.15 – Email notification of an error in production </p>
			<p>Congratulations – you have successfully set up an email action alert for an error! This way, you can investigate when an error has been discovered in order to resolve it and get the system up and running. </p>
			<p>Next, let's look at how to ensure we have quality assurance for models and can control them in order to maximize business value.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor242"/>Model QA and control</h2>
			<p>Evolving or dynamically changing data leads to increased prediction error rates. This may result from data <a id="_idIndexMarker880"/>drift as the business and external environment change, or it may be due to data poisoning attacks. This <a id="_idIndexMarker881"/>increase in prediction error rates results in having to re-evaluate ML models as they are retrained (manually or automatically), leading to the discovery of new algorithms that are more accurate than the previous ones. Here are some <a id="_idIndexMarker882"/>guidelines for testing ML models with new data:</p>
			<ul>
				<li>Enable continual learning by retraining your models and evaluating their performance.</li>
				<li>Evaluate the performance of all the models on a new dataset at periodic intervals.</li>
				<li>Raise an alert when an alternative model starts giving better performance or greater accuracy than the existing model.</li>
				<li>Maintain a registry of models containing their latest performance details and reports.</li>
				<li>Maintain end-to-end lineages of all the models to reproduce them or explain their performance to stakeholders. </li>
			</ul>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor243"/>Model auditing and reports </h2>
			<p>Establishing a <a id="_idIndexMarker883"/>periodic auditing and reporting system for <a id="_idIndexMarker884"/>MLOps is a healthy practice as it enables an <a id="_idIndexMarker885"/>organization to track its operations end to end, as well as comply with the law and enable them to explain its operations to stakeholders upon request. We can ensure that the ML system conforms to the conventions that have been established and deliberated at the societal and governmental levels. To audit and report MLOps, it is recommended for auditors to inspect the fundamentals of an audit, shown in the following image:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B16572_13_016.jpg" alt="Figure 13.16 – Fundamentals of an audit report for ML Operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.16 – Fundamentals of an audit report for ML Operations</p>
			<h3>Data audit</h3>
			<p>Data is what drives many of the decisions made by ML systems. Due to this, the auditors need to consider data for auditing and reporting, inspecting the training data, testing the data, inferring the data, and monitoring the data. This is essential and having end-to-end traceability to track the use of data (for example, which dataset was used to train which model) is needed for MLOps. Having a <em class="italic">Git for Data</em> type of mechanism that versions data can enable auditors to reference, examine, and document the data.  </p>
			<h3>Model audit (fairness and performance)</h3>
			<p>Auditors of ML systems <a id="_idIndexMarker886"/>need to have a hacker's mindset to identify the different ways in which a model could fail and not give fair predictions. First, the training data is inspected and compared to the inference data using Explainable AI techniques. This can help auditors make fair judgments about each model and each of its predictions on an individual level. To make fairness and performance assessments for each model, we can use data slicing techniques, which can reveal valuable information for making an assessment. Due to this, it is valuable for auditors to request the results of data slicing for the required demographics and slices of data. To make a collective assessment, we can compare models and assess their performance. This can reveal another angle of information for making fairness and performance assessments. </p>
			<p>If a model audit were to proceed, it would assess the model's inputs (training data), the model itself, and its outputs. Data consistency and possible biases in the training data will need to be assessed. For example, if a resume screening model had been trained on previous or historic decisions where candidates had received job offers and workers were promoted, we'd <a id="_idIndexMarker887"/>want to make sure that the training data hasn't been influenced by past recruiters' and managers' implicit biases. Benchmarking against competing models, performing statistical tests to ensure that the model generalizes from training to unknown results, and using state-of-the-art techniques to allow model interpretability are all parts of the model evaluation process.</p>
			<h3>Project and governance audit</h3>
			<p>Is it necessary to <a id="_idIndexMarker888"/>have a deep understanding of AI models to audit algorithms? Certainly not. An audit of an <a id="_idIndexMarker889"/>AI system's progress is like a project management audit. Is there a clear target for the desired achievement? This is a good and straightforward question to ask if a government entity has implemented AI in a particular environment. Furthermore, is there a viable framework to manage the model after the developers leave, if external developers have been applied to the AI system? To reduce the need for specialist expertise, the company must have extensive documentation of concept creation and the staff who are familiar with the model. Hence, auditing the development and governance practices can be rewarding in the long run. </p>
			<p>Auditing data considerations, model fairness and performance, and project management and governance of ML systems can provide a comprehensive view of MLOps. Using error alerts and actions, we can perform timely investigations into errors to get the system up and running and in some cases, we can even do automated debugging to automate error resolution and MLOps. Finally, by undertaking model quality assurance, control, and auditing, we can ensure efficient governance of our MLOps. Next, we will look at how to enable model retraining so that we have continual learning capabilities for our ML system.  </p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor244"/>Enabling model retraining </h1>
			<p>So far, we've <a id="_idIndexMarker890"/>talked about what model drift is and how to recognize it. So, the question is, what should we do about it? If a model's predictive performance has deteriorated due to changes in the environment, the solution is to retrain the model using a new training set that represents the current situation. How much should your model be retrained by? And how can you choose your new workout routine? The following diagram shows the <strong class="bold">Model retrain</strong> function triggering the <strong class="bold">Build</strong> module based on the <a id="_idIndexMarker891"/>results of the <strong class="bold">Monitor</strong> module. There are two ways to trigger the model retrain function. One is manually and the other is by automating the model retraining function. Let's see how we can enable both:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B16572_13_017.jpg" alt="Figure 13.17 – Model retraining enabled in an MLOps workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.17 – Model retraining enabled in an MLOps workflow</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor245"/>Manual model retraining </h2>
			<p>The product owner or <a id="_idIndexMarker892"/>quality assurance manager has the responsibility of ensuring manual model retraining is successful. The manual model triggering step involves evaluating model drift and if it goes above a threshold (you need to determine a threshold for drift that will trigger model retraining), then they must trigger the model training process by training the model using a new dataset (this can be the previous training dataset and the latest inference data). This way, the product owner or quality assurance manager has full control over the process, and also knows when and how to trigger model retraining to deliver maximized value from the ML system.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor246"/>Automated model retraining </h2>
			<p>If you want to fully <a id="_idIndexMarker893"/>automate the MLOps pipeline, automating model drift management can be an ideal approach to retraining the production model. Automating model drift management is done by configuring batch jobs that monitor application diagnostics and model performance. Then, you must activate model retraining. A key part of automating model drift management is setting the threshold that will automatically trigger the retraining model function. If the drift monitoring threshold is set too low, you run the risk of having to retrain too often, which will result in high compute costs. If the threshold is set too high, you risk not retraining often enough, resulting in suboptimal production models. Figuring out the right threshold is trickier than it seems because you have to figure out how much additional training data you'll need to reflect this new reality. Even if the environment has changed, replacing an existing model with one that has a very small training set is pointless. Once you have the threshold figured out, you could have jobs (for example, as part of the CI/CD pipeline) that compare the feature distributions of live datasets to those of training data on a regular basis (as we did in <a href="B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222"><em class="italic">Chapter 12</em></a>, <em class="italic">Model Serving and Monitoring</em>). When a large deviation is detected (or above the defined threshold), the system can schedule model <a id="_idIndexMarker894"/>retraining and deploy a new model automatically. This can be done using a work scheduler such as Jenkins or Kubernetes Jobs or CI/CD pipeline cron jobs. This way, you can fully automate the MLOps pipeline and the model retraining part. </p>
			<p>Note that it doesn't make sense to retrain the model in cases where there is low new data coming in or if you are doing batch inference once in a blue moon (for example, every 6 months). You can train the model before inference or periodically whenever you need to.</p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor247"/>Maintaining the CI/CD pipeline </h1>
			<p>As you may recall, in <a href="B16572_10_Final_JM_ePub.xhtml#_idTextAnchor189"><em class="italic">Chapter 10</em></a><em class="italic">, Essentials of Production Release</em>, we mentioned that <em class="italic">a model is not the product; the pipeline is the product</em>. Hence, after setting up automated or <a id="_idIndexMarker895"/>semi-automated CI/CD pipelines, it is critical to monitor the performance of our pipeline. We can do that by inspecting the releases in Azure DevOps, as shown in the following screenshot:  </p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B16572_13_018.jpg" alt="Figure 13.18 – Maintaining CI/CD pipeline releases&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.18 – Maintaining CI/CD pipeline releases</p>
			<p>The goal of an inspection is to keep the CI/CD pipeline in a healthy and robust state. Here are some <a id="_idIndexMarker896"/>guidelines for keeping the CI/CD pip<a id="_idTextAnchor248"/>eline healthy and robust:</p>
			<ul>
				<li>If a build is broken, a <strong class="bold">fix it asap</strong> policy from the team should be implemented.</li>
				<li>Integrate automated acceptance tests.</li>
				<li>Require pull requests.</li>
				<li>Peer code review each story or feature.</li>
				<li>Audit system logs and events periodically (recommended).</li>
				<li>Regularly report metrics visibly to all the team members (for example, slackbot or email notifications).</li>
			</ul>
			<p>By implementing these practices, we can avoid high failure rates and make the CI/CD pipeline robust, scalable, and transparent for all the team members. </p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor249"/>Summary</h1>
			<p>In this chapter, we learned about the key principles of continual learning in ML solutions. We learned about Explainable Monitoring (the governance component) by implementing hands-on error handling and configuring actions to alert developers of ML systems using email notifications. Lastly, we looked at ways to enable model retraining and how to maintain the CI/CD pipeline. With this, you have been equipped with the critical skills to automate and govern MLOps for your use cases. </p>
			<p>Congratulations on finishing this book! The world of MLOps is constantly evolving for the better. You are now equipped to help your business thrive using MLOps. I hope you enjoyed reading and learning by completing the hands-on MLOps implementations. Go out there and be the change you wish to see. All the best with your MLOps endeavors!</p>
		</div>
	</body></html>