- en: Chapter 8. Clustering based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the clustering-based learning methods, and in
    specific the k-means clustering algorithm among others. Clustering-based learning
    is an unsupervised learning technique and thus works without a concrete definition
    of the target attribute. You will learn basics and the advanced concepts of this
    technique, and get hands-on implementation guidance in using Apache Mahout, R,
    Julia, Apache Spark, and Python to implement the k-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts different learning models covered in this book,
    and the techniques highlighted in orange will be dealt in detail in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering based learning](img/B03980_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The topics listed next are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The core principles and objectives of the clustering-based learning methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to represent clusters and understand the required distance measurement techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning in depth, the k-means clustering and choosing the right clustering
    algorithm and the rules of cluster evaluation. More importantly, choosing the
    right number of clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of hierarchical clustering, data standardization, discovering holes,
    and data regions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample implementation using the Apache Mahout, R, Apache Spark, Julia, and Python
    (scikit-learn) libraries and modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering-based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The clustering-based learning method is identified as an unsupervised learning
    task wherein the learning starts from no specific target attribute in mind, and
    the data is explored with a goal of finding intrinsic structures in them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram represents the scope of the clustering-based learning
    method that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering-based learning](img/B03980_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The primary goal of the clustering technique is finding similar or homogenous
    groups in data that are called **clusters**. The way this is done is—data instances
    that are similar or, in short, are near to each other are grouped in one cluster,
    and the instances that are different are grouped into a different cluster. The
    following diagram shows a depiction of data points on a graph, and how the clusters
    are marked (in here, it is by pure intuition) by the three natural clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering-based learning](img/B03980_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, a cluster can be defined as a collection of objects that are similar
    to each other and dissimilar from the objects of another cluster. The following
    diagram depicts the clustering process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering-based learning](img/B03980_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Some simple examples of clustering can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Shirts are grouped based on the sizes small (S), medium (M), large (L), extra
    large (XL), and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Target Marketing: grouping customers according to their similarities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grouping text documents: The requirement here is to organize documents, and
    build a topic hierarchy based on their content similarities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In fact, clustering techniques are very heavily used in many domains such as
    archeology, biology, marketing, insurance, libraries, financial services, and
    many others.
  prefs: []
  type: TYPE_NORMAL
- en: Types of clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cluster analysis is all about the kind of algorithms that can be used to find
    clusters automatically given the data. There are primarily two classes of clustering
    algorithm; they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The Hierarchical Clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Partitional Clustering algorithms![Types of clustering](img/B03980_08_05.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hierarchical clustering algorithms define clusters that have a hierarchy,
    while the partitional clustering algorithms define clusters that divide the dataset
    into mutually disjoint partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Hierarchical clustering is about defining clusters that have a hierarchy,
    and this is done either by iteratively merging smaller clusters into a larger
    cluster, or dividing a larger cluster into smaller clusters. This hierarchy of
    clusters that are produced by a clustering algorithm is called a **dendogram**.
    A dendogram is one of the ways in which the hierarchical clusters can be represented,
    and the user can realize different clustering based on the level at which the
    dendogram is defined. It uses a similarity scale that represents the distance
    between the clusters that were grouped from the larger cluster. The following
    diagram depicts a dendogram representation for the Hierarchical clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical clustering](img/B03980_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is another simple way of representing the Hierarchical clusters; that
    is, the Venn diagram. In this representation, we circle the data points that are
    a part of the cluster. The following diagram depicts a Venn representation for
    five data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical clustering](img/B03980_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two clustering algorithms in the Hierarchical clustering: the Agglomerative
    and Divisive clustering algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: The Agglomerative clustering algorithm uses the bottom-up approach and merges
    a set of clusters into a larger cluster. The Divisive clustering algorithm uses
    the top-down approach and splits a cluster into subclusters. The identification
    of which cluster will be considered for merging or splitting is decided using
    greedy methods, and distance measurement becomes critical here. Let's have a quick
    recap of the instance-based learning methods from [Chapter 6](ch06.html "Chapter 6. Instance
    and Kernel Methods Based Learning"), *Instance and Kernel Methods Based Learning*.
    We have covered the Euclidean distance, Manhattan distance, and cosine similarity
    as some of the most commonly used metrics of similarity for numeric data, and
    hamming distance for non-numeric data. For the Hierarchical clustering, the actual
    data points are not required? only the distance measure matrix is sufficient,
    as the grouping is done based on the distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hierarchical clustering algorithm steps can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with clusters such as *S1={X1}, S2={X2} … Sm= {Xm}*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a set of the nearest clusters and merge them into a single cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the step 2 until the number of clusters formed is equal to a number defined.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partitional clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Partitional clustering algorithms are different in comparison to the Hierarchical
    clustering algorithms as the clusters or partitions are generated and evaluated
    using a specific predefined criterion that is domain specific. Since each cluster
    formed is mutually exclusive, there can never be a hierarchical relationship between
    the clusters. In fact, every instance can be placed in one and only one of the
    *k* clusters. The number of clusters (*k*) to be formed is input to this algorithm,
    and this one set of *k* clusters is the output of the partitional cluster algorithms.
    One of the most commonly used partitional clustering algorithms that we will be
    covering in this chapter is the k-means clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we take a deep dive into the k-means clustering algorithm, let''s have
    a quick definition stated here. With an input of *k*, which denotes the number
    of expected clusters, *k* centers or centroids will be defined that will facilitate
    defining the *k* partitions. Based on these centers (centroids), the algorithm
    identifies the members and thus builds a partition followed by the recomputation
    of the new centers based on the identified members. This process is iterated until
    the clear, and optimal dissimilarities that make the partition really unique are
    exposed. Hence, the accuracy of the centroids is the key for the partition-based
    clustering algorithm to be successful. The following are the steps involved in
    the centroid-based partitional clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: *k* (the number of clusters) and *d* (the data set with *n* objects)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output: Set of *k* clusters that minimize the sum of dissimilarities of all
    the objects to the identified mediod (centroid)'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the *k* objects as the first set of centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the remaining objects that are nearest to the centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly select a non-centroid object and recompute the total points that will
    be swapped to form a new set of centroids, until you need no more swapping.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Hierarchical and partitional clustering techniques inherently have key differences
    in many aspects, and some of them include some basic assumptions; execution time
    assumptions, input parameters, and resultant clusters. Typically, partitional
    clustering is faster than Hierarchical clustering. While the hierarchical clustering
    can work with similarity measure alone, partitional clustering requires number
    of clusters and details around the initial centers. The Hierarchical clustering
    does not require any input parameters while the partitional clustering algorithms
    require an input value that indicates the number of clusters required to start
    running. The cluster definition for hierarchical clustering technique is more
    subjective as against the partitional clustering results in a exact and precise
    "k" cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The quality of clustering depends on the chosen algorithm, distance function,
    and the application. A cluster quality is said to be the best when the inter-cluster
    distance is *maximized,* and the intra-cluster distance is *minimized*.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover the k-means clustering algorithm in depth. The
    k-means is a partitional clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let the set of data points (or instances) be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*D = {x*[1]*, x*[2]*, …, x*[n]*}*, where'
  prefs: []
  type: TYPE_NORMAL
- en: '*xi = (xi*[1]*, xi*[2]*, …, xi*[r]*)*, is a vector in a real-valued space *X
    ⊆ R*[r,] and *r* is the number of attributes in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm partitions the given data into *k* clusters with each
    cluster having a center called a centroid.
  prefs: []
  type: TYPE_NORMAL
- en: '*k* is specified by the user.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given *k*, the k-means algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm k-means (*k*, *D*)
  prefs: []
  type: TYPE_NORMAL
- en: Identify the *k* data points as the initial centroids (cluster centers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each data point *x ϵ D* do.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the distance from *x* to the centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign *x* to the closest centroid (a centroid represents a cluster).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: endfor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-compute the centroids using the current cluster memberships until the stopping
    criterion is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convergence or stopping criteria for the k-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following list describes the convergence criteria for the k-means clustering
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: There are zero or minimum number of reassignments for the data points to different
    clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are zero or minimum changes of centroids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the decrease in the **sum of squared error of prediction** (**SSE**)
    is minimum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *C[j]* is the *j*^(th) cluster, then *m*[j] is the centroid of cluster *C*[j]
    (the mean vector of all the data points in *C*[j]), and if *dist(x, m*[j]*)* is
    the distance between the data point *x* and centroid *m*[j] then the following
    example demonstrated using graphical representation explains the convergence criteria.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of random k centers:![Convergence or stopping criteria for the
    k-means clustering](img/B03980_08_08.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iteration 1: Compute centroids and assign the clusters:![Convergence or stopping
    criteria for the k-means clustering](img/B03980_08_09.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iteration 2: Recompute centroids and reassign the clusters:![Convergence or
    stopping criteria for the k-means clustering](img/B03980_08_10.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iteration 3: Recompute centroids and reassign the clusters:![Convergence or
    stopping criteria for the k-means clustering](img/B03980_08_11.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Terminate the process due to minimal changes to centroids or cluster reassignments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means clustering on disk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The k-means clustering algorithm can also be implemented with data on disk.
    This approach is used with large datasets that cannot be accommodated in memory.
    The strategy used here is to compute centroids incrementally by scanning the dataset
    only once for each iteration. The performance of this algorithm is determined
    by how well the number of iterations can be controlled. It is recommended that
    a limited set of iterations, less than 50 should be run. Although this version
    helps scaling, it is not the best algorithm for scaling up; there are other alternative
    clustering algorithms that scale-up, for example, BIRCH is one of them. The following
    algorithm describes the steps in disk the k-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm disk k-means (*k*, *D*)
  prefs: []
  type: TYPE_NORMAL
- en: Choose the *k* data points as the initial centroids (cluster centers) *m*[j],
    where *j = 1,2,3….k*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *s*[j]*=0*, where *j=1,2,3….k*; (a vector with all the zero values).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *n*[j]*=0*, where *j=1,2,3….k*; (n[j] is number points in the cluster),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each data point *x ϵ D* do.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*j = arg min dist(x, m*[j]*)*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign *x* to the cluster *j*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*s*[j] *= s*[j] *+ x*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*n*[j] *= n*[j] *+ 1*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: endfor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*m*[i] *= s*[j]*/n*[j], where *i=1,2,…k.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Until the stopping, the criterion is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages of the k-means approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The k-means way of unsupervised learning has many benefits; some of them are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The k-means clustering is popular and widely adopted due to its simplicity and
    ease of implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is efficient and has optimal time complexity defined by *O*(*ikn*), where
    *n* is the number of data points, *k* is the number of clusters, and *i* is the
    number of iterations. Since the *l* and *k* values are kept small, the k-means
    clustering can represent a linear expression too.
  prefs: []
  type: TYPE_NORMAL
- en: Disadvantages of the k-means algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the downsides or disadvantages of the k-means algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: The value of *k* is always a user input and is as good as the identified number
    *k.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This algorithm is applicable only when the means are available, and in the case
    of categorical data the centroids are none other than the frequent values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clusters can never be elliptical and are always hyperspherical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clusters identified are very sensitive to the initially identified seeds,
    and can be different when to run multiple times with different random seeds involved.
    The following figure depicts how two different centroids can change the clusters.
    This can be achieved by iterative processing:![Disadvantages of the k-means algorithm](img/B03980_08_12.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, k-means is very sensitive to outliers. Outliers can be the errors in
    the data recording or some special data points with very different values. The
    following diagram depicts the skew that an outlier can bring into the cluster
    formation. The first representation shows the ideal cluster, and the second one
    shows the undesirable cluster:![Disadvantages of the k-means algorithm](img/B03980_08_13.jpg)![Disadvantages
    of the k-means algorithm](img/B03980_08_14.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of the algorithms and learning techniques that we have seen until now are
    sensitive to outliers. There are some standard techniques that can be employed.
  prefs: []
  type: TYPE_NORMAL
- en: One way is to get the outliers filtered from evaluation, and this requires us
    to apply some techniques to handle the noise in the data. The noise reduction
    techniques will be covered in the next chapters. In the case of k-means clustering,
    the removal of outliers can be done after a few iterations just to make sure the
    identified data points are really the outliers. Or, another way is to stick to
    a smaller sample of data on which the algorithm will be run. This way, the possibility
    of choosing an outlier will be minimal.
  prefs: []
  type: TYPE_NORMAL
- en: Distance measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The distance measure is important in clustering algorithms. Reassigning data
    points to the clusters is determined by redefining the centroids. The following
    are some ways of measuring distance between two clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single link**: This method refers to measuring the distance between the two
    closest data points that belong to two different clusters. There can be noise
    in the data that might be considered with seriousness too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complete link**: This method refers to measuring the distance between two
    farthest data points that belong to two different clusters. This method can make
    the clusters more sensitive to outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average link**: This method uses the average distance measure of all the
    pairs of distances between the two clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroids**: This method refers to measuring the distance between the two
    clusters by measuring the distance between their centroids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complexity measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the best clustering algorithm has always been a challenge. There are
    many algorithms available and both, accuracy and complexity measures are important
    for choosing the right algorithm. The single link method can help achieve *O(n2)*;
    complete and average links can be done in *O(n2logn)*. There are both advantages
    and limitations for each of the algorithms, and they work well in certain contexts
    of data distribution; no standard patterns in the data distribution make it a
    complex problem to solve. Hence, data preparation and standardization becomes
    an important aspect in Machine learning. Which distance measure would be an ideal
    choice can only be determined by implementing the different distance measures
    iteratively, and comparing the results across iterations. The clustering methods
    overall are highly dependent on the initial choices and can be subjective.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing the k-means
    clustering methods (only supervised learning techniques - source code path `.../chapter08/...`
    under each of the folders for the technology).
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter8/k-meansexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter8/k-meansexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter8/k-meansexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Python (scikit-learn)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter8/k-meansexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter8/k-meansexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered the clustering-based learning methods. We have
    taken a deep dive into the k-means clustering algorithm using an example. You
    have learned to implement k-means clustering using Mahout, R, Python, Julia, and
    Spark. In the next chapter, we will cover the Bayesian methods and in specific,
    the Naïve-Bayes algorithm.
  prefs: []
  type: TYPE_NORMAL
