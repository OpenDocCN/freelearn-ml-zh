- en: Chapter 8. Clustering based learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 基于聚类的学习
- en: In this chapter, we will cover the clustering-based learning methods, and in
    specific the k-means clustering algorithm among others. Clustering-based learning
    is an unsupervised learning technique and thus works without a concrete definition
    of the target attribute. You will learn basics and the advanced concepts of this
    technique, and get hands-on implementation guidance in using Apache Mahout, R,
    Julia, Apache Spark, and Python to implement the k-means clustering algorithm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍基于聚类的学习方法，特别是k-means聚类算法等。基于聚类的学习是一种无监督学习技术，因此无需对目标属性有具体的定义。您将学习该技术的基礎和高级概念，并获得使用Apache
    Mahout、R、Julia、Apache Spark和Python实现k-means聚类算法的实践指导。
- en: 'The following figure depicts different learning models covered in this book,
    and the techniques highlighted in orange will be dealt in detail in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了本书中涵盖的不同学习模型，以及用橙色突出显示的技术将在本章中详细讨论：
- en: '![Clustering based learning](img/B03980_08_01.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![基于聚类的学习](img/B03980_08_01.jpg)'
- en: 'The topics listed next are covered in depth in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的主题将在本章中深入探讨：
- en: The core principles and objectives of the clustering-based learning methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于聚类的学习方法的核心原则和目标
- en: How to represent clusters and understand the required distance measurement techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何表示簇和理解所需的距离测量技术
- en: Learning in depth, the k-means clustering and choosing the right clustering
    algorithm and the rules of cluster evaluation. More importantly, choosing the
    right number of clusters.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入学习k-means聚类、选择合适的聚类算法和簇评估规则。更重要的是，选择合适的簇数量。
- en: An overview of hierarchical clustering, data standardization, discovering holes,
    and data regions.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类的概述、数据标准化、发现空洞和数据区域。
- en: Sample implementation using the Apache Mahout, R, Apache Spark, Julia, and Python
    (scikit-learn) libraries and modules.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Mahout、R、Apache Spark、Julia和Python（scikit-learn）库和模块的示例实现。
- en: Clustering-based learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于聚类的学习
- en: The clustering-based learning method is identified as an unsupervised learning
    task wherein the learning starts from no specific target attribute in mind, and
    the data is explored with a goal of finding intrinsic structures in them.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聚类的学习方法被识别为一种无监督学习任务，其中学习从没有特定目标属性开始，数据被探索以寻找其中的内在结构。
- en: 'The following diagram represents the scope of the clustering-based learning
    method that will be covered in this chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了本章将要介绍的基于聚类的学习方法范围：
- en: '![Clustering-based learning](img/B03980_08_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![基于聚类的学习](img/B03980_08_02.jpg)'
- en: 'The primary goal of the clustering technique is finding similar or homogenous
    groups in data that are called **clusters**. The way this is done is—data instances
    that are similar or, in short, are near to each other are grouped in one cluster,
    and the instances that are different are grouped into a different cluster. The
    following diagram shows a depiction of data points on a graph, and how the clusters
    are marked (in here, it is by pure intuition) by the three natural clusters:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类技术的首要目标是找到数据中的相似或同质群体，这些群体被称为**簇**。这样做的方式是——相似或简而言之，彼此靠近的数据实例被分组到一个簇中，而不同的实例被分组到不同的簇中。以下图表展示了图表上的数据点，以及簇是如何被标记的（在这里，是通过纯粹直觉）由三个自然簇：
- en: '![Clustering-based learning](img/B03980_08_03.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![基于聚类的学习](img/B03980_08_03.jpg)'
- en: 'Thus, a cluster can be defined as a collection of objects that are similar
    to each other and dissimilar from the objects of another cluster. The following
    diagram depicts the clustering process:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，簇可以被定义为包含彼此相似且与其他簇中的对象不同的对象的集合。以下图表展示了聚类过程：
- en: '![Clustering-based learning](img/B03980_08_04.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![基于聚类的学习](img/B03980_08_04.jpg)'
- en: 'Some simple examples of clustering can be as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的简单示例可以如下：
- en: Shirts are grouped based on the sizes small (S), medium (M), large (L), extra
    large (XL), and so on
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衬衫根据尺寸分为小号（S）、中号（M）、大号（L）、加大号（XL）等。
- en: 'Target Marketing: grouping customers according to their similarities'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标营销：根据客户的相似性对客户进行分组
- en: 'Grouping text documents: The requirement here is to organize documents, and
    build a topic hierarchy based on their content similarities'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本文档的分组：这里的要求是组织文档，并基于它们的内容相似性构建主题层次结构
- en: In fact, clustering techniques are very heavily used in many domains such as
    archeology, biology, marketing, insurance, libraries, financial services, and
    many others.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，聚类技术在许多领域都有非常广泛的应用，例如考古学、生物学、市场营销、保险、图书馆、金融服务以及许多其他领域。
- en: Types of clustering
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类类型
- en: 'Cluster analysis is all about the kind of algorithms that can be used to find
    clusters automatically given the data. There are primarily two classes of clustering
    algorithm; they are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析主要关注的是在给定数据的情况下，可以用来自动找到聚类的算法。主要有两类聚类算法；如下所示：
- en: The Hierarchical Clustering algorithms
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类算法
- en: The Partitional Clustering algorithms![Types of clustering](img/B03980_08_05.jpg)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分聚类算法![聚类类型](img/B03980_08_05.jpg)
- en: The Hierarchical clustering algorithms define clusters that have a hierarchy,
    while the partitional clustering algorithms define clusters that divide the dataset
    into mutually disjoint partitions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法定义了具有层次结构的聚类，而划分聚类算法定义了将数据集划分为互斥分区的聚类。
- en: Hierarchical clustering
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: 'The Hierarchical clustering is about defining clusters that have a hierarchy,
    and this is done either by iteratively merging smaller clusters into a larger
    cluster, or dividing a larger cluster into smaller clusters. This hierarchy of
    clusters that are produced by a clustering algorithm is called a **dendogram**.
    A dendogram is one of the ways in which the hierarchical clusters can be represented,
    and the user can realize different clustering based on the level at which the
    dendogram is defined. It uses a similarity scale that represents the distance
    between the clusters that were grouped from the larger cluster. The following
    diagram depicts a dendogram representation for the Hierarchical clusters:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是关于定义具有层次结构的聚类，这可以通过迭代地将较小的聚类合并成较大的聚类，或者将较大的聚类划分为较小的聚类来实现。由聚类算法产生的这种聚类层次结构被称为**树状图**。树状图是表示层次聚类的一种方式，用户可以根据树状图定义的级别实现不同的聚类。它使用一个相似度尺度，表示从较大的聚类中分组出来的聚类之间的距离。以下图展示了层次聚类的树状图表示：
- en: '![Hierarchical clustering](img/B03980_08_06.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/B03980_08_06.jpg)'
- en: 'There is another simple way of representing the Hierarchical clusters; that
    is, the Venn diagram. In this representation, we circle the data points that are
    a part of the cluster. The following diagram depicts a Venn representation for
    five data points:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 存在另一种表示层次聚类的简单方法；那就是维恩图。在这种表示中，我们圈出属于聚类的数据点。以下图展示了五个数据点的维恩图表示：
- en: '![Hierarchical clustering](img/B03980_08_07.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/B03980_08_07.jpg)'
- en: 'There are two clustering algorithms in the Hierarchical clustering: the Agglomerative
    and Divisive clustering algorithms.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类中有两种聚类算法：聚合聚类算法和划分聚类算法。
- en: The Agglomerative clustering algorithm uses the bottom-up approach and merges
    a set of clusters into a larger cluster. The Divisive clustering algorithm uses
    the top-down approach and splits a cluster into subclusters. The identification
    of which cluster will be considered for merging or splitting is decided using
    greedy methods, and distance measurement becomes critical here. Let's have a quick
    recap of the instance-based learning methods from [Chapter 6](ch06.html "Chapter 6. Instance
    and Kernel Methods Based Learning"), *Instance and Kernel Methods Based Learning*.
    We have covered the Euclidean distance, Manhattan distance, and cosine similarity
    as some of the most commonly used metrics of similarity for numeric data, and
    hamming distance for non-numeric data. For the Hierarchical clustering, the actual
    data points are not required? only the distance measure matrix is sufficient,
    as the grouping is done based on the distances.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类算法采用自底向上的方法，将一组聚类合并成一个更大的聚类。划分聚类算法采用自顶向下的方法，将一个聚类划分为子聚类。确定哪个聚类将被考虑合并或划分，是通过贪婪方法决定的，距离测量在这里变得至关重要。让我们快速回顾一下[第6章](ch06.html
    "第6章。基于实例和核方法的学习")中的实例学习方法，*基于实例和核方法的学习*。我们已经介绍了欧几里得距离、曼哈顿距离和余弦相似度作为数值数据中最常用的相似性度量，以及汉明距离用于非数值数据。对于层次聚类，实际上不需要实际数据点，只需要距离度量矩阵就足够了，因为分组是基于距离进行的。
- en: 'The Hierarchical clustering algorithm steps can be defined as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法的步骤可以定义为如下：
- en: Start with clusters such as *S1={X1}, S2={X2} … Sm= {Xm}*.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从如下聚类开始，例如 *S1={X1}, S2={X2} … Sm= {Xm}*。
- en: Find a set of the nearest clusters and merge them into a single cluster.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到一组最近的聚类并将它们合并成一个单独的聚类。
- en: Repeat the step 2 until the number of clusters formed is equal to a number defined.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2，直到形成的聚类数量等于定义的数量。
- en: Partitional clustering
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 划分聚类
- en: Partitional clustering algorithms are different in comparison to the Hierarchical
    clustering algorithms as the clusters or partitions are generated and evaluated
    using a specific predefined criterion that is domain specific. Since each cluster
    formed is mutually exclusive, there can never be a hierarchical relationship between
    the clusters. In fact, every instance can be placed in one and only one of the
    *k* clusters. The number of clusters (*k*) to be formed is input to this algorithm,
    and this one set of *k* clusters is the output of the partitional cluster algorithms.
    One of the most commonly used partitional clustering algorithms that we will be
    covering in this chapter is the k-means clustering algorithm.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与层次聚类算法相比，划分聚类算法不同，因为聚类或分区是通过使用特定预定义的标准来生成和评估的，该标准是特定领域的。由于每个形成的聚类都是互斥的，因此聚类之间永远不会存在层次关系。实际上，每个实例都可以放置在仅有的一个*k*个聚类中。要形成的聚类数量（*k*）作为输入输入到该算法中，这组*k*个聚类是划分聚类算法的输出。在本章中我们将要介绍的最常用的划分聚类算法之一是k-means聚类算法。
- en: 'Before we take a deep dive into the k-means clustering algorithm, let''s have
    a quick definition stated here. With an input of *k*, which denotes the number
    of expected clusters, *k* centers or centroids will be defined that will facilitate
    defining the *k* partitions. Based on these centers (centroids), the algorithm
    identifies the members and thus builds a partition followed by the recomputation
    of the new centers based on the identified members. This process is iterated until
    the clear, and optimal dissimilarities that make the partition really unique are
    exposed. Hence, the accuracy of the centroids is the key for the partition-based
    clustering algorithm to be successful. The following are the steps involved in
    the centroid-based partitional clustering algorithm:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨k-means聚类算法之前，让我们在这里快速定义一下。输入为*k*，表示期望的聚类数量，将定义*k*个中心或质心，这将有助于定义*k*个分区。基于这些中心（质心），算法识别成员并因此构建一个分区，然后根据识别的成员重新计算新的中心。这个过程会迭代进行，直到暴露出清晰且最优的不相似度，使分区真正独特。因此，质心的准确性是分区聚类算法成功的关键。以下是在基于质心的划分聚类算法中涉及到的步骤：
- en: 'Input: *k* (the number of clusters) and *d* (the data set with *n* objects)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：*k*（聚类数量）和*d*（包含*n*个对象的数据库）
- en: 'Output: Set of *k* clusters that minimize the sum of dissimilarities of all
    the objects to the identified mediod (centroid)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：一组*k*个聚类，这些聚类将所有对象到识别的中位数（质心）的不相似度之和最小化。
- en: Identify the *k* objects as the first set of centroids.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别*k*个对象作为第一组质心。
- en: Assign the remaining objects that are nearest to the centroid.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将剩余的最近质心的对象分配到聚类中。
- en: Randomly select a non-centroid object and recompute the total points that will
    be swapped to form a new set of centroids, until you need no more swapping.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个非质心对象，并重新计算将进行交换以形成新的质心集的总点数，直到不再需要交换。
- en: The Hierarchical and partitional clustering techniques inherently have key differences
    in many aspects, and some of them include some basic assumptions; execution time
    assumptions, input parameters, and resultant clusters. Typically, partitional
    clustering is faster than Hierarchical clustering. While the hierarchical clustering
    can work with similarity measure alone, partitional clustering requires number
    of clusters and details around the initial centers. The Hierarchical clustering
    does not require any input parameters while the partitional clustering algorithms
    require an input value that indicates the number of clusters required to start
    running. The cluster definition for hierarchical clustering technique is more
    subjective as against the partitional clustering results in a exact and precise
    "k" cluster.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类和划分聚类技术在许多方面本质上存在关键差异，其中一些包括一些基本假设；执行时间假设、输入参数和结果聚类。通常，划分聚类比层次聚类更快。虽然层次聚类可以仅使用相似度度量进行工作，但划分聚类需要指定聚类数量以及初始中心的相关细节。与划分聚类结果产生的精确且精确的“k”聚类相比，层次聚类技术的聚类定义更为主观。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The quality of clustering depends on the chosen algorithm, distance function,
    and the application. A cluster quality is said to be the best when the inter-cluster
    distance is *maximized,* and the intra-cluster distance is *minimized*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的质量取决于选择的算法、距离函数和应用。当簇间距离最大化，簇内距离最小化时，簇质量被认为是最佳的。
- en: The k-means clustering algorithm
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 聚类分析算法
- en: In this section, we will cover the k-means clustering algorithm in depth. The
    k-means is a partitional clustering algorithm.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 k-means 聚类分析算法。k-means 是一种划分聚类算法。
- en: 'Let the set of data points (or instances) be as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 设数据点集（或实例）如下：
- en: '*D = {x*[1]*, x*[2]*, …, x*[n]*}*, where'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*D = {x*[1]*, x*[2]*, …, x*[n]*}*, 其中'
- en: '*xi = (xi*[1]*, xi*[2]*, …, xi*[r]*)*, is a vector in a real-valued space *X
    ⊆ R*[r,] and *r* is the number of attributes in the data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*xi = (xi*[1]*, xi*[2]*, …, xi*[r]*)* 是实值空间 *X ⊆ R*[r,] 中的一个向量，且 *r* 是数据中的属性数量。'
- en: The k-means algorithm partitions the given data into *k* clusters with each
    cluster having a center called a centroid.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法将给定的数据划分为 *k* 个簇，每个簇都有一个称为中心点的中心。
- en: '*k* is specified by the user.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 由用户指定。'
- en: 'Given *k*, the k-means algorithm works as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *k*，k-means 算法的工作原理如下：
- en: Algorithm k-means (*k*, *D*)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 k-means (*k*, *D*)
- en: Identify the *k* data points as the initial centroids (cluster centers).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别 *k* 个数据点作为初始中心点（簇中心）。
- en: Repeat step 1.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1。
- en: For each data point *x ϵ D* do.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个数据点 *x ϵ D* 执行以下操作。
- en: Compute the distance from *x* to the centroid.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据点 *x* 到中心点的距离。
- en: Assign *x* to the closest centroid (a centroid represents a cluster).
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *x* 分配到最近的中心点（中心点代表一个簇）。
- en: endfor
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: endfor
- en: Re-compute the centroids using the current cluster memberships until the stopping
    criterion is met.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前的簇成员资格重新计算中心点，直到满足停止标准。
- en: Convergence or stopping criteria for the k-means clustering
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类分析的收敛或停止标准
- en: 'The following list describes the convergence criteria for the k-means clustering
    algorithm:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表描述了 k-means 聚类分析算法的收敛标准：
- en: There are zero or minimum number of reassignments for the data points to different
    clusters
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点重新分配到不同簇的数量为零或最小
- en: There are zero or minimum changes of centroids
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中心点的变化为零或最小
- en: Otherwise, the decrease in the **sum of squared error of prediction** (**SSE**)
    is minimum
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，预测平方误差和（**SSE**）的减少是最小的
- en: If *C[j]* is the *j*^(th) cluster, then *m*[j] is the centroid of cluster *C*[j]
    (the mean vector of all the data points in *C*[j]), and if *dist(x, m*[j]*)* is
    the distance between the data point *x* and centroid *m*[j] then the following
    example demonstrated using graphical representation explains the convergence criteria.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *C[j]* 是第 *j* 个簇，那么 *m*[j] 是簇 *C*[j] 的中心点（*C*[j] 中所有数据点的均值向量），如果 *dist(x,
    m*[j]*)* 是数据点 *x* 和中心点 *m*[j]* 之间的距离，那么以下使用图形表示的示例说明了收敛标准。
- en: 'For example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: Identification of random k centers:![Convergence or stopping criteria for the
    k-means clustering](img/B03980_08_08.jpg)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机识别 k 个中心点：![k-means 聚类分析的收敛或停止标准](img/B03980_08_08.jpg)
- en: 'Iteration 1: Compute centroids and assign the clusters:![Convergence or stopping
    criteria for the k-means clustering](img/B03980_08_09.jpg)'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代 1：计算中心点并分配簇：![k-means 聚类分析的收敛或停止标准](img/B03980_08_09.jpg)
- en: 'Iteration 2: Recompute centroids and reassign the clusters:![Convergence or
    stopping criteria for the k-means clustering](img/B03980_08_10.jpg)'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代 2：重新计算中心点并重新分配簇：![k-means 聚类分析的收敛或停止标准](img/B03980_08_10.jpg)
- en: 'Iteration 3: Recompute centroids and reassign the clusters:![Convergence or
    stopping criteria for the k-means clustering](img/B03980_08_11.jpg)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代 3：重新计算中心点并重新分配簇：![k-means 聚类分析的收敛或停止标准](img/B03980_08_11.jpg)
- en: Terminate the process due to minimal changes to centroids or cluster reassignments.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于中心点或簇重新分配的最小变化而终止过程。
- en: K-means clustering on disk
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 磁盘上的 k-means 聚类分析
- en: 'The k-means clustering algorithm can also be implemented with data on disk.
    This approach is used with large datasets that cannot be accommodated in memory.
    The strategy used here is to compute centroids incrementally by scanning the dataset
    only once for each iteration. The performance of this algorithm is determined
    by how well the number of iterations can be controlled. It is recommended that
    a limited set of iterations, less than 50 should be run. Although this version
    helps scaling, it is not the best algorithm for scaling up; there are other alternative
    clustering algorithms that scale-up, for example, BIRCH is one of them. The following
    algorithm describes the steps in disk the k-means algorithm:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 聚类算法也可以在磁盘上的数据上实现。这种方法用于无法容纳在内存中的大型数据集。这里使用的策略是通过对数据集进行一次扫描来增量计算质心。该算法的性能取决于迭代次数的控制程度。建议运行少于
    50 次的有限迭代次数。尽管这个版本有助于扩展，但它并不是扩展的最佳算法；还有其他可扩展的聚类算法，例如，BIRCH 就是其中之一。以下算法描述了磁盘上 k-means
    算法的步骤：
- en: Algorithm disk k-means (*k*, *D*)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘 k-means 算法 (*k*, *D*)
- en: Choose the *k* data points as the initial centroids (cluster centers) *m*[j],
    where *j = 1,2,3….k*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *k* 个数据点作为初始质心（聚类中心） *m*[j]，其中 *j = 1,2,3….k*。
- en: Repeat
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: Initialize *s*[j]*=0*, where *j=1,2,3….k*; (a vector with all the zero values).
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *s*[j]*=0*，其中 *j=1,2,3….k*；（一个所有值都为零的向量）。
- en: Initialize *n*[j]*=0*, where *j=1,2,3….k*; (n[j] is number points in the cluster),
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *n*[j]*=0*，其中 *j=1,2,3….k*；（n[j] 是聚类中的点数），
- en: For each data point *x ϵ D* do.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个数据点 *x ϵ D* 进行。
- en: '*j = arg min dist(x, m*[j]*)*.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*j = arg min dist(x, m*[j]*)*。'
- en: Assign *x* to the cluster *j*.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *x* 分配到聚类 *j*。
- en: '*s*[j] *= s*[j] *+ x*.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*s*[j] *= s*[j] *+ x*。'
- en: '*n*[j] *= n*[j] *+ 1*.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*n*[j] *= n*[j] *+ 1*。'
- en: endfor.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: endfor.
- en: '*m*[i] *= s*[j]*/n*[j], where *i=1,2,…k.*'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*m*[i] *= s*[j]*/n*[j]，其中 *i=1,2,…k*。'
- en: Until the stopping, the criterion is met.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到满足停止条件。
- en: Advantages of the k-means approach
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 方法的优点
- en: 'The k-means way of unsupervised learning has many benefits; some of them are
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的 k-means 方法有许多好处；其中一些如下：
- en: The k-means clustering is popular and widely adopted due to its simplicity and
    ease of implementation.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其简单性和易于实现，k-means 聚类在流行和广泛采用。
- en: It is efficient and has optimal time complexity defined by *O*(*ikn*), where
    *n* is the number of data points, *k* is the number of clusters, and *i* is the
    number of iterations. Since the *l* and *k* values are kept small, the k-means
    clustering can represent a linear expression too.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它是高效的，并且具有由 *O*(*ikn*) 定义的优化的时间复杂度，其中 *n* 是数据点的数量，*k* 是聚类的数量，*i* 是迭代的数量。由于 *l*
    和 *k* 的值保持较小，k-means 聚类也可以表示线性表达式。
- en: Disadvantages of the k-means algorithm
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 算法的缺点
- en: 'The following are the downsides or disadvantages of the k-means algorithm:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 k-means 算法的缺点或劣势：
- en: The value of *k* is always a user input and is as good as the identified number
    *k.*
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 的值始终是用户输入的，并且与识别出的 *k* 数值相同。'
- en: This algorithm is applicable only when the means are available, and in the case
    of categorical data the centroids are none other than the frequent values.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此算法仅适用于均值可用时，在分类数据的情况下，质心不过是频繁值。
- en: Clusters can never be elliptical and are always hyperspherical.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类永远不会是椭圆形的，总是超球形的。
- en: The clusters identified are very sensitive to the initially identified seeds,
    and can be different when to run multiple times with different random seeds involved.
    The following figure depicts how two different centroids can change the clusters.
    This can be achieved by iterative processing:![Disadvantages of the k-means algorithm](img/B03980_08_12.jpg)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别出的聚类对最初识别的种子非常敏感，当涉及不同的随机种子多次运行时可能会有所不同。以下图展示了两个不同的质心如何改变聚类。这可以通过迭代处理实现：![k-means
    算法的缺点](img/B03980_08_12.jpg)
- en: Again, k-means is very sensitive to outliers. Outliers can be the errors in
    the data recording or some special data points with very different values. The
    following diagram depicts the skew that an outlier can bring into the cluster
    formation. The first representation shows the ideal cluster, and the second one
    shows the undesirable cluster:![Disadvantages of the k-means algorithm](img/B03980_08_13.jpg)![Disadvantages
    of the k-means algorithm](img/B03980_08_14.jpg)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次强调，k-means算法对异常值非常敏感。异常值可能是数据记录中的错误，或者是具有非常不同值的特殊数据点。以下图表展示了异常值可能对簇形成带来的偏斜：第一个表示理想簇，第二个表示不理想的簇：![k-means算法的缺点](img/B03980_08_13.jpg)![k-means算法的缺点](img/B03980_08_14.jpg)
- en: Many of the algorithms and learning techniques that we have seen until now are
    sensitive to outliers. There are some standard techniques that can be employed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至今所见到的许多算法和学习技术都对异常值敏感。有一些标准技术可以被采用。
- en: One way is to get the outliers filtered from evaluation, and this requires us
    to apply some techniques to handle the noise in the data. The noise reduction
    techniques will be covered in the next chapters. In the case of k-means clustering,
    the removal of outliers can be done after a few iterations just to make sure the
    identified data points are really the outliers. Or, another way is to stick to
    a smaller sample of data on which the algorithm will be run. This way, the possibility
    of choosing an outlier will be minimal.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是从评估中过滤掉异常值，这需要我们应用一些技术来处理数据中的噪声。噪声减少技术将在下一章中介绍。在k-means聚类的情况下，可以在几次迭代后删除异常值，以确保确定的数据点确实是异常值。或者，另一种方法是坚持使用较小的数据样本来运行算法。这样，选择异常值的可能性将最小。
- en: Distance measures
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 距离度量
- en: 'The distance measure is important in clustering algorithms. Reassigning data
    points to the clusters is determined by redefining the centroids. The following
    are some ways of measuring distance between two clusters:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 距离度量在聚类算法中非常重要。重新分配数据点到簇中是由重新定义质心来决定的。以下是一些测量两个簇之间距离的方法：
- en: '**Single link**: This method refers to measuring the distance between the two
    closest data points that belong to two different clusters. There can be noise
    in the data that might be considered with seriousness too.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链**: 这种方法指的是测量属于两个不同簇的两个最近数据点之间的距离。数据中可能存在噪声，这也应被认真考虑。'
- en: '**Complete link**: This method refers to measuring the distance between two
    farthest data points that belong to two different clusters. This method can make
    the clusters more sensitive to outliers.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全链**: 这种方法指的是测量属于两个不同簇的两个最远数据点之间的距离。这种方法可以使簇对异常值更加敏感。'
- en: '**Average link**: This method uses the average distance measure of all the
    pairs of distances between the two clusters.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链**: 这种方法使用两个簇之间所有距离对的平均距离度量。'
- en: '**Centroids**: This method refers to measuring the distance between the two
    clusters by measuring the distance between their centroids.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质心**: 这种方法指的是通过测量两个簇之间的质心距离来测量两个簇之间的距离。'
- en: Complexity measures
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂度度量
- en: Choosing the best clustering algorithm has always been a challenge. There are
    many algorithms available and both, accuracy and complexity measures are important
    for choosing the right algorithm. The single link method can help achieve *O(n2)*;
    complete and average links can be done in *O(n2logn)*. There are both advantages
    and limitations for each of the algorithms, and they work well in certain contexts
    of data distribution; no standard patterns in the data distribution make it a
    complex problem to solve. Hence, data preparation and standardization becomes
    an important aspect in Machine learning. Which distance measure would be an ideal
    choice can only be determined by implementing the different distance measures
    iteratively, and comparing the results across iterations. The clustering methods
    overall are highly dependent on the initial choices and can be subjective.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的聚类算法一直是一个挑战。有许多算法可供选择，并且准确性和复杂性度量在选择正确的算法时都很重要。单链方法可以帮助实现 *O(n^2)*；完全链接和平均链接可以在
    *O(n^2logn)* 内完成。每种算法都有其优点和局限性，它们在数据分布的某些上下文中表现良好；数据分布中没有标准模式使得解决这个问题变得复杂。因此，数据准备和标准化成为机器学习中的一个重要方面。哪种距离度量是一个理想的选择，只能通过迭代实现不同的距离度量，并比较迭代结果来确定。总体而言，聚类方法高度依赖于初始选择，并且可能具有主观性。
- en: Implementing k-means clustering
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现k-means聚类
- en: Refer to the source code provided for this chapter for implementing the k-means
    clustering methods (only supervised learning techniques - source code path `.../chapter08/...`
    under each of the folders for the technology).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章提供的源代码以实现k-means聚类方法（仅监督学习技术 - 每个技术文件夹下的源代码路径为`.../chapter08/...`）。
- en: Using Mahout
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Mahout
- en: Refer to the folder `.../mahout/chapter8/k-meansexample/`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../mahout/chapter8/k-meansexample/`。
- en: Using R
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用R
- en: Refer to the folder `.../r/chapter8/k-meansexample/`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../r/chapter8/k-meansexample/`。
- en: Using Spark
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark
- en: Refer to the folder `.../spark/chapter8/k-meansexample/`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../spark/chapter8/k-meansexample/`。
- en: Using Python (scikit-learn)
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python（scikit-learn）
- en: Refer to the folder `.../python-scikit-learn/chapter8/k-meansexample/`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../python-scikit-learn/chapter8/k-meansexample/`。
- en: Using Julia
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Julia
- en: Refer to the folder `.../julia/chapter8/k-meansexample/`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹`.../julia/chapter8/k-meansexample/`。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have covered the clustering-based learning methods. We have
    taken a deep dive into the k-means clustering algorithm using an example. You
    have learned to implement k-means clustering using Mahout, R, Python, Julia, and
    Spark. In the next chapter, we will cover the Bayesian methods and in specific,
    the Naïve-Bayes algorithm.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了基于聚类的学习方法。我们通过一个示例深入探讨了k-means聚类算法。你已经学会了使用Mahout、R、Python、Julia和Spark实现k-means聚类。在下一章中，我们将介绍贝叶斯方法，特别是朴素贝叶斯算法。
