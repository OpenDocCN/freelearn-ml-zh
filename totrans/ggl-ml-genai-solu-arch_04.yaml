- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI/ML Tooling and the Google Cloud AI/ML Landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we take a look at the various tools in Google Cloud that can
    be used to implement AI/ML workloads. We start off with a quick overview of some
    of the fundamental Google Cloud services that function as the building blocks
    for almost all workloads on Google Cloud. We then progress toward more advanced
    services that are used specifically for data science and AI/ML workloads. This
    is the final chapter in the *Basics* part of this book, and like the previous
    two chapters, it provides foundational information that we build upon throughout
    the book. If you already have knowledge of Google Cloud’s services, this chapter
    may serve as a refresher for that knowledge. If you are new to Google Cloud, this
    chapter is an essential part of your learning process, because it introduces concepts
    that are assumed to be known in the rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: To describe how each tool is used in the context of data science projects, we
    will refer to the steps of the ML model lifecycle, as laid out in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035).
    *Figure 3**.1* shows a simplified diagram of the ML model lifecycle. In reality,
    combinations of these steps could be repeated in cycles throughout the model lifecycle,
    but we will omit those details for simplicity at this point. Our simplified workflow
    example assumes that the outputs from each step are satisfactory, and we can move
    on to the next step in the process. It also includes hyperparameter optimization
    in the **Train** **Model** step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Simplified ML model lifecycle](img/B18143_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Simplified ML model lifecycle'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why Google Cloud?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prerequisites for using Google Cloud tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud services overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud tools for data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud VertexAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard industry tools on Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right tool for the job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with a discussion of why we would want to use Google Cloud for data
    science and AI/ML use cases in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Why Google Cloud?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google has been a well-known leader in the AI/ML space for a very long time.
    They have contributed a lot to the AI/ML industry, through countless research
    papers, publications, and donations of AI/ML libraries to the open source community,
    such as TensorFlow, one of the most widely used ML libraries of all time. Their
    search and advertising algorithms have been leading their respective industries
    for years, and their peer organizations, such as DeepMind, dedicate their entire
    existence to pure AI/ML research.
  prefs: []
  type: TYPE_NORMAL
- en: Google has also been spearheading initiatives such as **Ethical AI**, championing
    the concepts of fairness and explainability to ensure that AI is held accountable
    and used only for purposes that are beneficial to humans. AI/ML is not something
    that Google is trying to use, but rather it is a core tenet of Google’s business.
  prefs: []
  type: TYPE_NORMAL
- en: A significant testament to Google Cloud’s leadership in this space was when
    Gartner officially recognized them as a leader in the 2022 Gartner® Magic Quadrant™
    for Cloud AI Developer Services. Google Cloud provides a wide array of services
    and tools for implementing AI/ML use cases and embraces open source and third-party
    solutions in order to provide the broadest selection possible to their customers.
    By using Google Cloud for AI/ML workloads, you can benefit from the decades of
    AI/ML research performed, and expertise gained, by Google in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for using Google Cloud tools and services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is going to be pretty simple because Google Cloud makes it very
    easy to get started in trying out its services. If you have a Gmail account, then
    you pretty much already have everything you need to get started on Google Cloud.
    As a generous bonus, Google Cloud gives USD $300 in credits to new customers,
    plus additional free credits to new customers who verify their business email
    addresses. You can use those free credits to explore and evaluate Google Cloud’s
    various services. Also, many of Google Cloud’s services provide a **Free Tier**,
    which allows you to use those services free of charge up to their specified free
    usage limit, the details of which you can find in the Google Cloud documentation
    ([https://cloud.google.com/free/docs/free-cloud-features](https://cloud.google.com/free/docs/free-cloud-features)).
  prefs: []
  type: TYPE_NORMAL
- en: If you need to create a new Google Cloud account, you can sign up at [https://console.cloud.google.com/freetrial](https://console.cloud.google.com/freetrial),
    and when you need to go beyond free usage, you can upgrade to a paid Cloud Billing
    account.
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve created and logged into your account, you can start using the Google
    Cloud services that we’ll be using in this book, and many more. When you first
    try to use a Google Cloud service, you may need to enable the API for that service.
    This is a simple, one-click action that you only need to perform once in each
    Google Cloud project. *Figure 3**.2* shows an example of the page that’s displayed
    when you first try to use the Google Filestore service (we will describe Filestore
    in more detail later in this chapter). You can simply click the **Enable** button
    to enable the API.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: A Google Cloud project organizes all your Google Cloud resources. It consists
    of a set of users; a set of APIs; and billing, authentication, and monitoring
    settings for those APIs. All Google Cloud resources, along with user permissions
    for accessing them, reside in a project.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Enabling a Google Cloud API on first use](img/B18143_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Enabling a Google Cloud API on first use'
  prefs: []
  type: TYPE_NORMAL
- en: Security, privacy, and compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we need to think about when we’ve created our Google Cloud account
    is security. This also extends to privacy and compliance, and these are hot topics
    in the data analytics and AI/ML industries today because your customers want to
    know that their data is being handled securely. Fortunately, these topics are
    major priorities for Google Cloud, and as a result, Google Cloud provides a plethora
    of default controls and dedicated services to facilitate and uphold these priorities.
    We will introduce some of the important concepts and related services briefly
    here, and we will dive deeper into these topics in later chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Who has access to what?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first topic to discuss in the context of security, privacy, and compliance
    is identity and access management; that is, identifying and controlling who has
    access to which resources in your Google Cloud environments. Google Cloud provides
    the **Identity and Access Management** (**IAM**) service for this purpose. This
    service enables you to define identities such as users and groups of users, and
    permissions with regard to accessing Google Cloud resources. For every attempted
    action on Google Cloud, whether it’s to read an object from storage or to run
    a piece of code as a cloud function, the permissions associated with that action,
    the resource upon which the action would act, and the invoking identity would
    be evaluated by Google Cloud IAM, and the action would only be permitted if the
    correct combination of permissions has been applied to all relevant identities
    and resources. For additional convenience, you can integrate Google Cloud IAM
    with external **Identity Providers** (**IdPs**) and directories, such as Active
    Directory.
  prefs: []
  type: TYPE_NORMAL
- en: Data security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud encrypts all data at rest by default. You can control the keys
    that are used to encrypt your data by using **Customer-Managed Encryption Keys**
    (**CMEKs**), or you can let Google Cloud manage all that functionality for you.
    Regarding data in transit, Google has built global networks with stringent security
    controls, and TLS encryption is used to protect data that is being transported
    throughout these global networks. Google Cloud also enables you to encrypt your
    data even when the data is actively in use, through Confidential Computing, which
    uses a hardware-based **Trusted Execution Environment** (**TEE**). TEEs are secure
    and isolated environments that prevent unauthorized access or modification of
    applications and data while they are in use.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to Google Cloud’s state-of-the-art infrastructure security controls,
    Google Cloud provides tools to help prevent and detect potential security threats
    and vulnerabilities. For example, you can use Cloud Firewall and Cloud Armor to
    prevent **Distributed Denial-of-Service** (**DDoS**) and common OWASP threats.
    You can use Chronicle, Security Command Center, and Mandiant, for **Security Incident
    and Event Monitoring** (**SIEM**), **Security Orchestration Automation and Response**
    (**SOAR**), intrusion detection, and threat intelligence. In addition to all of
    these Google Cloud services, you can use third-party observability and reporting
    services such as Splunk on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud provides audit data that tracks the actions being performed by
    identities on resources in your environments, which is important for compliance
    reasons. Google Cloud participates in formal compliance programs such as FedRamp,
    SOC2, and SOC3, and supports compliance standards such as **Payment Card Industry
    Data Security Standard** (**PCI DSS**), and multiple ISO/IEC international standards.
    You can see additional details regarding Google Cloud’s compliance program participation
    at [https://cloud.google.com/security/compliance](https://cloud.google.com/security/compliance).
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with Google Cloud services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are numerous ways in which you can interact with Google Cloud services.
    At a high level, you can either use the **Graphical User Interface** (**GUI**),
    the **Command-Line Interface** (**CLI**), or the API. We explore each of these
    options in more detail in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Console
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most straightforward ways to interact with Google Cloud services
    is via the Google Cloud console, which provides a GUI. You can access the console
    at [https://console.cloud.google.com/](https://console.cloud.google.com/).
  prefs: []
  type: TYPE_NORMAL
- en: The console enables you to perform actions in Google Cloud by clicking around
    in a web-based interface in your browser. For example, you create a **virtual
    machine** (**VM**) by clicking **Compute Engine** in the products menu, and then
    going to the VM instances page and clicking **Create an instance** to specify
    the desired properties of your VM, as shown in *Figure 3**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Creating a VM in the Google Cloud console](img/B18143_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Creating a VM in the Google Cloud console'
  prefs: []
  type: TYPE_NORMAL
- en: The gcloud CLI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you prefer to use a CLI, Google Cloud has built a tool named **gcloud**,
    which enables you to interact with Google Cloud services by executing text-based
    commands. This is particularly useful if you wish to automate sequences of Google
    Cloud service API actions by composing scripts that run multiple commands in order.
    For example, you could create a Bash script that contains multiple commands, and
    you could execute that script either manually or on a periodic schedule if it
    contains actions that frequently need to be repeated. This approach would be suitable
    for ad hoc automation that can be implemented with little effort. There are other
    ways to automate more complex sequences of actions on Google Cloud, which we will
    explore in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a `gcloud` command. This command will enable
    the API for `SERVICE_NAME`, which is a placeholder for the name of the Google
    Cloud service with which we want to interact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to enable the Filestore API, rather than clicking the `gcloud`
    CLI command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `file` is the command-line name for the Google Cloud Filestore
    service (the full service name is `file.googleapis.com`).
  prefs: []
  type: TYPE_NORMAL
- en: To use the gcloud CLI, you can install it on any machine on which you wish to
    run it, as it supports many different operating systems, such as Linux, macOS,
    and Windows, or you can use Google Cloud Shell, described next.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Shell
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Google Cloud Shell is a very convenient way to use the gcloud CLI and interact
    with Google Cloud APIs. It’s a tool that provides a Linux-based environment in
    which you can issue commands to Google Cloud service APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can open the Cloud Shell by clicking on the ![](img/icon.png) symbol in
    the top-right corner of the Google Cloud console screen, as shown in *Figure 3**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Activating Google Cloud Shell](img/B18143_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Activating Google Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: 'The terminal will then appear at the bottom of the screen, as shown in *Figure
    3**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Google Cloud Shell](img/B18143_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Google Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: When you first try to use Cloud Shell, you need to authorize it to interact
    with Google Cloud service APIs, as depicted in *Figure 3**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: Authorizing Google Cloud Shell](img/B18143_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Authorizing Google Cloud Shell'
  prefs: []
  type: TYPE_NORMAL
- en: API access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most low-level method for interacting with Google Cloud services is by
    programmatically invoking their APIs directly. This method differs from the GUI
    and CLI access because it is not intended for direct human interaction, but rather
    it is suitable for more advanced use cases, such as interacting with Google Cloud
    services via your application software. As an example, let’s consider an application
    that saves users’ photos in the cloud. When new users sign up, we may wish to
    create a new Google Cloud Storage bucket to store their photos, among other signup-related
    activities (we will describe the Google Cloud Storage service later in this chapter).
    We could create the following REST API request for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: '`JSON_FILE_NAME` is the name of the required JSON file that specifies the bucket
    details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OAUTH2_TOKEN` is an access token that is required to invoke the API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PROJECT_IDENTIFIER` is the ID or number of the project with which our bucket
    will be associated, for example, `my-project`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The required JSON file is structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the breakdown of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BUCKET_NAME` is the name we want to give our bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BUCKET_LOCATION` is the location where you want to store your bucket object
    data. For more information regarding Google Cloud locations, refer to the Google
    Cloud documentation here: [https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STORAGE_CLASS` is the default storage class of your bucket. For more information
    regarding Google Cloud Storage classes, refer to the Google Cloud documentation
    here: [https://cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, it’s most common to use Google Cloud’s client **Software Development
    Kits** (**SDK**s) to create such API calls programmatically. For example, in the
    following Python code, we import the Google Cloud Storage client library, and
    we then define a function to create a new bucket, specifying the bucket name,
    the location, and the storage class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve covered some of the basics of how to interact with Google Cloud
    services, let’s discuss the types of Google Cloud services we will use in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud services overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having covered the basics of how to set up a Google Cloud account and how to
    enable and interact with the various services, we will now introduce the services
    that we are going to use in this book to create AI/ML workloads. We will first
    cover the fundamental cloud services upon which almost all workloads are built,
    and then we will cover the more advanced services related to data science and
    AI/ML.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud computing services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considering that the word *computing* is included directly in the term **cloud
    computing**, and the fact that computing services form the basis of all other
    cloud services, we will start this section with a brief overview of Google Cloud’s
    computing services.
  prefs: []
  type: TYPE_NORMAL
- en: Google Compute Engine (GCE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A few years ago, the term cloud computing was pretty much synonymous with the
    term **virtualization**. Traditionally, companies had physical servers on their
    own premises, and this was then contrasted with creating virtual servers in the
    cloud, either public or private. Hence, perhaps the easiest concept to understand
    in cloud computing is virtualization, where we simply create a virtual server
    instead of a physical server by introducing an abstraction layer called a hypervisor
    between the hardware and our server’s operating system, as depicted in *Figure
    3**.7*. For most companies, if they’re already running physical servers, then
    their first foray into the world of the cloud is usually implemented by using
    VMs because this is the simplest step to transition from the physical paradigm
    to the cloud paradigm. **Google Compute Engine** (**GCE**) is Google Cloud’s service
    for running VMs in the cloud. It provides some useful features, such as auto-scaling
    based on demand, which is one of the well-established benefits of cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Example VM implementation](img/B18143_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Example VM implementation'
  prefs: []
  type: TYPE_NORMAL
- en: Google Kubernetes Engine (GKE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A newer type of virtualization was created in the 2000s by using Linux **cgroups**
    and **Namespaces** to isolate computing resources such as CPUs, RAM, and storage
    resources, for specific processes within a running operating system. With containerization,
    the abstraction layer moves higher in the stack, whereby it exists between the
    operating system and our applications, as depicted in *Figure 3**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Example container implementation](img/B18143_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Example container implementation'
  prefs: []
  type: TYPE_NORMAL
- en: This gives us some interesting benefits beyond those afforded by hypervisor-based
    virtualization. For example, containers are generally much smaller and more *lightweight*
    than VMs, meaning they contain much fewer software components. While a VM has
    to boot up an entire operating system and lots of software applications before
    it becomes usable, which can take a few minutes, a container usually only contains
    your application code and any required dependencies and can therefore be loaded
    in seconds. This makes a big difference when it comes to auto-scaling and auto-healing
    cloud-based software workloads. Starting up new VMs in relation to a sudden increase
    in traffic may not happen quickly enough and you may lose some requests while
    the VMs boot up and load your application. The same applies to restarting a VM
    due to some kind of problem. In both of those cases, a container would usually
    start much more quickly. Containing fewer components also means that containers
    can be deployed much more quickly, and this makes them a perfect environment for
    microservices with DevOps CI/CD pipelines. There are also many other benefits
    of containers, such as portability and easy manageability.
  prefs: []
  type: TYPE_NORMAL
- en: However, one of the challenges introduced by containerization also stems from
    their lighter footprint. Because they are generally smaller than VMs, it’s common
    to have more of them in a single application deployment. Managing lots of tiny
    containers can be challenging, especially in terms of application lifecycle management
    and orchestration; that is, determining how and where to run your workloads, and
    assigning adequate computing resources to them. This is where Kubernetes comes
    into the picture. The following are the official definitions for Kubernetes, in
    general, and **Google Kubernetes Engine** (**GKE**), in particular.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes, also known as **K8s**, is an open source system for automating the
    deployment, scaling, and management of containerized applications. It groups containers
    that make up an application into logical units for easy management and discovery.
    GKE provides a managed environment for deploying, managing, and scaling your containerized
    applications using Google infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.9* shows an example of how Kubernetes organizes and orchestrates
    applications. It deploys your applications as Pods, which are groups of containers
    with similar functionality, and it deploys agents on your hardware servers or
    the host operating systems that keep track of resource utilization and communicate
    that information back to the Kubernetes master, which uses that information to
    manage Pod deployments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: Example GKE implementation  (source: https://kubernetes.io/docs/concepts/architecture/)](img/B18143_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Example GKE implementation (source: https://kubernetes.io/docs/concepts/architecture/)'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud serverless computing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The term **serverless** in the context of cloud computing refers to the concept
    of running your code on a cloud provider’s infrastructure without needing to manage
    any of the servers that will be used to run your code. In reality, there are still
    servers that are being used, behind the scenes, but the cloud provider creates
    and manages them on your behalf so that you don’t need to perform those actions.
    Google Cloud has two primary services that relate to serverless computing, which
    are named **Cloud Functions**, and **Cloud Run**. Another Google Cloud service,
    named **App Engine**, is also often bundled under the serverless umbrella, and
    we will describe that service, and how it differs from Cloud Functions and Cloud
    Run, later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Many other Google Cloud services also run in a serverless fashion, whereby the
    actions they perform on your behalf run on servers that are managed in the background,
    without any need for you to manage those servers. However, Cloud Functions and
    Cloud Run are the two Google Cloud services that relate to *serverless computing*,
    which specifically refers to running your code without the need to explicitly
    manage servers.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With Cloud Functions, you simply write small pieces of code — for example, a
    single function — and Google Cloud will run that code for you in response to events
    that you specify as triggers to run that code. You don’t need to manage any containers,
    servers, or infrastructure on which your code executes, and there are many types
    of triggers that you can configure. For example, whenever a file is uploaded to
    your Google Cloud Storage bucket, that could trigger your piece of code to execute.
    Your code could then process that file in some way, feed it into another Google
    Cloud service to be processed, or simply send a notification to inform somebody
    that the file has been uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: This concept is referred to as **Functions as a Service** (**FaaS**) because
    it is usually used to execute a single function for each event trigger. This approach
    is suitable for when you want to simply write and run small code snippets that
    respond to events that occur in your environment. You can also use Cloud Functions
    to connect with other Google Cloud or third-party cloud services to streamline
    challenging orchestration problems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to sparing you the trouble of managing servers, another advantage
    of using Cloud Functions is that you don’t have to pay for servers when no events
    are happening in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Run
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cloud Run is a different type of serverless computing service that is more suitable
    for long-running application processes. While cloud functions are intended to
    run small pieces of code in response to specific events that occur, Cloud Run
    can run more complex applications. This also means that it provides more flexibility
    and control with regard to how your code executes. For example, it runs your code
    in containers, and you have more control over what executes in those containers.
    If your application requires custom software package dependencies, for example,
    you can provision those dependencies to be available in your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Run abstracts away all infrastructure management by automatically scaling
    up and down from zero almost instantaneously, depending on traffic, and it only
    charges you for the exact resources you use.
  prefs: []
  type: TYPE_NORMAL
- en: App Engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While App Engine can also be considered as a serverless service, because it
    manages the underlying infrastructure for you, its use cases differ from those
    of Cloud Functions and Cloud Run. App Engine comes in two levels of service, referred
    to as **Standard** and **Flexible**. In the standard environment, your application
    runs on a lightweight server inside a sandbox. This sandbox restricts what your
    application can do. For example, the sandbox only allows your app to use a limited
    set of software binary libraries, and your app cannot write to a permanent disk.
    The standard environment also limits the CPU and memory options available to your
    application. Because of these restrictions, most App Engine standard applications
    tend to be stateless web applications that respond to HTTP requests quickly. In
    contrast, the flexible environment runs your application in Docker containers
    on Google Compute Engine VMs, which have fewer restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that the standard environment can scale from zero instances up to
    thousands very quickly, but the flexible environment must have at least one instance
    running and can take longer to scale up in response to sudden traffic increases.
  prefs: []
  type: TYPE_NORMAL
- en: App Engine is generally suited to large web applications. Its flexible environment
    can be more customizable than Cloud Run. However, if you want to deploy a long-running
    web application without managing the underlying infrastructure, I recommend first
    evaluating whether Cloud Run could meet your application’s needs and comparing
    the costs of running your app on Cloud Run versus App Engine.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Batch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some workflows are intended to run for a long time without the need for human
    interaction. Examples of such workloads include media transcoding, computational
    fluid dynamics, Monte Carlo simulations, genomics processing, and drug discovery,
    among others. These kinds of workloads usually require large amounts of computing
    power and can be optimized by running tasks in parallel. Creating and running
    these jobs by yourself can incur overheads such as managing servers, queueing
    mechanisms, parallelization, and failure logic. Fortunately, the Google Cloud
    Batch service has been built to manage all these kinds of activities for you.
    As a fully managed job scheduler, it automatically scales the infrastructure required
    to run your batch jobs up or down and handles parallelization and retry logic
    that you can configure in case any errors occur during execution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the primary computing services on Google Cloud, let’s
    review some of the services you can use to integrate between different Google
    Cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud integration services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to Google Cloud infrastructure services such as compute and storage,
    we often need to implement integrations between the services in order to create
    complex workloads. Google Cloud has created tools specifically for this purpose,
    and we will briefly discuss some of the relevant tools in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Pub/Sub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud Pub/Sub is a messaging service that can be used to pass data between
    components of your system architecture, whether those components are other Google
    Cloud services, third-party services, or components you’ve built yourself. It’s
    an extremely versatile service that can be used for a wide array of system integration
    use cases, such as decoupling microservices, or streaming data into a data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Pub/Sub relates to the system architecture concept of publishing and subscribing,
    whereby one system can publish a message or a piece of data to a shared space,
    or **Topic**, and other systems can then receive that piece of data by subscribing
    to that topic. The messages can be delivered via either a **Push** or **Pull**
    mechanism. In the case of a push approach, the Pub/Sub service initiates the communication
    with the subscriber systems and sends the message to those systems. In the case
    of a pull model, the subscriber systems initiate the communication to the Pub/Sub
    service and then request or pull the information from the Pub/Sub service.
  prefs: []
  type: TYPE_NORMAL
- en: Pub/Sub also caters to nuanced messaging needs such as publishing messages in
    order (if required) and retrying failed message transmissions. Google Cloud also
    provides an offering called Pub/Sub Lite, which is a lower-cost option with fewer
    features than the regular Pub/Sub product.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to Pub/Sub, Google Cloud Tasks is a service that can be used to implement
    message passing and asynchronous system integration. With Pub/Sub, the publishers
    and subscribers are completely decoupled, and they have no control over each other’s
    implementations. On the other hand, with Cloud Tasks, the publisher (or **task
    producer**) fully controls the overall execution of the workload. It can be specifically
    used for cases where a task producer needs to control the execution timing of
    a specific webhook or remote procedure call. Cloud Tasks is included in this section
    for completeness because it’s an alternative to Pub/Sub for some use cases, but
    we will not use Cloud Tasks in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Eventarc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Eventarc is a Google Cloud service that enables you to build *event-driven*
    workloads. This is a common pattern for companies that want their workloads to
    execute in response to events that happen in their environment. We touched on
    this topic briefly when we introduced Cloud Functions. Cloud Functions can be
    triggered directly by certain event sources, but Eventarc provides much more flexibility
    and control for implementing complex event-driven architectures, in conjunction
    with Cloud Functions and other Google Cloud services, as well as some third-party
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Eventarc uses Pub/Sub to route messages from **Event Providers** to **Event
    Destinations**. As the names suggest, event providers send events to Eventarc,
    and Eventarc sends events to event destinations. It provides a way to standardize
    your event processing architectures, rather than building random, ad hoc event-driven
    implementations between your various system components.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While Eventarc provides a mechanism for standardizing your event-driven workloads,
    Google Cloud Workflows, as the name suggests, is a service that has been specifically
    built to orchestrate complex workflows, in which the coordination of activities
    among various systems needs to be implemented in a specific order. With this in
    mind, Google Cloud Workflows and Eventarc make a great pair when used together
    to implement complex, event-driven workloads. Workflows can either be triggered
    by events, or you can create batch workflows that can be triggered in different
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: Workflows can orchestrate activities between various microservices and custom
    or third-party APIs. The Workflows service maintains the state of each step in
    your workload during execution, meaning that it tracks the inputs and outputs
    of each step, and it knows which steps have already been completed, which steps
    are currently executing, and which steps remain to be invoked in the workflow.
    It allows you to visualize all of your workflow’s steps and their dependencies,
    and if any step in the process fails, you can use the Workflows service to figure
    out which one and determine what to do next. *Figure 3**.10* shows an example
    of a workflow process for an online retail system, in which a customer purchases
    an item. Various Google Cloud computing products are used to run each of the software
    services in the workflow, and the coordination of each of the steps in the process
    is managed by the Workflows service. While this is an example of a simple order
    processing workflow, note that most large retail companies work with supply chains
    consisting of extremely complex webs of interconnected systems and partners.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Example workflow for online retail system](img/B18143_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Example workflow for online retail system'
  prefs: []
  type: TYPE_NORMAL
- en: The Workflows service is best suited to orchestrating activities between services.
    If you want to implement an orchestration workflow for data engineering, then
    Google Cloud Composer may be more suitable. We discuss Google Cloud Composer later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud Scheduler is a relatively simple but very useful service that can
    be used to execute workloads according to a schedule. For example, if you want
    a process to run at the same time every day, every hour, or every month, you could
    use Cloud Scheduler to define and kick off those executions. Any of you who are
    familiar with Unix-based operating systems may see a similarity with the cron
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Scheduler can be used in conjunction with many of the integration
    services we’ve described in this section. For example, you could schedule a message
    to be sent to a Pub/Sub topic every 15 minutes, and that could then be sent to
    Eventarc and used to invoke a cloud function.
  prefs: []
  type: TYPE_NORMAL
- en: Networking and connectivity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Very few workloads exist without the need to set up some kind of network connectivity.
    For example, even if you have only a single server, you generally need to connect
    to it in some way in order to perform any actions on it. As you scale beyond a
    single server, those servers usually need to communicate with each other. In this
    section, we discuss the fundamental networking and connectivity concepts upon
    which we will build our workloads in later chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Private Cloud (VPC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first concept we introduce in this section is the **Virtual Private Cloud**
    (**VPC**) concept. A VPC is a virtual network that can span all Google Cloud regions.
    The reason it is called a virtual private cloud is that it defines the boundaries
    of your networking infrastructure, and therefore where you run your workloads
    within Google Cloud. You can, however, peer or share connectivity with other VPCs
    in order to communicate across VPC boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid networking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you’re working for a company that has its own on-premises servers and networks,
    and you want to connect them to the cloud, this is referred to as **Hybrid Connectivity**.
    It’s a common need for many companies, and therefore, Google Cloud has created
    specific solutions to facilitate this kind of connectivity, which consist of the
    following offerings.
  prefs: []
  type: TYPE_NORMAL
- en: Dedicated Interconnect
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dedicated Interconnect provides direct physical connections between your on-premises
    network and Google’s network. It offers a guaranteed uptime of 99.99% and can
    connect either one or two links that each can support up to 100 **gigabits per
    second** (**Gbps**) in bandwidth. It requires hardware connectivity to be set
    up at specific Dedicated Interconnect locations, and therefore it can require
    non-trivial effort to set it up. This option is for companies who need high-bandwidth
    networking between their premises and Google Cloud for long-lived connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: Partner Interconnect
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you don’t have your own infrastructure built at one of the Dedicated Interconnect
    locations, there are Google Cloud partners that offer connectivity with up to
    99.99% availability through Partner Interconnect. This option also requires some
    effort in working with the partners to set it up, but it doesn’t require the same
    amount of investment as Dedicated Interconnect. A trade-off is that the partners
    generally share the connections among many customers, so the bandwidth is less
    than that of Dedicated Interconnect.
  prefs: []
  type: TYPE_NORMAL
- en: Private Google Access (PGA) for on-premises
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is a basic connectivity option that provides direct access to Google services
    such as Cloud Storage and BigQuery from your on-premises locations.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Private Network (VPN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Perhaps the easiest way to connect your on-premises resources to your Google
    Cloud VPC is via a **Virtual Private Network** (**VPN**), which uses **IP security**
    (**IPsec**) mechanisms to offer a low-cost option that delivers 1.5 – 3.0 Gbps
    of throughput over an encrypted public internet connection. Unlike the Interconnect
    services mentioned previously, this option does not require any special, hardware-related
    network connectivity in any specific location.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the fundamental Google Cloud services that underpin the
    workloads that we’ll build in this book, it’s time to dive into the services that
    we will directly use to create our data processing workloads to prepare for our
    AI/ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud tools for data storage and processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since gathering data is the first major step in an AI/ML project (after establishing
    the business objectives of the project), we begin our exploration of Google Cloud’s
    AI/ML-related services by first reviewing the tools for storing and processing
    data. *Figure 3**.2* shows the steps in the life cycle that relate to ingesting,
    storing, and processing data. It should be noted that the **Train Model**, **Evaluate
    Model**, and **Monitor Model** steps would also usually create outputs that need
    to be stored somewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: Ingesting, storing, exploring, and processing data](img/B18143_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: Ingesting, storing, exploring, and processing data'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 3**.11*, and as we’ve discussed previously, working
    with data is a very prominent part of any AI/ML project.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we can do anything with data in Google Cloud, we need to get access to
    the data, and we often want to ingest that data into some kind of storage service
    on Google Cloud. In this section, we’ll discuss some of the tools that exist on
    Google Cloud for ingesting data, and in the next section, we will cover the Google
    Cloud storage systems into which the ingestion services ingest data. We will not
    focus on Google Cloud’s database services in this chapter, nor the related **Database
    Migration Service** (**DMS**), because for machine learning purposes, we would
    usually extract data from databases and place it into one of the storage systems
    described in this section. An exception to this may be Google Cloud Bigtable,
    but we will discuss that service separately in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: gsutil
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Perhaps the simplest way to transfer data to **Google Cloud Storage** (**GCS**),
    or between GCS buckets, is via the gsutil command-line tool, which can be used
    to transfer up to 1 TB of data with a simple command.
  prefs: []
  type: TYPE_NORMAL
- en: The Data transfer service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to transfer more than 1 TB of data, you can use the data transfer
    service, which transfers data quickly and securely from on-premises systems or
    other public cloud providers. For large data migration projects, in which you
    may wish to run multiple data transfer jobs, it lets you centralize your job management
    to monitor the status of each job. You can transfer petabytes of data consisting
    of billions of files, at up to tens of Gbps of bandwidth, and the data transfer
    service will optimize your network bandwidth to accelerate transfers. You can
    ingest your data into GCS and then have other Google Cloud services access it
    from there.
  prefs: []
  type: TYPE_NORMAL
- en: The BigQuery Data Transfer Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The BigQuery Data Transfer Service automates data movement specifically into
    BigQuery, on a scheduled, managed basis. You can access the BigQuery Data Transfer
    Service using the Google Cloud console, the **bq** command-line tool, or the BigQuery
    Data Transfer Service API. It supports lots of data sources, such as GCS, Google
    Ads, YouTube, Amazon S3, Amazon Redshift, Teradata, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Data storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many different ways to store data in Google Cloud, and the types of
    tools and services you select for data storage will depend on your use case and
    what you’re trying to achieve. In this section, we’ll take a look at the different
    products and services provided by Google Cloud in the data storage space, and
    the kinds of workloads to which they best relate.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts – data warehouses, data lakes, and lake houses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before diving into each of the major Google Cloud data storage services, it’s
    important to discuss the concepts of data warehouses, data lakes, and lake houses,
    which are all terms that have become quite popular in the industry in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: A **data warehouse** usually contains structured data in a format that is optimized
    for analytics purposes, such as columnar data formats like **Parquet** or **Optimized
    Row Columnar** (**ORC**). This is because data analytics queries often operate
    on database columns rather than rows. For example, we might run a query to find
    out the average age of customers who buy our products, and this query would therefore
    focus on the *age* column in our customer database table. Columnar data formats
    store all of the elements of each column near each other on the physical storage
    disks so queries that operate on database columns run more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: A **data lake**, as the name suggests, serves as a reservoir in which you can
    store huge amounts of data in a variety of formats, both structured and unstructured.
    Because there are no specific requirements regarding query optimization, data
    lakes can usually store much more data than data warehouses. Data lakes are a
    key ingredient in breaking down the problematic data silos that we described in
    [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), and they can serve as the foundation
    of your data management strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The term, **data lake house**, refers to more recently emerging patterns, in
    which companies utilize a combination of data warehouses and data lakes in order
    to get the best of both worlds and support a broader set of use cases, such as
    real-time analytics, batch data processing, machine learning, and visualization,
    all from the same source.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Storage (GCS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to Google Compute Engine, **Google Cloud Storage** (**GCS**) is
    one of the most fundamental services in Google Cloud. It supports what’s referred
    to as **object** storage, and it is perhaps the most versatile of all the storage
    services in Google Cloud, because you can store pretty much any type of data in
    GCS, and it can be directly accessed from most Google Cloud services that process
    data. It is especially suitable for large amounts of data, and it can be used
    as the basis for building an enterprise data lake.
  prefs: []
  type: TYPE_NORMAL
- en: GCS provides different storage classes to optimize your usage based on cost
    and frequency of access. For objects that you don’t access frequently, you can
    put them in a lower-cost storage class, and you can even configure GCS to automatically
    move your objects between storage classes based on criteria such as the age of
    each object. For more information on each of the different storage classes, and
    which one works best for different use cases, reference the table at [https://cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes).
  prefs: []
  type: TYPE_NORMAL
- en: Filestore
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Google Cloud Filestore service is a high-performance, fully managed file
    storage service, used for workloads in which a structured file system is needed.
    This is the concept of **Network Attached Storage**, in which your VMs and containers
    can *mount* a shared filesystem, and can access and operate on the files in the
    shared directory structure. It uses the **Network File System version 3** (**NFSv3**)
    protocol and supports any NFSv3-compatible clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Filestore is available in three different formats:'
  prefs: []
  type: TYPE_NORMAL
- en: Filestore Basic, which is best for file sharing, software development, and web
    hosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filestore Enterprise, which is best for critical applications such as SAP workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filestore High Scale, which is best for high-performance computing, including
    genome sequencing, financial services trading analysis, and other high-performance
    workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the shared file system depends on the permissions you’ve configured,
    and the networking connectivity that you have set up by using the products that
    we discussed in the *Networking and connectivity* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Disk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve covered object storage and file storage. Another type of storage
    is referred to as block storage. This type of storage may be most familiar because
    it’s the traditional type of storage used by disks that are directly attached
    to computers; that is, **Direct Attached Storage** (**DAS**). For example, the
    disk drive in your laptop uses this type of storage. In companies’ own on-premises
    data centers, many servers may be connected to shared block storage devices referred
    to as a **Storage Area Network** (**SAN**), using the types of shared RAID array
    configurations that we briefly discussed in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015).
    In either case, these block storage devices appear to our server operating systems
    as if they are directly attached disks, and they are used as such by the applications
    running on our servers or in our containers.
  prefs: []
  type: TYPE_NORMAL
- en: SANs can require a lot of effort to set up and maintain, but when using Google
    Cloud Persistent Disk, you can simply define what kind of disk storage you want,
    and the required capacity, and all of the underlying infrastructure is managed
    for you by Google.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, Persistent Disk provides two different storage types, which
    are **Hard Disk Drives** (**HDDs**) and **Solid State Drives** (**SSDs**). HDDs
    offer low-cost storage when bulk throughput is of primary importance. SSDs offer
    high performance and speed for both random-access workloads and bulk throughput.
    Both types can be sized up to 64 TB.
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud BigQuery is a serverless data warehouse, meaning that you can use
    it without needing to configure or manage any servers. As a data warehouse, it
    straddles both storage and processing. It can store your data in a format that’s
    optimized for data analytics workloads, and it provides tools that allow you to
    run SQL queries on that data. You can also use it to run queries on other storage
    systems such as GCS, Cloud SQL, Cloud Spanner, Cloud Bigtable, and even storage
    systems on AWS or Azure. Additionally, it provides built-in machine learning that
    enables you to get ML inferences from your data via SQL queries, without needing
    to use other services. On the other hand, if you explicitly want to use other
    services such as Vertex AI, which we will describe later in this chapter, it integrates
    easily with many other Google Cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: It supports geospatial analysis, so you can augment your analytics workflows
    with location data, and it supports real-time analytics on streaming data when
    you integrate streaming solutions such as Dataflow with BigQuery BI Engine, which
    is an in-memory analysis service that provides a sub-second query response time.
    BI Engine also natively integrates with Looker Studio and works with many business
    intelligence tools. BigQuery is an extremely popular service on Google Cloud,
    and you will learn how to use many of its features in this book.
  prefs: []
  type: TYPE_NORMAL
- en: After you’ve ingested and stored data in Google Cloud, you will often want to
    organize and manage it so that it can easily be discovered and utilized effectively.
    In the next section, we will discuss Google Cloud’s data management tools.
  prefs: []
  type: TYPE_NORMAL
- en: Data management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The services we describe in this section enable you to organize your data and
    make it easier to manage how your data can be discovered and accessed by users
    in your organization, thus breaking down or preventing data silos. These tools
    act as a supporting layer between the data storage services we discussed in the
    previous section and the data processing services we will discuss in subsequent
    sections in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: BigLake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BigLake is a storage engine that unifies data warehouses and data lakes by enabling
    BigQuery and open source frameworks such as Spark to access data with fine-grained
    access control. For example, you could store data in GCS and make it available
    as a BigLake table, and then you could access that data from BigQuery or Spark.
    Fine-grained access control means that you can control access to the data at the
    table, row, and column level. As an example, you could ensure that your data scientists
    can see all columns except the credit card information column, or you could ensure
    that the sales department for a particular geographical location can only see
    the rows that pertain to that location, and cannot see data in any rows that relate
    to other geographical locations.
  prefs: []
  type: TYPE_NORMAL
- en: BigLake allows you to perform analytics on distributed data regardless of where
    and how it’s stored, using your preferred analytics tools – open source or cloud
    native – over a single copy of the data. This is important because it means you
    don’t need to move the data around between your data lakes and data warehouses,
    which has traditionally been laborious and expensive to do. BigLake also supports
    open source engines such as Apache Spark, Presto, and Trino, and open formats
    such as Parquet, Avro, ORC, CSV, and JSON, serving multiple compute engines through
    Apache Arrow. You can centrally manage data security policies in one place and
    have them consistently enforced across multiple query engines, and across multiple
    clouds when using BigQuery Omni. It can also integrate with Google Cloud Dataplex,
    which we will describe next, to enhance this functionality and provide unified
    data governance and management at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Dataplex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google refers to Dataplex as an “*intelligent data fabric that enables organizations
    to centrally discover, manage, monitor, and govern their data across data lakes,
    data warehouses, and data marts, with consistent controls*.” This relates to the
    concept of breaking down data silos. In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    we talked about data silos being a common challenge that companies run into when
    they wish to perform data science tasks and the complexities of managing who can
    access the data securely when you have many datasets owned by various organizations
    throughout your company. Dataplex helps to overcome these challenges by enabling
    data discovery and providing a single pane of glass for data management across
    data silos, and centralized security and governance. This means that you can define
    security and governance policies in Dataplex, and have them applied to data that
    is stored and accessed by other systems, in a consistent manner. It integrates
    with other Google Cloud data management services such as BigQuery, Cloud Storage,
    and Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: With Dataplex, the idea is to create a *data mesh*, in which there are logical
    connections between your various data stores and data processing systems, rather
    than disjointed data silos. It also uses Google’s AI/ML capabilities to provide
    additional features such as automated data life cycle management, data quality
    enforcement, and lineage tracking (you may remember that, in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    we also talked about lineage tracking being a difficult, common challenge that
    companies face when implementing data science workloads).
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud originally had a standalone service named Data Catalog, which could
    be used to store metadata about your various datasets, and therefore provide discoverability
    by allowing you to view and search through the metadata to understand what datasets
    are available. This service is now provided within Dataplex, and it can even automate
    data discovery, classification, and metadata enrichment. It can then logically
    organize data that exists across multiple storage services into business-specific
    domains using the concepts of Dataplex lakes and data zones.
  prefs: []
  type: TYPE_NORMAL
- en: Dataplex also provides some data processing functionality via its *serverless
    data exploration workbench*, which provides one-click access to Spark SQL scripts
    and Jupyter notebooks, allowing you to interactively query your datasets. The
    workbench also allows teams to publish, share, and search for datasets, therefore
    enabling discoverability and collaboration across teams.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of data processing, our next section will cover some of the primary
    tools and services available for processing data on Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you have used some of the services we discussed in the previous sections
    to store, organize, and manage data, you may then wish to process that data in
    some way. Fortunately, there are a number of tools and services in Google Cloud
    that can be used for this purpose, and we will explore them here.
  prefs: []
  type: TYPE_NORMAL
- en: Dataproc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataproc is a fully managed and highly scalable service for running Apache Hadoop,
    Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks.
    It is therefore very popular for data processing workloads on Google Cloud; especially
    when open source tools are preferred. You can either manage the servers that process
    your data yourself, or Dataproc also provides a serverless option, in which Google
    will manage all of the servers for you. People sometimes want to manage the servers
    themselves if they want to use customized configurations or customized tools.
    Dataproc also integrates with other Google tools such as BigQuery, and Vertex
    AI to cater to flexible data management needs and data science projects, and you
    can enforce fine-grained row and column-level access controls with Dataproc, BigLake,
    and Dataplex. You can also manage and enforce user authorization and authentication
    using existing Kerberos and Apache Ranger policies, and it provides the built-in
    Dataproc Metastore, which eliminates the need to run your own Hive Metastore or
    catalog service.
  prefs: []
  type: TYPE_NORMAL
- en: Managing your own on-premises Hadoop or Spark clusters can require a lot of
    work. One of the great things about Dataproc is that you can easily spin up clusters
    on demand in order to run a data processing workload, and then automatically shut
    them down when you’re not using them, and clusters can automatically scale up
    and down to meet your needs, which helps to save costs. You can also use Google
    Compute Engine Spot instances to further save costs for workloads that can tolerate
    being interrupted. You can run your workloads in VMs or containers, and it also
    supports GPUs if you need to use those in your data processing workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Dataprep
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataprep by Trifacta is a tool for visually exploring, cleaning, and preparing
    structured and unstructured data for analysis, reporting, and machine learning.
    It’s serverless, so there is no infrastructure to deploy or manage. It’s a very
    useful tool for the data exploration phase of a data science project, enabling
    you to explore and understand data with visual data distributions, and automatically
    detecting schemas, data types, possible joins, and anomalies such as missing values,
    outliers, and duplicates. You can then define a sequence of transformations to
    clean up and prepare your data for training ML models. You can do all of this
    visually, without needing to write any code, and it even suggests what kinds of
    transformations you may wish to implement, such as aggregation, pivot, unpivot,
    joins, union, extraction, calculation, comparison, condition, merge, regular expressions,
    and more. You can also apply data quality rules to ensure that your data meets
    your quality requirements, and it allows teams to collaborate on datasets by sharing
    or copying them as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to discuss Google Cloud Dataflow, we will first take a minute to introduce
    Apache Beam. In previous sections and chapters of this book, we’ve referred to
    *batch* data processing, in which large amounts of data are processed by long-running
    jobs, and *streaming* data processing, in which small pieces of data are processed
    very quickly, usually in real time or near real time. There are generally slightly
    different tools for each of those processing types. For example, you might use
    Hadoop for batch processing and you might use Apache Flink for stream processing.
    Apache Beam provides a unified model that can be used for both batch and streaming
    workloads. In the words of the Apache Beam project management committee, this
    allows you to “*write once, run anywhere*.” This can be very useful as it enables
    your data engineers to simplify how they code their data processing workloads
    by using this unified model instead of using completely different tools and code
    for their batch and streaming use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataflow is a fully managed, serverless service for executing Apache
    Beam pipelines. Once you have defined your data processing steps as an Apache
    Beam pipeline, you can then use your preferred data processing engines, such as
    Spark, Flink, or Google Cloud Dataflow, to execute the pipeline steps. As a fully
    managed service, Dataflow can automatically provision and scale the resources
    required to run your data processing steps, and it integrates with other tools
    and services such as BigQuery, enabling you to use SQL to access your data, and
    Vertex AI notebooks for ML model training use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Looker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Looker is Google Cloud’s business intelligence platform with embedded analytics.
    One of its main advantages is LookML, which is a powerful SQL-based modeling language.
    You can use LookML to centrally define and manage business rules and definitions
    as a version-controlled data model, and LookML can then create efficient SQL queries
    on your behalf. As a business intelligence tool, Looker then provides a user interface
    in which you can visualize your data in graphs, charts, and dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: It comes in a few different service tiers, providing different levels of business
    intelligence functionality. Google originally had a business intelligence tool
    named Data Studio, and Looker was created by a separate company, but Google acquired
    that company and has integrated Looker into its Cloud Service portfolio. They
    also integrated their original Data Studio product into Looker, creating Looker
    Studio, and added an enterprise version of that tool, named Looker Studio Pro,
    which provides additional functionality as well as customer support.
  prefs: []
  type: TYPE_NORMAL
- en: Data Fusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to discuss Data Fusion, let’s first talk about the concepts of **ETL**
    and **ELT**, which stand for **Extract, Transform, Load**, and **Extract, Load,
    Transform**, respectively. The ETL concept has been around since the 1970s, and
    it’s a common pattern that’s used in data science and data engineering when you
    need to perform transformations on your data. The pattern used in this case is
    to extract your data from its source storage location, transform it in some way,
    and then load it into your desired destination storage location. Examples of transformations
    could be to change the format of the data from CSV to JSON, or to remove all rows
    that contain missing information. A complex data engineering project may require
    the creation of ETL pipelines that define multiple transformation steps. ELT,
    on the other hand, has gained popularity in recent years, especially in relation
    to cloud-based data processing workloads. The idea with ELT is that you can extract
    your data from source locations and load it into a data lake or data warehouse,
    and then perform different kinds of transformations based on your project needs.
    This is also often referred to as *data integration*. Using this approach can
    enable analysts to use SQL to get insights and value from the data without requiring
    data engineers to create complex ETL pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, Data Fusion is Google Cloud’s serverless product that allows
    you to run your ETL/ELT workloads without having to manage any servers or infrastructure.
    It provides a visual interface that enables you to define your data transformation
    steps without having to write code, which makes it easy for less technical analysts
    to process the data, and it even tracks that ever-important data lineage that
    we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), as your data
    progresses through each transformation step.
  prefs: []
  type: TYPE_NORMAL
- en: Data Fusion integrates with other Google Cloud services such as BigQuery, Dataflow,
    Dataproc, Datastore, Cloud Storage, and Pub/Sub, making it easy to perform data
    processing workflows across those services.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Composer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Composer is a Google Cloud orchestration service built on the open source Apache
    Airflow project, and it is particularly useful for data integration or data processing
    workloads. Like Data Fusion, it integrates with other Google Cloud services such
    as BigQuery, Dataflow, Dataproc, Datastore, Cloud Storage, and Pub/Sub, making
    it easy to orchestrate data processing workflows across those services.
  prefs: []
  type: TYPE_NORMAL
- en: It is highly scalable, and it can also be used to implement workloads across
    multiple cloud providers and on-premises locations. It orchestrates workflows
    by using **Directed Acyclic Graphs** (**DAGs**), which represent the tasks your
    workflow needs to execute, as well as all of the relationships and dependencies
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Google Cloud Workflows, Composer monitors the execution of the tasks
    in your workload and tracks whether each step is completed correctly or whether
    any problems have occurred. *Figure 3**.12* shows an example of a workflow being
    orchestrated by Google Cloud Composer, in which data regarding customer orders
    is ingested periodically from Cloud Bigtable into Cloud Dataproc and enriched
    with data regarding broader retail trends from third-party providers, which is
    provided via Google Cloud Storage. The outputs are then stored in BigQuery for
    analysis. This data could then be used to build business intelligence dashboards
    in Looker or to train machine learning models in Vertex AI, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: Data processing workflow](img/B18143_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Data processing workflow'
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in *Figure 3**.12*, when we’ve processed and stored our data, we
    may want to use it to train a machine learning model. Before we get into training
    our own models, let’s explore some of the AI/ML capabilities that we can use on
    Google Cloud that provide models trained by Google.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud AI tools and AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover Google Cloud’s AI tools that can be used to implement
    AI use cases without the need to understand the underlying machine learning concepts.
    With these services, you can simply send a request to an API, and get a response
    from ML models that are created and maintained by Google. There’s no need to manually
    preprocess data, train or hyper-tune machine learning models, or manage any infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will group these services into the following categories: **Natural Language
    Processing** (**NLP**), **Computer Vision**, and **Discovery**.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this, in January 2023, Google has also announced a preview
    of a service named **Timeseries Insights API**, which can be used to perform analysis
    and gather insights in real time from your time series datasets, for use cases
    such as time series forecasting, or detecting anomalies and trends in your data
    while they are happening. This is an interesting new service because running forecasting
    and anomaly detection workloads over billions of time series data points is computationally
    intensive, and most existing systems implement these workloads as batch jobs,
    which limits the type of analysis you can perform online, such as deciding whether
    to alert based on a sudden increase or decrease in data values.
  prefs: []
  type: TYPE_NORMAL
- en: NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Natural language processing, in the context of AI, refers to the use of computers
    to understand and process natural human language. It can be further broken down
    into **Natural Language Understanding** (**NLU**) and **Natural Language Generation**
    (**NLG**). NLU is concerned with understanding the content and meaning of words
    and sentences, as they are understood by humans. NLG takes this one step further
    and attempts to create or generate words and sentences in a way that can be understood
    by humans. In this section, we’ll discuss some of Google Cloud’s NLP-related services.
  prefs: []
  type: TYPE_NORMAL
- en: The Natural Language API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use the Google Cloud Natural Language API to understand the contents
    of textual inputs. This can be used for purposes such as sentiment analysis, entity
    analysis, content classification, and syntax analysis. With sentiment analysis,
    it can tell you what kinds of emotions are suggested by the content. For example,
    you could feed all of your product reviews into this API and get an understanding
    of whether people are responding positively to your product, or whether they may
    be frustrated by something about the product. Entity analysis can identify what
    kinds of content exist in the text, such as people’s names, place names, locations,
    addresses, and phone numbers. The content classification feature is useful if
    you have large amounts of textual data and you want to organize and categorize
    it based on the contents.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-Speech
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The name of this service is somewhat self-explanatory. The service will take
    a textual input that you provide, and it will convert it to an audible spoken
    output. This can be very useful for accessibility use cases, whereby if somebody
    is visually impaired and cannot read text content, it could be automatically spoken
    to them. It provides the option to use many different voices to personalize the
    user experience, and you can even create custom voices using your own recordings.
    As of January 2023, it supported more than 40 languages and variants. It also
    supports **Speech Synthesis Markup Language** (**SSML**), which enables you to
    have more control over how words and phrases are pronounced.
  prefs: []
  type: TYPE_NORMAL
- en: Speech-to-Text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This service does pretty much the opposite of the previous service. In this
    case, you can provide audio inputs, and the service will transcribe any spoken
    language into a textual output. This is useful for dictation purposes, accessibility
    use cases such as closed captioning, and other use cases such as quality control.
    For example, you could provide recordings of your customer service calls and it
    would convert them to text. Then you could feed that text into the Natural Language
    API to understand whether your customers are frustrated or happy with the service
    they are receiving. As of January 2023, the service supported an impressive 125
    languages and variants.
  prefs: []
  type: TYPE_NORMAL
- en: Translation AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another service whose name is self-explanatory, this service can be used to
    translate from one language to another. It can help you to internationalize your
    products, and engage your customers with localization of content. It can detect
    more than 100 languages, and you can customize the translations with industry-
    or domain-specific terms. It provides **Translation Hub**, which allows you to
    manage translation workloads at scale, as well as Media Translation API, which
    can deliver real-time audio translation directly to your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Contact Center AI (CCAI)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Contact Center AI** (**CCAI**) provides human-like AI-powered contact center
    experiences. It consists of a number of different components, such as Dialogflow,
    which can be used to create chatbots that can have intelligent, human-like conversations
    with customers.'
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever been on hold with a company’s customer service line for an hour
    before anybody helps you with your concerns? This, of course, happens when customer
    service centers are overloaded with calls. Using chatbots can offload a significant
    amount of cases with simple questions, freeing up humans to focus on more complex
    customer interactions.
  prefs: []
  type: TYPE_NORMAL
- en: When a human does need to get involved, CCAI has another feature, named Agent
    Assist, that provides support to human agents while they handle customer interactions.
    It can recommend ready-to-send responses to customers, provide answers to customer
    questions from a centralized knowledge base, and transcribe calls in real time.
  prefs: []
  type: TYPE_NORMAL
- en: CCAI also includes CCAI Insights, which uses NLP to identify customer sentiment
    and reasons for calls, which helps contact center managers learn about customer
    interactions to improve call outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: There’s also the option to use CCAI Platform, which provides a **Contact Center
    as a Service** (**CCaaS**) solution.
  prefs: []
  type: TYPE_NORMAL
- en: Document AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Document AI goes beyond understanding the content of textual inputs, and also
    incorporates structure. It provides pre-trained models for data extraction, or
    you can use Document AI Workbench to create custom models, and you can use Document
    AI Warehouse to search and store documents. For example, if you collect information
    from people via forms, Document AI could be used to extract the data from those
    forms and store it in a database, or send it to another data processing system
    to process the data in some way or feed it to another ML model to perform some
    kind of other task. It could also be used to categorize and organize documents
    based on their contents. Some companies process millions of forms and contracts
    per year, and before these kinds of AI systems existed, all of those documents
    had to be processed by humans, which led to extremely laborious and error-prone
    work. With Document AI, you can automate that work, and you can also enrich the
    data using Google **Enterprise Knowledge Graph** (**EKG**), or you can enhance
    the functionality of Document AI with human inputs by using its **Human-in-the-Loop**
    (**HITL**) AI functionality. With HITL AI, experts can verify the outputs from
    Document AI, and provide corrections if needed. You can either use your own workforce
    of experts for this purpose or, if you don’t have such experts in your employment,
    you can use Google’s HITL workforce.
  prefs: []
  type: TYPE_NORMAL
- en: Document AI’s **Optical Character Recognition** (**OCR**) functionality flows
    over into the computer vision realm, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use Google Cloud Vision AI to create your own computer vision applications
    or get insights from images and videos with pre-trained APIs, AutoML, or custom
    models. It enables you to spin up video and image analytics applications in minutes,
    for use cases such as detecting objects, reading handwriting, or creating image
    metadata. It consists of three main components: Vertex AI Vision, Vision API,
    and custom ML models. Vertex AI Vision includes **Streams** to ingest real-time
    video data, **Applications** to let you create an application by combining various
    components, and **Vision Warehouse** to store model output and streaming data.
    The Vision API offers pre-trained ML models that you can access through REST and
    RPC APIs, allowing you to assign labels to images and classify them into millions
    of predefined categories. If you need to develop more specialized models, you
    can use AutoML or build your own custom models.'
  prefs: []
  type: TYPE_NORMAL
- en: Vision AI can be used to implement interesting use cases such as image search,
    in which you could use Vision API and AutoML Vision to make images searchable
    based on topics and scenes detected in the images, or product search, in which
    you could enable customers to find products of interest within images and visually
    search product catalogs using the Vision API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also in the computer vision space is Google Cloud Video AI, which can analyze
    video content for use cases such as content discovery. Video AI can recognize
    over 20,000 objects, places, and actions in video, it can extract metadata at
    the video, shot, or frame level, and you can even create your own custom entity
    labels with AutoML Video Intelligence. It consists of two main components: the
    Video Intelligence API, which provides pre-trained ML models, and Vertex AI for
    AutoML video, which provides a graphical interface to train your own custom models
    to classify and track objects in videos, without the need for ML experience. Vertex
    AI for AutoML video can be used for projects requiring custom labels that aren’t
    covered by the pre-trained Video Intelligence API.'
  prefs: []
  type: TYPE_NORMAL
- en: Discovery AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Google Cloud Discovery AI includes services such as search and recommendation
    engines. These kinds of features are essential for today’s online businesses,
    in which it’s important to ensure that your customers find what they want on your
    website as quickly and easily as possible. If this doesn’t happen, they’ll go
    to your competitor’s website. Imagine if your company could integrate Google-quality
    search into its website. This includes functionality such as image-based product
    searches, which we discussed in the previous section, making it easier for customers
    to search for products with an image, by using object recognition to provide real-time
    results of similar items from your product catalog.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to your customers finding products by directly searching for them,
    companies see a significant amount of business being driven by recommendation
    engines, which can make personalized recommendations for products based on customers’
    previous purchasing behavior. I’ve experienced this many times myself as a consumer,
    whereby I’m purchasing something on a website, I see a recommendation for something
    else that I may be interested in purchasing, and I think “*Actually, yes I do
    also want one of those*,” and I go ahead and add it to my cart before checking
    out. These kinds of personalized experiences help to maintain customer loyalty,
    which is also extremely important in today’s online business world.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing these two concepts together – that is, search and personalization –
    makes a lot of sense, because not only will the search results intelligently match
    what the customer searched for, but the ranking of the results can be catered
    to what the specific customer is more likely to find relevant to their preferences.
  prefs: []
  type: TYPE_NORMAL
- en: A third component of Discovery AI is Browse AI, which extends the search functionality
    to work on category pages in addition to text queries. Without this, retailers
    would mostly sort products on their category and navigation pages by historic
    bestsellers. This ordering doesn’t adapt well to new product additions, changes
    in product availability, and sales. With Browse AI, retailers can sort products
    on these pages in an order that is personalized to each user, and based on predicted,
    rather than historic, bestsellers. This ordering can rapidly adapt to new products,
    product stockouts, and price changes, without needing to wait for backward-looking
    bestseller lists to catch up.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve taken a look at the high-level AI services on Google Cloud, which
    you can use to get inferences from ML models that are trained and maintained by
    Google by simply calling an API, let’s discuss how you can start to train your
    own models on Google Cloud, starting with AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of the services we discussed in the previous sections use completely pre-trained
    models, and others allow you to bring your own data to either train or *up-train*
    a model based on your data. Pre-trained models are trained on datasets provided
    by Google or other sources, and the term, up-train, refers to augmenting a pre-trained
    model with additional data. If you want to create more customized use cases than
    those supported by the high-level API services, you may want to train your own
    models. Vertex AI, which we will describe later in this chapter, provides a plethora
    of tools for implementing every step in the model development process. However,
    before we get to the level of customizing every step in the process, one way in
    which you can easily start getting inferences from ML models that are trained
    on your data in Google Cloud is to use AutoML, which enables developers with limited
    ML expertise to train models specific to their business needs in as little as
    a few minutes or hours. The actual amount of time depends on the algorithms being
    used, how much data is used for training, and some other factors, but in any case,
    AutoML saves you a lot of work and time, considering that data scientists can
    spend weeks on these tasks when not using AutoML. Looking at our ML model development
    lifecycle diagram in *Figure 3**.13*, AutoML automatically performs all of the
    steps in the process for us, as indicated by everything encapsulated within the
    blue box.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: ML model lifecycle managed by AutoML](img/B18143_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: ML model lifecycle managed by AutoML'
  prefs: []
  type: TYPE_NORMAL
- en: How does AutoML work? If we think back to all of the steps in a typical data
    science project that we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    you may remember that each step – especially in the early stages of a project
    – required a lot of trial and error. For example, you might try a number of different
    data transformation techniques, and then try out a few different algorithms during
    training, as well as different combinations of hyperparameter values for those
    algorithms. Each of those steps could take many days or weeks of work before you
    find a good candidate model (i.e., a model that we believe may satisfy our business
    objectives and metrics). AutoML automatically runs many trial jobs very quickly
    – generally much more quickly than a human could achieve – and evaluates the outcomes
    against desired metric thresholds. Jobs whose outcomes do not meet the desired
    criteria are not selected as candidates, and AutoML continues to try other options
    until suitable candidates are found, or some other threshold is met, such as all
    options being exhausted.
  prefs: []
  type: TYPE_NORMAL
- en: How can you start using AutoML? Some of the AI services we discussed in the
    previous sections in this chapter already use AutoML in order to train the models
    that you access via those APIs. Examples include AutoML image, in which you can
    get insights from object detection and image classification, AutoML Video for
    streaming video analysis, AutoML Text to understand the structure, meaning, and
    sentiment of text, AutoML Translation to translate between languages, and AutoML
    Forecasting to provide forecasts based on time series data.
  prefs: []
  type: TYPE_NORMAL
- en: Another AutoML use case that we haven’t explored yet is AutoML for tabular data
    (structured data that’s stored in tables). This is a very common format for storing
    business data, as it provides a way to organize information that is easy for humans
    to read and understand. AutoML Tabular supports multiple ML use cases with tabular
    data, such as binary classification, multi-class classification, regression, and
    forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: We can use AutoML to automate a lot of the trial and error steps and develop
    candidate models, and then we can customize further from that point if we wish.
    For example, we could use Vertex AI Tabular Workflows, which creates a *glassbox*-managed
    AutoML pipeline that lets us see and interpret each step in the model building
    and deployment process. We can then tweak any steps in the process as we see fit,
    and automate any updates via MLOps pipelines. We will be performing exactly these
    kinds of activities in later chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’re going to dive deeper into ML model customization, and we will explore
    Vertex AI in more detail, as it can be used to customize every step in a data
    science project.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is where we start getting to expert-level AI/ML. Think back on all of
    the data science concepts we’ve covered so far in this book; all of the different
    types of ML approaches, algorithms, use cases, and tasks. Vertex AI is where you
    can accomplish all of them, and many more that we will yet explore in this book.
    You can use Vertex AI as your central command center for performing everything
    AI/ML-related. Our ML model lifecycle diagram in *Figure 3**.14* illustrates this
    point graphically. Traditionally, we would expect an AI/ML platform to mainly
    take care of training, evaluating, deploying, and monitoring models. This is what’s
    represented by the blue box on the right-hand side of *Figure 3**.14*, and all
    of these activities are of course supported by Vertex AI. However, with additional
    features such as notebooks and MLOps pipelines, Vertex goes beyond just those
    traditional ML activities, to also enable us to perform all of the tasks in our
    lifecycle, including data exploration and processing, as represented by the light
    blue dashed box on the left-hand side of the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: ML model lifecycle with Vertex AI](img/B18143_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: ML model lifecycle with Vertex AI'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at Vertex AI’s features in a bit more detail. Starting with
    the basics, we can use Vertex AI’s Deep Learning VM Images to instantiate a VM
    image containing the most popular AI frameworks on a Compute Engine instance,
    or we can use Vertex AI Deep Learning Containers to quickly build and deploy models
    in a portable and consistent containerized environment.
  prefs: []
  type: TYPE_NORMAL
- en: The VMs and container images provide building blocks on which we can develop
    customized ML workloads, but we can also use Vertex AI in other ways to perform
    and manage all of the steps in our model development lifecycle. We can use Jupyter
    notebooks to explore data and experiment with each of the steps in the process,
    while Vertex AI Data Labeling enables us to get highly accurate labels from human
    labelers for creating better supervised ML models, and Vertex AI Feature Store
    provides a fully managed feature repository for serving, sharing, and reusing
    ML features. Vertex AI Training provides pre-built algorithms and allows users
    to bring their custom code to train models in a fully managed training service
    for greater flexibility and customization, or for users running training on-premises
    or in another cloud environment. Vertex AI Vizier automates all of our hyperparameter
    tuning jobs for us, finding an optimal set of hyperparameter values for us, and
    saving us from having to do a lot of painstaking work manually! Optimized hyperparameter
    values lead to more accurate and more efficient models.
  prefs: []
  type: TYPE_NORMAL
- en: When we need to deploy our models, we can use Vertex AI Predictions, which will
    host our models on infrastructure that’s managed by Google, which auto-scales
    to meet our models’ traffic needs. It can host our models for batch or online
    use cases, and it offers a unified framework to deploy custom models built on
    any framework, such as TensorFlow, PyTorch, scikit-learn, or XGB, as well as BigQuery
    ML and AutoML models, and on a broad range of machine types and GPUs. After deployment,
    we can use Vertex AI Model Monitoring to provide automated alerts for data drift,
    concept drift, or other model performance incidents that may require supervision.
    We can then automate the entire process as MLOps pipelines by using Vertex Pipelines,
    which allows us to trigger retraining of our models when needed, and to manage
    updates to our models in a version-controlled way. We can also use Vertex AI TensorBoard
    to visualize ML experiment outcomes and compare models and model versions against
    each other to easily identify the best-performing models. This visualization and
    tracking tool for ML experimentation includes model graphs that display images,
    text, and audio data.
  prefs: []
  type: TYPE_NORMAL
- en: What’s really great is that we can perform and manage all of these activities
    from Vertex AI Workbench, which is a Jupyter-based fully managed, scalable, enterprise-ready
    compute infrastructure with security controls and user management capabilities.
    All the while, the lineage details of our models as they progress through every
    step in the process can be tracked by Vertex Experiments and the Vertex ML Metadata
    service, which provides artifact, lineage, and execution tracking for ML workflows,
    with an easy-to-use Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI provides even more functionality beyond all of the features we mentioned
    previously, such as Vertex AI Matching Engine, which is a massively scalable,
    low latency, and cost-efficient vector similarity matching service. Vertex AI
    Neural Architecture Search enables us to build new model architectures targeting
    application-specific needs and optimize our existing model architectures for latency,
    memory, and power, in an automated service, and we can use Vertex Explainable
    AI to understand and build trust in our model predictions with actionable explanations
    integrated into Vertex AI Prediction, AutoML Tables, and Vertex AI Workbench.
    Explainable AI provides detailed model evaluation metrics and feature attributions,
    indicating how important each input feature is to our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Standard industry tools on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to Google Cloud’s own data science tools that we’ve been describing
    so far in this chapter, you can also use other data science tools such as open
    source frameworks or other popular industry solutions. There are lots of great
    libraries out there that make it easy to perform the various tasks in the model
    development life cycle. When it comes to data exploration and processing, for
    example, the beloved pandas library is a staple of any ML and data analysis course.
    You can use it for handling missing data, slicing, subsetting, reshaping, merging,
    and joining datasets. Matplotlib is right up there with pandas for data exploration
    as it allows you to visualize your data via customizable and interactive plots
    and charts that can be exported into various file formats. NumPy allows you to
    easily manipulate and play around with the kinds of *n*-dimensional arrays and
    vectors we find in so many ML implementations. Learning NumPy also sets you up
    to start using frameworks such as scikit-Learn, TensorFlow, and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of scikit-learn, it is as much of a staple in any machine learning
    course as pandas. If you take an ML course, you will almost certainly use scikit-learn
    at some point in your learning process, and for good reason; it’s a framework
    that’s easy to use and understand and contains lots of built-in algorithms and
    datasets that you can use to implement ML workloads. And, it’s more than just
    an easy framework for learning purposes; many companies also use scikit-learn
    for their production ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: While we’re still on the topic of general-purpose ML frameworks, the next framework
    we’ll discuss is the wildly popular TensorFlow, which was originally created by
    Google before they open sourced it, and is therefore very well supported on Google
    Cloud. TensorFlow can be used for everything from processing data to NLP and computer
    vision. You can use it to train, deploy, and serve models, and with **TensorFlow
    Extended** (**TFX**), you can implement end-to-end MLOps pipelines to automate
    all of those steps. We’ll certainly be exploring TensorFlow in more detail in
    this book, as well as Keras, which is an API that provides access to TensorFlow
    via a Python interface that’s popular for its ease of use, and advertises itself
    as “*an API designed for human beings,* *not machines*.”
  prefs: []
  type: TYPE_NORMAL
- en: We’ll round out our discussion of general-purpose ML frameworks with PyTorch,
    which was originally developed by Facebook (Meta AI) and is now open sourced under
    the Linux Foundation. PyTorch has been rapidly gaining popularity in recent years,
    especially among Python developers, and it has become a very widely used framework
    in addition to TensorFlow. In this book, we’re not going to get into the argument
    of which framework is better. There are staunch supporters on each side of this
    debate, and if you Google “TensorFlow versus PyTorch,” you’ll find no shortage
    of websites and forums highlighting how one is better than the other for particular
    types of use cases. We will also be using PyTorch in later chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Switching gears from general-purpose ML frameworks to more specialized frameworks,
    you might want to use something such as OpenCV for computer vision workloads,
    or SpaCy for NLP. OpenCV has a broad selection of algorithms for applying ML and
    deep learning to images and video content, to perform many of the tasks we discussed
    earlier in this chapter, such as object recognition, and tracking objects and
    actions through video frames. SpaCy has lots of pre-trained word embeddings and
    pipelines supporting multiple languages, and it supports custom models written
    in TensorFlow and PyTorch and lots of Python packages for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that all of the tools and frameworks we’ve discussed in this
    section can easily be used on Google Cloud. In addition to open source tools,
    there are many popular third-party data science solutions that can be used on
    Google Cloud, providing flexibility for people to use whatever tools they prefer
    for the objectives they want to achieve. We’ve already talked about using Spark
    on Google Cloud through services such as Dataproc and Vertex AI, and you can use
    third-party Spark offerings such as Databricks via the Google Cloud Marketplace.
    The marketplace allows you to find thousands of solutions that run on Google Cloud,
    including deep learning solutions from companies such as Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: With all of these tools and all of the services provided by Google Cloud, you
    might wonder how to choose the right tool for your data science workloads. Let’s
    take a look at that discussion in more detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right tool for the job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your choice of data processing tools will depend heavily on what kind of data
    processing tasks you need to accomplish. If you have a bunch of raw data that
    you need to transform in bulk, as in an ETL/ELT task, Data Fusion would be a good
    place to start your assessment, whereas if you want to perform relatively simple
    transformations using SQL syntax, then start with BigQuery, and if you want to
    visualize and transform data via an easy-to-use GUI, then go with Dataprep. If
    you prefer to stick to using open source tools, then you might want to use something
    like pandas or Spark. We discussed pandas being a good starting point for people
    who are beginning to learn about data exploration and preprocessing, and how it’s
    also more than an educational tool. pandas is really great for initial data exploration
    and data processing at a moderate scale. However, for large-scale data processing
    projects, Spark’s highly parallelized functionality will be a lot more efficient,
    and if you want to use it without managing the infrastructure yourself, then you
    can run it on Dataproc. Dataflow is the recommended choice for streaming data,
    and it has the additional benefit of also working well for batch data processing
    with its unified programming model.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to choosing a tool for AI/ML workloads, the decision may be a
    bit more straightforward. If you or your company has a pre-existing affinity for
    a specific third party such as Hugging Face, then you may be guided in that direction.
    The general best practice is that you should start with the highest level of abstraction
    available to you because then you won’t have to spend a lot of time and effort
    building and maintaining something that is already available as a service for
    you to use. If that does not meet your needs for any reason, such as specific
    business requirements that require some level of customization, then move to a
    solution that provides more control over how the workload is implemented. For
    example, if you’re building an application that needs to include some kind of
    NLP functionality, start by evaluating the Google Cloud NLP API. If, for any reason,
    you cannot achieve your desired objective with that solution, move on to evaluating
    the use of AutoML to automate training a model on your specific data. If your
    objective is still not met by the models created during that process, then it’s
    time to increase your level of customization, and potentially use Vertex AI to
    build a completely customized model.
  prefs: []
  type: TYPE_NORMAL
- en: Another very important factor in your decision process is your budget. Services
    that do more work for you – such as the higher-level AI services – may cost more
    than the lower-level services, but it’s very important to factor in how much you
    are paying your employees to manage infrastructure and perform the tasks that
    could be performed on your behalf by the higher-level services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered all of the foundational and primary services that
    you can use for implementing AI/ML workloads on Google Cloud. We started with
    the basic services such as Google Compute Engine, and Google Cloud networking
    services, upon which all other services are built. We then explored how you could
    use those services to set up connectivity with systems outside of Google Cloud.
    Next, we discussed the services that you can use to import or transfer data to
    Google Cloud and the various storage systems that you can use for storing that
    data in the cloud. Having covered the primary storage systems, we moved on to
    discuss Google Cloud’s data management and data processing services. The final
    step in our journey was to understand all of the different AI/ML services that
    exist in Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have already learned a lot, and with this knowledge, you
    could now have an intelligent discussion about AI/ML and Google Cloud. This is
    a significant achievement because people spend a very long time trying to learn
    about AI/ML, and a very long time trying to learn about Google Cloud. Right now,
    you know more than a lot of people out there, and you should feel proud and give
    yourself a pat on the back for coming this far.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the *Basics* part of our book, and all of the knowledge you have
    gained in the previous chapters will form the basis of what you will learn in
    the rest of this book. In the next chapter and beyond, we will start performing
    hands-on activities, diving deeper into the services and concepts we have covered
    thus far, and actually start to build data science workloads on Google Cloud!
  prefs: []
  type: TYPE_NORMAL
