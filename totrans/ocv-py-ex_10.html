<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 10. Object Recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Object Recognition</h1></div></div></div><p>In this chapter, we are going to learn about object recognition and how we can use it to build a visual search engine. We will discuss feature detection, building feature vectors, and using machine learning to build a classifier. We will learn how to use these different blocks to build an object recognition system.</p><p>By the end of this chapter, you will know:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is the difference between object detection and object recognition</li><li class="listitem" style="list-style-type: disc">What is a dense feature detector</li><li class="listitem" style="list-style-type: disc">What is a visual dictionary</li><li class="listitem" style="list-style-type: disc">How to build a feature vector</li><li class="listitem" style="list-style-type: disc">What is supervised and unsupervised learning</li><li class="listitem" style="list-style-type: disc">What are Support Vector Machines and how to use them to build a classifier</li><li class="listitem" style="list-style-type: disc">How to recognize an object in an unknown image</li></ul></div><div class="section" title="Object detection versus object recognition"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec80"/>Object detection versus object recognition</h1></div></div></div><p>Before we proceed, we <a id="id320" class="indexterm"/>need to understand what we are <a id="id321" class="indexterm"/>going to discuss in this chapter. You must have frequently heard the terms "object detection" and "object recognition", and they are often mistaken to be the same thing. There is a very distinct difference between the two.</p><p>Object detection refers to detecting the presence of a particular object in a given scene. We don't know what the object might be. For instance, we discussed face detection in <a class="link" href="ch04.html" title="Chapter 4. Detecting and Tracking Different Body Parts">Chapter 4</a>, <span class="emphasis"><em>Detecting and Tracking Different Body Parts</em></span>. During the discussion, we only detected whether or not a face is present in the given image. We didn't recognize the person! The reason we didn't recognize the person is because we didn't care about that in our discussion. Our goal was to find the location of the face in the given image. Commercial face recognition systems employ both face detection and face recognition to identify a person. First, we need to locate the face, and then, run the face recognizer on the cropped face.</p><p>Object recognition is the process of identifying an object in a given image. For instance, an object recognition system can tell you if a given image contains a dress or a pair of shoes. In fact, we can train an object recognition system to identify many different objects. The problem is that object recognition is a really difficult problem to solve. It has eluded computer vision researchers for decades now, and has become the holy grail of computer vision. Humans can identify a wide variety of objects very easily. We do it everyday and we do it effortlessly, but computers are unable to do it with that kind of accuracy.</p><p>Let's consider the<a id="id322" class="indexterm"/> following<a id="id323" class="indexterm"/> image of a latte cup:</p><div class="mediaobject"><img src="images/B04554_10_01.jpg" alt="Object detection versus object recognition"/></div><p>An object detector will give you the following information:</p><div class="mediaobject"><img src="images/B04554_10_02.jpg" alt="Object detection versus object recognition"/></div><p>Now, consider the <a id="id324" class="indexterm"/>following <a id="id325" class="indexterm"/>image of a teacup:</p><div class="mediaobject"><img src="images/B04554_10_03.jpg" alt="Object detection versus object recognition"/></div><p>If you run it through an object detector, you will see the following result:</p><div class="mediaobject"><img src="images/B04554_10_04.jpg" alt="Object detection versus object recognition"/></div><p>As you can see, the <a id="id326" class="indexterm"/>object detector detects the presence of the<a id="id327" class="indexterm"/> teacup, but nothing more than that. If you train an object recognizer, it will give you the following information, as shown in the image below:</p><div class="mediaobject"><img src="images/B04554_10_05.jpg" alt="Object detection versus object recognition"/></div><p>If you consider the second image, it will give you the following information:</p><div class="mediaobject"><img src="images/B04554_10_06.jpg" alt="Object detection versus object recognition"/></div><p>As you can see, a <a id="id328" class="indexterm"/>perfect object recognizer would give <a id="id329" class="indexterm"/>you all the information associated with that object. An object recognizer functions more accurately if it knows where the object is located. If you have a big image and the cup is a small part of it, then the object recognizer might not be able to recognize it. Hence, the first step is to detect the object and get the bounding box. Once we have that, we can run an object recognizer to extract more information.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is a dense feature detector?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec81"/>What is a dense feature detector?</h1></div></div></div><p>In order to extract a <a id="id330" class="indexterm"/>meaningful amount of information from the images, we need to make sure our feature extractor extracts features from all the parts of a given image. Consider the following image:</p><div class="mediaobject"><img src="images/B04554_10_07.jpg" alt="What is a dense feature detector?"/></div><p>If you extract<a id="id331" class="indexterm"/> features using a feature extractor, it will look like this:</p><div class="mediaobject"><img src="images/B04554_10_08.jpg" alt="What is a dense feature detector?"/></div><p>If you use <code class="literal">Dense</code> <a id="id332" class="indexterm"/>detector, it will look like this:</p><div class="mediaobject"><img src="images/B04554_10_09.jpg" alt="What is a dense feature detector?"/></div><p>We can control the<a id="id333" class="indexterm"/> density as well. Let's make it sparse:</p><div class="mediaobject"><img src="images/B04554_10_10.jpg" alt="What is a dense feature detector?"/></div><p>By doing this, we can <a id="id334" class="indexterm"/>make sure that every single part in the image is processed. Here is the code to do it:</p><div class="informalexample"><pre class="programlisting">import cv2
import numpy as np

class DenseDetector(object):
    def __init__(self, step_size=20, feature_scale=40, img_bound=20):
        # Create a dense feature detector
        self.detector = cv2.FeatureDetector_create("Dense")

        # Initialize it with all the required parameters
        self.detector.setInt("initXyStep", step_size)
        self.detector.setInt("initFeatureScale", feature_scale)
        self.detector.setInt("initImgBound", img_bound)

    def detect(self, img):
        # Run feature detector on the input image
        return self.detector.detect(img)

if __name__=='__main__':
    input_image = cv2.imread(sys.argv[1])
    input_image_sift = np.copy(input_image)

    # Convert to grayscale
    gray_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)

    keypoints = DenseDetector(20,20,5).detect(input_image)

    # Draw keypoints on top of the input image
    input_image = cv2.drawKeypoints(input_image, keypoints,
            flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # Display the output image
    cv2.imshow('Dense feature detector', input_image)

    # Initialize SIFT object
    sift = cv2.SIFT()

    # Detect keypoints using SIFT
    keypoints = sift.detect(gray_image, None)

    # Draw SIFT keypoints on the input image
    input_image_sift = cv2.drawKeypoints(input_image_sift,
            keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

    # Display the output image
    cv2.imshow('SIFT detector', input_image_sift)

    # Wait until user presses a key
    cv2.waitKey()</pre></div><p>This gives us close<a id="id335" class="indexterm"/> control over the amount of information that gets extracted. When we use a SIFT detector, some parts of the image are neglected. This works well when we are dealing with the detection of prominent features, but when we are building an object recognizer, we need to evaluate all parts of the image. Hence, we use a dense detector and then extract features from those keypoints.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is a visual dictionary?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec82"/>What is a visual dictionary?</h1></div></div></div><p>We will be using <a id="id336" class="indexterm"/>the <span class="strong"><strong>Bag of Words</strong></span> model to build our object recognizer. Each image is represented as a histogram of visual words. These visual words are basically the <span class="strong"><strong>N</strong></span> centroids built using all the keypoints extracted from training images. The pipeline is as shown in the image that follows:</p><div class="mediaobject"><img src="images/B04554_10_11.jpg" alt="What is a visual dictionary?"/></div><p>From each training image, we detect a set of keypoints and extract features for each of those keypoints. Every image will give rise to a different number of keypoints. In order to train a classifier, each image must be represented using a fixed length feature vector. This feature vector is nothing but a histogram, where each bin corresponds to a visual word.</p><p>When we extract all <a id="id337" class="indexterm"/>the features from all the keypoints in the training images, we perform K-Means clustering and extract N centroids. This N is the length of the feature vector of a given image. Each image will now be represented as a histogram, where each bin corresponds to one of the 'N' centroids. For simplicity, let's say that N is set to 4. Now, in a given image, we extract <span class="strong"><strong>K</strong></span> keypoints. Out of these K keypoints, some of them will be closest to the first centroid, some of them will be closest to the second centroid, and so on. So, we build a histogram based on the closest centroid to each keypoint. This histogram becomes our feature vector. This process is called <a id="id338" class="indexterm"/><span class="strong"><strong>vector quantization</strong></span>.</p><p>To understand vector quantization, let's consider an example. Assume we have an image and we've extracted a certain number of feature points from it. Now our goal is to represent this image in the form of a feature vector. Consider the following image:</p><div class="mediaobject"><img src="images/B04554_10_12.jpg" alt="What is a visual dictionary?"/></div><p>As you can see, we have 4 centroids. Bear in mind that the points shown in the figures represent the feature space and not the actual geometric locations of those feature points in the image. It is shown this way in the preceding figure so that it's easy to visualize. Points from many different geometric locations in an image can be close to each other in the feature space. Our goal is to represent this image as a histogram, where each bin corresponds to one of <a id="id339" class="indexterm"/>these centroids. This way, no matter how many feature points we extract from an image, it will always be converted to a fixed length feature vector. So, we "round off" each feature point to its nearest centroid, as shown in the next image:</p><div class="mediaobject"><img src="images/B04554_10_13.jpg" alt="What is a visual dictionary?"/></div><p>If you build a histogram for this image, it will look like this:</p><div class="mediaobject"><img src="images/B04554_10_14.jpg" alt="What is a visual dictionary?"/></div><p>Now, if you consider a<a id="id340" class="indexterm"/> different image with a different distribution of feature points, it will look like this:</p><div class="mediaobject"><img src="images/B04554_10_15.jpg" alt="What is a visual dictionary?"/></div><p>The clusters <a id="id341" class="indexterm"/>would look like the following:</p><div class="mediaobject"><img src="images/B04554_10_16.jpg" alt="What is a visual dictionary?"/></div><p>The histogram <a id="id342" class="indexterm"/>would look like this:</p><div class="mediaobject"><img src="images/B04554_10_17.jpg" alt="What is a visual dictionary?"/></div><p>As you can see, the <a id="id343" class="indexterm"/>histograms are very different for the two images even though the points seem to be randomly distributed. This is a very powerful technique and it's widely used in computer vision and signal processing. There are many different ways to do this and the accuracy depends on how fine-grained you want it to be. If you increase the number of centroids, you will be able to represent the image better, thereby increasing the uniqueness of your feature vector. Having said that, it's important to mention that you cannot just keep increasing the number of centroids indefinitely. If you do that, it will become too noisy and lose its power.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is supervised and unsupervised learning?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec83"/>What is supervised and unsupervised learning?</h1></div></div></div><p>If you are familiar with the basics of machine learning, you will certainly know what supervised<a id="id344" class="indexterm"/> and unsupervised learning<a id="id345" class="indexterm"/> is all about. To give a quick refresher, supervised learning refers to building a function based on labeled samples. For example, if we are building a system to separate dress images from footwear images, we first need to build a database and label it. We need to tell our algorithm what images correspond to dresses and what images correspond to footwear. Based on this data, the algorithm will learn how to identify dresses and footwear so that when an unknown image comes in, it can recognize what's inside that image.</p><p>Unsupervised learning is the opposite of what we just discussed. There is no labeled data available here. Let's say we have a bunch of images, and we just want to separate them into three groups. We don't know what the criteria will be. So, an unsupervised learning algorithm will try to separate the given set of data into 3 groups in the best possible way. The reason we are discussing this is because we will be using a combination of <a id="id346" class="indexterm"/>supervised and<a id="id347" class="indexterm"/> unsupervised learning to build our object recognition system.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="What are Support Vector Machines?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec84"/>What are Support Vector Machines?</h1></div></div></div><p>
<span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVM</strong></span>)<a id="id348" class="indexterm"/> are supervised learning models that are very popular in the realm of machine learning. SVMs are really good at analyzing labeled data and detecting patterns. Given a bunch of data points and the associated labels, SVMs will build the separating hyperplanes in the best possible way.</p><p>Wait a minute, what are "hyperplanes"? To understand that, let's consider the following figure:</p><div class="mediaobject"><img src="images/B04554_10_18.jpg" alt="What are Support Vector Machines?"/></div><p>As you can see, the points are being separated by line boundaries that are equidistant from the points. This is easy to visualize in 2 dimensions. If it were in 3 dimensions, the separators would be planes. When we build features for images, the length of the feature vectors is usually in the six-digit range. So, when we go to such a high dimensional space, the equivalent of "lines" would be hyperplanes. Once the hyperplanes are formulated, we use this mathematical model to classify unknown data, based on where it falls on this map.</p><div class="section" title="What if we cannot separate the data with simple straight lines?"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec45"/>What if we cannot separate the data with simple straight lines?</h2></div></div></div><p>There is something called the <span class="strong"><strong>kernel trick</strong></span><a id="id349" class="indexterm"/> that we use in SVMs. Consider the following image:</p><div class="mediaobject"><img src="images/B04554_10_19.jpg" alt="What if we cannot separate the data with simple straight lines?"/></div><p>As we can see, we cannot draw a simple straight line to separate the red points from the blue points. Coming up with a nice curvy boundary that will satisfy all the points is prohibitively expensive. SVMs are really good at drawing "straight lines". So, what's our answer here? The good thing about SVMs is that they can draw these "straight lines" in any number of dimensions. So technically, if you project these points into a high dimensional space, where they can separated by a simple hyperplane, SVMs will come up with an exact boundary. Once we have that boundary, we can project it back to the original space. The projection of this hyperplane on our original lower dimensional space looks curvy, as we can see in the next figure:</p><div class="mediaobject"><img src="images/B04554_10_20.jpg" alt="What if we cannot separate the data with simple straight lines?"/></div><p>The topic of SVMs is really deep and we will not be able to discuss it in detail here. If you are really interested, there is a ton of material available online. You can go through a simple tutorial to understand it better.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="How do we actually implement this?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec85"/>How do we actually implement this?</h1></div></div></div><p>We have now arrived at the<a id="id350" class="indexterm"/> core. The discussion up until now was necessary because it gives you the background required to build an object recognition system. Now, let's build an object recognizer that can recognize whether the given image contains a dress, a pair of shoes, or a bag. We can easily extend this system to detect any number of items. We are starting with three distinct items so that you can start experimenting with it later.</p><p>Before we start, we need to make sure that we have a set of training images. There are many databases available online where the images are already arranged into groups. Caltech256<a id="id351" class="indexterm"/> is perhaps one of the most popular databases for object recognition. You can<a id="id352" class="indexterm"/> download it from <a class="ulink" href="http://www.vision.caltech.edu/Image_Datasets/Caltech256">http://www.vision.caltech.edu/Image_Datasets/Caltech256</a>. Create a folder called <code class="literal">images</code> and create three subfolders inside it, that is, <code class="literal">dress</code>, <code class="literal">footwear</code>, and <code class="literal">bag</code>. Inside each of those subfolders, add 20 images corresponding to that item. You can just download these images from the internet, but make sure those images have a clean background.</p><p>For example, a dress image would like this:</p><div class="mediaobject"><img src="images/B04554_10_21.jpg" alt="How do we actually implement this?"/></div><p>A footwear image <a id="id353" class="indexterm"/>would look like this:</p><div class="mediaobject"><img src="images/B04554_10_22.jpg" alt="How do we actually implement this?"/></div><p>A bag image would look like this:</p><div class="mediaobject"><img src="images/B04554_10_23.jpg" alt="How do we actually implement this?"/></div><p>Now that we have 60 training images, we are ready to start. As a side note, object recognition systems actually need tens of thousands of training images in order to perform well in the real world. Since we are building an object recognizer to detect 3 types of objects, we will take only 20 training images per object. Adding more training images will increase the accuracy and robustness of our system.</p><p>The first step here is to extract <a id="id354" class="indexterm"/>feature vectors from all the training images and build the visual dictionary (also known as codebook). Here is the code:</p><div class="informalexample"><pre class="programlisting">import os
import sys
import argparse
import cPickle as pickle
import json

import cv2
import numpy as np
from sklearn.cluster import KMeans

def build_arg_parser():
    parser = argparse.ArgumentParser(description='Creates features for given images')
    parser.add_argument("--samples", dest="cls", nargs="+", action="append",
            required=True, help="Folders containing the training images. \
            The first element needs to be the class label.")
    parser.add_argument("--codebook-file", dest='codebook_file', required=True,
            help="Base file name to store the codebook")
    parser.add_argument("--feature-map-file", dest='feature_map_file', required=True,
            help="Base file name to store the feature map")

    return parser

# Loading the images from the input folder
def load_input_map(label, input_folder):
    combined_data = []

    if not os.path.isdir(input_folder):
        raise IOError("The folder " + input_folder + " doesn't exist")

    # Parse the input folder and assign the  labels
    for root, dirs, files in os.walk(input_folder):
        for filename in (x for x in files if x.endswith('.jpg')):
            combined_data.append({'label': label, 'image': os.path.join(root, filename)})

    return combined_data

class FeatureExtractor(object):
    def extract_image_features(self, img):
        # Dense feature detector
        kps = DenseDetector().detect(img)

        # SIFT feature extractor
        kps, fvs = SIFTExtractor().compute(img, kps)

        return fvs

    # Extract the centroids from the feature points
    def get_centroids(self, input_map, num_samples_to_fit=10):
        kps_all = []

        count = 0
        cur_label = ''
        for item in input_map:
            if count &gt;= num_samples_to_fit:
                if cur_label != item['label']:
                    count = 0
                else:
                    continue

            count += 1

            if count == num_samples_to_fit:
                print "Built centroids for", item['label']

            cur_label = item['label']
            img = cv2.imread(item['image'])
            img = resize_to_size(img, 150)

            num_dims = 128
            fvs = self.extract_image_features(img)
            kps_all.extend(fvs)

        kmeans, centroids = Quantizer().quantize(kps_all)
        return kmeans, centroids

    def get_feature_vector(self, img, kmeans, centroids):
        return Quantizer().get_feature_vector(img, kmeans, centroids)

def extract_feature_map(input_map, kmeans, centroids):
    feature_map = []

    for item in input_map:
        temp_dict = {}
        temp_dict['label'] = item['label']

        print "Extracting features for", item['image']
        img = cv2.imread(item['image'])
        img = resize_to_size(img, 150)

        temp_dict['feature_vector'] = FeatureExtractor().get_feature_vector(
                    img, kmeans, centroids)

        if temp_dict['feature_vector'] is not None:
            feature_map.append(temp_dict)

    return feature_map

# Vector quantization
class Quantizer(object):
    def __init__(self, num_clusters=32):
        self.num_dims = 128
        self.extractor = SIFTExtractor()
        self.num_clusters = num_clusters
        self.num_retries = 10

    def quantize(self, datapoints):
        # Create KMeans object
        kmeans = KMeans(self.num_clusters,
                        n_init=max(self.num_retries, 1),
                        max_iter=10, tol=1.0)

        # Run KMeans on the datapoints
        res = kmeans.fit(datapoints)

        # Extract the centroids of those clusters
        centroids = res.cluster_centers_

        return kmeans, centroids

    def normalize(self, input_data):
        sum_input = np.sum(input_data)
        if sum_input &gt; 0:
            return input_data / sum_input
        else:
            return input_data

    # Extract feature vector from the image
    def get_feature_vector(self, img, kmeans, centroids):
        kps = DenseDetector().detect(img)
        kps, fvs = self.extractor.compute(img, kps)
        labels = kmeans.predict(fvs)
        fv = np.zeros(self.num_clusters)

        for i, item in enumerate(fvs):
            fv[labels[i]] += 1

        fv_image = np.reshape(fv, ((1, fv.shape[0])))
        return self.normalize(fv_image)

class DenseDetector(object):
    def __init__(self, step_size=20, feature_scale=40, img_bound=20):
        self.detector = cv2.FeatureDetector_create("Dense")
        self.detector.setInt("initXyStep", step_size)
        self.detector.setInt("initFeatureScale", feature_scale)
        self.detector.setInt("initImgBound", img_bound)

    def detect(self, img):
        return self.detector.detect(img)

class SIFTExtractor(object):
    def compute(self, image, kps):
        if image is None:
            print "Not a valid image"
            raise TypeError

        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        kps, des = cv2.SIFT().compute(gray_image, kps)
        return kps, des

# Resize the shorter dimension to 'new_size'
# while maintaining the aspect ratio
def resize_to_size(input_image, new_size=150):
    h, w = input_image.shape[0], input_image.shape[1]
    ds_factor = new_size / float(h)

    if w &lt; h:
        ds_factor = new_size / float(w)

    new_size = (int(w * ds_factor), int(h * ds_factor))
    return cv2.resize(input_image, new_size)

if __name__=='__main__':
    args = build_arg_parser().parse_args()

    input_map = []
    for cls in args.cls:

        assert len(cls) &gt;= 2, "Format for classes is `&lt;label&gt; file`"
        label = cls[0]
        input_map += load_input_map(label, cls[1])

    # Building the codebook
    print "===== Building codebook ====="
    kmeans, centroids = FeatureExtractor().get_centroids(input_map)
    if args.codebook_file:
        with open(args.codebook_file, 'w') as f:
            pickle.dump((kmeans, centroids), f)

    # Input data and labels
    print "===== Building feature map ====="
    feature_map = extract_feature_map(input_map, kmeans, centroids)
    if args.feature_map_file:
        with open(args.feature_map_file, 'w') as f:
            pickle.dump(feature_map, f)</pre></div><div class="section" title="What happened inside the code?"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec46"/>What happened inside the code?</h2></div></div></div><p>The first thing we need to <a id="id355" class="indexterm"/>do is extract the centroids. This is how we are going to<a id="id356" class="indexterm"/> build our visual dictionary. The <code class="literal">get_centroids</code> method in the <code class="literal">FeatureExtractor</code> class is designed to do this. We keep collecting the image features extracted from keypoints until we have a sufficient number of them. Since we are using a dense detector, 10 images should be sufficient. The reason we are just taking 10 images is because they will give rise to a large number of features. The centroids will not change much even if you add more feature points.</p><p>Once we've extracted the centroids, we are ready to move on to the next step of feature extraction. The set of centroids is our visual dictionary. The function, <code class="literal">extract_feature_map</code>, will extract a feature vector from each image and associate it with the corresponding label. The reason we do this is because we need this mapping to train our classifier. We need a set of datapoints, and each datapoint should be associated with a label. So, we start from an image, extract the feature vector, and then associate it with the corresponding label (like bag, dress, or footwear).</p><p>The <code class="literal">Quantizer</code> class is designed to achieve vector quantization and build the feature vector. For each keypoint extracted from the image, the <code class="literal">get_feature_vector</code> method finds the closest visual word in our dictionary. By doing this, we end up building a histogram based on our visual dictionary. Each image is now represented as a combination from a set of visual words. Hence the name, <a id="id357" class="indexterm"/><span class="strong"><strong>Bag of Words</strong></span>.</p><p>The next step is to train the classifier using these features. Here is the code:</p><div class="informalexample"><pre class="programlisting">import os
import sys
import argparse

import cPickle as pickle
import numpy as np
from sklearn.multiclass import OneVsOneClassifier
from sklearn.svm import LinearSVC
from sklearn import preprocessing

def build_arg_parser():
    parser = argparse.ArgumentParser(description='Trains the classifier models')
    parser.add_argument("--feature-map-file", dest="feature_map_file", required=True,
            help="Input pickle file containing the feature map")
    parser.add_argument("--svm-file", dest="svm_file", required=False,
            help="Output file where the pickled SVM model will be stored")
    return parser

# To train the classifier
class ClassifierTrainer(object):
    def __init__(self, X, label_words):
        # Encoding the labels (words to numbers)
        self.le = preprocessing.LabelEncoder()

        # Initialize One vs One Classifier using a linear kernel
        self.clf = OneVsOneClassifier(LinearSVC(random_state=0))

        y = self._encodeLabels(label_words)
        X = np.asarray(X)
        self.clf.fit(X, y)

    # Predict the output class for the input datapoint
    def _fit(self, X):
        X = np.asarray(X)
        return self.clf.predict(X)

    # Encode the labels (convert words to numbers)
    def _encodeLabels(self, labels_words):
        self.le.fit(labels_words)
        return np.array(self.le.transform(labels_words), dtype=np.float32)

    # Classify the input datapoint
    def classify(self, X):
        labels_nums = self._fit(X)
        labels_words = self.le.inverse_transform([int(x) for x in labels_nums])
        return labels_words

if __name__=='__main__':
    args = build_arg_parser().parse_args()
    feature_map_file = args.feature_map_file
    svm_file = args.svm_file

    # Load the feature map
    with open(feature_map_file, 'r') as f:
        feature_map = pickle.load(f)

    # Extract feature vectors and the labels
    labels_words = [x['label'] for x in feature_map]

    # Here, 0 refers to the first element in the
    # feature_map, and 1 refers to the second
    # element in the shape vector of that element
    # (which gives us the size)
    dim_size = feature_map[0]['feature_vector'].shape[1]

    X = [np.reshape(x['feature_vector'], (dim_size,)) for x in feature_map]

    # Train the SVM
    svm = ClassifierTrainer(X, labels_words)
    if args.svm_file:
        with open(args.svm_file, 'w') as f:
            pickle.dump(svm, f)</pre></div></div><div class="section" title="How did we build the trainer?"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec47"/>How did we build the trainer?</h2></div></div></div><p>We use the <code class="literal">scikit-learn</code> package to build the<a id="id358" class="indexterm"/> SVM model. You can install it, as shown next:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install scikit-learn</strong></span>
</pre></div><p>We start with labeled data and feed it to the <code class="literal">OneVsOneClassifier</code> method. We have a <code class="literal">classify</code> method that classifies an input image and associates a label with it.</p><p>Let's give this a trial run, shall we? Make sure you have a folder called <code class="literal">images</code>, where you have the training images for the three classes. Create a folder called <code class="literal">models</code>, where the learning models will be stored. Run the following commands on your terminal to create the features and train the classifier:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python create_features.py --samples bag images/bag/ --samples dress images/dress/ --samples footwear images/footwear/ --codebook-file models/codebook.pkl --feature-map-file models/feature_map.pkl</strong></span>
<span class="strong"><strong>$ python training.py --feature-map-file models/feature_map.pkl --svm-file models/svm.pkl</strong></span>
</pre></div><p>Now that the classifier has been trained, we just need a module to classify the input image and detect the object inside. Here is the code to do it:</p><div class="informalexample"><pre class="programlisting">import os
import sys
import argparse
import cPickle as pickle

import cv2
import numpy as np

import create_features as cf
from training import ClassifierTrainer

def build_arg_parser():
    parser = argparse.ArgumentParser(description='Extracts features \
            from each line and classifies the data')
    parser.add_argument("--input-image", dest="input_image", required=True,
            help="Input image to be classified")
    parser.add_argument("--svm-file", dest="svm_file", required=True,
            help="File containing the trained SVM model")
    parser.add_argument("--codebook-file", dest="codebook_file",
            required=True, help="File containing the codebook")
    return parser

# Classifying an image
class ImageClassifier(object):
    def __init__(self, svm_file, codebook_file):
        # Load the SVM classifier
        with open(svm_file, 'r') as f:
            self.svm = pickle.load(f)

        # Load the codebook
        with open(codebook_file, 'r') as f:
            self.kmeans, self.centroids = pickle.load(f)

    # Method to get the output image tag
    def getImageTag(self, img):
        # Resize the input image
        img = cf.resize_to_size(img)

        # Extract the feature vector
        feature_vector = cf.FeatureExtractor().get_feature_vector(img, self.kmeans, self.centroids)

        # Classify the feature vector and get the output tag
        image_tag = self.svm.classify(feature_vector)

        return image_tag

if __name__=='__main__':
    args = build_arg_parser().parse_args()
    svm_file = args.svm_file
    codebook_file = args.codebook_file
    input_image = cv2.imread(args.input_image)

    print "Output class:", ImageClassifier(svm_file, codebook_file).getImageTag(input_image)</pre></div><p>We are all set! We just <a id="id359" class="indexterm"/>extract the <code class="literal">feature</code> vector from the input image and use it as the input argument to the classifier. Let's go ahead and see if this works. Download a random footwear image from the internet and make sure it has a clean background. Run the following command by replacing <code class="literal">new_image.jpg</code> with the right filename:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ python classify_data.py --input-image new_image.jpg --svm-file models/svm.pkl --codebook-file models/codebook.pkl</strong></span>
</pre></div><p>We can use the same technique to build a visual search engine. A visual search engine looks at the input image and shows a bunch of images that are similar to it. We can reuse the object recognition framework to build this. Extract the feature vector from the input image, and compare it with all the feature vectors in the training dataset. Pick out the top matches and display the results. This is a simple way of doing things!</p><p>In the real world, we<a id="id360" class="indexterm"/> have to deal with billions of images. So, you cannot afford to search through every single image before you display the output. There are a lot of algorithms that are used to make sure that this is efficient and fast in the real world. Deep Learning<a id="id361" class="indexterm"/> is being used extensively in this field and it has shown a lot of promise in recent years. It is a branch of machine learning that focuses on learning optimal representation of data, so that it becomes easier for the machines to <span class="emphasis"><em>learn</em></span> new tasks. You can <a id="id362" class="indexterm"/>learn more about it at <a class="ulink" href="http://deeplearning.net">http://deeplearning.net</a>.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec86"/>Summary</h1></div></div></div><p>In this chapter, we learned how to build an object recognition system. The differences between object detection and object recognition were discussed in detail. We learned about the dense feature detector, visual dictionary, vector quantization, and how to use these concepts to build a feature vector. The concepts of supervised and unsupervised learning were discussed. We talked about Support Vector Machines and how we can use them to build a classifier. We learned how to recognize an object in an unknown image, and how we can extend that concept to build a visual search engine.</p><p>In the next chapter, we are going to discuss stereo imaging and 3D reconstruction. We will talk about how we can build a depth map and extract the 3D information from a given scene.</p></div></div>
</body></html>