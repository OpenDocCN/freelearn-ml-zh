- en: Implementing a Spam Filter with Bayesian Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get to grips with advanced topics, such as cluster analysis, deep
    learning, and ensemble models, let''s turn our attention to a much simpler model
    that we have overlooked so far: the Naive Bayes classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classifiers have their roots in Bayesian inference, named after
    the famed statistician and philosopher Thomas Bayes (1701-1761). Bayes' theorem
    famously describes the probability of an event based on prior knowledge of conditions
    that might lead to the event. We can use Bayes' theorem to build a statistical
    model that not only can classify data but can also provide us with an estimate
    of how likely it is that our classification is correct. In our case, we can use
    Bayesian inference to dismiss an email as spam with high confidence and to determine
    the probability of a woman having breast cancer, given a positive screening test.
  prefs: []
  type: TYPE_NORMAL
- en: We have now gained enough experience with the mechanics of implementing machine
    learning methods, and so we should no longer be afraid to try and understand the
    theory behind them. Don't worry, we won't write a book on it, but we need some
    understanding of the theory to appreciate a model's inner workings. After that,
    I am sure you will find that Bayesian classifiers are easy to implement, are computationally
    efficient, and tend to perform quite well on relatively small datasets. In this
    chapter, we will understand the Naive Bayes classifier and then implement our
    first Bayesian classifier. We will then classify emails using the Naive Bayes
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing your first Bayesian classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying emails using the Naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter07](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any OS—macOS, Windows, and Linux-based OSes—with this book. We recommend
    you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Bayesian inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Bayesian classifiers are relatively simple to implement, the theory
    behind them can be quite counter-intuitive at first, especially if you are not
    too familiar with probability theory yet. However, the beauty of Bayesian classifiers
    is that they understand the underlying data better than all of the classifiers
    we have encountered so far. For example, standard classifiers, such as the *k*-nearest
    neighbor algorithm or decision trees, might be able to tell us the target label
    of a never-before-seen data point. However, these algorithms have no concept of
    how likely it is for their predictions to be right or wrong. We call them discriminative
    models. Bayesian models, on the other hand, have an understanding of the underlying
    probability distribution that caused the data. We call them generative models
    because they don't just put labels on existing data points—they can also generate
    new data points with the same statistics.
  prefs: []
  type: TYPE_NORMAL
- en: If this last paragraph was a bit over your head, you might enjoy the following
    brief on probability theory. It will be important for the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Taking a short detour through probability theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to appreciate Bayes'' theorem, we need to get a hold of the following
    technical terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random variable**: This is a variable whose value depends on chance. A good
    example is the act of flipping a coin, which might turn up heads or tails. If
    a random variable can take on only a limited number of values, we call it discrete
    (such as a coin flip or a dice roll); otherwise, we call it a continuous random
    variable (such as the temperature on a given day). Random variables are often
    typeset as capital letters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Probability**: This is a measure of how likely it is for an event to occur.
    We denote the probability of an event, *e*, happening as *p(e)*, which must be
    a number between 0 and 1 (or between ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are quite a few scenarios where it would be really good to know how likely
    it is for our classifier to make a mistake. For example, in [Chapter 5](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml),
    *Using Decision Trees to Make a Medical Diagnosis*, we trained a decision tree
    to diagnose women with breast cancer based on some medical tests. You can imagine
    that, in this case, we would want to avoid a misdiagnosis at all costs; diagnosing
    a healthy woman with breast cancer (a false positive) would be both soul-crushing
    and lead to unnecessary, expensive medical procedures, whereas missing a woman's
    breast cancer (a false negative) might eventually cost the woman her life.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s good to know we have Bayesian models to count on. Let''s walk through
    a specific (and quite famous) example from [http://yudkowsky.net/rational/bayes](http://yudkowsky.net/rational/bayes):'
  prefs: []
  type: TYPE_NORMAL
- en: '"1% of women at age forty who participate in routine screening have breast
    cancer. 80% of women with breast cancer will get positive mammographies. 9.6%
    of women without breast cancer will also get positive mammographies. A woman in
    this age group had a positive mammography in a routine screening. What is the
    probability that she actually has breast cancer?"'
  prefs: []
  type: TYPE_NORMAL
- en: What do you think the answer is?
  prefs: []
  type: TYPE_NORMAL
- en: Well, given that her mammography was positive, you might reason that the probability
    of her having cancer is quite high (somewhere near 80%). It seems much less likely
    that the woman would belong to 9.6% with false positives, so the real probability
    is probably somewhere between 70% and 80%.
  prefs: []
  type: TYPE_NORMAL
- en: I'm afraid that's not correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s one way to think about this problem. For the sake of simplicity, let''s
    assume we''re looking at some concrete number of patients, say 10,000\. Before
    the mammography screening, the 10,000 women can be divided into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Group X**: 100 women *with* breast cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group Y**: 9,900 women *without* breast cancer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, so good. If we sum up the numbers in the two groups, we get a total
    of 10,000 patients, confirming that nobody has been lost in the math. After the
    mammography screening, we can divide the 10,000 women into four groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Group 1**: 80 women with breast cancer and a positive mammography'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group 2**: 20 women with breast cancer and a negative mammography'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group 3**: Around 950 women without breast cancer and with a positive mammography'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Group 4**: Approx. 8,950 women without breast cancer and with a negative
    mammography'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the preceding analysis, you can see that the sum of all the four groups
    is 10,000\. The sum of **Group 1** and **Group 2** (with breast cancer) corresponds
    to **Group** **X**, and the sum of **Group 3** and **Group 4** (without breast
    cancer) corresponds to **Group Y**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This might become clearer when we draw it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbf90aac-09df-48e2-8fd2-5be6460eccbb.png)'
  prefs: []
  type: TYPE_IMG
- en: In this diagram, the top half corresponds to **Group X**, and the bottom half
    corresponds to **Group Y**. Analogously, the left half corresponds to all women
    with positive mammographies, and the right half corresponds to all women with
    negative mammographies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it is easier to see that what we are looking for concerns only the left
    half of the diagram. The proportion of cancer patients with positive results within
    the group of all patients with positive results is the proportion of Group 1 within
    Groups 1 and 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '*80 / (80 + 950) = 80 / 1,030 = 7.8%*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, if you offer a mammography to 10,000 patients, then out of
    the 1,030 with a positive mammographies, there would be 80 patients with positive
    mammography having cancer. The answer a doctor should give a positive mammography
    patient if she asks about the chance she has breast cancer: roughly 1 out of 13
    will have cancer, given that 13 patients ask this question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What we just calculated is called a **conditional probability**: what is our
    **degree of belief** that a woman has breast cancer **under the condition** of
    (we also say **given**) a positive mammography? As in the last subsection, we
    denote this with *p(cancer|mammography)*, or *p(C|M)* for short. Using capital
    letters once again enforces the idea that both health and the mammography can
    have several outcomes, depending on several underlying (and possibly unknown)
    causes. Therefore, they are random variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can express *P(C|M)* with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b561a41b-448a-49cc-8be0-3772a3e0f27f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p(C, M)* denotes the probability that both *C* and *M* are true (meaning
    the probability that a woman both has cancer and a positive mammography). This
    is equivalent to the probability of a woman belonging to Group 1, as shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The comma (*,*) means logical *and*, and the tilde (*~*) stands for logical
    *not*. Hence, *p(~C, M)* denotes the probability that *C* is not true and *M*
    is true (meaning the probability that a woman does not have cancer but has a positive
    mammography). This is equivalent to the probability of a woman belonging to Group
    3\. So, the denominator basically adds up women in Group 1 (*p(C, M)*) and Group
    3 (*p(~C, M)*).
  prefs: []
  type: TYPE_NORMAL
- en: 'But wait! Those two groups together simply denote the probability of a woman
    having a positive mammography, *p(M)*. Therefore, we can simplify the preceding
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55a3d8a8-bb02-4a6f-888c-8f322fab1880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Bayesian version is to re-interpret what *p(C, M)* means. We can express
    *p(C, M)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98a72194-6ede-4115-af51-7446f71a6a0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now it gets a bit confusing. Here, *p(C)* is simply the probability that a woman
    has cancer (corresponding to the aforementioned Group X). Given that a woman has
    cancer, what is the probability that her mammography will be positive? From the
    problem question, we know it is 80%. This is *p(M|C)*, the probability of *M* given
    *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Replacing *p(C, M)* in the first equation with this new formula, we get the
    following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc3c16dd-2798-44e8-976d-6a474464d8ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the Bayesian world, these terms all have their specific names:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p(C|M)* is called the **posterior**, which is always the thing we want to
    compute. In our example, this corresponds to the degree of belief that a woman
    has breast cancer, given a positive mammography.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(C)* is called the **prior** as it corresponds to our initial knowledge about
    how common breast cancer is. We also call this our initial degree of belief in
    *C*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(M|C)* is called the **likelihood**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(M)* is called **evidence**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, you can rewrite the equation one more time, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf3d09c1-5036-4405-b41b-2fab3f42f49e.png)'
  prefs: []
  type: TYPE_IMG
- en: Mostly, there is interest only in the numerator of that fraction because the
    denominator does not depend on *C,* so the denominator is constant and can be
    neglected.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have only talked about one piece of evidence. However, in most real-world
    scenarios, we have to predict an outcome (such as a random variable, *Y*) given
    multiple pieces of evidence (such as random variables *X[1]* and *X[2]*). So,
    instead of calculating *p(Y|X),* we would often have to calculate *p(Y|X[1], X[2],
    ..., X[n])*. Unfortunately, this makes the math very complicated. For two random
    variables, *X[1]* and *X[2]*, the joint probability would be computed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e0699f8-d90a-44d6-930a-678af88be987.png)'
  prefs: []
  type: TYPE_IMG
- en: The ugly part is the term *p(X[1]|X[2], C)*, which says that the conditional
    probability of *X[1]* depends on all other variables, including *C*. This gets
    even ...
  prefs: []
  type: TYPE_NORMAL
- en: Implementing your first Bayesian classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But enough with the math, let's do some coding!
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to generate a number of Gaussian blobs
    using scikit-learn. Do you remember how that is done?
  prefs: []
  type: TYPE_NORMAL
- en: Creating a toy dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The function I''m referring to resides within scikit-learn''s `datasets` module.
    Let''s create 100 data points, each belonging to one of two possible classes,
    and group them into two Gaussian blobs. To make the experiment reproducible, we
    specify an integer to pick a seed for `random_state`. You can again pick whatever
    number you prefer. Here, I went with Thomas Bayes'' year of birth (just for kicks):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look at the dataset we just created using our trusty friend,
    Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Classifying the data with a normal Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will then use the same procedure as in earlier chapters to train a **normal
    Bayes classifier**. Wait, why not a Naive Bayes classifier? Well, it turns out
    OpenCV doesn't really provide a true Naive Bayes classifier. Instead, it comes
    with a Bayesian classifier that doesn't necessarily expect features to be independent,
    but rather expects the data to be clustered into Gaussian blobs. This is exactly the
    kind of dataset we created earlier!
  prefs: []
  type: TYPE_NORMAL
- en: 'By following these steps, you will learn how to build a classifier with a normal
    Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a new classifier using the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, training is done via the `train` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the classifier has been trained successfully, it will return `True`. We
    go through the motions of predicting and scoring the classifier, just like we
    have done a million times before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Even better—we can reuse the plotting function from the last chapter to inspect
    the decision boundary! If you recall, the idea was to create a mesh grid that
    would encompass all data points and then classify every point on the grid. The
    mesh grid is created via the NumPy function of the same name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `meshgrid` function will return two floating-point matrices, `xx` and `yy`,
    that contain the *x* and *y* coordinates of every coordinate point on the grid.
    We can flatten these matrices into column vectors using the `ravel` function and
    stack them to form a new matrix, `X_hypo`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`X_hypo` now contains all *x* values in `X_hypo[:, 0]` and all *y* values in
    `X_hypo[:, 1]`. This is a format that the `predict` function can understand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we want to be able to use models from both OpenCV and scikit-learn.
    The difference between the two is that OpenCV returns multiple variables (a Boolean
    flag indicating success/failure and the predicted target labels), whereas scikit-learn
    returns only the predicted target labels. Hence, we can check whether the `ret`
    output is a tuple, in which case, we know we''re dealing with OpenCV. In this
    case, we store the second element of the tuple (`ret[1]`). Otherwise, we are dealing
    with scikit-learn and don''t need to index into `ret`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'All that''s left to do is to create a contour plot where `zz` indicates the
    color of every point on the grid. On top of that, we plot the data points using
    our trusty scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We call the function by passing a model (`model_norm`), a feature matrix (`X`),
    and a target label vector (`y`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67d6aaae-c7d9-4b3f-ae04-df35a7fbd1cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So far, so good. The interesting part is that a Bayesian classifier also returns
    the probability with which each data point has been classified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The function returns a Boolean flag (`True` for success and `False` for failure),
    the predicted target labels (`y_pred`), and the conditional probabilities (`y_proba`).
    Here, `y_proba` is an *N x* 2 matrix that indicates, for every one of the *N*
    data points, the probability with which it was classified as either class 0 or
    class 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This means that, for the first data point (top row), the probability of it belonging
    to class 0 (that is, *p(C[0]|X)*) is 0.15 (or 15%)). Similarly, the probability
    of belonging to class 1 is *p(C[1]|X)* = *0.05.*
  prefs: []
  type: TYPE_NORMAL
- en: The reason why some of the rows show values greater than 1 is that OpenCV does
    not really return probability values. Probability values are always between 0
    and 1, and each row in the preceding matrix should add up to 1\. Instead, what
    is being reported is a **likelihood**, which is basically the numerator of the
    conditional probability equation, *p(C)* *p(M|C)*. The denominator, *p*(*M*),
    does not need to be computed. All we need to know is that *0.15 > 0.05* (top row).
    Hence, the data point most likely belongs to class 0.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying the data with a Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following steps will help you build a Naive Bayes classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can compare the result to a true Naive Bayes classifier by asking scikit-learn
    for help:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, training the classifier is done via the `fit` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Scoring the classifier is built in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Again a perfect score! However, in contrast to OpenCV, this classifier''s `predict_proba`
    method returns true probability values, because all values are between 0 and 1
    and because all rows add up to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing conditional probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'By referring to the following steps, you will be able to visualize conditional
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will slightly modify the plot function from the previous example.
    We start out by creating a mesh grid between (`x_min`, `x_max`) and (`y_min`,
    `y_max`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we flatten `xx` and `yy` and add them column-wise to the feature matrix, `X_hypo`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to make this function work with both OpenCV and scikit-learn, we
    need to implement a switch for `predictProb` (in the case of OpenCV) and `predict_proba`
    (in the case of scikit-learn). For this, we check whether `model` has a method
    called `predictProb`. If the method exists, we can call it; otherwise, we assume
    we''re dealing with a model from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Like in `In [16]`, which we saw earlier, `y_proba` will be a 2D matrix containing,
    for each data point, the probability of the data belonging to class 0 (in `y_proba[:,
    0]`) and to class 1 (in `y_proba[:, 1]`). An easy way to convert these two values
    into a color that the contour function can understand is to simply take the difference
    of the two probability values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to plot `X_test` as a scatter plot on top of the colored mesh
    grid:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to call the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cb91905-247c-46b9-a208-fadaf2df1d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows conditional probabilities of a Naive Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying emails using the Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final task of this chapter will be to apply our newly gained skills to a
    real spam filter! This task deals with solving a binary-class (spam/ham) classification
    problem using the Naive Bayes algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classifiers are actually a very popular model for email filtering.
    Their naivety lends itself nicely to the analysis of text data, where each feature
    is a word (or a **bag of words**), and it would not be feasible to model the dependence
    of every word on every other word.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a bunch of good email datasets out there, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hewlett-Packard spam database: [https://archive.ics.uci.edu/ml/machine-learning-databases/spambase](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Enrom-Spam dataset: [http://www.aueb.gr/users/ion/data/enron-spam ...](http://www.aueb.gr/users/ion/data/enron-spam)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to these steps to load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: If you downloaded the latest code from GitHub, you will find several `.zip`
    files in the `notebooks/data/chapter7` directory. These files contain raw email
    data (with fields for To:, Cc:, and text body) that are either classified as spam
    (with the `SPAM = 1` class label) or not (also known as ham, the `HAM = 0` class
    label).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We build a variable called `sources`, which contains all of the raw data files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step is to extract these files into subdirectories. For this, we
    can use the `extract_tar` function we wrote in the previous chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the function to all data files in the sources, we need to run a loop.
    The `extract_tar` function expects a path to the `.tar.gz` file—which we build
    from `datadir` and an entry in `sources`—and a directory to extract the files
    to (`datadir`). This will extract all emails in, for example, `data/chapter7/beck-s.tar.gz`
    to the `data/chapter7/beck-s/` subdirectory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now here''s the tricky bit. Every one of these subdirectories contains many
    other directories, wherein the text files reside. So, we need to write two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`read_single_file(filename)`: This is a function that extracts the relevant
    content from a single file called `filename`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`read_files(path)`: This is a function that extracts the relevant content from
    all files in a particular directory called `path`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To extract the relevant content from a single file, we need to be aware of
    how each file is structured. The only thing we know is that the header section
    of the email (From:, To:, and Cc:) and the main body of text are separated by
    a newline character, `''\n''`. So, what we can do is iterate over every line in
    the text file and keep only those lines that belong to the main text body, which
    will be stored in the variable lines. We also want to keep a Boolean flag, `past_header`,
    around, which is initially set to `False` but will be flipped to `True` once we
    are past the header section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by initializing those two variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check whether a file with the name `filename` exists. If it does,
    we start looping over it line by line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed the `encoding="latin-1"` part. Since some of the emails
    are not in Unicode, this is an attempt to decode the files correctly.
  prefs: []
  type: TYPE_NORMAL
- en: We do not want to keep the header information, so we keep looping until we encounter
    the `'\n'` character, at which point we flip `past_header` from `False` to `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the first condition of the following `if-else` clause is met,
    and we append all remaining lines in the text file to the `lines` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we concatenate all lines into a single string, separated by the
    newline character, and return both the full path to the file and the actual content
    of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The job of the second function will be to loop over all files in a folder and
    call `read_single_file` on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, `yield` is a keyword that is similar to `return`. The difference is that
    `yield` returns a generator instead of the actual values, which is desirable if
    you expect to have a large number of items to iterate over.
  prefs: []
  type: TYPE_NORMAL
- en: Building a data matrix using pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it''s time to introduce another essential data science tool that comes
    preinstalled with Python Anaconda: **pandas**. pandas is built on NumPy and provides
    several useful tools and methods to deal with data structures in Python. Just
    as we generally import NumPy under the alias, `np`, it is common to import pandas
    under the `pd` alias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas provide a useful data structure called a DataFrame, which can be understood
    as a generalization of a 2D NumPy array, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scikit-learn offers several options when it comes to encoding text features,
    which we discussed in [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*. One of the simplest methods of encoding
    text data, as you may recall, is by **word count**; for each phrase, you count
    the number of occurrences of each word within it. In scikit-learn, this is easily
    done using `CountVectorizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a giant matrix, which tells us that we harvested a total of 52,076
    emails that collectively contain 643,270 different words. However, scikit-learn
    is smart and saved the data in a sparse matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To build the vector of target labels (`y`), we need to access data in the pandas
    DataFrame. This can be done by treating the DataFrame like a dictionary, where
    the `values` attribute will give us access to the underlying NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Training a normal Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From here on out, things are (almost) like they always were. We can use scikit-learn
    to split the data into training and test sets (let''s reserve 20% of all data
    points for testing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can instantiate a new normal Bayes classifier with OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: However, OpenCV does not know about sparse matrices (at least its Python interface
    does not). If we were to pass `X_train` and `y_train` to the `train` function
    as we did earlier, OpenCV would complain that the data matrix is not a NumPy array.
    ...
  prefs: []
  type: TYPE_NORMAL
- en: Training on the full dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, if you want to classify the full dataset, we need a more sophisticated
    approach. We turn to scikit-learn''s Naive Bayes classifier, as it understands
    how to handle sparse matrices. In fact, if you didn''t pay attention and treated
    `X_train` like every NumPy array before, you might not even notice that anything
    is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used `MultinomialNB` from the `naive_bayes` module, which is the version
    of Naive Bayes classifier that is best suited to handling categorical data, such
    as word counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classifier is trained almost instantly and returns the scores for both
    the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'And there we have it: 94.4% accuracy on the test set! Pretty good for not doing
    much other than using the default values, isn''t it?'
  prefs: []
  type: TYPE_NORMAL
- en: However, what if we were super critical of our own work and wanted to improve
    the result even further? There are a couple of things we could do.
  prefs: []
  type: TYPE_NORMAL
- en: Using n-grams to improve the result
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One thing to do is to use *n*-gram counts instead of plain word counts. So
    far, we have relied on what is known as a bag of words: we simply threw every
    word of an email into a bag and counted the number of its occurrences. However,
    in real emails, the order in which words appear can carry a great deal of information!'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what *n*-gram counts are trying to convey. You can think of
    an *n*-gram as a phrase that is *n* words long. For example, the phrase *Statistics
    has its moments* contains the following 1-grams: *Statistics*, *has*, *its*, and
    *moments*. It also has the following 2-grams: *Statistics has*, *has its*, and
    *its moments*. It also has two 3-grams (*Statistics has its* and *has its moments*)
    and only a single ...'
  prefs: []
  type: TYPE_NORMAL
