["```py\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n```", "```py\n# Set your working directory according to your requirement\nos.chdir(\".../Chapter 8/\")\nos.getcwd()\n```", "```py\ndf_creditcarddata = pd.read_csv(\"UCI_Credit_Card.csv\")\n```", "```py\ndf_creditcarddata = df_creditcarddata.drop(\"ID\", axis= 1) \n```", "```py\ndf_creditcarddata.shape\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\nX = df_creditdata.iloc[:,0:23]\nY = df_creditdata['default.payment.next.month']\n```", "```py\n# We first split the dataset into train and test subset\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=1)\n\n# Then we take the train subset and carve out a validation set from the same\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=1)\n```", "```py\n# Dimensions for train subsets\nprint(X_train.shape)\nprint(Y_train.shape)\n\n# Dimensions for validation subsets\nprint(X_val.shape)\nprint(Y_val.shape)\n\n# Dimensions for test subsets\nprint(X_test.shape)\nprint(Y_test.shape)\n```", "```py\n# for the base learners\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# for the meta learner\nfrom sklearn.linear_model import LogisticRegression\n```", "```py\n# The base learners\nmodel_1 = GaussianNB()\nmodel_2 = KNeighborsClassifier(n_neighbors=1)\nmodel_3 = DecisionTreeClassifier()\n\n# Now we train a list of models\nbase_learner_1 = model_1.fit(X_train, Y_train)\nbase_learner_2 = model_2.fit(X_train, Y_train)\nbase_learner_3 = model_3.fit(X_train, Y_train)\n```", "```py\n# We then use the models to make predictions on validation data\nval_prediction_base_learner_1 = base_learner_1.predict(X_val)\nval_prediction_base_learner_2 = base_learner_2.predict(X_val)\nval_prediction_base_learner_3 = base_learner_3.predict(X_val)\n```", "```py\n# And then use the predictions to create a new stacked dataset\nimport numpy as np\nprediction_test_stack = np.dstack([val_prediction_base_learner_1, val_prediction_base_learner_2, val_prediction_base_learner_3])\n\n# Now we stack the actual outcomes i.e. Y_Test with the prediction_stack\nfinal_train_stack = np.dstack([prediction_test_stack, Y_val])\n```", "```py\nstacked_train_dataframe = pd.DataFrame(final_train_stack[0,0:5400], columns='NB_VAL KNN_VAL DT_VAL Y_VAL'.split())\n\nprint(stacked_train_dataframe.shape)\nprint(stacked_train_dataframe.head(5))\n```", "```py\n# Build the Mata-learner\nmeta_learner = LogisticRegression()\nmeta_learner_model = meta_learner.fit(stacked_train_dataframe.iloc[:,0:3], stacked_train_dataframe['Y_VAL'])\n```", "```py\n# Take the test data (new data)\n# Apply the base learners on this new data to make predictions\n\n# We now use the models to make predictions on the test data and create a new stacked dataset\ntest_prediction_base_learner_1 = base_learner_1.predict(X_test)\ntest_prediction_base_learner_2 = base_learner_2.predict(X_test)\ntest_prediction_base_learner_3 = base_learner_3.predict(X_test)\n\n# Create the stacked data\nfinal_test_stack = np.dstack([test_prediction_base_learner_1, test_prediction_base_learner_2, test_prediction_base_learner_3])\n```", "```py\nstacked_test_dataframe = pd.DataFrame(final_test_stack[0,0:3000], columns='NB_TEST KNN_TEST DT_TEST'.split())\nprint(stacked_test_dataframe.shape)\nprint(stacked_test_dataframe.head(5))\n```", "```py\ntest_prediction_base_learner_1 = base_learner_1.predict(X_test)\ntest_prediction_base_learner_2 = base_learner_2.predict(X_test)\ntest_prediction_base_learner_3 = base_learner_3.predict(X_test)\n\nprint(\"Accuracy from GaussianNB:\", accuracy_score(Y_test, test_prediction_base_learner_1))\nprint(\"Accuracy from KNN:\", accuracy_score(Y_test, test_prediction_base_learner_2))\nprint(\"Accuracy from Decision Tree:\", accuracy_score(Y_test, test_prediction_base_learner_3))\n```", "```py\ntest_predictions_meta_learner = meta_learner_model.predict(stacked_test_dataframe)\nprint(\"Accuracy from Meta Learner:\", accuracy_score(Y_test, test_predictions_meta_learner))\n```", "```py\nimport h2o\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n```", "```py\nh2o.init()\n```", "```py\ndf_bankdata = h2o.import_file(\"bank-full.csv\")\n```", "```py\n# split into train and validation sets\ntrain, test = df_bankdata.split_frame(ratios = [.8], seed = 1234)\n```", "```py\ntrain.shape, test.shape\n```", "```py\ndf_bankdata.head()\n```", "```py\n# Set the predictor names \npredictors = train.columns\n\n# Set the response column name\nresponse = \"y\"\n\n# Remove the 'y' variable from the predictors\npredictors.remove(response)\n\nprint(predictors)\n```", "```py\ntrain[response] = train[response].asfactor()\ntest[response] = test[response].asfactor()\n```", "```py\n# Number of CV folds\nnfolds = 5\n\n# Using the `categorical_encoding` parameter\nencoding = \"OneHotExplicit\"\n```", "```py\n# Train and cross-validate a GBM\nbase_learner_gbm = H2OGradientBoostingEstimator(distribution=\"bernoulli\",\\\n                                                ntrees=100,\\\n                                                max_depth=5,\\\n                                                min_rows=2,\\\n                                                learn_rate=0.01,\\\n                                                nfolds=nfolds,\\\n                                                fold_assignment=\"Modulo\",\\\n                                                categorical_encoding = encoding,\\\n                                                keep_cross_validation_predictions=True)\n\nbase_learner_gbm.train(x=predictors, y=response, training_frame=train)\n```", "```py\n# Train and cross-validate a RF\nbase_learner_rf = H2ORandomForestEstimator(ntrees=250,\\\n                                           nfolds=nfolds,\\\n                                           fold_assignment=\"Modulo\",\\\n                                           categorical_encoding = encoding,\\\n                                           keep_cross_validation_predictions=True)\nbase_learner_rf.train(x=predictors, y=response, training_frame=train)\n```", "```py\n# Train and cross-validate a GLM\nbase_learner_glm = H2OGeneralizedLinearEstimator(family=\"binomial\",\\\n                                                 model_id=\"GLM\",\\\n                                                 lambda_search=True,\\\n                                                 nfolds = nfolds,\\\n                                                 fold_assignment = \"Modulo\",\\\n                                                 keep_cross_validation_predictions = True)\n\nbase_learner_glm.train(x = predictors, y = response,training_frame = train)\n```", "```py\n# Compare to base learner performance on the test set\ngbm_test_performance = base_learner_gbm.model_performance(test)\nrf_test_performance = base_learner_rf.model_performance(test)\nglm_test_performance = base_learner_glm.model_performance(test)\n\nprint(\"Best AUC from the GBM\", gbm_test_performance.auc())\nprint(\"Best AUC from the Random Forest\", rf_test_performance.auc())\nprint(\"Best AUC from the GLM\", glm_test_performance.auc())\n\nbaselearner_best_auc_test = max(gbm_test_performance.auc(), rf_test_performance.auc(), glm_test_performance.auc())\nprint(\"Best AUC from the base learners\", baselearner_best_auc_test)\n\nstack_auc_test = perf_stack_test.auc()\nprint(\"Best Base-learner Test AUC: \", baselearner_best_auc_test)\nprint(\"Ensemble Test AUC: \", stack_auc_test)\n```", "```py\nall_models = [base_learner_glm, base_learner_gbm, base_learner_rf]\n\n# Set up Stacked Ensemble. Using Deep Learning as the meta learner\nensemble_deep = H2OStackedEnsembleEstimator(model_id =\"stack_model_d\", base_models = all_models, metalearner_algorithm = 'deeplearning')\n\nensemble_deep.train(y = response, training_frame = train)\n\n# Eval ensemble performance on the test data\nperf_stack_test = ensemble_deep.model_performance(test)\nstack_auc_test = perf_stack_test.auc()\nprint(\"Ensemble_deep Test AUC: {0}\".format(stack_auc_test))\n```", "```py\nhyper_params = {\"max_depth\": [3, 4, 5, 8, 10],\n                \"min_rows\": [3,4,5,6,7,8,9,10],\n                \"mtries\": [10,15, 20],\n                \"ntrees\": [100,250,500, 750],\n                \"sample_rate\": [0.7, 0.8, 0.9, 1.0],\n                \"col_sample_rate_per_tree\": [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\n\nsearch_criteria = {\"strategy\": \"RandomDiscrete\", \"max_models\": 3, \"seed\": 1}\n```", "```py\n# Train the grid\ngrid = H2OGridSearch(model=H2ORandomForestEstimator(nfolds=nfolds,\\\n                                                    fold_assignment=\"Modulo\",\\\n                                                    keep_cross_validation_predictions=True),\\\n                     hyper_params=hyper_params,\\\n                     search_criteria=search_criteria,\\\n                     grid_id=\"rf_grid_binomial\")\n\ngrid.train(x=predictors, y=response, training_frame=train)\n```", "```py\n# Train a stacked ensemble using the RF grid\nensemble = H2OStackedEnsembleEstimator(model_id=\"ensemble_rf_grid_binomial_9\", base_models=grid.model_ids)\n\nensemble.train(x=predictors, y=response, training_frame=train)\n\n# Evaluate ensemble performance on the test data\nperf_stack_test = ensemble.model_performance(test)\n\n# Compare to base learner performance on the test set\nbaselearner_best_auc_test = max([h2o.get_model(model).model_performance(test_data=test).auc() for model in grid.model_ids])\n\nstack_auc_test = perf_stack_test.auc()\n\nprint(\"Best Base-learner Test AUC: \", baselearner_best_auc_test)\nprint(\"Ensemble Test AUC: \", stack_auc_test)\n```", "```py\nimport numpy as np\n\n# import required libraries from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\n# import StackNetClassifier and StackNetRegressor from pystacknet\nfrom pystacknet.pystacknet import StackNetClassifier,StackNetRegressor\nfrom pystacknet.metrics import rmse,mae\n```", "```py\ndf_creditcarddata = pd.read_csv(\"UCI_Credit_Card.csv\")\n\n#dropping the ID column, as it would not be required\ndf_creditcarddata.drop([\"ID\"],axis=1,inplace=True)\n\n# Check the shape of the data\ndf_creditcarddata.shape\n```", "```py\n#create the predictor & target set\nX = df_creditcarddata.iloc[:,0:23]\nY = df_creditcarddata['default.payment.next.month']\n\n# Create train & test sets\nX_train, X_test, Y_train, Y_test = \\\ntrain_test_split(X, Y, test_size=0.20, random_state=1)\n```", "```py\nmodels=[[DecisionTreeClassifier(criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1),\nGradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, max_features=0.5, random_state=1),\nLogisticRegression(random_state=1)],\n[RandomForestClassifier (n_estimators=500, criterion=\"entropy\", max_depth=5, max_features=0.5, random_state=1)]]\n```", "```py\nmodel=StackNetClassifier(models, metric=\"accuracy\", folds=4, restacking=False, use_retraining=True, use_proba=True, random_state=12345, n_jobs=1, verbose=1)\n\nmodel.fit(X_train,Y_train )\n\n# Uses the meta-learner model to predict the outcome\npreds=model.predict_proba(X_test)[:,1]\nprint (\"TEST ACCURACY without RESTACKING, auc %f \" % (roc_auc_score(Y_test,preds)))\n```"]