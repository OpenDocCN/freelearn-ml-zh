<html><head></head><body>
		<div id="_idContainer370">
			<h1 id="_idParaDest-131"><em class="italic"><a id="_idTextAnchor137"/>Chapter 14</em><span class="superscript">: Conclusions and Next Steps</span></h1>
			<p><span class="superscript">Congratulations on finishing this book! You have been introduced to a lot of interesting concepts, methods, and implementations related to hyperparameter tuning throughout the previous chapters. This chapter summarizes all the important lessons learned in the previous chapters, and will introduce you to several topics or implementations that you may benefit from that we have not covered yet in this book.</span></p>
			<p><span class="superscript">The following are the main topics that will be discussed in this chapter:</span></p>
			<ul>
				<li><span class="superscript">Revisiting hyperparameter tuning methods and packages</span></li>
				<li><span class="superscript">Revisiting HTDM</span></li>
				<li><span class="superscript">What’s next?</span></li>
			</ul>
			<h1 id="_idParaDest-132"><span class="superscript"><a id="_idTextAnchor138"/>Revisiting hyperparameter tuning methods an</span><span class="superscript">d packages</span></h1>
			<p><span class="superscript">Throughout this book, we have discussed four groups of hyperparameter tuning methods, including </span><span class="superscript"><a id="_idIndexMarker690"/></span><span class="superscript">exhaustive search, Bayesian </span><span class="superscript"><a id="_idIndexMarker691"/></span><span class="superscript">optimization, heuristic search, and multi-fidelity optimization. All the methods within each group have similar characteristics to each other. For example, manual search, grid search, and random search, which are part of the exhaustive search group, all work by exhaustively searching through the hyperparameter space, and can be categorized as uninformed search methods. </span></p>
			<p><span class="superscript">Bayesian optimization hyperparameter tuning methods are categorized as informed search methods, where all of them work by utilizing both surrogate model and acquisition function. Hyperparameter tuning methods, which are part of the heuristic search group, work by performing trial and error. As for hyperparameter tuning methods from the multi-fidelity optimization group, they all utilize the cheap approximation of the whole hyperparameter tuning pipeline, so that we can have similar performance results with much lesser computational cost and faster experiment time.</span></p>
			<p><span class="superscript">The following </span><span class="superscript"><a id="_idIndexMarker692"/></span><span class="superscript">table summarizes all of the </span><span class="superscript"><a id="_idIndexMarker693"/></span><span class="superscript">hyperparameter tuning methods discussed in this book, along with the supported packages:</span></p>
			<p><span class="superscript"><img src="image/B18753_14_table_001(a).png" alt="Figure 14.1 – Hyperparameter tuning methods and packages summary&#13;&#10;"/></span></p>
			<div>
				<div id="_idContainer368" class="IMG---Figure">
					<img src="image/B18753_14_table_001(b).jpg" alt="Figure 14.1 – Hyperparameter tuning methods and packages summary&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="superscript">Figure 14.1 – Hyperparameter tuning methods and packages summary</span></p>
			<p><span class="superscript">In this </span><span class="superscript"><a id="_idIndexMarker694"/></span><span class="superscript">section, we have revisited all of the </span><span class="superscript"><a id="_idIndexMarker695"/></span><span class="superscript">hyperparameter tuning methods and packages discussed throughout the book. In the next section, we will revisit the HTDM.</span></p>
			<h1 id="_idParaDest-133"><span class="superscript"><a id="_idTextAnchor139"/>Revis</span><span class="superscript">iting HTDM</span></h1>
			<p><span class="superscript">The </span><strong class="bold">Hyperparameter Tuning Decision Map</strong> (<strong class="bold">HTDM</strong>) is a map that you can use to help you <a id="_idIndexMarker696"/>decide which hyperparameter tuning method should be adopted in a particular situation. We discussed <a id="_idIndexMarker697"/>in detail how you can utilize HTDM, along with several use cases, in <a href="B18753_11_ePub.xhtml#_idTextAnchor110"><em class="italic">Chapter 11</em></a>, <em class="italic">Introducing the Hyperparameter Tuning Decision Map</em>. Here, we will only revisit the map, as shown in the following figure:</p>
			<div>
				<div id="_idContainer369" class="IMG---Figure">
					<img src="image/B18753_14_002.jpg" alt="Figure 14.2 – Hyperparameter Tuning Decision Map&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – HTDM</p>
			<p>In this section, we have revisited the HTDM. In the next section, we’ll discuss other topics you may find interesting to further boost your hyperparameter tuning knowledge.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor140"/>What’s next?</h1>
			<p>Even though we have discussed a lot of hyperparameter tuning methods and their implementations in various packages, there are several important concepts you may need to know <a id="_idIndexMarker698"/>about that have not been discussed in this book. As for the hyperparameter tuning method, you can also read more <a id="_idIndexMarker699"/>about the <strong class="bold">CMA-ES</strong> method, which is part of the heuristic search group (<a href="https://cma-es.github.io/">https://cma-es.github.io/</a>). You can also read more about the <strong class="bold">meta-learning</strong> concept to <a id="_idIndexMarker700"/>further boost the performance of your Bayesian optimization tuning results (<a href="https://lilianweng.github.io/posts/2018-11-30-meta-learning/">https://lilianweng.github.io/posts/2018-11-30-meta-learning/</a>). It is also worth noting that we can combine the manual search method with other hyperparameter tuning methods to boost the efficiency of our experiments, especially when we already have prior knowledge about the good range of the hyperparameter values.</p>
			<p>As for the <a id="_idIndexMarker701"/>packages, you can also learn more about the <strong class="bold">HpBandSter</strong> package, which implements the Hyper Band, BOHB, and random search methods (<a href="https://github.com/automl/HpBandSter">https://github.com/automl/HpBandSter</a>). Finally, there are also several packages that automatically create a scikit-learn wrapper from the non-scikit-learn model. For <a id="_idIndexMarker702"/>example, you can utilize the <strong class="bold">Skorch</strong> package to create scikit-learn wrappers from PyTorch models (<a href="https://skorch.readthedocs.io/en/stable/">https://skorch.readthedocs.io/en/stable/</a>).</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor141"/>Summary</h1>
			<p>In this chapter, we have summarized all the important concepts discussed throughout all chapters in this book. You have also been introduced to several new concepts that you may want to learn to further boost your hyperparameter tuning kno<a id="_idTextAnchor142"/>wledge. From now on, you will have the skills you need to take full control over your machine learning models and get the best models for the best results via hyperparameter tuning experiments. </p>
			<p>Thanks for investing your interest and time in reading this book. Best of luck on your hyperparameter tuning learning journey!</p>
		</div>
		<div>
			<div id="_idContainer371">
			</div>
		</div>
	</body></html>