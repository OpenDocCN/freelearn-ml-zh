- en: Chapter 7. Bayesian Models for Unsupervised Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。无监督学习的贝叶斯模型
- en: 'The machine learning models that we have discussed so far in the previous two
    chapters share one common characteristic: they require training data containing
    ground truth. This implies a dataset containing true values of the predicate or
    dependent variable that is often manually labeled. Such machine learning where
    the algorithm is trained using labeled data is called **supervised learning**.
    This type of machine learning gives a very good performance in terms of accuracy
    of prediction. It is, in fact, the de facto method used in most industrial systems
    using machine learning. However, the drawback of this method is that, when one
    wants to train a model with large datasets, it would be difficult to get the labeled
    data. This is particularly relevant in the era of Big Data as a lot of data is
    available for organizations from various logs, transactions, and interactions
    with consumers; organizations want to gain insight from this data and make predictions
    about their consumers'' interests.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前两章中讨论的机器学习模型具有一个共同特征：它们需要包含真实值的训练数据。这意味着包含谓词或因变量的真实值的数据集通常需要手动标记。这种使用标记数据训练算法的机器学习被称为**监督学习**。这种类型的机器学习在预测准确性方面表现出色。实际上，这是大多数使用机器学习的工业系统中事实上的方法。然而，这种方法的一个缺点是，当人们想要用大数据集训练模型时，获取标记数据会变得困难。这在大数据时代尤为重要，因为组织从各种日志、交易和与消费者的互动中获得了大量数据；组织希望从这些数据中获得洞察力，并对消费者的兴趣做出预测。
- en: In unsupervised methods, no labeled data is required for learning. The process
    of learning happens through identifying dominant patterns and correlations present
    in the dataset. Some common examples of unsupervised learning are clustering,
    association rule mining, density estimation, and dimensional reduction. In clustering,
    naturally occurring groups in data are identified using a suitable algorithm that
    makes use of some distance measure between data points. In association rule mining,
    items that frequently occur together in a transaction are identified from a transaction
    dataset. In dimensional reduction techniques such as principal component analysis,
    the original dataset containing a large number of variables (dimensions) is projected
    down to a lower dimensional space where the maximum information in the data is
    present. Though unsupervised learning doesn't require labeled training data, one
    would need a large amount of data to learn all the patterns of interest and often
    the learning is more computationally intensive.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习方法中，学习过程中不需要标记数据。学习过程是通过识别数据集中存在的占主导地位的模式和相关性来进行的。无监督学习的常见例子包括聚类、关联规则挖掘、密度估计和降维。在聚类中，使用一种合适的算法识别数据中的自然分组，该算法利用数据点之间的某种距离度量。在关联规则挖掘中，从事务数据集中识别出在事务中经常一起出现的项目。在降维技术，如主成分分析中，包含大量变量（维度）的原始数据集被投影到一个较低维度的空间，其中数据中包含最大信息。尽管无监督学习不需要标记的训练数据，但人们需要大量的数据来学习所有感兴趣的模式，而且通常学习过程计算量更大。
- en: 'In many practical cases, it would be feasible to create a small amount of labeled
    data. The third type of learning, semi-supervised learning, is a method that makes
    use of this small labeled dataset and propagates labels to the rest of the unlabeled
    training data using suitable algorithms. In this chapter, we will cover Bayesian
    approaches for unsupervised learnings. We will discuss in detail two important
    models: Gaussian mixture models for clustering and Latent Dirichlet allocation
    for topic modeling.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际情况下，创建少量标记数据是可行的。第三种学习方法，即半监督学习，是一种利用这个小规模标记数据集，并通过合适的算法将标签传播到其余未标记的训练数据的方法。在本章中，我们将介绍无监督学习的贝叶斯方法。我们将详细讨论两个重要模型：用于聚类的高斯混合模型和用于主题模型的潜在狄利克雷分配。
- en: Bayesian mixture models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯混合模型
- en: 'In general, a mixture model corresponds to representing data using a mixture
    of probability distributions. The most common mixture model is of the following
    type:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，混合模型对应于使用概率分布的混合来表示数据。最常见的混合模型如下所示：
- en: '![Bayesian mixture models](img/image00486.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00486.jpeg)'
- en: 'Here, ![Bayesian mixture models](img/image00487.jpeg) is a probability distribution
    of *X* with parameters ![Bayesian mixture models](img/image00488.jpeg), and ![Bayesian
    mixture models](img/image00489.jpeg) represents the weight for the *k*^(*th*)
    component in the mixture, such that ![Bayesian mixture models](img/image00490.jpeg).
    If the underlying probability distribution is a normal (Gaussian) distribution,
    then the mixture model is called a **Gaussian mixture model** (**GMM**). The mathematical
    representation of GMM, therefore, is given by:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![贝叶斯混合模型](img/image00487.jpeg)是*X*的概率分布，其参数为![贝叶斯混合模型](img/image00488.jpeg)，而![贝叶斯混合模型](img/image00489.jpeg)代表混合中第*k*个成分的权重，使得![贝叶斯混合模型](img/image00490.jpeg)。如果潜在的概率分布是正态（高斯）分布，那么混合模型被称为**高斯混合模型**（**GMM**）。因此，GMM的数学表示如下：
- en: '![Bayesian mixture models](img/image00491.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00491.jpeg)'
- en: Here, we have used the same notation, as in previous chapters, where *X* stands
    for an *N*-dimensional data vector ![Bayesian mixture models](img/image00492.jpeg)
    representing each observation and there are *M* such observations in the dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了与前面章节相同的符号，其中*X*代表一个*N*-维数据向量![贝叶斯混合模型](img/image00492.jpeg)，它表示每个观测值，数据集中有*M*个这样的观测值。
- en: A mixture model such as this is suitable for clustering when the clusters have
    overlaps. One of the applications of GMM is in computer vision. If one wants to
    track moving objects in a video, it is useful to subtract the background image.
    This is called background subtraction or foreground detection. GMMs are used for
    this purpose where the intensity of each pixel is modeled using a mixture of Gaussian
    distributions (reference 1 in the *References* section of this chapter).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当簇之间存在重叠时，这种混合模型适合于聚类。GMM的一个应用领域是计算机视觉。如果想在视频中跟踪移动对象，减去背景图像是有用的。这被称为背景减法或前景检测。GMM用于此目的，其中每个像素的强度使用高斯分布的混合来建模（本章“参考文献”部分的参考1）。
- en: 'The task of learning GMMs corresponds to learning the model parameters ![Bayesian
    mixture models](img/image00493.jpeg) and mixture weights ![Bayesian mixture models](img/image00489.jpeg)
    for all the components ![Bayesian mixture models](img/image00494.jpeg). The standard
    approach for learning GMMs is by using the **maximum likelihood** method. For
    a dataset consisting of *M* observations, the logarithm of the likelihood function
    is given by:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 学习GMM的任务对应于学习所有成分![贝叶斯混合模型](img/image00494.jpeg)的模型参数![贝叶斯混合模型](img/image00493.jpeg)和混合权重![贝叶斯混合模型](img/image00489.jpeg)。学习GMM的标准方法是通过使用**最大似然**方法。对于一个由*M*个观测值组成的数据集，似然函数的对数由以下给出：
- en: '![Bayesian mixture models](img/image00495.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00495.jpeg)'
- en: 'Unlike a single Gaussian model, maximizing the log-likelihood with respect
    to parameters ![Bayesian mixture models](img/image00496.jpeg) cannot be done in
    a straightforward manner in GMM. This is because there is no closed-form expression
    for the derivative in this case, since it is difficult to compute the logarithm
    of a sum. Therefore, one uses what is called an **expectation-maximization** (**EM**)
    algorithm to maximize the log-likelihood function. The EM algorithm is an iterative
    algorithm, where each iteration consists of two computations: expectation and
    maximization. The EM algorithm proceeds as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个高斯模型不同，在GMM中，无法以直接的方式对参数![贝叶斯混合模型](img/image00496.jpeg)进行对数似然函数的最大化。这是因为在这种情况下，由于难以计算和的对数，没有该导数的闭式表达式。因此，使用所谓的**期望最大化**（**EM**）算法来最大化对数似然函数。EM算法是一个迭代算法，其中每个迭代包括两个计算：期望和最大化。EM算法的步骤如下：
- en: Initialize parameters ![Bayesian mixture models](img/image00497.jpeg), ![Bayesian
    mixture models](img/image00498.jpeg), and ![Bayesian mixture models](img/image00489.jpeg)
    and evaluate the initial value of log-likelihood.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化参数![贝叶斯混合模型](img/image00497.jpeg)、![贝叶斯混合模型](img/image00498.jpeg)和![贝叶斯混合模型](img/image00489.jpeg)，并评估对数似然函数的初始值。
- en: In the expectation step, evaluate mixture components ![Bayesian mixture models](img/image00489.jpeg)
    from log-likelihood using the current parameter values ![Bayesian mixture models](img/image00497.jpeg)
    and ![Bayesian mixture models](img/image00498.jpeg).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在期望步骤中，使用当前参数值![贝叶斯混合模型](img/image00497.jpeg)和![贝叶斯混合模型](img/image00498.jpeg)从对数似然函数评估混合成分![贝叶斯混合模型](img/image00489.jpeg)。
- en: In the maximization step, using the values of ![Bayesian mixture models](img/image00489.jpeg)
    computed in step 2, estimate new parameter values ![Bayesian mixture models](img/image00497.jpeg)
    and ![Bayesian mixture models](img/image00498.jpeg) by the maximization of log-likelihood.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最大化步骤中，使用步骤2中计算的![贝叶斯混合模型](img/image00489.jpeg)的值，通过最大化对数似然来估计新的参数值![贝叶斯混合模型](img/image00497.jpeg)和![贝叶斯混合模型](img/image00498.jpeg)。
- en: Compute a new value of the log-likelihood function using the estimated values
    of ![Bayesian mixture models](img/image00489.jpeg), ![Bayesian mixture models](img/image00497.jpeg),
    and ![Bayesian mixture models](img/image00498.jpeg) from steps 2 and 3.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤2和3中估计的![贝叶斯混合模型](img/image00489.jpeg)、![贝叶斯混合模型](img/image00497.jpeg)和![贝叶斯混合模型](img/image00498.jpeg)的值，计算对数似然函数的新值。
- en: Repeat steps 2-4 until the log-likelihood function is converged.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-4，直到对数似然函数收敛。
- en: 'In the Bayesian treatment of GMM, the maximization of log-likelihood is simplified
    by introducing a latent variable *Z*. Let *Z* be a *K*-dimensional binary random
    variable having only one element *1*, and the rest of the *K – 1* elements are
    *0*. Using *Z*, one can write the joint distribution of *X* and *Z* as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯处理GMM中，通过引入一个潜在变量*Z*，简化了对对数似然函数的最大化。设*Z*为一个具有一个元素*1*和其余*K – 1*个元素为*0*的*K*-维二元随机变量。使用*Z*，可以写出*X*和*Z*的联合分布如下：
- en: '![Bayesian mixture models](img/image00499.jpeg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00499.jpeg)'
- en: 'Here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![Bayesian mixture models](img/image00500.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00500.jpeg)'
- en: 'And:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '![Bayesian mixture models](img/image00501.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00501.jpeg)'
- en: 'Therefore:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '![Bayesian mixture models](img/image00502.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00502.jpeg)'
- en: 'And:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后：
- en: '![Bayesian mixture models](img/image00503.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯混合模型](img/image00503.jpeg)'
- en: 'The advantage of introducing a latent variable *Z* in the problem is that the
    expression for log-likelihood is simplified, where the logarithm directly acts
    on the normal distribution as in the case of a single Gaussian model. Therefore,
    it is straightforward to maximize *P(X, Z)*. However, the problem that still remains
    is that we don''t know the value of *Z!* So, the trick is to use an EM-like iterative
    algorithm where, in the E-step, the expectation value of *Z* is estimated and
    in the M-step, using the last estimated value of *Z*, we find the parameter values
    of the Gaussian distribution. The Bayesian version of the EM algorithm for GMM
    proceeds as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中引入潜在变量*Z*的优势在于，对数似然的表达式被简化，其中对数直接作用于正态分布，就像单个高斯模型的情况一样。因此，最大化*P(X, Z)*是直接的。然而，仍然存在的问题是我们不知道*Z!*的值。所以，技巧是使用类似于EM的迭代算法，在E步骤中估计*Z*的期望值，在M步骤中，使用上一步估计的*Z*的最后一个值，找到高斯分布的参数值。贝叶斯版本的EM算法对于GMM的步骤如下：
- en: Initialize parameters ![Bayesian mixture models](img/image00497.jpeg), ![Bayesian
    mixture models](img/image00498.jpeg), and ![Bayesian mixture models](img/image00489.jpeg)
    and evaluate the initial value of log-likelihood.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化参数![贝叶斯混合模型](img/image00497.jpeg)、![贝叶斯混合模型](img/image00498.jpeg)和![贝叶斯混合模型](img/image00489.jpeg)，并评估对数似然函数的初始值。
- en: In the expectation step, use these values to compute the expectation value ![Bayesian
    mixture models](img/image00504.jpeg).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在期望步骤中，使用这些值来计算期望值![贝叶斯混合模型](img/image00504.jpeg)。
- en: In the maximization step, using ![Bayesian mixture models](img/image00504.jpeg)
    fixed, estimate ![Bayesian mixture models](img/image00497.jpeg) and ![Bayesian
    mixture models](img/image00498.jpeg) by maximizing ![Bayesian mixture models](img/image00505.jpeg).
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最大化步骤中，固定![贝叶斯混合模型](img/image00504.jpeg)，通过最大化![贝叶斯混合模型](img/image00505.jpeg)来估计![贝叶斯混合模型](img/image00497.jpeg)和![贝叶斯混合模型](img/image00498.jpeg)。
- en: Compute the new likelihood function.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新的似然函数。
- en: Repeat steps 2-4 until convergence.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-4，直到收敛。
- en: A more detailed treatment of the Bayesian version of the EM algorithm and GMM
    can be found in the book by Christopher M. Bishop (reference 2 in the *References*
    section of this chapter). Here, we leave the theoretical treatment of the Bayesian
    GMM and proceed to look at its R implementation in the **bgmm** package.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在Christopher M. Bishop所著的书中（本章“参考文献”部分的第2条参考文献）可以找到对贝叶斯版本的EM算法和GMM的更详细处理。在这里，我们省略了贝叶斯GMM的理论处理，转而查看其在**bgmm**包中的R实现。
- en: The bgmm package for Bayesian mixture models
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯混合模型的bgmm包
- en: 'The bgmm package was developed by Przemyslaw Biecek and Ewa Szczurek for modeling
    gene expressions data (reference 3 in the *References* section of this chapter).
    It can be downloaded from the CRAN website at [http://cran.r-project.org/web/packages/bgmm/index.html](http://cran.r-project.org/web/packages/bgmm/index.html).
    The package contains not only an unsupervised version of GMM but fully supervised
    and semi-supervised implementations as well. The following are the different models
    available in the bgmm package:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: bgmm 包是由 Przemyslaw Biecek 和 Ewa Szczurek 开发的，用于建模基因表达数据（本章“参考文献”部分的第3个参考文献）。可以从CRAN网站下载：[http://cran.r-project.org/web/packages/bgmm/index.html](http://cran.r-project.org/web/packages/bgmm/index.html)。该包不仅包含GMM的无监督版本，还包含完全监督和半监督的实现。bgmm包中包含以下不同的模型：
- en: '**Fully supervised GMM**: This is the labeled data available for all records
    in a training set. This includes the following:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全监督的GMM**: 这是指训练集中所有记录都有的标记数据。这包括以下内容：'
- en: The `supervised( )` function
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`supervised( )` 函数'
- en: '**Semi-supervised GMM**: This is the labeled data available for a small subset
    of all records in a training set. This includes the following'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督的GMM**: 这是指训练集中所有记录的小子集可用的标记数据。这包括以下内容'
- en: The `semisupervised( )` function
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`semisupervised( )` 函数'
- en: '**Partially supervised GMM**: This is the labeled data available for a small
    subset of all records, but these labels are uncertain. The values of labels are
    given with some probability. There are two functions in the package for partially
    supervised GMM. This includes the following::'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分监督的GMM**: 这是指所有记录的小子集可用的标记数据，但这些标签是不确定的。标签的值以某种概率给出。包中包含两个用于部分监督GMM的函数。这包括以下内容::'
- en: 'The `belief( )` function: The uncertainty of labels is expressed as a probability
    distribution over its components. For the first *m* observations, a belief matrix
    *B* of dimensions *m x k* is given as input where the matrix entry ![The bgmm
    package for Bayesian mixture models](img/image00506.jpeg) denotes the probability
    that the *i*^(th) record has the *j*^(th) label.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`belief( )` 函数：标签的不确定性以对其组件的概率分布来表示。对于前 *m* 个观测值，输入一个维度为 *m x k* 的信念矩阵 *B*，其中矩阵条目
    ![The bgmm package for Bayesian mixture models](img/image00506.jpeg) 表示 *i*^(th)
    条记录具有 *j*^(th) 标签的概率。'
- en: 'The `soft( )` function: In this approach, a plausibility matrix of dimension
    *M x k* is defined across all records in the training set of size *M*. The matrix
    element ![The bgmm package for Bayesian mixture models](img/image00507.jpeg) is
    interpreted as the weight of the prior probability that the *i*^(th) record has
    the *j*^(th) label. If there is no particular information about labels of any
    records, they can be given equal weights. For the purpose of implementation, a
    constraint is imposed on the matrix elements: ![The bgmm package for Bayesian
    mixture models](img/image00508.jpeg).'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`soft( )` 函数：在此方法中，定义了一个维度为 *M x k* 的可信度矩阵，跨越了大小为 *M* 的训练集中的所有记录。矩阵元素 ![The
    bgmm package for Bayesian mixture models](img/image00507.jpeg) 被解释为 *i*^(th) 条记录具有
    *j*^(th) 标签的先验概率的权重。如果没有关于任何记录标签的特定信息，它们可以被赋予相等的权重。为了实施目的，对矩阵元素施加了一个约束：![The bgmm
    package for Bayesian mixture models](img/image00508.jpeg)。'
- en: '**Unsupervised GMM**: This labeled data is not available for any records. This
    includes the following:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督的GMM**: 这是指任何记录都没有标记数据。这包括以下内容：'
- en: The `unsupervised( )` function
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unsupervised( )` 函数'
- en: 'The typical parameters that are passed to these functions are as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给这些函数的典型参数如下：
- en: '*X*: This is a data.frame with the unlabelled *X* data.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*: 这是一个包含未标记 *X* 数据的数据框。'
- en: '*knowns*: This is a data.frame with the labeled *X* data.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*knowns*: 这是一个包含标记 *X* 数据的数据框。'
- en: '*B*: This is a belief matrix that specifies the distribution of beliefs for
    the labeled records. The number of rows of *B* should be the same as that of *knowns*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*B*: 这是一个信念矩阵，指定了标记记录的信念分布。*B* 的行数应与 *knowns* 相同。'
- en: '*P*: This is a matrix of weights of prior probabilities (plausibilities).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*: 这是一个先验概率（可信度）的权重矩阵。'
- en: '*class*: This is a vector of classes or labels for the labeled records.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*class*: 这是一个包含标记记录的类别或标签的向量。'
- en: '*k*: This is the number of components or columns of the *B* matrix.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*: 这是矩阵 *B* 的组成部分或列数。'
- en: '*init.params*: These are the initial values for the estimates of model parameters.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*init.params*: 这些是模型参数估计的初始值。'
- en: The difference between the `belief( )` and `soft( )` functions is that, in the
    first case, the input is a matrix containing prior probability values for each
    possible label, whereas in the second case, the input is a matrix containing weights
    for each of the priors and not the prior probability itself. For more details,
    readers are requested to read the paper by Przemyslaw Biecek et.al (reference
    3 in the *References* section of this chapter).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`belief( )`和`soft( )`函数之间的区别在于，在前一种情况下，输入是一个包含每个可能标签的先验概率值的矩阵，而在第二种情况下，输入是一个包含每个先验的权重而不是先验概率本身的矩阵。更多细节，读者请参阅Przemyslaw
    Biecek等人撰写的论文（本章“参考文献”部分的第3条参考文献）。'
- en: Now, let's do a small illustrative example of using bgmm. We will use the ADL
    dataset from the UCI Machine Learning repository. This dataset contains acceleration
    data from wrist-worn accelerometers from 16 volunteers. The dataset and metadata
    details can be found at [https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer).
    The research work on ADL monitoring systems, where this dataset was generated,
    is published in the two papers by Bruno B. et.al. (reference 4 and reference 5
    in the *References* section of this chapter).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个小示例来展示如何使用bgmm。我们将使用来自UCI机器学习仓库的ADL数据集。这个数据集包含了16个志愿者佩戴在手腕上的加速度计的加速度数据。数据集和元数据详情可以在[https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer)找到。关于生成此数据集的ADL监控系统的研究工作发表在Bruno
    B.等人撰写的两篇论文中（本章“参考文献”部分的第4和第5条参考文献）。
- en: 'For the example of bgmm, we will only use one folder in the dataset directory,
    namely `Brush_teeth`. Firstly, we will do a small amount of preprocessing to combine
    data from the different volunteers into a single file. The following R script
    does this job:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于bgmm的示例，我们将在数据集目录中仅使用一个文件夹，即`Brush_teeth`。首先，我们将进行少量预处理，将来自不同志愿者的数据合并到一个文件中。下面的R脚本完成这项工作：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The last step is to select the *X* and *Z* components of acceleration to create
    a two-dimensional dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是从加速度中选择*X*和*Z*分量来创建一个二维数据集。
- en: 'The following R script calls the `bgmm` function and performs clustering. A
    simple scatter plot of the data suggests that there could be four clusters in
    the dataset and choosing *k = 4* would be sufficient:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下R脚本调用`bgmm`函数并执行聚类。数据的一个简单散点图表明数据集中可能有四个聚类，选择*k = 4*就足够了：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The clusters generated by bgmm can be seen in the following figure; there are
    four clusters whose centers are represented by the four color dots and their respective
    Gaussian densities are represented by the ellipses:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: bgmm生成的聚类可以在以下图中看到；有四个聚类，其中心由四个彩色点表示，其各自的高斯密度由椭圆表示：
- en: '![The bgmm package for Bayesian mixture models](img/image00509.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![The bgmm package for Bayesian mixture models](img/image00509.jpeg)'
- en: Topic modeling using Bayesian inference
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用贝叶斯推理进行主题建模
- en: We have seen the supervised learning (classification) of text documents in [Chapter
    6](part0049.xhtml#aid-1ENBI1 "Chapter 6. Bayesian Classification Models"), *Bayesian
    Classification Models*, using the Naïve Bayes model. Often, a large text document,
    such as a news article or a short story, can contain different topics as subsections.
    It is useful to model such intra-document statistical correlations for the purpose
    of classification, summarization, compression, and so on. The Gaussian mixture
    model learned in the previous section is more applicable for numerical data, such
    as images, and not for documents. This is because words in documents seldom follow
    normal distribution. A more appropriate choice would be multinomial distribution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](part0049.xhtml#aid-1ENBI1 "第6章。贝叶斯分类模型")中看到了文本文档的监督学习（分类），即使用朴素贝叶斯模型。通常，一篇大型的文本文档，如新闻文章或短篇小说，可以包含不同的主题作为子章节。对于分类、摘要、压缩等目的，对这种文档内部统计相关性进行建模是有用的。在前一节中学习的高斯混合模型更适合数值数据，如图像，而不是文档。这是因为文档中的单词很少遵循正态分布。一个更合适的选择是多项式分布。
- en: A powerful extension of mixture models to documents is the work of T. Hofmann
    on Probabilistic Semantic Indexing (reference 6 in the *References* section of
    this chapter) and that of David Blei, et. al. on Latent Dirichlet allocation (reference
    7 in the *References* section of this chapter). In these works, a document is
    described as a mixture of topics and each topic is described by a distribution
    of words. LDA is a generative unsupervised model for text documents. The task
    of LDA is to learn the parameters of the topic distribution, word distributions,
    and mixture coefficients from data. A brief overview of LDA is presented in the
    next section. Readers are strongly advised to read the paper by David Blei, et
    al. to comprehend their approach.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型向文档的强大扩展是T. Hofmann关于概率语义索引的工作（本章“参考文献”部分的第6条参考文献）以及David Blei等人关于潜在狄利克雷分配的工作（本章“参考文献”部分的第7条参考文献）。在这些工作中，文档被描述为主题混合，每个主题由单词分布描述。LDA是一个用于文本文档的生成无监督模型。LDA的任务是从数据中学习主题分布、单词分布和混合系数的参数。下一节将简要介绍LDA。强烈建议读者阅读David
    Blei等人的论文，以理解他们的方法。
- en: Latent Dirichlet allocation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Latent Dirichlet allocation
- en: In LDA, it is assumed that words are the basic units of documents. A word is
    one element of a set known as vocabulary, indexed by ![Latent Dirichlet allocation](img/image00510.jpeg).
    Here, *V* denotes the size of the vocabulary. A word can be represented by a unit-basis
    vector, whose all components are zero except the one corresponding to the word
    that has a value 1\. For example, the *n*^(th) word in a vocabulary is described
    by a vector of size *V*, whose *n*^(th) component ![Latent Dirichlet allocation](img/image00511.jpeg)
    and all other components ![Latent Dirichlet allocation](img/image00512.jpeg) for
    ![Latent Dirichlet allocation](img/image00513.jpeg). Similarly, a document is
    a collection of *N* words denoted by ![Latent Dirichlet allocation](img/image00514.jpeg)
    and a corpus is a collection of *M* documents denoted by ![Latent Dirichlet allocation](img/image00515.jpeg)
    (note that documents are represented here by a bold face **w**, whereas words
    are without bold face w).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在LDA中，假设单词是文档的基本单元。一个单词是称为词汇表的一个集合中的一个元素，由![Latent Dirichlet allocation](img/image00510.jpeg)索引。在这里，*V*表示词汇表的大小。一个单词可以通过一个单位基向量来表示，其所有分量都是零，除了对应于单词的那个分量，其值为1。例如，词汇表中的第*n*个单词由一个大小为*V*的向量描述，其第*n*个分量![Latent
    Dirichlet allocation](img/image00511.jpeg)和所有其他分量![Latent Dirichlet allocation](img/image00512.jpeg)对于![Latent
    Dirichlet allocation](img/image00513.jpeg)。同样，一个文档是由![Latent Dirichlet allocation](img/image00514.jpeg)表示的*N*个单词的集合，一个语料库是由![Latent
    Dirichlet allocation](img/image00515.jpeg)表示的*M*个文档的集合（注意，在这里文档用粗体**w**表示，而单词则没有粗体w）。
- en: 'As mentioned earlier, LDA is a generative probabilistic model of a corpus where
    documents are represented as random mixtures over latent topics and each topic
    is characterized by a distribution over words. To generate each document **w**
    in a corpus in an LDA model, the following steps are performed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LDA是一个语料库的生成概率模型，其中文档被表示为潜在主题上的随机混合，每个主题由单词分布表征。在LDA模型中生成语料库中的每个文档**w**时，执行以下步骤：
- en: Choose the value of *N* corresponding to the size of the document, according
    to a Poisson distribution characterized by parameter ![Latent Dirichlet allocation](img/image00516.jpeg):![Latent
    Dirichlet allocation](img/image00517.jpeg)
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据参数![Latent Dirichlet allocation](img/image00516.jpeg)定义的泊松分布选择与文档大小相对应的*N*值：![Latent
    Dirichlet allocation](img/image00517.jpeg)
- en: Choose the value of parameter ![Latent Dirichlet allocation](img/image00518.jpeg)
    that characterizes the topic distribution from a Dirichlet distribution characterized
    by parameter ![Latent Dirichlet allocation](img/image00519.jpeg):![Latent Dirichlet
    allocation](img/image00520.jpeg)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择参数![Latent Dirichlet allocation](img/image00518.jpeg)的值，该参数表征了由参数![Latent
    Dirichlet allocation](img/image00519.jpeg)定义的狄利克雷分布的主题分布：![Latent Dirichlet allocation](img/image00520.jpeg)
- en: For each of the *N* words ![Latent Dirichlet allocation](img/image00521.jpeg)
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个*N*个单词![Latent Dirichlet allocation](img/image00521.jpeg)
- en: Choose a topic ![Latent Dirichlet allocation](img/image00522.jpeg) according
    to the multinomial distribution characterized by the parameter ![Latent Dirichlet
    allocation](img/image00518.jpeg) drawn in step 2:![Latent Dirichlet allocation](img/image00523.jpeg)
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据步骤2中绘制的参数![Latent Dirichlet allocation](img/image00518.jpeg)定义的多项式分布选择一个主题![Latent
    Dirichlet allocation](img/image00522.jpeg)：![Latent Dirichlet allocation](img/image00523.jpeg)
- en: 'Choose a word ![Latent Dirichlet allocation](img/image00521.jpeg) from the
    multinomial probability distribution characterized by ![Latent Dirichlet allocation](img/image00524.jpeg)
    and conditioned on ![Latent Dirichlet allocation](img/image00522.jpeg):'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从由 ![潜在狄利克雷分配](img/image00524.jpeg) 描述并由 ![潜在狄利克雷分配](img/image00522.jpeg) 条件化的多项式概率分布中选择一个单词
    ![潜在狄利克雷分配](img/image00521.jpeg)：
- en: '![Latent Dirichlet allocation](img/image00525.jpeg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/image00525.jpeg)'
- en: 'Given values of *N*, ![Latent Dirichlet allocation](img/image00519.jpeg), and
    ![Latent Dirichlet allocation](img/image00524.jpeg), the joint distribution of
    a topic mixture ![Latent Dirichlet allocation](img/image00518.jpeg), set of topics
    **z**, and set of words **w**, is given by:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *N*、![潜在狄利克雷分配](img/image00519.jpeg) 和 ![潜在狄利克雷分配](img/image00524.jpeg)，主题混合
    ![潜在狄利克雷分配](img/image00518.jpeg)、主题集合 **z** 和单词集合 **w** 的联合分布如下：
- en: '![Latent Dirichlet allocation](img/image00526.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/image00526.jpeg)'
- en: Note that, in this case, only *w* is observed (the documents) and both ![Latent
    Dirichlet allocation](img/image00518.jpeg) and **z** are treated as latent (hidden)
    variables.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，只有 *w* 是可观察的（文档），而 ![潜在狄利克雷分配](img/image00518.jpeg) 和 **z** 都被视为潜在（隐藏）变量。
- en: 'The Bayesian inference problem in LDA is the estimation of the posterior density
    of latent variables ![Latent Dirichlet allocation](img/image00518.jpeg) and **z**,
    given a document given by:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LDA中的贝叶斯推断问题是给定一个文档，估计潜在变量 ![潜在狄利克雷分配](img/image00518.jpeg) 和 **z** 的后验密度：
- en: '![Latent Dirichlet allocation](img/image00527.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/image00527.jpeg)'
- en: As usual, with many Bayesian models, this is intractable analytically and one
    has to use approximate techniques, such as MCMC or variational Bayes, to estimate
    the posterior.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如同许多贝叶斯模型一样，这种情况在分析上难以处理，因此必须使用近似技术，如MCMC或变分贝叶斯，来估计后验。
- en: R packages for LDA
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LDA的R包
- en: There are mainly two packages in R that can be used for performing LDA on documents.
    One is the **topicmodels** package developed by Bettina Grün and Kurt Hornik and
    the second one is **lda** developed by Jonathan Chang. Here, we describe both
    these packages.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: R中有两个主要包可以用于在文档上执行LDA。一个是Bettina Grün和Kurt Hornik开发的**topicmodels**包，另一个是Jonathan
    Chang开发的**lda**包。在这里，我们将描述这两个包。
- en: The topicmodels package
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题模型包
- en: 'The topicmodels package is an interface to the C and C++ codes developed by
    the authors of the papers on LDA and **Correlated Topic Models** (**CTM**) (references
    7, 8, and 9 in the *References* section of this chapter). The main function `LDA`
    in this package is used to fit LDA models. It can be called by:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型包是LDA和**相关主题模型**（**CTM**）论文作者开发的C和C++代码的接口（本章参考文献部分的7、8和9）。该包中的主要函数 `LDA`
    用于拟合LDA模型。可以通过以下方式调用：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, *X* is a document-term matrix that can be generated using the **tm**
    package and *K* is the number of topics. The `method` is the method to be used
    for fitting. There are two methods that are supported: `Gibbs` and `VEM`.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X* 是一个文档-词矩阵，可以使用 **tm** 包生成，而 *K* 是主题的数量。`method` 是用于拟合的方法。支持两种方法：`Gibbs`
    和 `VEM`。
- en: 'Let''s do a small example of building LDA models using this package. The dataset
    used is the **Reuter_50_50** dataset from the UCI Machine Learning repository
    (references 10 and 11 in the *References* section of this chapter). The dataset
    can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).
    For this exercise, we will only use documents from one directory, namely `AlanCrosby`
    in the `C50train` directory. The required preprocessing can be done using the
    following R script; readers should have installed the tm and topicmodels packages
    before trying this exercise:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过这个包构建LDA模型的小例子。所使用的数据集是来自UCI机器学习仓库的**Reuter_50_50**数据集（本章参考文献部分的10和11）。数据集可以从[https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50)下载。对于这个练习，我们只将使用一个目录中的文档，即在`C50train`目录下的`AlanCrosby`。所需的预处理可以使用以下R脚本来完成；在尝试此练习之前，读者应已安装tm和topicmodels包：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The same set of steps can be used to create the test dataset from the `/…/C50/C50test/`
    directory.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用相同的步骤从 `/…/C50/C50test/` 目录创建测试数据集。
- en: 'Once we have the document-term matrices `xtrain` and `xtest`, the LDA model
    can be built and tested using the following R script:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了文档-词矩阵 `xtrain` 和 `xtest`，可以使用以下R脚本来构建和测试LDA模型：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A value of perplexity around 100 indicates a good fit. In this case, we need
    to add more training data or change the value of *K* to improve perplexity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 大约100的困惑度值表示拟合良好。在这种情况下，我们需要添加更多训练数据或更改*K*的值以提高困惑度。
- en: 'Now let''s use the trained LDA model to predict the topics on the test dataset:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用训练好的LDA模型来预测测试数据集上的主题：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![The topicmodels package](img/image00528.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![The topicmodels package](img/image00528.jpeg)'
- en: Here, the test set contains only one file, namely `42764newsML.txt`. The distribution
    of its topic among the 10 topics produced by the LDA model is shown.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，测试集只包含一个文件，即 `42764newsML.txt`。它在其由LDA模型产生的10个主题中的分布情况如下所示。
- en: The lda package
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: lda包
- en: The lda package was developed by Jonathan Chang and he implemented a collapsed
    Gibbs sampling method for the estimation of posterior. The package can be downloaded
    from the CRAN website at [http://cran.r-project.org/web/packages/lda/index.html](http://cran.r-project.org/web/packages/lda/index.html).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: lda包是由Jonathan Chang开发的，他为后验估计实现了折叠吉布斯抽样方法。该包可以从CRAN网站下载，网址为 [http://cran.r-project.org/web/packages/lda/index.html](http://cran.r-project.org/web/packages/lda/index.html)。
- en: 'The main function in the package, `lda.collapsed.gibbs.sampler`, uses a collapsed
    Gibbs sampler to fit three different models. These are **Latent Dirichlet allocation**
    (**LDA**), **supervised LDA** (**sLDA**), and the **mixed membership stochastic
    blockmodel** (**MMSB**). These functions take input documents and return point
    estimates of latent parameters. These functions can be used in R as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 包中的主要函数 `lda.collapsed.gibbs.sampler` 使用折叠吉布斯抽样的方法来拟合三个不同的模型。这些是**潜在狄利克雷分配**（**LDA**）、**监督LDA**（**sLDA**）和**混合成员随机块模型**（**MMSB**）。这些函数接受输入文档并返回潜在参数的点估计。这些函数可以在R中使用如下：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, `documents` represents a list containing documents, the length of the
    list is equal to `D`, and `K` is the number of topics; `vocab` is a character
    vector specifying the vocabulary of words; `alpha` and `eta` are the values of
    hyperparameters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`documents` 代表一个包含文档的列表，列表的长度等于 `D`，而 `K` 是主题的数量；`vocab` 是一个指定词汇的字符向量；`alpha`
    和 `eta` 是超参数的值。
- en: Exercises
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: For the Reuter_50_50 dataset, fit the LDA model using the `lda.collapsed.gibbs.sampler`
    function in the lda package and compare performance with that of the topicmodels
    package. Note that you need to convert the document-term matrix to lda format
    using the `dtm2ldaformat( )` function in the topicmodels package in order to use
    the lda package.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于Reuter_50_50数据集，使用lda包中的 `lda.collapsed.gibbs.sampler` 函数拟合LDA模型，并与topicmodels包的性能进行比较。请注意，您需要使用topicmodels包中的
    `dtm2ldaformat( )` 函数将文档-词矩阵转换为lda格式，以便使用lda包。
- en: References
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bouwmans, T., El Baf F., and "Vachon B. Background Modeling Using Mixture of
    Gaussians for Foreground Detection – A Survey" (PDF). Recent Patents on Computer
    Science 1: 219-237\. 2008'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bouwmans, T., El Baf F. 和 "Vachon B. 使用高斯混合模型进行背景建模以检测前景 – 一篇综述" (PDF). 近期计算机科学专利
    1: 219-237. 2008'
- en: Bishop C.M. *Pattern Recognition and Machine Learning*. Springer. 2006
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bishop C.M. *模式识别与机器学习*. Springer. 2006
- en: 'Biecek P., Szczurek E., Tiuryn J., and Vingron M. "The R Package bgmm: Mixture
    Modeling with Uncertain Knowledge". Journal of Statistical Software. Volume 47,
    Issue 3\. 2012'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Biecek P., Szczurek E., Tiuryn J., 和 Vingron M. "R包bgmm：不确定知识混合建模". 统计软件杂志.
    第47卷，第3期. 2012
- en: 'Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T., and Zaccaria R. "Analysis
    of human behavior recognition algorithms based on acceleration data". In: IEEE
    Int Conf on Robotics and Automation (ICRA), pp. 1602-1607\. 2013'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T. 和 Zaccaria R. "基于加速度数据的人行行为识别算法分析".
    在：IEEE国际机器人与自动化会议（ICRA），第1602-1607页. 2013
- en: 'Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T., and Zaccaria R. "Human
    Motion Modeling and Recognition: A computational approach". In: IEEE International
    Conference on Automation Science and Engineering (CASE). pp 156-161\. 2012'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T. 和 Zaccaria R. "人体运动建模与识别：一种计算方法".
    在：IEEE国际自动化科学和工程会议（CASE），第156-161页. 2012
- en: 'Hofmann T. "Probabilistic Latent Semantic Indexing". In: Twenty-Second Annual
    International SIGIR Conference. 1999'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hofmann T. "概率潜在语义索引". 在：第二十二届国际SIGIR会议. 1999
- en: Blei D.M., Jordan M.I., and Ng A.Y. "Latent Dirichlet Allocation". Journal of
    Machine Learning Research 3\. 993-1022\. 2003
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Blei D.M., Jordan M.I., 和 Ng A.Y. "潜在狄利克雷分配". 机器学习研究杂志 3. 993-1022. 2003
- en: Blei D.M., and Lafferty J.D. "A Correlated Topic Model of Science". The Annals
    of Applied Statistics. 1(1), 17-35\. 2007
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Blei D.M. 和 Lafferty J.D. "科学相关主题模型"。应用统计学年鉴。1(1)，17-35。2007
- en: 'Phan X.H., Nguyen L.M., and Horguchi S. "Learning to Classify Short and Sparse
    Text & Web with Hidden Topics from Large-scale Data Collections". In: 17th International
    World Wide Web Conference (WWW 2008). pages 91-100\. Beijing, China. 2008'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Phan X.H.，Nguyen L.M. 和 Horguchi S. "从大规模数据集中学习对短文本和网页进行分类，并使用隐藏主题"。在第17届国际万维网会议（WWW
    2008）中。第91-100页。北京，中国。2008
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed the concepts behind unsupervised and semi-supervised
    machine learning, and their Bayesian treatment. We learned two important Bayesian
    unsupervised models: the Bayesian mixture model and LDA. We discussed in detail
    the bgmm package for the Bayesian mixture model, and the topicmodels and lda packages
    for topic modeling. Since the subject of unsupervised learning is vast, we could
    only cover a few Bayesian methods in this chapter, just to give a flavor of the
    subject. We have not covered semi-supervised methods using both item labeling
    and feature labeling. Interested readers should refer to more specialized books
    in this subject. In the next chapter, we will learn another important class of
    models, namely neural networks.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了无监督和半监督机器学习背后的概念及其贝叶斯处理方法。我们学习了两个重要的贝叶斯无监督模型：贝叶斯混合模型和LDA。我们详细讨论了贝叶斯混合模型的bgmm包，以及用于主题建模的topicmodels和lda包。由于无监督学习的主题非常广泛，我们只能在本章中涵盖一些贝叶斯方法，仅为了给读者一个对该主题的初步了解。我们没有涵盖使用项目标注和特征标注的半监督方法。对此感兴趣的读者应参考该主题的更专业书籍。在下一章中，我们将学习另一类重要的模型，即神经网络。
