- en: Chapter 7. Bayesian Models for Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The machine learning models that we have discussed so far in the previous two
    chapters share one common characteristic: they require training data containing
    ground truth. This implies a dataset containing true values of the predicate or
    dependent variable that is often manually labeled. Such machine learning where
    the algorithm is trained using labeled data is called **supervised learning**.
    This type of machine learning gives a very good performance in terms of accuracy
    of prediction. It is, in fact, the de facto method used in most industrial systems
    using machine learning. However, the drawback of this method is that, when one
    wants to train a model with large datasets, it would be difficult to get the labeled
    data. This is particularly relevant in the era of Big Data as a lot of data is
    available for organizations from various logs, transactions, and interactions
    with consumers; organizations want to gain insight from this data and make predictions
    about their consumers'' interests.'
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised methods, no labeled data is required for learning. The process
    of learning happens through identifying dominant patterns and correlations present
    in the dataset. Some common examples of unsupervised learning are clustering,
    association rule mining, density estimation, and dimensional reduction. In clustering,
    naturally occurring groups in data are identified using a suitable algorithm that
    makes use of some distance measure between data points. In association rule mining,
    items that frequently occur together in a transaction are identified from a transaction
    dataset. In dimensional reduction techniques such as principal component analysis,
    the original dataset containing a large number of variables (dimensions) is projected
    down to a lower dimensional space where the maximum information in the data is
    present. Though unsupervised learning doesn't require labeled training data, one
    would need a large amount of data to learn all the patterns of interest and often
    the learning is more computationally intensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many practical cases, it would be feasible to create a small amount of labeled
    data. The third type of learning, semi-supervised learning, is a method that makes
    use of this small labeled dataset and propagates labels to the rest of the unlabeled
    training data using suitable algorithms. In this chapter, we will cover Bayesian
    approaches for unsupervised learnings. We will discuss in detail two important
    models: Gaussian mixture models for clustering and Latent Dirichlet allocation
    for topic modeling.'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian mixture models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In general, a mixture model corresponds to representing data using a mixture
    of probability distributions. The most common mixture model is of the following
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00486.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Bayesian mixture models](img/image00487.jpeg) is a probability distribution
    of *X* with parameters ![Bayesian mixture models](img/image00488.jpeg), and ![Bayesian
    mixture models](img/image00489.jpeg) represents the weight for the *k*^(*th*)
    component in the mixture, such that ![Bayesian mixture models](img/image00490.jpeg).
    If the underlying probability distribution is a normal (Gaussian) distribution,
    then the mixture model is called a **Gaussian mixture model** (**GMM**). The mathematical
    representation of GMM, therefore, is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00491.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have used the same notation, as in previous chapters, where *X* stands
    for an *N*-dimensional data vector ![Bayesian mixture models](img/image00492.jpeg)
    representing each observation and there are *M* such observations in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A mixture model such as this is suitable for clustering when the clusters have
    overlaps. One of the applications of GMM is in computer vision. If one wants to
    track moving objects in a video, it is useful to subtract the background image.
    This is called background subtraction or foreground detection. GMMs are used for
    this purpose where the intensity of each pixel is modeled using a mixture of Gaussian
    distributions (reference 1 in the *References* section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'The task of learning GMMs corresponds to learning the model parameters ![Bayesian
    mixture models](img/image00493.jpeg) and mixture weights ![Bayesian mixture models](img/image00489.jpeg)
    for all the components ![Bayesian mixture models](img/image00494.jpeg). The standard
    approach for learning GMMs is by using the **maximum likelihood** method. For
    a dataset consisting of *M* observations, the logarithm of the likelihood function
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00495.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike a single Gaussian model, maximizing the log-likelihood with respect
    to parameters ![Bayesian mixture models](img/image00496.jpeg) cannot be done in
    a straightforward manner in GMM. This is because there is no closed-form expression
    for the derivative in this case, since it is difficult to compute the logarithm
    of a sum. Therefore, one uses what is called an **expectation-maximization** (**EM**)
    algorithm to maximize the log-likelihood function. The EM algorithm is an iterative
    algorithm, where each iteration consists of two computations: expectation and
    maximization. The EM algorithm proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize parameters ![Bayesian mixture models](img/image00497.jpeg), ![Bayesian
    mixture models](img/image00498.jpeg), and ![Bayesian mixture models](img/image00489.jpeg)
    and evaluate the initial value of log-likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the expectation step, evaluate mixture components ![Bayesian mixture models](img/image00489.jpeg)
    from log-likelihood using the current parameter values ![Bayesian mixture models](img/image00497.jpeg)
    and ![Bayesian mixture models](img/image00498.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the maximization step, using the values of ![Bayesian mixture models](img/image00489.jpeg)
    computed in step 2, estimate new parameter values ![Bayesian mixture models](img/image00497.jpeg)
    and ![Bayesian mixture models](img/image00498.jpeg) by the maximization of log-likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute a new value of the log-likelihood function using the estimated values
    of ![Bayesian mixture models](img/image00489.jpeg), ![Bayesian mixture models](img/image00497.jpeg),
    and ![Bayesian mixture models](img/image00498.jpeg) from steps 2 and 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2-4 until the log-likelihood function is converged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Bayesian treatment of GMM, the maximization of log-likelihood is simplified
    by introducing a latent variable *Z*. Let *Z* be a *K*-dimensional binary random
    variable having only one element *1*, and the rest of the *K – 1* elements are
    *0*. Using *Z*, one can write the joint distribution of *X* and *Z* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00499.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00500.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00501.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00502.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian mixture models](img/image00503.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The advantage of introducing a latent variable *Z* in the problem is that the
    expression for log-likelihood is simplified, where the logarithm directly acts
    on the normal distribution as in the case of a single Gaussian model. Therefore,
    it is straightforward to maximize *P(X, Z)*. However, the problem that still remains
    is that we don''t know the value of *Z!* So, the trick is to use an EM-like iterative
    algorithm where, in the E-step, the expectation value of *Z* is estimated and
    in the M-step, using the last estimated value of *Z*, we find the parameter values
    of the Gaussian distribution. The Bayesian version of the EM algorithm for GMM
    proceeds as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize parameters ![Bayesian mixture models](img/image00497.jpeg), ![Bayesian
    mixture models](img/image00498.jpeg), and ![Bayesian mixture models](img/image00489.jpeg)
    and evaluate the initial value of log-likelihood.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the expectation step, use these values to compute the expectation value ![Bayesian
    mixture models](img/image00504.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the maximization step, using ![Bayesian mixture models](img/image00504.jpeg)
    fixed, estimate ![Bayesian mixture models](img/image00497.jpeg) and ![Bayesian
    mixture models](img/image00498.jpeg) by maximizing ![Bayesian mixture models](img/image00505.jpeg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the new likelihood function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2-4 until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A more detailed treatment of the Bayesian version of the EM algorithm and GMM
    can be found in the book by Christopher M. Bishop (reference 2 in the *References*
    section of this chapter). Here, we leave the theoretical treatment of the Bayesian
    GMM and proceed to look at its R implementation in the **bgmm** package.
  prefs: []
  type: TYPE_NORMAL
- en: The bgmm package for Bayesian mixture models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The bgmm package was developed by Przemyslaw Biecek and Ewa Szczurek for modeling
    gene expressions data (reference 3 in the *References* section of this chapter).
    It can be downloaded from the CRAN website at [http://cran.r-project.org/web/packages/bgmm/index.html](http://cran.r-project.org/web/packages/bgmm/index.html).
    The package contains not only an unsupervised version of GMM but fully supervised
    and semi-supervised implementations as well. The following are the different models
    available in the bgmm package:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fully supervised GMM**: This is the labeled data available for all records
    in a training set. This includes the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `supervised( )` function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-supervised GMM**: This is the labeled data available for a small subset
    of all records in a training set. This includes the following'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `semisupervised( )` function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partially supervised GMM**: This is the labeled data available for a small
    subset of all records, but these labels are uncertain. The values of labels are
    given with some probability. There are two functions in the package for partially
    supervised GMM. This includes the following::'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `belief( )` function: The uncertainty of labels is expressed as a probability
    distribution over its components. For the first *m* observations, a belief matrix
    *B* of dimensions *m x k* is given as input where the matrix entry ![The bgmm
    package for Bayesian mixture models](img/image00506.jpeg) denotes the probability
    that the *i*^(th) record has the *j*^(th) label.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `soft( )` function: In this approach, a plausibility matrix of dimension
    *M x k* is defined across all records in the training set of size *M*. The matrix
    element ![The bgmm package for Bayesian mixture models](img/image00507.jpeg) is
    interpreted as the weight of the prior probability that the *i*^(th) record has
    the *j*^(th) label. If there is no particular information about labels of any
    records, they can be given equal weights. For the purpose of implementation, a
    constraint is imposed on the matrix elements: ![The bgmm package for Bayesian
    mixture models](img/image00508.jpeg).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised GMM**: This labeled data is not available for any records. This
    includes the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `unsupervised( )` function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The typical parameters that are passed to these functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X*: This is a data.frame with the unlabelled *X* data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*knowns*: This is a data.frame with the labeled *X* data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*B*: This is a belief matrix that specifies the distribution of beliefs for
    the labeled records. The number of rows of *B* should be the same as that of *knowns*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*: This is a matrix of weights of prior probabilities (plausibilities).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*class*: This is a vector of classes or labels for the labeled records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k*: This is the number of components or columns of the *B* matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*init.params*: These are the initial values for the estimates of model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between the `belief( )` and `soft( )` functions is that, in the
    first case, the input is a matrix containing prior probability values for each
    possible label, whereas in the second case, the input is a matrix containing weights
    for each of the priors and not the prior probability itself. For more details,
    readers are requested to read the paper by Przemyslaw Biecek et.al (reference
    3 in the *References* section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's do a small illustrative example of using bgmm. We will use the ADL
    dataset from the UCI Machine Learning repository. This dataset contains acceleration
    data from wrist-worn accelerometers from 16 volunteers. The dataset and metadata
    details can be found at [https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Dataset+for+ADL+Recognition+with+Wrist-worn+Accelerometer).
    The research work on ADL monitoring systems, where this dataset was generated,
    is published in the two papers by Bruno B. et.al. (reference 4 and reference 5
    in the *References* section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the example of bgmm, we will only use one folder in the dataset directory,
    namely `Brush_teeth`. Firstly, we will do a small amount of preprocessing to combine
    data from the different volunteers into a single file. The following R script
    does this job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The last step is to select the *X* and *Z* components of acceleration to create
    a two-dimensional dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following R script calls the `bgmm` function and performs clustering. A
    simple scatter plot of the data suggests that there could be four clusters in
    the dataset and choosing *k = 4* would be sufficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The clusters generated by bgmm can be seen in the following figure; there are
    four clusters whose centers are represented by the four color dots and their respective
    Gaussian densities are represented by the ellipses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The bgmm package for Bayesian mixture models](img/image00509.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Topic modeling using Bayesian inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the supervised learning (classification) of text documents in [Chapter
    6](part0049.xhtml#aid-1ENBI1 "Chapter 6. Bayesian Classification Models"), *Bayesian
    Classification Models*, using the Naïve Bayes model. Often, a large text document,
    such as a news article or a short story, can contain different topics as subsections.
    It is useful to model such intra-document statistical correlations for the purpose
    of classification, summarization, compression, and so on. The Gaussian mixture
    model learned in the previous section is more applicable for numerical data, such
    as images, and not for documents. This is because words in documents seldom follow
    normal distribution. A more appropriate choice would be multinomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: A powerful extension of mixture models to documents is the work of T. Hofmann
    on Probabilistic Semantic Indexing (reference 6 in the *References* section of
    this chapter) and that of David Blei, et. al. on Latent Dirichlet allocation (reference
    7 in the *References* section of this chapter). In these works, a document is
    described as a mixture of topics and each topic is described by a distribution
    of words. LDA is a generative unsupervised model for text documents. The task
    of LDA is to learn the parameters of the topic distribution, word distributions,
    and mixture coefficients from data. A brief overview of LDA is presented in the
    next section. Readers are strongly advised to read the paper by David Blei, et
    al. to comprehend their approach.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In LDA, it is assumed that words are the basic units of documents. A word is
    one element of a set known as vocabulary, indexed by ![Latent Dirichlet allocation](img/image00510.jpeg).
    Here, *V* denotes the size of the vocabulary. A word can be represented by a unit-basis
    vector, whose all components are zero except the one corresponding to the word
    that has a value 1\. For example, the *n*^(th) word in a vocabulary is described
    by a vector of size *V*, whose *n*^(th) component ![Latent Dirichlet allocation](img/image00511.jpeg)
    and all other components ![Latent Dirichlet allocation](img/image00512.jpeg) for
    ![Latent Dirichlet allocation](img/image00513.jpeg). Similarly, a document is
    a collection of *N* words denoted by ![Latent Dirichlet allocation](img/image00514.jpeg)
    and a corpus is a collection of *M* documents denoted by ![Latent Dirichlet allocation](img/image00515.jpeg)
    (note that documents are represented here by a bold face **w**, whereas words
    are without bold face w).
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, LDA is a generative probabilistic model of a corpus where
    documents are represented as random mixtures over latent topics and each topic
    is characterized by a distribution over words. To generate each document **w**
    in a corpus in an LDA model, the following steps are performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose the value of *N* corresponding to the size of the document, according
    to a Poisson distribution characterized by parameter ![Latent Dirichlet allocation](img/image00516.jpeg):![Latent
    Dirichlet allocation](img/image00517.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the value of parameter ![Latent Dirichlet allocation](img/image00518.jpeg)
    that characterizes the topic distribution from a Dirichlet distribution characterized
    by parameter ![Latent Dirichlet allocation](img/image00519.jpeg):![Latent Dirichlet
    allocation](img/image00520.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of the *N* words ![Latent Dirichlet allocation](img/image00521.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a topic ![Latent Dirichlet allocation](img/image00522.jpeg) according
    to the multinomial distribution characterized by the parameter ![Latent Dirichlet
    allocation](img/image00518.jpeg) drawn in step 2:![Latent Dirichlet allocation](img/image00523.jpeg)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Choose a word ![Latent Dirichlet allocation](img/image00521.jpeg) from the
    multinomial probability distribution characterized by ![Latent Dirichlet allocation](img/image00524.jpeg)
    and conditioned on ![Latent Dirichlet allocation](img/image00522.jpeg):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Latent Dirichlet allocation](img/image00525.jpeg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Given values of *N*, ![Latent Dirichlet allocation](img/image00519.jpeg), and
    ![Latent Dirichlet allocation](img/image00524.jpeg), the joint distribution of
    a topic mixture ![Latent Dirichlet allocation](img/image00518.jpeg), set of topics
    **z**, and set of words **w**, is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Dirichlet allocation](img/image00526.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in this case, only *w* is observed (the documents) and both ![Latent
    Dirichlet allocation](img/image00518.jpeg) and **z** are treated as latent (hidden)
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bayesian inference problem in LDA is the estimation of the posterior density
    of latent variables ![Latent Dirichlet allocation](img/image00518.jpeg) and **z**,
    given a document given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Dirichlet allocation](img/image00527.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As usual, with many Bayesian models, this is intractable analytically and one
    has to use approximate techniques, such as MCMC or variational Bayes, to estimate
    the posterior.
  prefs: []
  type: TYPE_NORMAL
- en: R packages for LDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are mainly two packages in R that can be used for performing LDA on documents.
    One is the **topicmodels** package developed by Bettina Grün and Kurt Hornik and
    the second one is **lda** developed by Jonathan Chang. Here, we describe both
    these packages.
  prefs: []
  type: TYPE_NORMAL
- en: The topicmodels package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The topicmodels package is an interface to the C and C++ codes developed by
    the authors of the papers on LDA and **Correlated Topic Models** (**CTM**) (references
    7, 8, and 9 in the *References* section of this chapter). The main function `LDA`
    in this package is used to fit LDA models. It can be called by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, *X* is a document-term matrix that can be generated using the **tm**
    package and *K* is the number of topics. The `method` is the method to be used
    for fitting. There are two methods that are supported: `Gibbs` and `VEM`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do a small example of building LDA models using this package. The dataset
    used is the **Reuter_50_50** dataset from the UCI Machine Learning repository
    (references 10 and 11 in the *References* section of this chapter). The dataset
    can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Reuter_50_50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50).
    For this exercise, we will only use documents from one directory, namely `AlanCrosby`
    in the `C50train` directory. The required preprocessing can be done using the
    following R script; readers should have installed the tm and topicmodels packages
    before trying this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The same set of steps can be used to create the test dataset from the `/…/C50/C50test/`
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have the document-term matrices `xtrain` and `xtest`, the LDA model
    can be built and tested using the following R script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A value of perplexity around 100 indicates a good fit. In this case, we need
    to add more training data or change the value of *K* to improve perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s use the trained LDA model to predict the topics on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![The topicmodels package](img/image00528.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the test set contains only one file, namely `42764newsML.txt`. The distribution
    of its topic among the 10 topics produced by the LDA model is shown.
  prefs: []
  type: TYPE_NORMAL
- en: The lda package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lda package was developed by Jonathan Chang and he implemented a collapsed
    Gibbs sampling method for the estimation of posterior. The package can be downloaded
    from the CRAN website at [http://cran.r-project.org/web/packages/lda/index.html](http://cran.r-project.org/web/packages/lda/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main function in the package, `lda.collapsed.gibbs.sampler`, uses a collapsed
    Gibbs sampler to fit three different models. These are **Latent Dirichlet allocation**
    (**LDA**), **supervised LDA** (**sLDA**), and the **mixed membership stochastic
    blockmodel** (**MMSB**). These functions take input documents and return point
    estimates of latent parameters. These functions can be used in R as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, `documents` represents a list containing documents, the length of the
    list is equal to `D`, and `K` is the number of topics; `vocab` is a character
    vector specifying the vocabulary of words; `alpha` and `eta` are the values of
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the Reuter_50_50 dataset, fit the LDA model using the `lda.collapsed.gibbs.sampler`
    function in the lda package and compare performance with that of the topicmodels
    package. Note that you need to convert the document-term matrix to lda format
    using the `dtm2ldaformat( )` function in the topicmodels package in order to use
    the lda package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bouwmans, T., El Baf F., and "Vachon B. Background Modeling Using Mixture of
    Gaussians for Foreground Detection – A Survey" (PDF). Recent Patents on Computer
    Science 1: 219-237\. 2008'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bishop C.M. *Pattern Recognition and Machine Learning*. Springer. 2006
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Biecek P., Szczurek E., Tiuryn J., and Vingron M. "The R Package bgmm: Mixture
    Modeling with Uncertain Knowledge". Journal of Statistical Software. Volume 47,
    Issue 3\. 2012'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T., and Zaccaria R. "Analysis
    of human behavior recognition algorithms based on acceleration data". In: IEEE
    Int Conf on Robotics and Automation (ICRA), pp. 1602-1607\. 2013'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bruno B., Mastrogiovanni F., Sgorbissa A., Vernazza T., and Zaccaria R. "Human
    Motion Modeling and Recognition: A computational approach". In: IEEE International
    Conference on Automation Science and Engineering (CASE). pp 156-161\. 2012'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hofmann T. "Probabilistic Latent Semantic Indexing". In: Twenty-Second Annual
    International SIGIR Conference. 1999'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Blei D.M., Jordan M.I., and Ng A.Y. "Latent Dirichlet Allocation". Journal of
    Machine Learning Research 3\. 993-1022\. 2003
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Blei D.M., and Lafferty J.D. "A Correlated Topic Model of Science". The Annals
    of Applied Statistics. 1(1), 17-35\. 2007
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Phan X.H., Nguyen L.M., and Horguchi S. "Learning to Classify Short and Sparse
    Text & Web with Hidden Topics from Large-scale Data Collections". In: 17th International
    World Wide Web Conference (WWW 2008). pages 91-100\. Beijing, China. 2008'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the concepts behind unsupervised and semi-supervised
    machine learning, and their Bayesian treatment. We learned two important Bayesian
    unsupervised models: the Bayesian mixture model and LDA. We discussed in detail
    the bgmm package for the Bayesian mixture model, and the topicmodels and lda packages
    for topic modeling. Since the subject of unsupervised learning is vast, we could
    only cover a few Bayesian methods in this chapter, just to give a flavor of the
    subject. We have not covered semi-supervised methods using both item labeling
    and feature labeling. Interested readers should refer to more specialized books
    in this subject. In the next chapter, we will learn another important class of
    models, namely neural networks.'
  prefs: []
  type: TYPE_NORMAL
