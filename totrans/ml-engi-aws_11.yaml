- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Pipelines with SageMaker Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B18638_10.xhtml#_idTextAnchor215), *Machine Learning Pipelines
    with Kubeflow on Amazon EKS*, we used **Kubeflow**, **Kubernetes**, and **Amazon
    EKS** to build and run an end-to-end **machine learning** (**ML**) pipeline. Here,
    we were able to automate several steps in the ML process inside a running Kubernetes
    cluster. If you are wondering whether we can also build ML pipelines using the
    different features and capabilities of **SageMaker**, then the quick answer to
    that would be *YES*!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will use **SageMaker Pipelines** to build and run automated
    ML workflows. In addition to this, we will demonstrate how we can utilize **AWS
    Lambda** functions to deploy trained models to new (or existing) ML inference
    endpoints during pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into SageMaker Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running our first pipeline with SageMaker Pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Lambda functions for deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing our ML inference endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing the end-to-end ML pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommended strategies and best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After completing the hands-on solutions in this chapter, we should be equipped
    with the skills required to build more complex ML pipelines and workflows on AWS
    using the different capabilities of **Amazon SageMaker**!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, it is important that we have the following ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (preferably Chrome or Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account and the **SageMaker Studio** domain used in the previous
    chapters of this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A text editor (for example, **VS Code**) on your local machine that we will
    use for storing and copying string values for later use in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Jupyter notebooks, source code, and other files used for each chapter are
    available in the repository at [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended that you use an IAM user with limited permissions instead
    of the root account when running the examples in this book. If you are just starting
    out with using AWS, you can proceed with using the root account in the meantime.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into SageMaker Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, data science teams start by performing ML experiments and deployments
    manually. Once they need to standardize the workflow and enable **automated model
    retraining** to refresh the deployed models regularly, these teams would then
    start considering the use of ML pipelines to automate a portion of their work.
    In [*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and Debugging
    Solutions*, we learned how to use the **SageMaker Python SDK** to train an ML
    model. Generally, training an ML model with the SageMaker Python SDK involves
    running a few lines of code similar to what we have in the following block of
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*What if we wanted to prepare an automated ML pipeline and include this as
    one of the steps?* You would be surprised that all we need to do is add a few
    lines of code to convert this into a step that can be included in a pipeline!
    To convert this into a step using `TrainingStep` object similar to what we have
    in the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Wow! Isn’t that amazing?* This would mean that existing notebooks using the
    **SageMaker Python SDK** for manually training and deploying ML models can easily
    be converted into using SageMaker Pipelines using a few additional lines of code!
    *What about the other steps?* We have the following classes, as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ProcessingStep` – This is for processing data using **SageMaker Processing**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TuningStep` – This is for creating a hyperparameter tuning job using the **Automatic
    Model Tuning** capability of SageMaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModelStep` – This is for creating and registering a SageMaker model to the
    **SageMaker Model Registry**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TransformStep` – This is for running inference on a dataset using the **Batch
    Transform** capability of SageMaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ConditionStep` – This is for the conditional branching support of the execution
    of pipeline steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CallbackStep` – This is for incorporating custom steps not directly available
    or supported in SageMaker Pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LambdaStep` – This is for running an **AWS Lambda** function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is not an exhaustive list of steps as there are other steps that
    can be used for more specific use cases. You can find the complete list of **SageMaker
    Pipeline Steps** at [https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management
    on AWS*, we stored and queried our data inside a Redshift cluster and in an Athena
    table. If we need to directly query data from these data sources, we can use `ProcessingStep`
    object, which will be added later on to the pipeline. Once the processing job
    completes, it stores the output files in S3, which can then be picked up and processed
    by a training job or an automatic model tuning job. If we need to convert this
    into a step, we can create a corresponding `TrainingStep` object (if we will be
    running a training job) or a `TuningStep` object (if we will be running an automatic
    model tuning job), which would then be added later to the pipeline. *What happens
    after the training (or tuning) job completes?* We have the option to store the
    resulting model inside the `ModelStep` object that would then be added later to
    the pipeline, too. Let’s refer to *Figure 11.1* to help us visualize how this
    all works once we’ve prepared the different steps of the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Using SageMaker Pipelines ](img/B18638_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Using SageMaker Pipelines
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11.1*, we can see that `Pipeline` object, which maps to the ML pipeline
    definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to run the pipeline, all we need to do is call the `start()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once the pipeline starts, we would have to wait for all steps to finish executing
    (one step at a time) or for the pipeline to stop if an error occurs in one of
    the steps. To debug and troubleshoot running pipelines, we can easily navigate
    to the **SageMaker Resources** pane of **SageMaker Studio** and locate the corresponding
    pipeline resource. We should see a diagram corresponding to the pipeline execution
    that is similar to what we have in *Figure 11.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Pipeline execution ](img/B18638_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Pipeline execution
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that all steps in the pipeline have been completed successfully,
    and the model we trained has been registered to the SageMaker Model Registry,
    too. If we wish to run the pipeline again (for example, using a different input
    dataset), we can simply trigger another pipeline execution and pass a different
    pipeline parameter value that points to where the new input dataset is stored.
    *Pretty cool, huh?* In addition to this, we can also dive deeper into what’s happening
    (or what happened) in each of the steps by clicking on the corresponding rounded
    rectangle of the step we wish to check, and then reviewing the input parameters,
    the output values, the ML metric values, the hyperparameters used to train the
    model, and the logs generated during the execution of the step. This allows us
    to understand what’s happening during the execution of the pipeline and troubleshoot
    issues when errors are encountered in the middle of a pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we’ve been talking about a relatively simple pipeline involving three
    or four steps executed sequentially. Additionally, **SageMaker Pipelines** allows
    us to build more complex ML pipelines that utilize conditional steps similar to
    what we have in *Figure 11.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18638_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – An ML pipeline with a conditional step
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, using a `ConditionStep`, the pipeline checks whether an ML inference
    endpoint exists already (given the endpoint name) and performs one of the following
    steps depending on the existence of the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deploy model to a new endpoint* – Using `LambdaStep`, which maps to an **AWS
    Lambda** function that deploys the ML model to a new ML inference endpoint if
    the endpoint does not exist yet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deploy model to an existing endpoint* – Using a `LambdaStep`, which maps to
    an **AWS Lambda** function that deploys the ML model to an existing ML inference
    endpoint if the endpoint already exists (with **zero downtime deployment**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cool right?* *What’s cooler is that this is the pipeline we will build in
    this chapter!* Building an ML pipeline might seem intimidating at first. However,
    as long as we iteratively build and test the pipeline and use the right set of
    tools, we should be able to come up with the ML pipeline we need to automate the
    manual processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of how **SageMaker Pipelines** works,
    let’s proceed with the hands-on portion of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering why we should use **SageMaker Pipelines**
    instead of **Kubeflow** and **Kubernetes**. One of the major differences between
    SageMaker Pipelines and Kubeflow is that the instances used to train ML models
    in SageMaker automatically get deleted after the training step completes. This
    helps reduce the overall cost since these training instances are only expected
    to run when models need to be trained. On the other hand, the infrastructure required
    by Kubeflow needs to be up and running before any of the training steps can proceed.
    Note that this is just one of the differences, and there are other things to consider
    when choosing the “right” tool for the job. Of course, there are scenarios where
    a data science team would choose Kubeflow instead since the members are already
    comfortable with the usage of Kubernetes (or they are running production Kubernetes
    workloads already). To help you and your team assess these tools properly, I would
    recommend that, first, you try building sample ML pipelines using both of these
    options.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will ensure that the following prerequisites are ready:'
  prefs: []
  type: TYPE_NORMAL
- en: The SageMaker Studio Domain execution role with the `AWSLambda_FullAccess` AWS
    managed permission policy attached to it – This will allow the Lambda functions
    to run without issues in the *Completing the end-to-end ML pipeline* section of
    this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IAM role (`pipeline-lambda-role`) – This will be used to run the Lambda
    functions in the *Creating Lambda Functions for Deployment* section of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `processing.py` file – This will be used by the **SageMaker Processing**
    job to process the input data and split it into training, validation, and test
    sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `bookings.all.csv` file – This will be used as the input dataset for the
    ML pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will create and manage our resources in the `us-west-2`)
    region. Make sure that you have set the correct region before proceeding with
    the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Preparing these essential prerequisites is critical to ensure that we won’t
    encounter unexpected blockers while preparing and running the ML pipelines in
    this chapter. That said, let’s proceed with preparing the prerequisites in the
    next set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by navigating to the `sagemaker studio` in the search bar of the
    AWS Management Console, hovering over the search result box for **Amazon SageMaker**,
    and then clicking on the **SageMaker Studio** link under **Top features**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the SageMaker Studio **Control Panel** page, locate the **Execution role**
    section attached to the **Domain** box (as highlighted in *Figure 11.4*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Copying the Execution role name ](img/B18638_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Copying the Execution role name
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate and copy the following values into a text editor on your local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '`arn:aws:iam::<ACCOUNT ID>:role/service-role/`). The execution role name might
    follow the `AmazonSageMaker-ExecutionRole-<DATETIME>` format similar to what we
    have in *Figure 11.4*. Make sure that you exclude `arn:aws:iam::<ACCOUNT ID>:role/service-role/`
    when copying the execution role name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`arn:aws:iam::<ACCOUNT ID>:role/service-role/`). The execution role ARN should
    follow the `arn:aws:iam::<ACCOUNT ID>:role/service-role/AmazonSageMaker-ExecutionRole-<DATETIME>`
    format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will use the Execution role ARN when testing the Lambda functions in the
    *Creating Lambda functions for deployment* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the `iam` into the search bar of the AWS Management Console, hovering
    over the search result box for **IAM**, and then clicking on the **Roles** link
    under **Top features**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Roles** page, search and locate the execution role by typing the execution
    role name (which is copied to the text editor on your local machine) into the
    search box (as highlighted in *Figure 11.5*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Navigating to the specific role page ](img/B18638_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Navigating to the specific role page
  prefs: []
  type: TYPE_NORMAL
- en: This should filter the results and display a single row that is similar to what
    we have in *Figure 11.5*. Click on the link under the **Role name** column to
    navigate to the page where we can modify the permissions of the role.
  prefs: []
  type: TYPE_NORMAL
- en: Locate the **Permissions policies** table (inside the **Permissions** tab),
    and then click on **Add permissions** to open a drop-down menu of options. Select
    **Attach policies** from the list of available options. This should redirect us
    to the page where we can see the **Current permissions policies** section and
    attach additional policies under **Other permissions policies**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate the `AWSLambda_FullAccess` AWS managed permission policy using the search
    bar (under `AWSLambda_FullAccess` policy. After that, click on the **Attach policies**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should see the following success notification message after clicking on
    the **Attach policies** button, **Policy was successfully attached to role**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create the IAM role that we will use later when creating the Lambda
    functions. Navigate to the **Roles** page (using the sidebar) and then click on
    the **Create role** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Select trusted entity** page (*step 1*), perform the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Trusted entity type**, choose **AWS service** from the list of options
    available.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Under **Use case**, select **Lambda** under **Common use cases**.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Afterward, click on the **Next** button.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `AmazonSageMakerFullAccess` policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search and select the `AWSLambdaExecute` policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After toggling on the radio buttons for the two policies, click on the **Next**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `pipeline-lambda-role` under **Role name**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page, and then click on the **Create role**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following success notification message after clicking on
    the **Create role** button: **Role pipeline-lambda-role created**.'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the `sagemaker studio` into the search bar of the AWS Management
    Console and then clicking on the **SageMaker Studio** link under **Top features**
    (after hovering over the search result box for **Amazon SageMaker**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Launch app** and then select **Studio** from the list of drop-down
    options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This will redirect you to **SageMaker Studio**. Wait for a few seconds for the
    interface to load.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s proceed with creating the `CH11` folder where we will store the
    files relevant to our ML pipeline in this chapter. Right-click on the empty space
    in the **File Browser** sidebar pane to open a context menu that is similar to
    what is shown in *Figure 11.6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Creating a new folder ](img/B18638_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Creating a new folder
  prefs: []
  type: TYPE_NORMAL
- en: Select `CH11`. After that, navigate to the `CH11` directory by double-clicking
    on the corresponding folder name in the sidebar.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new notebook by clicking on the `.ipynb` file inside the `CH11` directory
    where we can run our Python code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `Data Science` (option found under Sagemaker image)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Python 3`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`No script`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, click on the **Select** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the kernel to start. This step could take around 3–5 minutes while
    an ML instance is being provisioned to run the Jupyter notebook cells. Make sure
    that you stop this instance after finishing all the hands-on solutions in this
    chapter (or if you’re not using it). For more information, feel free to check
    the *Cleaning up* section near the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Right-click on the tab name and then select `Machine Learning Pipelines with
    SageMaker Pipelines.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the first cell of the `Machine Learning Pipelines with SageMaker Pipelines.ipynb`
    notebook, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should download a `processing.py` file that does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loads the `dataset.all.csv` file and stores the data inside a DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs the **train-test split**, which would divide the DataFrame into three
    DataFrames (containing the training, validation, and test sets)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensures that the output directories have been created before saving the output
    CSV files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saves the DataFrames containing the training, validation, and test sets into
    their corresponding CSV files inside the output directories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to check the contents of the downloaded `processing.py` file. Additionally,
    you can find a copy of the `processing.py` script file at [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/processing.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/processing.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s use the `mkdir` command to create a `tmp` directory if it does
    not exist yet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, download the `bookings.all.csv` file using the `wget` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we download a clean(er) version of the synthetic `bookings.all.csv` file
    similar to what we used in [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction
    to ML Engineering on AWS*. However, this time, multiple data cleaning and transformation
    steps have been applied already to produce a higher quality model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify a unique S3 bucket name and prefix. Make sure that you replace the
    value of `<INSERT S3 BUCKET NAME HERE>` with a unique S3 bucket name before running
    the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You could use one of the S3 buckets created in the previous chapters and update
    the value of `s3_bucket` with the S3 bucket name. If you are planning to create
    and use a new S3 bucket, make sure that you update the value of `s3_bucket` with
    a name for a bucket that does not exist yet. After that, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that this command should only be executed if we are planning to create
    a new S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Copy the S3 bucket name to the text editor on your local machine. We will use
    this later in the *Testing our ML inference endpoint* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prepare the path where we will upload our CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s upload the `bookings.all.csv` file to the S3 bucket using the
    `aws s3 cp` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the CSV file gets renamed to `dataset.all.csv` file upon uploading it
    to the S3 bucket (since we specified this in the `source_path` variable).
  prefs: []
  type: TYPE_NORMAL
- en: With the prerequisites ready, we can now proceed with running our first pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: Running our first pipeline with SageMaker Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML Engineering
    on AWS*, we installed and used **AutoGluon** to train multiple ML models (with
    **AutoML**) inside an AWS Cloud9 environment. In addition to this, we performed
    the different steps of the ML process manually using a variety of tools and libraries.
    In this chapter, we will convert these manually executed steps into an automated
    pipeline so that all we need to do is provide an input dataset and the ML pipeline
    will do the rest of the work for us (and store the trained model in a model registry).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Instead of preparing a custom Docker container image to use AutoGluon for training
    ML models, we will use the built-in **AutoGluon-Tabular** algorithm instead. With
    a built-in algorithm available for use, all we need to worry about would be the
    hyperparameter values and the additional configuration parameters we will use
    to configure the training job.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, this section is divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Defining and preparing our first ML pipeline* – This is where we will define
    and prepare a pipeline with the following steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PrepareData` – This utilizes a **SageMaker Processing** job to process the
    input dataset and splits it into training, validation, and test sets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainModel` – This utilizes the **AutoGluon-Tabular** built-in algorithm to
    train a classification model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RegisterModel` – This registers the trained ML model to the **SageMaker Model
    Registry**.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Running our first ML pipeline* – This is where we will use the `start()` method
    to execute our pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these in mind, let’s start by preparing our ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and preparing our first ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first pipeline we will prepare would be a relatively simple pipeline with
    three steps—including the data preparation step, the model training step, and
    the model registration step. To help us visualize what our first ML pipeline using
    **SageMaker Pipelines** will look like, let’s quickly check *Figure 11.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Our first ML pipeline using SageMaker Pipelines ](img/B18638_11_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Our first ML pipeline using SageMaker Pipelines
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our pipeline accepts an input dataset and splits this
    dataset into training, validation, and test sets. Then, the training and validation
    sets are used to train an ML model, which then gets registered to the **SageMaker
    Model Registry**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a good idea of what our pipeline will look like, let’s run
    the following blocks of code in our `Machine Learning Pipelines with SageMaker
    Pipelines.ipynb` Jupyter notebook in the next set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the building blocks from `boto3` and `sagemaker`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the SageMaker execution role ARN inside the `role` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `get_execution_role()` function should return the ARN of the IAM role we
    modified in the *Preparing the essential prerequisites* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, let’s prepare the SageMaker `Session` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s initialize a `ParameterString` object that maps to the `Pipeline` parameter
    pointing to where the input dataset is stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s prepare the `ProcessingInput` object that contains the configuration
    of the input source of the `ProcessingOutput` object that maps to the configuration
    for the output results of the **SageMaker Processing** job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s initialize the `SKLearnProcessor` object along with the corresponding
    `ProcessingStep` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To help us visualize how we configured the `ProcessingStep` object, let’s quickly
    check *Figure 11.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Configuring and preparing the ProcessingStep ](img/B18638_11_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Configuring and preparing the ProcessingStep
  prefs: []
  type: TYPE_NORMAL
- en: Here, we initialized the `ProcessingStep` object using the configured `SKLearnProcessor`
    object along with the parameter values for the `inputs`, `outputs`, and `code`
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s prepare the `model_path` variable to point to where the model will
    be uploaded after the SageMaker training job has finished (when the ML pipeline
    is executed during a later step):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, let’s prepare the `model_id` variable to store the ID of the
    ML model we’ll use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s specify the region we are using inside `region_name`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `image_uris.retrieve()` to get the ECR container image URI of our training
    image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you are wondering what the value of `train_image_uri` is, it should have
    a string value equal (or similar to): `''763104351884.dkr.ecr.us-west-2.amazonaws.com/autogluon-training:0.4.0-cpu-py38''.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `script_uris.retrieve()` to get the script S3 URI associated with the model
    (given the values of `model_id`, `model_version`, and `script_scope`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that `train_source_uri` should have a string value equal (or similar) to
    `'s3://jumpstart-cache-prod-us-west-2/source-directory-tarballs/autogluon/transfer_learning/classification/v1.0.1/sourcedir.tar.gz'`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: What’s inside this `sourcedir.tar.gz` file? If the `script_scope` value used
    when calling `script_uris.retrieve()` is `"training"`, the `sourcedir.tar.gz`
    file should contain code that uses `autogluon.tabular.TabularPredictor` when training
    the ML model. Note that the contents of `sourcedir.tar.gz` change depending on
    the arguments specified when calling `script_uris.retrieve()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `model_uris.retrieve()` to get the model artifact S3 URI associated with
    the model (given the values of `model_id`, `model_version`, and `model_scope`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that `train_model_uri` should have a string value equal (or similar) to
    `'s3://jumpstart-cache-prod-us-west-2/autogluon-training/train-autogluon-classification-ensemble.tar.gz'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the values for `train_image_uri`, `train_source_uri`, `train_model_uri`,
    and `model_path` ready, we can now initialize the `Estimator` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `entry_point` value points to the `transfer_learning.py` script file
    stored inside `sourcedir.tar.gz` containing the relevant scripts for training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s use the `retrieve_default()` function to retrieve the default set
    of hyperparameters for our **AutoGluon** classification model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the `TrainingStep` object that uses the `Estimator` object as one of
    the parameter values during initialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, `s3_data` contains a `Properties` object that points to the path where
    the output files of the `s3_data` using `s3_data.__dict__`, we should get a dictionary
    similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'To help us visualize how we configured the `TrainingStep` object, let’s quickly
    check *Figure 11.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Configuring and preparing the TrainingStep object ](img/B18638_11_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Configuring and preparing the TrainingStep object
  prefs: []
  type: TYPE_NORMAL
- en: Here, we initialize the `TrainingStep` object using the configured `Estimator`
    object along with the parameter values for the `name` and `inputs` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use `image_uris.retrieve()` and `script_uris.retrieve()` to retrieve
    the container image URI and script URI for the deployment of AutoGluon classification
    models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `aws s3 cp` command to download the `sourcedir.tar.gz` file to the
    `tmp` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, upload the `sourcedir.tar.gz` file from the `tmp` directory to your S3
    bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s define the `random_string()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function should return a random alphanumeric string (with 6 characters).
  prefs: []
  type: TYPE_NORMAL
- en: 'With the values for `deploy_image_uri`, `updated_source_uri`, and `model_data`
    ready, we can now initialize the `Model` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `random_string()` function that we defined in the previous
    step for the name identifier of the `Model` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s prepare the `ModelStep` object that uses the output of `model.register()`
    during initialization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s initialize the `Pipeline` object using the different step objects
    we prepared in the previous steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s use the `upsert()` method to create our ML pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `upsert()` method can be used to update an existing ML pipeline,
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Now that our initial pipeline is ready, we can proceed with running the ML pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: Running our first ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the `Pipeline` object has been initialized and created, we can run it
    right away using the `start()` method, which is similar to what we have in the
    following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wish to override the default parameters of the pipeline inputs (for example,
    the input data used), we can specify parameter values when calling the `start()`
    method similar to what we have in the following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: Once the pipeline execution starts, we can then use `execution.wait()` to wait
    for the pipeline to finish running.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, let’s run our ML pipeline in the next set of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything ready, let’s run the (partial) ML pipeline using the `start()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use the `wait()` method to wait for the pipeline to complete before proceeding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This should take around 10–15 minutes to complete. Feel free to grab a cup of
    coffee or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following block of code to get the resulting model package ARN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should yield an ARN with a format similar to `arn:aws:sagemaker:us-west-2:<ACCOUNT
    ID>:model-package/autogluonmodelgroup/1`. Copy this value into your text editor.
    We will use this model package ARN later when testing our Lambda functions in
    the *Creating Lambda functions for deployment* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Locate and click on the triangle icon (**SageMaker resources**) near the bottom
    of the left-hand sidebar of SageMaker Studio (as highlighted in *Figure 11.10*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Opening the SageMaker resources pane  ](img/B18638_11_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Opening the SageMaker resources pane
  prefs: []
  type: TYPE_NORMAL
- en: This should open the **SageMaker resources** pane where we can view and inspect
    a variety of SageMaker resources.
  prefs: []
  type: TYPE_NORMAL
- en: Select **Pipelines** from the list of options available in the drop-down menu
    in the **SageMaker resources** pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, double-click on the row that maps to the `PARTIAL-PIPELINE` pipeline
    we just created. After that, double-click on the row that maps to the pipeline
    execution we triggered after calling `partial_pipeline.start()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the execution has finished, you should see a graph that is similar to
    what is shown in *Figure 11.11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Completed pipeline execution ](img/B18638_11_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Completed pipeline execution
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to click on the rounded rectangles to check the following details
    of each of the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input** – The input files, parameters, and configuration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output** – The output files and metrics (if any)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logs** – The generated logs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information** – Any additional information/metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigate back to the tab corresponding to the **Machine Learning Pipelines with
    SageMaker Pipelines.ipynb** notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s review the steps executed during the (partial) pipeline run using the
    `list_steps()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should return a list of dictionaries that map to the executed steps of
    the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: We are not yet done! At this point, we have only finished half of our ML pipeline.
    Make sure that you do not turn off the running apps and instances in SageMaker
    Studio, as we will be running more blocks of code inside the `Machine Learning
    Pipelines with SageMaker Pipelines.ipynb` notebook later to complete our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you need to take a break, you may turn off the running instances and apps
    (to manage costs), and then run all the cells again in the `Machine Learning Pipelines
    with SageMaker Pipelines.ipynb` notebook before working on the *Completing the
    end-to-end ML pipeline* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Lambda functions for deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our second (and more complete pipeline) will require a few additional resources
    to help us deploy our ML model. In this section, we will create the following
    Lambda functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`check-if-endpoint-exists` – This is a Lambda function that accepts the name
    of the ML inference endpoint as input and returns `True` if the endpoint exists
    already.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deploy-model-to-new-endpoint` – This is a Lambda function that accepts the
    model package ARN as input (along with the role and the endpoint name) and deploys
    the model into a new inference endpoint'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deploy-model-to-existing-endpoint` – This is a Lambda function that accepts
    the model package ARN as input (along with the role and the endpoint name) and
    deploys the model into an existing inference endpoint (by updating the deployed
    model inside the ML instance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use these functions later in the *Completing the end-to-end ML pipeline*
    section to deploy the ML model we will register in the SageMaker Model Registry
    (using `ModelStep`).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Lambda function for deploying a model to a new endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first **AWS Lambda** function we will create will be configured and programmed
    to deploy a model to a new endpoint. To help us visualize how our function will
    work, let’s quickly check *Figure 11.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Deploying a model to a new endpoint ](img/B18638_11_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Deploying a model to a new endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'This function will accept the following input parameters: an IAM role, the
    endpoint name, and the model package ARN. After receiving these input parameters,
    the function will create the corresponding set of resources needed to deploy the
    model (from the model package) to a new ML inference endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a Lambda function that we will use
    to deploy an ML model to a new inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the `lambda` in the search bar of the AWS Management Console, and
    then clicking on the **Lambda** link from the list of search results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will create and manage our resources in the `us-west-2`)
    region. Make sure that you have set the correct region before proceeding with
    the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Locate and click on the `deploy-model-to-new-endpoint`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Python 3.9`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Permissions** > **Change default execution role**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Use an existing role`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pipeline-lambda-role`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page and then click on the **Create function**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following success notification after clicking on the **Create
    function** button: **Successfully created the function deploy-model-to-new-endpoint**.You
    can now change its code and configuration. To invoke your function with a test
    event, choose **Test**.'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the `1024` MB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`15` min `0` sec'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, click on the **Save** button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the following link in another browser tab: [https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py).
    Copy the contents of the page into your clipboard using *Ctrl* + *A* and then
    *Ctrl* + *C* (or, alternatively, *CMD* + *A* and then *CMD* + *C* if you are using
    a Mac).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back in the browser tab showing the Lambda console, navigate to the `Untitled1`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the new tab (containing no code), paste the code copied to the clipboard.
    Open the `utils.py` as the **Filename** field value, and then click on **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the tab where we can modify the code inside `lambda_function.py`.
    Delete the boilerplate code currently stored inside `lambda_function.py` before
    proceeding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Type (or copy) the code blocks in the succeeding set of steps inside `lambda_function.py`.
    You can find a copy of the code for the Lambda function at [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-new-endpoint.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-new-endpoint.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lambda_function.py` file, import the functions we will need for deploying
    a trained ML model to a new ML inference endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define the `lambda_handler()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click on the **Deploy** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the **Test** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `test` under **Event name**, and then specify the following JSON value
    under **Event JSON**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that you replace the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<INSERT SAGEMAKER EXECUTION ROLE ARN>` – Replace this placeholder value with
    the `arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20220000T000000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<INSERT MODEL PACKAGE ARN>` – Replace this placeholder value with the `arn:aws:sagemaker:us-west-2:1234567890:model-package/autogluonmodelgroup/1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy this test event JSON value to the text editor on your local machine. We
    will use this test event JSON again later when testing our `deploy-model-to-existing-endpoint`
    Lambda function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, click on the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With everything ready, let’s click on the **Test** button. This should open
    a new tab that should show the execution results after a few minutes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step might take 5–15 minutes to complete. Feel free to grab a cup of coffee
    or tea!
  prefs: []
  type: TYPE_NORMAL
- en: While waiting, scroll up and locate the **Function overview** pane. Copy the
    **Function ARN** value to your text editor. We will use this **Function ARN**
    value later in the *Completing the end-to-end ML pipeline* section of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the `deploy-model-to-new-endpoint` Lambda function has finished running,
    we should have our ML model deployed already in an ML inference endpoint. Note
    that we are just testing the Lambda function, and we will delete the ML inference
    endpoint (launched by the `deploy-model-to-new-endpoint` Lambda function) in a
    later step before running the complete ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Lambda function for checking whether an endpoint exists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second **AWS Lambda** function we will create will be configured and programmed
    to check whether an endpoint exists already (given the endpoint name). To help
    us visualize how our function will work, let’s quickly check *Figure 11.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Check whether an endpoint exists already ](img/B18638_11_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Check whether an endpoint exists already
  prefs: []
  type: TYPE_NORMAL
- en: This function will accept one input parameter—the name of the ML inference endpoint.
    After receiving the input parameter, the function will use the `boto3` library
    to list all running endpoints in the region and check whether the name of one
    of these endpoints matches the input parameter value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a Lambda function that we will use
    to check whether an ML inference endpoint exists already:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new browser tab and navigate to the **Functions** page of the Lambda
    Management console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate and click on the **Create function** button (located in the upper-left
    corner of the **Functions** page), and then specify the following configuration
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Author from scratch**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`check-if-endpoint-exists`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Python 3.9`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permissions** > **Change default execution role**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use an existing role`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline-lambda-role`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page, and then click on the **Create function**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Type (or copy) the code blocks into the succeeding set of steps inside `lambda_function.py`.
    You can find a copy of the code for the Lambda function here at [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/check-if-endpoint-exists.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/check-if-endpoint-exists.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lambda_function.py` file, import `boto3` and initialize the client
    for the SageMaker service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s define the `endpoint_exists()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define the `lambda_handler()` function that makes use of the `endpoint_exists()`
    function to check whether an ML inference endpoint exists or not (given the endpoint
    name):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE246]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE247]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE248]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE249]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE250]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE251]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click on the **Deploy** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the `test` under **Event name** and then specify the following JSON
    value under **Event JSON**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE252]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE253]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE254]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Afterward, click on the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With everything ready, let’s click on the **Test** button. This should open
    a new tab that will show the execution results after a few seconds. We should
    get the following response value after testing the Lambda function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE256]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE257]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, scroll up and locate the **Function overview** pane. Copy the **Function
    ARN** value to your text editor. We will use this **Function ARN** value later
    in the *Completing the end-to-end ML pipeline* section of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have finished preparing and testing the `check-if-endpoint-exists`
    Lambda function, we can proceed with creating the last Lambda function (`deploy-model-to-existing-endpoint`).
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Lambda function for deploying a model to an existing endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The third **AWS Lambda** function we will create will be configured and programmed
    to deploy a model to an existing endpoint. To help us visualize how our function
    will work, let’s quickly check *Figure 11.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Deploying a model to an existing endpoint ](img/B18638_11_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Deploying a model to an existing endpoint
  prefs: []
  type: TYPE_NORMAL
- en: This function will accept three input parameters—an IAM role, the endpoint name,
    and the model package ARN. After receiving these input parameters, the function
    will perform the necessary steps to update the model deployed in an existing ML
    inference endpoint with the model from the model package provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a Lambda function that we will use
    to deploy an ML model to an existing inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new browser tab and navigate to the **Functions** page of the Lambda
    Management console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate and click on the **Create function** button (located in the upper-left
    corner of the **Functions** page), and then specify the following configuration
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Author from scratch**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deploy-model-to-existing-endpoint`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Python 3.9`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permissions** > **Change default execution role**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use an existing role`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipeline-lambda-role`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page and then click on the **Create function**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the `1024` MB
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`15` min `0` sec'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, click on the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the following link in another browser tab: [https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter11/utils.py).
    Copy the contents of the page into your clipboard using *Ctrl* + *A* and then
    *Ctrl* + *C* (or, alternatively, *CMD* + *A* and then *CMD* + *C* if you are using
    a Mac).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back in the browser tab showing the Lambda console, navigate to the `Untitled1`.
    In the new tab (containing no code), paste the code copied to the clipboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `utils.py` as the **Filename** field value and then click on **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the tab where we can modify the code inside `lambda_function.py`.
    Delete the boilerplate code currently stored inside `lambda_function.py` before
    proceeding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Type (or copy) the code blocks in the succeeding set of steps inside `lambda_function.py`.
    You can find a copy of the code for the Lambda function at [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-existing-endpoint.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter11/deploy-model-to-existing-endpoint.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lambda_function.py` file, import the functions we will need to update
    the deployed model of an existing endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE258]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE259]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE260]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE261]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE262]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE263]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE264]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE265]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define the `lambda_handler()` function using the following block
    of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE266]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE267]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE268]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE269]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE270]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE271]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE272]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE273]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE274]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE275]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE276]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE277]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE278]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE279]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE280]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE281]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE282]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE283]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE284]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE285]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE286]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE287]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE288]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE289]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE290]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click on the **Deploy** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the `test` under **Event name** and then specify the following JSON
    value under **Event JSON**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE292]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE293]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE294]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE295]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that you replace the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<INSERT SAGEMAKER EXECUTION ROLE ARN>` – Replace this placeholder value with
    the **Execution Role ARN** copied to your text editor in the *Preparing the essential
    prerequisites* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<INSERT MODEL PACKAGE ARN>` – Replace this placeholder value with the **model
    package ARN** copied to your text editor in the *Running our first pipeline with
    SageMaker Pipelines* section of this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, you can use the same test event JSON value that we copied to our
    text editor while testing our `deploy-model-to-new-endpoint` Lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, click on the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With everything ready, let’s click on the **Test** button. This should open
    a new tab that should show the execution results after a few minutes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take 5–15 minutes to complete. Feel free to grab a cup of coffee
    or tea!
  prefs: []
  type: TYPE_NORMAL
- en: While waiting, scroll up and locate the **Function overview** pane. Copy the
    **Function ARN** value to your text editor. We will use this **Function ARN**
    value later in the *Completing the end-to-end ML pipeline* section of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With all the Lambda functions ready, we can now proceed with testing our ML
    inference endpoint (before completing the end-to-end ML pipeline).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have 3 x `check-if-endpoint-exists` Lambda function,
    the `deploy-model-to-new-endpoint` Lambda function, and the `deploy-model-to-existing-endpoint`
    Lambda function. We will use these ARN values later in the *Completing the end-to-end
    ML pipeline* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our ML inference endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, we need to check whether the ML inference endpoint is working! In
    the next set of steps, we will download and run a Jupyter notebook (named `Test
    Endpoint and then Delete.ipynb`) that tests our ML inference endpoint using the
    test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by opening the following link in another browser tab: [https://bit.ly/3xyVAXz](https://bit.ly/3xyVAXz)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on any part of the page to open a context menu, and then choose
    `Test Endpoint then Delete.ipynb`, and then download it to the `Downloads` folder
    (or similar) on your local machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate back to your `CH11` folder similar to what we have in *Figure 11.15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.15 – Uploading the test endpoint and then the Delete.ipynb file
    ](img/B18638_11_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – Uploading the test endpoint and then the Delete.ipynb file
  prefs: []
  type: TYPE_NORMAL
- en: Click on the upload button (as highlighted in *Figure 11.15*), and then select
    the `Test Endpoint then Delete.ipynb` file that we downloaded in an earlier step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This should upload the `Test Endpoint then Delete.ipynb` notebook file from
    your local machine to the SageMaker Studio environment (in the `CH11` folder).
  prefs: []
  type: TYPE_NORMAL
- en: Double-click on the `Test Endpoint then Delete.ipynb` file in the **File tree**
    to open the notebook in the **Main work area** (which contains tabs of the open
    notebooks, files, and terminals).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the first cell with the name of the S3 bucket used in the `Machine Learning
    Pipelines with SageMaker Pipelines.ipynb` notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE296]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure to replace `<INSERT S3 BUCKET HERE>` with the S3 bucket name we copied
    to our text editor earlier in the *Preparing the essential prerequisites* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `Test Endpoint then Delete.ipynb` notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It should take around 1–2 minutes to run all the cells in the Jupyter notebook.
    Feel free to grab a cup of coffee or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the cells in the `Test Endpoint then Delete.ipynb` notebook have been
    executed, locate the cell containing the following block of code (along with the
    returned output):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE297]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE298]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify that the model got an accuracy score equal to or close to `0.88` (or
    88%).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the ML inference endpoint should be in a deleted state since
    the `Test``Endpoint then Delete.ipynb` Jupyter notebook also runs the `predictor.delete_endpoint()`
    line after computing for the ML model metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Completing the end-to-end ML pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build on top of the (partial) pipeline we prepared
    in the *Running our first pipeline with SageMaker Pipelines* section of this chapter.
    In addition to the steps and resources used to build our partial pipeline, we
    will also utilize the Lambda functions we created (in the *Creating Lambda functions
    for deployment* section) to complete our ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and preparing the complete ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second pipeline we will prepare would be slightly longer than the first
    pipeline. To help us visualize how our second ML pipeline using **SageMaker Pipelines**
    will look like, let’s quickly check *Figure 11.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Our second ML pipeline using SageMaker Pipelines ](img/B18638_11_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Our second ML pipeline using SageMaker Pipelines
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our pipeline accepts two input parameters—the input dataset
    and the endpoint name. When the pipeline runs, the input dataset is first split
    into training, validation, and test sets. The training and validation sets are
    then used to train an ML model, which then gets registered to the **SageMaker
    Model Registry**. After that, the pipeline checks whether an ML inference endpoint
    with the provided endpoint name exists already. If the endpoint does not exist
    yet, the model is deployed to a new endpoint. Otherwise, the model of an existing
    endpoint (with the provided endpoint name) is updated using the model trained
    during the pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a new ML pipeline using the steps
    and resources configured in the `Machine Learning Pipelines with SageMaker Pipelines.ipynb`
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the tab corresponding to the `Machine Learning Pipelines with
    SageMaker Pipelines.ipynb` notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will run the blocks of code in the succeeding set of steps inside the `Machine
    Learning Pipelines with SageMaker Pipelines.ipynb` notebook (after the existing
    set of cells). If you turned off the kernel and/or the SageMaker Studio instance
    after running the commands in the *Running our first pipeline with SageMaker Pipelines*
    section, make sure that you run all the cells again (and wait for the pipeline
    to finish running) by selecting **Run All Cells** from the list of options under
    the **Run** menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize the `ParameterString` object that maps to the `Pipeline` parameter
    for the name of the ML inference endpoint (which will be created or updated after
    the ML pipeline has finished running):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE299]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE300]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE301]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE302]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s import the classes we will need to complete the end-to-end ML pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE303]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE304]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE305]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE306]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE307]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE308]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE309]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE310]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE311]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE312]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE313]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE314]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE315]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE316]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE317]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the `LambdaOutput` object that will map (later) to the output of a
    `LambdaStep` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE318]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE319]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE320]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE321]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `LambdaStep` object, which maps to the Lambda function that
    checks whether a specified ML inference endpoint exists already (given the endpoint
    name):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE322]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE323]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE324]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE325]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE326]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE327]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE328]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE329]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE330]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE331]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE332]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE333]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE334]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure to replace `<INSERT FUNCTION ARN>` with the ARN of the `check-if-endpoint-exists`
    Lambda function we copied into our text editor. It should have a format that is
    similar to `arn:aws:lambda:us-west-2:<ACCOUNT ID>:function:check-if-endpoint-exists`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, initialize the `LambdaStep` object, which maps to the Lambda function
    that deploys the trained ML model to an existing ML inference endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE335]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE336]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE337]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE338]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE339]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE340]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE341]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE342]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE343]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE344]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE345]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE346]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Make sure that you replace `<INSERT FUNCTION ARN>` with the ARN of the `deploy-model-to-existing-endpoint`
    Lambda function we copied into our text editor. It should have a format similar
    to `arn:aws:lambda:us-west-2:<ACCOUNT ID>:function:` `deploy-model-to-existing-endpoint`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, initialize the `LambdaStep` object, which maps to the Lambda function
    that deploys the trained ML model to a new ML inference endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE347]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE348]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE349]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE350]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE351]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE352]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE353]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE354]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE355]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE356]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE357]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE358]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that you replace `<INSERT FUNCTION ARN>` with the ARN of the `deploy-model-to-new-endpoint`
    Lambda function we copied into our text editor. It should have a format that is
    similar to `arn:aws:lambda:us-west-2:<ACCOUNT ID>:function: deploy-model-to-new-endpoint`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the three `LambdaStep` objects ready, let’s prepare the `ConditionStep`
    object, which checks whether an endpoint exists already (using the output of the
    `endpoint_exists_lambda` `LambdaStep` object):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE359]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE360]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE361]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE362]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE363]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE364]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE365]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE366]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE367]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE368]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE369]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE370]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE371]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE372]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE373]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This step tells the ML pipeline to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy the model to a new endpoint if the endpoint does not exist yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the model to an existing endpoint if the endpoint exists already.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To help us visualize how we configured the `ConditionStep` object, let’s quickly
    check *Figure 11.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Configuring and preparing the ConditionStep object ](img/B18638_11_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Configuring and preparing the ConditionStep object
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the `ConditionStep` object is initialized with several
    parameters—`conditions`, `if_steps`, and `else_steps` (in addition to `name` of
    the endpoint). If `EndpointExists` `LambdaStep` returns `True`, then `DeployToExistingEndpoint`
    `LambdaStep` is executed. Otherwise, `DeployToNewEndpoint` `LambdaStep` is executed
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all of the steps ready, let’s initialize a new `Pipeline` object using
    the different step objects we prepared:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE374]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE375]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE376]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE377]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE378]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE379]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE380]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE381]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE382]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE383]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE384]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE385]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE386]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE387]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE388]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE389]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that this pipeline is different and separate from the (partial) pipeline
    we prepared in the *Running our first pipeline with SageMaker Pipelines* section
    of this chapter. We should see that this pipeline has a few more additional steps
    once we run it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Running the complete ML pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With everything ready, we can now run our end-to-end ML pipeline. Compared
    to the (partial) pipeline we executed in the *Running our first pipeline with
    SageMaker Pipelines* section of this chapter, our (complete) pipeline allows us
    to specify an optional name of the ML inference endpoint (*Note: Do not run the
    following block of code*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE390]'
  prefs: []
  type: TYPE_PRE
- en: If the endpoint name is not specified, the pipeline proceeds with using the
    default endpoint name value (that is, `AutoGluonEndpoint`) during pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will run our pipeline, wait for it to deploy a
    trained ML model to a new inference endpoint, and then test the deployed model
    using the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing where we left off after running the last block of code in the `Machine
    Learning Pipelines with SageMaker Pipelines.ipynb` notebook, let’s run the end-to-end
    ML pipeline using the following block of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE391]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE392]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s use the `wait()` method to wait for the entire pipeline to complete:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE393]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline execution should take around 15–30 minutes to complete. Feel free
    to grab a cup of coffee or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: While waiting, locate and click on the triangle icon (**SageMaker resources**)
    near the bottom of the left-hand sidebar of SageMaker Studio. This should open
    the **SageMaker resources** pane where we can view and inspect a variety of SageMaker
    resources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Pipelines** from the list of options available in the drop-down menu
    of the **SageMaker resources** pane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, double-click on the row that maps to the `COMPLETE-PIPELINE` pipeline
    we just created. After that double-click on the row that maps to the pipeline
    execution we triggered. You should see a graph similar to what is shown in *Figure
    11.18*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.18 – The ML pipeline is currently running the TrainModel step ](img/B18638_11_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – The ML pipeline is currently running the TrainModel step
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the `COMPLETE-PIPELINE` pipeline has more steps compared
    to the `PARTIAL-PIPELINE` pipeline we executed in the *Running our first pipeline
    with SageMaker Pipelines* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a few minutes, the graph should have more steps completed similar to
    what we have in *Figure 11.19*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.19 – The ML pipeline proceeds with running the DeployToNewEndpoint
    step  ](img/B18638_11_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – The ML pipeline proceeds with running the DeployToNewEndpoint
    step
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that since the ML endpoint does not exist yet (since we deleted
    it earlier while running the `Test Endpoint then Delete.ipynb` notebook), the
    ML pipeline proceeded with running the **DeployToNewEndpoint** step. Note that
    for succeeding runs, if the ML endpoint exists already, the **DeployToExistingEndpoint**
    step should run instead.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that the execution role (attached to the `AWSLambda_FullAccess` permission
    policy attached if you encounter the following error while running the Lambda
    functions: **ClientError: User: <ARN> is not authorized to perform: lambda:InvokeFunction
    on resource: <arn> because no identity-based policy allows the lambda:InvokeFunction
    action**. Feel free to check the *Preparing the essential prerequisites* section
    of this chapter for step-by-step instructions on how to update the permissions
    of the execution role.'
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the pipeline execution to finish. Once the pipeline has finished running,
    our AutoGluon model should be deployed inside an ML inference endpoint (named
    `AutoGluonEndpoint`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate back to the tab corresponding to the `Test Endpoint then Delete.ipynb`
    notebook. Open the `Test Endpoint then Delete.ipynb` notebook. Note that running
    all the cells in the notebook would also delete the existing ML inference endpoint
    (named `AutoGluonEndpoint`) after all cells have finished running.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It should take 1–2 minutes to run all the cells in the Jupyter notebook. Feel
    free to grab a cup of coffee or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the cells in the `Test Endpoint then Delete.ipynb` notebook have been
    executed, locate the cell containing the following block of code (along with the
    output returned):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE394]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE395]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Verify that our model obtained an accuracy score equal to or close to `0.88`
    (or 88%). Note that this should be similar to what we obtained earlier in the
    *Testing our ML inference endpoint* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '*What can we do with this pipeline?* With this pipeline, by specifying different
    endpoint names for each pipeline run, we would be able to train and deploy a model
    to multiple endpoints. This should help us handle scenarios where we would need
    to manage dedicated ML inference endpoints for different environments (such as
    the `production` and `staging` environments). For example, we can have two running
    ML inference endpoints at the same time—`AutoGluonEndpoint-production` and `AutoGluonEndpoint-staging`.
    If we wish to generate a new model from a new dataset, we can trigger a pipeline
    run and specify the endpoint name for the `staging` environment instead of the
    `production` environment. This will help us test and verify the quality of the
    new model deployed in the `staging` environment and ensure that the `production`
    environment is always in a stable state. Once we need to update the `production`
    environment, we can simply trigger another pipeline run and specify the endpoint
    name associated with the `production` environment when training and deploying
    the new model.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to manage these types of deployments, and this is one
    of the options available for ML engineers and data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: That’s pretty much it! Congratulations on being able to complete a relatively
    more complex ML pipeline! We were able to accomplish a lot in this chapter, and
    we should be ready to design and build our own custom pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have completed working on the hands-on solutions of this chapter,
    it is time we clean up and turn off the resources we will no longer use. In the
    next set of steps, we will locate and turn off any remaining running instances
    in **SageMaker Studio**:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to check and delete all running inference endpoints under **SageMaker
    resources** (if any). To check whether there are running inference endpoints,
    click on the **SageMaker resources** icon and then select **Endpoints** from the
    list of options in the drop-down menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the **File** menu and select **Shut down** from the list of available options.
    This should turn off all running instances inside SageMaker Studio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is important to note that this cleanup operation needs to be performed after
    using **SageMaker Studio**. These resources are not turned off automatically by
    SageMaker even during periods of inactivity. Make sure to review whether all delete
    operations have succeeded before proceeding to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to clean up and delete all the other resources in the AWS account
    (for example, the Cloud9 environment and the VPCs and Lambda functions we created),
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Recommended strategies and best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we end this chapter (and this book), let’s quickly discuss some of the
    recommended strategies and best practices when using SageMaker Pipelines to prepare
    automated ML workflows. *What improvements can we make to the initial version
    of our pipeline?* Here are some of the possible upgrades we can implement to make
    our setup more scalable, more secure, and more capable of handling different types
    of ML and ML engineering requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure and set up **autoscaling** (automatic scaling) of the ML inference
    endpoint upon creation to dynamically adjust the number of resources used to handle
    the incoming traffic (of ML inference requests).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow ML models to also be deployed in **serverless** and **asynchronous** endpoints
    (depending on the value of an additional pipeline input parameter) to help provide
    additional model deployment options for a variety of use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an additional step (or steps) in the pipeline that automatically evaluates
    the trained ML model using the test set and rejects the deployment of the model
    if the target metric value falls below a specified threshold score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add an additional step in the pipeline that uses **SageMaker Clarify** to check
    for biases and drifts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trigger a pipeline execution once an event happens through **Amazon EventBridge**
    (such as a file being uploaded in an Amazon S3 bucket).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cache specific pipeline steps to speed up repeated pipeline executions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilize **Retry policies** to automatically retry specific pipeline steps when
    exceptions and errors occur during pipeline executions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use **SageMaker Pipelines** with **SageMaker Projects** for building complete
    ML workflows, which may involve CI/CD capabilities (using AWS services such as
    **AWS CodeCommit** and **AWS CodePipeline**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the IAM roles used in this chapter with a more restrictive set of permissions
    to improve the security of the setup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To manage the long-term costs of running SageMaker resources, we can utilize
    the **Machine Learning Savings Plans**, which involves reducing the overall cost
    of running resources after making a long-term commitment (for example, a 1-year
    or 3-year commitment)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s more we can add to this list, but these should do for now! Make sure
    that you review and check the recommended solutions and strategies shared in [*Chapter
    9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and Compliance Strategies*,
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we used **SageMaker Pipelines** to build end-to-end automated
    ML pipelines. We started by preparing a relatively simple pipeline with three
    steps—including the data preparation step, the model training step, and the model
    registration step. After preparing and defining the pipeline, we proceeded with
    triggering a pipeline execution that registered a newly trained model to the **SageMaker
    Model Registry** after the pipeline execution finished running.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we prepared three AWS Lambda functions that would be used for the model
    deployment steps of the second ML pipeline. After preparing the Lambda functions,
    we proceeded with completing the end-to-end ML pipeline by adding a few additional
    steps to deploy the model to a new or existing ML inference endpoint. Finally,
    we discussed relevant best practices and strategies to secure, scale, and manage
    ML pipelines using the technology stack we used in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve finally reached the end of this book! Congratulations on completing all
    the chapters including the hands-on examples and solutions discussed in this book.
    It has been an amazing journey from start to finish, and it would be great if
    you can share this journey with others, too.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, you might want to dive deeper into the relevant subtopics discussed
    by checking the references listed in the *Further reading* section of each of
    the previous chapters. In addition to these, you can check the following resources,
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Amazon SageMaker Model Building Pipelines – Pipeline Steps* ([https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Boto3 – SageMaker Client* ([https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.xhtml](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Amazon SageMaker – AutoGluon-Tabular Algorithm* ([https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/autogluon-tabular.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automate MLOps with SageMaker Projects* ([https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-projects.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning Savings Plans* ([https://aws.amazon.com/savingsplans/ml-pricing/](https://aws.amazon.com/savingsplans/ml-pricing/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SageMaker – Amazon EventBridge Integration* ([https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/pipeline-eventbridge.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
