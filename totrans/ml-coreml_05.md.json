["```py\n!pip install coremltools\n```", "```py\nimport coremltools\n```", "```py\ncoreml_model = coremltools.converters.keras.convert(\n    'tinyyolo_voc2007_modelweights.h5',\n    input_names='image',\n    image_input_names='image',\n    output_names='output',\n    image_scale=1./255.)\n```", "```py\ncoreml_model.author = 'Joshua Newnham'\ncoreml_model.license = 'BSD'\ncoreml_model.short_description = 'Keras port of YOLOTiny VOC2007 by Joseph Redmon and Ali Farhadi'\ncoreml_model.input_description['image'] = '416x416 RGB Image'\ncoreml_model.output_description['output'] = '13x13 Grid made up of: [cx, cy, w, h, confidence, 20 x classes] * 5 bounding boxes'\n```", "```py\ncoreml_model.save('tinyyolo_voc2007.mlmodel')\n```", "```py\nstruct ObjectBounds {\n    public var object : DetectableObject\n    public var origin : CGPoint\n    public var size : CGSize\n\n    var bounds : CGRect{\n        return CGRect(origin: self.origin, size: self.size)\n    }\n}    \n```", "```py\nstruct DetectableObject{\n    public var classIndex : Int\n    public var label : String\n\n    static let objects = [\n        DetectableObject(classIndex:19, label:\"tvmonitor\"),\n        DetectableObject(classIndex:18, label:\"train\"),\n        DetectableObject(classIndex:17, label:\"sofa\"),\n        DetectableObject(classIndex:14, label:\"person\"),\n        DetectableObject(classIndex:11, label:\"dog\"),\n        DetectableObject(classIndex:7, label:\"cat\"),\n        DetectableObject(classIndex:6, label:\"car\"),\n        DetectableObject(classIndex:5, label:\"bus\"),\n        DetectableObject(classIndex:4, label:\"bottle\"),\n        DetectableObject(classIndex:3, label:\"boat\"),\n        DetectableObject(classIndex:2, label:\"bird\"),\n        DetectableObject(classIndex:1, label:\"bicycle\")\n    ]\n}\n```", "```py\nstruct SearchResult{\n    public var image : UIImage  \n    public var detectedObjects : [ObjectBounds]\n    public var cost : Float\n}\n```", "```py\nclass YOLOFacade{\n\n    // TODO add input size (of image)\n    // TODO add grid size\n    // TODO add number of classes\n    // TODO add number of anchor boxes\n    // TODO add anchor shapes (describing aspect ratio)\n\n    lazy var model : VNCoreMLModel? = {\n        do{\n            // TODO add model\n            return nil\n        } catch{\n            fatalError(\"Failed to obtain tinyyolo_voc2007\")\n        }\n    }()\n\n    func asyncDetectObjects(\n        photo:UIImage,\n        completionHandler:@escaping (_ result:[ObjectBounds]?) -> Void){\n\n        DispatchQueue.global(qos: .background).sync {\n\n            self.detectObjects(photo: photo, completionHandler: { (result) -> Void in\n                DispatchQueue.main.async {\n                    completionHandler(result)\n                }\n            })\n        }\n    }\n\n}  \n```", "```py\n// TODO add input size (of image)\nvar targetSize = CGSize(width: 416, height: 416)\n```", "```py\n// TODO add grid size\nlet gridSize = CGSize(width: 13, height: 13)\n// TODO add number of classes\nlet numberOfClasses = 20\n// TODO add number of anchor boxes\nlet numberOfAnchorBoxes = 5\n// TODO add anchor shapes (describing aspect ratio)\nlet anchors : [Float] = [1.08, 1.19, 3.42, 4.41, 6.63, 11.38, 9.42, 5.11, 16.62, 10.52]\n```", "```py\nlazy var model : VNCoreMLModel = {\n    do{\n        // TODO add model\n        let model = try VNCoreMLModel(\n for: tinyyolo_voc2007().model)\n return model\n    } catch{\n        fatalError(\"Failed to obtain tinyyolo_voc2007\")\n    }\n}()\n```", "```py\nlet request = VNCoreMLRequest(model: self.model)\nrequest.imageCropAndScaleOption = .centerCrop\n\nlet handler = VNImageRequestHandler(cgImage: cgImage, options: [:])\n\ndo {\n    try handler.perform([request])\n} catch {\n    print(\"Failed to perform classification.\\n\\(error.localizedDescription)\")\n    completionHandler(nil)\n    return\n}\n```", "```py\nguard let observations = request.results as? [VNCoreMLFeatureValueObservation] else{\n    completionHandler(nil)\n    return\n}\n\nvar detectedObjects = [ObjectBounds]()\n\nfor observation in observations{\n    guard let multiArray = observation.featureValue.multiArrayValue else{\n        continue\n    }\n\n    if let observationDetectedObjects = self.detectObjectsBounds(array: multiArray){\n\n        for detectedObject in observationDetectedObjects.map(\n            {$0.transformFromCenteredCropping(from: photo.size, to: self.targetSize)}){\n                detectedObjects.append(detectedObject)\n        }\n    }\n}\n\ncompletionHandler(detectedObjects) \n```", "```py\nlet gridStride = array.strides[0].intValue\nlet rowStride = array.strides[1].intValue\nlet colStride = array.strides[2].intValue\n\nlet arrayPointer = UnsafeMutablePointer<Double>(OpaquePointer(array.dataPointer))\n\nvar objectsBounds = [ObjectBounds]()\nvar objectConfidences = [Float]() \n```", "```py\nfor row in 0..<Int(gridSize.height) {\n    for col in 0..<Int(gridSize.width) {\n        for b in 0..<numberOfAnchorBoxes {\n\n            let gridOffset = row * rowStride + col * colStride\n            let anchorBoxOffset = b * (numberOfClasses + numberOfAnchorBoxes)\n            // TODO calculate the confidence of each class, ignoring if under threshold \n        }\n    }\n}\n```", "```py\nlet confidence = sigmoid(x: Float(arrayPointer[(anchorBoxOffset + 4) * gridStride + gridOffset]))\n\nvar classes = Array<Float>(repeating: 0.0, count: numberOfClasses)\nfor c in 0..<numberOfClasses{\n    classes[c] = Float(arrayPointer[(anchorBoxOffset + 5 + c) * gridStride + gridOffset])\n}\nclasses = softmax(z: classes)\n\nlet classIdx = classes.argmax\nlet classScore = classes[classIdx]\nlet classConfidence = classScore * confidence\n\nif classConfidence < objectThreshold{\n    continue\n}\n\n// TODO obtain bounding box and transform to image dimensions \n```", "```py\n/**\n Subtract a scalar c from a vector x\n @param x Vector x.\n @param c Scalar c.\n @return A vector containing the difference of the scalar and the vector\n */\npublic func sub(x: [Float], c: Float) -> [Float] {\n    var result = (1...x.count).map{_ in c} \n    catlas_saxpby(Int32(x.count), 1.0, x, 1, -1.0, &amp;result, 1) \n    return result\n}\n```", "```py\n/**\n Perform an elementwise exponentiation on a vector \n @param x Vector x.\n @returns A vector containing x exponentiated elementwise.\n */\nfunc exp(x: [Float]) -> [Float] {\n    var results = [Float](repeating: 0.0, count: x.count) \n    vvexpf(&amp;results, x, [Int32(x.count)]) \n    return results\n}\n```", "```py\n/**\n Compute the vector sum of a vector\n @param x Vector.\n @returns A single precision vector sum.\n */\npublic func sum(x: [Float]) -> Float {\n    return cblas_sasum(Int32(x.count), x, 1)\n}\n```", "```py\n /**\n Divide a vector x by a scalar y\n @param x Vector x.\n @parame c Scalar c.\n @return A vector containing x dvidided elementwise by vector c.\n */\npublic func div(x: [Float], c: Float) -> [Float] {\n    let divisor = [Float](repeating: c, count: x.count)\n    var result = [Float](repeating: 0.0, count: x.count) \n    vvdivf(&amp;result, x, divisor, [Int32(x.count)]) \n    return result\n}\n```", "```py\n/**\n Softmax function\n @param z A vector z.\n @return A vector y = (e^z / sum(e^z))\n */\nfunc softmax(z: [Float]) -> [Float] {\n    let x = exp(x:sub(x:z, c: z.maxValue))    \n    return div(x:x, c: sum(x:x))\n}\n```", "```py\nextension Array where Element == Float{\n\n    /**\n     @return index of the largest element in the array\n     **/\n    var argmax : Int {\n        get{\n            precondition(self.count > 0)\n\n            let maxValue = self.maxValue\n            for i in 0..<self.count{\n                if self[i] == maxValue{\n                    return i\n                }\n            }\n            return -1\n        }\n    }\n\n    /**\n     Find the maximum value in array\n     */\n    var maxValue : Float{\n        get{\n            let len = vDSP_Length(self.count)\n\n            var max: Float = 0\n            vDSP_maxv(self, 1, &amp;max, len)\n\n            return max\n        }\n    }\n}\n```", "```py\nlet tx = CGFloat(arrayPointer[anchorBoxOffset * gridStride + gridOffset])\nlet ty = CGFloat(arrayPointer[(anchorBoxOffset + 1) * gridStride + gridOffset])\nlet tw = CGFloat(arrayPointer[(anchorBoxOffset + 2) * gridStride + gridOffset])\nlet th = CGFloat(arrayPointer[(anchorBoxOffset + 3) * gridStride + gridOffset])\n\nlet cx = (sigmoid(x: tx) + CGFloat(col)) / gridSize.width \nlet cy = (sigmoid(x: ty) + CGFloat(row)) / gridSize.height\nlet w = CGFloat(anchors[2 * b + 0]) * exp(tw) / gridSize.width \nlet h = CGFloat(anchors[2 * b + 1]) * exp(th) / gridSize.height\n\n// TODO create a ObjectBounds instance and store it in our array of candidates  \n```", "```py\n/**\n A sigmoid function \n @param x Scalar\n @return 1 / (1 + exp(-x))\n */\npublic func sigmoid(x: CGFloat) -> CGFloat {\n    return 1 / (1 + exp(-x))\n}\n```", "```py\nguard let detectableObject = DetectableObject.objects.filter(\n    {$0.classIndex == classIdx}).first else{\n    continue\n}\n\nlet objectBounds = ObjectBounds(\n    object: detectableObject,\n    origin: CGPoint(x: cx - w/2, y: cy - h/2),\n    size: CGSize(width: w, height: h))\n\nobjectsBounds.append(objectBounds)\nobjectConfidences.append(classConfidence)\n```", "```py\nreturn self.filterDetectedObjects(\n    objectsBounds: objectsBounds,\n    objectsConfidence: objectConfidences)\n```", "```py\nfunc filterDetectedObjects(\n    objectsBounds:[ObjectBounds],\n    objectsConfidence:[Float],\n    nmsThreshold : Float = 0.3) -> [ObjectBounds]?{\n\n    // If there are no bounding boxes do nothing\n    guard objectsBounds.count > 0 else{\n        return []\n    }\n        // TODO implement Non-Max Supression\n\n    return nil\n}\n```", "```py\nvar detectionConfidence = objectsConfidence.map{\n    (confidence) -> Float in\n    return confidence\n}\n\nlet sortedIndices = detectionConfidence.indices.sorted {\n    detectionConfidence[$0] > detectionConfidence[$1]\n}\n\nvar bestObjectsBounds = [ObjectBounds]()\n\n// TODO iterate through each box \n```", "```py\nfor i in 0..<sortedIndices.count{\n    let objectBounds = objectsBounds[sortedIndices[i]]\n\n    guard detectionConfidence[sortedIndices[i]] > 0 else{\n        continue\n    }\n\n    bestObjectsBounds.append(objectBounds)\n\n    for j in (i+1)..<sortedIndices.count{\n        guard detectionConfidence[sortedIndices[j]] > 0 else {\n            continue\n        }\n        let otherObjectBounds = objectsBounds[sortedIndices[j]]\n\n        // TODO calculate IoU and compare against our threshold        \n    }\n}\n```", "```py\nif Float(objectBounds.bounds.computeIOU(\n    other: otherObjectBounds.bounds)) > nmsThreshold{\n    detectionConfidence[sortedIndices[j]] = 0.0\n}\n```", "```py\nextension CGRect{\n\n    ...\n\n    var area : CGFloat{\n        get{\n            return self.size.width * self.size.height\n        }\n    }\n\n    func computeIOU(other:CGRect) -> CGFloat{\n        return self.intersection(other).area / self.union(other).area\n    }    \n}\n```", "```py\npublic func asyncSearch(\n    searchCriteria : [ObjectBounds]?,\n    costThreshold : Float = 5){\n    DispatchQueue.global(qos: .background).async {\n        let photos = self.getPhotosFromPhotosLibrary()\n\n        let unscoredSearchResults = self.detectObjects(photos: photos)\n\n        var sortedSearchResults : [SearchResult]?\n\n        if let unscoredSearchResults = unscoredSearchResults{\n            sortedSearchResults = self.calculateCostForObjects(\n detectedObjects:unscoredSearchResults,\n searchCriteria: searchCriteria).filter({\n (searchResult) -> Bool in\n return searchResult.cost < costThreshold\n }).sorted(by: { (a, b) -> Bool in\n return a.cost < b.cost\n })\n        }\n\n        DispatchQueue.main.sync {\n            self.delegate?.onPhotoSearcherCompleted(\n                status: 1,\n                result: sortedSearchResults)\n        }\n    }\n} \n```", "```py\n private func calculateCostForObjects(\n    detectedObjects:[SearchResult],\n    searchCriteria:[ObjectBounds]?) -> [SearchResult]{\n\n    guard let searchCriteria = searchCriteria else{\n        return detectedObjects\n    }\n\n    var result = [SearchResult]()\n\n    for searchResult in detectedObjects{\n        let cost = self.costForObjectPresences(\n            detectedObject: searchResult,\n            searchCriteria: searchCriteria) +\n            self.costForObjectRelativePositioning(\n                detectedObject: searchResult,\n                searchCriteria: searchCriteria) +\n            self.costForObjectSizeRelativeToImageSize(\n                detectedObject: searchResult,\n                searchCriteria: searchCriteria) +\n            self.costForObjectSizeRelativeToOtherObjects(\n                detectedObject: searchResult,\n                searchCriteria: searchCriteria)\n\n        let searchResult = SearchResult(\n            image: searchResult.image,\n            detectedObjects:searchResult.detectedObjects,\n            cost: cost)\n\n        result.append(searchResult)\n    }\n\n    return result\n}\n```", "```py\nprivate func costForObjectPresences(\n    detectedObject:SearchResult,\n    searchCriteria:[ObjectBounds],\n    weight:Float=2.0) -> Float{\n\n    var cost : Float = 0.0\n\n    // TODO implement cost function for object presence\n\n    return cost * weight\n}\n```", "```py\nvar searchObjectCounts = searchCriteria.map {\n    (detectedObject) -> String in\n    return detectedObject.object.label\n    }.reduce([:]) {\n        (counter:[String:Float], label) -> [String:Float] in\n        var counter = counter\n        counter[label] = counter[label]?.advanced(by: 1) ?? 1\n        return counter\n}\n\nvar detectedObjectCounts = detectedObject.detectedObjects.map {\n    (detectedObject) -> String in\n    return detectedObject.object.label\n    }.reduce([:]) {\n        (counter:[String:Float], label) -> [String:Float] in\n        var counter = counter\n        counter[label] = counter[label]?.advanced(by: 1) ?? 1\n        return counter\n}\n\n// TODO accumulate cost based on the difference\n```", "```py\nfor detectableObject in DetectableObject.objects{\n    let label = detectableObject.label\n\n    let searchCount = searchObjectCounts[label] ?? 0\n    let detectedCount = detectedObjectCounts[label] ?? 0\n\n    cost += abs(searchCount - detectedCount)\n}\n```", "```py\n private func costForObjectRelativePositioning(\n    detectedObject:SearchResult,\n    searchCriteria:[ObjectBounds],\n    weight:Float=1.5) -> Float{\n\n    var cost : Float = 0.0\n\n    // TODO implement cost function for relative positioning\n\n    return cost * weight\n} \n```", "```py\nfunc indexOfClosestObject(\n    objects:[ObjectBounds],\n    forObjectAtIndex i:Int) -> Int{\n\n    let searchACenter = objects[i].bounds.center\n\n    var closestDistance = Float.greatestFiniteMagnitude\n    var closestObjectIndex : Int = -1\n\n    for j in 0..<objects.count{\n        guard i != j else{\n            continue\n        }\n\n        let searchBCenter = objects[j].bounds.center\n        let distance = Float(searchACenter.distance(other: searchBCenter))\n        if distance < closestDistance{\n            closestObjectIndex = j\n            closestDistance = distance\n        }\n    }\n\n    return closestObjectIndex\n}\n\n// TODO Iterate over all items in the searchCriteria array \n```", "```py\nfor si in 0..<searchCriteria.count{\n    let closestObjectIndex = indexOfClosestObject(\n        objects: searchCriteria,\n        forObjectAtIndex: si)\n\n    if closestObjectIndex < 0{\n        continue\n    }\n\n    // Get object types\n    let searchAClassIndex = searchCriteria[si].object.classIndex\n    let searchBClassIndex = searchCriteria[closestObjectIndex].object.classIndex\n\n    // Get centers of objects\n    let searchACenter = searchCriteria[si].bounds.center\n    let searchBCenter = searchCriteria[closestObjectIndex].bounds.center\n\n    // Calcualte the normalised vector from A -> B\n    let searchDirection = (searchACenter - searchBCenter).normalised\n\n    // TODO Find matching pair\n}  \n```", "```py\n// Find comparable objects in detected objects\nlet detectedA = detectedObject.detectedObjects.filter {\n    (objectBounds) -> Bool in\n    objectBounds.object.classIndex == searchAClassIndex\n}\n\nlet detectedB = detectedObject.detectedObjects.filter {\n    (objectBounds) -> Bool in\n    objectBounds.object.classIndex == searchBClassIndex\n}\n\n// Check that we have matching pairs\nguard detectedA.count > 0, detectedB.count > 0 else{\n    continue\n}\n\n// TODO Search for the most suitable pair\n```", "```py\nvar closestDotProduct : Float = Float.greatestFiniteMagnitude\nfor i in 0..<detectedA.count{\n    for j in 0..<detectedB.count{\n        if detectedA[i] == detectedB[j]{\n            continue\n        }\n\n        let detectedDirection = (detectedA[i].bounds.center - detectedB[j].bounds.center).normalised\n        let dotProduct = Float(searchDirection.dot(other: detectedDirection))\n        if closestDotProduct > 10 ||\n            (dotProduct < closestDotProduct &amp;&amp;\n                dotProduct >= 0) {\n            closestDotProduct = dotProduct\n        }\n    }\n}\n\n// TODO Add cost \n```", "```py\nextension CGPoint{\n\n    var length : CGFloat{\n        get{\n            return sqrt(\n                self.x * self.x + self.y * self.y\n            )\n        }\n    }\n\n    var normalised : CGPoint{\n        get{\n            return CGPoint(\n                x: self.x/self.length,\n                y: self.y/self.length)\n        }\n    }\n\n    func distance(other:CGPoint) -> CGFloat{\n        let dx = (self.x - other.x)\n        let dy = (self.y - other.y)\n\n        return sqrt(dx*dx + dy*dy)\n    }\n\n    func dot(other:CGPoint) -> CGFloat{\n        return (self.x * other.x) + (self.y * other.y)\n    }\n\n    static func -(left: CGPoint, right: CGPoint) -> CGPoint{\n        return CGPoint(\n            x: left.x - right.x,\n            y: left.y - right.y)\n    }\n}   \n```", "```py\ncost += abs((1.0-closestDotProduct))\n```", "```py\nlet model = tinyyolo_voc2007().model\n```", "```py\nfunc detectObjects(photos:[UIImage], completionHandler:(_ result:[[ObjectBounds]]?) -> Void){\n\n    // TODO batch items (array of MLFeatureProvider)\n\n    // TODO Wrap our items in an instance of MLArrayBatchProvider \n\n    // TODO Perform inference on the batch \n\n // TODO (As we did before) Process the outputs of the model \n\n // TODO Return results via the callback handler \n}\n```", "```py\nlet X = photos.map({ (photo) -> tinyyolo_voc2007Input in\n    guard let ciImage = CIImage(image: photo) else{\n        fatalError(\"\\(#function) Failed to create CIImage from UIImage\")\n    }\n    let cropSize = CGSize(\n        width:min(ciImage.extent.width, ciImage.extent.height),\n        height:min(ciImage.extent.width, ciImage.extent.height))\n\n    let targetSize = CGSize(width:416, height:416)\n\n    guard let pixelBuffer = ciImage\n        .centerCrop(size:cropSize)?\n        .resize(size:targetSize)\n        .toPixelBuffer() else{\n        fatalError(\"\\(#function) Failed to create CIImage from UIImage\")\n    }\n\n    return tinyyolo_voc2007Input(image:pixelBuffer)\n\n}) \n\n// TODO Wrap our items in an instance of MLArrayBatchProvider \n\n// TODO Perform inference on the batch \n\n// TODO (As we did before) Process the outputs of the model \n\n// TODO Return results via the callback handler \n```", "```py\nlet batch = MLArrayBatchProvider(array:X)\n\n// TODO Perform inference on the batch \n\n// TODO (As we did before) Process the outputs of the model \n\n// TODO Return results via the callback handler \n```", "```py\nguard let batchResults = try? self.model.predictions(\n    from: batch,\n    options: MLPredictionOptions()) else{\n        completionHandler(nil)\n        return\n}\n\n// TODO (As we did before) Process the outputs of the model \n// TODO Return results via the callback handler \n\n```", "```py\nvar results = [[ObjectBounds]]()\n\nfor i in 0..<batchResults.count{\n    var iResults = [ObjectBounds]()\n\n    if let features = batchResults.features(at: i)\n        as? tinyyolo_voc2007Output{\n\n        if let observationDetectObjects = self.detectObjectsBounds(\n            array: features.output){\n\n            for detectedObject in observationDetectObjects.map(\n                {$0.transformFromCenteredCropping(\n                    from: photos[i].size,\n                    to: self.targetSize)}){\n\n                    iResults.append(detectedObject)\n            }\n\n        }\n    }\n    results.append(iResults)\n}\n\n// TODO Return results via the callback handler \n```", "```py\ncompletionHandler(results)\n```", "```py\nvar results = [SearchResult]()\n\nyolo.detectObjects(photos: photos) { (photosObjectBounds) in\n    if let photosObjectBounds = photosObjectBounds,\n        photos.count == photosObjectBounds.count{\n        for i in 0..<photos.count{\n            results.append(SearchResult(\n                image: photos[i],\n                detectedObjects: photosObjectBounds[i],\n                cost: 0.0))\n        }\n    }\n}\n\nreturn results\n```"]