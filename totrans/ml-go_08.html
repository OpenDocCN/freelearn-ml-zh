<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Networks and Deep Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this book, we have talked a lot about training, or teaching, machines to make predictions. To this end, we have employed a variety of useful and interesting algorithms including various types of regression, decisions trees, and nearest neighbors. However, let's take a step back and think about what entity we might want to mimic if we are trying to make accurate predictions and learn about data.</p>
<p>Well, the most obvious answer to this question is that we should mimic our own brains. We as humans have a natural ability to recognize objects, predict quantities, recognize frauds, and more, and these are all things that we would like machines to do artificially. Granted, we are not perfect at these activities, but we are pretty good!</p>
<p>This type of thinking was what lead to the development of <strong>artificial neural networks</strong> (also known as <strong>neural networks</strong> or just <strong>neural nets</strong>). These models attempt to roughly mimic certain structures in our brains, such as <strong>neurons</strong>. They have been widely successful across industries and are currently being applied to solve a variety of interesting problems.</p>
<p>Recently, more specialized and complicated types of neural networks have been attracting a lot of interest and attention. These neural networks fall under the category of <strong>deep learning</strong> and generally have a <strong>deeper</strong> structure than what would be considered regular neural networks. That is, they have many hidden layers of structure and could be parameterized with tens of millions of parameters or weights.</p>
<p>We will attempt to introduce <span>both</span> Go-based neural networks and deep learning models in this chapter. These topics are incredibly broad and there are entire books on deep learning alone. Thus, we will only be touching the surface here. That being said, this following content should provide you with a solid starting point to build neural networks in Go.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding neural net jargon</h1>
                </header>
            
            <article>
                
<p>There are a huge variety of neural network flavors, and each of these flavors has its own set of jargon. However, there is some common jargon that we should know regardless of the type of neural network that we are utilizing. This jargon is presented in the following points:</p>
<ul>
<li><strong>Nodes</strong>, <strong>perceptrons</strong>, or <strong>neurons</strong>: These interchangeable terms refer to the basic building blocks of a neural network. Each node or neuron takes in input data and performs an operation on this data. After performing the operation, the node/neuron may or may not pass the results of the operation on to other nodes/neurons.</li>
<li><strong>Activation</strong>: The output or values associated with the operation of a node.</li>
<li><strong>Activation function</strong>: The definition of the function that transforms the inputs to a node into the output, or activation.</li>
<li><strong>Weights</strong> or <strong>biases</strong>: These values define the relationships between input and output data in the activation function.</li>
<li><strong>Input layer</strong>: The input layer of a neural network includes a series of nodes that take the initial input into the neural network model (a series of features or attributes, for example).</li>
<li><strong>Output layer</strong>: <span>The output layer of a neural network includes a series of nodes that take information passed inside the neural network and transform it into a final output.</span></li>
<li><strong>Hidden layers</strong>: These layers of a neural network exist between the input and output layers and are thus <strong>hidden</strong> from outside inputs or outputs.</li>
<li><strong>Feedforward</strong> or <strong>feed forward</strong>: This refers to a scenario in which data is fed into the input layer of a neural network and transferred in a forward direction to the output layer (without any cycles).</li>
<li><strong>Backpropagation:</strong> This is a method for training neural network models that involves feeding forward values through the network, calculating generated errors, and then transferring changes based on these errors back into the network.</li>
<li><strong>Architecture</strong>: The overall structure of how neurons are connected together in the neural network is called an <strong>architecture</strong>.</li>
</ul>
<p>To solidify these concepts, consider the following schematic of a neural network:</p>
<div class="CDPAlignCenter CDPAlign"><img height="258" width="499" class="alignnone size-full wp-image-588 image-border" src="assets/f2dedab7-b035-4fcf-b7c9-fdbec00e31de.png"/></div>
<p>This is a basic feedforward (that is, acyclic or recursive) neural network. It has two hidden layers, accepts two inputs, and outputs two class values (or results).</p>
<p>Don't worry if all this jargon seems a little bit overwhelming right now. We will be looking at a concrete example next that should solidify all of these pieces. Also, if you get down in the weeds with the various examples in this chapter and get confused with terminology, circle back here as a reminder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a simple neural network</h1>
                </header>
            
            <article>
                
<p class="mce-root">Many neural network packages and applications of neural networks treat the models as <strong>black boxes</strong>. That is, people tend to utilize some framework that allows them to quickly build a neural network using a bunch of default values and automation. They are often able to produce some results, but this sort of convenience usually does not build much intuition about how the models actually work. As a result, when the models do not behave as expected, it's very hard to understand why they might be making weird predictions or having trouble converging.</p>
<p>Before jumping into more complicated neural networks, let's build up some basic intuition about neural networks such that we do not fall into this pattern. We are going to build a simple neural network from scratch to learn about the basic components of neural networks and how they operate together.</p>
<div class="packt_tip"><strong>Note</strong>: that even though we are going to build our neural net from scratch here (which you may also want to do in certain scenarios), there are a variety of Go packages that help you build, train, and make predictions with neural networks. These include <kbd>github.com/tleyden/neurgo</kbd>, <kbd>github.com/fxsjy/gonn</kbd>, <kbd>github.com/NOX73/go-neural</kbd>, <kbd>github.com/milosgajdos83/gosom</kbd>, <kbd>github.com/made2591/go-perceptron-go</kbd>, and <kbd>github.com/chewxy/gorgonia</kbd>.</div>
<div class="packt_infobox">We are going to utilize neural networks for classification in this chapter. However, neural nets can also be utilized to perform regression. You can learn more about that topic here: <a href="https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/">https://heuristically.wordpress.com/2011/11/17/using-neural-network-for-regression/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nodes in the network</h1>
                </header>
            
            <article>
                
<p>The nodes, or neurons, of our neural network have a relatively simple functionality in and of themselves. Each neuron will take in one or more values (<em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, and so on), combine these values according to an activation function, and produce a single output. The following is the output pictured:</p>
<div class="CDPAlignCenter CDPAlign"><img height="211" width="325" class="alignnone size-full wp-image-589 image-border" src="assets/bfb6068c-26a3-4800-8b3a-7bdbf9dd764d.png"/></div>
<p>How exactly should we combine the inputs to get the output? Well, we need a method to combine the inputs that is adjustable (such that we can train the model), and we have already seen that combining variables using coefficients and an intercept is one trainable way to combine inputs. Just think back to <a href="c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml">Chapter 4</a>, <em>Regression</em>. In this spirit, we are going to combine the inputs linearly with some coefficients (<strong>weights</strong>) and an intercept (the <strong>bias</strong>):</p>
<div style="padding-left: 150px" class="mce-root CDPAlignLeft CDPAlign"><img height="32" width="208" src="assets/3feefece-dc75-4015-a1f8-e222859fdbac.jpg"/></div>
<p>Here, <em>w<sub>1</sub></em>, <em>w<sub>2</sub></em>, and so on are our weights and <em>b</em> is the bias.</p>
<p>This combination of inputs is a good start, but it is linear at the end of the day, and thus not able to model non-linear relationships between the input and output. To introduce some non-linearity, we are going to apply an activation function to this linear combination of inputs. The activation function that we will use here is similar to the logistic function that was introduced in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml">Chapter 5</a>, <em>Classification</em>. In the context of neural networks and in the following form, the logistic function is referred to as the <strong>sigmoid</strong> <strong>function</strong>:</p>
<div style="padding-left: 180px" class="mce-root CDPAlignLeft CDPAlign"><img height="41" width="135" src="assets/ad4415d6-4781-4d70-b883-b83fecd57f96.jpg"/></div>
<p>The <kbd>sigmoid</kbd> function is a good choice for use in our node because it introduces non-linearity, but it also has a limited range (between <em>0</em> and <em>1</em>), has a simply defined derivative (which we will use during training of the network), and it can have a probabilistic interpretation (as discussed further in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml" target="_blank">Chapter 5</a>, <em>Classification</em>).</p>
<div class="packt_infobox">The <kbd>sigmoid</kbd> function is by no means the only choice for an activation function in a neural network. Other popular choices include the hyperbolic tangent function, softmax, and rectified linear units. Choices of activation functions along with their advantages and disadvantages are further discussed at <a href="https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f">https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f</a>.</div>
<p>Let's go ahead and define our activation function in Go along with its derivative. These definitions are shown in the following code:</p>
<pre>// sigmoid implements the sigmoid function
// for use in activation functions.
func sigmoid(x float64) float64 {
    return 1.0 / (1.0 + math.Exp(-x))
}

// sigmoidPrime implements the derivative
// of the sigmoid function for backpropagation.
func sigmoidPrime(x float64) float64 {
    return x * (1.0 - x)
}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network architecture</h1>
                </header>
            
            <article>
                
<p>The simple neural network that we are going to build will contain an input and output layer (as does any neural network). The network will include a single hidden layer between the input and output layer. This architecture is depicted as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="303" width="372" class="alignnone size-full wp-image-592 image-border" src="assets/a0b2fc3d-f452-4196-83b0-a162870f3293.png"/></div>
<p>In particular, we are going to include four nodes in the input layer, three nodes in the hidden layer, and three nodes in the output layer. The four nodes in the input layer correspond to the number of attributes that we are going to feed into the network. Think about these four inputs as something like the four measurements we used to classify iris flowers in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml">Chapter 5</a>, <em>Classification</em>. The output layer will have three nodes because we will be setting up our network to make classifications for the iris flowers, which could be in one of three classes.</p>
<p>Now, regarding the hidden layer--why are we using one hidden layer with three nodes? Well, one hidden layer is sufficient for a very large majority of simple tasks. If you have a large amount of non-linearity in your data, many inputs, and/or large amounts of training data, you may need to introduce more hidden layers (as further discussed later in this chapter in relation to deep learning). The choice of three nodes in the hidden layer can be tuned based on an evaluation metric and a little trial and error. You can also search the numbers of nodes in the hidden layer to automate your choice.</p>
<div class="packt_tip">Keep in mind that the more nodes you introduce into the hidden layer, the more perfectly your network can learn your training set. In other words, you are putting yourself in danger of overfitting your model such that it does not generalize. When adding this complication, it is important to validate your model very carefully to ensure that it generalizes.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why do we expect this architecture to work?</h1>
                </header>
            
            <article>
                
<p>Let's pause for a moment and think about why setting up a series of nodes in the preceding arrangement would allow us to predict something. As you can see, all we are doing is combining inputs over and over to produce some result. How can we expect this result to be useful in making binary classifications?</p>
<p>Well, what exactly do we mean when we define a binary classification problem, or any classification or regression problem, for that matter? We are basically saying that we expect there to be some rules or relationships that exist between a series of inputs (our attributes) and an output (our labels or response). In essence, we are saying that there exists some simple or complicated function that is able to transform our attributes into the response or labels that we are after.</p>
<p>We could make a choice for a type of function that might model this transformation of attributes to output, as we do in linear or logistic regression. However, we could also set up a series of chained and adjustable functions that we could algorithmically train to detect the relationships and rules between our inputs and output. This is what we are doing with the neural network!</p>
<p>The nodes of the neural network are like sub-functions that are adjusted until they mimic the rules and relationships between our supplied inputs and the output, regardless of what those rules and relationships actually are (linear, non-linear, dynamic, and so on). If underlying rules exist between the inputs and outputs, it is likely that there exists some neural network that can mimic the rules.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training our neural network</h1>
                </header>
            
            <article>
                
<p>Okay, so now we have some good motivation for why this combination of nodes might help us make predictions. How are we actually going to adjust all of the sub-functions of our neural network nodes based on some input data? The answer is called <strong>backpropagation</strong>.</p>
<p>Backpropagation is a method for training our neural network that involves doing the following iteratively over a series of <strong>epochs</strong> (or exposure to our training dataset):</p>
<ul>
<li>Feeding our training data forward through the neural network to calculate an output</li>
<li>Calculating errors in the output</li>
<li>Using gradient descent (or other relevant method) to determine how we should change our weights and biases based on the errors</li>
<li>Backpropagating these weight/bias changes into the network</li>
</ul>
<p>We will implement this process later and go through some of the details. However, certain things like gradient descent are covered in more detail in the <a href="718ca26d-465a-47c9-91b9-14e749be0c30.xhtml" target="_blank">Appendix</a>, <em>Algorithms/Techniques Related to Machine Learning</em>.</p>
<div class="packt_infobox">This backpropagation method of training a neural network is ubiquitous in the modeling world, but there are tons of unique network architectures and methods that we do not have space to cover here. Neural nets are an active area of research in academia and industry.</div>
<p>First, let's define a struct <kbd>neuralNetConfig</kbd> that will contain all of the parameters that define our network architecture and how we will go about our backpropagation iterations. Let's also define a struct <kbd>neuralNet</kbd> that will contain all of the information that defines a trained neural network. Later on, we will utilize trained <kbd>neuralNet</kbd> values to make predictions. These definitions are shown in the following code:</p>
<pre>// neuralNet contains all of the information<br/>// that defines a trained neural network.<br/>type neuralNet struct {<br/>    config neuralNetConfig<br/>    wHidden *mat.Dense<br/>    bHidden *mat.Dense<br/>    wOut *mat.Dense<br/>    bOut *mat.Dense<br/>}<br/><br/>// neuralNetConfig defines our neural network<br/>// architecture and learning parameters.<br/>type neuralNetConfig struct {<br/>    inputNeurons int<br/>    outputNeurons int<br/>    hiddenNeurons int<br/>    numEpochs int<br/>    learningRate float64<br/>}</pre>
<p>Here, <kbd>wHidden</kbd> and <kbd>bHidden</kbd> are the weights and biases for the hidden layer of the network and <kbd>wOut</kbd> and <kbd>bOut</kbd> are the weights and biases for the output layer of the network, respectively. Note that we are using <span><kbd>gonum.org/v1/gonum/mat</kbd> matrices for all the weights and biases, and we will use similar matrices to represent our inputs and outputs. This will let us easily perform the operations related to the backpropagation and generalize our training to any number of nodes in the input, hidden, and output layers.</span></p>
<p>Next, let's define a function that initializes a new neural network based on a <kbd>neuralNetConfig</kbd> value and a method that trains a <kbd>neuralNet</kbd> value based on a matrix of inputs (<em>x</em>) and a matrix of labels (<em>y</em>):</p>
<pre>// NewNetwork initializes a new neural network.
func newNetwork(config neuralNetConfig) *neuralNet {
        return &amp;neuralNet{config: config}
}

// Train trains a neural network using backpropagation.
func (nn *neuralNet) train(x, y *mat.Dense) error {<br/><br/>    // To be filled in...<br/>}</pre>
<p>In the <kbd>train()</kbd> method, we need to complete our backpropagation method and place the resulting trained weights and biases into the receiver. First, let's initialize the weights and biases randomly in the <kbd>train()</kbd> method, as shown in the following code:</p>
<pre>// Initialize biases/weights.<br/>randSource := rand.NewSource(time.Now().UnixNano())<br/>randGen := rand.New(randSource)<br/><br/>wHiddenRaw := make([]float64, nn.config.hiddenNeurons*nn.config.inputNeurons)<br/>bHiddenRaw := make([]float64, nn.config.hiddenNeurons)<br/>wOutRaw := make([]float64, nn.config.outputNeurons*nn.config.hiddenNeurons)<br/>bOutRaw := make([]float64, nn.config.outputNeurons)<br/><br/>for _, param := range [][]float64{wHiddenRaw, bHiddenRaw, wOutRaw, bOutRaw} {<br/>    for i := range param {<br/>        param[i] = randGen.Float64()<br/>    }<br/>}<br/><br/>wHidden := mat.NewDense(nn.config.inputNeurons, nn.config.hiddenNeurons, wHiddenRaw)<br/>bHidden := mat.NewDense(1, nn.config.hiddenNeurons, bHiddenRaw)<br/>wOut := mat.NewDense(nn.config.hiddenNeurons, nn.config.outputNeurons, wOutRaw)<br/>bOut := mat.NewDense(1, nn.config.outputNeurons, bOutRaw)</pre>
<p>Then, we need to loop over each of our epochs completing the backpropagation of the network. This again involves a feed forward stage in which outputs are calculated and a backpropagating stage in which changes to the weights and biases are applied. The backpropagation process will be discussed in more detail in the <a href="718ca26d-465a-47c9-91b9-14e749be0c30.xhtml" target="_blank">Appendix</a>, <em>Algorithms/Techniques Related to Machine Learning</em>, if you are interested in diving a little deeper. For now, let's focus on the implementation. The loop over epochs look like the following, with the various pieces of the backpropagation process indicated by comments:</p>
<pre>// Define the output of the neural network.
output := mat.NewDense(0, 0, nil)<br/><br/>// Loop over the number of epochs utilizing<br/>// backpropagation to train our model.<br/>for i := 0; i &lt; nn.config.numEpochs; i++ {<br/><br/>    // Complete the feed forward process.<br/><br/>    ...<br/><br/>    // Complete the backpropagation.<br/><br/>    ...<br/><br/>    // Adjust the parameters.<br/><br/>    ...<br/>}</pre>
<p>In the feed forward section of this loop, the inputs are propagated forward through our network of nodes:</p>
<pre>// Complete the feed forward process.<br/>hiddenLayerInput := mat.NewDense(0, 0, nil)<br/>hiddenLayerInput.Mul(x, wHidden)<br/>addBHidden := func(_, col int, v float64) float64 { return v + bHidden.At(0, col) }<br/>hiddenLayerInput.Apply(addBHidden, hiddenLayerInput)<br/><br/>hiddenLayerActivations := mat.NewDense(0, 0, nil)<br/>applySigmoid := func(_, _ int, v float64) float64 { return sigmoid(v) }<br/>hiddenLayerActivations.Apply(applySigmoid, hiddenLayerInput)<br/><br/>outputLayerInput := mat.NewDense(0, 0, nil)<br/>outputLayerInput.Mul(hiddenLayerActivations, wOut)<br/>addBOut := func(_, col int, v float64) float64 { return v + bOut.At(0, col) }<br/>outputLayerInput.Apply(addBOut, outputLayerInput)<br/>output.Apply(applySigmoid, outputLayerInput)</pre>
<p>Then, once we have an output from the feed forward process, we go backward through the network calculating deltas (or changes) for the output and hidden layers, as can be seen in the following code:</p>
<pre>// Complete the backpropagation.<br/>networkError := mat.NewDense(0, 0, nil)<br/>networkError.Sub(y, output)<br/><br/>slopeOutputLayer := mat.NewDense(0, 0, nil)<br/>applySigmoidPrime := func(_, _ int, v float64) float64 { return sigmoidPrime(v) }<br/>slopeOutputLayer.Apply(applySigmoidPrime, output)<br/>slopeHiddenLayer := mat.NewDense(0, 0, nil)<br/>slopeHiddenLayer.Apply(applySigmoidPrime, hiddenLayerActivations)<br/><br/>dOutput := mat.NewDense(0, 0, nil)<br/>dOutput.MulElem(networkError, slopeOutputLayer)<br/>errorAtHiddenLayer := mat.NewDense(0, 0, nil)<br/>errorAtHiddenLayer.Mul(dOutput, wOut.T())<br/><br/>dHiddenLayer := mat.NewDense(0, 0, nil)<br/>dHiddenLayer.MulElem(errorAtHiddenLayer, slopeHiddenLayer)</pre>
<p>The deltas are then utilized to update the weights and biases of our network. A <strong>learning rate</strong> is also used to scale these changes, which can help the algorithm converge. This final piece of the backprogation loop is implemented here:</p>
<pre>// Adjust the parameters.<br/>wOutAdj := mat.NewDense(0, 0, nil)<br/>wOutAdj.Mul(hiddenLayerActivations.T(), dOutput)<br/>wOutAdj.Scale(nn.config.learningRate, wOutAdj)<br/>wOut.Add(wOut, wOutAdj)<br/><br/>bOutAdj, err := sumAlongAxis(0, dOutput)<br/>if err != nil {<br/>    return err<br/>}<br/>bOutAdj.Scale(nn.config.learningRate, bOutAdj)<br/>bOut.Add(bOut, bOutAdj)<br/><br/>wHiddenAdj := mat.NewDense(0, 0, nil)<br/>wHiddenAdj.Mul(x.T(), dHiddenLayer)<br/>wHiddenAdj.Scale(nn.config.learningRate, wHiddenAdj)<br/>wHidden.Add(wHidden, wHiddenAdj)<br/><br/>bHiddenAdj, err := sumAlongAxis(0, dHiddenLayer)<br/>if err != nil {<br/>    return err<br/>}<br/>bHiddenAdj.Scale(nn.config.learningRate, bHiddenAdj)<br/>bHidden.Add(bHidden, bHiddenAdj)</pre>
<p>Note, here we have utilized another helper function that allows us to sum matrices along one dimension while keeping the other dimension intact. This <kbd>sumAlongAxis()</kbd> function is shown in the following code for completeness:</p>
<pre>// sumAlongAxis sums a matrix along a
// particular dimension, preserving the
// other dimension.
func sumAlongAxis(axis int, m *mat.Dense) (*mat.Dense, error) {

    numRows, numCols := m.Dims()

    var output *mat.Dense

    switch axis {
    case 0:
        data := make([]float64, numCols)
        for i := 0; i &lt; numCols; i++ {
            col := mat.Col(nil, i, m)
            data[i] = floats.Sum(col)
        }
        output = mat.NewDense(1, numCols, data)
    case 1:
        data := make([]float64, numRows)
        for i := 0; i &lt; numRows; i++ {
            row := mat.Row(nil, i, m)
            data[i] = floats.Sum(row)
        }
        output = mat.NewDense(numRows, 1, data)
    default:
        return nil, errors.New("invalid axis, must be 0 or 1")
    }

    return output, nil
}</pre>
<p>The last thing that we will do in the <kbd>train()</kbd> method is add the trained weights and biases to the receiver value and return:</p>
<pre>// Define our trained neural network.<br/>nn.wHidden = wHidden<br/>nn.bHidden = bHidden<br/>nn.wOut = wOut<br/>nn.bOut = bOut<br/><br/>return nil</pre>
<p>Awesome! That wasn't so bad. We have a method to train our neural network in about 100 lines of Go code. To check whether this code runs and to get a sense about what our weights and biases might look like, let's train a neural network on some simple dummy data. We will perform a more realistic example with the network in the next section, but for now, let's just sanity check ourselves with some dummy data, as shown in the following code:</p>
<pre>// Define our input attributes.<br/>input := mat.NewDense(3, 4, []float64{<br/>    1.0, 0.0, 1.0, 0.0,<br/>    1.0, 0.0, 1.0, 1.0,<br/>    0.0, 1.0, 0.0, 1.0,<br/>})<br/><br/>// Define our labels.<br/>labels := mat.NewDense(3, 1, []float64{1.0, 1.0, 0.0})<br/><br/>// Define our network architecture and<br/>// learning parameters.<br/>config := neuralNetConfig{<br/>    inputNeurons: 4,<br/>    outputNeurons: 1,<br/>    hiddenNeurons: 3,<br/>    numEpochs: 5000,<br/>    learningRate: 0.3,<br/>}<br/><br/>// Train the neural network.<br/>network := newNetwork(config)<br/>if err := network.train(input, labels); err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Output the weights that define our network!<br/>f := mat.Formatted(network.wHidden, mat.Prefix(" "))<br/>fmt.Printf("\nwHidden = % v\n\n", f)<br/><br/>f = mat.Formatted(network.bHidden, mat.Prefix(" "))<br/>fmt.Printf("\nbHidden = % v\n\n", f)<br/><br/>f = mat.Formatted(network.wOut, mat.Prefix(" "))<br/>fmt.Printf("\nwOut = % v\n\n", f)<br/><br/>f = mat.Formatted(network.bOut, mat.Prefix(" "))<br/>fmt.Printf("\nbOut = % v\n\n", f)</pre>
<p>Compiling and running this neural network training code results in the following output weights and biases:</p>
<pre><strong>bOut$ go build</strong><br/><strong>$ ./myprogram </strong><br/><br/><strong>wHidden = [ 2.105009524680073 2.022752980359344 2.5200192466310547]</strong><br/><strong>          [ -2.033896626042324 -1.8652059633980662 -1.5861504822640748]</strong><br/><strong>          [1.821046795894909 2.436963623058306 1.717510016887383]</strong><br/><strong>          [-0.7400175664445425 -0.5800261988090052 -0.9277709997772002]</strong><br/><br/><br/><strong>bHidden = [ 0.06850421921273529 -0.40252869229501825 -0.03392255287230178]</strong><br/><br/><br/><strong>wOut = [ 2.321901257946553]</strong><br/><strong>       [ 3.343178681830189]</strong><br/><strong>       [1.7581701319375231]</strong><br/><br/><br/><strong>bOut = [-3.471613333924047]</strong></pre>
<p>These weights and biases fully define our trained neural network. That is, <kbd>wHidden</kbd> and <kbd>bHidden</kbd> allow us to translate our inputs into values that are input to the output layer of our network, and <kbd>wOut</kbd> and <kbd>bOut</kbd> allow us to translate those values into the final output of our neural net.</p>
<div class="packt_infobox">Because of our use of random numbers, your program may return slightly different results from those which are described earlier. However, you should see a similar range of values.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Utilizing the simple neural network</h1>
                </header>
            
            <article>
                
<p>Now that we have some neural network training functionality that appears to be working, let's try to utilize this functionality in a more realistic modeling scenario. In particular, let's bring back our favorite classification dataset, the iris flower dataset (utilized in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml">Chapter 5</a>, <em>Classification</em>).</p>
<p>If you remember, when trying to classify iris flowers using this dataset, we are trying to classify them into one of three species (setosa, virginica, or versicolor). As our neural net is expecting matrices of float values, we need to encode the three species into numerical columns. One way to do this is to create a column in our dataset for each species. We will then set that column's values to either <em>1.0</em> or <em>0.0</em> depending on whether the corresponding row's measurements correspond to that species (<em>1.0</em>) or to another species (<em>0.0</em>).</p>
<p>We are also going to standardize our data for reasons similar to those that were discussed in reference to logistic regression in <a href="f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml">Chapter 5</a>, <em>Classification</em>. This makes our dataset look slightly different than it did for previous examples, as you can see here:</p>
<pre><strong>$ head train.csv </strong><br/><strong>sepal_length,sepal_width,petal_length,petal_width,setosa,virginica,versicolor</strong><br/><strong>0.305555555556,0.583333333333,0.118644067797,0.0416666666667,1.0,0.0,0.0</strong><br/><strong>0.25,0.291666666667,0.491525423729,0.541666666667,0.0,0.0,1.0</strong><br/><strong>0.194444444444,0.5,0.0338983050847,0.0416666666667,1.0,0.0,0.0</strong><br/><strong>0.833333333333,0.375,0.898305084746,0.708333333333,0.0,1.0,0.0</strong><br/><strong>0.555555555556,0.208333333333,0.661016949153,0.583333333333,0.0,0.0,1.0</strong><br/><strong>0.416666666667,0.25,0.508474576271,0.458333333333,0.0,0.0,1.0</strong><br/><strong>0.666666666667,0.416666666667,0.677966101695,0.666666666667,0.0,0.0,1.0</strong><br/><strong>0.416666666667,0.291666666667,0.491525423729,0.458333333333,0.0,0.0,1.0</strong><br/><strong>0.5,0.416666666667,0.661016949153,0.708333333333,0.0,1.0,0.0</strong></pre>
<p>We will use an 80/20 training and test split of our data to evaluate the model. The training data will be stored in <kbd>train.csv</kbd> and the testing data will be stored in <kbd>test.csv</kbd>, as shown here:</p>
<pre><strong>$ wc -l *.csv</strong><br/><strong>  31 test.csv</strong><br/><strong> 121 train.csv</strong><br/><strong> 152 total</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the neural network on real data</h1>
                </header>
            
            <article>
                
<p>To train our neural network on this iris data, we need to read in the training data and create two matrices. The first matrix will hold all of the attributes (matrix <kbd>inputs</kbd>), and the second matrix will hold all the labels (matrix <kbd>labels</kbd>). We are going to construct these matrices as follows:</p>
<pre>// Open the training dataset file.<br/>f, err := os.Open("train.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()<br/><br/>// Create a new CSV reader reading from the opened file.<br/>reader := csv.NewReader(f)<br/>reader.FieldsPerRecord = 7<br/><br/>// Read in all of the CSV records<br/>rawCSVData, err := reader.ReadAll()<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// inputsData and labelsData will hold all the<br/>// float values that will eventually be<br/>// used to form our matrices.<br/>inputsData := make([]float64, 4*len(rawCSVData))<br/>labelsData := make([]float64, 3*len(rawCSVData))<br/><br/>// inputsIndex will track the current index of<br/>// inputs matrix values.<br/>var inputsIndex int<br/>var labelsIndex int<br/><br/>// Sequentially move the rows into a slice of floats.<br/>for idx, record := range rawCSVData {<br/><br/>    // Skip the header row.<br/>    if idx == 0 {<br/>        continue<br/>    }<br/><br/>    // Loop over the float columns.<br/>    for i, val := range record {<br/><br/>        // Convert the value to a float.<br/>        parsedVal, err := strconv.ParseFloat(val, 64)<br/>        if err != nil {<br/>            log.Fatal(err)<br/>        }<br/><br/>        // Add to the labelsData if relevant.<br/>        if i == 4 || i == 5 || i == 6 {<br/>            labelsData[labelsIndex] = parsedVal<br/>            labelsIndex++<br/>            continue<br/>        }<br/><br/>        // Add the float value to the slice of floats.<br/>        inputsData[inputsIndex] = parsedVal<br/>        inputsIndex++<br/>    }<br/>}<br/><br/>// Form the matrices.<br/>inputs := mat.NewDense(len(rawCSVData), 4, inputsData)<br/>labels := mat.NewDense(len(rawCSVData), 3, labelsData)</pre>
<p>We can then initialize and train our neural network, similar to how we did with the dummy data. The only difference is that we are going to use three output neurons corresponding to our three output classes. The training process is as follows:</p>
<pre>// Define our network architecture and
// learning parameters.
config := neuralNetConfig{
    inputNeurons:  4,
    outputNeurons: 3,
    hiddenNeurons: 3,
    numEpochs:     5000,
    learningRate:  0.3,
}

// Train the neural network.
network := newNetwork(config)
if err := network.train(inputs, labels); err != nil {
    log.Fatal(err)
}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the neural network</h1>
                </header>
            
            <article>
                
<p>To make predictions with the trained neural network, we will create a <kbd>predict()</kbd> method for the <kbd>neuralNet</kbd> type. This <kbd>predict</kbd> method will take in new inputs and complete and feed the new inputs forward through the network to produce predicted outputs, as shown here:</p>
<pre>// predict makes a prediction based on a trained<br/>// neural network.<br/>func (nn *neuralNet) predict(x *mat.Dense) (*mat.Dense, error) {<br/><br/>    // Check to make sure that our neuralNet value<br/>    // represents a trained model.<br/>    if nn.wHidden == nil || nn.wOut == nil || nn.bHidden == nil || nn.bOut == nil {<br/>        return nil, errors.New("the supplied neural net weights and biases are empty")<br/>    }<br/><br/>    // Define the output of the neural network.<br/>    output := mat.NewDense(0, 0, nil)<br/>github.com/tensorflow/tensorflow/tensorflow/go<br/>    // Complete the feed forward process.<br/>    hiddenLayerInput := mat.NewDense(0, 0, nil)<br/>    hiddenLayerInput.Mul(x, nn.wHidden)<br/>    addBHidden := func(_, col int, v float64) float64 { return v + nn.bHidden.At(0, col) }<br/>    hiddenLayerInput.Apply(addBHidden, hiddenLayerInput)<br/><br/>    hiddenLayerActivations := mat.NewDense(0, 0, nil)<br/>    applySigmoid := func(_, _ int, v float64) float64 { return sigmoid(v) }<br/>    hiddenLayerActivations.Apply(applySigmoid, hiddenLayerInput)<br/><br/>    outputLayerInput := mat.NewDense(0, 0, nil)<br/>    outputLayerInput.Mul(hiddenLayerActivations, nn.wOut)<br/>    addBOut := func(_, col int, v float64) float64 { return v + nn.bOut.At(0, col) }<br/>    outputLayerInput.Apply(addBOut, outputLayerInput)<br/>    output.Apply(applySigmoid, outputLayerInput)<br/><br/>    return output, nil<br/>}</pre>
<p>We can then use this <kbd>predict()</kbd> method on the testing data to evaluate our trained model. Specifically, we will read in the data in <kbd>test.csv</kbd> into two new matrices <kbd>testInputs</kbd> and <kbd>testLabels</kbd>, similar to how we read in <kbd>train.csv</kbd> (as such, we will not include the details here). <kbd>testInputs</kbd> can be supplied to the <kbd>predict()</kbd> method to get our predictions and then we can compare our predictions and <kbd>testLabels</kbd> to calculate an evaluation metric. In this case, we will calculate the accuracy of our model.</p>
<p>One detail that we will take care of as we calculate the accuracy is determining a single output prediction from our model. The network actually produces one output between 0.0 and 1.0 for each of the iris species. We will take the highest of these as the predicted species.</p>
<p>The calculation of accuracy for our model is shown in the following code:</p>
<pre>// Make the predictions using the trained model.<br/>predictions, err := network.predict(testInputs)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Calculate the accuracy of our model.<br/>var truePosNeg int<br/>numPreds, _ := predictions.Dims()<br/>for i := 0; i &lt; numPreds; i++ {<br/><br/>    // Get the label.<br/>    labelRow := mat.Row(nil, i, testLabels)<br/>    var species int<br/>    for idx, label := range labelRow {<br/>        if label == 1.0 {<br/>            species = idx<br/>            break<br/>        }<br/>    }<br/><br/>    // Accumulate the true positive/negative count.<br/>    if predictions.At(i, species) == floats.Max(mat.Row(nil, i, predictions)) {<br/>        truePosNeg++<br/>    }<br/>}<br/><br/>// Calculate the accuracy (subset accuracy).<br/>accuracy := float64(truePosNeg) / float64(numPreds)<br/><br/>// Output the Accuracy value to standard out.<br/>fmt.Printf("\nAccuracy = %0.2f\n\n", accuracy)</pre>
<p>Compiling and running the evaluation yields something similar to the following:</p>
<pre><strong>$ go build</strong><br/><strong>$ ./myprogram </strong><br/><br/><strong>Accuracy = 0.97</strong></pre>
<p>Yeah! That's pretty good. 97% accuracy in our predictions for the iris flower species with our from-scratch neural network. As mentioned earlier, these single hidden layer neural networks are pretty powerful when it comes to a majority of classification tasks, and you can see that the underlying principles are not really that complicated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing deep learning</h1>
                </header>
            
            <article>
                
<p>Although simple neural networks, like the one utilized in the preceding section, are extremely powerful for many scenarios, deep neural network architectures have been applied across industries in recent years on various types of data. These more complicated architectures have been used to beat champions at board/video games, drive cars, generate art, transform images, and much more. It almost seems like you can throw anything at these models and they will do something interesting, but they seem to be particularly well-suited for computer vision, speech recognition, textual inference, and other very complicated and hard-to-define tasks.</p>
<p>We are going to introduce deep learning here and run a deep learning model in Go. However, the application and diversity of deep learning models is huge and growing every day.</p>
<p>There are many books and tutorials on the subject, so if this brief introduction sparks your interest, we recommend that you look into one of the following resources:</p>
<ul>
<li>Hands-On Deep Learning with TensorFlow: <a href="https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow">https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow</a></li>
<li>Deep Learning by Example: <a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example">https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example</a></li>
<li>Deep Learning for Computer Vision: <a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision">https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision</a></li>
</ul>
<p>These resources might not all mention or acknowledge Go but, as you will see in the following example, Go is perfectly capable of applying these techniques and interfacing with things like TensorFlow or H2O.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is a deep learning model?</h1>
                </header>
            
            <article>
                
<p>The deep in deep learning refers to the successive combination and utilization of various neural network structures to form an architecture that is large and complicated. These large and complicated architectures generally require large amounts of data to train, and the resulting structure is very hard to interpret.</p>
<p>To give an example of the scale and complexity of modern deep learning models, take Google's LeNet (<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf</a>) as an example. This model, which can be utilized for object recognition, is depicted here (image courtesy of <a href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html</a>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/9b4148b1-6241-4279-8d3e-9190f72f1e2e.png"/></div>
<p class="mce-root">This structure is hard to digest in and of itself. However, it is made even more impressive when you realize that each of the blocks in this diagram can be a complicated, neural network primitive in and of itself (as shown in the legend).</p>
<p>Generally, deep learning models are constructed by chaining together and, in some cases, rechaining together, one or more of the following structures:</p>
<ul>
<li>A fully connected neural network architecture: This building block might be something similar to what we built previously in this chapter for iris flower detection. It includes a number of neural network nodes arranged in fully connected layers. That is, the layers flow from one to the other and all the nodes are connected from one layer to the next.</li>
<li>A <strong>convolutional neural network</strong> (<strong>ConvNet</strong> or <strong>CNN</strong>): This is a neural network that implements at least one <strong>convolutional layer</strong>. A convolutional layer contains neurons parameterized by a set of weights (also called a convolutional kernel or filter) that is only partially connected to the input data. Most ConvNets include a hierarchical combination of these convolutional layers allows a ConvNet to respond to low level features in the input data.</li>
<li>A <strong>recurrent neural network</strong> and/or <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) <strong>cells</strong>: These components attempt to factor in something like memory into the model. Recurrent neural nets make their output dependent on a sequence of inputs rather than assuming that every input is independent. LSTM cells are related to recurrent neural networks but also try to factor in a longer term sort of memory.</li>
</ul>
<div class="packt_tip">Although we will utilize image data to illustrate deep learning in this chapter, deep learning models using recurrent neural nets and LSTMs are particularly well-adapted at solving text-based and other NLP problems. The memory that is built into these networks allows them to understand what words might come next in a sentence, for example. If you are interested in running a deep learning model that is based on text data, we recommend looking at this example that uses the <kbd>github.com/chewxy/gorgonia</kbd> package:<br/>
<a href="https://github.com/chewxy/gorgonia/tree/master/examples/charRNN">https://github.com/chewxy/gorgonia/tree/master/examples/charRNN.</a></div>
<div class="packt_infobox">Deep learning models are powerful! Especially for tasks like computer vision. However, you should also keep in mind that complicated combinations of these neural net components are also extremely hard to interpret. That is, determining why the model made a certain prediction can be near impossible. This can be a problem when you need to maintain compliance in certain industries and jurisdictions, and it also might inhibit debugging or maintenance of your applications. That being said, there are some major efforts to improve the interpretability of deep learning models. Notable among these efforts is the LIME project:<br/>
<a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime.</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning with Go</h1>
                </header>
            
            <article>
                
<p>There are a variety of options when you are looking to build or utilize deep learning models from Go. This, as with deep learning itself, is an ever changing landscape. However, the options for building, training and utilizing deep learning models in Go are generally as follows:</p>
<ul>
<li><strong>Use a Go package</strong>: There are Go packages that allow you to use Go as your main interface to build and train deep learning models. The most features and developed of these packages is <kbd>github.com/chewxy/gorgonia</kbd>. <span><kbd>github.com/chewxy/gorgonia</kbd> treats Go as a first-class citizen and is written in Go, even if it does make significant usage of <kbd>cgo</kbd> to interface with numerical libraries.</span></li>
<li><strong>Use an API or Go client for a non-Go DL framework</strong>: You can interface with popular deep learning services and frameworks from Go including TensorFlow, MachineBox, H2O, and the various cloud providers or third-party API offerings (such as IBM Watson). TensorFlow and Machine Box actually have Go bindings or SDKs, which are continually improving. For the other services, you may need to interact via REST or even call binaries using <kbd>exec</kbd>.</li>
<li><strong>Use cgo</strong>: Of course, Go can talk to and integrate with C/C++ libraries for deep learning, including the TensorFlow libraries and various libraries from Intel. However, this is a difficult road, and it is only recommended when absolutely necessary.</li>
</ul>
<p>As TensorFlow is by far the most popular framework for deep learning (at the moment), we will briefly explore the second category listed here. However, the Tensorflow Go bindings are under active development and some functionality is quite crude at the moment. The TensorFlow team recommends that if you are going to use a TensorFlow model in Go, you first train and export this model using Python. That pre-trained model can then be utilized from Go, as we will demonstrate in the next section.</p>
<p>There are a number of members of the community working very hard to make Go more of a first-class citizen for TensorFlow. As such, it is likely that the rough edges of the TensorFlow bindings will be smoothed over the coming year. To keep updated on this front, make sure and join the #data-science channel on Gophers Slack (<a href="https://invite.slack.golangbridge.org/">https://invite.slack.golangbridge.org/</a>). This is a frequent topic of conversation there and would be a great place to ask questions and get involved.</p>
<div class="packt_tip">Even though we will explore a quick TensorFlow example here, we highly recommend that you should look into <kbd>github.com/chewxy/gorgonia</kbd>, especially if you are considering doing more custom or extensive modeling in Go. This package is super powerful and is used in production.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up TensorFlow for use with Go</h1>
                </header>
            
            <article>
                
<p>The TensorFlow team has provided some good docs to install TensorFlow and get it ready for usage with Go. These docs can be found at <a href="https://www.tensorflow.org/install/install_go">https://www.tensorflow.org/install/install_go</a>. There are a couple of preliminary steps, but once you have the TensorFlow C libraries installed, you can get the following Go package:</p>
<pre><strong>$ go get github.com/tensorflow/tensorflow/tensorflow/go</strong></pre>
<p>Everything should be good to go if you were able to get <kbd>github.com/tensorflow/tensorflow/tensorflow/go</kbd> without error, but you can make sure that you are ready to use TensorFlow by executing the following tests:</p>
<pre><strong>$ go test github.com/tensorflow/tensorflow/tensorflow/go</strong><br/><strong>ok github.com/tensorflow/tensorflow/tensorflow/go 0.045s</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Retrieving and calling a pretrained TensorFlow model</h1>
                </header>
            
            <article>
                
<p>The model that we are going to use is a Google model for object recognition in images called Inception. The model can be retrieved as follows:</p>
<pre><strong>$ mkdir model</strong><br/><strong>$ cd model</strong><br/><strong>$ wget https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip</strong><br/><strong>--2017-09-09 18:29:03-- https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip</strong><br/><strong>Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.6.112, 2607:f8b0:4009:812::2010</strong><br/><strong>Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.6.112|:443... connected.</strong><br/><strong>HTTP request sent, awaiting response... 200 OK</strong><br/><strong>Length: 49937555 (48M) [application/zip]</strong><br/><strong>Saving to: inception5h.zip</strong><br/><br/><strong>inception5h.zip 100%[=========================================================================================================================&gt;] 47.62M 19.0MB/s in 2.5s </strong><br/><br/><strong>2017-09-09 18:29:06 (19.0 MB/s) - inception5h.zip saved [49937555/49937555]</strong><br/><br/><strong>$ unzip inception5h.zip</strong><br/><strong>Archive: inception5h.zip</strong><br/><strong>  inflating: imagenet_comp_graph_label_strings.txt </strong><br/><strong>  inflating: tensorflow_inception_graph.pb </strong><br/><strong>  inflating: LICENSE</strong></pre>
<p>After unzipping the compressed model, you should see a <kbd>*.pb</kbd> file. This is a <kbd>protobuf</kbd> file that represents a frozen state of the model. Think back to our simple neural network. The network was fully defined by a series of weights and biases. Although more complicated, this model can be defined in a similar way and these definitions are stored in this <kbd>protobuf</kbd> file.</p>
<p>To call this model, we will use some example code from the TensorFlow Go bindings docs--<a href="https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go">https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go</a>. This code loads the model and uses the model to detect and label the contents of a <kbd>*.jpg</kbd> image.</p>
<p>As the code is included in the TensorFlow docs, I will spare the details and just highlight a couple of snippets. To load the model, we perform the following:</p>
<pre>// Load the serialized GraphDef from a file.<br/>modelfile, labelsfile, err := modelFiles(*modeldir)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>model, err := ioutil.ReadFile(modelfile)<br/>if err != nil {<br/>    log.Fatal(err)<br/>} </pre>
<p>Then we load the graph definition of the deep learning model and create a new TensorFlow session with the graph, as shown in the following code:</p>
<pre>// Construct an in-memory graph from the serialized form.<br/>graph := tf.NewGraph()<br/>if err := graph.Import(model, ""); err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// Create a session for inference over graph.<br/>session, err := tf.NewSession(graph, nil)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer session.Close()</pre>
<p>Finally, we can make an inference using the model as follows:</p>
<pre>// Run inference on *imageFile.<br/>// For multiple images, session.Run() can be called in a loop (and<br/>// concurrently). Alternatively, images can be batched since the model<br/>// accepts batches of image data as input.<br/>tensor, err := makeTensorFromImage(*imagefile)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>output, err := session.Run(<br/>    map[tf.Output]*tf.Tensor{<br/>        graph.Operation("input").Output(0): tensor,<br/>    },<br/>    []tf.Output{<br/>        graph.Operation("output").Output(0),<br/>    },<br/>    nil)<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/><br/>// output[0].Value() is a vector containing probabilities of<br/>// labels for each image in the "batch". The batch size was 1.<br/>// Find the most probable label index.<br/>probabilities := output[0].Value().([][]float32)[0]<br/>printBestLabel(probabilities, labelsfile)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection using TensorFlow from Go</h1>
                </header>
            
            <article>
                
<p>The Go program for object detection, as specified in the TensorFlow GoDocs, can be called as follows:</p>
<pre><strong>$ ./myprogram -dir=&lt;path/to/the/model/dir&gt; -image=&lt;path/to/a/jpg/image&gt;</strong></pre>
<p>When the program is called, it will utilize the pretrained and loaded model to infer the contents of the specified image. It will then output the most likely contents of that image along with its calculated probability.</p>
<p>To illustrate this, let's try performing the object detection on the following image of an airplane, saved as <kbd>airplane.jpg</kbd>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="271" width="434" class="alignnone size-full wp-image-590 image-border" src="assets/b9a16e04-62b2-4c65-b75e-ce06878aa534.png"/></div>
<p>Running the TensorFlow model from Go gives the following results:</p>
<pre><strong>$ go build</strong><br/><strong>$ ./myprogram -dir=model -image=airplane.jpg</strong><br/><strong>2017-09-09 20:17:30.655757: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:17:30.655807: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:17:30.655814: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:17:30.655818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:17:30.655822: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>BEST MATCH: (86% likely) airliner</strong></pre>
<p>After some suggestions about speeding up CPU computations, we get a result: <kbd>airliner</kbd>. Wow! That's pretty cool. We just performed object recognition with TensorFlow right from our Go program!</p>
<p>Let try another one. This time, we will use <kbd>pug.jpg</kbd>, which looks like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img height="126" width="165" class="image-border" src="assets/d3701af5-dc11-4561-9bad-135d546807e6.jpg"/></div>
<p>Running our program again with this image gives the following:</p>
<pre><strong>$ ./myprogram -dir=model -image=pug.jpg </strong><br/><strong>2017-09-09 20:20:32.323855: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:20:32.323896: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:20:32.323902: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:20:32.323906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:20:32.323911: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>BEST MATCH: (84% likely) pug</strong></pre>
<p>Success! Not only did the model detect that there was a dog in the picture, it correctly identified that there was a pug dog in the picture.</p>
<p>Let try just one more. As this is a Go book, we cannot resist trying <kbd>gopher.jpg</kbd>, which looks like the following (huge thanks to Renee French, the artist behind the Go gopher):</p>
<div class="CDPAlignCenter CDPAlign"><img height="146" width="157" class="image-border" src="assets/fb3144d9-4aef-4af3-b3f4-cd22b41df16f.jpg"/></div>
<p>Running the model gives the following result:</p>
<pre><strong>$ ./myprogram -dir=model -image=gopher.jpg </strong><br/><strong>2017-09-09 20:25:57.967753: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:25:57.967801: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:25:57.967808: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:25:57.967812: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>2017-09-09 20:25:57.967817: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.</strong><br/><strong>BEST MATCH: (12% likely) safety pin</strong></pre>
<p>Well, I guess we can't win them all. Looks like we need to refactor our model to be able to recognize Go gophers. More specifically, we should probably add a bunch of Go gophers to our training dataset, because a Go gopher is definitely not a safety pin!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p><span>Basic neural networks:</span></p>
<ul>
<li>A quick introduction to neural networks: <a href="https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/">https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/</a></li>
<li>A friendly introduction to deep learning and neural networks: <a href="https://youtu.be/BR9h47Jtqyw">https://youtu.be/BR9h47Jtqyw</a></li>
<li><kbd>github.com/tleyden/neurgo</kbd> docs: <a href="https://godoc.org/github.com/tleyden/neurgo">https://godoc.org/github.com/tleyden/neurgo</a></li>
<li><kbd>github.com/fxsjy/gonn/gonn</kbd> docs: <a href="https://godoc.org/github.com/fxsjy/gonn/gonn">https://godoc.org/github.com/fxsjy/gonn/gonn</a></li>
</ul>
<p>More detailed deep learning resources:</p>
<ul>
<li>Hands-On Deep Learning with TensorFlow - <a href="https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow">https://www.packtpub.com/big-data-and-business-intelligence/hands-deep-learning-tensorflow</a></li>
<li>Deep Learning by Example - <a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example">https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-example</a></li>
<li>Deep Learning for Computer Vision - <a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision">https://www.packtpub.com/big-data-and-business-intelligence/deep-learning-computer-vision</a></li>
</ul>
<p>Deep learning with Go:</p>
<ul>
<li>TensorFlow Go bindings docs: <a href="https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go">https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go</a></li>
<li><kbd>github.com/chewxy/gorgonia</kbd> docs: <a href="https://godoc.org/github.com/chewxy/gorgonia">https://godoc.org/github.com/chewxy/gorgonia</a></li>
<li>MachineBox Go SDK docs: <a href="https://godoc.org/github.com/machinebox/sdk-go">https://godoc.org/github.com/machinebox/sdk-go</a></li>
<li>Calling pretrained models using <kbd>cgo</kbd> example: <a href="https://github.com/gopherdata/dlinfer">https://github.com/gopherdata/dlinfer</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Congratulations! We have gone from parsing data with Go to calling deep learning models from Go. You now know the basics of neural networks and can implement them and utilize them in your Go programs. In the next chapter, we will discuss how to get these models and applications off of your laptops and run them at production scale in data pipelines.</p>


            </article>

            
        </section>
    </body></html>