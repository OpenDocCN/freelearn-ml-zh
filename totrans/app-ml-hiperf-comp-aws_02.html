<html><head></head><body>
		<div id="_idContainer031">
			<h1 id="_idParaDest-36" class="chapter-number"><a id="_idTextAnchor035"/>2</h1>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Data Management and Transfer </h1>
			<p>In <a href="B18493_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">High-Performance Computing Fundamentals</em>, we introduced the concepts of HPC applications, why we need HPC, and its use cases across different industries. Before we begin developing HPC applications, we need to migrate the required data into the cloud. In this chapter, we will uncover some of the challenges in managing and transferring data to the cloud and the ways to mitigate them. We will dive deeper into the services in <strong class="bold">AWS online and offline data transfer services</strong> using which you can securely transfer data to the AWS cloud, while maintaining data integrity and consistency. We will cover different data transfer scenarios and provide guidance on how to select the right service for each one. </p>
			<p>We will cover the following topics in this chapter: </p>
			<ul>
				<li>Importance of data management </li>
				<li>Challenges of moving data into the cloud </li>
				<li>How to securely transfer large amounts of data into the cloud</li>
				<li>AWS online data transfer services </li>
				<li>AWS offline data transfer services</li>
			</ul>
			<p>These topics will help you understand how you can transfer <strong class="bold">Gigabytes</strong> (<strong class="bold">GB</strong>), <strong class="bold">Terabytes</strong> (<strong class="bold">TB</strong>), or <strong class="bold">Petabytes</strong> (<strong class="bold">PB</strong>) of data onto the cloud with minimal disruption, cost, and time involved. </p>
			<p>Let’s get started with data management and its role in HPC applications. </p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Importance of data management</h1>
			<p><strong class="bold">Data management</strong> is the<a id="_idIndexMarker045"/> process of effectively capturing, storing, and collating data created by different applications in your company to make sure it’s accurate, consistent, and available when needed. It includes developing policies and procedures for managing your end-to-end data life cycle. The following are some of the elements of the data life cycle specific to HPC applications, due to which it’s important to have data management policies in place:</p>
			<ul>
				<li>Cleaning and transforming raw data to perform detailed faultless analysis.</li>
				<li>Designing and building data pipelines to automatically transfer data from one system to another. </li>
				<li><strong class="bold">Extracting, Transforming, and Loading</strong> (<strong class="bold">ETL</strong>) data into appropriate data<a id="_idIndexMarker046"/> storage systems such as databases, data warehouses, and object storage or filesystems from disparate data sources. </li>
				<li>Building data <a id="_idIndexMarker047"/>catalogs for storing metadata to make it easier to find and track the data lineage. </li>
				<li>Following policies and procedures as outlined by your data governance model. This also involves conforming to the compliance requirements of the federal and regional authorities of the country where data is being captured and stored. For example, if you are a healthcare organization in California, United States, you would need to follow both federal and state data privacy<a id="_idIndexMarker048"/> laws, including the <strong class="bold">Health Insurance Portability and Accountability Act</strong> (<strong class="bold">HIPAA</strong>) and <a id="_idIndexMarker049"/>California’s health data privacy law, the <strong class="bold">Confidentiality of Medical Information Act</strong> (<strong class="bold">CMIA</strong>). Additionally, you would also need<a id="_idIndexMarker050"/> to follow the <strong class="bold">California Consumer Privacy Act</strong> (<strong class="bold">CCPA</strong>), which came into effect starting January 1, 2020, as it relates to healthcare data. If you are in Europe, you would have to follow the data guidelines governed by the European <a id="_idIndexMarker051"/>Union’s<strong class="bold"> General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>). </li>
				<li>Protecting your data from unauthorized access, while at rest or in transit.</li>
			</ul>
			<p>Now that we have understood the significance of data management in HPC applications, let’s <a id="_idIndexMarker052"/>see some of the challenges of transferring large amounts of data into the cloud.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Challenges of moving data into the cloud</h1>
			<p>In order to<a id="_idIndexMarker053"/> start building HPC applications on the cloud, you need to have data on the cloud, and also think about the various elements of your data life cycle in order to be able to manage data effectively. One way is to write custom code for transferring data, which will be time-consuming and might involve the following challenges:</p>
			<ul>
				<li>Preserving the permissions and metadata of files. </li>
				<li>Making sure that data transfer does not impact other existing applications in terms of performance, availability, and scalability, especially in the case of online data transfer (transferring data over the network). </li>
				<li>Scheduling data transfer for non-business hours to ensure other applications are not impeded.</li>
				<li>In terms of structured data, you might have to think about schema conversion and database migration. </li>
				<li>Maintaining data integrity and validating the transfer. </li>
				<li>Monitoring the status of the data transfer, having the ability to look up the history of previous transfers, and having a retry mechanism in place to ensure successful transfers. </li>
				<li>Making sure there are no duplicates – once the data has been transferred, the system should not trigger the transfer again. </li>
				<li>Protecting data during the transfer, which will include encrypting data both in transit and at rest. </li>
				<li>Ensuring data arrives intact and is not corrupted. You would need a mechanism to check that the data arriving at the destination matches the data read from the source to validate data consistency. </li>
				<li>Last but not least, you would have to manage, version-control, and optimize your data-copying scripts.</li>
			</ul>
			<p>The data transfer and migration services offered by AWS can assist you in securely transferring data to the cloud without you having to write and manage code, helping you overcome these aforementioned challenges. In order to select the right service <a id="_idIndexMarker054"/>based on your business requirement, you first need to build a data transfer strategy. We will discuss the AWS data transfer services in a subsequent section of this chapter. Let’s first understand the items that you need to consider while building your strategy. </p>
			<p>In a nutshell, your data transfer strategy needs to take the following into account in order to move data with minimal disruption, time, and cost:</p>
			<ul>
				<li>What kind of data do you need for developing your HPC application – for example, structured data, unstructured data (such as images and PDF documents), or a combination of both? </li>
				<li>For unstructured<a id="_idIndexMarker055"/> data, which filesystem do <a id="_idIndexMarker056"/>you use for storing your files currently? Is it on <strong class="bold">Network Attached Storage</strong> (<strong class="bold">NAS</strong>) or <strong class="bold">Storage Area Network</strong> (<strong class="bold">SAN</strong>)?</li>
				<li>How much purchased storage is available right now, and for how long will it last based on the rate of growth of your data, before you plan to buy more storage? </li>
				<li>For structured data, which database do you use? </li>
				<li>Are you tied up in database licenses? If yes, when are they due for renewal, and what are the costs of the licenses? </li>
				<li>What is the volume of data that you need to transfer to the cloud? </li>
				<li>What are the other applications that are using this data? </li>
				<li>Do these applications require local access to data? Will there be any performance impact on the existing applications if the data is moved to the cloud? </li>
				<li>What is your network bandwidth? Is it good enough to transfer data over the network? </li>
				<li>How quickly do you need to move your data to the cloud? </li>
			</ul>
			<p>Based on the answers to these questions, you can create your data strategy and select appropriate AWS services that will help you to transfer data with ease and mitigate the<a id="_idIndexMarker057"/> challenges mentioned in the preceding list. To understand it better, let’s move to the next topic and see how to securely transfer large amounts of data into the cloud with a simple example. </p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>How to securely transfer large amounts of data into the cloud</h1>
			<p>To understand <a id="_idIndexMarker058"/>this topic, let’s start with a simple example where you want to build and train a computer vision deep learning model to detect product defects in your manufacturing production line. You have cameras installed on each production line, which capture hundreds of images each day. Each image can be up to 5 MB in size, and you have about 1 TB of data, which is currently stored on-premises in a NAS filesystem that you want to use to train your machine learning model. You have about 1 Gbps of network bandwidth and need to start training your model in 2-4 weeks. There is no impact on other applications if the data is moved to the cloud and no structured data is needed for building the computer vision model. Let’s rearrange this information into the following structure, which will become part of your data <a id="_idIndexMarker059"/>strategy document: </p>
			<ul>
				<li><strong class="bold">Objective</strong>: To transfer 1 TB of image data to the cloud, where the file size can be up to 5 MB. Need to automate the data transfer to copy about 10 GB of images every night to the cloud. Additionally, need to preserve the metadata and file permissions while copying data to the cloud.</li>
				<li><strong class="bold">Timeline</strong>: 2-4 weeks</li>
				<li><strong class="bold">Data type</strong>: Unstructured data – JPG or PNG format image files</li>
				<li><strong class="bold">Dependency</strong>: None</li>
				<li><strong class="bold">Impact on existing applications</strong>: None</li>
				<li><strong class="bold">Network bandwidth</strong>: 1 Gbps</li>
				<li><strong class="bold">Existing storage type</strong>: Network attached storage</li>
				<li><strong class="bold">Purpose of data transfer</strong>: To perform distributed training on a computer vision deep learning model using multiple GPUs</li>
				<li><strong class="bold">Data destination</strong>: Amazon S3, which is secure, durable, and the most cost-effective object storage on AWS for storing large amounts of data</li>
				<li><strong class="bold">Sensitive data</strong>: None, but data should not be available for public access</li>
				<li><strong class="bold">Local data access</strong>: Not required</li>
			</ul>
			<p>Since you have 5 TB of data with a maximum file size of 5 MB to transfer securely to Amazon S3, you can use the AWS DataSync service. It is an AWS online data transfer service to migrate data securely <a id="_idIndexMarker060"/>using a <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) endpoint to avoid your data going through the open internet. We will discuss all the AWS data transfer services in detail in the later sections of this chapter. </p>
			<p>The following architecture visually depicts how the transfer will take place: </p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B18493_02_001.jpg" alt="Figure 2.1 – Data transfer using AWS DataSync with a VPC endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Data transfer using AWS DataSync with a VPC endpoint</p>
			<p>The AWS DataSync agent transfers the data between your local storage, NAS in this case, and AWS. You deploy the agent <a id="_idIndexMarker061"/>in a <strong class="bold">Virtual Machine</strong> (<strong class="bold">VM</strong>) in your on-premises<a id="_idIndexMarker062"/> network, where your data source resides. With this approach, you can <a id="_idIndexMarker063"/>minimize the network overhead while<a id="_idIndexMarker064"/> transferring data using the <strong class="bold">Network File System</strong> (<strong class="bold">NFS</strong>) and <strong class="bold">Server Message Block</strong> (<strong class="bold">SMB</strong>) protocols. </p>
			<p>Let’s take a deeper look into AWS DataSync in the next section.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>AWS online data transfer services</h1>
			<p>Online data transfer <a id="_idIndexMarker065"/>services are out-of-the-box solutions built by AWS for transferring data between on-premises systems and the AWS cloud via the internet. They include the following services:</p>
			<ul>
				<li>AWS DataSync</li>
				<li>AWS Transfer Family</li>
				<li>Amazon S3 Transfer Acceleration</li>
				<li>Amazon Kinesis </li>
				<li>AWS Snowcone</li>
			</ul>
			<p>Let’s look at each of these services in detail to understand the scenarios in which we can use the relevant services. </p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>AWS DataSync</h2>
			<p>AWS <a id="_idIndexMarker066"/>DataSync <a id="_idIndexMarker067"/>helps you overcome the challenges of transferring data from on-premises to AWS storage services and between AWS storage services in a fast and secure fashion. It also enables you to automate or schedule the data transfer to optimize your use of network bandwidth, which might be shared with other applications. You can monitor the data transfer task, add data integrity checks to make sure that the data transfer was successful, and validate that data was not corrupted during the transfer, while preserving the file permissions and associated metadata. DataSync offers integration with multiple filesystems and enables you to transfer data between the following resources:</p>
			<ul>
				<li>On-premises file servers and object storage:<ul><li>NFS file servers</li><li>SMB file <a id="_idIndexMarker068"/>servers</li><li><strong class="bold">Hadoop Distributed File System</strong> (<strong class="bold">HDFS</strong>)</li><li>Self-managed object storage</li></ul></li>
				<li>AWS <a id="_idIndexMarker069"/>storage services:<ul><li><strong class="bold">Snow Family Devices</strong></li><li><strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>) buckets</li><li><strong class="bold">Amazon Elastic File System</strong> (<strong class="bold">EFS</strong>)</li><li><strong class="bold">Amazon FSx for Windows File Server</strong></li><li><strong class="bold">Amazon FSx for Lustre</strong> filesystems</li></ul></li>
			</ul>
			<p class="callout-heading">Important note </p>
			<p class="callout">We will discuss AWS storage services in detail in <a href="B18493_04.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Data Storage</em>.</p>
			<h3>Use cases</h3>
			<p>As discussed, AWS DataSync<a id="_idIndexMarker070"/> is used for transferring data to the cloud over the network. Let’s now see some of the specific use cases for which you can use DataSync: </p>
			<ul>
				<li>Hybrid cloud workloads, where data is generated by on-premises applications and needs to be moved to and from the AWS cloud for processing. This can include HPC applications in healthcare, manufacturing, life sciences, big data analytics in financial services, and research purposes. </li>
				<li>Migrate data rapidly over the network into AWS storage services such as Amazon S3, where you need to make sure that data arrives securely and completely. DataSync has encryption and data integrity during transfer enabled by default. You can also choose to enable additional data verification checks to compare the source and destination data. </li>
				<li>Data archiving, where you want to archive the infrequently accessed data (cold data) directly into durable and long-term storage in the AWS cloud, such as <strong class="bold">Amazon S3 Glacier</strong> or <strong class="bold">S3 Glacier Deep Archive</strong>. This helps you to free <a id="_idIndexMarker071"/>up your on-premises <a id="_idIndexMarker072"/>storage capacity and reduce costs. </li>
				<li>Scheduling a data transfer job to automatically start on a recurring basis at a particular time of the day to optimize network bandwidth usage, which might be shared with other applications. For example, in the life sciences domain, you may want to upload genomic data generated by on-premises applications for processing and training machine learning models on a daily basis. You can both schedule data transfer tasks and monitor them <a id="_idIndexMarker073"/>as required using DataSync. </li>
			</ul>
			<h3>Workings of AWS DataSync</h3>
			<p>We will use an <a id="_idIndexMarker074"/>architecture diagram to show how DataSync can transfer data between on-premises self-managed storage systems to AWS storage services and between AWS storage resources. </p>
			<p>We will start with on-premises storage to AWS storage services. </p>
			<h4>Data transfer from on-premises to AWS storage services</h4>
			<p>The <a id="_idIndexMarker075"/>architecture<a id="_idIndexMarker076"/> in <em class="italic">Figure 2.2</em> depicts <a id="_idIndexMarker077"/>the data transfer from on-premises to AWS storage resources:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B18493_02_002.jpg" alt="Figure 2.2 – Data transfer from on-premises to AWS storage services using AWS DataSync"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Data transfer from on-premises to AWS storage services using AWS DataSync</p>
			<p>The<strong class="bold"> DataSync agent</strong> is a VM that reads from and writes the data to the on-premises storage. You can configure and activate your agent using the DataSync console or API. This process associates your agent with your AWS account. Once the agent is activated, you can create the data transfer task from the console or API to kick start the data transfer. DataSync encrypts and performs a data integrity check during transfer to make sure that data is transferred securely. You can enable additional checks as well to verify the data copied to the destination is the same as that read at<a id="_idIndexMarker078"/> the <a id="_idIndexMarker079"/>source. Additionally, you<a id="_idIndexMarker080"/> can also monitor your data transfer task. The time that DataSync takes to transfer depends on your network bandwidth, the amount of data, and the network traffic. However, a single data transfer task is capable of utilizing a 10-Gbps network link. </p>
			<h4>Data transfer between AWS storage resources</h4>
			<p>Let’s<a id="_idIndexMarker081"/> take a <a id="_idIndexMarker082"/>deeper<a id="_idIndexMarker083"/> look to understand the data transfer between AWS storage resources using AWS DataSync.</p>
			<p>The architecture in <em class="italic">Figure 2.3</em> depicts the data transfer between AWS storage resources using DataSync in the same AWS account. The same architecture applies for data transfers within the same region as well as cross-region: </p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B18493_02_003.jpg" alt="Figure 2.3 – Data transfer between AWS storage resources using AWS DataSync"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Data transfer between AWS storage resources using AWS DataSync</p>
			<p>As shown in the <a id="_idIndexMarker084"/>architecture, DataSync does not use the agent for transferring data between AWS resources in the same account. However, if you want to transfer data between different AWS accounts, then you need to set up and activate the DataSync Amazon EC2 agent in an AWS Region.</p>
			<p>In summary, you can use AWS DataSync for online data transfer from on-premises to AWS storage services, and between AWS storage resources. AWS DataSync transfers data quickly, safely, and in a cost-effective manner while ensuring data integrity and consistency, without the need to write and manage data-copy scripts. </p>
			<p>Now, let’s move on to another AWS data transfer service, AWS Transfer Family, which is used for scaling your recurring business-to-business file transfers to Amazon S3 and Amazon EFS.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>AWS Transfer Family</h2>
			<p>File <a id="_idIndexMarker085"/>transfer <a id="_idIndexMarker086"/>protocols such as <strong class="bold">File Transfer Protocol</strong> (<strong class="bold">FTP</strong>), <strong class="bold">Secure File Transfer Protocol</strong> (<strong class="bold">SFTP</strong>), and <strong class="bold">File Transfer Protocol Secure</strong> (<strong class="bold">FTPS</strong>) are <a id="_idIndexMarker087"/>commonly <a id="_idIndexMarker088"/>used<a id="_idIndexMarker089"/> in business-to-business data exchange workflows across different industries including financial services, healthcare, manufacturing, and retail. AWS Transfer Family helps in scaling and migrating these file workflows to the AWS cloud. It uses the FTP, SFTP, and FTPS protocols for data transfer. It enables you to transfer files to and from Amazon EFS and Amazon S3. </p>
			<h3>Use cases</h3>
			<p>As discussed, AWS Transfer <a id="_idIndexMarker090"/>Family uses protocols such as FTP, SFTP, and FTPS for data exchange workflows in business-to-business contexts. So, let’s understand some of the common use cases for transferring data to and from Amazon S3 and Amazon EFS using AWS Transfer Family:</p>
			<ul>
				<li>Securely transfer files internally within your organization or with third-party vendors. Some industries, including financial services, life sciences, and healthcare, have to make sure to have a secure file transfer workflow in place due to the sensitive nature of the data and to comply with regulations<a id="_idIndexMarker091"/> such as <strong class="bold">Payment Card Industry Data Security Standard</strong> (<strong class="bold">PCI DSS</strong>), HIPPA, or GDPR, depending on their location. </li>
				<li>To distribute subscription-based content to your customers. For example, BluTV is a famous subscription-based video-on-demand service in Turkey, which is available globally and caters to Turkish and Arabic speaking viewers. Previously, they self-managed their SFTP setup on the cloud, and ran into a lot of problems such as managing open source projects for mounting S3 to Amazon EC2 and scaling issues when additional resources were required. After moving their setup to the fully managed AWS Transfer Family for SFTP, they no longer have to monitor their file transfers, manage open source projects, or pay for unused resources.</li>
				<li>For building a central repository of data (also known as a data lake) on AWS for storing both structured and unstructured data coming from disparate data sources and third parties such as vendors or partners. For example, FINRA, a government-authorized non-profit organization that oversees US stockbrokers, has a data lake on Amazon S3 as its central source of data. FINRA uses the AWS Transfer Family for SFTP service to alleviate operational overheads while maintaining a connection to their existing authentication systems for external users to avoid any disruption while<a id="_idIndexMarker092"/> migrating their SFTP services to AWS. </li>
			</ul>
			<p>Now that we have gone over some of the use cases for AWS Transfer Family, let’s see how it works.</p>
			<h3>Workings of AWS Transfer Family</h3>
			<p>The <a id="_idIndexMarker093"/>architecture in <em class="italic">Figure 2.4</em> shows how files are transferred using AWS Transfer Family from on-premises file servers to Amazon S3 or Amazon EFS, which can then be used for downstream file processing workflows such as content distribution, machine learning, and data analysis: </p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B18493_02_004.jpg" alt="Figure 2.4 – File transfer workflow using AWS Transfer Family"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – File transfer workflow using AWS Transfer Family</p>
			<p>You can configure any standard file transfer protocol client such as WinSCP, FileZilla, or OpenSSH to initially transfer to Amazon S3 or EFS using AWS Transfer Family. It will first authenticate the user based on the identity provider type that you have configured, and once the user is authenticated, it will initiate the file transfer. </p>
			<p>So far, we have seen how we can transfer data using AWS DataSync and AWS Transfer Family over the network and understood their use cases and how these services work to<a id="_idIndexMarker094"/> transfer data securely while reducing the operational burden in a cost-effective manner. Let’s now see how we can accelerate the data transfer to S3 using Amazon S3 Transfer Acceleration. </p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Amazon S3 Transfer Acceleration</h2>
			<p><strong class="bold">Amazon S3 Transfer Acceleration</strong> (<strong class="bold">S3TA</strong>) is a<a id="_idIndexMarker095"/> feature<a id="_idIndexMarker096"/> in Amazon S3 buckets that lets you speed up your data transfer to a S3 bucket over long distances, regardless of internet traffic and without the need for any special clients or proprietary network protocols. You can speed up transfers to and from Amazon S3 by 50-500% using the transfer acceleration feature. </p>
			<p>Some of the use cases include the following: </p>
			<ul>
				<li>Time-sensitive transfers of large files, such as lab imagery or media, from distributed locations to your data lake built on Amazon S3 (centralized data repository). </li>
				<li>Web or mobile applications with a file upload or download feature where users are geographically distributed and are far from the destination S3 bucket. S3TA can accelerate this long-distance transfer of files and helps you provide a better user experience. </li>
			</ul>
			<p>It uses Amazon CloudFront’s globally distributed edge locations, AWS backbone networks, and network protocol optimizations to route traffic, which speeds up the transfer, reduces the internet traffic variability, and helps in logically shortening the distance to S3 for remote applications.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">There is an additional charge to use Amazon S3 Transfer Acceleration.</p>
			<p>We have discussed using online data transfer services such as AWS DataSync, AWS Transfer Family, and S3TA for moving data from on-premises storage to AWS storage resources over the network. There might be scenarios where you want to transfer streaming data in real time to the AWS cloud, for example, telemetry<a id="_idIndexMarker097"/> data<a id="_idIndexMarker098"/> from IoT sensors, video for online streaming applications, and so on. For this, we will go deeper into Amazon Kinesis, which is a fully managed streaming service built by AWS.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Amazon Kinesis</h2>
			<p><strong class="bold">Amazon Kinesis</strong> is <a id="_idIndexMarker099"/>a <strong class="bold">fully managed</strong> service <a id="_idIndexMarker100"/>used to collect, process, and analyze streaming data in <strong class="bold">real time at any scale</strong>. Streaming data can include ingesting application logs, audio, video, website clickstreams, or IoT sensor data for deep learning, machine learning, analytics, and other applications. It allows you to perform data analysis as the data arrives in real time, instead of waiting for all the data to be transferred before processing. </p>
			<p>Amazon Kinesis includes the following services: </p>
			<ul>
				<li><strong class="bold">Kinesis Video Streams</strong>: This<a id="_idIndexMarker101"/> is used when you have to securely stream video from connected devices to AWS for applications such as processing, analytics, or machine learning to drive insights in real time. It has a built-in autoscaling mechanism to provision the infrastructure required for ingesting video streams coming from millions of devices. It automatically encrypts the data at rest as well as in transit. It uses Amazon S3 as its underlying storage, allowing you to store and retrieve data reliably. This helps you to develop real-time computer vision applications by integrating it with other fully managed AWS services or using popular open source machine learning frameworks on AWS. <em class="italic">Figure 2.5</em> shows how Kinesis video streams can be used to collect, process, and store video<a id="_idIndexMarker102"/> streams coming from media devices for machine learning, analytics, and playback with integration with other media applications: </li>
			</ul>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B18493_02_005.jpg" alt="Figure 2.5 – Capture, process, and store video streams for machine learning, analytics, and playback"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Capture, process, and store video streams for machine learning, analytics, and playback</p>
			<ul>
				<li><strong class="bold">Kinesis Data Streams</strong>: This is a fully managed serverless service used for securely <a id="_idIndexMarker103"/>streaming data at any scale in a cost-effective manner. You can stream gigabytes of data per second by adjusting your capacity or use it in on-demand mode for automatic scaling and provisioning the underlying infrastructure based on the capacity required by your application. It has built-in integration with other AWS services, and you only pay for what you use. <em class="italic">Figure 2.6</em> shows how Kinesis Data Streams can be used to ingest, process, and store streaming data generated by different <a id="_idIndexMarker104"/>sources to other AWS services to gain insights in real time: </li>
			</ul>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18493_02_006.jpg" alt="Figure 2.6 – Capture data from different sources into Amazon Kinesis Data Streams"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Capture data from different sources into Amazon Kinesis Data Streams</p>
			<ul>
				<li><strong class="bold">Kinesis Data Firehose</strong>: This is used to securely stream data into data lakes built on Amazon<a id="_idIndexMarker105"/> S3, or data warehouses such as Amazon Redshift, into the required formats for further processing or analysis without the need to build data processing pipelines. Let’s look at some of the benefits of using Kinesis Data Firehose: <ul><li>It enables you to create your delivery stream easily by extracting, transforming, and loading streaming data securely at scale, without the need to manage the underlying infrastructure.</li><li>It has built-in autoscaling to provision the resources required by your streaming application, without any continuous management.</li><li>It enables you to transform raw streaming data using either built-in or custom <a id="_idIndexMarker106"/>transformations. It supports converting data into different formats such as Apache Parquet and can dynamically partition data without building any custom processing logic.</li><li>You can enhance your data streams using machine learning models within Kinesis Firehose to analyze and perform inference as the data travels to the destination. It provides enhanced network security by monitoring and creating alerts in real time when there is a potential threat<a id="_idIndexMarker107"/> using <strong class="bold">Security Information and Event Management</strong> (<strong class="bold">SIEM</strong>) tools. </li><li>You can connect with 30+ AWS services and streaming destinations, which are fully integrated with Kinesis Data Firehose.</li></ul></li>
			</ul>
			<p><em class="italic">Figure 2.7</em> shows how Amazon Kinesis Data Firehose can be used for ETL use cases without having to write long lines of code or managing your own infrastructure at scale:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B18493_02_007.jpg" alt="Figure 2.7 – ETL using Amazon Kinesis Data Firehose"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – ETL using Amazon Kinesis Data Firehose</p>
			<ul>
				<li><strong class="bold">Kinesis Data Analytics</strong>: This is used to process data streams in real time with serverless and fully <a id="_idIndexMarker108"/>managed <strong class="bold">Apache Flink</strong> or SQL, from data sources such as Amazon S3, Amazon Kinesis Data Streams, and <strong class="bold">Amazon Managed Apache Kafka</strong> (<strong class="bold">MSK</strong>) (used to ingest and process<a id="_idIndexMarker109"/> streaming data). It can also be used to trigger real-time actions such as anomaly detection from long-running stateful computations based on past data trends. It <a id="_idIndexMarker110"/>has a built-in autoscaling mechanism to match the volume and throughput of your input data stream. You only pay for what you use; there is no minimum fee or<a id="_idIndexMarker111"/> set-up cost associated with it. It helps you to understand your data in real time, for example, by building a leaderboard for your gaming application, analyzing sensor data, log analytics, web clickstream analytics, building a streaming ETL application, or continuously generating metrics for understanding data trends.</li>
			</ul>
			<p><em class="italic">Figure 2.8</em> shows how a typical Kinesis Data Analytics application works. It has three main components: </p>
			<ul>
				<li>An input data streaming source on which to perform real-time analytics</li>
				<li><strong class="bold">Amazon Kinesis Data Analytics Studio Notebook</strong> to analyze streaming data using <a id="_idIndexMarker112"/>SQL queries and Python or Scala programs</li>
				<li>Finally, it stores processed results on a destination service or application, such as Amazon Redshift or Amazon DynamoDB (NoSQL database) and Amazon Kinesis Data Streams:</li>
			</ul>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18493_02_008.jpg" alt="Figure 2.8 – Real-time processing of streaming data using Amazon Kinesis Data Analytics"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Real-time processing of streaming data using Amazon Kinesis Data Analytics</p>
			<p>In this section, we discussed how to transfer and process streaming data to AWS Storage, using Amazon Kinesis. There are some use cases, such as edge computing and<a id="_idIndexMarker113"/> edge storage, for which you can use <strong class="bold">AWS Snowcone</strong>, a portable, rugged, and secure device for edge computing, storage, and data transfer. Next, let’s see how we can transfer data online from AWS Snowcone to AWS. </p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>AWS Snowcone</h2>
			<p>AWS Snowcone<a id="_idIndexMarker114"/> is a small, rugged, and <a id="_idIndexMarker115"/>portable device, used for running edge computing workloads, edge storage, and data transfer. The device weighs about 4.5 lbs (2.1 kg) and has multiple layers of security and encryption. It has 8 TB of storage, while the AWS <a id="_idIndexMarker116"/>Snowcone <strong class="bold">Solid State Drive</strong> (<strong class="bold">SSD</strong>) version provides 14 TB. Some of the common use cases for Snowcone are as follows: </p>
			<ul>
				<li>Healthcare IoT, for transferring critical and sensitive data from emergency medical vehicles to hospitals for processing data faster and reducing the response time to serve the patients in a better way. You can then transfer the data securely to the AWS cloud. </li>
				<li>Industrial IoT, for capturing sensor or machine data, as it can withstand extreme temperatures, vibrations, and humidity found on factory floors where traditional edge devices might not work.</li>
				<li>Capturing and storing sensor data from autonomous vehicles and drones.</li>
			</ul>
			<p>You can transfer terabytes of data from various AWS Snowcone devices over the network using <a id="_idIndexMarker117"/>AWS DataSync, as <a id="_idIndexMarker118"/>discussed in the <em class="italic">AWS DataSync</em> section. </p>
			<p>AWS online data transfer services are helpful when you have to transfer up to terabytes of data over the network to AWS. The time taken to transfer data is dependent on your available network bandwidth and internet traffic. When you have to transfer data from remote locations, or when your network bandwidth is heavily used by existing applications, you would need an alternative mechanism to transfer data offline. Let’s discuss the AWS offline data transfer services in the next section. </p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>AWS offline data transfer services</h1>
			<p>For transferring<a id="_idIndexMarker119"/> up to petabytes of data via offline methods, in a secure and cost-effective fashion, you can <a id="_idIndexMarker120"/>use <strong class="bold">AWS Snow Family devices</strong>. Sometimes, your applications may require enhanced performance at the edge, where you want to process and analyze your data close to the source in order to deliver real-time meaningful insights. This would mean having AWS-managed hardware and software services beyond the AWS cloud. AWS Snow Family can help you to run operations outside of your data center, as well as in remote locations with limited network connectivity. </p>
			<p>It consists of the following devices: </p>
			<ul>
				<li><strong class="bold">AWS Snowcone</strong>: In the <em class="italic">AWS online data transfer services</em> section, we introduced and <a id="_idIndexMarker121"/>discussed how Snowcone can be used for collecting and storing data at the edge and then transferring it to the AWS cloud using AWS DataSync. In cases of limited network bandwidth, you can also use it for offline data transfer by sending the device to an AWS facility. It includes an E Ink shipping label, which also aids in tracking. </li>
				<li><strong class="bold">AWS Snowball</strong>: This is used for migrating data and performing edge computing. It comes with two options: <ul><li>Snowball Edge Compute Optimized has 42 TB of block or Amazon S3-compatible storage with 52 vCPUs and an optional GPU for edge computing use cases such as machine learning, video analysis, and big data processing in environments with intermittent network connectivity, such as industrial and transportation use cases, or extremely remote locations found in defense or military applications. </li><li>Snowball Edge Storage Optimized has 80 TB of usable block or Amazon S3-compatible object storage, with 40 vCPUs for performing computing at the edge. It is primarily used for either local storage or large-scale offline data transfers to the AWS cloud.</li></ul></li>
				<li>AWS Snowmobile is used to migrate up to 100 PB of data in a 45-foot-long shipping container, which is tamper resistant, waterproof, and temperature controlled with multiple layers of logical and physical security. It is ideal for use cases where you have to transfer exabytes or hundreds of petabytes of data, which might occur due to data center shutdowns. You need to order it from the AWS Snow Family console, and it arrives at your site as a network-attached data store that connects to your local network to perform high-speed data transfers. Once the data is moved to the device, it is driven back to the AWS facility where the data is then uploaded to the specified Amazon S3 bucket. To ensure data security in transit and the successful delivery of data to the AWS facility, it comes with fire suppression, encryption, dedicated security personnel, GPS tracking, alarm monitoring, 24/7 video surveillance, and an escort security vehicle. </li>
			</ul>
			<p>Now that we <a id="_idIndexMarker122"/>understand the various offline data transfer options offered by AWS, let’s understand the process for ordering the device. </p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Process for ordering a device from AWS Snow Family</h2>
			<p>To order <a id="_idIndexMarker123"/>AWS<a id="_idIndexMarker124"/> Snowmobile, you need to contact AWS sales support. For Snowcone or Snowball devices, you can follow these steps:</p>
			<ol>
				<li>Log in to the AWS Console and type <strong class="source-inline">AWS Snow Family</strong> in the search bar. Click on <strong class="bold">AWS Snow Family</strong>, which will take you to the AWS Snow Family console as shown in <em class="italic">Figure 2.9</em>: </li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18493_02_009.jpg" alt="Figure 2.9 – AWS Snow Family ﻿console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – AWS Snow Family console</p>
			<ol>
				<li value="2">Click on the <a id="_idIndexMarker125"/>orange <a id="_idIndexMarker126"/>button reading <strong class="bold">Order an AWS Snow Family device</strong>, which opens another screen, as shown in <em class="italic">Figure 2.10</em>. This will provide you with steps to create a job for ordering the device:</li>
			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18493_02_010.jpg" alt="Figure 2.10 – AWS Snow Family – Create new job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – AWS Snow Family – Create new job</p>
			<ol>
				<li value="3">Click on <strong class="bold">Next</strong> to go to <strong class="bold">Step 2</strong>, <strong class="bold">Get started with import to S3 job</strong>, as shown in <em class="italic">Figure 2.11</em>:</li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B18493_02_011.jpg" alt="Figure 2.11 – Getting started with import to S3 job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Getting started with import to S3 job</p>
			<ol>
				<li value="4">Read <a id="_idIndexMarker127"/>the<a id="_idIndexMarker128"/> instructions carefully, check the acknowledgment checkbox, and click <strong class="bold">Next</strong> to go to <strong class="bold">Step 3</strong>, <strong class="bold">Choose your shipping preferences</strong>, as shown in <em class="italic">Figure 2.12</em>:</li>
			</ol>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18493_02_012.jpg" alt="Figure 2.12 – Choose your shipping preferences"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Choose your shipping preferences</p>
			<ol>
				<li value="5">Fill in your <a id="_idIndexMarker129"/>shipping<a id="_idIndexMarker130"/> details, select your preferred shipping speed, and click <strong class="bold">Next</strong> to go to <strong class="bold">Step 4</strong>, <strong class="bold">Choose your job details</strong>, as shown in <em class="italic">Figure 2.13</em>. Make sure to enter a valid address as it will give you an error message if your address is incorrect: </li>
			</ol>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18493_02_013.jpg" alt="Figure 2.13 – Choose your job details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Choose your job details</p>
			<p>On this <a id="_idIndexMarker131"/>screen<a id="_idIndexMarker132"/>, you can select your Snow device, power supply, wireless options for Snowcone, S3 bucket, compute using EC2 instances, and the option to install the AWS IoT Greengrass validated AMI. </p>
			<p>Please note that the S3 bucket will appear as directories on your device, and the data in those directories will be transferred back to S3. If you have selected the AWS IoT Greengrass AMI to run IoT workloads on the device, you also need to select the <strong class="bold">Remote device management </strong>option to open and manage the device remotely with OpsHub or Snowball Client. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">All the options as mentioned in <em class="italic">step 4</em> are not shown in <em class="italic">Figure 2.13</em> – <em class="italic">Choose your job details</em>, but will be present on your console screen.</p>
			<ol>
				<li value="6">After you<a id="_idIndexMarker133"/> have<a id="_idIndexMarker134"/> filled out the job details, click on the <strong class="bold">Next</strong> button to go to <strong class="bold">Step 5</strong>, <strong class="bold">Choose your security preferences</strong>, as shown in <em class="italic">Figure 2.14</em>: </li>
			</ol>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18493_02_014.jpg" alt="Figure 2.14 – Choose your security preferences"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Choose your security preferences</p>
			<ol>
				<li value="7">Select the permission and encryption settings for your job on this screen, which will help you to protect your data while in transit, and then click on <strong class="bold">Next</strong> to go to <strong class="bold">Step 6</strong>, <strong class="bold">Choose your notification preferences</strong>, as shown in <em class="italic">Figure 2.15</em>: </li>
			</ol>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B18493_02_015.jpg" alt="Figure 2.15 – Choose your notification preferences"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Choose your notification preferences</p>
			<ol>
				<li value="8">To receive<a id="_idIndexMarker135"/><a id="_idIndexMarker136"/> email <a id="_idIndexMarker137"/>notifications of your job status changes, you can choose either an <a id="_idIndexMarker138"/>existing <strong class="bold">Simple Notification Service</strong> (<strong class="bold">SNS</strong>) topic or create a new SNS topic. Click on <strong class="bold">Next</strong> to go to <strong class="bold">Step 7</strong>, <strong class="bold">Review and create your job</strong>, as shown in <em class="italic">Figure 2.16</em>:</li>
			</ol>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B18493_02_016.jpg" alt="Figure 2.16 – Review and create your job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Review and create your job</p>
			<ol>
				<li value="9">You can <a id="_idIndexMarker139"/>review <a id="_idIndexMarker140"/>all the details that you have entered from <strong class="bold">Step 1</strong> through <strong class="bold">Step 7</strong> and then click on the <strong class="bold">Create job</strong> button. </li>
				<li>Once the job is created, it will take you to the <strong class="bold">Jobs</strong> screen, as shown in <em class="italic">Figure 2.17</em>, where you can see details of your job including the status: </li>
			</ol>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B18493_02_017.jpg" alt="Figure 2.17 – Snow ﻿Family jobs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Snow Family jobs</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">On the <strong class="bold">Actions</strong> drop-down menu, you also have options to cancel a job, edit a job name, and clone a job.</p>
			<p>In this section, we learned about AWS Snow Family devices to transfer data offline based on our application requirements, network connectivity, available bandwidth, and the location of our data sources. We also discussed how we can use these devices not only for transferring data but also for edge computing. </p>
			<p>One of the most frequently asked questions on this topic is, how do we calculate the time taken to move data to the cloud based on the network speed and available bandwidth? For this, AWS provides a simple formula based on the best-case scenario, which<a id="_idIndexMarker141"/> is<a id="_idIndexMarker142"/> given as follows: </p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B18493_02_F01.jpg" alt=""/>
				</div>
			</div>
			<p>For example, if we have a network connection of 1.544 Mbps, and we want to move 1 TB of data into and out of the AWS cloud, then theoretically the minimum time that it would take to transfer over your network connection at 80% network utilization is 82 days.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Please note that this formula only gives a high-level estimate; the actual time taken might differ based on the variability of network traffic and available bandwidth.</p>
			<p>Let’s now take a brief look at all the topics that we have covered in this chapter.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, we talked about various aspects of data management, including data governance and compliance with the legal requirements of federal and regional authorities of the country where the data resides. We also discussed that in order to build HPC applications on the cloud, we need to have data on the cloud, and looked at the challenges of transferring this data to the cloud. In order to mitigate these challenges, we can use the managed AWS data transfer services, and in order to select which service to use for your application, we then discussed the elements of building a data strategy. </p>
			<p>We then took an example of how we can transfer petabyte-scale data to the cloud in order to understand the concepts involved in a data transfer strategy. Finally, we did a deep dive on various AWS data transfer services for both online and offline data transfer based on your network bandwidth, connectivity, type of application, speed of data transfer, and location of your data source.Now that we understand the mechanisms for transferring data to the cloud, the challenges involved, and how to mitigate them, in the next chapter, we will focus on understanding the various compute options provided by AWS for running HPC applications, and how to optimize these based on the application requirements. </p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Further reading</h1>
			<p>The following are some additional resources for this chapter:</p>
			<ul>
				<li><em class="italic">Migrate Petabyte Scale Data</em>: <a href="https://aws.amazon.com/getting-started/projects/migrate-petabyte-scale-data/">https://aws.amazon.com/getting-started/projects/migrate-petabyte-scale-data/</a></li>
				<li><em class="italic">Introduction to HPC on AWS</em>: <a href="https://d1.awsstatic.com/whitepapers/Intro_to_HPC_on_AWS.pdf">https://d1.awsstatic.com/whitepapers/Intro_to_HPC_on_AWS.pdf</a></li>
				<li><em class="italic">Data management versus data governance</em>: <a href="https://www.tableau.com/learn/articles/data-management-vs-data-governance">https://www.tableau.com/learn/articles/data-management-vs-data-governance</a></li>
				<li><em class="italic">What is DataSync?</em>: <a href="https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html">https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html</a></li>
				<li><em class="italic">S3 Transfer Acceleration</em>: <a href="https://aws.amazon.com/s3/transfer-acceleration/">https://aws.amazon.com/s3/transfer-acceleration/</a></li>
				<li><em class="italic">AWS Transfer Family Customers</em>: <a href="https://aws.amazon.com/aws-transfer-family/customers/">https://aws.amazon.com/aws-transfer-family/customers/</a></li>
				<li><em class="italic">Kinesis Data Streams</em>: <a href="https://aws.amazon.com/kinesis/data-streams/">https://aws.amazon.com/kinesis/data-streams/</a></li>
				<li><em class="italic">Kinesis Video Streams</em>: <a href="https://aws.amazon.com/kinesis/video-streams/">https://aws.amazon.com/kinesis/video-streams/</a></li>
				<li><em class="italic">Kinesis Data Analytics</em>: <a href="https://aws.amazon.com/kinesis/data-analytics/">https://aws.amazon.com/kinesis/data-analytics/</a></li>
				<li><em class="italic">AWS Snowcone</em>: <a href="https://aws.amazon.com/snowcone">https://aws.amazon.com/snowcone</a></li>
				<li><em class="italic">AWS Snow Family</em>: <a href="https://aws.amazon.com/snow/">https://aws.amazon.com/snow/</a></li>
				<li><em class="italic">Cloud Data Migration</em>: <a href="https://aws.amazon.com/cloud-data-migration/">https://aws.amazon.com/cloud-data-migration/</a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer032">
			</div>
		</div>
	</body></html>