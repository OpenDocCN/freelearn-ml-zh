<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer033">
<h1 class="chapter-number" id="_idParaDest-50"><a id="_idTextAnchor051"/>3</h1>
<h1 id="_idParaDest-51"><a id="_idTextAnchor052"/>An Overview of LightGBM in Python</h1>
<p>In the previous chapter, we looked at ensemble learning<a id="_idIndexMarker129"/> methods for decision trees. Both <strong class="bold">bootstrap aggregation</strong> (<strong class="bold">bagging</strong>) and gradient boosting were discussed in detail, with practical examples of how to apply the techniques in scikit-learn. We also showed how <strong class="bold">gradient-boosted decision trees</strong> (<strong class="bold">GBDTs</strong>) are slow to train and may underperform<a id="_idIndexMarker130"/> on <span class="No-Break">some problems.</span></p>
<p>This chapter introduces LightGBM, a gradient-boosting framework that uses tree-based learners. We look at the innovations and optimizations LightGBM makes to the ensemble learning methods. Further details and examples are given for using LightGBM practically via Python. Finally, the chapter includes a modeling example using LightGBM, incorporating more advanced techniques for model validation and <span class="No-Break">parameter optimization.</span></p>
<p>By the end of the chapter, you will have a thorough understanding of the theoretical and practical properties of LightGBM, allowing us to dive deeper into using LightGBM for data science and <span class="No-Break">production systems.</span></p>
<p>The main topics of this chapter are set <span class="No-Break">out here:</span></p>
<ul>
<li><span class="No-Break">Introducing LightGBM</span></li>
<li>Getting started with LightGBM <span class="No-Break">in Python</span></li>
<li>Building <span class="No-Break">LightGBM models</span></li>
</ul>
<h1 id="_idParaDest-52"><a id="_idTextAnchor053"/>Technical requirements</h1>
<p>The chapter includes examples and code excerpts illustrating how to use LightGBM in Python. Complete examples and instructions for setting up a suitable environment for this chapter are available <span class="No-Break">a</span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3"><span class="No-Break">t </span><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter</span></a><span class="No-Break">-3</span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor054"/>Introducing LightGBM</h1>
<p>LightGBM is an open <a id="_idIndexMarker131"/>source, gradient-boosting framework for tree-based ensembles (<a href="https://github.com/microsoft/LightGBM">https://github.com/microsoft/LightGBM</a>). LightGBM focuses on efficiency in speed, memory usage, and improved accuracy, especially for problems with high dimensionality and large <span class="No-Break">data sizes.</span></p>
<p>LightGBM was first introduced in the paper <em class="italic">LightGBM: A Highly Efficient Gradient Boosting Decision </em><span class="No-Break"><em class="italic">Tree</em></span><span class="No-Break"> [1].</span></p>
<p>The efficiency and accuracy of LightGBM are achieved via several technical and theoretical optimizations to the standard ensemble learning methods, particularly GBDTs. Additionally, LightGBM supports distributed training of ensembles with optimizations in network communication and support for GPU-based training of <span class="No-Break">tree ensembles.</span></p>
<p>LightGBM supports many <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) applications: regression, binary and multiclass classification, cross-entropy<a id="_idIndexMarker132"/> loss functions, and ranking <span class="No-Break">via LambdaRank.</span></p>
<p>The LightGBM algorithm<a id="_idIndexMarker133"/> is also very customizable via its hyperparameters. It supports many metrics and features, including <strong class="bold">Dropouts meet Multiple Additive Regression Trees</strong> (<strong class="bold">DART</strong>), bagging (random forests), continuous training, multiple metrics, and <span class="No-Break">early stopping.</span></p>
<p>This section reviews the theoretical and practical optimizations LightGBM utilizes, including a detailed overview of the hyperparameters to control <span class="No-Break">LightGBM features.</span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor055"/>LightGBM optimizations</h2>
<p>At its core, LightGBM implements<a id="_idIndexMarker134"/> the same ensemble algorithms we discussed in the previous chapter. However, LightGBM applies theoretical and technical optimizations to improve performance and accuracy while significantly reducing memory usage. Next, we discuss the most significant optimizations implemented <span class="No-Break">in LightGBM.</span></p>
<h3>Computational complexity in GBDTs</h3>
<p>First, we must understand<a id="_idIndexMarker135"/> where the inefficiency in building GBDTs stems from to understand how LightGBM improves the efficiency<a id="_idIndexMarker136"/> of GBDTs. The most computationally complex part of the GBDT algorithm is training the regression tree for each iteration. More specifically, finding the optimal split is very expensive. Pre-sort-based algorithms are among the most popular methods for finding the best splits [2], [3]. A na√Øve approach requires the data to be sorted by feature for every decision node with algorithmic complexity <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable_v-normal">data</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">√ó</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable_v-normal">feature</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Space"> </span>Pre-sort-based algorithms sort the data once before training, which reduces the complexity when building a decision node to <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">data</span><span class="_-----MathTools-_Math_Base">)</span> [2]. Even with pre-sorting, the complexity is too high for large datasets when finding splits for <span class="No-Break">decision nodes.</span></p>
<h3>Histogram-based sampling</h3>
<p>An alternative approach<a id="_idIndexMarker137"/> to pre-sorting involves building histograms<a id="_idIndexMarker138"/> for continuous features [4]. The continuous<a id="_idIndexMarker139"/> values are added into discrete bins when building these <strong class="bold">feature histograms</strong>. Instead of using the data directly when calculating the splits for decision nodes, we can now use the histogram bins. Constructing the histograms has a complexity of <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">data</span><span class="_-----MathTools-_Math_Base">)</span>. However, the complexity for building a decision node now reduces to <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">bins</span><span class="_-----MathTools-_Math_Base">)</span>, and since the number of bins is much smaller than the amount of data, this significantly speeds up the process of building regression trees, as illustrated in the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<img alt="Figure 3.1 ‚Äì Creating feature histograms from continuous features allows calculating splits for decision nodes using bin boundary values instead of having to sample each data point, significantly reducing the algorithm‚Äôs complexity since #bins &lt;&lt; #data" height="444" src="image/B16690_03_1.jpg" width="948"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 ‚Äì Creating feature histograms from continuous features allows calculating splits for decision nodes using bin boundary values instead of having to sample each data point, significantly reducing the algorithm‚Äôs complexity since #bins &lt;&lt; #data</p>
<p>A secondary optimization that stems from using histograms is ‚Äúhistogram subtraction‚Äù for building the histograms for the leaves. Instead of calculating the histogram for each leaf, we can subtract the leaf‚Äôs neighbor‚Äôs histogram from the parent‚Äôs histogram. Choosing the leaf with the smaller amount of data leads to a smaller <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">data</span><span class="_-----MathTools-_Math_Variable">)</span> complexity for the first leaf and <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">bin</span><span class="_-----MathTools-_Math_Base">)</span> complexity for the second leaf due to <span class="No-Break">histogram subtraction.</span></p>
<p>A third optimization that LightGBM applies using histograms is to reduce the memory cost. Feature pre-sorting requires a supporting data structure (a dictionary) for each feature. No such data structures are required when building histograms, reducing memory costs. Further, since <span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">bins</span> is small, a smaller data type, such as <strong class="source-inline">uint8_t</strong>, can store the training data, reducing <span class="No-Break">memory usage.</span></p>
<p>Detailed information<a id="_idIndexMarker140"/> regarding the algorithms for building<a id="_idIndexMarker141"/> feature histograms is available in the paper <em class="italic">CLOUDS: A decision tree classifier for large </em><span class="No-Break"><em class="italic">datasets</em></span><span class="No-Break"> [4].</span></p>
<h3>Exclusive Feature Bundling</h3>
<p><strong class="bold">Exclusive Feature Bundling</strong> (<strong class="bold">EFB</strong>) is another data-based optimization<a id="_idIndexMarker142"/> that LightGBM applies when working with sparse<a id="_idIndexMarker143"/> data (sparse data is pervasive in high-dimensional datasets). When the feature data is sparse, it‚Äôs common to find that many features are <em class="italic">mutually exclusive</em>, signifying they never present non-zero values simultaneously. Combining these features into a single one is generally safe, given this exclusivity. EFB is illustrated in the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 3.2 ‚Äì Building a feature bundle from two mutually exclusive features" height="624" src="image/B16690_03_2.jpg" width="649"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 ‚Äì Building a feature bundle from two mutually exclusive features</p>
<p>Bundling mutually exclusive features allows building the same feature histograms as from the individual features [1]. The optimization reduces the complexity of building feature histograms from <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">data</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">√ó</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">feature</span><span class="_-----MathTools-_Math_Base">)</span> to <span class="_-----MathTools-_Math_Variable">O</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">data</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">√ó</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">bundle</span><span class="_-----MathTools-_Math_Base">)</span>. For datasets where there are many mutually exclusive features, this dramatically improves performance since <span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">bundle</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">‚â™</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Variable">feature</span>. Detailed algorithms<a id="_idIndexMarker144"/> for, and proof of the correctness of, EFB <a id="_idIndexMarker145"/>are available <span class="No-Break">in [1].</span></p>
<h3>Gradient-based One-Side Sampling</h3>
<p>A final data-based optimization<a id="_idIndexMarker146"/> available in the LightGBM framework is <strong class="bold">Gradient-based One-Side Sampling</strong> (<strong class="bold">GOSS</strong>) [1]. GOSS is a method of discarding training data samples that no longer contribute<a id="_idIndexMarker147"/> significantly to the training process, effectively reducing the training data size and speeding up <span class="No-Break">the process.</span></p>
<p>We can use the gradient calculation of each sample to determine its importance. If the gradient change is small, it implies that the training error was also small, and we can infer that the tree is well fitted to the specific data instance [1]. One option would be to discard all instances with small gradients. However, this changes the distribution of the training data, reducing the tree‚Äôs ability to generalize. GOSS is a method for choosing which instances to keep in the <span class="No-Break">training data.</span></p>
<p>To maintain the data distribution, GOSS is applied <span class="No-Break">as follows:</span></p>
<ol>
<li>The data samples are sorted by the absolute value of <span class="No-Break">their gradients.</span></li>
<li>The top <span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">√ó</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">100</span><span class="_-----MathTools-_Math_Symbol">%</span> instances are then selected (instances with <span class="No-Break">large gradients).</span></li>
<li>A random sample of <span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">√ó</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">100</span><span class="_-----MathTools-_Math_Symbol">%</span> instances is then taken from the rest of <span class="No-Break">the data.</span></li>
<li>A factor is added to the loss function (for these instances) to amplify their influence: <span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">‚àí</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">¬†</span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Variable">¬†</span>, thereby compensating for the underrepresentation of data with <span class="No-Break">small gradients.</span></li>
</ol>
<p>Therefore, GOSS samples a large portion of instances with large gradients and a random portion of instances with small gradients and amplifies the influence of the small gradients when calculating <span class="No-Break">information gain.</span></p>
<p>The downsampling enabled by GOSS can significantly reduce the amount of data processed during<a id="_idIndexMarker148"/> training (and the training<a id="_idIndexMarker149"/> time for the GBDTs), especially<a id="_idIndexMarker150"/> in the case of <span class="No-Break">large</span><span class="No-Break"><a id="_idIndexMarker151"/></span><span class="No-Break"> datasets.</span></p>
<h3>Best-first tree growth</h3>
<p>The most common method for building decision trees is to grow the tree by level (that is, one level at a time). LightGBM uses an alternative approach and grows the tree leaf-wise or best-first. The leaf-wise approach selects an existing leaf with the most significant change in the loss of the tree and builds the tree from there. The downside of this approach is that if the dataset is small, the tree is likely to overfit the data. A maximum depth has to be set to counteract this. However, if the number of leaves to construct is fixed, leaf-wise tree building is shown to outperform level-wise <span class="No-Break">algorithms [5].</span></p>
<h3>L1 and L2 regularization</h3>
<p>LightGBM supports<a id="_idIndexMarker152"/> both L1 and L2 regularization of the objective function when training the regression trees in the ensemble. From <a href="B16690_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><em class="italic">, Introducing Machine Learning</em>, we recall that regularization is a way to control overfitting. In the case of decision trees, simpler, shallow trees <span class="No-Break">overfit less.</span></p>
<p>To support L1 and L2 regularization, we extend the objective function with a regularization term, <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">obj</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal">Œ©</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">w</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span> is the loss function discussed in <a href="B16690_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Ensemble Learning ‚Äì Bagging and Boosting</em>, and <span class="_-----MathTools-_Math_Variable_v-normal">Œ©</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base">)</span> is the regularization function defined over <span class="_-----MathTools-_Math_Variable">w</span>, the leaf scores (the leaf score is the output calculated from the leaf as per <em class="italic">step 2.3</em> in the GBDT algorithm defined in <a href="B16690_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Ensemble Learning ‚Äì Bagging </em><span class="No-Break"><em class="italic">and Boosting</em></span><span class="No-Break">).</span></p>
<p>The regularization term effectively adds a penalty to the objective function, where we aim to penalize more complex trees prone <span class="No-Break">to overfitting.</span></p>
<p>There are multiple definitions for <span class="_-----MathTools-_Math_Variable_v-normal">Œ©</span>. A typical implementation for the terms in decision trees <span class="No-Break">is this:</span></p>
<p><span class="_-----MathTools-_Math_Variable_v-normal">Œ©</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Œ±</span><span class="_-----MathTools-_Math_Base">‚àë</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">¬†</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Œª</span><span class="_-----MathTools-_Math_Base">‚àë</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">¬†</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">¬†</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable">Œ±</span><span class="_-----MathTools-_Math_Base">‚àë</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">|</span> is the L1 regularization term, controlled by the parameter <span class="_-----MathTools-_Math_Variable">Œ±</span><span class="_-----MathTools-_Math_Operator">,</span> <span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">‚â§</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">Œ±</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">‚â§</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span> and <span class="_-----MathTools-_Math_Variable">Œª</span><span class="_-----MathTools-_Math_Base">‚àë</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Space"> </span>is the L2 regularization term, controlled by the <span class="No-Break">parameter </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Œª</span></span><span class="No-Break">.</span></p>
<p>L1 regularization has the effect of driving leaf scores to zero by penalizing leaves with large absolute outputs. <em class="italic">Smaller leaf outputs have a smaller effect on the tree‚Äôs prediction, effectively simplifying </em><span class="No-Break"><em class="italic">the tree</em></span><span class="No-Break">.</span></p>
<p>L2 regularization is similar but has an outsized effect on outliers‚Äô leaves due to taking the square of <span class="No-Break">the output.</span></p>
<p>Finally, when larger trees<a id="_idIndexMarker153"/> are built (trees with more leaves, and therefore a large <span class="_-----MathTools-_Math_Variable">w</span> vector), both sum terms for <span class="_-----MathTools-_Math_Variable_v-normal">Œ©</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Base">)</span> increase, increasing the objective function output. Therefore, <em class="italic">larger trees are penalized</em>, and overfitting <span class="No-Break">is reduced.</span></p>
<h3>Summary of LightGBM optimizations</h3>
<p>In summary, LightGBM improves<a id="_idIndexMarker154"/> upon the standard ensemble algorithms by doing <span class="No-Break">the following:</span></p>
<ul>
<li>Implementing histogram-based sampling of features to reduce the computational cost of finding <span class="No-Break">optimal splits</span></li>
<li>Calculating exclusive feature bundles to reduce the number of features in <span class="No-Break">sparse datasets</span></li>
<li>Applying GOSS to downsample the training data without <span class="No-Break">losing accuracy</span></li>
<li>Building trees leaf-wise to <span class="No-Break">improve accuracy</span></li>
<li>Overfitting can be controlled through L1 and L2 regularization and other <span class="No-Break">control parameters</span></li>
</ul>
<p>In conjunction, the optimizations<a id="_idIndexMarker155"/> improve the computational performance of LightGBM by <strong class="bold">orders of magnitude</strong> (<strong class="bold">OOM</strong>) over the standard GBDT algorithm. Additionally, LightGBM is implemented in C++ with a Python interface, which results in much faster code than Python-based GBDTs, such as <span class="No-Break">in scikit-learn.</span></p>
<p>Finally, LightGBM also has support<a id="_idIndexMarker156"/> for improved data-parallel and feature-parallel distributed training. Distributed training and GPU support are discussed in a later <a href="B16690_11.xhtml#_idTextAnchor177"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><em class="italic">, Distributed and GPU-Based Learning </em><span class="No-Break"><em class="italic">with LightGBM</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>Hyperparameters</h2>
<p>LightGBM exposes many parameters<a id="_idIndexMarker157"/> that can be used to customize<a id="_idIndexMarker158"/> the training process, goals, and performance. Next, we discuss the most notable parameters and how they may be used to control <span class="No-Break">specific phenomena.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The core LightGBM framework is developed in C++ but includes APIs to work with LightGBM in C, Python, and R. The parameters discussed in this section are the framework parameters and are exposed differently by each API. The following section discusses the parameters available when <span class="No-Break">using Python.</span></p>
<p>The following are <strong class="bold">core framework parameters</strong> used to control the optimization<a id="_idIndexMarker159"/> process <span class="No-Break">and goal:</span></p>
<ul>
<li><strong class="source-inline">objective</strong>: LightGBM supports the following optimization objectives, among others‚Äî<strong class="source-inline">regression</strong> (including regression applications with other loss functions such as Huber and Fair), <strong class="source-inline">binary</strong> (classification), <strong class="source-inline">multiclass</strong> (classification), <strong class="source-inline">cross-entropy</strong>, and <strong class="source-inline">lambdarank</strong> for <span class="No-Break">ranking problems.</span></li>
<li><strong class="source-inline">boosting</strong>: The boosting parameter controls the boosting type. By default, this is set to <strong class="source-inline">gbdt</strong>, the standard GBDT algorithm. The other options are <strong class="source-inline">dart</strong> and <strong class="source-inline">rf</strong> for random forests. The random forest mode does not perform boosting but instead builds a <span class="No-Break">random forest.</span></li>
<li><strong class="source-inline">num_iterations</strong> (or <strong class="source-inline">n_estimators</strong>): Controls the number of boosting iterations and, therefore, the number of <span class="No-Break">trees built.</span></li>
<li><strong class="source-inline">num_leaves</strong>: Controls the maximum number of leaves in a <span class="No-Break">single tree.</span></li>
<li><strong class="source-inline">learning_rate</strong>: Controls the learning, or shrinkage rate, which is the contribution of each tree to the <span class="No-Break">overall prediction.</span></li>
</ul>
<p>LightGBM also provides<a id="_idIndexMarker160"/> many parameters to control the learning process. We‚Äôll discuss these parameters relative to how they may be used to tune specific aspects <span class="No-Break">of training.</span></p>
<p>The following control parameters can be used to <span class="No-Break">improve </span><span class="No-Break"><strong class="bold">accuracy</strong></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="source-inline">boosting</strong>: Use <strong class="source-inline">dart</strong>, which has been shown to outperform <span class="No-Break">standard GBDTs.</span></li>
<li><strong class="source-inline">learning_rate</strong>: The learning rate must be tuned alongside <strong class="source-inline">num_iterations</strong> for better accuracy. A small learning rate with a large value for <strong class="source-inline">num_iterations</strong> leads to better accuracy at the expense of <span class="No-Break">optimization speed.</span></li>
<li><strong class="source-inline">num_leaves</strong>: A larger number of leaves improves accuracy but may lead <span class="No-Break">to overfitting.</span></li>
<li><strong class="source-inline">max_bin</strong>: The maximum number of bins in which features are bucketed when constructing histograms. A larger <strong class="source-inline">max_bin</strong> size slows the training and uses more memory but may <span class="No-Break">improve accuracy.</span></li>
</ul>
<p>The following <strong class="bold">learning control parameters</strong> can be used to deal <span class="No-Break">with </span><span class="No-Break"><strong class="bold">overfitting</strong></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="source-inline">bagging_fraction</strong> and <strong class="source-inline">bagging_freq</strong>: Setting<a id="_idIndexMarker161"/> both parameters<a id="_idIndexMarker162"/> enables feature bagging. Bagging may be used in addition to boosting and doesn‚Äôt force the use of a random forest. Enabling bagging <span class="No-Break">reduces overfitting.</span></li>
<li><strong class="source-inline">early_stopping_round</strong>: Enables early stopping and controls the number of iterations used to determine whether training should be stopped. Training is stopped if no improvement is made to any metric in the iterations set <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">early_stopping_round</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">min_data_in_leaf</strong>: The minimum samples allowed in a leaf. Larger values <span class="No-Break">reduce overfitting.</span></li>
<li><strong class="source-inline">min_gain_to_split</strong>: The minimum amount of information gain required to perform a split. Higher values <span class="No-Break">reduce overfitting.</span></li>
<li><strong class="source-inline">reg_alpha</strong>: Controls L1 regularization. Higher values <span class="No-Break">reduce overfitting.</span></li>
<li><strong class="source-inline">reg_lambda</strong>: Controls L2 regularization. Higher values <span class="No-Break">reduce overfitting.</span></li>
<li><strong class="source-inline">max_depth</strong>: Controls the maximum depth of individual trees. Shallower trees <span class="No-Break">reduce overfitting.</span></li>
<li><strong class="source-inline">max_drop</strong>: Controls the maximum number of dropped trees when using the DART algorithm (is only used when <strong class="source-inline">boosting</strong> is set to <strong class="source-inline">dart</strong>). A larger value <span class="No-Break">reduces overfitting.</span></li>
<li><strong class="source-inline">extra_trees</strong>: Enables the <strong class="bold">Extremely Randomized Trees</strong> (<strong class="bold">ExtraTrees</strong>) algorithm. LightGBM then chooses a split<a id="_idIndexMarker163"/> threshold at random for each feature. Enabling Extra-Trees can reduce overfitting. The parameter can be used in conjunction with any <span class="No-Break">boosting mode.</span></li>
</ul>
<p>The parameters discussed<a id="_idIndexMarker164"/> here include only some of the parameters available in LightGBM and focus on improving accuracy and overfitting. A complete list of parameters <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.xhtml"><span id="_idIndexMarker165"/>is available at the following <span class="No-Break">link: </span><span class="No-Break">https://lightgbm.rea</span></a><span class="No-Break">dthedocs.io/en/latest/Parameters.xhtml</span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor057"/>Limitations of LightGBM</h2>
<p>LightGBM is designed<a id="_idIndexMarker166"/> to be more efficient and effective than traditional methods. It is particularly well known for its ability to handle large datasets. However, as with any algorithm or framework, it also has its limitations and potential disadvantages, including <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Sensitive to overfitting</strong>: LightGBM can be sensitive to overfitting, especially with small or noisy datasets. Care should be taken to monitor and control for overfitting when <span class="No-Break">using LightGBM.</span></li>
<li><strong class="bold">Optimal performance requires tuning</strong>: As discussed previously, LightGBM has many hyperparameters that need to be properly tuned to get the best performance from <span class="No-Break">the algorithm.</span></li>
<li><strong class="bold">Lack of representation learning</strong>: Unlike <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) approaches, which excel at learning from raw data, LightGBM requires feature engineering to be applied to the data before learning. Feature engineering is a time-consuming process that requires <span class="No-Break">domain knowledge.</span></li>
<li><strong class="bold">Handling sequential data</strong>: LightGBM is not inherently designed for working with sequential data such as time series. For LightGBM to be used with time-series data, feature engineering needs to be applied to create lagged features and capture <span class="No-Break">temporal dependencies.</span></li>
<li><strong class="bold">Complex interactions and non-linearities</strong>: LightGBM is a decision-tree-driven approach that might be incapable of capturing complex feature interactions and non-linearities. Proper feature engineering needs to be applied to ensure the algorithm <span class="No-Break">models these.</span></li>
</ul>
<p>Although these are potential limitations of using the algorithm, they may not apply to all use cases. LightGBM is often a very effective tool in the right circumstances. As with any model, understanding the trade-offs is vital to making the right choice for <span class="No-Break">your application.</span></p>
<p>In the next session, we look at getting started using the various LightGBM APIs <span class="No-Break">with Python.</span></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor058"/>Getting started with LightGBM in Python</h1>
<p>LightGBM is implemented<a id="_idIndexMarker167"/> in C++ but has official C, R, and Python APIs. This section<a id="_idIndexMarker168"/> discusses the Python APIs that are available for working with LightGBM. LightGBM provides three Python APIs: the standard <strong class="bold">LightGBM</strong> API, the <strong class="bold">scikit-learn</strong> API (which is fully compatible<a id="_idIndexMarker169"/> with other scikit-learn functionality), and a <strong class="bold">Dask</strong> API for working with Dask. Dask<a id="_idIndexMarker170"/> is a parallel computing library discussed in <a href="B16690_11.xhtml#_idTextAnchor177"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><em class="italic">, Distribu</em><a href="https://www.dask.org/"><em class="italic">ted and GPU-Based Lea</em></a><em class="italic">rning with </em><span class="No-Break"><em class="italic">LightGBM</em></span><span class="No-Break"> (</span><a href="https://www.dask.org/"><span class="No-Break">https://www.dask.org/</span></a><span class="No-Break">).</span></p>
<p>Throughout the rest of the book, we mainly use the scikit-learn API for LightGBM, but let‚Äôs first look at the standard <span class="No-Break">Python API.</span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor059"/>LightGBM Python API</h2>
<p>The best way to dive<a id="_idIndexMarker171"/> into the Python API is with a hands-on example. The following are excerpts from a code listing that illustrates the use of the LightGBM Python API. The complete code example is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3</span></a><span class="No-Break">.</span></p>
<p>LightGBM needs to be imported. The import is often abbreviated <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">lgb</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
import lightgbm as lgb</pre>
<p>LightGBM provides a <strong class="source-inline">Dataset</strong> wrapper class to work with data. <strong class="source-inline">Dataset</strong> supports a variety of formats. Commonly, it is used to wrap a <strong class="source-inline">numpy</strong> array or a <strong class="source-inline">pandas</strong> DataFrame. <strong class="source-inline">Dataset</strong> also accepts a <strong class="source-inline">Path</strong> to a CSV, TSV, LIBSVM text file, or LightGBM <strong class="source-inline">Dataset</strong> binary file. When a path is supplied, LightGBM loads the data from <span class="No-Break">the disk.</span></p>
<p>Here, we load our Forest Cover dataset from <strong class="source-inline">sklearn</strong> and wrap the <strong class="source-inline">numpy</strong> arrays in a <span class="No-Break">LightGBM </span><span class="No-Break"><strong class="source-inline">Dataset</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
dataset = datasets.fetch_covtype()
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=179)
training_set = lgb.Dataset(X_train, y_train - 1)
test_set = lgb.Dataset(X_test, y_test - 1)</pre>
<p>We subtract 1 from the <strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong> arrays because the classes supplied by <strong class="source-inline">sklearn</strong> are labeled in the range [1, 7], whereas LightGBM expects zero-indexed class labels in the range [<span class="No-Break">0, 7].</span></p>
<p>We cannot set up the parameters for training. We‚Äôll be using the <span class="No-Break">following parameters:</span></p>
<pre class="source-code">
params = {
¬†¬†¬†¬†'boosting_type': 'gbdt',
¬†¬†¬†¬†'objective': 'multiclass',
¬†¬†¬†¬†'num_classes': '7',
¬†¬†¬†¬†'metric': {'auc_mu'},
¬†¬†¬†¬†'num_leaves': 120,
¬†¬†¬†¬†'learning_rate': 0.09,
¬†¬†¬†¬†'force_row_wise': True,
¬†¬†¬†¬†'verbose': 0
}</pre>
<p>We are using the standard GBDT<a id="_idIndexMarker172"/> as a boosting type and setting the objective to multiclass classification<a id="_idIndexMarker173"/> for seven classes. During training, we are going to capture the <strong class="source-inline">auc_mu</strong> metric. <span class="_-----MathTools-_Math_Variable">AU</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">Œº</span> is a multiclass adaptation of the <strong class="bold">area under the receiver operating characteristic curve</strong> (<strong class="bold">AUC</strong>), as defined by Kleiman and <span class="No-Break">Page [6].</span></p>
<p>We set <strong class="source-inline">num_leaves</strong> and <strong class="source-inline">learning_rate</strong> to reasonable values for the problem. Finally, we specify <strong class="source-inline">force_row_wise</strong> as <strong class="source-inline">True</strong>, a recommended setting for <span class="No-Break">large datasets.</span></p>
<p>LightGBM‚Äôs training function also supports <strong class="bold">callbacks</strong>. A callback is a hook into the training process that is executed each boosting<a id="_idIndexMarker174"/> iteration. To illustrate their purpose, we‚Äôll be using the <span class="No-Break">following callbacks:</span></p>
<pre class="source-code">
metrics = {}
callbacks = [
¬†¬†¬†¬†lgb.log_evaluation(period=15),
¬†¬†¬†¬†lgb.record_evaluation(metrics),
¬†¬†¬†¬†lgb.early_stopping(15),
¬†¬†¬†¬†lgb.reset_parameter(learning_rate=learning_rate_decay(0.09, 0.999))
]</pre>
<p>We use the <strong class="source-inline">log_evaluation</strong> callback with a period of 15, which logs (prints) our metrics to standard output every 15 boosting iterations. We also set a <strong class="source-inline">record_evaluation</strong> callback that captures our evaluation metrics in the <strong class="source-inline">metrics</strong> dictionary. We also specify an <strong class="source-inline">early_stopping</strong> callback, with stopping rounds set to 15. The <strong class="source-inline">early_stopping</strong> callback stops training if no validation metrics improve after the specified number of <span class="No-Break">stopping rounds.</span></p>
<p>Finally, we also use the <strong class="source-inline">reset_parameter</strong> callback to implement <strong class="bold">learning rate decay</strong>. The decay function is defined<a id="_idIndexMarker175"/> <span class="No-Break">as follows:</span></p>
<pre class="source-code">
def learning_rate_decay(initial_lr, decay_rate):
¬†¬†¬†¬†def _decay(iteration):
¬†¬†¬†¬†¬†¬†¬†¬†return initial_lr * (decay_rate ** iteration)
¬†¬†¬†¬†return _decay</pre>
<p>The <strong class="source-inline">reset_parameter</strong> callback takes a function<a id="_idIndexMarker176"/> as input. The function receives the current iteration and returns the parameter value. Learning rate decay is a technique where we decrease the value of the learning rate over time. Learning rate decay improved the overall accuracy achieved. Ideally, we want the initial trees to have a more significant impact on correcting the prediction errors. In contrast, later on, we want to reduce the impact of additional trees and have them make minor adjustments to the errors. We implement a slight exponential decay that reduces the learning rate from 0.09 to 0.078 <span class="No-Break">throughout training.</span></p>
<p>Now, we are ready for training. We use <strong class="source-inline">lgb.train</strong> to train <span class="No-Break">the model:</span></p>
<pre class="source-code">
gbm = lgb.train(params, training_set, num_boost_round=150, valid_sets=test_set, callbacks=callbacks)</pre>
<p>We use 150 boosting rounds (or boosted trees). In conjunction with a lower learning rate, having many boosting rounds should <span class="No-Break">improve accuracy.</span></p>
<p>After training, we can use <strong class="source-inline">lgb.predict</strong> to get predictions for our test set and calculate the <span class="No-Break">F1 score:</span></p>
<pre class="source-code">
y_pred = np.argmax(gbm.predict(X_test, num_iteration=gbm.best_iteration), axis=1)
f1_score(y_test - 1, y_pred, average="macro")</pre>
<p>The LightGBM predict function outputs<a id="_idIndexMarker177"/> an array of activations, one for each class. Therefore, we use <strong class="source-inline">np.argmax</strong> to choose the class with the highest activation as the predicted class. LightGBM also has support for some plotting functions. For instance, we can use <strong class="source-inline">plot_metric</strong> to plot our <span class="_-----MathTools-_Math_Variable">AU</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Variable">Œº</span> results as captured in <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">metrics</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
lgb.plot_metric(metrics, 'auc_mu')</pre>
<p>The results of this are shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 3.3 ‚Äì A plot of the ‚ÄãAU‚Äâ‚ÄãC‚Äã‚ÄØ&lt;?AID d835?&gt;&lt;?AID df41?&gt;‚Äã‚Äã‚Äã metric per training iteration created using lgb.plot_metric" height="731" src="image/B16690_03_3.jpg" width="1050"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 ‚Äì A plot of the <span class="_-----MathTools-_Math_Variable_v-bold-italic">AU</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">C</span><span class="_-----MathTools-_Math_Base">¬†</span><span class="_-----MathTools-_Math_Symbol_Extended">ùùÅ</span> metric per training iteration created using lgb.plot_metric</p>
<p>Running the preceding code should produce a LightGBM GBDT tree with an F1 score of around 0.917, in line with the score the Random Forest and Extra-Trees algorithms achieved in <a href="B16690_02.xhtml#_idTextAnchor036"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Ensemble Learning ‚Äì Bagging and Boosting</em>. However, LightGBM is significantly faster in reaching these accuracies. LightGBM completed the training in just 37 seconds on our hardware: this is 4.5 times faster than running Extra-Trees on the same problem and hardware<a id="_idIndexMarker178"/> and 60-70 times faster than scikit-learn‚Äôs <strong class="source-inline">GradientBoostingClassifier</strong> in <span class="No-Break">our testing.</span></p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor060"/>LightGBM scikit-learn API</h2>
<p>We now take a look<a id="_idIndexMarker179"/> at the scikit-learn Python API for LightGBM. The scikit-learn API provides four classes: <strong class="source-inline">LGBMModel</strong>, <strong class="source-inline">LGBMClassifier</strong>, <strong class="source-inline">LGBMRegressor</strong>, and <strong class="source-inline">LGBMRanker</strong>. Each of these provides the same functionality as the LightGBM Python API, but with the same convenient scikit-learn interfaces we have worked with before. Additionally, the scikit-learn classes are compatible and interoperable with the rest of the <span class="No-Break">scikit-learn ecosystem.</span></p>
<p>Let‚Äôs replicate the previous example using the <span class="No-Break">scikit-learn API.</span></p>
<p>The dataset is loaded precisely as before. The scikit-learn API doesn‚Äôt require wrapping the data in a <strong class="source-inline">Dataset</strong> object. We also don‚Äôt have to zero-index our target classes, as scikit-learn supports any label for <span class="No-Break">the classes:</span></p>
<pre class="source-code">
dataset = datasets.fetch_covtype()
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, random_state=179)</pre>
<p>The scikit-learn API also supports<a id="_idIndexMarker180"/> LightGBM callbacks; as such, we use the same callbacks <span class="No-Break">as before:</span></p>
<pre class="source-code">
metrics = {}
callbacks = [
¬†¬†¬†¬†lgb.log_evaluation(period=15),
¬†¬†¬†¬†lgb.record_evaluation(metrics),
¬†¬†¬†¬†lgb.early_stopping(15),
¬†¬†¬†¬†lgb.reset_parameter(learning_rate=learning_rate_decay(0.09, 0.999))
]</pre>
<p>We then create the <strong class="source-inline">LGBMClassifier</strong> exactly as we would any other scikit-learn model. When creating the classifier, we also set <span class="No-Break">the parameters:</span></p>
<pre class="source-code">
model = lgb.LGBMClassifier(
¬†¬†¬†¬†boosting_type='gbdt',
¬†¬†¬†¬†n_estimators=150,
¬†¬†¬†¬†num_leaves=120,
¬†¬†¬†¬†learning_rate=0.09,
¬†¬†¬†¬†force_row_wise=True
)</pre>
<p>Note that we do not have to specify the number of classes; scikit-learn infers this automatically. We then call <strong class="source-inline">fit</strong> on the model, passing the training and test data along with <span class="No-Break">our callbacks:</span></p>
<pre class="source-code">
model = model.fit(X_train, y_train, eval_set=(X_test, y_test), eval_metric='auc_mu', callbacks=callbacks)</pre>
<p>Finally, we evaluate our model with the F1 score. We don‚Äôt have to use <strong class="source-inline">np.argmax</strong> on the predictions as this is done automatically with the <span class="No-Break">scikit-learn API:</span></p>
<pre class="source-code">
f1_score(y_test, model.predict(X_test), average="macro")</pre>
<p>Overall, we can see that using LightGBM via the scikit-learn API is more straightforward than the standard Python API. The scikit-learn API was also approximately 40% faster than the LightGBM API on our hardware. This section examined the ins and outs of using the various Python APIs<a id="_idIndexMarker181"/> available for LightGBM. The following section looks at training LightGBM models using the <span class="No-Break">scikit-learn API.</span></p>
<h1 id="_idParaDest-60"><a id="_idTextAnchor061"/>Building LightGBM models</h1>
<p>This section provides<a id="_idIndexMarker182"/> an end-to-end example of solving a real-world problem using LightGBM. We provide a more detailed look at data preparation for a problem and explain how to find suitable parameters for our algorithms. We use multiple variants of LightGBM to explore relative performance and compare them against <span class="No-Break">random forests.</span></p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor062"/>Cross-validation</h2>
<p>Before we delve<a id="_idIndexMarker183"/> into solving a problem, we need to discuss a better way of validating<a id="_idIndexMarker184"/> algorithm performance. Splitting the data into two or three subsets is standard practice when training a model. The training data is used to train the model, the validation data is a hold-out set used to validate the data during training, and the test data is used to validate the performance <span class="No-Break">after training.</span></p>
<p>In previous examples, we have done this split only once, building a single training and test to train and validate the model. The issue with this approach is that our model could get ‚Äúlucky.‚Äù If, by chance, our test set closely matches the training data but is not representative of real-world data, we would report a good test error, even though we can‚Äôt be confident of our <span class="No-Break">model‚Äôs performance.</span></p>
<p>An alternative is to do the dataset splitting multiple times and train the model multiple times, once for each split. This approach is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">cross-validation</strong></span><span class="No-Break">.</span></p>
<p>The most common application of cross-validation is <em class="italic">k-fold cross-validation</em>. With k-fold cross-validation, we choose a value, <em class="italic">k</em>, and partition the (shuffled) dataset into <em class="italic">k</em> subsamples (or folds). We then repeat the training process <em class="italic">k</em> times, using a different subset as the validation data and all other subsets as training data. The model‚Äôs performance is calculated as the mean (or median) score across all folds. The following diagram illustrates <span class="No-Break">this process:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 3.4 ‚Äì k-fold cross-validation with k = 3; the original dataset is shuffled and split into 3 equal parts (or folds); training and validation are repeated for each combination of subsampled data, and the average performance is reported" height="604" src="image/B16690_03_4.jpg" width="1113"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 ‚Äì k-fold cross-validation with k = 3; the original dataset is shuffled and split into 3 equal parts (or folds); training and validation are repeated for each combination of subsampled data, and the average performance is reported</p>
<p>Using a high value for <em class="italic">k</em> reduces the chance that the model coincidentally shows good performance and indicates how the model might perform in the real world. However, the entire training process is repeated for each fold, which could be computationally expensive and time-consuming. Therefore, we need to balance the resources available with the need to validate<a id="_idIndexMarker185"/> the model. A typical<a id="_idIndexMarker186"/> value for <em class="italic">k</em> is 5 (the default for scikit-learn), also called <span class="No-Break">5-fold</span><span class="No-Break"><a id="_idIndexMarker187"/></span><span class="No-Break"> cross-validation.</span></p>
<h3>Stratified k-fold validation</h3>
<p>A problem that might<a id="_idIndexMarker188"/> arise with k-fold cross-validation is that, due to chance, a fold may contain samples from only a single class. <strong class="bold">Stratified sampling</strong> solves this issue by preserving the percentage<a id="_idIndexMarker189"/> of samples for each class when creating folds. In this way, each fold has the same distribution of classes as the original dataset. When applied to cross-validation, this technique is called stratified <span class="No-Break">k-fold cross-validation.</span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor063"/>Parameter optimization</h2>
<p><strong class="bold">Parameter optimization</strong>, also called parameter tuning, is the process<a id="_idIndexMarker190"/> of finding good hyperparameters<a id="_idIndexMarker191"/> for the model and training process specific<a id="_idIndexMarker192"/> to the problem being solved. In the previous examples of training models, we have been setting the model and training algorithm‚Äôs parameters based on intuition and minimal experimentation. There is no guarantee that the parameter choices were optimal for the <span class="No-Break">optimization problem.</span></p>
<p>But how might we go about finding the best parameter choices? A na√Øve strategy is to try an extensive range of values for a parameter, find the best value, and then repeat the process for the following<a id="_idIndexMarker193"/> parameter. However, it is frequently the case that parameters are <strong class="bold">co-dependent</strong>. When we change one parameter, the optimal value for another might differ. An excellent example of co-dependence in GBDTs is the number of boosting rounds and the learning rate. Having a small learning rate necessitates more boosting rounds. Therefore, optimizing the learning rate and then, independently, the number of boosting rounds is unlikely<a id="_idIndexMarker194"/> to produce optimal results. <em class="italic">Both parameters must be optimized </em><span class="No-Break"><em class="italic">in unison</em></span><span class="No-Break">.</span></p>
<h3>Grid search</h3>
<p>An approach that accounts<a id="_idIndexMarker195"/> for parameter<a id="_idIndexMarker196"/> co-dependence is grid search. With grid search, a parameter grid is set up. The grid consists of a range of values to try for each parameter we are optimizing. An exhaustive search is then performed, training and validating the model on each possible combination <span class="No-Break">of parameters.</span></p>
<p>Here is an example of a parameter grid for <span class="No-Break">three parameters:</span></p>
<pre class="source-code">
grid = {
¬†¬†¬†¬†'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,5],
¬†¬†¬†¬†'num_rounds': [20, 40, 60, 80, 100],
¬†¬†¬†¬†'num_leaves': [2, 16, 32, 64, 128, 256],
}</pre>
<p>Each parameter is specified with a range of possible values. The previous grid would require 150 trails <span class="No-Break">to search.</span></p>
<p>Since grid search is exhaustive, it has the advantage that it is guaranteed to find the best combination of parameters within the ranges specified. However, the downside to grid search is the cost. Trying each possible combination of parameters is very expensive and quickly becomes intractable for many parameters and large <span class="No-Break">parameter ranges.</span></p>
<p>Scikit-learn provides a utility class to implement grid search and perform cross-validation at the same time. <strong class="source-inline">GridSearchCV</strong> takes a model, a parameter grid, and the number of cross-validation folds as parameters. <strong class="source-inline">GridSearchCV</strong> then proceeds to search the grid for the best parameters, using cross-validation to validate the performance for each combination of parameters. We‚Äôll illustrate the use of <strong class="source-inline">GridSearchCV</strong> in the <span class="No-Break">next section.</span></p>
<p>Parameter optimization is a crucial part of the modeling process. Finding suitable parameters for a model could be the difference between a successful or failed process. However, as discussed previously, parameter optimization is also often enormously expensive regarding time and computational<a id="_idIndexMarker197"/> complexity, necessitating a trade-off between<a id="_idIndexMarker198"/> cost <span class="No-Break">and performance.</span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor064"/>Predicting student academic success</h2>
<p>We now move<a id="_idIndexMarker199"/> on to our example. We build<a id="_idIndexMarker200"/> a model to predict students‚Äô dropout rate based on a range of social and economic factors using LightGBM [7] (<a href="https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success">https://archive-beta.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success</a>). The data is available in CSV format. We start by exploring <span class="No-Break">the data.</span></p>
<h3>Exploratory data analysis</h3>
<p>One of the most fundamental properties<a id="_idIndexMarker201"/> of any dataset is the shape: the rows and columns our data consists of. It‚Äôs also an excellent way to validate that the data read succeeded. Here, our data consists of 4,424 rows and 35 columns. Taking a random sample of the data gives us a sense of the columns and <span class="No-Break">their values:</span></p>
<pre class="source-code">
df = pd.read_csv("students/data.csv", sep=";")
print(f"Shape: {df.shape}")
df.sample(10)</pre>
<p>Next, we can run <strong class="source-inline">df.info()</strong> to see all the columns, their non-null counts, and their <span class="No-Break">data types:</span></p>
<pre class="source-code">
df.info()
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 4424 entries, 0 to 4423
Data columns (total 35 columns):
 #¬†¬†¬†Column¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†Non-Null Count¬†¬†Dtype
---¬†¬†------¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†--------------¬†¬†-----
 0¬†¬†¬†Marital status¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†4424 non-null¬†¬†¬†int64
 1¬†¬†¬†Application mode¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†4424 non-null¬†¬†¬†int64
 2¬†¬†¬†Application order¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†4424 non-null¬†¬†¬†int64
‚Ä¶</pre>
<p>Running the preceding code shows<a id="_idIndexMarker202"/> us that most columns are integer types, except for the <strong class="source-inline">Target</strong> column, with a few floats in between. The <strong class="source-inline">Target</strong> column is listed as type <strong class="source-inline">object</strong>; if we look at the values in the sample, we can see the <strong class="source-inline">Target</strong> column consists of <strong class="source-inline">Graduate</strong>, <strong class="source-inline">Dropout</strong>, and <strong class="source-inline">Enrolled</strong> strings. LightGBM can‚Äôt work with strings as targets, so we‚Äôll map these to integer values before training <span class="No-Break">our models.</span></p>
<p>We can also run <strong class="source-inline">df.describe()</strong> to get a statistical description (mean, standard deviation, min, max, and percentiles) of the values in each column. Calculating descriptive statistics helps check the bounds of the data (not a big problem with working with decision tree models) and check for outliers. For this dataset, there aren‚Äôt any data bounds or <span class="No-Break">outlier concerns.</span></p>
<p>Next, we need to check for duplicated and missing values. We need to drop the rows containing missing values or impute appropriate substitutes if there are any missing values. We can check for missing values using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df.isnull().sum()</pre>
<p>Running the preceding code shows us there are no missing values for <span class="No-Break">this dataset.</span></p>
<p>To locate duplicates, we can run the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df.loc[df.duplicated()]</pre>
<p>There are also no duplicates in the dataset. If there were any duplicated data, we would drop the <span class="No-Break">extra rows.</span></p>
<p>We also need to check the distribution<a id="_idIndexMarker203"/> of the target class to ensure it is balanced. Here, we show a histogram that indicates the target class distribution. We create the histogram using Seaborn‚Äôs <strong class="source-inline">countplot()</strong> method, <span class="No-Break">like so:</span></p>
<pre class="source-code">
sns.countplot(data=df, x='Target')</pre>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 3.5 ‚Äì Distribution of target class in the academic success dataset" height="614" src="image/B16690_03_5.jpg" width="840"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 ‚Äì Distribution of target class in the academic success dataset</p>
<p>Although not perfectly balanced, the target distribution is not overly skewed to any one class, and we don‚Äôt have to perform any <span class="No-Break">compensating action.</span></p>
<p>So far, we have found<a id="_idIndexMarker204"/> that our dataset is suitable for modeling (we still need to remap <strong class="source-inline">Target</strong>) and clean (it does not contain missing or duplicated values and is well balanced). We can now take a deeper look at some features, starting with feature correlation. The following code plots a correlation heatmap. Pairwise Pearson correlations are calculated using <strong class="source-inline">df.corr()</strong>. The screenshot that follows the snippet shows a correlation heatmap built using pairwise <span class="No-Break">Pearson correlations:</span></p>
<pre class="source-code">
sns.heatmap(df.corr(), cmap='coolwarm')</pre>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 3.6 ‚Äì Pairwise Pearson feature correlation of the academic success dataset" height="920" src="image/B16690_03_6.jpg" width="1054"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 ‚Äì Pairwise Pearson feature correlation of the academic success dataset</p>
<p>We can see three patterns<a id="_idIndexMarker205"/> of correlations: first-semester credits, enrollments, evaluations, and approvals are all correlated. First-semester and second-semester values for these are also correlated. These correlations imply that students tend to see through the year once enrolled instead of dropping out mid-semester. Although correlated, the correlations aren‚Äôt strong enough to consider dropping <span class="No-Break">any features.</span></p>
<p>The third correlation pattern is between <strong class="source-inline">Nacionality</strong> and <strong class="source-inline">International</strong>, which are <span class="No-Break">strongly correlated.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The word <em class="italic">Nacionality</em> refers to <em class="italic">nationality</em>. We have retained the spelling from the original dataset here too for the purpose <span class="No-Break">of consistency.</span></p>
<p>A closer look at <strong class="source-inline">Nacionality</strong> shows that almost all rows<a id="_idIndexMarker206"/> have a single value: the country where the dataset was collected. The strong correlation implies the same <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">International</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
nationalities = df.groupby(['Nacionality', 'Target']).size().reset_index().pivot(columns='Target', index='Nacionality', values=0)
nationalities_total = nationalities.sum(axis=1)
nationalities_total = nationalities_total.sort_values(ascending=True)
nationalities.loc[nationalities_total.index].plot(kind='barh', stacked=True)</pre>
<p>The following screenshot shows a stacked bar plot of <span class="No-Break">the nationalities:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<img alt="Figure 3.7 ‚Äì Distribution of the ‚ÄòNacionality‚Äô feature, showing almost all rows have a single value in the academic success dataset" height="768" src="image/B16690_03_7.jpg" width="1064"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 ‚Äì Distribution of the ‚ÄòNacionality‚Äô feature, showing almost all rows have a single value in the academic success dataset</p>
<p>The distribution of <strong class="source-inline">'Nacionality'</strong> and <strong class="source-inline">'International'</strong> means that they are not very informative (nearly all rows have the same value), so we can drop them from <span class="No-Break">the dataset.</span></p>
<p>Finally, we notice the <strong class="source-inline">'Gender'</strong> feature. When working<a id="_idIndexMarker207"/> with gender, it‚Äôs always good to check for bias. We can visualize the distribution of the <strong class="source-inline">'Gender'</strong> feature relative to the target classes using a histogram. The results are shown in the screenshot that follows this <span class="No-Break">code snippet:</span></p>
<pre class="source-code">
sns.countplot(data=df, x='Gender', hue='Target', hue_order=['Dropout', 'Enrolled', 'Graduate'])
plt.xticks(ticks=[0,1], labels=['Female','Male'])</pre>
<div>
<div class="IMG---Figure" id="_idContainer032">
<img alt="Figure 3.8 ‚Äì Distribution of the ‚ÄòGender‚Äô feature in the academic success dataset" height="799" src="image/B16690_03_8.jpg" width="1096"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 ‚Äì Distribution of the ‚ÄòGender‚Äô feature in the academic success dataset</p>
<p>There is a slight bias toward<a id="_idIndexMarker208"/> female students, but not enough to <span class="No-Break">warrant concern.</span></p>
<h3>Modeling</h3>
<p>We can now prepare<a id="_idIndexMarker209"/> our dataset for modeling. We must map our <strong class="source-inline">Target</strong> values to integers and drop the <strong class="source-inline">Nacionality</strong> and <strong class="source-inline">International</strong> features. We also need to remove the spaces in the feature names. LightGBM cannot work with spaces in the names; we can replace them <span class="No-Break">with underscores:</span></p>
<pre class="source-code">
df.columns = df.columns.str.strip().str.replace(' ', '_')
df = df.drop(columns=["Nacionality", "International"], axis=1)
df["Target"]=df["Target"].map({
¬†¬†¬†¬†"Dropout":0,
¬†¬†¬†¬†"Enrolled":1,
¬†¬†¬†¬†"Graduate":2
})
X = df.drop(columns=["Target"], axis=1)
y = df["Target"]</pre>
<p>We train and compare four models: a LightGBM GBDT, a LightGBM DART tree, a LightGBM DART tree with GOSS, and a scikit-learn <span class="No-Break">random forest.</span></p>
<p>We‚Äôll perform parameter optimization<a id="_idIndexMarker210"/> with 5-fold cross-validation using <strong class="source-inline">GridSearchCV</strong> to ensure good performance for <span class="No-Break">the models.</span></p>
<p>The following code sets up the parameter optimization for the GBDT. A similar pattern is followed for the other models, which can be seen in the <span class="No-Break">source code:</span></p>
<pre class="source-code">
def gbdt_parameter_optimization():
¬†¬†¬†¬†params = {
¬†¬†¬†¬†¬†¬†¬†¬†"max_depth": [-1, 32, 128],
¬†¬†¬†¬†¬†¬†¬†¬†"n_estimators": [50, 100, 150],
¬†¬†¬†¬†¬†¬†¬†¬†"min_child_samples": [10, 20, 30],
¬†¬†¬†¬†¬†¬†¬†¬†"learning_rate": [0.001, 0.01, 0.1],
¬†¬†¬†¬†¬†¬†¬†¬†"num_leaves": [32, 64, 128]
¬†¬†¬†¬†}
¬†¬†¬†¬†model = lgb.LGBMClassifier(force_row_wise=True, boosting_type="gbdt", verbose=-1)
¬†¬†¬†¬†grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, verbose=10)
¬†¬†¬†¬†grid_search.fit(X, y)
¬†¬†¬†¬†return grid_search
results = gbdt_parameter_optimization()
print(results.best_params_)
print(results.best_score_)</pre>
<p>Running the preceding code takes some time, but once completed, it prints the best parameters found along with the score of the <span class="No-Break">best model.</span></p>
<p>After all the models<a id="_idIndexMarker211"/> are trained, we can evaluate each using F1-scoring, taking the mean of 5-fold cross-validation, using the best parameters found. The following code illustrates how to do this for the <span class="No-Break">GBDT model:</span></p>
<pre class="source-code">
model = lgb.LGBMClassifier(force_row_wise=True, boosting_type="gbdt", learning_rate=0.1, max_depth=-1, min_child_samples=10, n_estimators=100, num_leaves=32, verbose=-1)
scores = cross_val_score(model, X, y, scoring="f1_macro")
scores.mean()</pre>
<p>Jupyter notebooks for<a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3"> the parameter optimization for each model are available in the GitHub <span class="No-Break">repository: </span><span class="No-Break">https://github.com/Pack</span></a><span class="No-Break">tPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-3</span><span class="No-Break">.</span></p>
<p>The following table summarizes the best parameter values found and the cross-validated F1 scores for <span class="No-Break">each model:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Model</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Learning </strong><span class="No-Break"><strong class="bold">Rate</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Max </strong><span class="No-Break"><strong class="bold">Depth</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Min Child </strong><span class="No-Break"><strong class="bold">Samples</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">N </strong><span class="No-Break"><strong class="bold">Estimators</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Num </strong><span class="No-Break"><strong class="bold">Leaves</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Min </strong><span class="No-Break"><strong class="bold">Samples Leaf</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Min </strong><span class="No-Break"><strong class="bold">Samples Split</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">F1 score</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">GBDT</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.1</span></p>
</td>
<td class="No-Table-Style">
<p>-</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">10</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">100</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">32</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.716</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">DART</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">128</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">30</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">150</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">128</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.703</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>DART (<span class="No-Break">GOSS)</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">128</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">30</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">150</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">128</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.703</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Random <span class="No-Break">Forest</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">150</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">N/A</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">10</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">20</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.665</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1 ‚Äì Summary of best parameters found for each model with the corresponding F1 scores</p>
<p>As we can see from the table, the LightGBM models performed much better than the scikit-learn random forest. Both DART models achieved nearly the same F1 score, with GOSS having a slightly lower F1 score (the table values are rounded to <span class="No-Break">3 digits).</span></p>
<p>This concludes our end-to-end example of exploring a dataset and building an optimized model for the dataset (using parameter grid search). We look at more complicated datasets in the coming chapters<a id="_idIndexMarker212"/> and delve deeper into analyzing <span class="No-Break">model performance.</span></p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor065"/>Summary</h1>
<p>This chapter introduced LightGBM as a library to train boosted machines efficiently. We looked at where the complexity of building GBDTs comes from and the features in LightGBM that address them, such as histogram-based sampling, feature bundling, and GOSS. We also reviewed LightGBM‚Äôs most <span class="No-Break">important hyperparameters.</span></p>
<p>We also gave a detailed overview of using LightGBM in Python, covering both the LightGBM Python API and the scikit-learn API. We then built our first tuned models using LightGBM to predict student academic performance, utilizing cross-validation and grid-search-based <span class="No-Break">parameter optimization.</span></p>
<p>In the next chapter, we compare LightGBM against another popular gradient-boosting library, XGBoost, and DL techniques for <span class="No-Break">tabular data.</span></p>
<h1 id="_idParaDest-65"><a id="_idTextAnchor066"/>References</h1>
<table class="No-Table-Style" id="table002-2">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">1]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye and T.-Y. Liu, ‚ÄúLightGBM: A Highly Efficient Gradient Boosting Decision Tree,‚Äù in Advances in Neural Information Processing </em><span class="No-Break"><em class="italic">Systems, 2017.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">2]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">M. Mehta, R. Agrawal and J. Rissanen, ‚ÄúSLIQ: A fast scalable classifier for data mining,‚Äù in Advances in Database Technology‚ÄîEDBT‚Äô96: 5th International Conference on Extending Database Technology Avignon, France, March 25-29, 1996 Proceedings </em><span class="No-Break"><em class="italic">5, 1996.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">3]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">J. Shafer, R. Agrawal, M. Mehta and others, ‚ÄúSPRINT: A scalable parallel classifier for data mining,‚Äù in </em><span class="No-Break"><em class="italic">Vldb, 1996.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">4]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">S. Ranka and V. Singh, ‚ÄúCLOUDS: A decision tree classifier for large datasets,‚Äù in Proceedings of the 4th Knowledge Discovery and Data Mining </em><span class="No-Break"><em class="italic">Conference, 1998.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">5]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">H. Shi, ‚ÄúBest-first decision tree </em><span class="No-Break"><em class="italic">learning,‚Äù 2007.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">6]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">R. Kleiman and D. Page, ‚ÄúAucŒº: A performance metric for multi-class machine learning models,‚Äù in International Conference on Machine </em><span class="No-Break"><em class="italic">Learning, 2019.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">7]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">V. Realinho, J. Machado, L. Baptista and M. V. Martins, Predicting student dropout and academic success, </em><span class="No-Break"><em class="italic">Zenodo, 2021.</em></span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div></body></html>