- en: Introduction to Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习（ML）简介
- en: Let's begin our journey by peering into the future and envision how we'll see
    ourselves interacting with computers. Unlike today's computers, where we are required
    to continuously type in our emails and passwords to access information, the computers
    of the future will easily be able to recognize us by our face, voice, or activity.
    Unlike today's computers, which require step-by-step instructions to perform an
    action, the computer of the future will anticipate our intent and provide a natural
    way for us to converse with it, similar to how we engage with other people, and
    then proceed to help us achieve our goal. Our computer will not only assist us
    but also be our friend, our doctor, and so on. It could deliver our groceries
    at the door and be our interface with an increasingly complex and information-rich
    physical world.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从展望未来开始，设想我们将如何与计算机互动。与今天需要我们不断输入电子邮件和密码才能访问信息的计算机不同，未来的计算机将能够通过我们的面部、声音或活动轻松地识别我们。与今天需要逐步指令才能执行操作的计算机不同，未来的计算机将能够预测我们的意图，并为我们提供一种自然的方式与它进行对话，就像我们与其他人互动一样，然后继续帮助我们实现目标。我们的计算机不仅会帮助我们，还会成为我们的朋友、医生等等。它可以在门口递送我们的杂货，并成为我们与日益复杂和充满信息丰富的物理世界之间的接口。
- en: What is exciting about this vision is that it is no longer in the realm of science
    fiction but an emergent reality. One of the major drivers of this is the progress
    and adoption of **machine learning** (**ML**) techniques, a discipline that gives
    computers the perceptual power of humans, thus giving them the ability to see,
    hear, and make sense of the world—physical and digital.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个愿景令人兴奋的是，它不再属于科幻领域，而是一个正在出现的现实。推动这一进展的主要因素之一是机器学习（ML）技术的进步和采用，这是一门赋予计算机人类感知能力的学科，从而赋予它们看到、听到和理解世界——无论是物理世界还是数字世界——的能力。
- en: But despite all the great progress over the last 3-4 years, most of the ideas
    and potential are locked away in research projects and papers rather than being
    in the hands of the user. So it's the aim of this book to help developers understand
    these concepts better. It will enable you to put them into practice so that we
    can arrive at this future—a future where computers augment us, rather than enslave
    us due to their inability to understand our world.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在过去3-4年中取得了巨大的进步，但大多数想法和潜力都锁在研究项目和论文中，而不是用户手中。因此，本书的目标是帮助开发者更好地理解这些概念。它将使您能够将它们付诸实践，以便我们能够到达这个未来——一个计算机增强我们而不是由于它们无法理解我们的世界而奴役我们的未来。
- en: Because of the constraint of Core ML—it being only able to perform inference—this
    book differs vastly from other ML books, in the sense that the core focus is on
    the application of ML. Specifically we'll focus on computer vision applications
    rather than the details of ML. But in order to better enable you to take full
    advantage of ML, we will spend some time introducing the associated concepts with
    each example.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Core ML的限制——它只能执行推理——本书与其他机器学习（ML）书籍有很大不同，核心关注点是机器学习的应用。具体来说，我们将专注于计算机视觉应用而不是机器学习的细节。但为了更好地让您充分利用机器学习（ML），我们将花一些时间介绍每个示例相关的概念。
- en: 'And before jumping into the hands-on examples, let''s start from the beginning
    and build an appreciation for what ML is and how it can be applied. In this chapter
    we will:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在动手实践示例之前，让我们从头开始，培养对机器学习（ML）是什么以及如何应用它的欣赏。在本章中，我们将：
- en: Start by introducing ML. We'll learn how it differs from classical programming
    and why you might choose it.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，介绍机器学习（ML）。我们将了解它与经典编程的不同之处以及为什么您可能会选择它。
- en: Look at some examples of how ML is being used today, along with the type of
    data and ML algorithm being used.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看一看机器学习（ML）今天的一些应用示例，以及所使用的数据类型和机器学习（ML）算法。
- en: Finally, present the typical workflow for ML projects.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，介绍机器学习（ML）项目的典型工作流程。
- en: Let's kick off by first discussing what ML is and why everyone is talking about
    it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从首先讨论机器学习（ML）是什么以及为什么每个人都谈论它开始。
- en: What is machine learning?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习（ML）？
- en: ML is a subfield of **Artificial Intelligence** (**AI**), a topic of computer
    science born in the 1950s with the goal of trying to get computers to think or
    provide a level of automated intelligence similar to that of us humans.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能（AI）的一个子领域，人工智能（AI）是20世纪50年代诞生的计算机科学话题，其目标是试图让计算机思考或提供与我们人类相似水平的自动化智能。
- en: Early success in AI was achieved by using an extensive set of defined rules,
    known as **symbolic AI**, allowing expert decision making to be mimicked by computers. This
    approach worked well for many domains but had a big shortfall in that in order
    to create an expert, you needed one. Not only this, but also their expertise needed
    to be digitized somehow, which normally required explicit programming.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 早期在人工智能领域取得的成功是通过使用一套广泛的定义规则实现的，这些规则被称为**符号人工智能**，允许专家决策通过计算机来模仿。这种方法在许多领域都取得了良好的效果，但有一个很大的不足，那就是为了创建一个专家，你需要一个专家。不仅如此，他们的专业知识还需要以某种方式数字化，这通常需要明确的编程。
- en: ML provides an alternative; instead of having to handcraft rules, it learns
    from examples and experience. It also differs from classical programming in that
    it is probabilistic as opposed to being discrete. That is, it is able to handle
    fuzziness or uncertainty much better than its counterpart, which will likely fail
    when given an ambiguous input that wasn't explicitly identified and handled.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习提供了一种替代方案；我们不需要手动编写规则，它从示例和经验中学习。它还与经典编程不同，它是概率性的，而不是离散的。也就是说，它能够比其对应物更好地处理模糊性或不确定性，当给出一个未明确识别和处理的模糊输入时，其对应物可能会失败。
- en: I am going to borrow an example used by Google engineer Josh Godron in an introductory
    video to ML to better highlight the differences and value of ML.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将借用谷歌工程师Josh Godron在机器学习入门视频中使用的例子，以更好地突出机器学习的差异和价值。
- en: 'Suppose you were given the task of classifying apples and oranges. Let''s first
    approach this using what we will call classical programming:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被分配了一个对苹果和橙子进行分类的任务。让我们首先使用我们称之为经典编程的方法来处理这个问题：
- en: '![](img/3089eaf2-9ce4-4d02-b02b-c428c5edfb14.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3089eaf2-9ce4-4d02-b02b-c428c5edfb14.png)'
- en: 'Our input is an array of pixels for each image, and for each input, we will
    need to explicitly define some rules that will be able to distinguish an apple
    from an orange. Using the preceding examples, you can solve this by simply counting
    the number of orange and green pixels. Those with a higher ratio of green pixels
    would be classified as an apple, while those with a higher ratio of orange pixels
    would be classified as an orange. This works well with these examples but breaks
    if our input becomes more complex:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入是每个图像的像素数组，对于每个输入，我们需要明确定义一些规则，这些规则能够区分苹果和橙子。使用前面的例子，你可以通过简单地计算橙色和绿色像素的数量来解决这个问题。绿色像素比例较高的将被分类为苹果，而橙色像素比例较高的将被分类为橙子。这对于这些例子来说效果很好，但如果我们的输入变得更加复杂，这种方法就会失效：
- en: '![](img/77a2450a-0737-42d0-8f01-32e032465cee.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/77a2450a-0737-42d0-8f01-32e032465cee.png)'
- en: 'The introduction of new images means our simple color-counting function can
    no longer sufficiently differentiates our apples from our oranges, or even classify
    apples. We are required to reimplement the function to handle the new nuances
    introduced. As a result, our function grows in complexity and becomes more tightly
    coupled to the inputs and less likely able to generalize to other inputs. Our
    function might resemble something like the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 新图像的引入意味着我们简单的颜色计数函数再也无法充分区分我们的苹果和橙子，甚至无法对苹果进行分类。我们需要重新实现该函数以处理引入的新细微差别。因此，我们的函数变得更加复杂，并且与输入更加紧密地耦合，不太可能泛化到其他输入。我们的函数可能类似于以下内容：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This function can be considered our model, which models the relationship of
    the inputs with respect to their labels (apple or orange), as illustrated in the
    following diagram:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以被认为是我们的模型，它描述了输入与其标签（苹果或橙子）之间的关系，如下面的图示所示：
- en: '![](img/53712b8b-a731-4fc0-aaa5-093cb2abe92e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/53712b8b-a731-4fc0-aaa5-093cb2abe92e.png)'
- en: The alternative, and the approach we're interested in, is getting this model
    created to automatically use examples; this, in essence, is what ML is all about.
    It provides us with an effective tool to model complex tasks that would otherwise
    be nearly impossible to define by rules.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 替代方案，以及我们感兴趣的方法，是让这个模型自动使用示例；本质上，这就是机器学习的全部内容。它为我们提供了一个有效的工具来模拟复杂任务，这些任务否则几乎不可能通过规则来定义。
- en: 'The creation phase of the ML model is called **training** and is determined
    by the type of ML algorithm selected and data being fed. Once the model is trained,
    that is, once it has learned, we can use it to make inferences from the data,
    as illustrated in the following diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的创建阶段被称为**训练**，它由所选的机器学习算法和数据输入类型决定。一旦模型被训练，也就是说，一旦它学会了，我们就可以用它从数据中做出推断，如下面的图示所示：
- en: '![](img/af1d0e57-9687-4f78-9474-aaac5bf93221.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af1d0e57-9687-4f78-9474-aaac5bf93221.png)'
- en: The example we have presented here, classifying oranges and apples, is a specific
    type of ML algorithm called a **classifier**, or, more specifically, a multi-class
    classifier. The model was trained through **supervision**; that is, we fed in
    examples of input with their associated labels (or classes). It is useful to understand
    the types of ML algorithms that exist along with the types of training, which
    is the topic of the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示的例子，对橙子和苹果进行分类，是一种特定的机器学习算法，称为**分类器**，或者更具体地说，是多分类分类器。该模型是通过**监督学习**训练的；也就是说，我们提供了带有相关标签（或类别）的输入示例。了解存在的机器学习算法类型以及训练类型是有用的，这是下一节的主题。
- en: A brief tour of ML algorithms
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法简要概述
- en: In this section, we will look at some examples of how ML is used, and with each
    example, we'll speculate about the type of data, learning style, and ML algorithm
    used. I hope that by the end of this section, you will be inspired by what is
    possible with ML and gain some appreciation for the types of data, algorithms,
    and learning styles that exist.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些机器学习应用的例子，并且在每个例子中，我们将推测所使用的数据类型、学习风格和机器学习算法。我希望到本节结束时，你将受到机器学习可能性的启发，并对存在的数据类型、算法和学习风格有所欣赏。
- en: In this section, we will be presenting some real-life examples in the context
    of introducing types of data, algorithms, and learning styles. It is **not **our
    intention to show accurate data representations or implementations for the example,
    but rather use the examples as a way of making the ideas more tangible.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过介绍数据类型、算法和学习风格来展示一些现实生活中的例子。我们的意图并不是展示示例的准确数据表示或实现，而是利用这些例子使思想更加具体化。
- en: Netflix – making recommendations
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Netflix – 提供推荐
- en: No ML book is complete without mentioning recommendation engines—probably one
    of the most well known applications of ML. In part, this is thanks to the publicity
    gained when Netflix announced a $1 million competition for movie rating predictions,
    also known as **recommendations**. Add to this Amazon's commercial success in
    making use of it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一本机器学习书籍会不提及推荐引擎——这可能是机器学习最广为人知的应用之一。部分原因在于Netflix宣布了一项针对电影评分预测的100万美元竞赛，也称为**推荐**。再加上亚马逊在利用这一技术上的商业成功。
- en: The goal of recommendation engines is to predict the likelihood of someone wanting
    a particular product or service. In the context of Netflix, this would mean recommending
    movies or TV shows to its users.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐引擎的目标是预测某人想要特定产品或服务的可能性。在Netflix的背景下，这意味着向其用户推荐电影或电视节目。
- en: 'One intuitive way of making recommendations is to try and mimic the real world,
    where a person is likely to seek recommendations from like-minded people. What
    constitutes likeness is dependent on the domain. For example, you are most likely
    to have one group of friends that you would ask for restaurant recommendations
    and another group of friends for movie recommendations. What determines these
    groups is how similar their tastes are to your own taste for that particular domain.
    We can replicate this using the (user-based) **Collaborative Filtering** (**CF**)
    algorithm. This algorithm achieves this by finding the distance between each user
    and then using these distances as a similarity metric to infer predictions on
    movies for a particular user; that is, those that are more similar will contribute
    more to the prediction than those that have different preferences. Let''s have
    a look at what form the data might take from Netflix:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 提供推荐的一种直观方式是尝试模仿现实世界，在现实世界中，一个人可能会向志同道合的人寻求推荐。构成相似性的因素取决于领域。例如，你可能会有一组朋友，你会向他们寻求餐厅推荐，另一组朋友则是电影推荐。决定这些群体的因素是他们对你特定领域的品味与你自己的品味相似程度。我们可以使用（基于用户的）**协同过滤（CF**）算法来复制这一点。该算法通过找到每个用户之间的距离，然后使用这些距离作为相似性指标来推断特定用户的电影预测；也就是说，那些更相似的人将对预测做出更大的贡献，而那些有不同偏好的人则贡献较小。让我们看看Netflix的数据可能的形式：
- en: '| **User** | **Movie** | **Rating** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **用户** | **电影** | **评分** |'
- en: '| 0: Jo | A: Monsters Inc | 5 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 0: Jo | A: 怪兽电力公司 | 5 |'
- en: '|  | B: The Bourne Identity | 2 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | B: 波恩身份 | 2 |'
- en: '|  | C: The Martian | 2 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | C: 火星救援 | 2 |'
- en: '|  | D: Blade Runner | 1 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | D: 银河系漫游指南 | 1 |'
- en: '| 1: Sam | C: The Martian | 4 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 1: Sam | C: 火星救援 | 4 |'
- en: '|  | D: Blade Runner | 4 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | D: 银河系漫游指南 | 4 |'
- en: '|  | E: The Matrix  | 4 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | E: 矩阵 | 4 |'
- en: '|  | F: Inception | 5 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | F: 梦幻特工 | 5 |'
- en: '| 2: Chris | B: The Bourne Identity | 4 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 2: Chris | B: 波恩身份 | 4 |'
- en: '|  | C: The Martian | 5 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | C: 火星救援 | 5 |'
- en: '|  | D: Blade Runner | 5 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | D: Blade Runner | 5 |'
- en: '|  | F: Inception | 4 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | F: Inception | 4 |'
- en: 'For each example, we have a user, a movie, and an assigned rating. To find
    the similarity between each user, we can first calculate the Euclidean distance
    of the shared movies between each pair of users. The Euclidean distance gives
    us larger values for users who are most dissimilar; we invert this by dividing
    1 by this distance to give us a result, where 1 represents perfect matches and
    0 means the users are most dissimilar. The following is the formula for Euclidean
    distance and the function used to calculate similarities between two users:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个示例，我们有一个用户、一部电影和一个分配的评分。为了找到每个用户之间的相似度，我们首先可以计算每对用户之间共享电影的欧几里得距离。欧几里得距离为最不相似的用户提供了较大的值；我们通过将1除以这个距离来反转这个值，得到一个结果，其中1代表完美匹配，0表示用户之间最不相似。以下为欧几里得距离和计算两个用户之间相似度的函数的公式：
- en: '![](img/b1e4ef9c-d020-4548-a992-271c303da8eb.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b1e4ef9c-d020-4548-a992-271c303da8eb.png)'
- en: Equation for Euclidian distance and similarity
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离和相似度公式
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To make this more concrete, let''s walk through how we can find the most similar
    user for Sam, who has rated the following movies: `["The Martian" : 4, "Blade
    Runner" : 4, "The Matrix" : 4, "Inception" : 5]`. Let''s now calculate the similarity
    between Sam and Jo and then between Sam and Chris.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使这一点更加具体，让我们通过以下步骤来找到与山姆最相似的用户，山姆对以下电影进行了评分：`["The Martian" : 4, "Blade Runner"
    : 4, "The Matrix" : 4, "Inception" : 5]`。现在，让我们计算山姆和乔之间的相似度，然后是山姆和克里斯之间的相似度。'
- en: '**Sam and Jo **'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**山姆和乔**'
- en: 'Jo has rated the movies `["Monsters Inc." : 5, "The Bourne Identity" : 2, "The
    Martian" : 2, "Blade Runner" : 1]`; by calculating the similarity of intersection
    of the two sets of ratings for each user, we get a value of *0.22*.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '乔对电影的评分是`["Monsters Inc." : 5, "The Bourne Identity" : 2, "The Martian" : 2,
    "Blade Runner" : 1]`；通过计算每个用户评分集合交集的相似度，我们得到一个值为*0.22*。'
- en: '**Sam and Chris **'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**山姆和克里斯**'
- en: 'Similar to the previous ones, but now, by calculating the similarity using
    the movie ratings from Chris (`["The Bourne Identity" : 4, "The Martian" : 5,
    "Blade Runner" : 5, "Inception" : 4]`), we get a value of *0.37*.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '与之前类似，但现在，通过使用克里斯的电影评分（`["The Bourne Identity" : 4, "The Martian" : 5, "Blade
    Runner" : 5, "Inception" : 4]`）来计算相似度，我们得到一个值为*0.37*。'
- en: Through manual inspection, we can see that Chris is more similar to Sam than
    Jo is, and our similarity rating shows this by giving Chris a higher value than
    Jo.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过人工检查，我们可以看到克里斯与山姆的相似度高于乔，我们的相似度评分通过给克里斯一个比乔更高的值来显示这一点。
- en: 'To help illustrate why this works, let''s project the ratings of each user
    onto a chart as shown in the following graph:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助说明为什么这有效，让我们将每个用户的评分投影到以下图表中，如图所示：
- en: '![](img/ed056a0d-b014-43ca-a054-711518fa3d6e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed056a0d-b014-43ca-a054-711518fa3d6e.png)'
- en: The preceding graph shows the users plotted in a preference space; the closer
    two users are in this preference space, the more similar their preferences are.
    Here, we are just showing two axes, but, as seen in the preceding table, this
    extends to multiple dimensions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了用户在偏好空间中的分布；在这个偏好空间中，两个用户越接近，他们的偏好就越相似。这里我们只展示了两个轴，但正如前表所示，这可以扩展到多个维度。
- en: We can now use these similarities as weights that contribute to predicting the
    rating a particular user would give to a particular movie. Then, using these predictions,
    we can recommend some movies that a user is likely to want to watch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这些相似性作为权重，这些权重有助于预测特定用户会对特定电影给出何种评分。然后，利用这些预测，我们可以推荐一些用户可能想要观看的电影。
- en: The preceding approach is a type of **clustering** algorithm that falls under **unsupervised
    learning**,a learning style where examples have no associated label and the job
    of the ML algorithm is to find patterns within the data. Other common unsupervised
    learning algorithms include the Apriori algorithm (basket analysis) and K-means.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法是一种**聚类**算法，属于**无监督学习**，在这种学习风格中，示例没有关联的标签，机器学习算法的职责是在数据中找到模式。其他常见的无监督学习算法包括Apriori算法（篮子分析）和K-means。
- en: Recommendations are applicable anytime when there is an abundance of information
    that can benefit from being filtered and ranked before being presented to the
    user. Having recommendations performed on the device offers many benefits, such
    as being able to incorporate the context of the user when filtering and ranking
    the results.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当有大量信息可以受益于在呈现给用户之前进行过滤和排序时，推荐是适用的。在设备上执行推荐操作提供了许多好处，例如能够在过滤和排序结果时结合用户的上下文。
- en: Shadow draw – real-time user guidance for freehand drawing
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阴影绘制 – 为自由手绘提供实时用户指导
- en: To highlight the synergies between man and machine, AI is sometimes referred
    to as **Augmented Intelligence** (**AI**), putting the emphasis on the system
    to augment our abilities rather than replacing us altogether.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出人与机器之间的协同作用，人工智能有时被称为**增强智能**（**AI**），强调系统增强我们的能力而不是完全取代我们。
- en: One area that is becoming increasingly popular—and of particular interest to
    myself—is assisted creation systems, an area that sits at the intersection of
    the fields of **human-computer interaction** (**HCI**) and ML. These are systems
    created to assist in some creative tasks such as drawing, writing, video, and
    music.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个越来越受欢迎的领域，并且对我个人来说特别感兴趣的是辅助创作系统，这个领域位于**人机交互**（**HCI**）和机器学习（ML）的交叉点。这些系统是为了辅助一些创作任务而创建的，例如绘画、写作、视频和音乐。
- en: 'The example we will discuss in this section is shadow draw, a research project
    undertaken at Microsoft in 2011 by Y.J. Lee, L. Zitnick, and M. Cohen. Shadow
    draw is a system that assists the user in drawing by matching and aligning a reference
    image from an existing dataset of objects and then lightly rendering shadows in
    the background to be used as guidelines for the user. For example, if the user
    is predicted to be drawing a bicycle, then the system would render guidelines
    under the user''s pen to assist them in drawing the object, as illustrated in
    this diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中我们将讨论的例子是阴影绘制，这是一个由Y.J. Lee、L. Zitnick和M. Cohen在2011年于微软进行的研究项目。阴影绘制是一个通过匹配和定位现有物体数据集中的参考图像来辅助用户绘制的系统，然后在背景中轻柔地渲染阴影作为用户绘图的指南。例如，如果预测用户正在绘制一辆自行车，那么系统就会在用户的笔下方渲染指南来帮助他们绘制该物体，如图所示：
- en: '![](img/d2c2bac9-45d4-4fc1-84dd-0b7bc23a5b9b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d2c2bac9-45d4-4fc1-84dd-0b7bc23a5b9b.png)'
- en: As we did before, let's walk through how we might approach this, focusing specifically
    on classifying the sketch; that is, we'll predict what object the user is drawing.
    This will give us the opportunity to see new types of data, algorithms, and applications
    of ML.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前做的那样，让我们来看看我们可能如何处理这个问题，具体关注对草图进行分类；也就是说，我们将预测用户正在绘制什么物体。这将给我们机会看到新的数据类型、算法和机器学习的应用。
- en: 'The dataset used in this project consisted of 30,000 natural images collected
    from the internet via 40 category queries such as face, car, and bicycle, with
    each category stored in its own directory; the following diagram shows some examples
    of these images:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目中使用的数据集由从互联网上通过40个类别查询（如人脸、汽车和自行车）收集的30,000张自然图像组成，每个类别存储在其自己的目录中；以下图表显示了这些图像的一些示例：
- en: '![](img/5631134e-855f-4119-ae0b-cc81c8c02b7a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5631134e-855f-4119-ae0b-cc81c8c02b7a.png)'
- en: 'After obtaining the raw data, the next step, and typical of any ML project,
    is to perform **data preprocessing** and **feature engineering**. The following
    diagram shows the preprocessing steps, which consist of:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得原始数据后，下一步，也是任何机器学习项目的典型步骤，是执行**数据预处理**和**特征工程**。以下图表显示了预处理步骤，包括：
- en: Rescaling each image
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新缩放每张图像
- en: Desaturating (turning black and white)
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去饱和（变为黑白）
- en: Edge detection
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边缘检测
- en: '![](img/aa7c256b-44f0-4bb4-94ae-d130d75ce389.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aa7c256b-44f0-4bb4-94ae-d130d75ce389.png)'
- en: Our next step is to abstract our data into something more meaningful and useful
    for our ML algorithm to work with; this is known as **feature engineering**, and
    is a critical step in a typical ML workflow.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步是将我们的数据抽象成对我们的机器学习算法更有意义和有用的东西；这被称为**特征工程**，在典型的机器学习工作流程中是一个关键步骤。
- en: 'One approach, and the approach we will describe, is creating something known
    as a **visual bag of words**. This is essentially a histogram of features (visual
    words) used to describe each image, and collectively to describe each category.
    What constitutes a feature is dependent on the data and ML algorithm; for example,
    we can extract and count the colors of each image, where the colors become our
    features and collectively describe our image, as shown in the following diagram:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法，以及我们将描述的方法，是创建一个被称为**视觉词袋**的东西。这本质上是对描述每个图像以及集体描述每个类别的特征（视觉词）的直方图。构成特征的是什么取决于数据和机器学习算法；例如，我们可以提取并计算每张图像的颜色，其中颜色成为我们的特征，并集体描述我们的图像，如下面的图表所示：
- en: '![](img/f6379527-35b9-4262-ab86-4752edf3f8c4.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f6379527-35b9-4262-ab86-4752edf3f8c4.png)'
- en: 'But because we are dealing with sketches, we want something fairly coarse—something
    that can capture the general strokes directions that will encapsulate the general
    structure of the image. For example, if we were to describe a square and a circle,
    the square would consist of horizontal and vertical strokes, while the circle
    would consist mostly of diagonal strokes. To extract these features, we can use
    a computer vision algorithm called **histogram of oriented gradients** (**HOG**);
    after processing an image you are returned a histogram of gradient orientations
    in localized portions of the image. Exactly what we want! To help illustrate the
    concept, this process is summarized for a single image here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但因为我们处理的是草图，我们想要一个相当粗糙的东西——可以捕捉到将封装图像的一般结构的一般线条方向。例如，如果我们描述一个正方形和一个圆，正方形将包含水平和垂直线条，而圆将主要包含对角线条。为了提取这些特征，我们可以使用一种称为**方向梯度直方图**（**HOG**）的计算机视觉算法；在处理完图像后，你将得到图像局部区域中的梯度方向直方图。这正是我们想要的！为了帮助说明这个概念，这里对单个图像的此过程进行了总结：
- en: '![](img/ad5f2e2c-230f-4e80-8bce-f3c206c39dee.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad5f2e2c-230f-4e80-8bce-f3c206c39dee.png)'
- en: 'After processing all the images in our dataset, our next step is to find a histogram
    (or histograms) that can be used to identify each category; we can use an **unsupervised
    learning** clustering technique called **K-means**, where each category histogram
    is the centroid for that cluster. The following diagram describes this process;
    we first extract features for each image and then cluster these using K-means,
    where the distance is calculated using the histogram of gradients. Once our images
    have been clustered into their groups, we extract the center (mean) histogram
    of each of these groups to act as our category descriptor:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完我们数据集中的所有图像后，我们的下一步是找到一个（或多个）直方图，可以用来识别每个类别；我们可以使用一种称为**K-means**的无监督学习聚类技术，其中每个类别的直方图是该聚类的中心。以下图表描述了这一过程；我们首先为每个图像提取特征，然后使用K-means对这些特征进行聚类，其中距离是通过直方图梯度来计算的。一旦我们的图像被聚类到它们的组中，我们就提取每个组的中心（均值）直方图作为我们的类别描述符：
- en: '![](img/23029d2b-f64b-4b1e-9baa-114ccdd35766.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/23029d2b-f64b-4b1e-9baa-114ccdd35766.png)'
- en: 'Once we have obtained a histogram for each category (codebook), we can train
    a **classifier**  using each image''s extracted features (visual words) and the
    associated category (label). One popular and effective classifier is **support
    vector machines** (**SVM**). What SVM tries to find is a hyperplane that best
    separates the categories; here, *best *refers to a plane that has the largest
    distance between each of the category members. The term *hyper* is used because
    it transforms the vectors into high-dimensional space such that the categories
    can be separated with a linear plane (plane because we are working within a space).
    The following diagram shows how this may look for two categories in a two-dimensional
    space:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为每个类别（代码簿）获得了直方图，我们就可以使用每个图像提取的特征（视觉词）和相关的类别（标签）来训练一个**分类器**。一个流行且有效的分类器是**支持向量机**（**SVM**）。SVM试图找到一个最佳分离类别的超平面；这里的“最佳”指的是具有最大类别成员之间距离的平面。术语“超”是因为它将向量转换到高维空间，这样类别就可以用线性平面（因为我们在一个空间内工作）来分离。以下图表显示了在二维空间中两个类别可能看起来是什么样子：
- en: '![](img/2fb64dc6-ac0a-4f1a-903d-b310f0e32389.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fb64dc6-ac0a-4f1a-903d-b310f0e32389.png)'
- en: With our model now trained, we can perform real-time classification on the image
    as the user is drawing, thus allowing us to assist the user by providing them with
    guidelines for the object they are wanting to draw (or at least, mention the object
    we predicted them to be drawing). Perfectly suited for touch interfaces such as
    your iPhone or iPad! This assists not just in drawing applications, but anytime
    where an input is required by the user, such as image-based searching or note
    taking.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在已经训练完成，因此用户在绘制图像时，我们可以实时地对图像进行分类，这样我们就可以通过为他们提供想要绘制的对象（或至少提及我们预测他们将要绘制的对象）的指南来协助用户。这非常适合像iPhone或iPad这样的触摸界面！这不仅有助于绘图应用，而且在用户需要输入的任何时候，如基于图像的搜索或记笔记时，都非常有用。
- en: 'In this example, we showed how feature engineering and **unsupervised learning**
    are used to augment data, making it easier for our model to sufficiently perform
    **classification** using the **supervised learning** algorithm SVM. Prior to deep
    neural networks, feature engineering was a critical step in ML and sometimes a
    limiting factor for these reasons:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们展示了如何使用特征工程和**无监督学习**来增强数据，使得我们的模型能够更充分地使用**监督学习**算法SVM进行**分类**。在深度神经网络出现之前，特征工程是机器学习中的一个关键步骤，有时也是这些原因的限制因素：
- en: It required special skills and sometimes domain expertise
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这需要特殊技能和有时需要领域专业知识
- en: It was at the mercy of a human being able to find and extract meaningful features
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它取决于一个能够找到和提取有意义特征的人
- en: It required that the features extracted would generalize across the population,
    that is, be expressive enough to be applied to all examples
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它要求提取的特征能够在整个人群中推广，也就是说，足够表达，可以应用于所有例子
- en: In the next example, we introduce a type of neural network called a **convolutional
    neural network** (**CNN** or **ConvNet**), which takes care of a lot of the feature
    engineering itself.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个例子中，我们介绍了一种称为**卷积神经网络（CNN**或**ConvNet**）的神经网络类型，它负责处理大量的特征工程。
- en: The paper describing the actual project and approach can be found here: [http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html](http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 描述实际项目和方法的论文可以在这里找到：[http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html](http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html)。
- en: Shutterstock – image search based on composition
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Shutterstock – 基于构图进行图像搜索
- en: Over the past 10 years, we have seen an explosive growth in visual content created
    and consumed on the Web, but before the success of CNNs, images were found by
    performing simple keyword searches on the tags assigned manually. All this changed
    around 2012, when A. Krizhevsky, I. Sutskever, and G. E. Hinton published their
    paper *ImageNet Classification with Deep Convolutional Networks*. The paper described
    their architecture used to win the 2012 **ImageNet Large-Scale Visual Recognition
    Challenge** (**ILSVRC**). It's a competition like the Olympics of computer vision,
    where teams compete across a range of CV tasks such as classification, detection,
    and object localization. And that was the first year a CNN gained the top position
    with a test error rate of 15.4% (the next best entry achieved an test error rate
    of 26.2%). Ever since then, CNNs have become the de facto approach for computer
    vision tasks, including becoming the new approach for performing visual search.
    Most likely, it has been adopted by the likes of Google, Facebook, and Pinterest,
    making it easier than ever to find that right image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的10年里，我们在网络上看到了视觉内容的创建和消费呈爆炸式增长，但在CNN成功之前，图像是通过在手动分配的标签上执行简单的关键词搜索来找到的。所有这些都改变了，大约在2012年，A.
    Krizhevsky，I. Sutskever和G. E. Hinton发表了他们的论文《使用深度卷积网络的ImageNet分类》。这篇论文描述了他们用于赢得2012年**ImageNet大规模视觉识别挑战赛（ILSVRC**）的架构。这是一个像奥运会一样的计算机视觉竞赛，团队在一系列CV任务（如分类、检测和目标定位）中进行竞争。而且那一年，CNN首次以15.4%的测试错误率获得了第一名（下一个最佳参赛者的测试错误率为26.2%）。从那时起，CNN已经成为计算机视觉任务的默认方法，包括成为执行视觉搜索的新方法。很可能，它已经被Google、Facebook和Pinterest等公司采用，使得找到正确的图像变得比以往任何时候都容易。
- en: 'Recently, (October 2017), Shutterstock announced one of the more novel uses
    of CNNs, where they introduced the ability for their users to search for not only multiple items
    in an image, but also the composition of those items. The following screenshot
    shows an example search for a kitten and a computer, with the kitten on the left
    of the computer:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，（2017年10月），Shutterstock宣布了一种CNN（卷积神经网络）更为新颖的应用，他们引入了用户不仅能够在图像中搜索多个项目，还能搜索这些项目的构图的能力。以下截图展示了一个搜索小猫和电脑的例子，小猫位于电脑的左侧：
- en: '![](img/7caaf8d3-6a1b-4e0e-847a-6f9607cf864f.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7caaf8d3-6a1b-4e0e-847a-6f9607cf864f.jpg)'
- en: So what are CNNs? As previously mentioned, CNNs are a type of neural network
    that are well suited for visual content due to their ability to retain spatial
    information. They are somewhat similar to the previous example, where we explicitly
    define a filter to extract localized features from the image. A CNN performs a
    similar operation, but unlike our previous example, filters are not explicitly
    defined. They are learned through training, and they are not confined to a single
    layer but rather build with many layers. Each layer builds upon the previous one
    and each becomes increasingly more abstract (abstract here means a higher-order
    representation, that is, from pixels to shapes) in what it represents.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，CNNs是什么？如前所述，CNNs是一种非常适合视觉内容的神经网络，因为它们能够保留空间信息。它们在某种程度上类似于之前的例子，其中我们明确定义了一个过滤器来从图像中提取局部特征。CNN执行类似的操作，但与我们的前一个例子不同，过滤器不是明确定义的。它们是通过训练学习的，并且它们不是局限于单层，而是由许多层构建的。每一层都建立在上一层的基础上，每一层在它所代表的内容上变得越来越抽象（这里的抽象意味着更高阶的表示，即从像素到形状）。
- en: 'To help illustrate this, the following diagram visualizes how a network might
    build up its understanding of a cat. The first layer''s filters extract simple
    features, such as edges and corners. The next layer builds on top of these with
    its own filters, resulting in higher-level concepts being extracted, such as shapes
    or parts of the cat. These high-level concepts are then combined for classification
    purposes:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助说明这一点，以下图表展示了网络如何建立对猫的理解。第一层的过滤器提取简单的特征，例如边缘和角落。下一层在这些基础上使用自己的过滤器，从而提取出更高层次的概念，例如形状或猫的各个部分。这些高层次的概念随后被组合起来用于分类目的：
- en: '![](img/9a864245-2210-4579-a19d-43a61b59e379.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9a864245-2210-4579-a19d-43a61b59e379.png)'
- en: This ability to get a deeper understanding of the data and reduce the dependency
    on manual feature engineering has made deep neural networks one of the most popular
    ML algorithms over the past few years.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种更深入理解数据并减少对手动特征工程依赖的能力，使得深度神经网络在过去几年中成为最受欢迎的机器学习算法之一。
- en: To train the model, we feed the network examples using images as inputs and
    labels as the expected outputs. Given enough examples, the model will build an
    internal representation for each label, which can be sufficiently used for **classification**;
    this, of course, is a type of **supervised learning**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们使用图像作为输入，标签作为预期输出，向网络提供示例。给定足够的示例，模型将为每个标签构建一个内部表示，这可以足够用于**分类**；当然，这是一种**监督学习**。
- en: Our last task is to find the location of the item or items; to achieve this,
    we can inspect the weights of the network to find out which pixels activated a
    particular class, and then create a bounding box around the inputs with the largest
    weights.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后一个任务是找到项目或项目的位置；为了实现这一点，我们可以检查网络的权重，以找出哪些像素激活了特定的类别，然后围绕具有最大权重的输入创建一个边界框。
- en: We have now identified the items and their locations within the image. With
    this information, we can preprocess our repository of images and cache it as metadata
    to make it accessible via search queries. We will revisit this idea later in the
    book when you will get a chance to implement a version of this to assist the user
    in finding images in their photo album.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经识别了图像中的项目及其位置。有了这些信息，我们可以预处理我们的图像库，并将其作为元数据缓存，以便通过搜索查询访问。我们将在本书的后面部分再次探讨这个想法，届时你将有机会实现一个版本，以帮助用户在相册中找到图像。
- en: In this section, we saw how ML can be used to improve user experience and briefly
    introduced the intuition behind CNNs, a neural network well suited for visual
    contexts, where retaining proximity of features and building higher levels of
    abstraction is important. In the next section, we will continue our exploration
    of ML applications by introducing another example that improves the user experience
    and a new type of neural network that is well suited for sequential data such
    as text.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何使用机器学习来改善用户体验，并简要介绍了CNNs背后的直觉，这是一种非常适合视觉环境的神经网络，其中保留特征邻近性和构建更高层次的抽象很重要。在下一节中，我们将继续探索机器学习应用，通过介绍另一个提高用户体验的例子以及一种非常适合序列数据（如文本）的新类型的神经网络。
- en: iOS keyboard prediction – next letter prediction
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iOS键盘预测 - 下一个字母预测
- en: Quoting usability expert Jared Spool, *Good design, when done well, should be
    invisible.* This holds true for ML as well. The application of ML need not be
    apparent to the user and sometimes (more often than not) more subtle uses of ML
    can prove just as impactful.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 引用可用性专家Jared Spool的话，“好的设计，当做得好时，应该是无形的。”这对于机器学习也是正确的。机器学习的应用不必对用户明显，有时（更常见的情况）机器学习的微妙应用可以证明同样有影响力。
- en: 'A good example of this is an iOS feature called **dynamic target resizing**;
    it is working every time you type on an iOS keyboard, where it actively tries
    to predict what word you''re trying to type:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这的一个很好的例子是iOS的一个功能，称为**动态目标调整大小**；每次你在iOS键盘上输入时，它都在工作，它积极地尝试预测你想要输入的单词：
- en: '![](img/f2b17f2a-3cb9-4c0c-a73a-c90a9c8f6fca.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f2b17f2a-3cb9-4c0c-a73a-c90a9c8f6fca.png)'
- en: Using this prediction, the iOS keyboard dynamically changes the touch area of
    a key (here illustrated by the red circles) that is the most likely character
    based on what has already been typed before it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个预测，iOS键盘会动态地改变键的触摸区域（在此由红色圆圈表示），这个键是基于之前输入的最可能字符。
- en: For example, in the preceding diagram, the user has entered `"Hell"`; now it
    would be reasonable to assume that the most likely next character the user wants
    to tap is `"o"`. This is intuitive given our knowledge of the English language,
    but how do we teach a machine to know this?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在先前的图中，用户输入了“Hell”；现在合理地假设用户最可能点击的下一个字符是“o”。鉴于我们对英语语言的知识，这是直观的，但我们如何教会机器知道这一点呢？
- en: 'This is where **recurrent neural networks** (**RNNs**) come in; it''s a type
    of neural network that persists state over time. You can think of this persisted
    state as a form of memory, making RNNs suitable for sequential data such as text
    (any data where the inputs and outputs are dependent on each other). This state
    is created by using a feedback loop from the output of the cell, as shown in the
    following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**循环神经网络**（**RNNs**）发挥作用的地方；它是一种在时间上持续状态的神经网络。你可以将这种持续状态视为一种记忆形式，这使得RNNs非常适合处理序列数据，如文本（任何输入和输出相互依赖的数据）。这种状态是通过使用从细胞输出到反馈循环来创建的，如下面的图所示：
- en: '![](img/34a16c15-b94a-4a9d-88e3-af66e79228f3.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34a16c15-b94a-4a9d-88e3-af66e79228f3.png)'
- en: 'The preceding diagram shows a single RNN cell. If we unroll this over time,
    we would get something that looks like the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的图显示了单个RNN细胞。如果我们将其展开到时间上，我们会得到如下所示的东西：
- en: '![](img/6e01fc27-e451-4304-8bcc-d80337aaa38e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6e01fc27-e451-4304-8bcc-d80337aaa38e.png)'
- en: Using **hello** as our example, the preceding diagram shows an unrolled RNN
    over five time steps; at each time step, the RNN predicts the next likely character.
    This prediction is determined by its internal representation of the language (from
    training) and subsequent inputs. This internal representation is built by training
    it on samples of text where the output is using the inputs but at the next time
    step (as illustrated earlier). Once trained, the inference follows a similar path,
    except that we feed to the network the predicted character from the output, to
    get the next output (to generate the sequence, that is, words).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以**hello**为例，先前的图显示了在五个时间步长上的展开RNN；在每一个时间步长，RNN预测下一个可能的字符。这个预测是由其内部的语言表示（来自训练）和后续输入决定的。这种内部表示是通过在文本样本上训练来构建的，其中输出使用输入，但在下一个时间步（如前面所示）。一旦训练完成，推理遵循类似的路径，只是我们将预测的字符从输出馈送到网络，以获取下一个输出（以生成序列，即单词）。
- en: Neural networks and most ML algorithms require their inputs to be numbers, so
    we need to convert our characters to numbers, and back again. When dealing with
    text (characters and words), there are generally two approaches: **one-hot encoding**
    and **embeddings**. Let's quickly cover each of these to get some intuition of
    how to handle text.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和大多数机器学习算法需要它们的输入是数字，因此我们需要将我们的字符转换为数字，然后再转换回来。处理文本（字符和单词）时，通常有两种方法：**独热编码**和**嵌入**。让我们快速了解每个方法，以获得一些处理文本的直觉。
- en: 'Text (characters and words) is considered categorical, meaning that we cannot
    use a single number to represent text because there is no inherit relationship
    between the text and the value; that is, assigning **the** 10 and **cat** 20 implies
    that **cat** has a greater value than **the**. Instead, we need to encode them
    into something where no bias is introduced. One solution to this is encoding them
    using one-hot encoding, which uses an array of the size of your vocabulary (number
    of characters in our case), with the index of the specific character set to 1
    and the rest set to 0\. The following diagram illustrates the encoding process
    for the corpus **"hello"**:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 文本（字符和单词）被认为是分类的，这意味着我们不能用一个单一的数字来表示文本，因为文本和价值之间没有固有的关系；也就是说，将**the**分配为10和**cat**分配为20意味着**cat**比**the**更有价值。相反，我们需要将它们编码成一种不引入偏差的形式。一种解决方案是使用单热编码，它使用一个大小为你的词汇表（在我们的例子中是字符的数量）的数组，将特定字符的索引设置为1，其余设置为0。以下图展示了语料库**"hello"**的编码过程：
- en: '![](img/4694039f-def1-441f-a43d-ecaaa8639385.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4694039f-def1-441f-a43d-ecaaa8639385.png)'
- en: In the preceding diagram, we show some of the steps required when encoding characters;
    we start off by splitting the corpus into individual characters (called **tokens**,
    and the process is called **tokenization**). Then we create a set that acts as
    our vocabulary, and finally we encode this with each character being assigned
    a vector.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们展示了编码字符时所需的步骤；我们首先将语料库拆分为单个字符（称为**标记**，这个过程称为**标记化**）。然后我们创建一个作为我们的词汇表集合，最后我们用每个字符分配一个向量来编码这个集合。
- en: Here, we'll only present some of the steps required for preparing text before
    passing it to our ML algorithm.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只展示在将文本传递给我们的机器学习算法之前所需的步骤。
- en: Once our inputs are encoded, we can feed them into our network. Outputs will
    also be represented in this format, with the most likely character being the index
    with the greatest value. For example, if **'e'** is predicted, then the most likely the
    output may resemble something like [0.95, 0.2, 0.2, 0.1].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的输入被编码，我们就可以将它们输入到我们的网络中。输出也将以这种格式表示，其中最可能的字符是具有最大值的索引。例如，如果预测到**'e'**，那么最可能的输出可能类似于[0.95,
    0.2, 0.2, 0.1]。
- en: But there are two problems with one-hot encoding. The first is that for a large
    vocabulary, we end up with a very sparse data structure. This is not only an inefficient
    use of memory, but also requires additional calculations for training and inference.
    The second problem, which is more obvious when operating on words, is that we
    lose any contextual meaning after they have been encoded. For example, if we were
    to encode the words **dog** and **dogs**, we would lose any relationship between
    these words after encoding.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但单热编码有两个问题。第一个问题是对于大型词汇表，我们最终得到一个非常稀疏的数据结构。这不仅是对内存的不高效使用，还需要额外的计算来进行训练和推理。第二个问题，当操作单词时更为明显，是我们编码后失去了任何上下文意义。例如，如果我们对单词**dog**和**dogs**进行编码，那么编码后我们就会失去这些单词之间的任何关系。
- en: An alternative, and something that addresses these two problems, is using an
    embedding. These are generally weights from a trained network that use a dense
    vector representation for each token, one that preserves some contextual meaning. This
    book focuses on computer vision tasks, so we won't be going into the details here.
    Just remember that we need to encode our text (characters) into something our
    ML algorithm will accept.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方案，也是解决这两个问题的方案，是使用嵌入。这些通常是从训练好的网络中得到的权重，每个标记使用密集向量表示，这可以保留一些上下文意义。这本书主要关注计算机视觉任务，所以我们不会在这里详细介绍。只需记住，我们需要将我们的文本（字符）编码成我们的机器学习算法可以接受的形式。
- en: We train the model using **weak supervision**, similar to supervised learning,
    but inferring the label without it having been explicitly labelled. Once trained,
    we can predict the next character using **multi-class classification**, as described
    earlier.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**弱监督**来训练模型，类似于监督学习，但不需要显式标记来推断标签。一旦训练完成，我们可以使用前面描述的**多类分类**来预测下一个字符。
- en: Over the past couple of years, we have seen the evolution of assistive writing;
    one example is Google's Smart Reply, which provides an end-to-end method for automatically
    generating short email responses. Exciting times!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，我们见证了辅助写作的演变；一个例子是谷歌的智能回复（Smart Reply），它提供了一种端到端的方法来自动生成简短的电子邮件回复。令人兴奋的时代！
- en: This concludes our brief tour of introducing types of ML problems along with
    the associated data types, algorithms, and learning style. We have only scratched
    the surface of each, but as you make your way through this book, you will be introduced
    to more data types, algorithms, and learning styles.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对机器学习问题类型及其相关数据类型、算法和学习风格的简要介绍。我们对每个方面都只是触及了皮毛，但随着你阅读本书，你将接触到更多数据类型、算法和学习风格。
- en: In the next section, we will take a step back and review the overall workflow
    for training and inference before wrapping up this chapter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾训练和推理的整体工作流程，然后结束本章。
- en: A typical ML workflow
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个典型的机器学习工作流程
- en: If we analyze each of the examples presented so far, we see that each follows
    a similar pattern. First is the definition of the problem or desired functionality.
    Once we have established what we want to do, we then identify the available data
    and/or what data is required. With the data in hand, our next step is to create
    our ML model and prepare the data for training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析到目前为止所提供的每个示例，我们会看到每个都遵循一个类似的模式。首先是定义问题或所需的功能。一旦我们确定了我们想要做什么，我们就确定可用的数据以及/或所需的数据。有了数据在手，我们的下一步是创建我们的机器学习模型并为训练准备数据。
- en: 'After training, something we hadn''t discussed here, is validating our ML model,
    that is, testing that it satisfactorily achieves what we require of it. An example
    is being able to make an accurate prediction. Once we have trained a model, we
    can make use of it by feeding in real data, that is, data outside our training
    set. In the following diagram, we see these steps summarized for training and
    inference:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 训练之后，我们在这里没有讨论的内容是验证我们的机器学习模型，即测试它是否满意地达到了我们的要求。一个例子是能够做出准确的预测。一旦我们训练了一个模型，我们就可以通过输入真实数据来利用它，即输入训练集之外的数据。在下面的图中，我们看到了训练和推理步骤的总结：
- en: '![](img/8925f3f4-0c7c-42fc-bbe0-7daf4518c481.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8925f3f4-0c7c-42fc-bbe0-7daf4518c481.png)'
- en: We will spend most of our time using trained models in this book, but understanding
    how we arrive at these models will prove helpful as you start creating your own
    intelligent apps. This will also help you identify opportunities to apply ML on
    existing data or inspire you to seek out new data sources. It's also worth noting
    that the preprocessing step on training data is equivalent to preprocessing on
    input data when performing inference—something we will spend a lot of time discussing
    and coding for throughout this book.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将大部分时间用于使用训练好的模型，但了解我们如何得到这些模型将有助于你开始创建自己的智能应用。这也有助于你识别在现有数据上应用机器学习的机会，或者激发你寻找新的数据来源。还值得注意的是，在训练数据上的预处理步骤与在推理时对输入数据进行预处理是等效的——这是我们将在整本书中花费大量时间讨论和编码的内容。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced ML and its value by contrasting it against classical
    programming. We then spent some time exploring different applications of ML, and
    for each we speculated about the type of data, algorithms, and learning style
    used. This approach was taken to help demystify how ML works and to encourage you
    to start thinking about how you can leverage data to improve user experience and/or
    offer new functionality. We'll continue this approach throughout this book with
    (obviously) more emphasis on making use of ML by way of example applications related
    to computer vision.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过将其与经典编程进行对比，介绍了机器学习及其价值。然后，我们花了一些时间探索了机器学习的不同应用，并对每种应用都推测了所使用的数据类型、算法和学习风格。采取这种方法是为了帮助揭开机器学习的工作原理，并鼓励你开始思考如何利用数据来改善用户体验或提供新的功能。我们将继续以这种方式贯穿整本书，显然会更多地强调通过与计算机视觉相关的示例应用来利用机器学习。
- en: In the next chapter, we will introduce Core ML, iOS's specifically designed
    framework for making ML accessible to developers with little or no experience
    with ML.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍Core ML，这是iOS专门设计的框架，旨在使那些对机器学习经验很少或没有经验的开发者能够使用机器学习。
