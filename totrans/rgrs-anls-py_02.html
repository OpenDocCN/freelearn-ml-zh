<html><head></head><body>
<div class="book" title="Chapter&#xA0;2.&#xA0;Approaching Simple Linear Regression"><div class="book" id="H5A42-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02" class="calibre1"/>Chapter 2. Approaching Simple Linear Regression</h1></div></div></div><p class="calibre8">Having set up all your working tools (directly installing Python and IPython or using a scientific distribution), you are now ready to start using linear models to incorporate new abilities into the software you plan to build, especially predictive capabilities. Up to now, you have developed software solutions based on certain specifications you defined (or specifications that others have handed to you). Your approach has always been to tailor the response of the program to particular inputs, by writing code carefully mapping every single situation to a specific, predetermined response. Reflecting on it, by doing so you were just incorporating practices that you (or others) have learned from experience.</p><p class="calibre8">However, the world is complex, and sometimes your experience is not enough to make your software smart enough to make a difference in a fairly competitive business or in challenging problems with many different and mutable facets.</p><p class="calibre8">In this chapter, we will start exploring an approach that is different from manual programming. We are going to present an approach that enables the software to self-learn the correct answers to particular inputs, provided you can define the problem in terms of data and target response and that you can incorporate in the processes some of your domain expertise—for instance, choosing the right features for prediction. Therefore, your experience will go on being critical when it comes to creating your software, though in the form of learning from data. In fact, your software will be learning from data accordingly to your specifications. We are also going to illustrate how it is possible to achieve this by resorting to one of the simplest methods for deriving knowledge from data: linear models.</p><p class="calibre8">Specifically, in this chapter, we are going to discuss the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Understanding what problems machine learning can solve</li><li class="listitem">What problems a regression model can solve</li><li class="listitem">The strengths and weaknesses of correlation</li><li class="listitem">How correlations extends to a simple regression model</li><li class="listitem">The when, what, and why of a regression model</li><li class="listitem">The essential mathematics behind gradient descent</li></ul></div><p class="calibre8">In the process, we will be using some statistical terminology and concepts in order to provide you with the prospect of linear regression in the larger frame of statistics, though our approach will remain practical, offering you the tools and hints to start building linear models using Python and thus enrich your software development.</p></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Approaching Simple Linear Regression">
<div class="book" title="Defining a regression problem"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch02lvl1sec12" class="calibre1"/>Defining a regression problem</h1></div></div></div><p class="calibre8">Thanks to <a id="id69" class="calibre1"/>machine learning algorithms, deriving knowledge from data is possible. Machine learning has solid roots in years of research: it has really been a long journey since the end of the fifties, when Arthur Samuel clarified machine learning as being a "field of study that gives computers the ability to learn without being explicitly programmed."</p><p class="calibre8">The data explosion (the availability of previously unrecorded amounts of data) has enabled the widespread usage of both recent and classic machine learning techniques and made them high-performance techniques. If nowadays you can talk by voice to your mobile phone and expect it to answer properly to you, acting as your secretary (such as Siri or Google Now), it is uniquely because of machine learning. The same holds true for every application based on machine learning such as face recognition, search engines, spam filters, recommender systems for books/music/movies, handwriting recognition, and automatic language translation.</p><p class="calibre8">Some other actual usages of machine learning algorithms are somewhat less obvious, but nevertheless important and profitable, such as credit rating and fraud detection, algorithmic trading, advertising profiling on the Web, and health diagnostics.</p><p class="calibre8">Generally speaking, machine learning algorithms can learn in three ways:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Supervised learning</strong></span>: This is when we present labeled examples to learn from. For instance, when<a id="id70" class="calibre1"/> we want to be able to predict the selling price of a house in advance in a real estate market, we can get the historical prices of houses and have a supervised learning algorithm successfully figure out how to associate the prices to the house characteristics.</li><li class="listitem"><span class="strong"><strong class="calibre2">Unsupervised learning</strong></span>: This is when we present examples without any hint, leaving it to <a id="id71" class="calibre1"/>the algorithm to create a label. For instance, when we need to figure out how the groups inside a customer database can be partitioned into similar segments based on their characteristics and behaviors.</li><li class="listitem"><span class="strong"><strong class="calibre2">Reinforcement learning</strong></span>: This is when we present examples without labels, as in unsupervised learning, but get feedback from the environment as to whether label guessing is correct or not. For instance, when we need software to act successfully in a competitive setting, such as a videogame or the stock market, we<a id="id72" class="calibre1"/> can use reinforcement learning. In this <a id="id73" class="calibre1"/>case, the software will then start acting in the setting and it will learn directly from its errors until it finds a set of rules that ensure its success.</li></ul></div></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Approaching Simple Linear Regression">
<div class="book" title="Defining a regression problem">
<div class="book" title="Linear models and supervised learning"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec22" class="calibre1"/>Linear models and supervised learning</h2></div></div></div><p class="calibre8">Unsupervised learning has important applications in robotic vision and automatic feature creation, <a id="id74" class="calibre1"/>and reinforcement learning is critical for developing<a id="id75" class="calibre1"/> autonomous AI (for instance, in <a id="id76" class="calibre1"/>robotics, but also in creating intelligent software agents); however, supervised learning is most important in data science because it allows us to accomplish <a id="id77" class="calibre1"/>something the human race has aspired to for ages: prediction.</p><p class="calibre8"><span class="strong"><strong class="calibre2">Prediction</strong></span> has<a id="id78" class="calibre1"/> applications in business and for general usefulness, enabling us to take the best course of action since we know from predictions the likely outcome of a situation. Prediction can make us successful in our decisions and actions, and since ancient times has been associated with magic or great wisdom.</p><p class="calibre8">Supervised learning is no magic at all, though it may look like sorcery to some people, as Sir Arthur Charles Clarke stated, "any sufficiently advanced technology is indistinguishable from magic." Supervised learning, based on human achievements in mathematics and statistics, helps to leverage human experience and observations and turn them into precise predictions in a way that no human mind could. However, supervised learning can predict only in certain favorable conditions. It is paramount to have examples from the past at hand from which we can extract rules and hints that can support wrapping up a highly likely prediction given certain premises.</p><p class="calibre8">In one way or another, no matter the exact formulation of the machine learning algorithm, the idea is that you can tell the outcome because there have been certain premises in the observed past that led to particular conclusions.</p><p class="calibre8">In mathematical formalism, we call the outcome we want to predict the response or target variable and we usually label it using the lower case letter <span class="strong"><em class="calibre9">y</em></span>.</p><p class="calibre8">The premises are instead called<a id="id79" class="calibre1"/> the <span class="strong"><strong class="calibre2">predictive</strong></span> <span class="strong"><strong class="calibre2">variables</strong></span>, or simply attributes or features, and they are labeled as a lowercase <span class="strong"><em class="calibre9">x</em></span> if there is a single one and by an uppercase <span class="strong"><em class="calibre9">X</em></span> if there are many. Using the uppercase letter <span class="strong"><em class="calibre9">X</em></span> we intend to use matrix notation, since we can also treat the <span class="strong"><em class="calibre9">y</em></span> as a response vector (technically a column vector) and the <span class="strong"><em class="calibre9">X</em></span> as a matrix containing all values of the feature vectors, each arranged into a separate column of the matrix.</p><p class="calibre8">It is also important to always keep a note of the dimensions of <span class="strong"><em class="calibre9">X</em></span> and <span class="strong"><em class="calibre9">y</em></span>; thus, by convention, we can call <span class="strong"><em class="calibre9">n</em></span> the number of observations and <span class="strong"><em class="calibre9">p</em></span> the number of variables. Consequently our <span class="strong"><em class="calibre9">X</em></span> will be a matrix of size (<span class="strong"><em class="calibre9">n</em></span>, <span class="strong"><em class="calibre9">p</em></span>), and our <span class="strong"><em class="calibre9">y</em></span> will always be a vector of size <span class="strong"><em class="calibre9">n</em></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip08" class="calibre1"/>Tip</h3><p class="calibre8">Throughout the book, we will also have recourse to statistical notation, which is actually a bit more explicit and verbose. A statistical formula tries to give an idea of all the predictors involved in the formula (we will show an example of this later) whereas matrix notation is more implicit.</p></div><p class="calibre8">We can <a id="id80" class="calibre1"/>affirm that, when we are learning to predict from <a id="id81" class="calibre1"/>data in a supervised way, we are actually<a id="id82" class="calibre1"/> building a function that can answer the question about<a id="id83" class="calibre1"/> how <span class="strong"><em class="calibre9">X</em></span> can imply <span class="strong"><em class="calibre9">y</em></span>.</p><p class="calibre8">Using these new matrix symbolic notations, we can define a function, a functional mapping that can translate <span class="strong"><em class="calibre9">X</em></span> values into <span class="strong"><em class="calibre9">y</em></span> without error or with an acceptable margin of error. We can affirm that all our work will be to determinate a function of the following kind:</p><div class="mediaobject"><img src="../images/00005.jpeg" alt="Linear models and supervised learning" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">When the function is specified, and we have in mind a certain algorithm with certain parameters and an <span class="strong"><em class="calibre9">X</em></span> matrix made up of certain data, conventionally we can refer to it as a hypothesis. The term is suitable because we can intend our function as a ready hypothesis, set with all its parameters, to be tested if working more or less well in predicting our target <span class="strong"><em class="calibre9">y</em></span>.</p><p class="calibre8">Before talking about the function (the supervised algorithm that does all the magic), we should first spend some time reasoning about what feeds the algorithm itself. We have already introduced the matrix <span class="strong"><em class="calibre9">X</em></span>, the predictive variables, and the vector <span class="strong"><em class="calibre9">y</em></span>, the target answer variable; now it is time to explain how we can extract them from our data and what exactly their role is in a learning algorithm.</p><div class="book" title="Reflecting on predictive variables"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec01" class="calibre1"/>Reflecting on predictive variables</h3></div></div></div><p class="calibre8">Reflecting on<a id="id84" class="calibre1"/> the role of your predictive variable in a <a id="id85" class="calibre1"/>supervised algorithm, there are a few caveats that you have to keep in mind throughout our illustrations in the book, and yes, they are very important and decisive.</p><p class="calibre8">To store the predictive variables, we use a matrix, usually called the <span class="strong"><em class="calibre9">X</em></span> matrix:</p><div class="mediaobject"><img src="../images/00006.jpeg" alt="Reflecting on predictive variables" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In this example, our <span class="strong"><em class="calibre9">X</em></span> is made up of only one variable and it contains <span class="strong"><em class="calibre9">n</em></span> cases (or observations).</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre8">If you would like to know when to use a variable or feature, just consider that in machine learning <span class="strong"><em class="calibre9">feature</em></span> and <span class="strong"><em class="calibre9">attribute</em></span> are terms that are favored over <span class="strong"><em class="calibre9">variable</em></span>, which has a definitively statistical flavor hinting at something that varies. Depending on the context and audience, you can effectively use one or the other.</p></div><p class="calibre8">In Python <a id="id86" class="calibre1"/>code, you can build a one-column matrix <a id="id87" class="calibre1"/>structure by typing:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: import numpy as np</strong></span>
<span class="strong"><strong class="calibre2">  vector = np.array([1,2,3,4,5])</strong></span>
<span class="strong"><strong class="calibre2">  row_vector = vector.reshape((5,1))</strong></span>
<span class="strong"><strong class="calibre2">  column_vector = vector.reshape((1,5))</strong></span>
<span class="strong"><strong class="calibre2">  single_feature_matrix = vector.reshape((1,5))</strong></span>
</pre></div><p class="calibre8">Using the NumPy <code class="email">array</code> we can quickly derive a vector and a matrix. If you start from a Python list, you will get a vector (which is neither a row nor a column vector, actually). By using the <code class="email">reshape</code> method, you can transform it into a row or column vector, based on your specifications.</p><p class="calibre8">Real-world data usually need matrices that are more complex, and real-world matrices comprise uncountable different data columns (the variety element of big data). Most likely, a standard <span class="strong"><em class="calibre9">X</em></span> matrix will have more columns, so the notation we will be referring to is:</p><div class="mediaobject"><img src="../images/00007.jpeg" alt="Reflecting on predictive variables" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, our matrix has more variables, all <span class="strong"><em class="calibre9">p</em></span> variables, so its size is <span class="strong"><em class="calibre9">n</em></span> x <span class="strong"><em class="calibre9">p</em></span>. In Python, there are two methods to make up such a data matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In:  multiple_feature_matrix = np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])</strong></span>
</pre></div><p class="calibre8">You just have to transform with the <code class="email">array</code> function a list of lists, where each internal list is a row <a id="id88" class="calibre1"/>matrix; or you create a vector with your data and<a id="id89" class="calibre1"/> then reshape it in the shape of your desired matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: multiple_feature_matrix = \</strong></span>
<span class="strong"><strong class="calibre2">np.array([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15]])</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre8">In NumPy there are also special functions for rapidly creating matrices of ones and zeros. As an argument, just specify the intended (<code class="email">x</code>, <code class="email">y</code>) shape in a tuple:</p><p class="calibre8"><code class="email">all_zeros = np.zeros((5,3))</code></p><p class="calibre8"><code class="email">all_ones = np.ones((5,3))</code></p></div><p class="calibre8">The information present in the set of observations from the past, that we are using as <span class="strong"><em class="calibre9">X</em></span>, can deeply affect how we are going to build the link between our <span class="strong"><em class="calibre9">X</em></span> and the <span class="strong"><em class="calibre9">y</em></span>.</p><p class="calibre8">In fact, usually it is the case that we do not know the full range of possible associations between <span class="strong"><em class="calibre9">X</em></span> and <span class="strong"><em class="calibre9">y</em></span> because:</p><div class="book"><ul class="itemizedlist"><li class="listitem">We have just observed a certain <span class="strong"><em class="calibre9">X</em></span>, so our experience of <span class="strong"><em class="calibre9">y</em></span> for a given <span class="strong"><em class="calibre9">X</em></span> is biased, and this is a sampling bias because, as in a lottery, we have drawn only certain numbers in a game and not all the available ones</li><li class="listitem">We never observed certain <span class="strong"><em class="calibre9">(X, y)</em></span> associations (please note the formulation in a tuple, indicating the interconnection between <span class="strong"><em class="calibre9">X</em></span> and <span class="strong"><em class="calibre9">y</em></span>), because they never happened before, but that does not exclude them from happening in the future (and <a id="id90" class="calibre1"/>incidentally we are striving to forecast the future)</li></ul></div><p class="calibre8">There is little to do with the second problem, (we can extrapolate the future only through the directions pointed out by the past), but you can actually check how recent the data you are using is. If you are trying to forecast in a context that is very susceptible to changes and mutable from day to day, you have to keep in mind that your data could quickly become outdated and you may be unable to guess new trends. An example of a mutable context where we constantly need to update models is the advertising sector (where the competitive scenery is frail and continually changing). Consequently, you continually need to gather fresher data that could allow you to build a much more effective supervised algorithm.</p><p class="calibre8">As for the first problem, you can solve it using more and more cases from different sources. The more you sample, the more likely your drawn set of <span class="strong"><em class="calibre9">X</em></span> will resemble a complete set of possible and true associations of <span class="strong"><em class="calibre9">X</em></span> with <span class="strong"><em class="calibre9">y</em></span>. This is understandable via an important idea in probability and statistics: the law of large numbers.</p><p class="calibre8">The law<a id="id91" class="calibre1"/> of large numbers suggests that, as the number<a id="id92" class="calibre1"/> of your experiments grows, so the likelihood that the average of their results will represent the true value (that the experiments themselves are trying to figure out) will increase.</p><p class="calibre8">Supervised algorithms learn from large samples<a id="id93" class="calibre1"/> of historical data, called <span class="strong"><strong class="calibre2">batches</strong></span>, fetched all at once from large data repositories, such as databases or data lakes. Alternatively, they also could pick the examples that are most useful for their learning by themselves and <a id="id94" class="calibre1"/>ignore the bulk of the data (this is called <span class="strong"><strong class="calibre2">active learning</strong></span> and it is a kind of semi-supervised learning that we won't discuss here).</p><p class="calibre8">If our environment is fast-paced, they also could just stream data as it is available, continuously adapting to any new association between the predictive variables and the response (this is called online learning and we will discuss it in <a class="calibre1" title="Chapter 7. Online and Batch Learning" href="part0046_split_000.html#1BRPS2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 7</a>, <span class="strong"><em class="calibre9">Online and Batch Learning</em></span>).</p><p class="calibre8">Another important aspect of the <span class="strong"><em class="calibre9">X</em></span> matrix of predictors to be considered is that up to now we assumed that we could deterministically derive the response <span class="strong"><em class="calibre9">y</em></span> using the information in the matrix <span class="strong"><em class="calibre9">X</em></span>. Unfortunately this is not always so in the real world and it is not rare that you actually try to figure out your response <span class="strong"><em class="calibre9">y</em></span> using a completely wrong set of predictive <span class="strong"><em class="calibre9">X</em></span>. In such cases, you have to figure out that you are actually wasting your time in trying to fit something working between your <span class="strong"><em class="calibre9">X</em></span> and <span class="strong"><em class="calibre9">y</em></span> and that you should look for some different <span class="strong"><em class="calibre9">X</em></span> (again more data in the sense of more variables).</p><p class="calibre8">According to the model used, having more variables and cases is usually beneficial under different points of view. More cases reduces the possibility of learning from a biased and limited set of observations. Many algorithms can better estimate their internal parameters (and produce more accurate predictions) if trained using large sets of observations. Also, having more variables at hand can be beneficial, but in the sense that it increases the chance of having explicative features to be used for machine learning. Many algorithms are in fact sensitive to redundant information and noise present in features, consequently requiring some feature selection to reduce the predictors involved in the model. This is quite the case with linear regression, which can surely take advantage of more cases for its training, but it should also receive a parsimonious and efficient set of features to perform at its best. Another important aspect to know about the <span class="strong"><em class="calibre9">X</em></span> matrix is that it should be made up solely of numbers. Therefore, it really matters what you are working with. You can work with the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Physical measurements, which are always OK because they are naturally numbers (for example, height)</li><li class="listitem">Human measurements, which are a bit less OK, but are still fine when they have a certain order (that is, all numbers that we give as scores based on our judgment) and so they can be converted into rank numbers (such as 1, 2, and 3 for the first, second, and third values, and so on)</li></ul></div><p class="calibre8">We call such values quantitative measurements. We expect quantitative measurement to be continuous and that means a quantitative variable can take any real positive or negative number as a valid value. Human measurements are usually only positive, starting from zero or one, so it is just a fair approximation to consider them quantitative.</p><p class="calibre8">For physical<a id="id95" class="calibre1"/> measurements, in statistics, we distinguish <a id="id96" class="calibre1"/>between interval and ratio variables. The difference is that ratio variables have a natural zero whereas in interval data the zero is an arbitrary one. A good example is temperature; in fact, unless you use the Kelvin scale, whose zero is an absolute one, both Fahrenheit and Celsius have arbitrary scales. The main implication is about the ratios (if the zero is arbitrary, the ratio is also arbitrary).</p><p class="calibre8">Human measurements that are<a id="id97" class="calibre1"/> numerical are called <span class="strong"><strong class="calibre2">ordinal variables</strong></span>. Unlike interval data, ordinal data does not have a natural zero. Moreover, the interval between each value on an interval scale is equal and regular; however, in an ordinal scale, though the distance between the values is the same, their real distance could be very different. Let's think of a scale made of three textual values: good, average, and bad. Next, let's say that we arbitrarily decide that good is 3, average is 2, and bad is 1. We call this arbitrary assignment of values ordinal encoding. Now, from a mathematical point of view, though the interval between 3 and 2 in respect of the interval from 2 to 1 is the same (that is, one point), are we really sure that the real distance between good and average is the same as that from average and bad? For instance, in terms of customer satisfaction, does it costs the same effort going from an evaluation of bad to one of average and from one of average to one of excellent?</p><p class="calibre8">Qualitative measurements (for example, a value judgment such as good, average, or bad, or an attribute such as <a id="id98" class="calibre1"/>being colored red, green, or blue) need some work to be done, some clever data manipulation, but they can still be part of our <span class="strong"><em class="calibre9">X</em></span> matrix using the right transformation. Even more unstructured qualitative information (such as text, sound, or a drawing) can be transformed and reduced to a pool of numbers and can be ingested into an <span class="strong"><em class="calibre9">X</em></span> matrix.</p><p class="calibre8">Qualitative variables can be stored as numbers into single-value vectors or they can have a vector for each class. In such a case, we are talking about binary variables (also called dummy variables in statistical language).</p><p class="calibre8">We are going to discuss in greater detail how to transform the data at hand, especially if its type is qualitative, into an input matrix suitable for supervised learning in <a class="calibre1" title="Chapter 5. Data Preparation" href="part0035_split_000.html#11C3M2-a2faae6898414df7b4ff4c9a487a20c6">Chapter 5</a>, <span class="strong"><em class="calibre9">Data Preparation</em></span>.</p><p class="calibre8">As a starting point before working on the data itself, it is necessary to question the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The quality of the data—that is, whether the data available can really represent the right information pool for extracting <span class="strong"><em class="calibre9">X</em></span>-<span class="strong"><em class="calibre9">y</em></span> rules</li><li class="listitem">The <a id="id99" class="calibre1"/>quantity of data—that is, checking<a id="id100" class="calibre1"/> how much data is available, keeping in mind that, for building robust machine learning solutions, it is safer to have a large variety of variables and cases (at least when you're dealing with thousands of examples)</li><li class="listitem">The extension of data in time—that is, checking how much time the data spans in the past (since we are learning from the past)</li></ul></div></div><div class="book" title="Reflecting on response variables"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch02lvl3sec02" class="calibre1"/>Reflecting on response variables</h3></div></div></div><p class="calibre8">Reflecting on the role of the response variable, our attention should be first drawn to what type of variable<a id="id101" class="calibre1"/> we are going to predict, because that will <a id="id102" class="calibre1"/>distinguish the type of supervised problem to be solved.</p><p class="calibre8">If our response variable is a quantitative one, a numeric value, our problem will be a regression one. Ordinal variables can be solved as a regression problem, especially if they take many different distinct values. The output of a regression supervised algorithm is a value that can be directly used and compared with other predicted values and with the real response values used for learning.</p><p class="calibre8">For instance, as an example of a regression problem, in the real estate business a regression model could predict the value of a house just from some information about its location and its characteristics, allowing an immediate discovery of market prices that are too cheap or too expensive by using the model's predictions as an indicator of a fair fact-based estimation (if we can reconstruct the price by a model, it is surely well justified by the value of the measurable characteristics we used as our predictors).</p><p class="calibre8">If our response variable is a qualitative one, our problem is one of classification. If we have to guess between just two <a id="id103" class="calibre1"/>classes, our problem is called <span class="strong"><strong class="calibre2">a binary classification</strong></span>; otherwise, if more classes are involved, it is a called a multi-label classification problem.</p><p class="calibre8">For instance, if we want to guess the winner in a game between two football teams, we have a binary classification problem because we just need to know if the first team will win or not (the two classes are <span class="strong"><em class="calibre9">team wins</em></span>, <span class="strong"><em class="calibre9">team loses</em></span>). A multi-label classification could instead be used to predict which football team among a certain number will win (so in our prediction, the classes to be guessed are the teams).</p><p class="calibre8">Ordinal variables, if they do not take many distinct values, can be solved as a multi-label classification problem. For instance, if you have to guess the final ranking of a team in a football championship, you could try to predict its final position in the leader board as a class. Consequently, in this ordinal problem you have to guess many classes corresponding to different positions in the championship: class 1 could represent the first position, class 2 the second position, and so on. In conclusion, you could figure the final ranking of a team as the positional class whose likelihood of winning is the greatest.</p><p class="calibre8">As for the output, classification algorithms can provide both classification into a precise class and an estimate of the probability of being part of any of the classes at hand.</p><p class="calibre8">Continuing with examples from the real estate business, a classification model could predict if a house could be a bargain or if it could increase its value given its location and its characteristics, thus allowing a careful investment selection.</p><p class="calibre8">The most <a id="id104" class="calibre1"/>noticeable problem with response variables <a id="id105" class="calibre1"/>is their exactness. Measurement errors in regression problems and misclassification in classification ones can damage the ability of your model to perform well on real data by providing inaccurate information to be learned. In addition, biased information (such as when you provide cases of a certain class and not from all those available) can hurt the capacity of your model to predict in real-life situations because it will lead the model to look at data from a non-realistic point of view. Inaccuracies in the response variable are more difficult and more dangerous for your model than problems with your features.</p><p class="calibre8">For single predictors, the outcome variable <span class="strong"><em class="calibre9">y</em></span> is also a vector. In NumPy, you just set it up as a generic vector or as a column vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: y = np.array([1,2,3,4,5]).reshape((5,1))</strong></span>
</pre></div></div></div></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Approaching Simple Linear Regression">
<div class="book" title="Defining a regression problem">
<div class="book" title="The family of linear models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec23" class="calibre1"/>The family of linear models</h2></div></div></div><p class="calibre8">The family of linear models is so named because the function that specifies the relationship between the <span class="strong"><em class="calibre9">X</em></span>, the predictors, and the <span class="strong"><em class="calibre9">y</em></span>, the target, is a linear combination of the <span class="strong"><em class="calibre9">X</em></span> values. A linear<a id="id106" class="calibre1"/> combination is just a sum where each addendum value is modified by a weight. Therefore, a linear model is simply a smarter form of a summation.</p><p class="calibre8">Of course there is a trick in this summation that makes the predictors perform like they do while predicting the answer value. As we mentioned before, the predictors should tell us something, they should give us some hint about the answer variable; otherwise any machine learning algorithm won't work properly. We can predict our response because the information about the answer is already somewhere inside the features, maybe scattered, twisted, or transformed, but it is just there. Machine learning just gathers and reconstructs such information.</p><p class="calibre8">In linear models, such inner information is rendered obvious and extracted by the weights used for the summation. If you actually manage to have some meaningful predictors, the weights will just do all the heavy work to extract it and transform it into a proper and exact answer.</p><p class="calibre8">Since the <span class="strong"><em class="calibre9">X</em></span> matrix is a numeric one, the sum of its elements will result in a number itself. Linear models are consequently the right tool for solving any regression problem, but they are not limited to just guessing real numbers. By a transformation of the response variable, they can be enabled to predict counts (positive integer numbers) and probabilities relative to being part of a certain group or class (or not).</p><p class="calibre8">In statistics, the<a id="id107" class="calibre1"/> linear model family is called the <span class="strong"><strong class="calibre2">generalized linear model</strong></span> (<span class="strong"><strong class="calibre2">GLM</strong></span>). By means of special link functions, proper transformation of the answer variable, proper constraints on the weights and different optimization procedures (the learning procedures), GLM can solve a very wide range of different problems. In this book, our treatise won't extend beyond what is necessary to the statistical field. However, we will propose a couple of models of the larger family of the GLM, namely linear regression and logistic regression; both methods are appropriate to solve the two most basic problems in data science: regression and classification.</p><p class="calibre8">Because linear <a id="id108" class="calibre1"/>regression does not require any particular transformation of the answer variable and because it is conceptually the real foundation of linear models, we will start by understanding how it works. To make things easier, we will start from the case of a linear model using just a single predictor variable, a so<a id="id109" class="calibre1"/>-called <span class="strong"><strong class="calibre2">simple linear regression</strong></span>. The predictive power of a simple linear regression is very limited in comparison with its multiple form, where many predictors at once contribute to the model. However, it is much easier to understand and figure out its functioning.</p></div></div></div>

<div class="book" title="Chapter&#xA0;2.&#xA0;Approaching Simple Linear Regression">
<div class="book" title="Defining a regression problem">
<div class="book" title="Preparing to discover simple linear regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch02lvl2sec24" class="calibre1"/>Preparing to discover simple linear regression</h2></div></div></div><p class="calibre8">We provide <a id="id110" class="calibre1"/>some practical <a id="id111" class="calibre1"/>examples in Python throughout the book and do not leave explanations about the various regression models at a purely theoretical level. Instead, we will explore together some example datasets, and systematically illustrate to you the commands necessary to achieve a working regression model, interpret its structure, and deploy a predicting application.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip11" class="calibre1"/>Tip</h3><p class="calibre8">A dataset<a id="id112" class="calibre1"/> is a data structure containing predictive variables and sometimes response ones. For machine learning purposes, it can be structured or semi-structured into a matrix form, in the shape of a table with rows and columns.</p></div><p class="calibre8">For the initial presentation of the linear regression in its simple version (using only one predictive variable to forecast the response variable), we have chosen a couple of datasets relative to real estate evaluation.</p><p class="calibre8">Real estate is quite an interesting topic for an automatic predictive model since there is quite a lot of freely available data from censuses and, being an open market, even more data can be scraped from websites monitoring the market and its offers. Moreover, because the renting or buying of a house is quite an important economic decision for many individuals, online services that help to gather and digest the large amounts of available information are indeed a good business model idea.</p><p class="calibre8">The first dataset is quite a historical one. Taken from the paper by Harrison, D. and Rubinfeld, D.L. <span class="strong"><em class="calibre9">Hedonic Housing Prices and the Demand for Clean Air</em></span> (J. Environ. Economics &amp; Management, vol.5, 81-102, 1978), the dataset can be found in many analysis<a id="id113" class="calibre1"/> packages and is present at the UCI Machine Learning Repository (<a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Housing">https://archive.ics.uci.edu/ml/datasets/Housing</a>).</p><p class="calibre8">The dataset is made up of 506 census tracts of Boston from the 1970 census and it features 21 variables regarding various aspects that could impact real estate value. The target variable is the median monetary value of the houses, expressed in thousands of USD. Among the available features, there are some fairly obvious ones such as the number of rooms, the age of the buildings, and the crime levels in the neighborhood, and some others that are a bit less obvious, such as the pollution concentration, the availability of nearby<a id="id114" class="calibre1"/> schools, the access to highways, and the distance from employment centers.</p><p class="calibre8">The second dataset from the Carnegie Mellon University Statlib repository (<a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Housing">https://archive.ics.uci.edu/ml/datasets/Housing</a>) contains 20,640 observations derived from the 1990 US Census. Each observation is a series of statistics (9 predictive variables) regarding a block group—that is, approximately 1,425 individuals living in a geographically compact area. The target variable is an indicator of the house value of that block (technically it is the natural logarithm of the median house value at the time of the census). The predictor variables are basically median income.</p><p class="calibre8">The <a id="id115" class="calibre1"/>dataset has been used in Pace and Barry (1997), <span class="strong"><em class="calibre9">Sparse Spatial Autoregressions</em></span>, <span class="strong"><em class="calibre9">Statistics and Probability Letters</em></span>, (<a class="calibre1" href="http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf">http://www.spatial-statistics.com/pace_manuscripts/spletters_ms_dir/statistics_prob_lets/pdf/fin_stat_letters.pdf</a>), a paper on regression analysis including spatial variables (information about places including their position or their nearness to other places in the analysis). The idea behind the dataset is that variations of house values can be explained by exogenous variables (that is, external to the house itself) representing population, the density of buildings, and the population's affluence aggregated by area.</p><p class="calibre8">The code for downloading the data is as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: from sklearn.datasets import fetch_california_housing</strong></span>
<span class="strong"><strong class="calibre2">  from sklearn.datasets import load_boston</strong></span>
<span class="strong"><strong class="calibre2">  boston = load_boston()</strong></span>
<span class="strong"><strong class="calibre2">  california = fetch_california_housing()</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip12" class="calibre1"/>Tip</h3><p class="calibre8">As the Boston dataset is already included in the Scikit-learn package, the <a id="id116" class="calibre1"/>California one has to be downloaded from the Statlib datasets archive and it is necessary for you to have an Internet connection. It <a id="id117" class="calibre1"/>will also take some time, depending on your speed of connection.</p></div><p class="calibre8">We will be<a id="id118" class="calibre1"/> exclusively using the Boston dataset in this<a id="id119" class="calibre1"/> and the following chapters, but you can explore the California one and try to replicate the analysis done on the Boston one. Please remember to use the preceding code snippet before running any other code or you won't have the <code class="email">boston</code> and <code class="email">california</code> variables available for analysis.</p></div></div></div>

<div class="book" title="Starting from the basics"><div class="book" id="I3QM2-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec13" class="calibre1"/>Starting from the basics</h1></div></div></div><p class="calibre8">We will start<a id="id120" class="calibre1"/> exploring the first dataset, the Boston dataset, but before delving into numbers, we will upload a series of helpful packages that will be used during the rest of the chapter:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: import numpy as np</strong></span>
<span class="strong"><strong class="calibre2">  import pandas as pd</strong></span>
<span class="strong"><strong class="calibre2">  import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong class="calibre2">  import matplotlib as mpl</strong></span>
</pre></div><p class="calibre8">If you are working from an IPython Notebook, running the following command in a cell will instruct the Notebook to represent any graphic output in the Notebook itself (otherwise, if you are not working on IPython, just ignore the command because it won't work in IDEs such as Python's IDLE or Spyder):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: %matplotlib inline</strong></span>
<span class="strong"><strong class="calibre2">  # If you are using IPython, this will make the images available in the Notebook</strong></span>
</pre></div><p class="calibre8">To immediately select the variables that we need, we just frame all the data available into a Pandas data structure, <code class="email">DataFrame</code>.</p><p class="calibre8">Inspired by a similar data structure present in the R statistical language, a <code class="email">DataFrame</code> renders data vectors of different types easy to handle under the same dataset variable, offering at the same time much convenient functionality for handling missing values and manipulating data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: dataset = pd.DataFrame(boston.data, columns=boston.feature_names)</strong></span>
<span class="strong"><strong class="calibre2">  dataset['target'] = boston.target</strong></span>
</pre></div><p class="calibre8">At this point, we are ready to build our first regression model, learning directly from the data present in our <a id="id121" class="calibre1"/>Pandas DataFrame.</p><p class="calibre8">As we <a id="id122" class="calibre1"/>mentioned, linear regression is just a simple summation, but it is indeed not the simplest model possible. The simplest is the statistical mean. In fact, you can simply guess by always using the same constant number, and the mean very well absolves such a role because it is a powerful descriptive number for data summary.</p><p class="calibre8">The mean works very well with normally distributed data but often it is quite suitable even for different distributions. A normally distributed curve is a distribution of data that is symmetric and has certain characteristics regarding its shape (a certain height and spread).</p><p class="calibre8">The characteristics of a normal distribution are defined by formulas and there are appropriate statistical tests to find out if your variable is normal or not, since many other distributions resemble the bell shape of the normal one and many different normal distributions are generated by different mean and variance parameters.</p><p class="calibre8">The key to understanding if a distribution is normal is the <span class="strong"><strong class="calibre2">probability density function</strong></span> (<span class="strong"><strong class="calibre2">PDF</strong></span>), a <a id="id123" class="calibre1"/>function describing the probability of values in the distribution.</p><p class="calibre8">In the case of a normal distribution, the PDF is as follows:</p><div class="mediaobject"><img src="../images/00008.jpeg" alt="Starting from the basics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In such a formulation, the symbol <span class="strong"><em class="calibre9">µ</em></span> represents the mean (which coincides with the median and the mode) and the symbol <span class="strong"><em class="calibre9">σ</em></span> is the variance. Based on different means and variances, we can calculate different value distributions, as the following code demonstrates and visualizes:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong class="calibre2">import numpy as np</strong></span>
<span class="strong"><strong class="calibre2">import matplotlib.mlab as mlab</strong></span>
<span class="strong"><strong class="calibre2">import math</strong></span>
<span class="strong"><strong class="calibre2">x = np.linspace(-4,4,100)</strong></span>
<span class="strong"><strong class="calibre2">for mean, variance in [(0,0.7),(0,1),(1,1.5),(-2,0.5)]:</strong></span>
<span class="strong"><strong class="calibre2">    plt.plot(x,mlab.normpdf(x,mean,variance))</strong></span>
<span class="strong"><strong class="calibre2">plt.show()</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00009.jpeg" alt="Starting from the basics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Because of its properties, the <a id="id124" class="calibre1"/>normal distribution is a fundamental distribution in statistics since all statistical models involve working on normal<a id="id125" class="calibre1"/> variables. In particular, when the mean is zero and the variance is one (unit variance), the normal distribution, called a standard normal distribution under such conditions, has even more favorable characteristics for statistical models.</p><p class="calibre8">Anyway, in the real world, normally distributed variables are instead rare. Consequently, it is important to verify that the actual distribution we are working on is not so far from an ideal normal one or it will pose problems in your expected results. Normally distributed variables are an important requirement for statistical models (such as mean and, in certain aspects, linear regression). On the contrary, machine learning models do not depend on any previous assumption about how your data should be distributed. But, as a matter of fact, even machine learning models work well if data has certain characteristics, so working with a normally distributed variable is preferable to other distributions. Throughout the book, we will provide warnings about what to look for and check when building and applying machine learning solutions.</p><p class="calibre8">For the calculation of a mean, relevant problems can arise if the distribution is not symmetric and there are extreme cases. In such an occurrence, the extreme cases will tend to draw the mean estimate towards them, which consequently won't match with the bulk of the data. Let's then calculate the mean of the value of the 506 tracts in Boston:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: mean_expected_value = dataset['target'].mean()</strong></span>
</pre></div><p class="calibre8">In this<a id="id126" class="calibre1"/> case, we calculated the mean using a method available in the Pandas DataFrame; however, the NumPy function <code class="email">mean</code> can be also called to calculate a mean from an array of data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: np.mean(dataset['target'])</strong></span>
</pre></div><p class="calibre8">In terms of a mathematical formulation, we can express this simple solution as follows:</p><div class="mediaobject"><img src="../images/00010.jpeg" alt="Starting from the basics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can now evaluate the results by measuring the error produced in predicting the real <span class="strong"><em class="calibre9">y</em></span> values by this rule. Statistics suggest that, to measure the difference between the prediction and the<a id="id127" class="calibre1"/> real value, we should square the differences and then sum them all. This is called <span class="strong"><strong class="calibre2">the squared sum of errors</strong></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: Squared_errors = pd.Series(mean_expected_value -\</strong></span>
<span class="strong"><strong class="calibre2">                           dataset['target'])**2</strong></span>
<span class="strong"><strong class="calibre2">  SSE = np.sum(Squared_errors)</strong></span>
<span class="strong"><strong class="calibre2">  print ('Sum of Squared Errors (SSE): %01.f' % SSE)</strong></span>
</pre></div><p class="calibre8">Now that we have calculated it, we can visualize it as a distribution of errors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: density_plot = Squared_errors.plot('hist')</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00011.jpeg" alt="Starting from the basics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The plot shows how frequent certain errors are in respect of their values. Therefore, you will immediately notice that most errors are around zero (there is a high density around that value). Such a situation can be considered a good one, since in most cases the mean is a good approximation, but some errors are really very far from the zero and they can attain <a id="id128" class="calibre1"/>considerable values (don't forget that the errors are squared, anyway, so the effect is emphasized). When trying to figure out such values, your approach will surely lead to a relevant error and we should find a way to minimize it using a more sophisticated approach.</p></div>

<div class="book" title="Starting from the basics">
<div class="book" title="A measure of linear relationship"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec25" class="calibre1"/>A measure of linear relationship</h2></div></div></div><p class="calibre8">Evidently, the mean is not a good representative of certain values, but it is certainly a good baseline to start from. Certainly, an important problem with the mean is its being fixed, whereas the target variable is changeable. However, if we assume that the target variable changes<a id="id129" class="calibre1"/> because of the effect of some other variable we are measuring, then we can adjust the mean with respect to the variations in cause.</p><p class="calibre8">One improvement on our previous approach could be to build a mean conditional on certain values of another variable (or even more than one) actually related to our target, whose variation is somehow similar to the variation of the target one.</p><p class="calibre8">Intuitively, if we know the dynamics we want to predict with our model, we can try to look for variables that we know can impact the answer values.</p><p class="calibre8">In the real estate business, we actually know that usually the larger a house is, the more expensive it is; however, this rule is just part of the story and the price is affected by many other considerations. For the moment, we will keep it simple and just assume that an extension to a house is a factor that positively affects the price, and consequently, more space equals more costs when building the house (more land, more construction materials, more work, and consequently a higher price).</p><p class="calibre8">Now, we have a variable that we know should change with our target and we just need to measure it and extend our initial formula based on constant values with something else.</p><p class="calibre8">In statistics, there is a measure that helps to measure how (in the sense of how much and in what direction) two variables relate to each other: <span class="strong"><strong class="calibre2">correlation</strong></span>.</p><p class="calibre8">In correlation, a <a id="id130" class="calibre1"/>few steps are to be considered. First, your variables have to be standardized (or your result won't be a correlation but a covariation, a measure of association that is affected by the scale of the variables you are working with).</p><p class="calibre8">In statistical <span class="strong"><em class="calibre9">Z</em></span> score standardization, you subtract from each variable its mean and then you divide the <a id="id131" class="calibre1"/>result by the standard deviation. The resulting transformed variable will have a mean of 0 and a standard deviation of 1 (or unit variance, since variance is the squared standard deviation).</p><p class="calibre8">The formula for standardizing a variable is as follows:</p><div class="mediaobject"><img src="../images/00012.jpeg" alt="A measure of linear relationship" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This can be achieved in Python using a simple function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: def standardize(x):</strong></span>
<span class="strong"><strong class="calibre2">           return (x-np.mean(x))/np.std(x)</strong></span>
</pre></div><p class="calibre8">After standardizing, you compare the squared difference of each variable with its own mean. If the<a id="id132" class="calibre1"/> two differences agree in sign, their multiplication will become positive (evidence that they have the same directionality); however, if they differ, the multiplication will turn negative. By summing all the multiplications between the squared differences, and dividing them by the number of observations, you will finally get the correlation which will be a number ranging from -1 to 1.</p><p class="calibre8">The absolute value of the correlation will provide you with the intensity of the relation between the two variables compared, 1 being a sign of a perfect match and zero a sign of complete independence between them (they have no relation between them). The sign instead will hint at the proportionality; positive is direct (when one grows the other does the same), negative is indirect (when one grows, the other shrinks).</p><p class="calibre8">Covariance can <a id="id133" class="calibre1"/>be expressed as follows:</p><div class="mediaobject"><img src="../images/00013.jpeg" alt="A measure of linear relationship" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Whereas, Pearson's correlation can be expressed as follows:</p><div class="mediaobject"><img src="../images/00014.jpeg" alt="A measure of linear relationship" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Let's check these two formulations directly on Python. As you may have noticed, Pearson's correlation is really covariance calculated on standardized variables, so we define the <code class="email">correlation</code> function as a wrapper of both the <code class="email">covariance</code> and <code class="email">standardize</code> ones (you can find all these functions ready to be imported from <code class="email">Scipy</code>; we are actually recreating them here just to help you understand how they work):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: </strong></span>
<span class="strong"><strong class="calibre2">def covariance(variable_1, variable_2, bias=0):</strong></span>
<span class="strong"><strong class="calibre2">       observations = float(len(variable_1))</strong></span>
<span class="strong"><strong class="calibre2">      return np.sum((variable_1 - np.mean(variable_1)) * \</strong></span>
<span class="strong"><strong class="calibre2">      (variable_2 - np.mean(variable_2)))/(observations-min(bias,1))</strong></span>

<span class="strong"><strong class="calibre2">  def standardize(variable):</strong></span>
<span class="strong"><strong class="calibre2">      return (variable - np.mean(variable)) / np.std(variable)</strong></span>

<span class="strong"><strong class="calibre2">  def correlation(var1,var2,bias=0):</strong></span>
<span class="strong"><strong class="calibre2">      return covariance(standardize(var1), standardize(var2),bias)</strong></span>

<span class="strong"><strong class="calibre2">  from scipy.stats.stats import pearsonr</strong></span>
<span class="strong"><strong class="calibre2">  print ('Our correlation estimation: %0.5f' % (correlation(dataset['RM'], dataset['target'])))</strong></span>
<span class="strong"><strong class="calibre2">  print ('Correlation from Scipy pearsonr estimation: %0.5f' % pearsonr(dataset['RM'], dataset['target'])[0])</strong></span>

<span class="strong"><strong class="calibre2">Out: Our correlation estimation: 0.69536</strong></span>
<span class="strong"><strong class="calibre2">    Correlation from Scipy pearsonr estimation: 0.69536</strong></span>
</pre></div><p class="calibre8">Our <a id="id134" class="calibre1"/>correlation estimation for the relation between the value of the target variable and the average number of rooms in houses in the area is 0.695, which is positive and remarkably strong, since the maximum positive score of a correlation is 1.0.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip13" class="calibre1"/>Tip</h3><p class="calibre8">As a way to estimate if a correlation is relevant or not, just square it; the result will represent the percentage of the variance shared by the two variables.</p></div><p class="calibre8">Let's graph what <a id="id135" class="calibre1"/>happens when we correlate two variables. Using a <span class="strong"><strong class="calibre2">scatterplot</strong></span>, we can easily<a id="id136" class="calibre1"/> visualize the two involved variables. A scatterplot is a graph where the values of two variables are treated as Cartesian coordinates; thus, for every (<span class="strong"><em class="calibre9">x</em></span>, <span class="strong"><em class="calibre9">y</em></span>) value a point is represented in the graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: x_range = [dataset['RM'].min(),dataset['RM'].max()]</strong></span>
<span class="strong"><strong class="calibre2">  y_range = [dataset['target'].min(),dataset['target'].max()]</strong></span>
<span class="strong"><strong class="calibre2">  scatter_plot = dataset.plot(kind='scatter', x='RM', y='target',\xlim=x_range, ylim=y_range)</strong></span>
<span class="strong"><strong class="calibre2">  meanY = scatter_plot.plot(x_range, [dataset['target'].mean(),\  dataset['target'].mean()], '--' , color='red', linewidth=1)</strong></span>
<span class="strong"><strong class="calibre2">  meanX = scatter_plot.plot([dataset['RM'].mean(),\dataset['RM'].mean()], y_range, '--', color='red', linewidth=1)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00015.jpeg" alt="A measure of linear relationship" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The scatterplot also plots the average value for both the target and the predictor variables as dashed lines. This divides the plot into four quadrants. If we compare it with the previous covariance and correlation formulas, we can understand why the correlation value was <a id="id137" class="calibre1"/>close to 1: in the bottom-right and in top-left quadrants, there are just a few mismatching points where one of variables is above its average and the other is below its own.</p><p class="calibre8">A perfect match (correlation values of 1 or -1) is possible only when the points are in a straight line (and all points are therefore concentrated in the right-uppermost and left-lowermost quadrants). Thus, correlation is a measure of linear association, of how close to a straight line your points are. Ideally, having all your points on a single line favors a perfect mapping of your predictor variable to your target.</p></div></div>

<div class="book" title="Extending to linear regression"><div class="book" id="J2B82-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec14" class="calibre1"/>Extending to linear regression</h1></div></div></div><p class="calibre8">Linear regression<a id="id138" class="calibre1"/> tries to fit a line through a given set of points, choosing the best fit. The best fit is the line that minimizes the summed squared difference between the value dictated by the line for a certain value of <span class="strong"><em class="calibre9">x</em></span> and its corresponding <span class="strong"><em class="calibre9">y</em></span> values. (It is optimizing the same squared error that we met before when checking how good a mean was as a predictor.)</p><p class="calibre8">Since linear regression is a line; in bi-dimensional space (<span class="strong"><em class="calibre9">x</em></span>, <span class="strong"><em class="calibre9">y</em></span>), it takes the form of the classical formula of a line in a Cartesian plane: <span class="strong"><em class="calibre9">y = mx + q</em></span>, where <span class="strong"><em class="calibre9">m</em></span> is the angular coefficient (expressing the angle between the line and the <span class="strong"><em class="calibre9">x</em></span> axis) and <span class="strong"><em class="calibre9">q</em></span> is the intercept between the line and the <span class="strong"><em class="calibre9">x</em></span> axis.</p><p class="calibre8">Formally, machine learning indicates the correct expression for a linear regression as follows:</p><div class="mediaobject"><img src="../images/00016.jpeg" alt="Extending to linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, again, <span class="strong"><em class="calibre9">X</em></span> is a matrix of the predictors, <span class="strong"><em class="calibre9">β</em></span> is a matrix of coefficients, and <span class="strong"><em class="calibre9">β<sub class="calibre20">0</sub></em></span> is a constant value called<a id="id139" class="calibre1"/> the <span class="strong"><strong class="calibre2">bias</strong></span> (it is the same as the Cartesian formulation, only the notation is different).</p><p class="calibre8">We can better<a id="id140" class="calibre1"/> understand its functioning mechanism by seeing it in action with Python, first using the <code class="email">StatsModels</code> package, then using the Scikit-learn one.</p></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="Regressing with Statsmodels"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec26" class="calibre1"/>Regressing with Statsmodels</h2></div></div></div><p class="calibre8">Statsmodels is<a id="id141" class="calibre1"/> a package designed with statistical analysis<a id="id142" class="calibre1"/> in mind; therefore, its function offers quite a rich output of statistical checks and information. Scalability is not an issue for the package; therefore, it is really a good starting point for learning, but is certainly not the optimal solution if you have to crunch quite large datasets (or even big data) because of its optimization algorithm.</p><p class="calibre8">There are two different methods (two modules) to work out a linear regression with Statsmodels:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">statsmodels.api</code>: This works with distinct predictor and answer variables and requires <a id="id143" class="calibre1"/>you to define any transformation of the variables on the predictor variable, including adding the intercept</li><li class="listitem"><code class="email">statsmodels.formula.api</code>: This works in a similar way to R, allowing you to<a id="id144" class="calibre1"/> specify a functional form (the formula of the summation of the predictors)</li></ul></div><p class="calibre8">We will illustrate our example using the <code class="email">statsModels.api</code>; however, we will also show you an alternative method with <code class="email">statsmodels.formula.api</code>.</p><p class="calibre8">As a first step, let's upload both the modules of Statsmodels, naming them as conventionally indicated in the package documentation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: import statsmodels.api as sm</strong></span>
<span class="strong"><strong class="calibre2">  import statsmodels.formula.api as smf</strong></span>
</pre></div><p class="calibre8">As a second step, it is necessary to define the <span class="strong"><em class="calibre9">y</em></span> and <span class="strong"><em class="calibre9">X</em></span> variables:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: y = dataset['target']</strong></span>
<span class="strong"><strong class="calibre2">  X = dataset['RM']</strong></span>
<span class="strong"><strong class="calibre2">  X = sm.add_constant(X)</strong></span>
</pre></div><p class="calibre8">The <span class="strong"><em class="calibre9">X</em></span> variable needs to be extended by a constant value <code class="email">()</code>; the bias will be calculated accordingly. In fact, as you remember, the formula of a linear regression is as follows:</p><div class="mediaobject"><img src="../images/00017.jpeg" alt="Regressing with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">However, using <code class="email">StatsModels.api</code>, the formula actually becomes the following:</p><div class="mediaobject"><img src="../images/00018.jpeg" alt="Regressing with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This can be<a id="id145" class="calibre1"/> interpreted as a combination of the variables<a id="id146" class="calibre1"/> in <span class="strong"><em class="calibre9">X</em></span>, multiplied by its corresponding <span class="strong"><em class="calibre9">β</em></span> value.</p><p class="calibre8">Consequently, the predictor <span class="strong"><em class="calibre9">X</em></span> now contains both the predictive variable and a unit constant. Also, <span class="strong"><em class="calibre9">β</em></span> is no longer a single coefficient, but a vector of coefficients.</p><p class="calibre8">Let's have a visual confirmation of this by requiring the first values of the Pandas DataFrame using the <code class="email">head</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: X.head()</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00019.jpeg" alt="Regressing with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">At this point, we just need to set the initialization of the linear regression calculation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: linear_regression = sm.OLS(y,X)</strong></span>
</pre></div><p class="calibre8">Also, we need to ask for the estimation of the regression coefficients, the <span class="strong"><em class="calibre9">β</em></span> vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: fitted_model = linear_regression.fit()</strong></span>
</pre></div><p class="calibre8">If we had wanted to manage the same result using the <code class="email">StatsModels.formula.api</code>, we should have typed the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: linear_regression = smf.ols(formula='target ~ RM', data=dataset)</strong></span>
<span class="strong"><strong class="calibre2">  fitted_model = linear_regression.fit()</strong></span>
</pre></div><p class="calibre8">The previous two code lines simultaneously comprise both steps seen together, without requiring any particular variable preparation since the bias is automatically incorporated. In fact, the specification about how the linear regression should work is incorporated into the string <code class="email">target ~ RM</code>, where the variable name left of the tilde (<code class="email">~</code>) indicates the answer variable, the variable name (or names, in the case of a multiple regression analysis) on the right being for the predictor.</p><p class="calibre8">Actually, <code class="email">smf.ols</code> expects quite a different input compared to <code class="email">sm.OLS</code>, because it can accept our <a id="id147" class="calibre1"/>entire original dataset (it selects what variables <a id="id148" class="calibre1"/>are to be used by using the provided formula), whereas <code class="email">sm.OLS</code> expects a matrix containing just the features to be used for prediction. Consequently, some caution has to be exercised when using two such different approaches.</p><p class="calibre8">A summary (a method of the fitted model) can quickly tell you everything that you need to know about regression analysis. In case you have tried <code class="email">statsmodesl.formula.api</code>, we also re-initialize the linear regression using the <code class="email">StatsModels.api</code> since they are not working on the same <span class="strong"><em class="calibre9">X</em></span> and our following code relies on <code class="email">sm.OLS</code> specifications:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: linear_regression = sm.OLS(y,X)</strong></span>
<span class="strong"><strong class="calibre2">  fitted_model = linear_regression.fit()</strong></span>
<span class="strong"><strong class="calibre2">  fitted_model.summary()</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00020.jpeg" alt="Regressing with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00021.jpeg" alt="Regressing with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00022.jpeg" alt="Regressing with Statsmodels" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">You will <a id="id149" class="calibre1"/>receive quite a long series of tables containing many<a id="id150" class="calibre1"/> statistical tests and information. Though quite daunting at the beginning, you actually do not need all these outputs, unless the purpose of your analysis is a statistical one. Data science is mainly concerned with real models working on predicting real data, not on formally correct specifications of statistical problems. Nevertheless, some of these outputs are still useful for successful model building <a id="id151" class="calibre1"/>and we are going to provide you with an<a id="id152" class="calibre1"/> insight into the main figures.</p><p class="calibre8">Before explaining the outputs, we first need to extract two elements from the fitted model: the coefficients and the predictions calculated on the data on which we built the model. They both are going to come in very handy during the following explanations:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: print (fitted_model.params)</strong></span>
<span class="strong"><strong class="calibre2">  betas = np.array(fitted_model.params)</strong></span>
<span class="strong"><strong class="calibre2">  fitted_values = fitted_model.predict(X)</strong></span>
</pre></div></div></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="The coefficient of determination"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec27" class="calibre1"/>The coefficient of determination</h2></div></div></div><p class="calibre8">Let's start from the first table of results. The first table is divided into two columns. The first one contains<a id="id153" class="calibre1"/> a description of the fitted model:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Dep. Variable</strong></span>: It just reminds you what the target variable was</li><li class="listitem"><span class="strong"><strong class="calibre2">Model</strong></span>: Another reminder of the model that you have fitted, the OLS is ordinary least squares, another way to refer to linear regression</li><li class="listitem"><span class="strong"><strong class="calibre2">Method</strong></span>: The parameters fitting method (in this case least squares, the classical computation method)</li><li class="listitem"><span class="strong"><strong class="calibre2">No. Observations</strong></span>: The number of observations that have been used</li><li class="listitem"><span class="strong"><strong class="calibre2">DF Residuals</strong></span>: The degrees of freedom of the residuals, which is the number of observations minus the number of parameters</li><li class="listitem"><span class="strong"><strong class="calibre2">DF Model</strong></span>: The number of estimated parameters in the model (excluding the constant term from the count)</li></ul></div><p class="calibre8">The second table gives a more interesting picture, focusing how good the fit of the linear regression model is and pointing out any possible problems with the model:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">R-squared</strong></span>: This is the coefficient of determination, a measure of how well the regression does with respect to a simple mean.</li><li class="listitem"><span class="strong"><strong class="calibre2">Adj. R-squared</strong></span>: This is the coefficient of determination adjusted based on the number of parameters in a model and the number of observations that helped build it.</li><li class="listitem"><span class="strong"><strong class="calibre2">F-statistic</strong></span>: This is a measure telling you if, from a statistical point of view, all your coefficients, apart from the bias and taken together, are different from zero. In simple words, it tells you if your regression is really better than a simple average.</li><li class="listitem"><span class="strong"><strong class="calibre2">Prob (F-statistic)</strong></span>: This is the <a id="id154" class="calibre1"/>probability that you got that F-statistic just by lucky chance due to the observations that you have used (such a probability is actually called the <span class="strong"><strong class="calibre2">p-value</strong></span> of F-statistic). If it is low enough you can be confident that your regression is really better than a simple mean. Usually in statistics and science a test probability has to be equal or lower than 0.05 (a conventional criterion of statistical significance) for having such a confidence.</li><li class="listitem"><span class="strong"><strong class="calibre2">AIC</strong></span>: This is the <span class="strong"><strong class="calibre2">Akaike Information Criterion</strong></span>. AIC is a score that evaluates<a id="id155" class="calibre1"/> the model based on the number of observations and the complexity of the model itself. The lesser the AIC score, the better. It is very useful for comparing different models and for statistical variable selection.</li><li class="listitem"><span class="strong"><strong class="calibre2">BIC</strong></span>: This is the <span class="strong"><strong class="calibre2">Bayesian Information Criterion</strong></span>. It works as AIC, but it presents<a id="id156" class="calibre1"/> a higher penalty for models with more parameters.</li></ul></div><p class="calibre8">Most of these<a id="id157" class="calibre1"/> statistics make sense when we are dealing with more than one predictor variable, so they will be discussed in the next chapter. Thus, for the moment, as we are working with a simple linear regression, the two measures that are worth examining closely are F-statistic and R-squared. F-statistic is actually a test that doesn't tell you too much if you have enough observations and you can count on a minimally correlated predictor variable. Usually it shouldn't be much of a concern in a data science project.</p><p class="calibre8">R-squared is instead much more interesting because it tells you how much better your regression model is in comparison to a single mean. It does so by providing you with a percentage of the unexplained variance of a mean as a predictor that actually your model was able to explain.</p><p class="calibre8">If you want to compute the measure yourself, you just have to calculate the sum of squared errors of the mean of the target variable. That's your baseline of unexplained variance (the variability in house prices that in our example we want to explain by a model). If from that baseline you subtract the sum of squared errors of your regression model, you will get the residual sum of squared errors, which can be compared using a division with your baseline:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: mean_sum_squared_errors = np.sum((dataset['target']-\dataset['target'].mean())**2)</strong></span>
<span class="strong"><strong class="calibre2">  regr_sum_squared_errors = np.sum((dataset['target']-\fitted_values)**2)</strong></span>
<span class="strong"><strong class="calibre2">  (mean_sum_squared_errors-\regr_sum_squared_errors) / mean_sum_squared_errors</strong></span>

<span class="strong"><strong class="calibre2">Out: 0.48352545599133412</strong></span>
</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="tip14" class="calibre1"/>Tip</h3><p class="calibre8">When working with floats, rounding errors are possible, so don't be afraid if some of the lesser decimals don't match in your calculations; if they match the 8th decimal, you can be quite confident that the result is the same.</p></div><p class="calibre8">Ideally, if you <a id="id158" class="calibre1"/>can reduce your sum of squared errors of the regression to zero, you will get the maximum percentage of explained variance—that is, a score of 1.</p><p class="calibre8">The R-squared measure is also comparable with the percentage that you obtain squaring the correlation between your predictor and the target variable.</p><p class="calibre8">In our example, it is 0.484, which actually is exactly our R-squared correlation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: (pearsonr(dataset['RM'], dataset['target'])[0])**2</strong></span>

<span class="strong"><strong class="calibre2">Out: 0.4835254559913339</strong></span>
</pre></div><p class="calibre8">As we have seen, R-squared is perfectly aligned with the squared errors that the linear regression is trying to minimize; thus, a better R-squared means a better model. However, there are some problems with the measure (and with linear regression itself, actually) when working with more predictors at once. Again, we have to wait until we model more predictors at once; therefore, just for a simple linear regression, a better R-squared should hint at a better model.</p></div></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="Meaning and significance of coefficients"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec28" class="calibre1"/>Meaning and significance of coefficients</h2></div></div></div><p class="calibre8">The second output table informs us about the coefficients and provides us with a series of tests. These<a id="id159" class="calibre1"/> tests can make us confident that we have not been fooled by <a id="id160" class="calibre1"/>a few extreme observations in the foundations of our analysis or by some other problem:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">coef</strong></span>: The estimated coefficient</li><li class="listitem"><span class="strong"><strong class="calibre2">std err</strong></span>: The standard error of the estimate of the coefficient; the larger it is, the more uncertain the estimation of the coefficient</li><li class="listitem"><span class="strong"><strong class="calibre2">t</strong></span>: The t-statistic value, a <a id="id161" class="calibre1"/>measure indicating whether the coefficient true value is different from zero</li><li class="listitem"><span class="strong"><strong class="calibre2">P &gt; |t|</strong></span>: The p-value indicating the probability that the coefficient is different from zero just by chance</li><li class="listitem"><span class="strong"><strong class="calibre2">[95.0% Conf. Interval]</strong></span>: The lower and upper values of the coefficient, considering 95% of all the chances of having different observations and so different estimated coefficients</li></ul></div><p class="calibre8">From a data science viewpoint, t-tests and confidence bounds are not very useful because we are mostly interested in <a id="id162" class="calibre1"/>verifying whether our regression is working while predicting answer variables. Consequently, we will focus just on the <code class="email">coef</code> value (the estimated coefficients) and on their standard error.</p><p class="calibre8">The coefficients are the most important output that we can obtain from our regression model because they allow us to re-create the weighted summation that can predict our outcomes.</p><p class="calibre8">In our example, our coefficients are <span class="strong"><em class="calibre9">−34.6706</em></span> for the bias (also called the <span class="strong"><strong class="calibre2">intercept</strong></span>, recalling the<a id="id163" class="calibre1"/> formula for a line in a Cartesian space) and <span class="strong"><em class="calibre9">9.1021</em></span> for the <code class="email">RM</code> variable. Recalling <a id="id164" class="calibre1"/>our formula, we can plug in the numbers we obtained:</p><div class="mediaobject"><img src="../images/00023.jpeg" alt="Meaning and significance of coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, if you<a id="id165" class="calibre1"/> replace the betas and <span class="strong"><em class="calibre9">X</em></span> with the estimated coefficients, and the variables' names with <span class="strong"><em class="calibre9">−34.6706</em></span> and <span class="strong"><em class="calibre9">9.1021</em></span>, everything becomes the following:</p><div class="mediaobject"><img src="../images/00024.jpeg" alt="Meaning and significance of coefficients" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, if you know the average number of rooms in an area of Boston, you can make a quick estimate of the expected value. For instance, <span class="strong"><em class="calibre9">x<sub class="calibre20">RM</sub></em></span> is <code class="email">4.55</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: 9.1021*4.55-34.6706</strong></span>

<span class="strong"><strong class="calibre2">Out: 6.743955</strong></span>
</pre></div><p class="calibre8">We have to notice two points here. First, in such a formulation, the beta of each variable becomes its <span class="strong"><em class="calibre9">unit change</em></span> measure, which corresponds to the change the outcome will undergo if the variable increases by one unit. In our case, our average room space becomes <code class="email">5.55</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: 9.1021*5.55-34.6706</strong></span>

<span class="strong"><strong class="calibre2">Out: 15.846055</strong></span>
</pre></div><p class="calibre8">The increase for a unit change in <span class="strong"><em class="calibre9">x<sub class="calibre20">RM</sub></em></span> corresponds to a change in the outcome equivalent to <span class="strong"><em class="calibre9">β<sub class="calibre20">RM</sub></em></span>. The other point to be noticed is that, if our average room space becomes 1 or 2, our estimated value will turn negative, which is completely unrealistic. This is because the mapping between predictor and the target variable happened in a delimited bound of values of the predictor:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: (np.min(dataset['RM']),np.max(dataset['RM']))</strong></span>
<span class="strong"><strong class="calibre2">Out: (3.5609999999999999, 8.7799999999999994)</strong></span>
</pre></div><p class="calibre8">Whenever we try to estimate our answer values using an <span class="strong"><em class="calibre9">x</em></span> (or a set of <span class="strong"><em class="calibre9">X</em></span>) that is outside the boundaries we used for fitting the model, we risk a response that has not been optimized at all by the linear regression calculations. Expressed in another way, linear regression can learn what it sees, and, unless there is a clear linear functional form between the predictor and the target (they can be truly expressed as a line), you risk weird estimations when your predictors have an unusual value. In other words, a linear regression can always work within the range of values it learned from (this is called <span class="strong"><strong class="calibre2">interpolation</strong></span>) but <a id="id166" class="calibre1"/>can provide correct values for its learning boundaries (a different<a id="id167" class="calibre1"/> predictive activity called <span class="strong"><strong class="calibre2">extrapolation</strong></span>) only<a id="id168" class="calibre1"/> in certain conditions.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip15" class="calibre1"/>Tip</h3><p class="calibre8">As we previously mentioned, the number of observations used for fitting the model is of paramount importance to obtain a robust and reliable linear regression model. The more observations, the less likely the model is to be surprised by unusual values when running in production.</p></div><p class="calibre8">Standard errors<a id="id169" class="calibre1"/> instead are very important because they signal a weak or unclear relationship between the predictor and the answer. You can notice this by dividing the standard error by its beta. If the ratio is 0.5 or even larger, then it's a clear sign that the model has little confidence that it provided you with the right coefficient estimates. Having more cases is always the solution because it can reduce the standard errors of the coefficients and improve our estimates; however, there are also other methods to reduce errors, such as removing the redundant variance present among the features by a principal component analysis or selecting a parsimonious set of predictors by greedy selections. All these topics will be discussed when we work with multiple predictors; at this point in the book, we will illustrate the remedies to such a problem.</p></div></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="Evaluating the fitted values"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch02lvl2sec29" class="calibre1"/>Evaluating the fitted values</h2></div></div></div><p class="calibre8">The last table<a id="id170" class="calibre1"/> deals with an analysis of the residuals of the regression. The residuals are the difference between the target values and the predicted fitted values:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Skewness</strong></span>: This is a<a id="id171" class="calibre1"/> measure of the symmetry of the residuals around the mean. For symmetric distributed residuals, the value should be around zero. A positive value indicates a long tail to the right; a negative value a long tail to the left.</li><li class="listitem"><span class="strong"><strong class="calibre2">Kurtosis</strong></span>: This is a<a id="id172" class="calibre1"/> measure of the shape of the distribution of the residuals. A bell-shaped <a id="id173" class="calibre1"/>distribution has a zero measure. A negative value points to a too flat distribution; a positive one has too great a peak.</li><li class="listitem"><span class="strong"><strong class="calibre2">Omnibus D'Angostino's test</strong></span>: This is a <a id="id174" class="calibre1"/>combined statistical test for skewness and <a id="id175" class="calibre1"/>kurtosis.</li><li class="listitem"><span class="strong"><strong class="calibre2">Prob(Omnibus)</strong></span>: This is the <a id="id176" class="calibre1"/>Omnibus statistic turned into a probability.</li><li class="listitem"><span class="strong"><strong class="calibre2">Jarque-Bera</strong></span>: This is another<a id="id177" class="calibre1"/> test of skewness and kurtosis.</li><li class="listitem"><span class="strong"><strong class="calibre2">Prob (JB)</strong></span>: This is the JB<a id="id178" class="calibre1"/> statistic turned into a probability.</li><li class="listitem"><span class="strong"><strong class="calibre2">Durbin-Watson</strong></span>: This is a<a id="id179" class="calibre1"/> test for the presence of correlation among the residuals (relevant during <a id="id180" class="calibre1"/>analysis of time-based data).</li><li class="listitem"><span class="strong"><strong class="calibre2">Cond. No</strong></span>: This is a<a id="id181" class="calibre1"/> test for multicollinearity (we will deal with the concept of multicollinearity when working with many predictors).</li></ul></div><p class="calibre8">A close analysis of residuals is quite relevant in statistical practice since it can highlight the presence of serious problems with regression analysis. When working with a single variable it is interesting to visually check its residuals to figure out if there are strange cases or if the residuals don't distribute randomly. In particular, it is important to keep an eye out for any of these three problems showing up:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Values too far from the average. Large standardized residuals hint at a serious difficulty when modeling such observations. Also, in the process of learning these values, the regression coefficients may have been distorted.</li><li class="listitem" value="2">Different variance in respect of the value of the predictor. If the linear regression is an average conditioned on the predictor, dishomogeneous variance points out that the regression is not working properly when the predictor has certain values.</li><li class="listitem" value="3">Strange shapes in the cloud of residual points may indicate that you need a more complex model for the data you are analyzing.</li></ol><div class="calibre18"/></div><p class="calibre8">In our case, we can easily compute the residuals by subtracting the fitted values from the answer <a id="id182" class="calibre1"/>variable and then plotting the resulting standardized residuals in a graph:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: residuals = dataset['target']-fitted_values</strong></span>
<span class="strong"><strong class="calibre2">  normalized_residuals = standardize(residuals)</strong></span>

<span class="strong"><strong class="calibre2">In: residual_scatter_plot = plt.plot(dataset['RM'], normalized_residuals,'bp')</strong></span>
<span class="strong"><strong class="calibre2">mean_residual = plt.plot([int(x_range[0]),round(x_range[1],0)], [0,0], '-', color='red', linewidth=2)</strong></span>
<span class="strong"><strong class="calibre2">upper_bound = plt.plot([int(x_range[0]),round(x_range[1],0)], [3,3], '--', color='red', linewidth=1)</strong></span>
<span class="strong"><strong class="calibre2">lower_bound = plt.plot([int(x_range[0]),round(x_range[1],0)], [-3,-3], '--', color='red', linewidth=1)</strong></span>
<span class="strong"><strong class="calibre2">plt.grid()</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00025.jpeg" alt="Evaluating the fitted values" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The resulting scatterplot indicates that the residuals show some of the problems we previously indicated as a warning that something is not going well with your regression analysis. First, there are a few points lying outside the band delimited by the two dotted lines at normalized residual values −3 and +3 (a range that should hypothetically cover 99.7% of values if the residuals have a normal distribution). These are surely influential points with large errors and they can actually make the entire linear regression under-perform. We will talk about possible solutions to this problem when we discuss outliers in the next chapter.</p><p class="calibre8">Then, the cloud of points is not at all randomly scattered, showing different variances at different values of the <a id="id183" class="calibre1"/>predictor variable (the <span class="strong"><strong class="calibre2">abscissa axis</strong></span>) and you can spot unexpected patterns (points in a straight line, or the core points placed in a kind of U shape).</p><p class="calibre8">We are not at<a id="id184" class="calibre1"/> all surprised; the average number of rooms is likely a good predictor but it is not the only cause, or it has to be rethought as a direct cause (the number of rooms indicates a larger house, but what if the rooms are smaller than average?). This leads us to discuss whether a strong correlation really makes a variable a good working candidate for a linear relationship.</p></div></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="Correlation is not causation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch02lvl2sec30" class="calibre1"/>Correlation is not causation</h2></div></div></div><p class="calibre8">Actually, seeing a<a id="id185" class="calibre1"/> correlation between your predictor and your target <a id="id186" class="calibre1"/>variable, and managing to model it successfully using a linear regression, doesn't really mean that there is a causal relation between the two (though your regression may work very well, and even optimally).</p><p class="calibre8">Though using a data science approach, instead of a statistical one, will guarantee a certain efficacy in your model, it is easy to fall into some mistakes when having no clue why your target variable is correlated with a predictor.</p><p class="calibre8">We will tell you about six different reasons, and offer a cautionary word to help you handle such predictors without difficulty:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Direct causation</strong></span>: <span class="strong"><em class="calibre9">x</em></span> causes <span class="strong"><em class="calibre9">y</em></span>; for instance, in the real estate business the value is<a id="id187" class="calibre1"/> directly proportional to the size of the house in square meters.</li><li class="listitem"><span class="strong"><strong class="calibre2">Reciprocal effects</strong></span>: <span class="strong"><em class="calibre9">x</em></span> causes <span class="strong"><em class="calibre9">y</em></span> but it is also influenced by <span class="strong"><em class="calibre9">y</em></span>. This is quite typical of<a id="id188" class="calibre1"/> many macro-economic dynamics where the effect of a policy augments or diminishes its effects. As an example in real estate, high crime rates in an area can lower its prices but lower prices mean that the area could quickly become even more degraded and dangerous.</li><li class="listitem"><span class="strong"><strong class="calibre2">Spurious causation</strong></span>: This happens when the real cause is actually <span class="strong"><em class="calibre9">z</em></span>, which causes<a id="id189" class="calibre1"/> both <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span>; consequently it is just a fallacious illusion that <span class="strong"><em class="calibre9">x</em></span> implies <span class="strong"><em class="calibre9">y</em></span> because it is <span class="strong"><em class="calibre9">z</em></span> behind the scenes. For instance, the presence of expensive art shops and galleries may seem to correlate with house prices; in reality, both are determined by the presence of affluent residents.</li><li class="listitem"><span class="strong"><strong class="calibre2">Indirect causation</strong></span>: <span class="strong"><em class="calibre9">x</em></span> in reality is not causing <span class="strong"><em class="calibre9">y</em></span> but it is causing something else, which then <a id="id190" class="calibre1"/>causes <span class="strong"><em class="calibre9">y</em></span>. A good municipality investing in infrastructures after higher taxes can indirectly affect house prices because the area becomes more comfortable to live in, thus attracting more demand. Higher taxes, and thus more investments, indirectly affect house prices.</li><li class="listitem"><span class="strong"><strong class="calibre2">Conditional effect</strong></span>: <span class="strong"><em class="calibre9">x</em></span> causes <span class="strong"><em class="calibre9">y</em></span> in respect of the values of another variable <span class="strong"><em class="calibre9">z</em></span>; for instance, when <span class="strong"><em class="calibre9">z</em></span> has certain values <span class="strong"><em class="calibre9">x</em></span> is not influencing <span class="strong"><em class="calibre9">y</em></span> but, when <span class="strong"><em class="calibre9">z</em></span> takes <a id="id191" class="calibre1"/>particular values, the <span class="strong"><em class="calibre9">x</em></span> starts impacting <span class="strong"><em class="calibre9">y</em></span>. We also call this situation interaction. For instance the presence of schools in an area can become an attractor when the crime rate is low, so it affects house prices only when there is little criminality.</li><li class="listitem"><span class="strong"><strong class="calibre2">Random effect</strong></span>: Any<a id="id192" class="calibre1"/> recorded correlation between <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span> has been due to a lucky sampling selection; in reality there is no relationship with <span class="strong"><em class="calibre9">y</em></span> at all.</li></ul></div><p class="calibre8">The ideal case is when you have a direct causation; then, you will have a predictor in your model that will always provide you with the best values to derive your responses.</p><p class="calibre8">In the other cases, it is likely that the imperfect cause-effect relationship with the target variable will lead to more noisy estimates, especially in production when you will have to work with data not seen before by the model.</p><p class="calibre8">Reciprocal effects <a id="id193" class="calibre1"/>are more typical of econometric models. They <a id="id194" class="calibre1"/>require special types of regression analysis. Including them in your regression analysis may improve your model; however, their role may be underestimated.</p><p class="calibre8">Spurious and indirect causes will add some noise to your <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span> relationship; this could bring noisier estimates (larger standard errors). Often, the solution is to get more observations for your analysis.</p><p class="calibre8">Conditional effects, if not caught, can limit your model's ability to produce accurate estimates. If you are not aware of any of them, given your domain knowledge of the problem, it is a good step to check for any of them using some automatic procedure to test possible interactions between the variables.</p><p class="calibre8">Random effects are the worst possible thing that could happen to your model, but they are easily avoided if you follow the data science procedure that we will be describing in <a class="calibre1" title="Chapter 6. Achieving Generalization" href="part0041_split_000.html#173722-a2faae6898414df7b4ff4c9a487a20c6">Chapter 6</a>, <span class="strong"><em class="calibre9">Achieving Generalization</em></span>, when we deal with all the actions necessary to validate your model's results.</p></div></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="Predicting with a regression model"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch02lvl2sec31" class="calibre1"/>Predicting with a regression model</h2></div></div></div><p class="calibre8">When we plug the coefficients into the regression formula, predicting is just a matter of applying <a id="id195" class="calibre1"/>new data to the vector of coefficients by a matrix multiplication.</p><p class="calibre8">First, you can rely on the fitted model by providing it with an array containing new cases. In the following example, you can see how, given the <code class="email">Xp</code> variable with a single new case, this is easily predicted using the <code class="email">predict</code> method on the fitted model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: RM = 5</strong></span>
<span class="strong"><strong class="calibre2">  Xp = np.array([1,RM])</strong></span>
<span class="strong"><strong class="calibre2">  print ("Our model predicts if RM = %01.f the answer value \is %0.1f" % (RM, fitted_model.predict(Xp)))</strong></span>

<span class="strong"><strong class="calibre2">Out:  Our model predicts if RM = 5 the answer value is 10.8</strong></span>
</pre></div><p class="calibre8">A nice usage of the <code class="email">predict</code> method is to project the fitted predictions on our previous scatterplot to allow us to visualize the price dynamics in respect of our predictor, the average number of rooms:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: x_range = [dataset['RM'].min(),dataset['RM'].max()]</strong></span>
<span class="strong"><strong class="calibre2">  y_range = [dataset['target'].min(),dataset['target'].max()]</strong></span>
<span class="strong"><strong class="calibre2">  scatter_plot = dataset.plot(kind='scatter', x='RM', y='target',\xlim=x_range, ylim=y_range)</strong></span>
<span class="strong"><strong class="calibre2">  meanY = scatter_plot.plot(x_range,\[dataset['target'].mean(),dataset['target'].mean()], '--',\color='red', linewidth=1)</strong></span>
<span class="strong"><strong class="calibre2">  meanX =scatter_plot.plot([dataset['RM'].mean(),\dataset['RM'].mean()], y_range, '--', color='red', linewidth=1)</strong></span>
<span class="strong"><strong class="calibre2">  regression_line = scatter_plot.plot(dataset['RM'], fitted_values,\'-', color='orange', linewidth=1)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00026.jpeg" alt="Predicting with a regression model" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Using the <a id="id196" class="calibre1"/>previous code snippet, we will obtain the preceding graphical representation of the regression line containing a further indication of how such a line crosses the cloud of data points. For instance, thanks to this graphical display, we can notice that the regression line exactly passes at the intersection of the <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span> averages.</p><p class="calibre8">Besides the <code class="email">predict</code> method, generating the predictions is quite easy by just using the <code class="email">dot</code> function in <code class="email">NumPy</code>. After preparing an <span class="strong"><em class="calibre9">X</em></span> matrix containing both the variable data and the bias (a column of ones) and the coefficient vectors, all you have to do is to multiply the matrix by the vector. The result will itself be a vector of length equal to the number of observations:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: predictions_by_dot_product = np.dot(X,betas)</strong></span>
<span class="strong"><strong class="calibre2">  print ("Using the prediction method: %s" % fitted_values[:10])</strong></span>
<span class="strong"><strong class="calibre2">  print ("Using betas and a dot product: %s" % </strong></span>
<span class="strong"><strong class="calibre2">predictions_by_dot_product[:10])</strong></span>

<span class="strong"><strong class="calibre2">Out: Using the prediction method: [ 25.17574577  23.77402099  30.72803225  29.02593787  30.38215211</strong></span>
<span class="strong"><strong class="calibre2">  23.85593997  20.05125842  21.50759586  16.5833549   19.97844155]</strong></span>
<span class="strong"><strong class="calibre2">Using betas and a dot product: [ 25.17574577  23.77402099  30.72803225  29.02593787  30.38215211</strong></span>
<span class="strong"><strong class="calibre2">  23.85593997  20.05125842  21.50759586  16.5833549   19.97844155]</strong></span>
</pre></div><p class="calibre8">A comparison of the results obtained by the <code class="email">predict</code> method and this simple multiplication <a id="id197" class="calibre1"/>will reveal a perfect match. Because predicting from a linear regression is simple, if necessary you could even implement this multiplication on your application in a language different from Python. In such a case, you will just need to find a matrix calculation library or program a function by yourself. To our knowledge, you can easily write such a function even in the SQL script language.</p></div></div>

<div class="book" title="Extending to linear regression">
<div class="book" title="Regressing with Scikit-learn"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_7"><a id="ch02lvl2sec32" class="calibre1"/>Regressing with Scikit-learn</h2></div></div></div><p class="calibre8">As we have<a id="id198" class="calibre1"/> seen while working with the <code class="email">StatsModels</code> package, a<a id="id199" class="calibre1"/> linear model can be built using a more oriented machine learning package such as Scikit-learn. Using the <code class="email">linear_model</code> module, we can set a linear regression model specifying that the predictors shouldn't be normalized and that our model should have a bias:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: from sklearn import linear_model</strong></span>
<span class="strong"><strong class="calibre2">  linear_regression = \linear_model.LinearRegression(normalize=False,\fit_intercept=True)</strong></span>
</pre></div><p class="calibre8">Data preparation, instead, requires counting the observations and carefully preparing the predictor array to specify its two dimensions (if left as a vector, the fitting procedure will raise an error):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: observations = len(dataset)</strong></span>
<span class="strong"><strong class="calibre2">X = dataset['RM'].values.reshape((observations,1)) </strong></span>
<span class="strong"><strong class="calibre2"># X should be always a matrix, never a vector</strong></span>
<span class="strong"><strong class="calibre2">y = dataset['target'].values # y can be a vector</strong></span>
</pre></div><p class="calibre8">After completing all the previous steps, we can fit the model using the <code class="email">fit</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: linear_regression.fit(X,y)</strong></span>
</pre></div><p class="calibre8">A very convenient feature of the Scikit-learn package is that all the models, no matter their type of complexity, share the same methods. The <code class="email">fit</code> method is always used for fitting and it expects an <span class="strong"><em class="calibre9">X</em></span> and a <span class="strong"><em class="calibre9">y</em></span> (when the model is a supervised one). Instead, the two common methods for making an exact prediction (always for regression) and its probability (when the model is probabilistic) are <code class="email">predict</code> and <code class="email">predict_proba</code>, respectively.</p><p class="calibre8">After fitting the model, we can inspect the vector of the coefficients and the bias constant:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: print (linear_regression.coef_)</strong></span>
<span class="strong"><strong class="calibre2">  print (linear_regression.intercept_)</strong></span>

<span class="strong"><strong class="calibre2">Out: [ 9.10210898]</strong></span>
<span class="strong"><strong class="calibre2">    -34.6706207764</strong></span>
</pre></div><p class="calibre8">Using the <code class="email">predict</code> method and slicing the first 10 elements of the resulting list, we output the first 10 predicted values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: print (linear_regression.predict(X)[:10])</strong></span>

<span class="strong"><strong class="calibre2">Out: [ 25.17574577  23.77402099  30.72803225  29.02593787  30.38215211</strong></span>
<span class="strong"><strong class="calibre2">  23.85593997  20.05125842  21.50759586  16.5833549   19.97844155]</strong></span>
</pre></div><p class="calibre8">As previously seen, if we prepare a new matrix and we add a constant, we can calculate the results by ourselves using a simple matrix–vector multiplication:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: Xp = np.column_stack((X,np.ones(observations)))</strong></span>
<span class="strong"><strong class="calibre2">  v_coef = list(linear_regression.coef_) +\[linear_regression.intercept_]</strong></span>
</pre></div><p class="calibre8">As expected, the result of the product provides us with the same estimates as the <code class="email">predict</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: np.dot(Xp,v_coef)[:10]</strong></span>

<span class="strong"><strong class="calibre2">Out: array([ 25.17574577,  23.77402099,  30.72803225,  29.02593787,</strong></span>
<span class="strong"><strong class="calibre2">        30.38215211,  23.85593997,  20.05125842,  21.50759586,</strong></span>
<span class="strong"><strong class="calibre2">        16.5833549 ,  19.97844155])</strong></span>
</pre></div><p class="calibre8">At this point, it <a id="id200" class="calibre1"/>would be natural to question the usage of such a <code class="email">linear_model</code> module. Compared with the previous functions offered by Statsmodels, Scikit-learn seems to<a id="id201" class="calibre1"/> offer little statistical output, and one seemingly with many linear regression features stripped out. In reality, it offers exactly what is needed in data science and it is perfectly fast-performing when dealing with large datasets.</p><p class="calibre8">If you are working on IPython, just try the following simple test to generate a large dataset and check the performance of the two versions of linear regression:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: from sklearn.datasets import make_regression</strong></span>
<span class="strong"><strong class="calibre2">  HX, Hy = make_regression(n_samples=10000000, n_features=1,\n_targets=1, random_state=101)</strong></span>
</pre></div><p class="calibre8">After generating ten million observations of a single variable, start by measuring using the <code class="email">%%time</code> magic function for IPython. This magic function automatically computes how long it takes to complete the calculations in the IPython cell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: %%time</strong></span>
<span class="strong"><strong class="calibre2">  sk_linear_regression = linear_model.LinearRegression(\normalize=False,fit_intercept=True)</strong></span>
<span class="strong"><strong class="calibre2">  sk_linear_regression.fit(HX,Hy)</strong></span>

<span class="strong"><strong class="calibre2">Out: Wall time: 647 ms</strong></span>
</pre></div><p class="calibre8">Now, it is the turn of the Statsmodels package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: %%time</strong></span>
<span class="strong"><strong class="calibre2">  sm_linear_regression = sm.OLS(Hy,sm.add_constant(HX))</strong></span>
<span class="strong"><strong class="calibre2">  sm_linear_regression.fit()</strong></span>

<span class="strong"><strong class="calibre2">Out: Wall time: 2.13 s</strong></span>
</pre></div><p class="calibre8">Though a <a id="id202" class="calibre1"/>single variable is involved in the model, Statsmodels's default<a id="id203" class="calibre1"/> algorithms prove to be three times slower than Scikit-learn. We will repeat this test in the next chapter, too, when using more predictive variables in one go and other different <code class="email">fit</code> methods.</p></div></div>

<div class="book" title="Minimizing the cost function"><div class="book" id="K0RQ2-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec15" class="calibre1"/>Minimizing the cost function</h1></div></div></div><p class="calibre8">At the core of linear regression, there is the search for a line's equation that it is able to minimize<a id="id204" class="calibre1"/> the sum of the squared errors of the difference between the line's <span class="strong"><em class="calibre9">y</em></span> values and the original ones. As a reminder, let's say our regression function is called <code class="email">h</code>, and its predictions <code class="email">h(X)</code>, as in this formulation:</p><div class="mediaobject"><img src="../images/00027.jpeg" alt="Minimizing the cost function" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Consequently, our cost function to be minimized is as follows:</p><div class="mediaobject"><img src="../images/00028.jpeg" alt="Minimizing the cost function" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">There are quite a few methods to minimize it, some performing better than others in the presence of large quantities of data. Among the better performers, the most important ones<a id="id205" class="calibre1"/> are <span class="strong"><strong class="calibre2">Pseudoinverse</strong></span> (you can find this in books <a id="id206" class="calibre1"/>on <a id="id207" class="calibre1"/>statistics), <span class="strong"><strong class="calibre2">QR factorization</strong></span>, and<a id="id208" class="calibre1"/> <span class="strong"><strong class="calibre2">gradient descent</strong></span>.</p></div>

<div class="book" title="Minimizing the cost function">
<div class="book" title="Explaining the reason for using squared errors"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch02lvl2sec33" class="calibre1"/>Explaining the reason for using squared errors</h2></div></div></div><p class="calibre8">Looking<a id="id209" class="calibre1"/> under the hood of a linear regression analysis, at first it <a id="id210" class="calibre1"/>could be puzzling to realize that we are striving to minimize the squared differences between our estimates and the data from which we are building the model. Squared differences are not as intuitively explainable as absolute differences (the difference without a sign).</p><p class="calibre8">For instance, if you have to predict a monetary value, such as the price of a stock or the return from an advertising activity, you are more interested in knowing your absolute error, not your R-squared one, which could be perceived as misleading (since with squares larger losses are emphasized).</p><p class="calibre8">As we mentioned before, linear regression takes its steps from the statistical knowledge domain, and there are actually quite a few reasons in statistics that make minimizing a squared error preferable to minimizing an absolute one.</p><p class="calibre8">Unfortunately, such reasons are quite complex and too technical and consequently beyond the real scope of this book; however, from a high-level point of view, a good and reasonable explanation is that squaring nicely achieves two very important objectives:</p><div class="book"><ul class="itemizedlist"><li class="listitem">It removes negative values; therefore opposite errors won't reciprocally cancel each other when summed</li><li class="listitem">It emphasizes larger differences, because as they are squared they will proportionally increase the sum of the errors compared to a simple sum of absolute values</li></ul></div><p class="calibre8">Minimizing the squared differences with an estimator leads us to use the mean (as we suggested before as a basic model, without providing any justification for it).</p><p class="calibre8">Let's just check together using Python, without developing all the formulations. Let's define an <code class="email">x</code> vector of values:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: import numpy as np</strong></span>
<span class="strong"><strong class="calibre2">  x = np.array([9.5, 8.5, 8.0, 7.0, 6.0])</strong></span>
</pre></div><p class="calibre8">Let's also define a function returning the cost function as squared differences:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: def squared_cost(v,e):</strong></span>
<span class="strong"><strong class="calibre2">     return np.sum((v-e)**2)</strong></span>
</pre></div><p class="calibre8">Using the <code class="email">fmin</code> minimization procedure offered by the <code class="email">scipy</code> package, we try to figure out, for a vector (which will be our x vector of values), the value that makes the least squared summation:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: from scipy.optimize import fmin</strong></span>
<span class="strong"><strong class="calibre2">  xopt = fmin(squared_cost, x0=0, xtol=1e-8, args=(x,))</strong></span>

<span class="strong"><strong class="calibre2">Out: Optimization terminated successfully.</strong></span>
<span class="strong"><strong class="calibre2">      Current function value: 7.300000</strong></span>
<span class="strong"><strong class="calibre2">      Iterations: 44</strong></span>
<span class="strong"><strong class="calibre2">      Function evaluations: 88</strong></span>
</pre></div><p class="calibre8">We just output our best <code class="email">e</code> value and verify if it actually is the mean of the <code class="email">x</code> vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: print ('The result of optimization is %0.1f' % (xopt[0]))</strong></span>
<span class="strong"><strong class="calibre2">  print ('The mean is %0.1f' % (np.mean(x)))</strong></span>

<span class="strong"><strong class="calibre2">Out: The result of optimization is 78.0</strong></span>
<span class="strong"><strong class="calibre2">    The mean is 78.0</strong></span>
</pre></div><p class="calibre8">If instead we try to figure out what minimizes the sum of absolute errors:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: def absolute_cost(v,e):</strong></span>
<span class="strong"><strong class="calibre2">     return np.sum(np.abs(v-e))</strong></span>

<span class="strong"><strong class="calibre2">In: xopt = fmin(absolute_cost, x0=0, xtol=1e-8, args=(x,))</strong></span>

<span class="strong"><strong class="calibre2">Out: Optimization terminated successfully.</strong></span>
<span class="strong"><strong class="calibre2">      Current function value: 5.000000</strong></span>
<span class="strong"><strong class="calibre2">         Iterations: 44</strong></span>
<span class="strong"><strong class="calibre2">         Function evaluations: 88</strong></span>

<span class="strong"><strong class="calibre2">In: print ('The result of optimization is %0.1f' % (xopt[0]))</strong></span>
<span class="strong"><strong class="calibre2">  print ('The median is %0.1f' % (np.median(x)))</strong></span>

<span class="strong"><strong class="calibre2">Out: The result of optimization is 8.0</strong></span>
<span class="strong"><strong class="calibre2">    The median is 8.0</strong></span>
</pre></div><p class="calibre8">We will<a id="id211" class="calibre1"/> find out that it is the median, not the mean. Unfortunately, the <a id="id212" class="calibre1"/>median does not have the same statistical properties as the mean.</p></div></div>

<div class="book" title="Minimizing the cost function">
<div class="book" title="Pseudoinverse and other optimization methods"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch02lvl2sec34" class="calibre1"/>Pseudoinverse and other optimization methods</h2></div></div></div><p class="calibre8">There is an <a id="id213" class="calibre1"/>analytical formula for solving a regression <a id="id214" class="calibre1"/>analysis and getting a vector of coefficients out of data, minimizing the cost function:</p><div class="mediaobject"><img src="../images/00029.jpeg" alt="Pseudoinverse and other optimization methods" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Demonstrating this equation goes beyond the practical scope of this book, but we can test it using the power of Python coding.</p><p class="calibre8">We can therefore directly solve for this by using <code class="email">np.linalg.inv</code> from <code class="email">NumPy</code> to obtain the inverse <a id="id215" class="calibre1"/>of a matrix, or alternative methods<a id="id216" class="calibre1"/> such as solving for <span class="strong"><em class="calibre9">w</em></span> in linear equations that are called normal equations:</p><div class="mediaobject"><img src="../images/00030.jpeg" alt="Pseudoinverse and other optimization methods" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here the function <code class="email">np.linalg.solve</code> can do all the calculations for us:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: observations = len(dataset)</strong></span>
<span class="strong"><strong class="calibre2">  X  = dataset['RM'].values.reshape((observations,1)) # X should be always a matrix, never a vector</strong></span>
<span class="strong"><strong class="calibre2">  Xb = np.column_stack((X,np.ones(observations))) # We add the bias</strong></span>
<span class="strong"><strong class="calibre2">  y  = dataset['target'].values # y can be a vector</strong></span>

<span class="strong"><strong class="calibre2">  def matrix_inverse(X,y, pseudo=False):</strong></span>
<span class="strong"><strong class="calibre2">    if pseudo:</strong></span>
<span class="strong"><strong class="calibre2">        return np.dot(np.linalg.pinv(np.dot(X.T,X)),np.dot(X.T,y))</strong></span>
<span class="strong"><strong class="calibre2">    else:</strong></span>
<span class="strong"><strong class="calibre2">        return np.dot(np.linalg.inv(np.dot(X.T, X)),np.dot(X.T,y))</strong></span>

<span class="strong"><strong class="calibre2">  def normal_equations(X,y):</strong></span>
<span class="strong"><strong class="calibre2">      return np.linalg.solve(np.dot(X.T,X), np.dot(X.T,y))</strong></span>

<span class="strong"><strong class="calibre2">  print (matrix_inverse(Xb, y))</strong></span>
<span class="strong"><strong class="calibre2">  print (matrix_inverse(Xb, y, pseudo=True))</strong></span>
<span class="strong"><strong class="calibre2">  print (normal_equations(Xb, y))</strong></span>

<span class="strong"><strong class="calibre2">Out:</strong></span>
<span class="strong"><strong class="calibre2">  [  9.10210898 -34.67062078]</strong></span>
<span class="strong"><strong class="calibre2">  [  9.10210898 -34.67062078]</strong></span>
<span class="strong"><strong class="calibre2">  [  9.10210898 -34.67062078]</strong></span>
</pre></div><p class="calibre8">The only problem in solving a linear regression using these approaches is complexity, possibly some loss in accuracy of the computation when directly calculating the inverse using <code class="email">np.linalg.inv</code>, and, naturally, the fact that the <span class="strong"><em class="calibre9">X<sup class="calibre21">T</sup>X</em></span> multiplication has to be invertible (sometimes it isn't when using multiple variables that are strongly related to each other).</p><p class="calibre8">Even using <a id="id217" class="calibre1"/>another algorithm (QR factorization, a <a id="id218" class="calibre1"/>core algorithm in Statsmodels that can overcome some previously quoted numeric misbehaviors), the worst performance can be estimated to be <span class="strong"><em class="calibre9">O(n<sup class="calibre21">3</sup>)</em></span>; that is, cubic complexity.</p><p class="calibre8">Using Pseudoinverse (in NumPy, <code class="email">np.linalg.pinv</code>) can help achieve a <span class="strong"><em class="calibre9">O(n<sup class="calibre21">m</sup>)</em></span> complexity where <span class="strong"><em class="calibre9">m</em></span> is estimated to be &lt;2.37 (approximately quadratic, then).</p><p class="calibre8">This can really be a great limitation in being able to quickly estimate linear regression analysis. In fact, if you are working with <span class="strong"><em class="calibre9">10<sup class="calibre21">3</sup></em></span> observations, a feasible number of observations in statistical analysis, it will take at worst <span class="strong"><em class="calibre9">10<sup class="calibre21">9</sup></em></span> computations; however, when working with data science projects, which easily reach <span class="strong"><em class="calibre9">10<sup class="calibre21">6</sup></em></span> observations, the number of computations required to find the solution to a regression problem may rocket to <span class="strong"><em class="calibre9">10<sup class="calibre21">18</sup></em></span>.</p></div></div>

<div class="book" title="Minimizing the cost function">
<div class="book" title="Gradient descent at work"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch02lvl2sec35" class="calibre1"/>Gradient descent at work</h2></div></div></div><p class="calibre8">As an alternative <a id="id219" class="calibre1"/>to the usual classical optimization <a id="id220" class="calibre1"/>algorithms, the gradient descent technique is able to minimize the cost function of a linear regression analysis using far fewer computations. Gradient descent complexity ranks in the order <span class="strong"><em class="calibre9">O(n*p)</em></span>, thus making learning regression coefficients feasible even in the occurrence of a large <span class="strong"><em class="calibre9">n</em></span> (which stands for the number of observations) and a large <span class="strong"><em class="calibre9">p</em></span> (number of variables).</p><p class="calibre8">The method works by leveraging a simple heuristic that gradually converges on the optimal solution starting from a random one. Explaining it simply, it resembles walking blind in the mountains. If you want to descend to the lowest valley, even if you don't know and can't see the path, you can proceed approximately by going downhill for a while, then stopping, then going downhill again and so on, always aiming at each stage for where the surface descends until you arrive at a point when you cannot descend anymore. Hopefully, at that point you will have reached your destination.</p><p class="calibre8">In such a situation, your only risk is happening on an intermediate valley (where there is a wood or a lake, for instance) and mistaking it for your desired arrival because the land stops descending there.</p><p class="calibre8">In an optimization process, such a situation is defined as a local minimum (whereas your target is a global minimum instead, the best minimum possible) and it is a possible outcome of your journey downhill depending on the function you are working on minimizing. The good news is, in any case, that the error function of the linear model family is a bowl-shaped one (technically our cost function is a concave one) and it is unlikely that you can get caught anywhere if you descend properly.</p><p class="calibre8">The necessary steps to work out a gradient-descent-based solution are easily described, given our cost function for a set of coefficients (the vector <span class="strong"><em class="calibre9">w</em></span>):</p><div class="mediaobject"><img src="../images/00031.jpeg" alt="Gradient descent at work" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We first start by choosing a random initialization for <span class="strong"><em class="calibre9">w</em></span>, by choosing some random numbers (taken from a standardized normal curve, for instance, having a zero mean and unit variance).</p><p class="calibre8">Then we start reiterating an update of the values of <span class="strong"><em class="calibre9">w</em></span> (opportunely using the gradient descent computations) until the marginal improvement from the previous <span class="strong"><em class="calibre9">J(w)</em></span> is small enough to let us figure out that we have finally reached an optimum minimum.</p><p class="calibre8">We can opportunely update our coefficients, one by one, by subtracting from each of them a portion alpha (<span class="strong"><em class="calibre9">α</em></span>, the learning rate) of the partial derivative of the cost function:</p><div class="mediaobject"><img src="../images/00032.jpeg" alt="Gradient descent at work" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, in our formula, <span class="strong"><em class="calibre9">w<sub class="calibre20">j</sub></em></span> is to be intended as a single coefficient (we are iterating over them). After resolving the partial derivative, the final resolution form is as follows:</p><div class="mediaobject"><img src="../images/00033.jpeg" alt="Gradient descent at work" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Simplifying <a id="id221" class="calibre1"/>everything, our gradient for the coefficient of <span class="strong"><em class="calibre9">x</em></span> is<a id="id222" class="calibre1"/> just the average of our predicted values multiplied by their respective <span class="strong"><em class="calibre9">x</em></span> value.</p><p class="calibre8">Alpha, called<a id="id223" class="calibre1"/> the <span class="strong"><strong class="calibre2">learning rate</strong></span>, is very important in the process, because, if it is too large, it may cause the optimization to detour and fail. You have to think of each gradient as a jump or as a run in a direction. If you fully take it, you may happen to pass over the optimum minimum and end up in another rising slope. Too many consecutive long steps may even force you to climb up the cost slope, worsening your initial position (given by a cost function that is its summed square, the loss of an overall score of fitness).</p><p class="calibre8">Using a small alpha, gradient descent won't jump beyond the solution but it may take a much longer time to reach the desired minimum. How to choose the right alpha is a matter of trial and error; anyway, starting from an alpha such as 0.01 is never a bad choice, based on our experience in many optimization problems.</p><p class="calibre8">Naturally, the gradient, given the same alpha, will in any case produce shorter steps as you approach the solution. Visualizing the steps in a graph can really give you a hint about whether gradient descent is working out a solution or not.</p><p class="calibre8">Though quite conceptually simple (it is based on an intuition that we have surely applied ourselves to move step-by-step, directing where we can optimize our result), gradient descent is very effective and indeed scalable when working with real data. Such interesting characteristics have elevated it to the core optimization algorithm in machine learning; it is not limited to just the linear model family, but it can also be extended, for instance, to neural networks for the process of back propagation, which updates all the weights of the neural net in order to minimize training errors. Surprisingly, gradient descent is also at the core of another complex machine learning algorithm, gradient boosting tree ensembles, where we have an iterative process minimizing the errors using a simpler learning algorithm (a so-called <a id="id224" class="calibre1"/>
<span class="strong"><strong class="calibre2">weak learner</strong></span> because it is limited by a high bias) to progress towards optimization.</p><p class="calibre8">Here is a first implementation in Python. We will slightly modify it in the next chapter to make it work efficiently with more predictors than one:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: observations = len(dataset)</strong></span>
<span class="strong"><strong class="calibre2">  X  = dataset['RM'].values.reshape((observations,1))</strong></span>
<span class="strong"><strong class="calibre2">  # X should be always a matrix, never a vector</strong></span>
<span class="strong"><strong class="calibre2">  X = np.column_stack((X,np.ones(observations))) # We add the bias</strong></span>
<span class="strong"><strong class="calibre2">  y  = dataset['target'].values # y can be a vector</strong></span>
</pre></div><p class="calibre8">Now, after<a id="id225" class="calibre1"/> defining the response variable, selecting our <a id="id226" class="calibre1"/>predictor (the <code class="email">RM</code> feature, the average number of rooms per dwelling), and adding a bias (the constant number <code class="email">1</code>), we are ready in the following code to define all the functions in our optimization process:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">In: import random</strong></span>

<span class="strong"><strong class="calibre2">  def random_w( p ):</strong></span>
<span class="strong"><strong class="calibre2">      return np.array([np.random.normal() for j in range(p)])</strong></span>

<span class="strong"><strong class="calibre2">  def hypothesis(X,w):</strong></span>
<span class="strong"><strong class="calibre2">      return np.dot(X,w)</strong></span>

<span class="strong"><strong class="calibre2">  def loss(X,w,y):</strong></span>
<span class="strong"><strong class="calibre2">      return hypothesis(X,w) - y</strong></span>

<span class="strong"><strong class="calibre2">  def squared_loss(X,w,y):</strong></span>
<span class="strong"><strong class="calibre2">      return loss(X,w,y)**2</strong></span>

<span class="strong"><strong class="calibre2">  def gradient(X,w,y):</strong></span>
<span class="strong"><strong class="calibre2">      gradients = list()</strong></span>
<span class="strong"><strong class="calibre2">      n = float(len( y ))</strong></span>
<span class="strong"><strong class="calibre2">      for j in range(len(w)):</strong></span>
<span class="strong"><strong class="calibre2">          gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)</strong></span>
<span class="strong"><strong class="calibre2">      return gradients</strong></span>

<span class="strong"><strong class="calibre2">  def update(X,w,y, alpha=0.01):</strong></span>
<span class="strong"><strong class="calibre2">      return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]</strong></span>

<span class="strong"><strong class="calibre2">  def optimize(X,y, alpha=0.01, eta = 10**-12, iterations = 1000):</strong></span>
<span class="strong"><strong class="calibre2">      w = random_w(X.shape[1])</strong></span>
<span class="strong"><strong class="calibre2">      path = list()</strong></span>
<span class="strong"><strong class="calibre2">      for k in range(iterations):</strong></span>
<span class="strong"><strong class="calibre2">          SSL = np.sum(squared_loss(X,w,y))</strong></span>
<span class="strong"><strong class="calibre2">          new_w = update(X,w,y, alpha=alpha)</strong></span>
<span class="strong"><strong class="calibre2">          new_SSL = np.sum(squared_loss(X,new_w,y))</strong></span>
<span class="strong"><strong class="calibre2">          w = new_w</strong></span>
<span class="strong"><strong class="calibre2">          if k&gt;=5 and (new_SSL - SSL &lt;= eta and \new_SSL - SSL &gt;= -eta):</strong></span>
<span class="strong"><strong class="calibre2">              path.append(new_SSL)</strong></span>
<span class="strong"><strong class="calibre2">              return w, path</strong></span>
<span class="strong"><strong class="calibre2">          if k % (iterations / 20) == 0:</strong></span>
<span class="strong"><strong class="calibre2">              path.append(new_SSL)</strong></span>
<span class="strong"><strong class="calibre2">      return w, path</strong></span>
</pre></div><p class="calibre8">After <a id="id227" class="calibre1"/>finally defining all the functions necessary for gradient descent to <a id="id228" class="calibre1"/>work, we can start optimizing it for a solution to our single regression problem:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">IN: alpha = 0.048</strong></span>
<span class="strong"><strong class="calibre2">  w, path = optimize(X,y,alpha, eta = 10**-12, iterations = 25000)</strong></span>
<span class="strong"><strong class="calibre2">  print ("These are our final coefficients: %s" % w)</strong></span>
<span class="strong"><strong class="calibre2">  print ("Obtained walking on this path of squared loss %s" % path)</strong></span>

<span class="strong"><strong class="calibre2">Out: These are our final coefficients: [9.1021032698295059,\-34.670584445862119]</strong></span>
<span class="strong"><strong class="calibre2">  Obtained walking on this path of squared loss [369171.02494038735,   23714.645148620271, 22452.194702610999, 22154.055704515144,   22083.647505550518, 22067.019977742671, 22063.093237887566,   22062.165903044533, 22061.946904602359, 22061.895186155631,   22061.882972380481, 22061.880087987909, 22061.879406812728,   22061.879245947097, 22061.879207957238, 22061.879198985589,   22061.879196866852, 22061.879196366495, 22061.879196248334,   22061.879196220427, 22061.879196220034]</strong></span>
</pre></div><p class="calibre8">Scikit-learn <code class="email">linear_regression</code> (and other linear models present in the linear methods module) are <a id="id229" class="calibre1"/>actually powered by gradient descent, making <a id="id230" class="calibre1"/>Scikit-learn our favorite choice when working in data science projects with large and big data.</p></div></div>
<div class="book" title="Summary" id="KVCC1-a2faae6898414df7b4ff4c9a487a20c6"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch02lvl1sec16" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we introduced linear regression as a supervised machine learning algorithm. We explained its functional form, its relationship with the statistical measures of mean and correlation, and we tried to build a simple linear regression model on the Boston house prices data. After doing that we finally glanced at how regression works under the hood by proposing its key mathematical formulations and their translation into Python code.</p><p class="calibre8">In the next chapter, we will continue our discourse about linear regression, extending our predictors to multiple variables and carrying on our explanation where we left it suspended during our initial illustration with a single variable. We will also point out the most useful transformations you can apply to data to make it suitable for processing by a linear regression algorithm.</p></div></body></html>