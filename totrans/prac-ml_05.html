<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Decision Tree based learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Decision Tree based learning</h1></div></div></div><p>Starting this chapter, we will take a deep dive into each of the Machine learning algorithms. We begin with a non-parametric supervised learning method, Decision trees, and advanced techniques, used for classification and regression. We will outline a business problem that can be addressed by building a Decision tree-based model and learn how it can be implemented in Apache Mahout, R, Julia, Apache Spark, and Python.</p><p>The following topics are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Decision trees: definition, terminology, the need, advantages, and limitations.</li><li class="listitem" style="list-style-type: disc">The basics of constructing and understanding Decision trees and some key aspects such as Information gain and Entropy. You will also learn to build regression, the classification of trees and measuring errors.</li><li class="listitem" style="list-style-type: disc">Understanding some common problems with Decision trees, need for pruning Decision trees, and techniques for pruning.</li><li class="listitem" style="list-style-type: disc">You will learn Decision tree algorithms such as CART, C4.5, C5.0 and so on; and specialized trees such as Random forests, Oblique trees, Evolutionary and Hellinger trees.</li><li class="listitem" style="list-style-type: disc">Understanding a business use case for classification and regression trees, and an implementation of the same using Apache Mahout, R, Apache Spark, and Julia and Python (scikit-learn) libraries and modules.</li></ul></div><div class="section" title="Decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Decision trees</h1></div></div></div><p>Decision trees are <a id="id653" class="indexterm"/>known to be one of the most powerful and widely used modeling techniques in the field of Machine learning.</p><p>Decision trees naturally induce rules that can be used in data classification and prediction. Following is an example of a rule definition derived from building a Decision tree:</p><p>If (laptop model is <span class="emphasis"><em>x</em></span>) and (manufactured by <span class="emphasis"><em>y</em></span>) and (is <span class="emphasis"><em>z</em></span> years old) and (with some owners being <span class="emphasis"><em>k</em></span>) then (the battery life is <span class="emphasis"><em>n</em></span> hours).</p><p>When closely<a id="id654" class="indexterm"/> observed, these rules are expressed in simple, human readable, and comprehensible formats. Additionally, these rules can be stored for later reference in a data store. The following concept map depicts various characteristics and attributes of Decision trees that will be covered in the following sections.</p><div class="mediaobject"><img src="graphics/B03980_05_01.jpg" alt="Decision trees"/></div><div class="section" title="Terminology"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec70"/>Terminology</h2></div></div></div><p>Decision trees<a id="id655" class="indexterm"/> classify instances by representing in a tree structure starting from the root to a leaf. Most importantly, at a high level, there are two representations of a Decision tree—a node and an arc that connects nodes. To make a decision, the flow starts at the root nodes, navigates to the arcs until it has reached a leaf node, and then makes a decision. Each node of the tree denotes testing of an attribute, and the branches denote the possible values that the attribute can take.</p><p>Following are some of the characteristics of a Decision tree representation:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Every non-leaf node (for example, a decision node) denotes a representation of the attribute value</li><li class="listitem" style="list-style-type: disc">Every branch denotes the rest of the value representation</li><li class="listitem" style="list-style-type: disc">Every leaf (or terminal) node represents the value of the target attribute</li><li class="listitem" style="list-style-type: disc">The starting <a id="id656" class="indexterm"/>node is called the root node</li></ul></div><p>The following figure is a representation of the same:</p><div class="mediaobject"><img src="graphics/B03980_05_02.jpg" alt="Terminology"/></div></div><div class="section" title="Purpose and uses"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec71"/>Purpose and uses</h2></div></div></div><p>Decision trees are<a id="id657" class="indexterm"/> used for classification and regression. Two types of trees are used in this context:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Classification trees</li><li class="listitem" style="list-style-type: disc">Regression trees</li></ul></div><p>Classification<a id="id658" class="indexterm"/> trees are used to classify the given data set into categories. To use classification trees, the response of the target variable needs to be a categorical value such as yes/no, true/false. On the other hand, regression trees are used to address prediction requirements and are always used when the target or response variable is a numeric or discrete value such as stock value, commodity price, and so on.</p><p>The next figure depicts the purpose of the Decision tree and relevant tree category as the classification or regression tree:</p><div class="mediaobject"><img src="graphics/B03980_05_03.jpg" alt="Purpose and uses"/></div></div><div class="section" title="Constructing a Decision tree"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec72"/>Constructing a Decision tree</h2></div></div></div><p>Decision trees can be <a id="id659" class="indexterm"/>learned best by taking a simple example and constructing a Decision tree by hand. In this section, let's look at a simple example; the following table shows the dataset on hand. Our target is to predict whether a customer will accept a loan or not, given their demographics. Clearly, it will be most useful for the business user if we can come out with a rule as a model for this dataset.</p><div class="mediaobject"><img src="graphics/B03980_05_04.jpg" alt="Constructing a Decision tree"/></div><p>From the previous table, since age and experience are highly correlated, we can choose to ignore one of the attributes. This aids the feature selection implicitly.</p><p>Case 1: Let's start building the Decision tree. To start with, we will choose to split by CCAvg (the average credit card balance).</p><div class="mediaobject"><img src="graphics/B03980_05_05.jpg" alt="Constructing a Decision tree"/></div><p>With this<a id="id660" class="indexterm"/> Decision tree, we now have two very explicit rules:</p><p>
<span class="emphasis"><em>If CCAvg is medium then loan = accept</em></span> or <span class="emphasis"><em>if CCAvg is high then loan = accept</em></span>
</p><p>For more clarity in the rules, let's add the income attribute. We have two more rules:</p><p>
<span class="emphasis"><em>If CCAvg is low and income is low, then loan is not accept</em></span>
</p><p>
<span class="emphasis"><em>If CCAvg is low and income is high, then loan is accept</em></span>
</p><p>By combining the second rule here and the first two rules, we can derive the following rule:</p><p>
<span class="emphasis"><em>If (CCAvg is medium) or (CCAvg is high) or (CCAvg is low, and income is high) then loan = accept</em></span>
</p><p>Case 2: Let's start building the Decision tree using Family:</p><div class="mediaobject"><img src="graphics/B03980_05_06.jpg" alt="Constructing a Decision tree"/></div><p>In this case, there<a id="id661" class="indexterm"/> is just one rule that it is not giving an accurate result as it has only two data points.</p><p>So, choosing a valid attribute to start the tree makes a difference to the accuracy of the model. From the previous example, let's list out some core rules for building Decision trees:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We usually start building Decision trees with one attribute, split the data based on the attribute, and continue with the same process for other attributes.</li><li class="listitem" style="list-style-type: disc">There can be many Decision trees for the given problem.</li><li class="listitem" style="list-style-type: disc">The depth of the tree is directly proportional to the number of attributes chosen.</li><li class="listitem" style="list-style-type: disc">There needs to be a Termination Criteria that will determine when to stop further building the tree. In the case of no termination criteria, the model will result in the over-fitting of the data.</li><li class="listitem" style="list-style-type: disc">Finally, the output is always in the form of simple rule(s) that can be stored and applied to different datasets for classification and/or prediction.</li></ul></div><p>One of the reasons<a id="id662" class="indexterm"/> why Decision trees are preferred in the field of Machine learning is because of their robustness to errors; they can be used when there are some unknown values in the training datasets too (for example, the data for income is not available for all the records).</p><div class="section" title="Handling missing values"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec67"/>Handling missing values</h3></div></div></div><p>One of the interesting ways of assigning values to some unknowns is to see that the most common value in terms of occurrence is assigned and in some cases they can belong to the same class, if possible we should bring it closer to accuracy.</p><p>There is another probabilistic way of doing this where the prediction is distributed proportionately:</p><p>Assign a<a id="id663" class="indexterm"/> probability <span class="emphasis"><em>pi</em></span> for every value <span class="emphasis"><em>vi</em></span> of <span class="emphasis"><em>x</em></span>.</p><p>Now, assign the fraction <span class="emphasis"><em>pi</em></span> of <span class="emphasis"><em>x</em></span> to each of the descendants. These probabilities can be estimated again based on the observed frequencies of the various values for A, among the examples at node <span class="emphasis"><em>n</em></span>.</p><p>For example, let's consider a Boolean attribute <span class="emphasis"><em>A</em></span>. Let there be 10 values for <span class="emphasis"><em>A</em></span> out of which three have a value of True and the rest 7 have a value of False. So, the probability of <span class="emphasis"><em>A(x) = True</em></span> is 0.3, and the probability that <span class="emphasis"><em>A(x) = False</em></span> is 0.7.</p><p>A fractional 0.3 of this is distributed down the branch for <span class="emphasis"><em>A = True</em></span>, and a fractional 0.7 is distributed down the other. These probability values are used for computing the information gain, and can be used if a second missing attribute value needs to be tested. The same methodology can be applied in the case of learning when we need to fill any unknowns for the new branches. The C4.5 algorithm uses this mechanism for filling the missing values.</p></div><div class="section" title="Considerations for constructing Decision trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec68"/>Considerations for constructing Decision trees</h3></div></div></div><p>The key to constructing<a id="id664" class="indexterm"/> Decision trees is knowing where to<a id="id665" class="indexterm"/> split them. To do this, we need to be clear on the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Which attribute to start and which attribute to apply subsequently?</li><li class="listitem" style="list-style-type: disc">When do we stop building the Decision tree (that is avoid over-fitting)?</li></ul></div><div class="section" title="Choosing the appropriate attribute(s)"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec18"/>Choosing the appropriate attribute(s)</h4></div></div></div><p>There are three<a id="id666" class="indexterm"/> different ways to identify the best-suited attributes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Information Gain and Entropy</li><li class="listitem" style="list-style-type: disc">Gini index</li><li class="listitem" style="list-style-type: disc">Gain ratio</li></ul></div><div class="section" title="Information gain and Entropy"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec07"/>Information gain and Entropy</h5></div></div></div><p>This entity is used in an<a id="id667" class="indexterm"/> algorithm <a id="id668" class="indexterm"/>known as<a id="id669" class="indexterm"/> C4.5. Entropy is a measure of uncertainty in the data. Let us take an intuitive approach to understand the concepts of Information gain and Entropy.</p><p>For example, consider a coin is being tossed, and there are five coins with a probability for heads as 0, 0.25, 0.5, 0.75, and 1 respectively. So, if we think which one has the highest and which one has the lowest uncertainty, then the case of 0 or 1 will be the lowest certain one and highest would be when it is 0.5. The following figure depicts the representation of the same:</p><div class="mediaobject"><img src="graphics/B03980_05_07.jpg" alt="Information gain and Entropy"/></div><p>A mathematical<a id="id670" class="indexterm"/> representation<a id="id671" class="indexterm"/> is<a id="id672" class="indexterm"/> shown here:</p><p>H = -∑p<sub>i</sub>log2p<sub>i</sub></p><p>Here, p<sub>i</sub> is the probability of a specific state.</p><p>If a system has four events with probabilities 1/2, 1/4, 1/5, and 1/8 indicate the total Entropy of the system as shown here:</p><p>H = -1/2 log2(1/2)-1/4log2(1/4)-1/5log2(1/5)-1/8log2(1/8)</p><p>In the original version of the C5.0 and C4.5 algorithms (ID3), a root node was chosen on the basis of how much of the total Entropy was reduced if this node was chosen. This is called information gain.</p><p>Information gain = Entropy of the system before split - Entropy of the system after split</p><p>Entropy in the system before split is shown as follows:</p><div class="mediaobject"><img src="graphics/B03980_05_08.jpg" alt="Information gain and Entropy"/></div><p>Entropy after using <span class="emphasis"><em>A</em></span> to split <span class="emphasis"><em>D</em></span> into <span class="emphasis"><em>v</em></span> partitions to classify <span class="emphasis"><em>D</em></span>:</p><div class="mediaobject"><img src="graphics/B03980_05_09.jpg" alt="Information gain and Entropy"/></div><p>Information<a id="id673" class="indexterm"/> gained <a id="id674" class="indexterm"/>by branching <a id="id675" class="indexterm"/>on an attribute is:</p><p>Let's now compute the information gained from our data:</p><div class="mediaobject"><img src="graphics/B03980_05_10.jpg" alt="Information gain and Entropy"/></div><p>Class P <a id="id676" class="indexterm"/>accepts the<a id="id677" class="indexterm"/> loan = yes/ 1. Class <a id="id678" class="indexterm"/>N accepts the loan = no / 0</p><p>Entropy before split is as follows:</p><div class="mediaobject"><img src="graphics/B03980_05_11.jpg" alt="Information gain and Entropy"/></div><p>This is obvious and expected as we have almost a fifty-fifty split of the data. Let's now see which attribute gives the best information gain.</p><p>In case the split is based on CCAvg and Family, the Entropy computations can be shown as follows. The total Entropy is weighted as the sum of the Entropies of each of the nodes that were created.</p><div class="mediaobject"><img src="graphics/B03980_05_12.jpg" alt="Information gain and Entropy"/></div><p>The <a id="id679" class="indexterm"/>Entropy <a id="id680" class="indexterm"/>after its split is <a id="id681" class="indexterm"/>shown here:</p><div class="mediaobject"><img src="graphics/B03980_05_13.jpg" alt="Information gain and Entropy"/></div><p>The information gain is as follows:</p><div class="mediaobject"><img src="graphics/B03980_05_14.jpg" alt="Information gain and Entropy"/></div><p>This methodology is<a id="id682" class="indexterm"/> applied to <a id="id683" class="indexterm"/>compute the information gain for all other attributes. It chooses the one with the highest information gain. This is tested at each node to select the best node.</p></div><div class="section" title="Gini index"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec08"/>Gini index</h5></div></div></div><p>Gini index <a id="id684" class="indexterm"/>is a <a id="id685" class="indexterm"/>general splitting criterion. It is named after an Italian statistician and economist—Corrado Gini. Gini Index is used to measure the probability of two random items belonging to the same class. In the case of a real dataset, this probability value is 1. The Gini measure of a node is the sum of the squares of the proportions of the classes. A node with two classes each has a score of <span class="emphasis"><em>0.52 + 0.52 = 0.5</em></span>. This is because the probability of picking the same class at random is 1 out of 2. Now, if we apply Gini index for the data set we get the following:</p><p>The original Gini Index = <span class="inlinemediaobject"><img src="graphics/B03980_05_15.jpg" alt="Gini index"/></span> = 0.502959</p><p>When split with CCAvg and Family, the Gini Index changes to the following:</p><div class="mediaobject"><img src="graphics/B03980_05_16.jpg" alt="Gini index"/></div></div><div class="section" title="Gain ratio"><div class="titlepage"><div><div><h5 class="title"><a id="ch05lvl5sec09"/>Gain ratio</h5></div></div></div><p>Another <a id="id686" class="indexterm"/>improvement in C4.5 compared to ID3 is that the factor that decides the attribute is the gain ratio. The gain ratio is the ratio of information gain and information content. The attribute that gives the maximum amount of gain ratio is the attribute that is used to split it.</p><p>Let's do <a id="id687" class="indexterm"/>some calculations with an extremely simple example to highlight why the gain ratio is a better attribute than the information gain:</p><div class="mediaobject"><img src="graphics/B03980_05_17.jpg" alt="Gain ratio"/></div><p>The dependent variable is whether they are married under a specific circumstance. Let's assume that in this case, no man is married. Whereas all women, except the last one (60 women), are married.</p><p>So, intuitively the rule has to be as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If it is a man, then he is unmarried</li><li class="listitem" style="list-style-type: disc">If it is a woman then she is married (the only isolated case where she is not married must be noise).</li></ul></div><p>Let's systematically solve this problem to gain insights into various parameters. First let's split the data into two halves as training and testing data. So, our training set consists of the last 20 males (all insensible and aged between 21-40), and the last 30 females (all married and aged between 71-99, except the last one). Testing contains the other half where all the women are married.</p><p>The gain ratio requires measure for <span class="strong"><strong>Information content</strong></span>.</p><p>Information content is defined as <span class="emphasis"><em>-f<sub>i</sub> log<sub>2</sub> f<sub>i</sub></em></span>. Note that here, we do not take the value of the dependent variable into account. We only want to know the fraction of the members in a state divided by the total members.</p><p>The information content of gender is that it has only two states; males are 20 and females are 30. So, the information content for the gender is <span class="emphasis"><em>2/5*LOG(2/5,2)-3/5*LOG(3/5,2)=0.9709</em></span>.</p><p>The <a id="id688" class="indexterm"/>information<a id="id689" class="indexterm"/> content of age is that there is a total of 49 states for the age. For the states that have only one data point, the information content is <span class="emphasis"><em>-(1/50)*log(1/50,2) = 0.1129</em></span>.</p><p>There are 48 such states with a single data point. So, their information content is (0.1129*48), 5.4192. In the last state, there are two data points. So, its information content is <span class="emphasis"><em>-(2/50 * LOG(2/50,2)) = 0.1857</em></span>. The total information content for the age is 5.6039.</p><p>The gain ratio for the gender = Information gain for gender / Information content for gender = 0.8549/0.9709 = 0.8805.</p><p>The gain ratio for the age = 0.1680</p><p>So, if we consider the gain ratio, we get that the gender is a more suitable measure. This aligns with the intuition. Let's now say that we used the gain ratio and built the tree. Our rule is if the gender is male, the person is unmarried and if it is female, the person is married.</p></div></div><div class="section" title="Termination Criteria / Pruning Decision trees"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec19"/>Termination Criteria / Pruning Decision trees</h4></div></div></div><p>Each branch is <a id="id690" class="indexterm"/>grown deeply <a id="id691" class="indexterm"/>enough to classify the training examples perfectly by the Decision tree algorithm. This can turn out to be an acceptable approach and most of the times results in problems when there is some noise in the data. In case the training dataset is too small and cannot represent the true picture of the actual data set the Decision tree might end up over-fitting the training examples.</p><p>There are many ways of avoiding over-fitting in Decision tree learning. Following are the two different cases:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">One case where the Decision tree is terminated for growth way before a perfect classification of the training data is done</li><li class="listitem" style="list-style-type: disc">Another case where the over-fitting of data is done and then the tree is pruned to recover</li></ul></div><p>Though the first case might seem to be more direct, the second case of post-pruning the over-fitting trees is more successful in reality. The reason is the difficulty to know when to stop growing the tree. Irrespective of the approach taken, it is more important to identify the criterion to determine the final, appropriate tree size.</p><p>Following are a couple of approaches to find the correct tree size:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Identify a separate and different dataset to that of the target training data set to be used, and evaluate the correctness of post-pruning nodes in the tree. This is a common approach and is called training and validation set approach.</li><li class="listitem">Instead <a id="id692" class="indexterm"/>of <a id="id693" class="indexterm"/>having a<a id="id694" class="indexterm"/> subset of data in the training set, use up all the data in the training set, and apply probabilistic methods to check if pruning a particular node has any likelihood to produce any improvement over and above the training dataset. Use all the available data for training. For example, the chi-square test can be used to check this probability.</li></ol></div><p>Reduced-Error-Pruning (D): We prune at a node by removing the subtree that is rooted at the node. We make that node a leaf (with the majority label of associated examples); algorithm is shown as follows:</p><div class="mediaobject"><img src="graphics/B03980_05_18.jpg" alt="Termination Criteria / Pruning Decision trees"/></div><p>Rule <a id="id695" class="indexterm"/>post-pruning is a <a id="id696" class="indexterm"/>more<a id="id697" class="indexterm"/> commonly used method and is a highly accurate hypotheses technique. A variation of this pruning method is used in C4.5.</p><p>Following are the steps of the Rule Post-Pruning process:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Construct a Decision tree from the training set by growing it until there is an obvious over-fitting seen.</li><li class="listitem">Generate rules from the constructed Decision tree with every path, starting from the root node to a particular leaf node mapping to a rule.</li><li class="listitem">Apply pruning to each rule for removing identified preconditions and help improve the probabilistic accuracy.</li><li class="listitem">Next, use the pruned rules in the order of their increased accuracy on the subsequent instances.</li></ol></div><p>Following are the advantages of rule-based pruning and its need for converting into rules:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Improving the readability of the rules</li><li class="listitem" style="list-style-type: disc">A consistent testing can be done at both the root and leaf level nodes</li><li class="listitem" style="list-style-type: disc">There is a clear decision that can be made of either removing the decision node or retaining it</li></ul></div></div></div><div class="section" title="Decision trees in a graphical representation"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec69"/>Decision trees in a graphical representation</h3></div></div></div><p>Until now, we have seen how <a id="id698" class="indexterm"/>Decision trees are described by dividing the data at the node and comparing the value with a constant. Another way of representing Decision trees is to visualize and have graphical representation. For example, we can choose two input attributes in two dimensions, then compare the value of one attribute with constant and show the split on the data to a parallel axis. We can also compare two attributes with one another along with a linear combination of attributes, instead of a hyperplane that is not parallel to an axis.</p><div class="mediaobject"><img src="graphics/B03980_05_19.jpg" alt="Decision trees in a graphical representation"/></div><p>Constructing multiple Decision trees for the given data is possible. The process of identifying the smallest and a perfect tree is called a minimum consistent hypothesis. Let's use two arguments to see why this is the best Decision tree:</p><p>Occam's Razor<a id="id699" class="indexterm"/> is simple; when there are two ways to solve a problem and both give the same result, the simplest of them prevails.</p><p>In data mining analysis, one is likely to fall into the trap of complex methods and large computations. So, it is essential to internalize the line of reasoning adopted by Occam. Always choose a Decision tree that has an optimum combination of size and errors.</p></div><div class="section" title="Inducing Decision trees – Decision tree algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec70"/>Inducing Decision trees – Decision tree algorithms</h3></div></div></div><p>There are many <a id="id700" class="indexterm"/>Decision tree inducing methods. Among all the methods, C4.5 and CART are the<a id="id701" class="indexterm"/> most adopted or popular ones. In this section, we will cover these methods in depth and list a brief on other methods.</p><div class="section" title="CART"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec20"/>CART</h4></div></div></div><p>CART<a id="id702" class="indexterm"/> stands for Classification<a id="id703" class="indexterm"/> and Regression Trees (Breiman et al., 1984). CART creates binary trees. This means there are always two branches that can emerge from a given node. The philosophy of the CART algorithm is to follow a <span class="emphasis"><em>goodness</em></span> criterion, which is all about choosing the best possible partition. Moreover, as the tree grows, a cost-complexity pruning mechanism is adopted. CART uses the Gini index to select appropriate attributes or the splitting criteria.</p><p>Using CART, the prior probability distribution can be provided. We can generate Regression trees using CART that in turn help in predicting real numbers against a class. The prediction is done by applying the weighted mean for the node. CART identifies splits that minimize the prediction squared error (that is, the least-squared deviation).</p><p>The depiction in the following figure of CART is for the same example referred in the previous section, where Decision tree construction is demonstrated:</p><div class="mediaobject"><img src="graphics/B03980_05_20.jpg" alt="CART"/></div></div><div class="section" title="C4.5"><div class="titlepage"><div><div><h4 class="title"><a id="ch05lvl4sec21"/>C4.5</h4></div></div></div><p>Similar to CART, C4.5 <a id="id704" class="indexterm"/>is a Decision tree algorithm with a primary<a id="id705" class="indexterm"/> difference that it can generate more than binary trees, which means support for multiway splits. For attribute selection, C4.5 uses the information gain measure. As explained in the previous section, an attribute with the largest information gain (or the lowest Entropy reduction) value helps to achieve closer to accurate classification with the least quantity of data. One of the key drawbacks of C4.5 is the need for large memory and CPU capacity for generating rules. The C5.0 algorithm is a commercial version of C4.5 that was presented in 1997.</p><p>C4.5 is an evolution of the ID3 algorithm. The gain ratio measure is used for identifying the splitting criteria. The splitting process stops when the number of splits reaches a boundary condition definition that acts as a threshold. Post this growing phase of the tree, pruning is done, and an error-based pruning method is followed.</p><p>Here is a representation of the C4.5 way of constructing the Decision tree for the same example used in the previous section:</p><div class="mediaobject"><img src="graphics/B03980_05_21.jpg" alt="C4.5"/></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Tree Induction method</p>
</th><th style="text-align: left" valign="bottom">
<p>How does it work?</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>ID3</p>
</td><td style="text-align: left" valign="top">
<p>The <a id="id706" class="indexterm"/>ID3 (<span class="strong"><strong>Iterative Dichotomiser 3</strong></span>) algorithm is considered the <a id="id707" class="indexterm"/>simplest among the Decision tree algorithms. The information gain method is used as splitting criteria; the splitting is done until the best information gain is not greater than zero. There is no specific pruning done with ID3. It cannot handle numeric attributes and missing values.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>CHAID</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>CHAID</strong></span> (<span class="strong"><strong>Chi-squared Automatic Interaction Detection</strong></span>) was built to support<a id="id708" class="indexterm"/> only nominal attributes. For every attribute, a value<a id="id709" class="indexterm"/> is chosen in such a way that it is the closest to the target attribute. There is an additional statistical measure, depending on the type of the target attribute that differentiates this algorithm.</p>
<p>F test for a continuous target attribute, Pearson chi-squared test for nominal target attribute, and likelihood–ratio test for an ordinal target attribute is used. CHAID checks a condition to merge that can have a threshold and moves for a next check for merging. This process is repeated until no matching pairs are found.</p>
<p>CHAID addresses missing values in a simple way, and it operates on the assumption that all values belong to a single valid category. No pruning is done in this process.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>QUEST</p>
</td><td style="text-align: left" valign="top">
<p>The acronym <a id="id710" class="indexterm"/>QUEST stands for Quick, Unbiased, Efficient, and Statistical Tree.</p>
<p>This algorithm <a id="id711" class="indexterm"/>supports univariate and linear combination splits. ANOVA F-test or Pearson's chi-square or two-means clustering methods are used to compute the relationship between each input attribute and the target attribute, depending on the type of the attribute. Splitting is applied on attributes that have stronger association with the target attribute. To ensure that there is an optimal splitting point achieved, <span class="strong"><strong>Quadratic Discriminant Analysis</strong></span> (<span class="strong"><strong>QDA</strong></span>) is <a id="id712" class="indexterm"/>applied. Again, QUEST achieves binary trees and for pruning 10-fold cross-validation is used.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>CAL5</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id713" class="indexterm"/>works <a id="id714" class="indexterm"/>with numerical attributes.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>FACT</p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id715" class="indexterm"/>algorithm is an earlier version of QUEST that uses<a id="id716" class="indexterm"/> statistical methods followed by discriminant analysis for attribute selection.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LMDT</p>
</td><td style="text-align: left" valign="top">
<p>This uses a <a id="id717" class="indexterm"/>multivariate testing mechanism to build<a id="id718" class="indexterm"/> Decision trees.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>MARS</p>
</td><td style="text-align: left" valign="top">
<p>A multiple<a id="id719" class="indexterm"/> regression function is approximated using<a id="id720" class="indexterm"/> linear splines and their tensor products.</p>
</td></tr></tbody></table></div></div></div><div class="section" title="Greedy Decision trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec71"/>Greedy Decision trees</h3></div></div></div><p>A vital characteristic of <a id="id721" class="indexterm"/>Decision trees is that they are <span class="emphasis"><em>Greedy!</em></span> A greedy algorithm targets achieving optimal solutions globally by achieving local optimums at every stage. Though the global optimum is not always guaranteed, the local optimums help in achieving global optimum to a maximum extent.</p><p>Every node is greedily searched to reach the local optimum, and the possibility of getting stuck at achieving local optima is high. Most of the time, targeting local optima might help in providing a good enough solution.</p></div><div class="section" title="Benefits of Decision trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec72"/>Benefits of Decision trees</h3></div></div></div><p>Some of the advantages of using<a id="id722" class="indexterm"/> Decision trees are listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Decision trees are fast and easy to build and require little experimentation</li><li class="listitem" style="list-style-type: disc">They are robust</li><li class="listitem" style="list-style-type: disc">They are easy to understand and interpret</li><li class="listitem" style="list-style-type: disc">Decision trees do not require complex data preparation</li><li class="listitem" style="list-style-type: disc">They can handle both categorical and numerical data</li><li class="listitem" style="list-style-type: disc">They are supported using statistical models for validation</li><li class="listitem" style="list-style-type: disc">They can handle highly dimensional data and also operate large datasets</li></ul></div></div></div><div class="section" title="Specialized trees"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec73"/>Specialized trees</h2></div></div></div><p>In this section, we will explore some<a id="id723" class="indexterm"/> important special situations we face and special types of Decision trees. These become handy while solving special kinds of problems.</p><div class="section" title="Oblique trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec73"/>Oblique trees</h3></div></div></div><p>Oblique trees are used in<a id="id724" class="indexterm"/> cases where the data is extremely complex. If the attributes<a id="id725" class="indexterm"/> are <span class="emphasis"><em>x1, x2, AND x3…xn</em></span>, then the C4.5 and CART tests the criteria as <span class="emphasis"><em>x1&gt;some value</em></span> or <span class="emphasis"><em>x2&lt; some other value</em></span>, and so on. The goal in such cases is to find an attribute to test at each node. These are graphically parallel axis splits as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B03980_05_22.jpg" alt="Oblique trees"/></div><p>Clearly, we need to construct enormous trees. At this point, let's learn a data mining jargon called hyperplanes.</p><p>In a <span class="strong"><strong>1 D</strong></span> problem, a point classifies the space. In <span class="strong"><strong>2 D</strong></span>, a line (straight or curved) classifies the space. In a <span class="strong"><strong>3 D</strong></span> problem, a plane (linear or curved) classifies the space. In higher dimensional space, we imagine a plane like a thing splitting and classifying the space, calling it <span class="strong"><strong>hyperplane</strong></span>. This is <a id="id726" class="indexterm"/>shown in the following figure:</p><div class="mediaobject"><img src="graphics/B03980_05_23.jpg" alt="Oblique trees"/></div><p>So, the traditional Decision tree<a id="id727" class="indexterm"/> algorithms produce axis parallel hyperplanes that split the data. These can be cumbersome if the data is complex. If we can construct oblique planes, the explicability may come down, but we might reduce the tree size substantially. So, the idea is to change the testing conditions from the following:</p><p>xi &gt; K or &lt; K to a1x1+ a2x2+ … + c &gt; K or &lt; K</p><p>These oblique hyperplanes can at times drastically reduce the length of the tree. The same data shown in figure 2 is classified using oblique planes in the figure here:</p><div class="mediaobject"><img src="graphics/B03980_05_24.jpg" alt="Oblique trees"/></div></div><div class="section" title="Random forests"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec74"/>Random forests</h3></div></div></div><p>These specialized trees are<a id="id728" class="indexterm"/> used when there are too many dimensions. We have learned <a id="id729" class="indexterm"/>about curse of dimensionality in the Machine learning introduction chapter. The basic premise of the curse of dimensionality is that high dimensional data brings in complexity. With more dimensions and features, the possibility of errors is also high. Before we take a deep dive into Random forests, let's understand the concept of Boosting. More details on boosting methods are covered as a part of <a class="link" href="ch13.html" title="Chapter 13. Ensemble learning">Chapter 13</a>, <span class="emphasis"><em>Ensemble learning</em></span>. In the case of Random forests, the application of boosting is about how single tree methods are brought together to see a boost in the result regarding accuracy.</p><div class="mediaobject"><img src="graphics/B03980_05_25.jpg" alt="Random forests"/></div><p>A Random forest extends Decision trees by including more number of Decision trees. These Decision trees are built by a combination of random selection of data (samples) and a random selection of a subset of attributes. The following diagram depicts the random selection of datasets to build each of the Decision trees:</p><div class="mediaobject"><img src="graphics/B03980_05_26.jpg" alt="Random forests"/></div><p>Another variable input<a id="id730" class="indexterm"/> required for the making of multiple Decision trees are random<a id="id731" class="indexterm"/> subsets of the attributes, which is represented in the diagram here:</p><div class="mediaobject"><img src="graphics/B03980_05_27.jpg" alt="Random forests"/></div><p>Since each tree is built using random dataset and random variable set, these trees are called Random trees. Moreover, many such Random trees define a Random forest.</p><p>The result of a Random<a id="id732" class="indexterm"/> tree is based on two radical beliefs. One is that each of the trees <a id="id733" class="indexterm"/>make an accurate prediction for maximum part of the data. Second, mistakes are encountered at different places. So, on an average, a poll of results is taken across the Decision trees to conclude a result.</p><p>There are not enough observations to get good estimates, which leads to sparsity issues. There are two important causes for exponential increase on spatial density, one, is increase in dimensionality and the other is increase in the equidistant points in data. Most of the data is in the tails. </p><p>To estimate the density of a given accuracy, the following table shows how the sample size increases with dimensionality. The subsequent computations table shows how the mean square error of an estimate of multivariate normal distribution increases with an increase in dimensionality (as demonstrated by Silverman and computed by the formula given here):</p><div class="mediaobject"><img src="graphics/B03980_05_31.jpg" alt="Random forests"/></div><div class="mediaobject"><img src="graphics/B03980_05_28.jpg" alt="Random forests"/></div><p>Random forests are<a id="id734" class="indexterm"/> a vital extension of the Decision trees that are very simple to understand and are extremely efficient, particularly when one is dealing with high dimensional spaces. When the original data has many dimensions, we randomly pick a small subset<a id="id735" class="indexterm"/> of the dimensions (columns) and construct a tree. We let it grow all the way without pruning. Now, we iterate this process and construct hundreds of trees with a different set of attributes each time.</p><p>For prediction, a new sample is pushed down the tree. A new label of the training sample is assigned to the terminal node, where it ends up. This procedure is iterated over all the trees in the group, and the average vote of all trees is reported as the Random forest prediction.</p></div><div class="section" title="Evolutionary trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec75"/>Evolutionary trees</h3></div></div></div><p>When achieving the global <a id="id736" class="indexterm"/>optima seems almost impossible, Evolutionary trees <a id="id737" class="indexterm"/>are used. As you learned, Decision trees are greedy. So sometimes, we may be constructing much bigger trees just because we are stuck in local optima. So, if your tree length is just too much, try oblique trees or evolutionary trees.</p><p>The concept of evolutionary trees is originated from a very exciting concept called genetic algorithms. You will learn about it in detail in a different course. Let us only look at the essence.</p><p>Instead of mathematically computing the best attribute at every node, an Evolutionary tree randomly picks a node at each point and creates a tree. It then iterates and creates a collection of trees (forest). Now, it identifies the best trees in the forest for the data. It then creates the next generation of the forest by combining these trees randomly.</p><p>Evolutionary trees, on the other hand, choose a radically different top node and produce a much shorter tree, which has the same efficiency. Evolutionary algorithms take more time to compute.</p></div><div class="section" title="Hellinger trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec76"/>Hellinger trees</h3></div></div></div><p>There have been attempts to identify<a id="id738" class="indexterm"/> impurity measures that are less sensitive to the<a id="id739" class="indexterm"/> distribution of dependent variable values than Entropy or Gini index. A very recent paper suggested Hellinger distance as a measure of impurity that does not depend on the distribution of the target variable.</p><div class="mediaobject"><img src="graphics/B03980_05_29.jpg" alt="Hellinger trees"/></div><p>Essentially, <span class="emphasis"><em>P(Y+|X)</em></span> is the probability of finding <span class="emphasis"><em>Y+</em></span> for each attribute and similarly, <span class="emphasis"><em>P(Y-|X)</em></span> for each attribute is computed.</p><div class="mediaobject"><img src="graphics/B03980_05_30.jpg" alt="Hellinger trees"/></div><p>From the previous image, for a <span class="strong"><strong>High</strong></span> value of the first attribute, only a <span class="strong"><strong>High</strong></span> value of the second attribute results in a probability value of 1. This brings the total distance value to <span class="emphasis"><em>sqrt(2)</em></span>.</p></div></div></div></div>
<div class="section" title="Implementing Decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Implementing Decision trees</h1></div></div></div><p>Refer to the <a id="id740" class="indexterm"/>source code provided for this chapter for implementing Decision Trees and Random Forests (source code path <code class="literal">.../chapter5/...</code> under each of the folder for the technology).</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec74"/>Using Mahout</h2></div></div></div><p>Refer to the <a id="id741" class="indexterm"/>folder <code class="literal">.../mahout/chapter5/decisiontreeexample/</code>.</p><p>Refer to <a id="id742" class="indexterm"/>the folder<code class="literal">.../mahout/chapter5/randomforestexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec75"/>Using R</h2></div></div></div><p>Refer to the <a id="id743" class="indexterm"/> folder <code class="literal">.../r/chapter5/decisiontreeexample/</code>.</p><p>Refer to the <a id="id744" class="indexterm"/>folder <code class="literal">.../r/chapter5/randomforestexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec76"/>Using Spark</h2></div></div></div><p>Refer to the <a id="id745" class="indexterm"/>folder <code class="literal">.../spark/chapter5/decisiontreeexample/</code>.</p><p>Refer to the <a id="id746" class="indexterm"/>folder <code class="literal">.../spark/chapter5/randomforestexample/</code>.</p></div><div class="section" title="Using Python (scikit-learn)"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec77"/>Using Python (scikit-learn)</h2></div></div></div><p>Refer to the <a id="id747" class="indexterm"/>folder <code class="literal">.../python scikit-learn/chapter5/decisiontreeexample/</code>.</p><p>Refer to the <a id="id748" class="indexterm"/>folder <code class="literal">.../python scikit-learn/chapter5/randomforestexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec78"/>Using Julia</h2></div></div></div><p>Refer to the <a id="id749" class="indexterm"/>folder <code class="literal">.../julia/chapter5/decisiontreeexample/</code>.</p><p>Refer to the <a id="id750" class="indexterm"/>folder <code class="literal">.../julia/chapter5/randomforestexample/</code>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter, you learned a supervised learning technique with Decision trees to solve classification and regression problems. We also covered methods to select attributes, split the tree, and prune the tree. Among all other Decision tree algorithms, we have explored the CART and C4.5 algorithms. For a special requirement or a problem, you have also learned how to implement Decision tree-based models using MLib of Spark, R, and Julia. In the next chapter, we will cover <span class="strong"><strong>Nearest Neighbour</strong></span> and <span class="strong"><strong>SVM</strong></span> (<span class="strong"><strong>Support Vector Machines</strong></span>) to solve supervised and unsupervised learning problems.</p></div></body></html>