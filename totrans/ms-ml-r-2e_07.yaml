- en: Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Forget artificial intelligence - in the brave new world of big data, it''s
    artificial idiocy we should be looking out for."'
  prefs: []
  type: TYPE_NORMAL
- en: '- Tom Chatfield'
  prefs: []
  type: TYPE_NORMAL
- en: I recall that at some meeting circa mid-2012, I was part of a group discussing
    the results of some analysis or other, when one of the people around the table
    sounded off with a hint of exasperation mixed with a tinge of fright, *this* *isn't*
    *one* *of* *those* *neural* *networks,* *is* *it?* I knew of his past run-ins
    with and deep-seated anxiety about neural networks, so I assuaged his fears making
    some sarcastic comment that neural networks have basically gone the way of the
    dinosaur. No one disagreed! Several months later, I was gobsmacked when I attended
    a local meeting where the discussion focused on, of all things, neural networks
    and this mysterious deep learning. Machine learning pioneers such as Ng, Hinton,
    Salakhutdinov, and Bengio have revived neural networks and improved their performance.
  prefs: []
  type: TYPE_NORMAL
- en: Much media hype revolves around these methods with high-tech companies such
    as Facebook, Google, and Netflix investing tens, if not hundreds, of millions
    of dollars. The methods have yielded promising results in voice recognition, image
    recognition, and automation. If self-driving cars ever stop running off the road
    and into each other, it will certainly be from the methods discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how the methods work, their benefits, and inherent
    drawbacks so that you can become conversationally competent about them. We will
    work through a practical business application of a neural network. Finally, we
    will apply the deep learning methodology in a cloud-based application.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural network is a fairly broad term that covers a number of related methods,
    but in our case, we will focus on a **feed forward** network that trains with
    **backpropagation**. I'm not going to waste our time discussing how the machine
    learning methodology is similar or dissimilar to how a biological brain works.
    We only need to start with a working definition of what a neural network is. I
    think the Wikipedia entry is a good start.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning and cognitive science, **Artificial neural networks** (**ANNs**)
    are a family of statistical learning models inspired by biological neural networks
    (the central nervous systems of animals, in particular, the brain) and are used
    to estimate or approximate functions that can depend on a large number of inputs
    and are generally unknown. [https://en.wikipedia.org/wiki/Artificial_neural_network](https://en.wikipedia.org/wiki/Artificial_neural_network)
  prefs: []
  type: TYPE_NORMAL
- en: The motivation or benefit of ANNs is that they allow the modeling of highly
    complex relationships between inputs/features and response variable(s), especially
    if the relationships are highly nonlinear. No underlying assumptions are required
    to create and evaluate the model, and it can be used with qualitative and quantitative
    responses. If this is the yin, then the yang is the common criticism that the
    results are black box, which means that there is no equation with the coefficients
    to examine and share with the business partners. In fact, the results are almost
    not interpretable. The other criticisms revolve around how results can differ
    by just changing the initial random inputs and that training ANNs is computationally
    expensive and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematics behind ANNs is not trivial by any measure. However, it is crucial
    to at least get a working understanding of what is happening. A good way to intuitively
    develop this understanding is to start a diagram of a simplistic neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this simple network, the inputs or covariates consist of two nodes or neurons.
    The neuron labeled **1** represents a constant or more appropriately, the intercept.
    **X1** represents a quantitative variable. The **W**''s represent the weights
    that are multiplied by the input node values. These values become **Input Nodes**
    to **Hidden Node**. You can have multiple hidden nodes, but the principal of what
    happens in just this one is the same. In the hidden node, **H1**, the *weight
    * value* computations are summed. As the intercept is notated as **1**, then that
    input value is simply the weight, **W1**. Now the magic happens. The summed value
    is then transformed with the **Activation** function, turning the input signal
    to an output signal. In this example, as it is the only **Hidden Node**, it is
    multiplied by **W3** and becomes the estimate of **Y**, our response. This is
    the feed-forward portion of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But wait, there''s more! To complete the cycle or epoch, as it is known, backpropagation
    happens and trains the model based on what was learned. To initiate the backpropagation,
    an error is determined based on a loss function such as **Sum of Squared Error**
    or **Cross-Entropy**, among others. As the weights, **W1** and **W2**, were set
    to some initial random values between *[-1, 1]*, the initial error may be high.
    Working backward, the weights are changed to minimize the error in the loss function.
    The following diagram portrays the backpropagation portion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This completes one epoch. This process continues, using gradient descent (discussed
    in [Chapter 5](a7511867-5362-4215-a7dd-bbdc162740d1.xhtml), *More Classification
    Techniques - K-Nearest Neighbors and Support Vector Machines*) until the algorithm
    converges to the minimum error or prespecified number of epochs. If we assume
    that our activation function is simply linear, in this example, we would end up
    with *Y = W3(W1(1) + W2(X1))*.
  prefs: []
  type: TYPE_NORMAL
- en: The networks can get complicated if you add numerous input neurons, multiple
    neurons in a hidden node, and even multiple hidden nodes. It is important to note
    that the output from a neuron is connected to all the subsequent neurons and has
    weights assigned to all these connections. This greatly increases the model complexity.
    Adding hidden nodes and increasing the number of neurons in the hidden nodes has
    not improved the performance of ANNs as we had hoped. Thus, the development of
    deep learning occurs, which in part relaxes the requirement of all these neuron
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of activation functions that one can use/try, including a
    simple linear function, or for a classification problem, the `sigmoid` function,
    which is a special case of the logistic function ([Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml),
    *Logistic Regression and Discriminant Analysis*). Other common activation functions
    are `Rectifier`, `Maxout`, and **hyperbolic tangent** (**tanh**).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot a `sigmoid` function in R, first creating an `R` function in order
    to calculate the `sigmoid` function values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it is a simple matter to plot the function over a range of values, say
    `-5` to `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `tanh` function (hyperbolic tangent) is a rescaling of the logistic `sigmoid`
    with the output between **-1** and **1**. The `tanh` function relates to `sigmoid`
    as follows, where **x** is the `sigmoid` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tanh(x)* *=* *2* *** *sigmoid(2x)* *-* *1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the `tanh` and `sigmoid` functions for comparison purposes. Let''s
    also use `ggplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_02-1.png)'
  prefs: []
  type: TYPE_IMG
- en: So why use the `tanh` function versus `sigmoid`? It seems there are many opinions
    on the subject; is `tanh` popular in neural networks? In short, assuming you have
    scaled data with mean 0 and variance 1, the `tanh` function permits weights that
    are on average close to zero (zero-centered). This helps in avoiding bias and
    improves convergence. Think about the implications of always having positive weights
    from an output neuron to an input neuron as in a `sigmoid` function activation.
    During backpropagation, the weights will become either all positive or all negative
    between layers. This may cause performance issues. Also, since the gradient at
    the tails of a `sigmoid` (0 and 1) are almost zero, during backpropagation it
    can happen that almost no signal will flow between neurons of different layers.
     A full discussion of the issue is available, LeCun (1998). Keep in mind it is
    not a foregone conclusion that `tanh` is always better.
  prefs: []
  type: TYPE_NORMAL
- en: This all sounds fascinating, but the ANN almost went the way of disco as it
    just did not perform as well as advertised, especially when trying to use deep
    networks with many hidden layers and neurons. It seems that a slow yet gradual
    revival came about with the seminal paper by Hinton and Salakhutdinov (2006) in
    the reformulated and, dare I say, rebranded neural network, deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning, a not-so-deep overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, what is this deep learning that is grabbing our attention and headlines?
    Let''s turn to Wikipedia again for a working definition: *Deep learning is a branch
    of machine learning based on a set of algorithms that attempt to model high-level
    abstractions in data by using model architectures, with complex structures or
    otherwise, composed of multiple nonlinear transformations*. That sounds as if
    a lawyer wrote it. The characteristics of deep learning are that it is based on
    ANNs where the machine learning techniques, primarily unsupervised learning, are
    used to create new features from the input variables. We will dig into some unsupervised
    learning techniques in the next couple of chapters, but one can think of it as
    finding structure in data where no response variable is available. A simple way
    to think of it is the **Periodic Table of Elements**, which is a classic case
    of finding structure where no response is specified. Pull up this table online
    and you will see that it is organized based on atomic structure, with metals on
    one side and non-metals on the other. It was created based on latent classification/structure.
    This identification of latent structure/hierarchy is what separates deep learning
    from your run-of-the-mill ANN. Deep learning sort of addresses the question whether
    there is an algorithm that better represents the outcome than just the raw inputs.
    In other words, can our model learn to classify pictures other than with just
    the raw pixels as the only input? This can be of great help in a situation where
    you have a small set of labeled responses but a vast amount of unlabeled input
    data. You could train your deep learning model using unsupervised learning and
    then apply this in a supervised fashion to the labeled data, iterating back and
    forth.'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of these latent structures is not trivial mathematically, but
    one example is the concept of regularization that we looked at in [Chapter 4](04f803d3-791a-4505-80a7-905dfd6fdcc3.xhtml),
    *Advanced Feature Selection in Linear Models*. In deep learning, one can penalize
    weights with regularization methods such as *L1* (penalize non-zero weights),
    *L2* (penalize large weights), and dropout (randomly ignore certain inputs and
    zero their weight out). In standard ANNs, none of these regularization methods
    takes place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way is to reduce the dimensionality of the data. One such method is
    the `autoencoder`. This is a neural network where the inputs are transformed into
    a set of reduced dimension weights. In the following diagram, notice that **Feature
    A** is not connected to one of the hidden nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: This can be applied recursively and learning can take place over many hidden
    layers. What you have seen happening in this case is that the network is developing
    features of features as they are stacked on each other. Deep learning will learn
    the weights between two layers in sequence first and then only use backpropagation
    in order to fine-tune these weights. Other feature selection methods include **Restricted
    Boltzmann Machine** and **Sparse Coding Model**.
  prefs: []
  type: TYPE_NORMAL
- en: The details are beyond our scope, and many resources are available to learn
    about the specifics. Here are a couple of starting points:[ ](http://www.cs.toronto.edu/~hinton/)
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.cs.toronto.edu/~hinton/](http://www.cs.toronto.edu/~hinton/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://deeplearning.net/](http://deeplearning.net/)'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has performed well on many classification problems, including
    winning a Kaggle contest or two. It still suffers from the problems of ANNs, especially
    the black box problem. Try explaining to the uninformed what is happening inside
    a neural network. However, it is appropriate for problems where an explanation
    of How is not a problem and the important question is What. After all, do we really
    care why an autonomous car avoided running into a pedestrian, or do we care about
    the fact that it did not? Additionally, the Python community has a bit of a head
    start on the R community in deep learning usage and packages. As we will see in
    the practical exercise, the gap is closing.
  prefs: []
  type: TYPE_NORMAL
- en: 'While deep learning is an exciting undertaking, be aware that to achieve the
    full benefit of its capabilities, you will need a high degree of computational
    power along with taking the time to train the best model by fine-tuning the hyperparameters.
    Here is a list of some that you will need to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: An activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size and number of the hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction, that is, Restricted Boltzmann versus autoencoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient descent learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning resources and advanced methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the more interesting visual tools you can use for both learning and
    explaining is the interactive widget provided by TensorFlow^(TM): [http://playground.tensorflow.org/](http://playground.tensorflow.org/).
    This tool allows you to explore, or **tinker**, as the site calls it, the various
    parameters and how they impact on the response, be it a classification problem
    or a regression problem. I could spend, well I have spent hours tinkering with
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an interesting task: create your own experimental design and see how
    the various parameters affect your prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, it seems that the two fastest growing deep learning open-source
    tools are TensorFlow^(TM) and MXNet. I still prefer working with the package we
    will see, `h2o`, but it is important to understand and learn the latest techniques.
    You can access TensorFlow^(TM )with R, but it requires you to install python first.
    This series of tutorials will walk you through how to get it up and running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://rstudio.github.io/tensorflow/](https://rstudio.github.io/tensorflow/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'MXNet does not require the installation of Python and is relatively easy to
    install and make operational. It also offers a number of pretrained models that
    allow you to start making predictions quickly. Several R tutorials are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://mxnet.io/](http://mxnet.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: I now want to take the time to enumerate some of the variations of deep neural
    networks along with the learning tasks where they have performed well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNN**) make the assumption that the inputs
    are images and create features from slices or small portions of the data, which
    are combined to create a feature map. Think of these small slices as filters or
    probably more appropriately, kernels that the network learns during training.
    The activation function for CNN is a **Rectified** **Linear** **Unit** (**ReLU**).
    It is simply *f(x) = max(0, x)*, where *x* is the input to the neuron. CNNs perform
    well on image classification, object detection, and even sentence classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNN**) are created to make use of sequential
    information. In traditional neural networks, the inputs and outputs are independent
    of each other. With RNN, the output is dependent on the computations of previous
    layers, permitting information to persist across layers. So, take an output from
    a neuron (y); it is calculated not only on its input (t) but on all previous layers
    (t-1, t-n...). It is effective at handwriting and speech detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Long Short-Term Memory** (**LSTM**) is a special case of RNN. The problem
    with RNN is that it does not perform well on data with long signals. Thus LSTMs
    were created to capture complex patterns in data. RNNs combine information during
    training from previous steps in the same way, regardless of the fact that information
    in one step is more or less valuable than other steps. LSTMs seek to overcome
    this limitation by deciding what to remember at each step during training. This
    multiplication of a weight matrix by the data vector is referred to as a gate,
    which acts as an information filter. A neuron in LSTM will have two inputs and
    two outputs. The input from prior outputs and the memory vector passed from the
    previous gate. Then, it produces the output values and output memory as inputs
    to the next layer. LSTMs have the limitation of requiring a healthy dose of training
    data and are computationally intensive. LSTMs have performed well on speech recognition
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: I recommend you work with the tutorials on MXNet to help you understand how
    to develop these models for your own use.
  prefs: []
  type: TYPE_NORMAL
- en: With that, let's move on to some practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It was a calm, clear night on the 20th of April, 1998\. I was a student pilot
    in a Hughes 500D helicopter on a cross-country flight from the St. Paul, MN downtown
    airport back home to good old Grand Forks, ND. The flight was my final requirement
    prior to taking the test to achieve a helicopter instrument rating. My log book
    shows that we were 35 **Distance Measuring Equipment** (**DME**) or 35 nautical
    miles from the VOR on Airway Victor 2\. This put us somewhere south/southeast
    of St. Cloud, MN, cruising along at what I recall was 4,500 feet above sea level
    at approximately 120 knots. Then, it happened...BOOOOM! It is not hyperbole to
    say that it was a thunderous explosion, followed by a hurricane blast of wind
    to the face.
  prefs: []
  type: TYPE_NORMAL
- en: 'It all started when my flight instructor asked a mundane question about our
    planned instrument approach into Alexandria, MN. We swapped control of the aircraft
    and I bent over to consult the instrument approach plate on my kneeboard. As I
    snapped on the red lens flashlight, the explosion happened. Given my face-down
    orientation, the sound, and ensuing blast of wind, several thoughts crossed my
    mind: the helicopter is falling apart, I''m plunging to my death, and the Space
    Shuttle Challenger explosion as an HD quality movie going off in my head. In the
    1.359 seconds that it took us to stop screaming, we realized that the Plexiglas
    windscreen in front of me was essentially gone, but everything else was good to
    go. After slowing the craft, a cursory inspection revealed that the cockpit was
    covered in blood, guts, and feathers. We had done the improbable by hitting a
    Mallard duck over Central Minnesota and in the process, destroyed the windscreen.
    Had I not been looking at my kneeboard, I would have been covered in pate. We
    simply declared an emergency and canceled our flight plan with Minneapolis Center
    and, like the Memphis Belle, limped our way into Alexandria to await rescue from
    our compatriots at the University of North Dakota (home of the Fighting Sioux).'
  prefs: []
  type: TYPE_NORMAL
- en: So what? Well, I wanted to point out how much of a NASA fan and astronaut I
    am. In a terrifying moment, where for a split second I thought that I was checking
    out, my mind drifted to the Space Shuttle. Most males my age wanted to shake the
    hands of George Brett or Wayne Gretzky. I wanted to, and in fact did, shake the
    hands of Buzz Aldrin. (he was after all on the North Dakota faculty at the time.)
    Thus, when I found the `shuttle` dataset in the `MASS` package, I had to include
    it in this tome. By the way, if you ever get the chance to see the Space Shuttle
    Atlantis display at Kennedy Space Center, do not miss it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this problem, we will try and develop a neural network to answer the question
    of whether or not the shuttle should use the autolanding system. The default decision
    is to let the crew land the craft. However, the autoland capability may be required
    for situations of crew incapacitation or adverse effects of gravity upon re-entry
    after extended orbital operations. This data is based on computer simulations,
    not actual flights. In reality, the autoland system went through some trials and
    tribulations and, for the most part, the shuttle astronauts were in charge during
    the landing process. Here are a couple of links for further background information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.spaceref.com/news/viewsr.html?pid=10518](http://www.spaceref.com/news/viewsr.html?pid=10518)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://waynehale.wordpress.com/2011/03/11/breaking-through/](https://waynehale.wordpress.com/2011/03/11/breaking-through/)'
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start, we will load these four packages. The data is in the `MASS` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `neuralnet` package will be used for the building of the model and `caret`
    for the data preparation. The `vcd` package will assist us in data visualization.
    Let''s load the data and examine its structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The data consists of `256` observations and `7 variables`. Notice that all
    of the variables are categorical and the response is `use` with two levels, `auto`
    and `noauto`. The covariates are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stability`: This is stable positioning or not (`stab`/`xstab`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`error`: This is the size of the error (`MM` / `SS` / `LX`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sign`: This is the sign of the error, positive or negative (`pp`/`nn`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wind`: This is the `wind` sign (`head` / `tail`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`magn`: This is the `wind` strength (`Light` / `Medium` / `Strong` / `Out of
    Range`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vis`: This is the visibility (`yes` / `no`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will build a number of tables to explore the data, starting with the response/outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Almost 57 per cent of the time, the decision is to use the autolander. There
    are a number of possibilities to build tables for categorical data. The `table()`
    function is perfectly adequate to compare one with another, but if you add a third,
    it can turn into a mess to look at. The `vcd` package offers a number of table
    and plotting functions. One is `structable()`. This function will take a formula
    (*column1 + column2 ~ column3*), where *column3* becomes the rows in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see that in the cases of a headwind that was `Light` in magnitude,
    `auto` occurred `19` times and `noauto`, `13` times. The `vcd` package offers
    the `mosaic()` function to plot the table created by `structable()` and provide
    the **p-value** for a chi-squared test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot tiles correspond to the proportional size of their respective cells
    in the table, created by recursive splits. You can also see that the **p-value**
    is not significant, so the variables are independent, which means that knowing
    the levels of wind and/or **magn** does not help us predict the use of the autolander.
    You do not need to include a `structable()` object in order to create the plot
    as it will accept a formula just as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the shading of the table has changed, reflecting the rejection of
    the null hypothesis and dependence in the variables. The plot first takes and
    splits the visibility. The result is that if the visibility is **no**, then the
    autolander is used. The next split is horizontal by **error**. If **error** is
    **SS** or **MM** when **vis** is **no**, then the autolander might be recommended,
    otherwise it is not. A p-value is not necessary as the gray shading indicates
    significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'One can also examine proportional tables with the `prop.table()` function as
    a wrapper around `table()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In case we forget, the chi-squared tests are quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Preparing the data for a neural network is very important as all the covariates
    and responses need to be numeric. In our case, all of the input features are categorical.
    However, the `caret` package allows us to quickly create dummy variables as our
    input features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To put this into a data frame, we need to predict the `dummies` object to an
    existing data, either the same or different, in `as.data.frame()`. Of course,
    the same data is needed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We now have an input feature space of ten variables. Stability is now either
    `0` for `stab` or `1` for `xstab`. The base error is `LX`, and three variables
    represent the other categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response can be created using the `ifelse()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `caret` package also provides us with the functionality to create the `train`
    and `test` sets. The idea is to index each observation as `train` or `test` and
    then split the data accordingly. Let''s do this with a 70/30 `train` to `test`
    split, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The values in `trainIndex` provide us with the row number; in our case, 70
    per cent of the total row numbers in `shuttle.2`. It is now a simple case of creating
    the `train`/`test` datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Nicely done! We are now ready to begin building the neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, the package that we will use is `neuralnet`. The function in
    `neuralnet` will call for the use of a formula as we used elsewhere, such as *y~x1+x2+x3+x4*,
    *data = df*. In the past, we used *y~,* to specify all the other variables in
    the data as inputs. However, `neuralnet` does not accommodate this at the time
    of writing. The way around this limitation is to use the `as.formula()` function.
    After first creating an object of the variable names, we will use this as an input
    in order to paste the variables properly on the right side of the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Keep this function in mind for your own use as it may come in quite handy.
    In the `neuralnet` package, the function that we will use is appropriately named
    `neuralnet()`. Other than the formula, there are four other critical arguments
    that we will need to examine:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hidden`: This is the number of hidden neurons in each layer, which can be
    up to three layers; the default is 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`act.fct`: This is the activation function with the default logistic and `tanh`
    available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`err.fct`: This is the function used to calculate the error with the default
    `sse`; as we are dealing with binary outcomes, we will use `ce` for cross-entropy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linear.output`: This is a logical argument on whether or not to ignore `act.fct`
    with the default TRUE, so for our data, this will need to be `FALSE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also specify the algorithm. The default is resilient with backpropagation
    and we will use it along with the default of one hidden neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the overall results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the error is extremely low at `0.0099`. The number of steps
    required for the algorithm to reach the threshold, which is when the absolute
    partial derivatives of the error function become smaller than this error (default
    = 0.1). The highest weight of the first neuron is `vis.yes.to.1layhid1` at 6.21.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also look at what are known as generalized weights. According to the
    authors of the `neuralnet` package, the generalized weight is defined as the contribution
    of the *i*th covariate to the log-odds:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The generalized weight expresses the effect of each covariate x[i] and thus
    has an analogous interpretation as the ith regression parameter in regression
    models. However, the generalized weight depends on all other covariates* (Gunther
    and Fritsch, 2010).'
  prefs: []
  type: TYPE_NORMAL
- en: The weights can be called and examined. I've abbreviated the output to the first
    four variables and six observations only. Note that if you sum each row, you will
    get the same number, which means that the weights are equal for each covariate
    combination. Please note that your results might be slightly different because
    of random weight initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To visualize the neural network, simply use the `plot()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_06-1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This plot shows the weights of the variables and intercepts. You can also examine
    the generalized weights in a plot. Let''s look at `vis.yes` versus `wind.tail`,
    which has a low overall synaptic weight. Notice how `vis.yes` is skewed and `wind.tail`
    has an even distribution of weights, implying little predictive power:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now want to see how well the model performs. This is done with the `compute()`
    function and specifying the fit model and covariates. This syntax will be the
    same for the predictions on the `test` and `train` sets. Once computed, a list
    of the predictions is created with `$net.result`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'These results are in probabilities, so let''s turn them into `0` or `1` and
    follow this up with a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Lo and behold, the neural network model has achieved 100 per cent accuracy.
    We will now hold our breath and see how it does on the `test` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Only one false positive in the `test` set. If you wanted to identify which
    one this was, use the `which()` function to single it out, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: It is row `62` in the `test` set and observation `203` in the full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: I'll leave it to you to see if you can build a neural network that achieves
    100% accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: An example of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shifting gears away from the Space Shuttle, let's work through a practical example
    of deep learning, using the `h2o` package. We will do this on data I've modified
    from the UCI Machine Learning Repository. The original data and its description
    is available at [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing/).
     What I've done is, take the smaller dataset `bank.csv`, scale the numeric variables
    to mean 0 and variance of 1, create dummies for the character variables/sparse
    numerics, and eliminate near zero variance varaibles.  The data is available on
    github [https://github.com/datameister66/data/](https://github.com/datameister66/data/)
    named also `bank_DL.csv`. In this section, we will focus on how to load the data
    in the H20 platform and run the deep learning code to build a classifier to predict
    whether a customer will respond to a marketing campaign.
  prefs: []
  type: TYPE_NORMAL
- en: H2O background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O is an open source predictive analytics platform with prebuilt algorithms,
    such as k-nearest neighbor, gradient boosted machines, and deep learning. You
    can upload data to the platform via Hadoop, AWS, Spark, SQL, noSQL, or your hard
    drive. The great thing about it is that you can utilize the machine learning algorithms
    in R and, at a much greater scale, on your local machine. If you are interested
    in learning more, you can visit the site: [http://h2o.ai/product/](http://h2o.ai/product/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of installing H2O on R is a little different. I put the code here
    that gave me the latest update (as of February 25, 2017). You can use it to reinstall
    the latest version or pull it off of the website: [http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/](http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html/).
    The following is the code to install the latest version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Data upload to H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume you have the  `bank_DL.csv` file saved in your working directory.
    Remember, `getwd()` will provide you with the path to it. So, let''s load the
    library and create an object with the file path to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now connect to H2O and start an instance on the cluster. Specifying
    `nthreads = -1` requests our instance use all CPUs on the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The H2O function, `h2o.uploadFile()`, allows you to upload/import your file
    to the H2O cloud. The following functions are also available for uploads:'
  prefs: []
  type: TYPE_NORMAL
- en: '`h2o.importFolder`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h2o.importURL`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h2o.importHDFS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is quite simple to upload the file and a per cent indicator tracks the status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is now in `H2OFrame`, which you can verify with `class()`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Many of the R commands in H2O may produce a different output than what you
    are used to seeing. For instance, look at the structure of our data (abbreviated
    output):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that it consists of 4,521 observations (nrow) and 64 columns (ncol).
    By the way, the `head()` and `summary()` functions work exactly the same as in
    regular R. Before splitting the datasets, let''s have a look at our response distribution.
    It is the column named **y**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We see that 521 of the bank's customers responded yes to the offer and 4,000
    did not. This response is a bit unbalanced. Techniques that can be used to handle
    unbalanced response labels are discussed in the chapter on multi-class learning.
    In this exercise, let's see how deep learning will perform with this lack of label
    balance.
  prefs: []
  type: TYPE_NORMAL
- en: Create train and test datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can use H2O''s functionality to partition the data into train and test
    sets. The first thing to do is create a vector of random and uniform numbers for
    the full data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then build your partitioned data and assign it with a desired `key`
    name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With these created, it is probably a good idea that we have a balanced response
    variable between the `train` and `test` sets. To do this, you can use the `h2o.table()`
    function and, in our case, it would be column 64:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This appears all well and good, so let''s begin the modeling process:'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we will see, the deep learning function has quite a few arguments and parameters
    that you can tune. The thing that I like about the package is the ability to keep
    it as simple as possible and let the defaults do their thing. If you want to see
    all the possibilities along with the defaults, see help or run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Documentation on all the arguments and tuning parameters is available online
    at [http://h2o.ai/docs/master/model/deep-learning/](http://h2o.ai/docs/master/model/deep-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: As on a side note, you can run a demo for the various machine learning methods
    by just running `demo("method")`. For instance, you can go through the deep learning
    demo with `demo(h2o.deeplearning)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next goal is to tune the hyper-parameters using a random search. It takes
    less time than a full grid search. We will look at `tanh`, with and without dropout,
    three different hidden layer/neuron combinations, two different dropout ratios,
    and two different learning rates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You now help specify the random search criteria in a list. Since we want a
    random search we will specify `RandomDiscrete`. A full grid search would require
    `Cartesian`. It is recommended to specify one or more early stopping criterion
    for a random search such as `max_runtime_secs`, `max_models`. We also specify
    here that it will stop when the top five models are withing 1% error of each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this is where the magic should happen using the `h2o.grid()` function.
    We tell it, we want to use the deep learning algorithm, our test data, any validation
    data (we will use the test set), our input features, and response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: An indicator bar tracks the progress, and with this dataset, it should take
    less than a few seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now examine the results of the top five models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: So the winning model is `#57` with activation of `TanhWithDropout`, three hidden
    layers with 30 neurons each, dropout ratio of 0.05, and learning rate of 0.25,
    which had an AUC of almost 0.864\.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have a look at our error rates in the validation/test data with a confusion
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Even though we only have 11% error, we had high errors for the `yes` label with
    high rates of false positives and false negatives. It possibly indicates that
    class imbalance may be an issue. We also have just started the hyper-parameter
    tuning process, so much work could be done to improve the outcome. I'll leave
    that task to you!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s examine how to build a model using cross-validation. Notice how
    the hyper-parameters are included in the function `h2o.deeplearning()` with the
    exception of learning rate, which is specified as adaptive. I also included the
    functionality to up-sample the minority class to achieve balanced labels during
    training. On another note, the folds are a stratified sample based on the response
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you call the object `dlmodel`, you will receive rather lengthy output. In
    this instance, let''s examine the performance on the holdout folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Given these results, I think more tuning is in order for the hyper-parameters,
    particularly with the hidden layers/neurons. Examining out of sample performance
    is a little different, but is quite comprehensive, utilizing the `h2o.performance()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The overall error increased, but we have lower false positive and false negative
    rates. As before, additional tuning is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the variable importance can be produced. This is calculated based
    on the so-called `Gedeon` Method. Keep in mind that these results can be misleading.
    In the table, we can see the order of the variable importance, but this importance
    is subject to the sampling variation, and if you change the seed value, the order
    of the variable importance could change quite a bit. These are the top five variables
    by importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have completed the introduction to deep learning in R using the
    capabilities of the `H2O` package. It is simple to use while offering plenty of
    flexibility to tune the hyperparameters and create deep neural networks. Enjoy!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the goal was to get you up and running in the exciting world
    of neural networks and deep learning. We examined how the methods work, their
    benefits, and their inherent drawbacks with applications to two different datasets.
    These techniques work well where complex, nonlinear relationships exist in the
    data. However, they are highly complex, potentially require a ton of hyper-parameter
    tuning, are the quintessential black boxes, and are difficult to interpret. We
    don't know why the self-driving car made a right on red, we just know that it
    did so properly. I hope you will apply these methods by themselves or supplement
    other methods in an ensemble modeling fashion. Good luck and good hunting! We
    will now shift gears to unsupervised learning, starting with clustering.
  prefs: []
  type: TYPE_NORMAL
