<html><head></head><body>
		<div id="_idContainer269">
			<h1 class="chapter-number" id="_idParaDest-134"><a id="_idTextAnchor325"/>7</h1>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor326"/>Productionizing ML on Databricks<a id="_idTextAnchor327"/></h1>
			<p class="author-quote">“Production is 80% of the work.”</p>
			<p class="author-quote">— Matei Zaharia</p>
			<p>Once you’ve refined your model and have satisfactory results, you are ready to put it into production. We’ve now entered the field of <strong class="bold">machine learning operations</strong> (<strong class="bold">MLOps</strong>)! Unfortunately, this is<a id="_idIndexMarker450"/> where many data scientists and ML engineers get stuck, and it’s common for companies to struggle here. Implementing models in production is much more complex than running models ad hoc because MLOps requires distinct tools and skill sets and sometimes, entirely new teams. MLOps is an essential part of the data science process because the actual value of a model is often only <span class="No-Break">realized post-deployment.</span></p>
			<p>You can think of MLOps as combining <strong class="bold">DevOps</strong>, <strong class="bold">DataOps</strong>, and <strong class="bold">ModelOps</strong>. MLOps is often divided into two parts: inner and outer loops. The inner loop covers the data science work and includes tracking various stages of the model development and experimentation process. The outer loop encompasses methods to orchestrate your data science project throughout its life cycle, from testing to staging and ultimately <span class="No-Break">into production.</span></p>
			<p>Fortunately, the path from model development to production doesn’t have to depend entirely on another team and tool<a id="_idIndexMarker451"/> stack when using the Databricks <strong class="bold">Data Intelligence</strong> (<strong class="bold">DI</strong>) platform. Productionizing an ML model using Databricks products makes the journey more straightforward and cohesive by incorporating functionality such <a id="_idIndexMarker452"/>as the <strong class="bold">Unity Catalog Registry</strong> (<strong class="bold">UC Registry</strong>), Databricks workflows, <strong class="bold">Databricks Asset Bundles</strong> (<strong class="bold">DABs</strong>), and model serving<a id="_idIndexMarker453"/> capabilities. This chapter will cover the tools and practices for taking your models from development <span class="No-Break">to production.</span></p>
			<p>Here is what you will learn as part of <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Deploying the MLOps <span class="No-Break">inner loop</span></li>
				<li>Deploying the MLOps <span class="No-Break">outer loop</span></li>
				<li>Deploying <span class="No-Break">your model</span></li>
				<li>Applying <span class="No-Break">our learn<a id="_idTextAnchor328"/>ing</span></li>
			</ul>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor329"/>Deploying the MLOps inner loop</h1>
			<p>In Databricks, the MLOps inner loop uses <a id="_idIndexMarker454"/>a variety of tools within the DI platform that we’ve already touched upon throughout this book, such as MLflow, Feature Engineering with Unity Catalog, and Delta. This chapter will highlight how you can leverage them together to facilitate MLOps from one place. MLOps is covered in even more depth by Databricks’ ebook, <em class="italic">The Big Book of MLOps</em>, which we highly recommend if you wish to learn more about the guiding principles and design decisions when architecting your own MLOps<a id="_idIndexMarker455"/> solution. We use <strong class="bold">GitHub</strong> to help facilitate DevOps and code reproducibility. For the DataOps portion, we use <strong class="bold">Unity Catalog</strong> and <strong class="bold">Delta</strong>. These<a id="_idIndexMarker456"/> tools help us track the versions of data and the code <a id="_idIndexMarker457"/>associated with the features created. This is the data reproducibility piece of DataOps. We use Delta time travel to query data from previous versions of the same table in the short term. For long-term reproducibility, we recommend saving the training dataset as a Delta table and logging the table path with the <strong class="source-inline">mlflow.log_input()</strong> method. The upstream sources of the feature tables we create are automatically tracked by <span class="No-Break">UC li<a id="_idTextAnchor330"/>neage.</span></p>
			<h2 id="_idParaDest-137">Registering <a id="_idTextAnchor331"/>a model</h2>
			<p>Let’s jump into model registries and their role in productionalization. A model registry is a centralized model store that <a id="_idIndexMarker458"/>helps manage the entire model life cycle, including versioning or aliasing, CI/CD integration, webhooks, and notifications. In Databricks, the UC Registry extends the governance features of Unity Catalog to the model registries, including centralized access control, auditing, lineage, and model discovery across workspaces. The UC Registry is a centralized repository for your models and their chronological lineage. This means that the experiment and experiment run that created each respective model are linked to the respective code and data source. Once you are satisfied with your ML model, the first thing to do is register it in Databricks under your central model registry. A registered model is a logical grouping of a model’s version history. The UC Registry tracks different model versions, including each version’s training data, hyperparameters, and evaluation metrics. A model rarely, if ever, has only one version. In addition to experimentation of model types and hyperparameter values, there are also cases where we want to experiment with different versions of a model. We can refer to these different model versions using model aliases. For example, deploying different versions simultaneously can be helpful for A/B testing; we’ll cover this in more detail in the <em class="italic">Model inference</em> section. With the UC Registry, you can easily create and manage multiple model aliases, making <a id="_idIndexMarker459"/>it easier to track changes, compare performance, and revert to earlier versions <span class="No-Break">i<a id="_idTextAnchor332"/>f needed.</span></p>
			<h2 id="_idParaDest-138">Collaborative de<a id="_idTextAnchor333"/>velopment</h2>
			<p>The UC Registry provides a<a id="_idIndexMarker460"/> collaborative model development environment, enabling teams to share and review models. This collaboration is also tracked, allowing multiple teams or leads to stay abreast of the model development process. Team or project leads can also require documentation before allowing a model to continue through the life cycle. We find it easier to add snippets of documentation throughout the project rather than trying to remember everything and take the time to write it all <span class="No-Break">down afterward.</span></p>
			<p>The UC Registry allows you to tag models and model versions. In the Streaming Transactions project, we use tags to track the validation status of a model. The approval process can be automated or may require human interaction. An approval process can ensure that models are high quality and meet <span class="No-Break">business requirements.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> shows a screenshot of the UC Model Registry. Note that there are built-in locations for tags, aliases, <span class="No-Break">and comments:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer209">
					<img alt="Figure 7.1 – The UC Model Registry UI shows each model version" src="image/B16865_07_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The UC Model Registry UI shows each model version</p>
			<p>The UC Registry is tightly integrated with the DI platform. This provides a single technical stack – a unified environment for moving a model through experimentation to deployment. The seamless integration lets you leverage other Databricks components such as Databricks Notebooks, Databricks<a id="_idIndexMarker461"/> Jobs, Databricks Lakehouse Monitoring, and Databricks <span class="No-Break">Model Serving.</span></p>
			<p>Next, let’s move on to product features that support the outer <span class="No-Break">loop<a id="_idTextAnchor334"/> process.</span></p>
			<h1 id="_idParaDest-139">Deploying the MLOps <a id="_idTextAnchor335"/>outer loop</h1>
			<p>The ML life cycle looks<a id="_idIndexMarker462"/> different for different use cases. However, the set of tools available in the Databricks platform makes it possible to automate as you like and supports your MLOps. The outer loop connects the inner loop products with the help of Workflows, Databricks Terraform Provider, REST API, DABs, and more. We covered automating the tracking process through MLflow Tracking and the UC Registry. The UC Registry is tightly integrated with the Model Serving feature and has a robust API that can easily be integrated into the automation process using webhooks. Each of these features can play a role in automating the ML <span class="No-Break">l<a id="_idTextAnchor336"/>ife cycle.</span><a id="_idTextAnchor337"/></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor338"/>Workflows</h2>
			<p>Databricks Workflows is a<a id="_idIndexMarker463"/> flexible orchestration tool for productionizing and <a id="_idIndexMarker464"/>automating ML projects. Workflows help the ML life cycle by providing a unified way to chain together all aspects of ML, from data preparation to model deployment. With Databricks Workflows, you can designate dependencies between tasks to ensure tasks are completed in the required order. These dependencies are visualized with arrows connecting the tasks, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer210">
					<img alt="Figure 7.2 – A Databricks workflow with five tasks, with the feature engineering task having two﻿ dependencies" src="image/B16865_07_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – A Databricks workflow with five tasks, with the feature engineering task having two<a id="_idTextAnchor339"/> dependencies</p>
			<p>Tasks in a workflow are not <a id="_idIndexMarker465"/>limited to notebooks. In <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, we prepared a DLT <a id="_idIndexMarker466"/>pipeline to prepare data in the Bronze layer, and DLT pipelines can be a workflow component. Additional objects, such as JAR files, Python scripts, and SQL, can be tasks within a workflow, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer211">
					<img alt="Figure 7.3 – Examples of objects that can be used as tasks within a workflow" src="image/B16865_07_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Examples of objects that can be used as tasks within a workflow</p>
			<p>Workflows are robust and play a<a id="_idIndexMarker467"/> key role in automating work within<a id="_idIndexMarker468"/> Databricks. DABs are another tool for productionization <a id="_idTextAnchor340"/><span class="No-Break">in Databricks.</span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor341"/>DABs</h2>
			<p>DABs are a way to bring uniformity and standardization to the deployment approach for all data products built on <a id="_idIndexMarker469"/>the Databricks platform. They are an <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>) approach to managing your projects, allowing developers to outline a project’s infrastructure and <a id="_idIndexMarker470"/>resources using a YAML <a id="_idIndexMarker471"/>configuration file. DABs are especially useful for managing complex projects that involve a lot of <a id="_idIndexMarker472"/>collaborators and require automation. Where <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) is necessary, DABs are a wonderful way to manage ML pipeline resources across environments and to help your team follow best practices throughout the development and <span class="No-Break">productionalization processes.</span></p>
			<p>Under the hood, DABs are collections of Databricks artifacts (including jobs, DLT pipelines, and ML models) and assets (for example, notebooks, SQL queries, and dashboards). These bundles <a id="_idIndexMarker473"/>are managed through YAML templates that are created and maintained alongside source code. You can build DAB YAML files manually or use templates to automate. You can also build custom templates for more complex <span class="No-Break">processing tasks.</span></p>
			<p>Using DABs requires<a id="_idIndexMarker474"/> the Databricks CLI. We discussed installing the CLI in <a href="B16865_02.xhtml#_idTextAnchor073"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> if you want to review it again. Also, DABs are fairly new and not incorporated into the projects. However, great resources are listed in <em class="italic">Further reading</em> that cover this new prod<a id="_idTextAnchor342"/>uct feature <span class="No-Break">in depth.</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor343"/>REST API</h2>
			<p>Everything you can do in the UI can <a id="_idIndexMarker475"/>be accomplished via the API as well. The UI is great for exploring product features <a id="_idIndexMarker476"/>and building workflows for the first time. For example, we automated the process after building out our AutoML experiment in the UI. Additionally, we saw how secrets are completely contained in the API and not available via the UI. As we’ll see in the next section, it is possible to deploy yo<a id="_idTextAnchor344"/>ur models via <span class="No-Break">the API.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor345"/>Deploying your model</h1>
			<p>Deploying a model can be done in many ways, depending on the use case and data availability. For example, deployment <a id="_idIndexMarker477"/>may look like packaging a model in a container and deploying it on an endpoint or model that runs daily in a production workflow to provide predictions in tables that can be consumed by applications. Databricks has product features to pave the way to production <a id="_idTextAnchor346"/>for all <span class="No-Break">inference types.</span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor347"/>Model Inference</h2>
			<p>We’ve walked through the<a id="_idIndexMarker478"/> methods and tools that help you set up your model in production, and finally, you have a model ready for inference! But one key question you should consider as part of this process is how your model should be used. Do you need the results once a day? Is the model powering an application that requires real-time results? Your model’s purpose will help you decide the type of deployment you need. You’ve seen the words “batch” and “streaming” a few times in this chapter already, so let’s quickly <a id="_idIndexMarker479"/>define what those mean in the co<a id="_idTextAnchor348"/>ntext of <span class="No-Break">model inference:</span></p>
			<ul>
				<li><strong class="bold">Batch inference</strong>: Batch inference (also known as offline<a id="_idIndexMarker480"/> inference) refers to a job that generates predictions on a group (or “batch”) of data all at once. Batch jobs are scheduled to run on a specified cadence, such as once a day. This type of inference is best when there are no/low-latency requirements and allows you to take advantage of scalable <span class="No-Break">compute resources.</span></li>
				<li><strong class="bold">Streaming inference</strong>: Streaming inference (also<a id="_idIndexMarker481"/> known as online inference) refers to a job that generates predictions on data as it is streamed. This is possible in the Databricks pla<a id="_idTextAnchor349"/>tform via Delta <span class="No-Break">Live Tables.</span></li>
				<li><strong class="bold">Real-time inference</strong>: Real-time<a id="_idIndexMarker482"/> inference (also known as model serving) refers to the process of exposing your models as REST API endpoints. This enables universally available, low-latency predictions and is especially useful when deploying models that generate predictions requir<a id="_idTextAnchor350"/>ed by <span class="No-Break">real-time applications.</span></li>
			</ul>
			<p>All these options are available via the Databricks UI. Let’s use the Favorita Store example again. Perhaps we’ve met with our beverages business team, and they would like to see weekly forecasts to help decide how much of each product they should purchase. We will opt for batch inference since we only need to produce an updated forecast once a week. It just takes a few clicks to set up a model for batch processing. Follow the <em class="italic">Applying our learning</em> section of the Favorita Sales dataset for detailed instructions on deploying yo<a id="_idTextAnchor351"/>ur model for <span class="No-Break">b<a id="_idTextAnchor352"/>atch inference.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor353"/>Model serving</h2>
			<p>Let’s dive deeper into real-time inference or <strong class="bold">model serving</strong>. The process of model serving can be complex and <a id="_idIndexMarker483"/>costly, involving additional tools and systems to achieve real-time needs. Fortunately, deploying a registered model as an endpoint only takes a single click from the Databricks platform! Because model serving is tightly integrated with MLflow, the path from development to production is much faster. Using a model registered in the MLflow Model Registry, Databricks automatically prepares a production-ready container and deploys the container to <span class="No-Break">serverless compute.</span></p>
			<p>It’s also easy to deploy models via API, as shown here. A forecasting problem doesn’t make much sense as a model serving use case, so let’s consider instead the MIC problem we’ve been <a id="_idIndexMarker484"/>working on throughout this book. We will use model serving to serve our classification model in real time. The following example code snippet creates an endpoint that serves a version of a model <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">asl_model</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer212">
					<img alt="Figure 7.4 – Deploying a mo﻿del through a serving endpoint" src="image/B16865_07_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Deploying a mo<a id="_idTextAnchor354"/>del through a serving endpoint</p>
			<p>Model serving lets you build workflows around model endpoints, which provides plenty of flexibility in terms of model deployment. For example, an organization may want to A/B test two or more models. Model serving makes it easy to deploy multiple models behind the same endpoint and distribute traffic among them. In another pattern, you can deploy the same model behind multiple endpoints. The following is an example of the UI and the analysis you can perform when A/B <span class="No-Break">testing models:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer213">
					<img alt="Figure 7.5 – Model serving gives us the flexibility to A/B test models deployed behind the same endpoint" src="image/B16865_07_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Model serving gives us the flexibility to A/B test models deployed behind the same endpoint</p>
			<p>Get ready to follow along in<a id="_idIndexMarker485"/> your own Databricks workspace as we review the <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> code project <span class="No-Break">by project.</span></p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor355"/>Applying our learning</h1>
			<p>Let’s use what we have learned <a id="_idTextAnchor356"/>to productionalize <span class="No-Break">our models.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor357"/>Technical requirements</h2>
			<p>Here are the technical requirements needed to complete the hands-on examples in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>On-demand features require the use of DBR ML 13.1 <span class="No-Break">or higher.</span></li>
				<li>RAG and CV parts require DBR ML 14.2 <span class="No-Break">and higher.</span></li>
				<li>Python UDFs are created and governed in UC; hence, Unity Catalog must be enabled for the workspace – no <span class="No-Break">shared clusters.</span></li>
				<li>The Streaming Transactions project uses <strong class="source-inline">scikit-learn==1.4.0rc1</strong>. The notebooks that need it <span class="No-Break">install it.</span></li>
				<li>The Streaming Transactions project, again, performs better with parallel compute. We’ll use the multi-node cluster from <a href="B16865_05.xhtml#_idTextAnchor244"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. See <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> for the multi-node <span class="No-Break">CPU configuration:</span></li>
			</ul>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer214">
					<img alt="Figure 7.6 – Multi-node CPU c﻿luster configuration (on AWS)" src="image/B16865_07_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Multi-node CPU c<a id="_idTextAnchor358"/>luster configuration (on AWS)</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor359"/>Project – Favorita Sales forecasting</h2>
			<p>In this chapter, we discussed using managed MLflow and the UC Model Registry to register and prepare models for<a id="_idIndexMarker486"/> deployment. We’ll start by walking through the UI, so please open the following notebooks and a tab within the Experiments <span class="No-Break">UI page:</span></p>
			<ul>
				<li><strong class="source-inline">CH7-01-Registering </strong><span class="No-Break"><strong class="source-inline">the Model</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH7-02-Batch Inference</strong></span></li>
			</ul>
			<p>As a reminder, in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, we ran experiments to find a baseline model, and you can review those experiments from the Experiments UI page in Databricks. To follow along in your workspace, please open the Experiments UI page from <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer215">
					<img alt="Figure 7.7 – The Experiments UI page for ﻿exploring the experiment runs" src="image/B16865_07_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – The Experiments UI page for <a id="_idTextAnchor360"/>exploring the experiment runs</p>
			<p>For the sake of the project, we<a id="_idIndexMarker487"/> will move forward as though the best baseline model is the model we want in production. To productionalize the model, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>On the AutoML experiment page, click on the best run (the run at the top when sorting by the evaluation metric in descending order). This will open the run details page. As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.8</em>, there are four tabs – <strong class="bold">Overview</strong>, <strong class="bold">Model metrics</strong>, <strong class="bold">System metrics</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Artifacts</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer216">
					<img alt="Figure 7.8 – Exploring the model details page" src="image/B16865_07_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Exploring the model details page</p>
			<ol>
				<li value="2">Click <strong class="bold">Register model</strong> in the top corner. This opens a dialogue box where you can register the model in<a id="_idIndexMarker488"/> the workspace or UC Model Registry. Of course, we want to use Unity Catalog. Selecting Unity Catalog provides the code to register the model via API. This code is already included in the <strong class="source-inline">CH7-01-Registering the Model</strong> notebook. When running the notebook in your workspace, you must update the <strong class="source-inline">runs:/</strong> URL in the notebook before <span class="No-Break">executing it:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer217">
					<img alt="Figure ﻿7.9 – Registering a new model" src="image/B16865_07_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure <a id="_idTextAnchor361"/>7.9 – Registering a new model</p>
			<ol>
				<li value="3">While still in your notebook, execute the final cell a second time. This creates a second version of the <span class="No-Break">same model.</span></li>
				<li>Navigate to the <strong class="source-inline">favorita_forecasting</strong> database in Unity Catalog. Selecting the <strong class="bold">Models</strong> tab <a id="_idIndexMarker489"/>opens a new UI, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer218">
					<img alt="Figure 7.10 – Viewing the model in Unity Catalog" src="image/B16865_07_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Viewing the model in Unity Catalog</p>
			<ol>
				<li value="5">Select the forecasting model. You’ll notice that we have two versions of the model. Add an alias to each: <strong class="bold">champion</strong> and <strong class="bold">challenger</strong> are common to indicate the current best model and the newer version being considered to replace the current <span class="No-Break">best model:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer219">
					<img alt="Figure 7.11 – Assigning aliases to identify model status" src="image/B16865_07_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – Assigning aliases to identify model status</p>
			<p class="list-inset">Now, it’s time to think about how we want to deploy this model. Since this is a sales forecasting use case that’s predicting 10+ days in advance, real-time inference doesn’t make the <span class="No-Break">most sense.</span></p>
			<ol>
				<li value="6">Open <strong class="source-inline">CH7-02-Batch Inference</strong> to run inference on the test set. Notice that in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.12</em>, we <a id="_idIndexMarker490"/>define <strong class="source-inline">model_uri</strong> using the alias rather than the model <span class="No-Break">version number:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer220">
					<img alt="Figure 7.12 – Batch forecasting on the test set using the Champion model" src="image/B16865_07_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Batch forecasting on the test set using the Champion model</p>
			<p>This inference code will always run inference on the data provided using the Champion model. If we later determine that an updated version is better, we can change the model alias and run the correct model without making any code changes to the <span class="No-Break">inference code.</span></p>
			<p>The next practical step is to set up a workflow on a schedule. Please refer to the streaming project to see this demonstrated. This wraps up the end of this project. In <a href="B16865_08.xhtml#_idTextAnchor384"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, we will use the Favorita Sales data to show how easily a SQLbot can be created using<a id="_idTextAnchor362"/> the Foundational <span class="No-Break">Model API.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor363"/>Project – Streaming Transactions</h2>
			<p>We have much to do to wrap up<a id="_idIndexMarker491"/> the Streaming Transactions project! We will build our model, wrap it, validate it, and implement batch inference. To accomplish this, we’ll begin ingesting the labels into a different table. This allows us to set up the inference table and merge the actual labels after the predictions happen. We will create two workflows: <em class="italic">Production Streaming Transactions</em> and <em class="italic">Production Batch Inference and </em><span class="No-Break"><em class="italic">Model Retraining</em></span><span class="No-Break">.</span></p>
			<p>As with any project, we must refine the previously written code as we work toward production. The notebooks that look familiar from earlier in this book may have some minor updates, but most of <a id="_idIndexMarker492"/>the code <span class="No-Break">remains unchanged.</span></p>
			<p>Before we jump in, let’s remember where we are and where we are going by taking a quick look at the <span class="No-Break">project pipeline:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer221">
					<img alt="Figure 7.13 – The project pipeline for the Production Streaming Transactions project" src="image/B16865_07_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – The project pipeline for the Production Streaming Transactions project</p>
			<p>To follow along in your workspace, please open the <span class="No-Break">following notebooks:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH7-01-Generating Records</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH7-02-Auto Loader</strong></span></li>
				<li><strong class="source-inline">CH7-03-Feature </strong><span class="No-Break"><strong class="source-inline">Engineering Streams</strong></span></li>
				<li><strong class="source-inline">CH7-04-Update Maximum Price </strong><span class="No-Break"><strong class="source-inline">Feature Table</strong></span></li>
				<li><strong class="source-inline">CH7-05-Wrapping and Logging the </strong><span class="No-Break"><strong class="source-inline">Baseline Model</strong></span></li>
				<li><strong class="source-inline">CH7-06-Wrapping and Logging the </strong><span class="No-Break"><strong class="source-inline">Production Model</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH7-07-Model Validation</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH7-08-Batch Inference</strong></span></li>
				<li><strong class="source-inline">CH7-09-Production </strong><span class="No-Break"><strong class="source-inline">Batch Inference</strong></span></li>
				<li><strong class="source-inline">CH7-10-Auto </strong><span class="No-Break"><strong class="source-inline">Label Loader</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">mlia_utils/transactions_funcs.py</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">production_streams.yaml</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">model_retraining_n_inference_workflow.yaml</strong></span></li>
			</ul>
			<p>We recommend using a multi-node CPU cluster for this project. The first four notebooks (<strong class="source-inline">CH7-01</strong> through <strong class="source-inline">CH7-04</strong>) are nearly identical to their previous versions in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, but the <a href="B16865_07.xhtml#_idTextAnchor325"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> versions all<a id="_idIndexMarker493"/> point to the production rather than the development catalog. Table names are parameterized in widgets so that they can be set in workflows. Here is a list of the essential notebook-specific changes that have been made to the first <span class="No-Break">four notebooks:</span></p>
			<ul>
				<li><strong class="source-inline">CH7-01-Generating Records</strong>: Labels and transactions are now being written to separate folders. The data generation components have also been moved to the <strong class="source-inline">transactions_funcs</strong> file in the <span class="No-Break"><strong class="source-inline">utils</strong></span><span class="No-Break"> folder.</span></li>
				<li><strong class="source-inline">CH7-02-Auto Loader</strong>: The label column is no longer being added to the transactions table. For production, we ensure that the table being written to has <strong class="source-inline">delta.enableChangeDataFeed = true</strong> before the stream starts. If the table property is set after the stream starts, the stream is interrupted and will require a restart. Lastly, if the table property is never set, Lakehouse Monitoring is negatively impacted and will not support <span class="No-Break">continuous monitoring.</span></li>
				<li><strong class="source-inline">CH7-03-Feature Engineering Streams</strong>: Similar to the <strong class="source-inline">CH7-02-Pipeline Auto Loader</strong> notebook, table properties are set before any data <span class="No-Break">is written.</span></li>
				<li><strong class="source-inline">CH7-04-Update Maximum Price Feature Table</strong>: The code is cleaned up for a step toward production. Specifically, the feature table is updated rather than created once <span class="No-Break">it exists.</span></li>
			</ul>
			<p>You will need transaction data in the production catalog to create the model. Note the new setup variable, <strong class="source-inline">$env=prod</strong>. The pipeline workflow notebooks – that is, <strong class="source-inline">CH7-01</strong>, <strong class="source-inline">CH7-02</strong>, and <strong class="source-inline">CH7-03</strong> – are <a id="_idIndexMarker494"/>ready to be added to a workflow in the jobs section of Databricks. Start by clicking the <strong class="bold">Workflows</strong> icon on the left-side menu bar. Then, click <strong class="bold">Create job</strong>. We have provided you with a YAML file, <strong class="source-inline">production_streams.yaml</strong>, to guide you. Note that in the YAML file and <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.14</em>, the tasks do not depend on <span class="No-Break">one another:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer222">
					<img alt="Figure 7.14 – The task DAG (left) and some settings (right) for the Production Streaming Transactions job" src="image/B16865_07_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – The task DAG (left) and some settings (right) for the Production Streaming Transactions job</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.14</em>, you can use the compute cluster shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em> for the job and the other notebooks. Also, we chose to add parameters at the job level rather than the individual tasks. You can now easily generate the data needed to build <span class="No-Break">the model.</span></p>
			<p>We will use <strong class="source-inline">training_set</strong>, which we created in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, to train an updated version of the model. Recall that logging a model in Unity Catalog packages the feature metadata with the model. Hence, at inference time, it automatically looks up features from feature tables provided in the specified training set. We added this spec to the <strong class="source-inline">CH7-05-Wrapping and Logging the Baseline Model</strong> and <strong class="source-inline">CH7-06-Wrapping and Logging the Production Model</strong> notebooks. We will not go through these notebooks separately. The difference between them is that the baseline model is trained on data in the development catalog. Then, the model is re-trained in production using production data <a id="_idIndexMarker495"/>from the <span class="No-Break">inference table.</span></p>
			<p>We know that we don’t have an inference table yet. Don’t worry – it’s coming! Going back to <strong class="source-inline">training_set</strong>, being able to match up the features is only useful if we have feature values. We use the time boundaries of our feature table to ensure that the raw transactions being used for training have feature values. Also, we require the label column to be present, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer223">
					<img alt="Figure 7.15 – Defining the feature lookups for the inference model" src="image/B16865_07_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Defining the feature lookups for the inference model</p>
			<p>The transactions shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.15</em> come from the inference table, <strong class="source-inline">packaged_transaction_model_predictions</strong>, which was created in <strong class="source-inline">CH7-08-Batch Inference</strong>. The baseline model does something similar with the<strong class="source-inline"> raw_transactions</strong> table. The baseline model also sets the model description, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer224">
					<img alt="Figure 7.16 – Setting the model description" src="image/B16865_07_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Setting the model description</p>
			<p>We are now ready to focus <a id="_idIndexMarker496"/>on the model. Let’s walk through the rest of <span class="No-Break">the process:</span></p>
			<ol>
				<li>Set the model registry to Unity Catalog <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">mlflow.set_registry_uri("databricks-uc")</strong></span><span class="No-Break">.</span></li>
				<li>Use <strong class="source-inline">pip</strong> to save a <span class="No-Break"><strong class="source-inline">requirements.txt</strong></span><span class="No-Break"> file.</span></li>
				<li>Create a PyFunc wrapper <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">TransactionModelWrapper</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>Most of the code for <strong class="source-inline">TransactionModelWrapper</strong> should look familiar to those who have created a model using Sklearn. The initialization function, <strong class="source-inline">__init__(self, model, X, y, numeric_columns, cat_columns)</strong>, accepts a model and DataFrames. The data preprocessing for training and inference data is standardized within the wrapper. <strong class="source-inline">TransactionModelWrapper</strong> consists of four methods: <strong class="source-inline">init</strong>, <strong class="source-inline">predict</strong>, <strong class="source-inline">preprocess_data</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">fit</strong></span><span class="No-Break">.</span></p>
			<p>The initialization method does <span class="No-Break">the following:</span></p>
			<ol>
				<li>Splits the data into train and <span class="No-Break">test sets.</span></li>
				<li>Initializes and fits <strong class="source-inline">OneHotEncoder</strong> for the categorical feature <span class="No-Break">columns provided.</span></li>
				<li>Initializes and fits <strong class="source-inline">StandardScaler</strong> for the numerical feature <span class="No-Break">columns provided.</span></li>
				<li>Applies the <strong class="source-inline">preprocess_data</strong> method to the training and <span class="No-Break">test data.</span></li>
				<li>Defines an <strong class="source-inline">evaluation</strong> method to calculate the log loss on the <em class="italic">X</em> and <em class="italic">y</em> <span class="No-Break">sets provided.</span></li>
				<li>Invokes the <strong class="source-inline">evaluation</strong> method on preprocessed <span class="No-Break">test data.</span></li>
				<li>Defines and invokes the <strong class="source-inline">_model_signature</strong> method to easily provide the signature when logging <span class="No-Break">the model.</span></li>
			</ol>
			<p>The <strong class="source-inline">predict</strong> method calls the <strong class="source-inline">preprocess_data</strong> (<span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.17)</em> method on the input DataFrame before <a id="_idIndexMarker497"/>performing and returning the prediction. This method is used to process the training data and the inference data, ensuring identical preprocessing <span class="No-Break">for predictions:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer225">
					<img alt="Figure 7.17 – The preprocessing method for the model" src="image/B16865_07_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – The preprocessing method for the model</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.17</em>, the fitted numeric scaler and one-hot encoder are passed as input. This protects against skew between the training and inference features. Notice how <strong class="source-inline">TransactionTimestamp</strong> is dropped. This is done because after the features from the feature tables are present, we no longer need a timestamp. The input DataFrame can be a Spark or pandas DataFrame. This is why we need different syntax to drop the <span class="No-Break">timestamp column.</span></p>
			<p>In the following command cell, you customize <strong class="source-inline">mlflow.autolog</strong> and start the MLflow experiment for training, testing, wrapping, and logging the model. You will use <strong class="source-inline">mlflow.evaluate</strong> to handle the evaluation process. The logging process is easy – you call <strong class="source-inline">log_model</strong> with the model name, wrapped model, <strong class="source-inline">pyfunc</strong> flavor, and previously created training set. This process also registers the model in Unity Catalog. The last thing in this notebook is a quick test that’s performed on the predict function showing how to pass in the Spark context with the input data. You are now ready to validate <span class="No-Break">the model.</span></p>
			<p>Next, focus on the <strong class="source-inline">CH7-07-Model Validation</strong> notebook, which checks that the input model has the correct metadata so that it’s ready for production. This notebook can be used to test any model. Ideally, you will add numerous checks, including the ability to predict and possibly test the accuracy of specific slices of data. For example, you could check the model <a id="_idIndexMarker498"/>performance on each product or geography. You can pass those columns with tags when slices <span class="No-Break">need testing.</span></p>
			<p>Collect the model details, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.18</em>. Notice the use of the <strong class="source-inline">util</strong> function, which is imported in a previous cell from <strong class="source-inline">mlia_utils.mlflow_funcs </strong><span class="No-Break"><strong class="source-inline">import get_latest_model_version</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer226">
					<img alt="Figure 7.18 – Using the mlfclient and util functions to access the model details" src="image/B16865_07_18.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Using the mlfclient and util functions to access the model details</p>
			<p>Each time you train and log the model, a new model version is created. Using tags, you can indicate which model version(s) must be tested and validated <span class="No-Break">before deployment:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer227">
					<img alt="Figure 7.19 – Assertions to ensure the model needs to be tested" src="image/B16865_07_19.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Assertions to ensure the model needs to be tested</p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.20</em>, you need an informative model description for all production models. We recommend that you include information about the use case the model is used for. Metadata hygiene is becoming increasingly important as companies want to leverage generative AI on internal data. This is because LLMs use the metadata fields to find relevant information in <span class="No-Break">the models:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer228">
					<img alt="Figure 7.20 – Validating that a model description is present" src="image/B16865_07_20.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – Validating that a model description is present</p>
			<p>Similar to what’s shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.20</em>, the notebook checks for tags. These are examples to get you started. This is <a id="_idIndexMarker499"/>an ideal section to expand on the current code and continue adding validation results and <span class="No-Break">update tags:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer229">
					<img alt="Figure 7.21 – Processing the results of the validation tests" src="image/B16865_07_21.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 – Processing the results of the validation tests</p>
			<p>With a tested model, you can progress to inference; see the <strong class="source-inline">CH7-08-Batch Inference</strong> notebook. Review the examples of performing batch inference on a DataFrame (<span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.22</em>)<a id="_idTextAnchor364"/> and a JSON <span class="No-Break">data string:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer230">
					<img alt="Figure 7.22 – Loading and scoring the model by performing batch inference on a DataFrame" src="image/B16865_07_22.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.22 – Loading and scoring the model by performing batch inference on a DataFrame</p>
			<p>Now, look at the code in <strong class="source-inline">CH7-09-Production Batch Inference</strong>. The substantial changes include <strong class="source-inline">scoring_df</strong>, which is the DataFrame we apply our model to for predictions. Notice<a id="_idIndexMarker500"/> that in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.23</em>, the <strong class="source-inline">min_time</strong> and <strong class="source-inline">max_time</strong> variables provide boundaries for the transactions, ensuring the batch feature values have been calculated. Additionally, the inference table provides a boundary that prevents duplicate <span class="No-Break">prediction calculations:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer231">
					<img alt="Figure 7.23 – The scoring_df query’s configuration" src="image/B16865_07_23.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.23 – The scoring_df query’s configuration</p>
			<p>The inference table in <strong class="source-inline">CH7-08</strong> needs to be updated to fit the requirements for the inference table monitoring <a id="_idIndexMarker501"/>provided by Lakehouse Monitoring. This means adding the <strong class="source-inline">model_version</strong> and <strong class="source-inline">actual_label</strong> columns. The <strong class="source-inline">actual_label</strong> column is set to <strong class="source-inline">NULL</strong> so that it is clear the value has not been updated yet; see <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.24</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer232">
					<img alt="Figure 7.24 – The addition of the model version and actual label columns" src="image/B16865_07_24.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.24 – The addition of the model version and actual label columns</p>
			<p>These two additional columns for the inference table are requirements for Lakehouse Monitoring. The <strong class="source-inline">InferenceLog</strong> monitor comes with autogenerated dashboards. However, you need to populate the table. Begin by creating a bronze table for the transaction labels. The Auto Loader is back again, focusing on labels; see <strong class="source-inline">CH7-10-Auto Label Loader</strong> and <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.25</em>. In the notebook, the <strong class="source-inline">transaction_labels</strong> table was created; this is similar to the code from <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.25</em>, you can use the CDF and CDC Delta features to update the new inference table with the<a id="_idIndexMarker502"/> ground <span class="No-Break">truth label:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer233">
					<img alt="Figure 7.25 – Merging transaction labels into the inference table" src="image/B16865_07_25.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.25 – Merging transaction labels into the inference table</p>
			<p>You now have a bronze table and the ability to merge new labels into the inference table. However, the inference table is still empty. So, let’s create a workflow job, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.26</em>, to generate predictions every <span class="No-Break">15 minutes:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer234">
					<img alt="Figure 7.26 – The DAG for the inference (upper) and model retraining (lower) job" src="image/B16865_07_26.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.26 – The DAG for the inference (upper) and model retraining (lower) job</p>
			<p>Walk through the two workflow paths, starting with the upper path. The batch features are updated, thus providing feature data for inference. The data is ready for predictions, and the inference task <span class="No-Break">can begin.</span></p>
			<p>As its first task, the lower path updates labels. It adds the latest data to the <strong class="source-inline">transaction_labels</strong> table and merges all new labels that match previous predictions into the inference table. Skip forward beyond the first iteration and the inference table contains not only <a id="_idIndexMarker503"/>previous predictions but also the labels for those predictions. Model training is performed using the updated table containing the features. Retraining the model only occurs if there is data in the inference table, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.27</em>. The retraining process is, of course, followed by validation when needed. The validation notebook exits when it detects that the latest version of the model has already <span class="No-Break">been tested:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer235">
					<img alt="Figure 7.27 – The retraining notebook exits if there is no data to retrain on" src="image/B16865_07_27.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.27 – The retraining notebook exits if there is no data to retrain on</p>
			<p>Create the workflow job shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.29</em>. You can reference the configuration for the job in the <strong class="source-inline">model_retraining_n_inference_workflow.yaml</strong> file. The workflow automatically provides the lineage of all upstream and downstream tables. You can see these in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.26</em>. This saves us time <span class="No-Break">on documentation:</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div class="IMG---Figure" id="_idContainer236">
					<img alt="Figure 7.28 – The workflow table lineage" src="image/B16865_07_28.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.28 – The workflow table lineage</p>
			<p>All that is left is to run both workflows simultaneously. After letting both run for a bit (don’t forget to schedule the Production Batch Inference and Model Retraining workflow), you should have a <a id="_idIndexMarker504"/>screen that looks similar to what’s shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.29</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer237">
					<img alt="Figure 7.29 –  The historical view of successful job runs" src="image/B16865_07_29.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.29 – The historical view of successful job runs</p>
			<p>You now have all of the pieces to productionize <span class="No-Break">this project.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor365"/>Project – multilabel image classification</h2>
			<p>We currently have a working image classification model that we trained and evaluated in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. Now, let’s add<a id="_idIndexMarker505"/> some infrastructure around our code to serve our model and make it available to downstream applications and, ultimately, our end users. To follow along in your workspace, please open the <span class="No-Break">following notebooks:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">Ch7-01-Create_Final_Wrapper_Production</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">Ch7-02-Serve_In_Production</strong></span></li>
			</ul>
			<p>We’ll start by creating our model class wrapper. This wrapper includes two functions, <strong class="source-inline">feature_extractor</strong> and <strong class="source-inline">predict</strong>. The <strong class="source-inline">feature_extractor</strong> function is required because otherwise, our fine-tuned model won’t contain the same preprocessing step that’s used during fine-tuning and would, therefore, not be consistent during serving. Of course, you can simply serve your original model if you do not need to make any custom modifications to your model and only need the raw <span class="No-Break">format outputs:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer238">
					<img alt="Figure 7.30 – Creating the model class wrapper" src="image/B16865_07_30.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.30 – Creating the model class wrapper</p>
			<p>The <strong class="source-inline">feature_extractor</strong> function, which transforms an image into the format required by the served model, is the same code we used to score the model in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. Let’s dive into the prediction part; it’s similar to what we created in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> to score <span class="No-Break">our model.</span></p>
			<p>The <strong class="source-inline">predict</strong> function is similar to the one we used to score our model in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic"> </em>using <strong class="source-inline">pandas_udf</strong>. Note that we are not only returning the predicted label but also a label corresponding to it in <a id="_idIndexMarker506"/>a dictionary format (this isn’t required, but we wanted to show the output <span class="No-Break">format’s flexibility):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer239">
					<img alt="Figure 7.31 – Including the feature_extractor and predict functions in the model class wrapper" src="image/B16865_07_31.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.31 – Including the feature_extractor and predict functions in the model class wrapper</p>
			<p>Now, we are ready to wrap our fine-tuned model from <a href="B16865_04.xhtml#_idTextAnchor180"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> into the wrapper. To do this, we must load the artifact from MLflow and pass it to the pre-created <span class="No-Break"><strong class="source-inline">CVModelWrapper</strong></span><span class="No-Break"> class:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer240">
					<img alt="Figure 7.32 – Loading our model from the existing MLflow experiment" src="image/B16865_07_32.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.32 – Loading our model from the existing MLflow experiment</p>
			<p>Let’s test whether our <a id="_idIndexMarker507"/>wrapper is functioning as expected. To do so, we must encode a few images (as the model serving accepts strings and cannot accept images and convert them yet) and save them into a pandas DataFrame. Then, we must use our model wrapper to get <span class="No-Break">a prediction:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer241">
					<img alt="Figure 7.33 – Using our model wrapper to create predictions on a few images" src="image/B16865_07_33.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.33 – Using our model wrapper to create predictions on a few images</p>
			<p>Next, we will use MLflow to log and serve the model via Databricks <span class="No-Break">Model Serving:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer242">
					<img alt="Figure 7.34 – Logging and running models using MLflow" src="image/B16865_07_34.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.34 – Logging and running models using MLflow</p>
			<p>During the production phase, you<a id="_idIndexMarker508"/> will usually operate on the aliases and the latest model version, so here, we’ll simulate setting the alias, Champion, to the best-performing model and getting the latest model version to <span class="No-Break">be deployed:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer243">
					<img alt="Figure 7.35 – Loading our champion model" src="image/B16865_07_35.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.35 – Loading our champion model</p>
			<p>Next, we must create our model serving endpoint using the Databricks SDK. You could also create your endpoint using the UI. If you decide to use the SDK, you must create a configuration file for your <a id="_idIndexMarker509"/>endpoint. The following example is for a CPU container with a small workload size. If you are unfamiliar with this option, please check out <em class="italic">Create model serving endpoints</em> in the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer244">
					<img alt="Figure 7.36 – Setting config input for the endpoint to serve our model" src="image/B16865_07_36.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.36 – Setting config input for the endpoint to serve our model</p>
			<p>Once the settings have been provided, you are ready to deploy or update your endpoint if it <span class="No-Break">already exists:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer245">
					<img alt="Figure 7.37 – Deploying a new endpoint if it does not exist or updating the existing one" src="image/B16865_07_37.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.37 – Deploying a new endpoint if it does not exist or updating the existing one</p>
			<p>Keep in mind that if your <a id="_idIndexMarker510"/>endpoint does not exist, it will take a while <span class="No-Break">to deploy:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer246">
					<img alt="Figure 7.38 – UI example while the GPU endpoint is deploying/updating" src="image/B16865_07_38.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.38 – UI example while the GPU endpoint is deploying/updating</p>
			<p>Now, we can score our model. Again, for simplicity and reusability, we are covering the serving call into a <span class="No-Break"><strong class="source-inline">score_model</strong></span><span class="No-Break"> function:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer247">
					<img alt="Figure 7.39 – Defining a model scoring function" src="image/B16865_07_39.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.39 – Defining a model scoring function</p>
			<p>Lastly, we must score our model using our model scoring function, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.40</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer248">
					<img alt="Figure 7.40 – Scoring our model" src="image/B16865_07_40.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.40 – Scoring our model</p>
			<p>Here is an example of <a id="_idIndexMarker511"/>scoring under the <span class="No-Break">UI page:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer249">
					<img alt="Figure 7.41 – Scoring our model under the UI page of the Databricks Model Serving" src="image/B16865_07_41.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.41 – Scoring our model under the UI page of the Databricks Model Serving</p>
			<p>Now that our model is ready for production, we can train and create it, designate our champion model, serve it on an <a id="_idIndexMarker512"/>endpoint, and score. The next steps might include setting up a monitor to track image pixel distributions, the number of images, label distributions, and the distribution of the response. We will talk more about model moni<a id="_idTextAnchor366"/>toring in <a href="B16865_08.xhtml#_idTextAnchor384"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor367"/>Project – retrieval augmented generation chatbot</h2>
			<p>In the previous chapter, we completed <a id="_idIndexMarker513"/>our chatbot and tested it out in a notebook. It might be tempting to jump to the final deployment step immediately, but that would involve skipping a critical step in the process – evaluation! Evaluating projects for correctness before providing them to the end users should always be part of the development process. However, it can be especially tricky to build automated evaluation solutions for newer technologies and techniques, such as when working with LLMs. This is a continually developing area of research, and we recommend reading Databricks’ recommendations on <em class="italic">Best Practices for LLM Evaluation</em> for more information (linked in <em class="italic">Further reading</em>). We’ll walk through the MLflow evaluation capability to evaluate the RAG chatbot <span class="No-Break">we’ve built.</span></p>
			<p>To follow along in your workspace, please open the <span class="No-Break">following note<a id="_idTextAnchor368"/>books:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">CH7-01-GetData</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">CH7-<a id="_idTextAnchor369"/>02-Evaluating_ChatBot</strong></span></li>
			</ul>
			<h3>Loading the ground truth labels</h3>
			<p>We’ll start in the first notebook, <strong class="source-inline">CH7-01-GetData</strong>. To evaluate our model, we must have ground truth labels – that is, the correct answers. This generally involves human effort to write out some<a id="_idIndexMarker514"/> typical questions you expect users to ask your chatbot and the answers you expect your chatbot to respond with. To simplify this process, we created a file containing 35 sample questions and the corresponding human-generated answers for this project, saved to <strong class="source-inline">Questions_Evaluation.csv</strong>. Let’s load this file and examine the question and <span class="No-Break">answer pairs:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer250">
					<img alt="Figure 7.42 – Reading our pre-created evaluation set" src="image/B16865_07_42.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.42 – Reading our pre-created evaluation set</p>
			<p>Take a look at some of the records to get a sense of the questions a user might ask and the expected answers. You can also add your own examples to augment the evaluation <span class="No-Break">data further:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer251">
					<img alt="Figure 7.43 – Question-answer pair examples" src="image/B16865_07_43.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.43 – Question-answer pair examples</p>
			<p>Let’s save these examples to a Delta table <a id="_idIndexMarker515"/>named <strong class="source-inline">evaluation_table</strong>. That way, if a new <strong class="source-inline">Question_Evaluation.csv</strong> file with different examples is uploaded, you can append the new examples to the existing table without risking losing the <span class="No-Break">original data:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer252">
					<img alt="Figure 7.44 – Creating evaluation_table to store the question/answer pairs" src="image/B16865_07_44.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.44 – Creating evaluation_table to store the question/answer pairs</p>
			<p>Upon saving the table, we are now ready to evaluate <span class="No-Break">our model:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer253">
					<img alt="Figure 7.45 – Saving  thequestions_evaluation.csv da﻿ta to a Delta table" src="image/B16865_07_45.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.45 – Saving thequestions_evaluation.csv da<a id="_idTextAnchor370"/>ta to a Delta table</p>
			<h3>Setting up the evaluation workflow</h3>
			<p>We’re now ready to open the second notebook, <strong class="source-inline">CH7-02-Evaluating_ChatBot</strong>, and evaluate our chatbot against the ground truth labels we saved to <strong class="source-inline">evaluation_table</strong>. Let’s briefly<a id="_idIndexMarker516"/> discuss how we will compare our model’s outputs to the human-generated answers. While there is plenty of ongoing research in the realm of automated LLM evaluation methods, we will focus on one technique in particular: <em class="italic">LLM-as-a-judge</em>. This method brings in a second LLM to evaluate the performance of the first, automating the tedious task of comparing the generated answer to the true answer, something<a id="_idIndexMarker517"/> a human would traditionally have to do. Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>To use an LLM as a judge, we must load the original model we created in <a href="B16865_06.xhtml#_idTextAnchor297"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer254">
					<img alt="Figure 7.46 – Loading mlaction_chatbot_model as rag_model" src="image/B16865_07_46.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.46 – Loading mlaction_chatbot_model as rag_model</p>
			<ol>
				<li value="2">Now, we must run a quick test to verify that our RAG model works <span class="No-Break">as expected:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer255">
					<img alt="Figure 7.47 – Verifying our loaded model works as expected" src="image/B16865_07_47.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.47 – Verifying our loaded model works as expected</p>
			<ol>
				<li value="3">Next, we must use our RAG chatbot to generate answers for all of the example questions we stored in <strong class="source-inline">evaluation_table</strong>. These responses are what we will compare against the ground truth answers. We’ll build a <strong class="source-inline">pandas_udf</strong> function to make this part <span class="No-Break">run faster:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer256">
					<img alt="Figure 7.48 – Creating a function to receive a question and return an answer using our RAG model" src="image/B16865_07_48.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.48 – Creating a function to receive a question and return an answer using our RAG model</p>
			<p class="list-inset">We used Llama-2-70b for our judge, but you could use GPT-4 or any other LLM you prefer (though we cannot guarantee satisfactory results!). Our code leverages the<a id="_idIndexMarker518"/> Databricks Foundational Model API, which we also used in <a href="B16865_03.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 3</em></span></a> when creating the embeddings of ArXiv article text chunks. As a reminder, the Databricks Foundation Model APIs provide direct access to state-of-the-art open models from a serving endpoint, allowing you to incorporate high-quality generative AI models into your application without the need to maintain your model deployment. We call the Llama-2-70b endpoint in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.49</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer257">
					<img alt="Figure 7.49 – Testing the Foundation Model endpoint for Llama-2-70b" src="image/B16865_07_49.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.49 – Testing the Foundation Model endpoint for Llama-2-70b</p>
			<ol>
				<li value="4">Next, we must build a DataFrame from the <strong class="source-inline">evaluation_table</strong> questions and answers. If you have added many more question/answer examples to this dataset, you may want to downsample the number of questions and speed up the prediction process. Then, we must call our <span class="No-Break">UDF, </span><span class="No-Break"><strong class="source-inline">predict_answer</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer258">
					<img alt="Figure 7.50 – Building a DataFrame with questions and ground truth answers, and adding RAG chatbot answers" src="image/B16865_07_50.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.50 – Building a DataFrame with questions and ground truth answers, and adding RAG chatbot answers</p>
			<ol>
				<li value="5">Now that we have our<a id="_idIndexMarker519"/> DataFrame with the chatbot’s responses to each question, we must save this to a Delta table. We’ll continue to reference the DataFrame throughout the rest of this code, but this way, we won’t have to generate the chatbot’s responses again if we want to query this data in <span class="No-Break">the future:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer259">
					<img alt="Figure 7.51 – Writing our evaluation DataFrame to a new table" src="image/B16865_07_51.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.51 – Writing our evaluation DataFrame to a new table</p>
			<ol>
				<li value="6">As we mentioned at the beginning of this section, we are using MLflow’s Evaluate capability to facilitate our model evaluation. Before we evaluate our RAG chatbot, let’s load the methods and verify how MLflow Evaluate works by default. First, we must load the “answer correctness” metric, which we will <span class="No-Break">use as-is:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer260">
					<img alt="Figure 7.52 – Viewing the answer correctness metrics" src="image/B16865_07_52.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.52 – Viewing the answer correctness metrics</p>
			<p class="list-inset">The out-of-the-box metrics<a id="_idIndexMarker520"/> are good, but professionalism is also an important criterion for our use case, so we’ll create a custom professionalism metric. <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.53</em> shows how to use the <strong class="source-inline">make_genai_metric()</strong> function to build out our professionalism <span class="No-Break">evaluation metric:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer261">
					<img alt="Figure 7.53 – Adding a custom professionalism metric" src="image/B16865_07_53.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.53 – Adding a custom professionalism metric</p>
			<ol>
				<li value="7">As you can see from <strong class="source-inline">grading_prompt</strong>, we’ve designed this metric to give a score between one and five, where a score of one identifies text as casual and a score of five evaluates <a id="_idIndexMarker521"/>text as noticeably formal. This is a powerful tool to evaluate your model based on criteria that are important to your business use case. You can modify the template according to your needs. We must also add examples of the metric, as defined in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.54</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer262">
					<img alt="Figure 7.54 – Adding an example for the custom professionalism metric" src="image/B16865_07_54.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.54 – Adding an example for the custom professionalism metric</p>
			<ol>
				<li value="8">With our professionalism metric, let’s run the model evaluation using MLfLow. To run an evaluation, we can call <strong class="source-inline">mlflow.evaluate()</strong> against the pandas DataFrame containing the <a id="_idIndexMarker522"/>questions, ground truth answers, and chatbot-generated answers. We’ll include the answer correctness and professionalism metrics as extra metrics. The following code will calculate many other metrics in addition to the two we specified, such as token count, toxicity, and Automated Readability Index grade (the approximate grade level required to comprehend <span class="No-Break">the text):</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer263">
					<img alt="Figure 7.55 – Running an MLflow experiment with m﻿lflow.evaluate()" src="image/B16865_07_55.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.55 – Running an MLflow experiment with m<a id="_idTextAnchor371"/>lflow.evaluate()</p>
			<p>Once we’ve run the <a id="_idIndexMarker523"/>experiment, the metrics can be accessed in our notebook and via the UI so that we can easily see how our chatbot is performing in terms of accuracy <span class="No-Break">and<a id="_idTextAnchor372"/> professionalism.</span></p>
			<h3>Evaluating the chatbot’s responses</h3>
			<p>First, let’s look at the MLfLow UI in Databricks to compare the results between the human-generated and <a id="_idIndexMarker524"/>chatbot-generated responses. To do so, navigate to the <strong class="bold">Experiments</strong> page and open the experiment matching your notebook’s name (in this case, <strong class="source-inline">CH7-02-Evaluating Chatbot</strong>). Then, navigate to the <strong class="bold">Evaluation</strong> tab. We’re using this view to see some examples of how closely the bot’s <strong class="source-inline">answers</strong> match what we would expect to see, but it is also particularly useful when you want to test and compare outputs across different models or <span class="No-Break">chunking strategies:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer264">
					<img alt="Figure 7.56 – Using the Evaluation view in Databricks MLflow" src="image/B16865_07_56.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.56 – Using the Evaluation view in Databricks MLflow</p>
			<p>Take a look at a few examples from the evaluation dataset. We’ll see that our model, according to a quick human assessment, is performing reasonably well. Of course, this form of evaluation isn’t <a id="_idIndexMarker525"/>scalable, so let’s dig into the other metrics as well. We’ll use the common visualization library, <strong class="source-inline">plotly</strong>, to take a closer look at our model results. First, we’ll look at the distribution of token counts in our <span class="No-Break">chatbot’s responses:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer265">
					<img alt="Figure 7.57 – Plotting token counts from our chatbot’s responses" src="image/B16865_07_57.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.57 – Plotting token counts from our chatbot’s responses</p>
			<p>While interesting, what we care about here are the two metrics we discussed earlier: correctness and professionalism. Let’s take a look at the distribution of the <span class="No-Break">correctness scores:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer266">
					<img alt="Figure 7.58 – Plotting the correctness score distribution" src="image/B16865_07_58.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.58 – Plotting the correctness score distribution</p>
			<p>Let’s also view the professionalism score distribution. Our distribution is threes and fours, which means the tone<a id="_idIndexMarker526"/> is most often “borderline professional.” This is how we defined it in our <span class="No-Break">custom metric:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer267">
					<img alt="Figure 7.59 – Plotting the professionalism score distribution" src="image/B16865_07_59.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.59 – Plotting the professionalism score distribution</p>
			<p>Overall, our model is looking <a id="_idIndexMarker527"/>good! If we’re satisfied with the accuracy and professionalism scores, we can mark this model as ready for production by giving it a <span class="No-Break">production alias:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer268">
					<img alt="Figure 7.60 – Aliasing our model to show it is production-ready" src="image/B16865_07_60.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.60 – Aliasing our model to show it is production-ready</p>
			<p>With that, we evaluated our chatbot by creating predictions from our question-and-answer dataset, created a custom evaluation metrics to evaluate the professionalism of each response, and visualized information about our model outputs. In the last chapter, we will demonstrate how to build a Gradio app so that you can bring your RAG chatbot to <a id="_idTextAnchor373"/>your <span class="No-Break">end<a id="_idTextAnchor374"/> users!</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor375"/>Summary</h1>
			<p>Implementing models in production can be challenging, but with tools designed to support productionizing models and automating the MLOps process, it is much easier. In this chapter, we looked at using the UC Model Registry to manage the life cycle of an ML model. We highlighted MLflow and how it can be used to create reproducible, modularized data science workflows that automatically track parameters and performance metrics. We also discussed techniques for calculating features at the time of inference. To make the end-to-end MLOps process more manageable, we showed how to use workflows and webhooks to automate the ML life cycle. We also showed how to serve models and make inferences using MLflow and the <span class="No-Break">Data<a id="_idTextAnchor376"/>bricks platform.</span></p>
			<p>In the last chapter, <em class="italic">Monitoring, Evaluating, and More</em>, we will look at monitoring our data and ML models within the Databricks Lakehouse so that you can get the most value <a id="_idTextAnchor377"/>from <span class="No-Break">your data.</span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor378"/>Questions</h1>
			<p>Let’s test ourselves on what we’ve learned by going through the <span class="No-Break">following questions:</span></p>
			<ol>
				<li>Can more than one model be in production simultaneously? When would you want to use two model<a id="_idTextAnchor379"/>s <span class="No-Break">in production?</span></li>
				<li>What component of MLflow could you use to <span class="No-Break">route approvals?</span></li>
				<li>Can you use an MLflow API to s<a id="_idTextAnchor380"/>erve <span class="No-Break">your model?</span></li>
			</ol>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor381"/>Answers</h1>
			<p>After putting some thought into the preceding questions, compare your answers <span class="No-Break">to ours:</span></p>
			<ol>
				<li>Yes, having multiple models in production simultaneously is possible, and this is appropriate for use cases such as comparing models in a challenger/champion test or running <span class="No-Break">A/B tests.</span></li>
				<li>The UC Model Registry can be used to <span class="No-Break">route approvals.</span></li>
				<li>The Model Serving API within MLFlow can be used for<a id="_idTextAnchor382"/> <span class="No-Break">model serving.</span></li>
			</ol>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor383"/>Further reading</h1>
			<p>This chapter discussed tools and technologies to help productionize ML. Please take a look at these resources to dive deeper into the areas that interest you most – and help you get more of your ML projects <span class="No-Break">into production!</span></p>
			<ul>
				<li><em class="italic">Best Practices for Using Structured Streaming in Production - The Databricks </em><span class="No-Break"><em class="italic">Blog</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/streaming-production-collected-best-practices"><span class="No-Break">https://www.databricks.com/blog/streaming-production-collected-best-practices</span></a></li>
				<li><em class="italic">The big book of machine learning use </em><span class="No-Break"><em class="italic">cases</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/resources/ebook/big-book-of-machine-learning-use-cases"><span class="No-Break">https://www.databricks.com/resources/ebook/big-book-of-machine-learning-use-cases</span></a></li>
				<li><em class="italic">Databricks Model </em><span class="No-Break"><em class="italic">Serving</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/2023/03/07/announcing-general-availability-databricks-model-serving.html"><span class="No-Break">https://www.databricks.com/blog/2023/03/07/announcing-general-availability-databricks-model-serving.html</span></a></li>
				<li><em class="italic">Create and manage serving endpoints using </em><span class="No-Break"><em class="italic">Mlflow</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/machine-learning/model-serving/create-serving-endpoints-mlflow.html"><span class="No-Break">https://docs.databricks.com/en/machine-learning/model-serving/create-serving-endpoints-mlflow.html</span></a></li>
				<li><em class="italic">Model Evaluation in </em><span class="No-Break"><em class="italic">MLFLow</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html"><span class="No-Break">https://www.databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html</span></a></li>
				<li><em class="italic">The big book of </em><span class="No-Break"><em class="italic">MLOps</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/resources/ebook/the-big-book-of-mlops"><span class="No-Break">https://www.databricks.com/resources/ebook/the-big-book-of-mlops</span></a></li>
				<li><em class="italic">Databricks Asset Bundles - Programmatically define, deploy, and run Databricks jobs, Delta Live Tables pipelines, and MLOps Stacks using CI/CD best practices and </em><span class="No-Break"><em class="italic">workflows</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/dev-tools/bundles/index.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/bundles/index.html</span></a></li>
				<li><em class="italic">Model Registry Webhooks</em>: <strong class="source-inline">MLflow Model Registry Webhooks </strong><span class="No-Break"><strong class="source-inline">on Databricks</strong></span></li>
				<li><span class="No-Break"><em class="italic">Webhooks</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/mlflow/model-registry-webhooks.html"><span class="No-Break">https://docs.databricks.com/en/mlflow/model-registry-webhooks.html</span></a></li>
				<li><em class="italic">CI/CD workflows with Git and Databricks Repos - Use GitHub and Databricks Repos for source control and CI/CD </em><span class="No-Break"><em class="italic">workflows</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/repos/ci-cd-techniques-with-repos.html"><span class="No-Break">https://docs.databricks.com/en/repos/ci-cd-techniques-with-repos.html</span></a></li>
				<li><em class="italic">Continuous integration and delivery using GitHub Actions - Build a CI/CD workflow on GitHub that uses GitHub Actions developed for </em><span class="No-Break"><em class="italic">Databricks</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-github.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-github.html</span></a></li>
				<li><em class="italic">CI/CD with Jenkins on Databricks – Develop a CI/CD pipeline for Databricks that uses </em><span class="No-Break"><em class="italic">Jenkins</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-jenkins.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-jenkins.html</span></a></li>
				<li><em class="italic">Orchestrate Databricks jobs with Apache Airflow – Manage and schedule a data pipeline that uses Apache </em><span class="No-Break"><em class="italic">Airflow</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html"><span class="No-Break">https://docs.databricks.com/en/workflows/jobs/how-to/use-airflow-with-jobs.html</span></a></li>
				<li><em class="italic">Service principals for CI/CD – </em><em class="italic">Use service principals instead of users with CI/CD </em><span class="No-Break"><em class="italic">systems</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-sp.html"><span class="No-Break">https://docs.databricks.com/en/dev-tools/ci-cd/ci-cd-sp.html</span></a></li>
				<li><em class="italic">DFE </em><span class="No-Break"><em class="italic">client</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/machine-learning/feature-store/python-api.html#use-the-clients-for-unit-testing"><span class="No-Break">https://docs.databricks.com/en/machine-learning/feature-store/python-api.html#use-the-clients-for-unit-testing</span></a></li>
				<li><span class="No-Break"><em class="italic">Unity</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/udf/unity-catalog.html"><span class="No-Break">https://docs.databricks.com/en/udf/unity-catalog.html</span></a></li>
				<li><em class="italic">Best Practices for LLM Evaluation of RAG Applications, Part </em><span class="No-Break"><em class="italic">1</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG"><span class="No-Break">https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG</span></a></li>
				<li><em class="italic">Best Practices for LLM Evaluation of RAG Applications, Part </em><span class="No-Break"><em class="italic">2</em></span><span class="No-Break">: </span><a href="https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part"><span class="No-Break">https://www.databricks.com/blog/announcing-mlflow-28-llm-judge-metrics-and-best-practices-llm-evaluation-rag-applications-part</span></a></li>
				<li><em class="italic">Create model serving </em><span class="No-Break"><em class="italic">endpoints</em></span><span class="No-Break">: </span><a href="https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html"><span class="No-Break">https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html</span></a></li>
			</ul>
		</div>
	</body></html>