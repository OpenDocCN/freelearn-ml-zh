- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying, Monitoring, and Scaling in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some people may read this book from beginning to end to gain an overall understanding
    of as many concepts as possible in the realm of AI/ML on Google Cloud, while others
    may use it as a reference, whereby they pick it up and read certain chapters on
    specific topics whenever they need to work with those topics as part of a project
    or client engagement. If you’ve been reading this book from the beginning, then
    you have come a long way, and we have journeyed together through the majority
    of the **ML model development life cycle** (**MDLC**). While model training is
    what often gets the most attention in the press – and that is where a lot of the
    magic happens – you know by now that training is just one piece of the overall
    life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: When we’ve trained and tested our models, and we believe they’re ready to be
    exposed to our clients, we need to find a way to host them so that they can be
    used accordingly. In this chapter, we will dive into that part of the process
    in more detail, including some of the challenges that exist when it comes to hosting
    and managing models and monitoring them on an ongoing basis to ensure that they
    stay relevant and perform optimally in perpetuity. We’ll begin by discussing how
    we can host our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I make my models available to my applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fundamental concepts for serving models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common challenges of serving models in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring models in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing for AI/ML at the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do I make my models available to my applications?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced this concept in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015),
    and we talked about the various things you would need to do to host a model on
    your own, such as setting up all of the required infrastructure, including load
    balancers, routers, switches, cables, servers, and storage, among other things,
    and then managing all of that infrastructure on an ongoing basis. This would require
    a lot of your time and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, all of that stuff was in the old days, and you no longer need to do
    any of that. This is because Google Cloud provides the Vertex AI prediction service,
    which enables you to host models in production within minutes, using infrastructure
    that is all managed for you by Google.
  prefs: []
  type: TYPE_NORMAL
- en: For completeness, I will also mention that if you would like to host your models
    on Google Cloud without using Vertex, numerous other Google Cloud services can
    be used for that purpose, such as **Google Compute Engine** (**GCE**), **Google
    Kubernetes Engine** (**GKE**), **Google App Engine** (**GAE**), Cloud Run, and
    Cloud Functions. We described all of these services, as well as some pointers
    on how to choose between them, in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059).
  prefs: []
  type: TYPE_NORMAL
- en: Remember that choosing the right platform to host your ML models depends on
    your specific use case and requirements. Factors such as scalability, latency,
    costs, development effort, and operational management all play a role in choosing
    the best solution. You may also make certain decisions based on the framework
    you’re using to build your ML models. For example, you might want to use TensorFlow
    Serving if you’re building models in TensorFlow, or TorchServe if you’re building
    models in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, my recommendation would be to start with a service that is dedicated
    and optimized for the task at hand, and in the case of building and hosting ML
    models, Vertex AI is that service. In this chapter, we will deploy our first model
    using Vertex AI, but before we dive into the hands-on activities, we’ll introduce
    some important concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental concepts for serving models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce some important topics related to how we can
    host our models so that our clients can interact with them.
  prefs: []
  type: TYPE_NORMAL
- en: Online and offline model serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In ML, there are generally two options we have for serving predictions from
    models: **online** serving (also known as **real-time** serving) and **offline**
    serving (also known as **batch** serving). The high-level use cases associated
    with each of these methods are **online (or real-time) inference** and **offline
    (or batch) inference**, respectively. Let’s take a few minutes to introduce these
    methods and understand their use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Online/real-time model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, in the case of real-time model serving, the model needs
    to respond “in real time” to prediction requests, which usually means that a client
    (perhaps a customer, or some other system) needs to receive an inference response
    as quickly as possible, and may be waiting synchronously for a response from the
    model. An example of this would be a fraud detection system for credit card transactions.
    As you can imagine, credit card companies want to detect possible fraudulent transactions
    as quickly as possible – ideally during the transaction process, in which case
    they could prevent the transaction from processing completely, if possible. It
    would not be as useful for them to check for fraudulent transactions at some arbitrary
    point later.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that online inference requests usually require a response to be
    returned as quickly as possible, factors such as ensuring low latency, handling
    high request volumes, and providing a reliable, always-available service are some
    of the main challenges in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, when we refer to low latency, we mean that the prediction response
    time is usually in the order of milliseconds or, at the very most, seconds. The
    actual response time requirements will depend on the business use case. For example,
    some users may accept needing to wait for a few seconds for their credit card
    transaction to be approved or rejected, but another example of real-time inference
    is the use of ML models in self-driving cars, and in that scenario, for example,
    a car must be able to respond to its environment in milliseconds if it needs to
    suddenly take some kind of action, such as avoiding an unexpected obstacle that
    comes into its path. *Figure 10**.1* shows an example of a batch prediction workflow
    that we will implement in the practical exercises in this chapter. We will explain
    each of the components that are depicted in detail. In *Figure 10**.1*, the solid
    lines represent steps we will explicitly perform in the practical exercises, whereas
    the dotted lines represent the steps that Vertex AI will perform automatically
    on our behalf:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1: Online prediction](img/B18143_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Online prediction'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps outlined in *Figure 10**.1* are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model in our notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the resulting model artifacts to Google Cloud Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the model details in the Vertex AI Model Registry (explained in more
    detail in this chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an endpoint to host and serve our model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Online Prediction service (explained in more detail in this chapter)
    fetches our model’s details from the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Online Prediction service fetches our saved model from Google
    Cloud Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We send a prediction request to our model and receive a response. In this case,
    we are sending the request from our Vertex AI Workbench notebook, but it’s important
    to note that when our model is hosted on an endpoint, prediction requests could
    be sent from any client application that can access that endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Online Prediction service saves the prediction inputs, outputs,
    and other details to Google Cloud BigQuery. This is an optional feature that we
    can enable so that we can perform analytical queries on the inputs, outputs, and
    other details related to our model’s predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It should also be noted that in the case of online model serving, predictions
    are generally made on demand, and usually for a single instance or a small batch
    of instances. In this case, your model and its serving infrastructure need to
    be able to quickly react to sudden – and possibly unexpected – changes in inference
    traffic volume.
  prefs: []
  type: TYPE_NORMAL
- en: Offline/batch model serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given our description of online model serving, it may have become obvious that
    offline serving means that no client is waiting for an immediate response in real
    time. In fact, rather than our model receiving on-demand inference requests from
    individual clients, we can feed many input observations into our model in large
    batches, which it can process over longer periods; perhaps hours or even days,
    depending on the business case. The predictions produced by our models can then
    be stored and used later, rather than being acted on immediately. Examples of
    batch inference use cases include predicting the next day’s stock prices or sending
    out targeted emails to users based on their predicted preferences. *Figure 10**.2*
    shows an example of a batch prediction workflow that we will implement in the
    practical exercises in this chapter. In *Figure 10**.2*, the solid lines represent
    steps we will explicitly perform in the practical exercises, whereas the dotted
    lines represent the steps that Vertex AI will perform automatically on our behalf:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2: Batch prediction](img/B18143_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Batch prediction'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps outlined in *Figure 10**.2* are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the model in our notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the resulting model artifacts to Google Cloud Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the model details in the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the test data in Google Cloud Storage. This will be used as the input data
    in our batch prediction job later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a batch prediction job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Batch Prediction service (explained in more detail in this chapter)
    fetches our model’s details from the Vertex AI Model Registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Batch Prediction service fetches our saved model from Google Cloud
    Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Batch Prediction service fetches our input data from Google Cloud
    Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Batch Prediction runs a batch prediction job, using our model
    and input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Vertex AI Batch Prediction service saves the prediction outputs to Google
    Cloud Storage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rather than being optimized for low latency, batch prediction systems are generally
    optimized to handle a large number of instances at once (that is, high throughput),
    and therefore usually fall into the category of large-scale distributed computing
    use cases, which can benefit from parallelized execution, and can be scheduled
    to automatically execute periodically (for example, once a day), or can be triggered
    by an event (for example, when a new batch of data is available).
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the decision between online and offline serving
    is not always strictly binary, and you may find that a combination of online and
    offline serving best meets your needs. For example, you may use offline serving
    to generate large-scale reports, while also using online serving to make real-time
    predictions in user-facing applications. In either case, both online and offline
    serving require the model to be deployed in a serving infrastructure. This infrastructure
    is responsible for loading the model, receiving prediction requests, making predictions
    using the model, and returning the predictions. In Google Cloud, there are various
    tools and platforms available to assist with this, such as TensorFlow Serving,
    and the Vertex AI prediction service, which we will discuss in more detail in
    subsequent sections in this chapter. First, however, let’s introduce another tool
    that is important for managing our models as part of our end-to-end model development
    life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Earlier in this book, we made an analogy between the traditional **software
    development life cycle** (**SDLC**) and the MDLC. To dive into this analogy in
    more depth, let’s consider some important tools in the SDLC process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shared repositories**: As a part of the SDLC process, we usually want to
    store our code and related artifacts in registries or repositories that can be
    accessed by multiple contributors who need to collaborate on a specific development
    project. Such repositories often include metadata that helps describe certain
    aspects of the code assets so that people can easily understand how those assets
    were developed, as well as how they are used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version control**: As contributors make changes to code assets in a given
    project, we want to ensure that we are tracking such changes and contributions
    and that all contributors can easily access that information. This also enables
    us to roll back to previous versions if we notice issues in a newly deployed version
    of our software.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experience has taught us that similar tools are required when we want to efficiently
    deploy and manage ML models, especially when doing so at a large scale (remember
    that some companies may have thousands of ML models, owned by hundreds of different
    teams).
  prefs: []
  type: TYPE_NORMAL
- en: Also, bear in mind that data science projects are often highly experimental,
    in which the data science teams may try out lots of different algorithms, datasets,
    and hyperparameter values, training lots of different models to see which options
    produce the best results. This is especially true in the early stages of a data
    science project, but this also often holds true on an ongoing basis, whereby the
    data scientists constantly strive to improve the models even after they have been
    deployed to a production environment. They do this to keep abreast of emerging
    trends in the industry and produce better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Cloud’s Vertex AI platform includes a Model Registry service that allows
    us to manage our ML models in a centralized place, making it much easier for us
    to track how models are developed, even when multiple teams are contributing to
    the development of those models. Let’s take a look at some of the Model Registry’s
    important features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model versioning**: The Model Registry allows us to create multiple versions
    of a model, where each version can correspond to a different set of training parameters
    or a different set of training data. This helps us keep track of different experiments
    or deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model metadata**: For each model, we can record metadata such as the model’s
    description, the input and output schemas, the labels (useful for categorization),
    and the metrics (useful for comparing models). For each version, we can record
    additional metadata such as the description, the runtime version (corresponding
    to the version of the Vertex AI platform services), the Python version, the machine
    type used for serving, and the serving settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model artifacts**: These are the artifacts that are used to produce the model.
    They can be stored in Google Cloud Storage and linked to the model in the registry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access control**: We can control who can view, edit, and deploy models in
    the registry through Google Cloud’s **Identity and Access Management** (**IAM**)
    system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s also important to understand that the Model Registry is well-integrated
    with other Vertex AI components. For example, we can use the Vertex AI training
    service to train a model and then automatically upload the trained model to the
    registry, after which we can deploy the model to the Google Cloud Vertex AI prediction
    service, which we’ll describe next. We can compare model versions against each
    other and easily change which versions are deployed to production. We can also
    automate all of those steps using Vertex AI Pipelines, something we’ll explore
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI prediction service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Google Cloud Vertex AI prediction service is an offering within the Vertex
    AI ecosystem that makes it easy for us to host ML models and serve them to our
    clients, thus supporting both batch and online model-serving use cases. It’s a
    managed service, so when we use it to host our models, we don’t need to worry
    about managing the servers and infrastructure required to do so; the service will
    automatically scale the required infrastructure and computing resources up and
    down based on the amount of traffic being sent to our models.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in the previous section, it integrates with Vertex AI Model
    Registry so that we can easily control which versions of our models are deployed
    to production, and it also integrates with many other Google Cloud services, such
    as Vertex AI Pipelines, which allows us to automate the development and deployment
    of our models, and Google Cloud Operations Suite, which provides integrated logging
    and monitoring functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform hands-on activities in which we will use the Vertex AI Model
    Registry and the Vertex AI prediction service to store and serve models in Google
    Cloud shortly, but first, let’s cover one more concept that’s important in the
    context of model deployment and management: **A/B testing**.'
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A/B testing is the practice of testing one model (model A) against another model
    (model B) to see which one performs better. While the term could technically apply
    to testing and comparing any models, the usual scenario is to test a new version
    of a model to improve model performance concerning the business objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vertex AI allows us to deploy more than one model to a single endpoint, as
    well as control the amount of traffic that is served by each model by using the
    `traffic_split` variable, as shown in *Figure 10**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3: A/B configuration using Vertex AI’s traffic_split](img/B18143_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: A/B configuration using Vertex AI’s traffic_split'
  prefs: []
  type: TYPE_NORMAL
- en: As you will see in the practical exercises in this chapter, if we don’t set
    any value for the `traffic_split` variable, the default behavior is to keep all
    traffic directed to the original model that was already deployed to our endpoint.
    This is a safety mechanism that prevents unexpected behavior in terms of how our
    models serve traffic from our clients. The `traffic_split` configuration gives
    us very granular control over how much traffic we want to send to each deployed
    model or model version. For example, we could set the `traffic_split` configuration
    so that it suddenly starts sending all traffic to our new model by allocating
    100% of the traffic to that model, which would effectively perform an in-place
    replacement of our model. However, we may want to test our new model version with
    a small subset of our production traffic before completely replacing our prior
    model version, which equates to the idea of **canary testing** in software development.
  prefs: []
  type: TYPE_NORMAL
- en: When we determine that our new model is behaving as intended, we can gradually
    (or suddenly, depending on the business requirements) change the `traffic_split`
    variable to send more (or all) traffic to the new model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the important concepts related to hosting and
    serving models in production, let’s see how this works in the real world by deploying
    a model using Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve prepared a Vertex AI Workbench notebook that will walk you through all
    of the steps required to do this. Again, we can use the same Vertex AI Workbench
    managed notebook instance that we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left-hand side of the screen, navigate to the `Chapter-10` directory
    and open the `deployment-prediction.ipynb` notebook. You can choose TensorFlow
    2 (Local) as the kernel. Again, you can run each cell in the notebook by selecting
    the cell and pressing *Shift* + *Enter* on your keyboard. In addition to the relevant
    code, the notebook contains markdown text that describes what the code is doing.
    I recommend only executing the model training and deployment sections, and A/B
    testing, and then reading through some more of the topics in this chapter before
    proceeding to the other activities in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: We must also enable a feature named `prediction-request-response-logging` in
    the notebook, which will log our models’ responses for the prediction requests
    that are received. We can save those responses in a Google Cloud BigQuery table,
    which enables us to perform analysis on the prediction responses from each of
    our models and see how they are performing.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have completed the model training and deployment sections of the notebook
    (or if you’d like to just keep reading for now), you can move on to the next section,
    where we will discuss the kinds of challenges that companies usually run into
    when deploying, serving, and managing models in production.
  prefs: []
  type: TYPE_NORMAL
- en: Common challenges of serving models in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying and hosting ML models in production often comes with numerous challenges.
    If you’re developing and serving just one model, you may encounter some of these
    challenges, but if you are developing tens, hundreds, or thousands of models,
    then you will likely run into the majority of these challenges and concerns.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing the right infrastructure to host ML models, setting it up, and managing
    it can be complex, particularly in hybrid or multi-cloud environments. Again,
    Google Cloud Vertex AI takes care of all of this for us automatically, but without
    such cloud offerings, many companies find this to be perhaps one of the most challenging
    aspects of any data science project.
  prefs: []
  type: TYPE_NORMAL
- en: Model availability and scaling in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an extension of deployment infrastructure management. As demand increases,
    our model needs to serve more predictions. The ability to scale services up and
    down based on demand is crucial and can be difficult to manage manually. The Vertex
    AI Autoscaling feature enables us to do this easily. All we have to do is specify
    the minimum and maximum number of machines that we want to run for each model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we know that we always need at least three nodes running to
    handle our regularly expected traffic, but we sometimes get increased levels of
    traffic up to double the normal amount, we could specify that we want Vertex AI
    to always run at least three machines, and to automatically scale up to a maximum
    of six machines when needed.
  prefs: []
  type: TYPE_NORMAL
- en: We can specify our autoscaling preferences by configuring the `minReplicaCount`
    and `maxReplicaCount` variables, which are elements of the machine specification
    for our deployed model. We’ve provided steps on how to do this in the Jupyter
    Notebook that is associated with this chapter. Note that we can specify these
    details per model, not just per endpoint. This enables us to scale each of our
    models independently, which gives us flexibility, depending on our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI also gives us the flexibility to either deploy multiple models to
    the same endpoint or to create separate, dedicated endpoints for each model. If
    we have completely different types of models for different use cases, then we
    would typically deploy those models to separate endpoints. At this point, you
    might be wondering why we would ever want to deploy multiple models to a single
    endpoint. The most common reason for doing this is when we want to implement A/B
    testing. Again, we’ve covered the steps for implementing A/B testing use cases
    in the Jupyter Notebook that accompanies this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, the data that’s used in a production environment may contain errors or
    may not be as clean as the data used for model training, which could lead to inaccurate
    predictions. As a result, we may need to implement data processing steps in production
    that prepare and clean up the data “on-the-fly” at prediction time. In general,
    any data transformation techniques that we may have applied when preparing the
    data for training also need to be applied at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Model/data/concept drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to preparing and cleaning up our data, if you think back to some
    of the data exploration activities we performed in previous chapters of this book,
    you may remember that one important aspect of our data is the statistical distribution
    of the variables in our dataset. Examples include the mean value of each variable
    in the dataset, the observed range between the minimum and maximum values of each
    variable, or the kinds of values of each variable, such as discreet or continuous.
    Collectively, we can refer to these characteristics as the “shape” of our data.
    *Figure 10**.4* shows some examples of different types of data distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4: Data distributions  (source: https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)](img/B18143_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Data distributions (source: https://commons.wikimedia.org/wiki/File:Cauchy_pdf.svg)'
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, the shape of the data that is used to train our model should
    be the same as the shape of the data that the model is expected to encounter in
    production. For this reason, we usually want to use real-world data (that is,
    data that was previously observed in production) to train our models. However,
    over time, the shape of the data that’s observed in production might begin to
    deviate from the shape of the data that was used to train the model. This can
    happen suddenly, such as when a global pandemic drastically changes the purchasing
    behavior of consumers within a short period (for example, everybody is suddenly
    purchasing toilet paper and hand sanitizer, rather than fashion and cosmetic products),
    or seasonally, or it could occur more gradually due to natural evolutionary changes
    in a given business domain or market.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, such a deviation is generally referred to as “drift,” or more specifically
    **model drift**, **data drift**, or **concept drift**. Model drift refers to the
    overall degradation of the model’s objective performance over time, and it can
    be caused by factors such as data drift or concept drift. Data drift is when the
    statistical distribution of the data in production differs from the statistical
    distribution of the data that was used to train our model. On the other hand,
    concept drift is when the relationships change between the input features and
    the target variable that a model is trying to predict. This means that even if
    the input data stays the same, the underlying relationships between the variables
    may change over time, which can affect the accuracy of our model. The example
    of consumer behavior changing during a pandemic is an instance of concept drift.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the type of drift, it can lead to degrading model performance.
    As such, we need to constantly evaluate the performance of our models to ensure
    they consistently meet our business requirements. If we find that the performance
    of our models is degrading, we need to assess whether it is due to drift, and
    if we detect that it is, we need to take corrective measures accordingly. We will
    discuss such measures in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: Security and privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring that the data used by our model complies with security and privacy
    regulations can be complex, particularly in industries dealing with sensitive
    data. Depending on the industry and use case, this can be considered one of the
    most important aspects of model development and management.
  prefs: []
  type: TYPE_NORMAL
- en: Model interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, models are treated as “black box” implementations, in which case we don’t
    have much visibility into how the model is operating internally, making it difficult
    to understand how they’re making predictions. This can cause issues, especially
    in regulated industries where explanations are required. We will explore this
    topic in a lot more detail later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking ML model metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, ML model development usually involves some experimentation,
    in which data scientists evaluate different versions of their datasets, as well
    as different algorithms, hyperparameter values, and other components of the development
    process. Also, the development process often involves collaboration among multiple
    team members or multiple separate teams. Considering that some companies develop
    and deploy hundreds or even thousands of ML models, it’s important to track all
    of these experiments and development iterations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we have a model deployed in production, and we observe that
    the model is making inaccurate or inappropriate predictions, then we need to understand
    why that model is behaving as it is. To do that, we need to know all of the steps
    that were performed to create that specific model version, as well as all of the
    inputs and outputs associated with the development of that model, such as the
    version of the input dataset that was used to train the model, as well as the
    algorithm and hyperparameter values used during the training process. We refer
    to this information as the metadata of the model. Without this, it would be very
    difficult to understand why our model behaves the way it does.
  prefs: []
  type: TYPE_NORMAL
- en: This concept is somewhat linked to the topic of model interpretability but also
    includes additional aspects of ML model development.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with existing systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most companies have various software systems that were developed or procured
    over many years, and integrating the ML model into these systems can be complex.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Setting up robust monitoring for the performance of ML models in production
    can be a challenge if you manage the infrastructure yourself. We need to constantly
    monitor the model’s predictive performance to ensure it is still valid and hasn’t
    degraded over time.
  prefs: []
  type: TYPE_NORMAL
- en: This is such an important aspect of model management that we will dedicate the
    next section of this chapter specifically to this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring models in production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our work isn’t over once we’ve developed and deployed a model – we need to track
    the model’s performance and its overall health over time and make adjustments
    if we observe that the model performance deteriorates. In this section, we’ll
    discuss some of the characteristics of our models that we typically need to monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Objective model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not surprisingly, the most prominent aspect of our model that we need to monitor
    is how it performs concerning the objective it was created to achieve. We discussed
    objective metrics in previous chapters of this book, such as **Mean Squared Error**
    (**MSE**), Accuracy, F1 score, and AUC-ROC, among others. An example of AUC-ROC
    is depicted in *Figure 10**.5* for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5: AUC-ROC](img/B18143_10_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: AUC-ROC'
  prefs: []
  type: TYPE_NORMAL
- en: Objective metrics tell us how our model is performing in terms of the main purpose
    our model is intended to serve, such as predicting housing prices or identifying
    cats in photographs. If we notice a sustained degradation in these metrics beyond
    a certain threshold that we deem acceptable to the needs of the business, then
    we usually need to take corrective action.
  prefs: []
  type: TYPE_NORMAL
- en: A common cause of degradation of model performance is data drift, which we discussed
    in the previous section. If we find that data drift has occurred, a corrective
    action is often to retrain our model with updated data that matches the shape
    or distribution of the data that is currently being observed by the model in production.
    This may require gathering fresh data from our production systems or other appropriate
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring specifically for data drift
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Monitoring for data drift involves comparing the statistical properties of the
    incoming data with those of the training data. In this case, we analyze each feature
    or variable in the data, and we compare the distribution of the incoming data
    with the distribution of the training data. In addition to comparing simple descriptive
    statistics such as the mean, median, mode, and standard deviation, there are also
    some specific statistical tests we can evaluate, such as the Kullback-Leibler
    divergence, the Kolmogorov-Smirnov test, or the chi-squared test. We will discuss
    these mechanisms in more detail later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Monitoring provides automated tests that can detect different
    varieties of drift. Specifically, it can check for **feature skew and drift**
    and **attribution skew and drift**, both of which we’ll describe next. It also
    provides model explanation functionality, which we’ll explore in great detail
    later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Skew is also called **training-serving skew**, and it refers to a scenario in
    which the distribution of feature data in production differs from the distribution
    of feature data that was used to train the model. To detect this type of skew,
    we generally need to have access to the original training data since Vertex AI
    Model Monitoring will compare the distribution of the training data against what
    is seen in the inference requests that are sent to our model in production.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction drift** refers to the scenario in which the feature data in production
    changes over time. In the absence of access to the original training data, we
    can still turn on drift detection to check the input data for changes over time.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that some amount of drift and skew are likely tolerable,
    but we generally need to determine and configure thresholds beyond which we need
    to take corrective actions. These thresholds depend on the needs of the business
    for our particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: Anomalous model behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anomalies could be spikes in prediction requests, unusual values of input features,
    or unusual prediction response values. For example, if a model that is built for
    fraud detection starts flagging an unusually high number of transactions as fraudulent,
    it may be an anomaly and warrant further investigation. Anomalies may also occur
    due to data drift, or due to temporary changes in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Resource utilization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This includes monitoring aspects such as CPU usage, memory usage, network I/O,
    and others to ensure that the model’s serving infrastructure is operating within
    its capacity. These metrics are important indicators for determining when to scale
    resources up or down based on factors such as the amount of traffic currently
    being sent to our models.
  prefs: []
  type: TYPE_NORMAL
- en: Model bias and fairness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to ensure that our models are making fair predictions on an ongoing
    basis. This can involve tracking fairness metrics and checking for bias in our
    models’ predictions. We will explore this topic in much more detail in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: These are some of the main aspects of our models that we need to monitor. The
    exact items that need to be prioritized depend on our business case.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve talked about many of the most common topics in the space of model
    monitoring, let’s discuss what to do if we detect that our model’s performance
    is degrading.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing model performance degradation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we find that the values of the metrics we’re monitoring for our models are
    showing a decline in performance, there are some actions we can take to correct
    the situation, and possibly prevent such a scenario from occurring in the future.
    This section discusses some relevant corrective actions.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most effective way to ensure that our models stay up to date and
    are performing optimally is to implement a robust MLOps pipeline. This includes
    **continuous integration and continuous deployment** (**CI/CD**) for ML models,
    and a major component of this is to regularly train our models on new data that
    is seen in production – this is referred to as **continuous training**. For this
    purpose, we need to implement a mechanism for capturing data in production, and
    then automatically update our models.
  prefs: []
  type: TYPE_NORMAL
- en: We could either update our models periodically (for example, every night or
    every month), or we could trigger retraining to occur based on outputs from a
    Vertex AI Model Monitoring job. For example, if a Model Monitoring job detects
    that drift has occurred beyond an acceptable threshold, an automated notification
    could be sent, and it could automatically kick off a pipeline to train and deploy
    a new version of our model.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is dedicated entirely to the topic of MLOps, so we will dive
    into these concepts in much more detail there.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, if you’d like to switch from theory to practical learning,
    now would be a good time to execute the *Model Monitoring* section of the Vertex
    AI Workbench notebook that accompanies this chapter. Otherwise, let’s continue
    with the rest of the topics in this chapter, which include optimizing for AI/ML
    use cases at the edge.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing for AI/ML at the edge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serving ML models at the edge refers to running your models directly on user
    devices such as smartphones or IoT devices. The term “edge” is based on traditional
    network architecture terminology, in which the core of the network is in the network
    owner’s data centers, and the edge of the network is where user devices connect
    to the network. Running models and other types of systems at the edge can provide
    benefits such as lower latency, increased privacy, and reduced server costs. However,
    edge devices usually have limited computing power, so we may need to make some
    changes to our models for them to run efficiently on those devices. There are
    several things we can do to optimize our models to run at the edge, all of which
    we will discuss in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Model optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by discussing what kinds of measures we can take to optimize our
    models so that they can be used at the edge.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we should try to choose lightweight models that can still provide good
    performance on our objectives. For example, decision trees and linear models often
    require less memory and computational power than deep learning models. Of course,
    our model selection process also depends on our business needs. Sometimes, we
    will need larger models to achieve the required objective. As such, this recommendation
    for optimizing AI/ML workloads at the edge is simply a first-step guideline. We
    will discuss more advanced strategies next.
  prefs: []
  type: TYPE_NORMAL
- en: Model pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Pruning is a technique for reducing the size of a model by removing parameters
    (for example, “weight pruning”) or whole neurons (for example, “neuron pruning”)
    that contribute the least to the model’s performance. An example of neuron pruning
    is depicted in *Figure 10**.6*, in which a neuron has been removed from each hidden
    layer (as represented by the red X covering each of the removed neurons):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6: Neuron pruning](img/B18143_10_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Neuron pruning'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting pruned model requires less memory and computational resources.
    If we remove too many weights or neurons, then it could affect the accuracy of
    our model, so the idea, of course, is to find a balance that reduces the computational
    resources required, with minimal impact on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Model quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantization is a way of reducing the numerical precision of the model’s weights.
    For example, weights might be stored as 32-bit floating-point numbers during training,
    but they can often be quantized as 8-bit integers for inference without a significant
    reduction in performance. This reduces the memory requirements and computational
    cost of the model. This is particularly useful in the context of **Large Language
    Models** (**LLMs**), which can have hundreds of billions of weights. We will cover
    this in detail in the *Generative AI* section of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This technique involves training a smaller model, sometimes referred to as a
    “student” model, to mimic the behavior of a larger, “teacher” model, or ensemble
    of models. The smaller model is trained to produce outputs as similar as possible
    to those of the larger model so that it can perform a somewhat similar task, with
    perhaps a tolerable reduction in model accuracy. Again, we need to find a balance
    in the trade-off between model size reduction and reduction in accuracy. Distillation
    is also particularly useful in the context of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Making use of efficient model architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some model architectures are designed to be efficient on edge devices. For example,
    MobileNet and EfficientNet are efficient variants of **convolutional neural networks**
    (**CNNs**) that are suitable for mobile devices.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed some changes that we can make to our models to optimize
    them for edge use cases, let’s take a look at what other kinds of mechanisms we
    can use for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization beyond model techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of the previous optimization techniques involved making changes to our models
    to make them run more efficiently on edge devices. There are also additional measures
    we can take, such as converting trained models into other formats that are optimized
    for edge devices or optimizing the hardware that runs our models at the edge.
    Let’s discuss these mechanisms in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware-specific optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on the specific hardware on the edge device (for example, CPU, GPU,
    **tensor processing unit** (**TPU**), and so on), different optimization strategies
    can be used. For example, some libraries provide tools for optimizing computation
    graphs based on specific hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Specialized libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Libraries such as TensorFlow Lite and the **Open Neural Network Exchange** (**ONNX**)
    Runtime can convert models into a format optimized for edge devices, further reducing
    the memory footprint and increasing the speed of models. We’ll discuss these libraries
    in more detail in this section.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Lite
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TensorFlow Lite is a set of tools provided by TensorFlow to help us run models
    on mobile, embedded, and IoT devices. It does this by converting our TensorFlow
    models into a more efficient format for use on such edge devices. It also includes
    tools for optimizing the model’s size and performance, as well as for implementing
    hardware acceleration. We’ve used TensorFlow Lite to convert our model; this can
    be found in the Jupyter Notebook that accompanies this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Edge TPU Compiler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Google’s Edge TPU Compiler is a tool for compiling models to run on Google’s
    Edge TPUs, which are designed for running TensorFlow Lite models on edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX Runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ONNX is an open format for representing ML models that enables models to be
    transferred between various ML frameworks. It also provides a cross-platform inference
    engine called the ONNX Runtime, which includes support for various kinds of hardware
    accelerators and is designed to provide fast inference and lower the resource
    requirements of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: TensorRT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: TensorRT is a deep learning model optimizer and runtime library developed by
    NVIDIA for deploying neural network models on GPUs, particularly on NVIDIA’s embedded
    platform called Jetson.
  prefs: []
  type: TYPE_NORMAL
- en: TVM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apache TVM is an open source ML compiler stack that aims to enable efficient
    deployment of ML models on a variety of hardware platforms. TVM supports model
    inputs from various deep learning frameworks, including TensorFlow, Keras, PyTorch,
    ONNX, and others.
  prefs: []
  type: TYPE_NORMAL
- en: These are just some of the tools that exist for optimizing ML models so that
    they can run at the edge. With the constant proliferation of new types of technological
    devices, edge optimization is an active area of research, and new tools and mechanisms
    continue to be developed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our hands-on activities in the Jupyter Notebook that accompanies this chapter,
    we used TensorFlow Lite to optimize our model, after which we stored the optimized
    model in Google Cloud Storage. From there, we can easily deploy our model to any
    device that supports the TensorFlow Lite interpreter. A list of supported platforms
    is provided in the TensorFlow Lite documentation, which also contains a lot of
    additional useful information on how TensorFlow Lite works in greater detail:
    [https://www.tensorflow.org/lite/guide/inference#supported_platforms](https://www.tensorflow.org/lite/guide/inference#supported_platforms).'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we’ve covered a lot of different topics related to model deployment.
    Let’s take some time to recap what we’ve learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed various Google Cloud services for hosting ML models,
    such as Vertex AI, Cloud Functions, GKE, and Cloud Run. We differentiated between
    online and offline model serving, whereby online serving is used for real-time
    predictions, and offline serving is used for batch predictions. Then, we explored
    common challenges in deploying ML models, such as data/model drift, scaling, monitoring,
    performance, and keeping our models up to date. We also introduced specific components
    of Vertex AI that make it easier for us to deploy and manage models, such as the
    Vertex AI Model Registry, the Vertex AI prediction service, and Vertex AI Model
    Monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we dived quite deep into monitoring models in production, focusing
    on data drift and model drift. We discussed mechanisms to combat these drifts,
    such as automated continuous training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explained A/B testing for comparing two versions of a model, and we
    discussed optimizing ML models for edge deployment using methods such as model
    pruning and quantization, as well as libraries and tools for optimizing our models,
    such as TensorFlow Lite.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have covered all the major steps in the MDLC. Our next topic
    of focus will be how to automate the entire life cycle using MLOps. Join us in
    the next chapter, where you’ll continue your journey of becoming an expert AI/ML
    solutions architect, in which you have already come a very long way and are making
    great progress!
  prefs: []
  type: TYPE_NORMAL
