- en: Introduction to Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: Natural language processing is a set of machine learning techniques that allows
    working with text documents, considering their internal structure and the distribution
    of words. In this chapter, we're going to discuss all common methods to collect
    texts, split them into atoms, and transform them into numerical vectors. In particular,
    we'll compare different methods to tokenize documents (separate each word), to
    filter them, to apply special transformations to avoid inflected or conjugated
    forms, and finally to build a common vocabulary. Using the vocabulary, it will
    be possible to apply different vectorization approaches to build feature vectors
    that can easily be used for classification or clustering purposes. To show how
    to implement the whole pipeline, at the end of the chapter, we're going to set
    up a simple classifier for news lines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理是一组机器学习技术，允许处理文本文档，考虑其内部结构和单词的分布。在本章中，我们将讨论收集文本、将它们拆分为原子并转换为数值向量的所有常用方法。特别是，我们将比较不同的方法来分词文档（分离每个单词）、过滤它们、应用特殊转换以避免屈折或动词变位形式，并最终构建一个共同词汇。使用词汇，将能够应用不同的向量化方法来构建特征向量，这些向量可以轻松用于分类或聚类目的。为了展示如何实现整个管道，在本章末尾，我们将设置一个简单的新闻行分类器。
- en: NLTK and built-in corpora
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLTK和内置语料库
- en: '**Natural Language Toolkit** (**NLTK**) is a very powerful Python framework
    that implements most NLP algorithms and will be adopted in this chapter together
    with scikit-learn. Moreover, NLTK provides some built-in corpora that can be used
    to test algorithms. Before starting to work with NLTK, it''s normally necessary
    to download all the additional elements (corpora, dictionaries, and so on) using
    a specific graphical interface. This can be done in the following way:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**）是一个非常强大的Python框架，实现了大多数NLP算法，并将与scikit-learn一起在本章中使用。此外，NLTK提供了一些内置的语料库，可用于测试算法。在开始使用NLTK之前，通常需要使用特定的图形界面下载所有附加元素（语料库、词典等）。这可以通过以下方式完成：'
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This command will launch the user interface, as shown in the following figure:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将启动用户界面，如图所示：
- en: '![](img/b9adf74f-9ab4-4b3f-856e-7aeae1f3c586.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9adf74f-9ab4-4b3f-856e-7aeae1f3c586.png)'
- en: It's possible to select every single feature or download all elements (I suggest
    this option if you have enough free space) to immediately exploit all NLTK functionalities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可以选择每个单独的特征或下载所有元素（如果您有足够的空闲空间，我建议选择此选项）以立即利用所有NLTK功能。
- en: NLTK can be installed using pip (`pip install -U nltk`) or with one of the binary
    distributions available at [http://www.nltk.org](http://www.nltk.org). On the
    same website, there's complete documentation that can be useful for going deeper
    into each topic.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK可以使用pip（`pip install -U nltk`）或通过[http://www.nltk.org](http://www.nltk.org)上可用的二进制分发安装。在同一网站上，有完整的文档，对于深入了解每个主题非常有用。
- en: Corpora examples
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库示例
- en: 'A subset of the Gutenberg project is provided and can be freely accessed in
    this way:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 格鲁吉亚项目的一个子集被提供，并且可以通过这种方式免费访问：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A single document can be accessed as a raw version or split into sentences
    or words:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 单个文档可以以原始版本访问或拆分为句子或单词：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we''re going to discuss, in many cases, it can be useful to have the raw
    text so as to split it into words using a custom strategy. In many other situations,
    accessing sentences directly allows working with the original structural subdivision.
    Other corpora include web texts, Reuters news lines, the Brown corpus, and many
    more. For example, the Brown corpus is a famous collection of documents divided
    by genre:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将要讨论的，在许多情况下，拥有原始文本以便使用自定义策略将其拆分为单词是有用的。在许多其他情况下，直接访问句子允许使用原始的结构性细分。其他语料库包括网络文本、路透社新闻行、布朗语料库以及许多更多。例如，布朗语料库是一个按体裁划分的著名文档集合：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Further information about corpora can be found at [http://www.nltk.org/book/ch02.html](http://www.nltk.org/book/ch02.html).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于语料库的更多信息可以在[http://www.nltk.org/book/ch02.html](http://www.nltk.org/book/ch02.html)找到。
- en: The bag-of-words strategy
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋策略
- en: 'In NLP, a very common pipeline can be subdivided into the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，一个非常常见的管道可以细分为以下步骤：
- en: Collecting a document into a corpus.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文档收集到语料库中。
- en: Tokenizing, stopword (articles, prepositions and so on) removal, and stemming
    (reduction to radix-form).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分词、去除停用词（冠词、介词等）和词干提取（还原到词根形式）。
- en: Building a common vocabulary.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建共同词汇。
- en: Vectorizing the documents.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量化文档。
- en: Classifying or clustering the documents.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文档进行分类或聚类。
- en: The pipeline is called **bag-of-words** and will be discussed in this chapter.
    A fundamental assumption is that the order of each single word in a sentence is
    not important. In fact, when defining a feature vector, as we're going to see,
    the measures taken into account are always related to frequencies and therefore
    they are insensitive to the local positioning of all elements. From some viewpoints,
    this is a limitation because in a natural language the internal order of a sentence
    is necessary to preserve the meaning; however, there are many models that can
    work efficiently with texts without the complication of local sorting. When it's
    absolutely necessary to consider small sequences, it will be done by adopting
    groups of tokens (called n-grams) but considering them as a single atomic element
    during the vectorization step.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道被称为**词袋模型**，将在本章中讨论。一个基本假设是句子中每个单词的顺序并不重要。实际上，当我们定义特征向量时，我们将要看到，所采取的措施总是与频率相关，因此它们对所有元素的局部位置不敏感。从某些观点来看，这是一个限制，因为在自然语言中，句子的内部顺序对于保留意义是必要的；然而，有许多模型可以在不涉及局部排序的复杂性的情况下有效地处理文本。当绝对有必要考虑小序列时，将通过采用标记组（称为n-gram）来实现，但在向量化步骤中将它们视为单个原子元素。
- en: 'In the following figure, there''s a schematic representation of this process
    (without the fifth step) for a sample document (sentence):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图例中，有一个该过程的示意图（不包括第五步）对于一个示例文档（句子）：
- en: '![](img/7a3fb850-b3c2-48b1-9d30-21db8e60009a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7a3fb850-b3c2-48b1-9d30-21db8e60009a.png)'
- en: 'There are many different methods used to carry out each step and some of them
    are context-specific. However, the goal is always the same: maximizing the information
    of a document and reducing the size of the common vocabulary by removing terms
    that are too frequent or derived from the same radix (such as verbs). The information
    content of a document is in fact determined by the presence of specific terms
    (or group of terms) whose frequency in the corpus is limited. In the example shown
    in the previous figure, **fox** and **dog** are important terms, while **the**
    is useless (often called a **stopword**). Moreover, **jumps** can be converted
    to the standard form **jump**, which expresses a specific action when present
    in different forms (like jumping or jumped). The last step is transforming into
    a numerical vector, because our algorithms work with numbers, and it''s important
    to limit the length of the vectors so as to improve the learning speed and the
    memory consumption. In the following sections, we''re going to discuss each step
    in detail, and at the end, we''re going to build a sample classifier for news
    lines.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行每个步骤有许多不同的方法，其中一些是上下文特定的。然而，目标始终相同：通过移除过于频繁或来自同一词根（如动词）的术语来最大化文档的信息量并减少常用词汇表的大小。实际上，文档的信息含量是由在语料库中频率有限的特定术语（或术语组）的存在决定的。在前面图例中显示的例子中，**狐狸**和**狗**是重要术语，而**the**则无用（通常称为**停用词**）。此外，**跳跃**可以转换为标准形式**跳**，当以不同形式出现时（如跳跃或跳过），它表达了一个特定的动作。最后一步是将其转换为数值向量，因为我们的算法处理的是数字，因此限制向量的长度对于提高学习速度和内存消耗非常重要。在接下来的章节中，我们将详细讨论每个步骤，并在最后构建一个用于新闻分类的示例分类器。
- en: Tokenizing
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: The first step in processing a piece of text or a corpus is splitting it into
    atoms (sentences, words, or parts of words), normally defined as **tokens**. Such
    a process is quite simple; however, there can be different strategies to solve
    particular problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本或语料库的第一步是将它们拆分成原子（句子、单词或单词的一部分），通常定义为**标记**。这个过程相当简单；然而，针对特定问题可能会有不同的策略。
- en: Sentence tokenizing
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子标记化
- en: 'In many cases, it''s useful to split large text into sentences, which are normally
    delimited by a full stop or another equivalent mark. As every language has its
    own orthographic rules, NLTK offers a method called `sent_tokenize()`that accepts
    a language (the default is English) and splits the text according to the specific
    rules. In the following example, we show the usage of this function with different
    languages:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，将大文本拆分成句子是有用的，这些句子通常由句号或其他等效标记分隔。由于每种语言都有自己的正字法规则，NLTK提供了一个名为`sent_tokenize()`的方法，它接受一种语言（默认为英语）并根据特定规则拆分文本。在以下示例中，我们展示了该函数在不同语言中的使用：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Word tokenizing
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词标记化
- en: 'The simplest way to tokenize a sentence into words is provided by the class `TreebankWordTokenizer`,
    which, however, has some limitations:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将句子拆分成单词的最简单方法是类`TreebankWordTokenizer`提供的，然而，它也有一些局限性：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see, in the first case the sentence has been correctly split into
    words, keeping the punctuation separate (this is not a real issue because it can
    be removed in a second step). However, in the complex example, the contraction
    `isn''t` has been split into `is` and `n''t`. Unfortunately, without a further
    processing step, it''s not so easy converting a token with a contraction into
    its normal form (like `not` ), therefore, another strategy must be employed. A
    good way to solve the problem of separate punctuation is provided by the class `RegexpTokenizer`,
    which offers a flexible way to split words according to a regular expression:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在第一种情况下，句子已经被正确地拆分成单词，同时保持了标点符号的分离（这不是一个真正的问题，因为可以在第二步中将其删除）。然而，在复杂示例中，缩写词`isn't`被拆分为`is`和`n't`。不幸的是，没有进一步的加工步骤，将带有缩写词的标记转换为正常形式（如`not`）并不那么容易，因此，必须采用另一种策略。通过类`RegexpTokenizer`提供了一种灵活的方式来根据正则表达式拆分单词，这是解决单独标点问题的一个好方法：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Most of the common problems can be easily solved using this class, so I suggest
    you learn how to write simple regular expressions that can match specific patterns.
    For example, we can remove all numbers, commas, and other punctuation marks from
    a sentence:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数常见问题都可以很容易地使用这个类来解决，所以我建议你学习如何编写可以匹配特定模式的简单正则表达式。例如，我们可以从句子中移除所有数字、逗号和其他标点符号：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Even if there are other classes provided by NLTK, they can always be implemented
    using a customized `RegexpTokenizer`, which is powerful enough to solve almost
    every particular problem; so I prefer not to go deeper in this discussion.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使NLTK提供了其他类，它们也总是可以通过自定义的`RegexpTokenizer`来实现，这个类足够强大，可以解决几乎每一个特定问题；因此，我更喜欢不深入这个讨论。
- en: Stopword removal
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词移除
- en: 'Stopwords are part of a normal speech (articles, conjunctions, and so on),
    but their occurrence frequency is very high and they don''t provide any useful
    semantic information. For these reasons, it''s a good practice to filter sentences
    and corpora by removing them all. NLTK provides lists of stopwords for the most
    common languages and their usage is immediate:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是正常言语的一部分（如冠词、连词等），但它们的出现频率非常高，并且不提供任何有用的语义信息。因此，过滤句子和语料库时移除它们是一个好的做法。NLTK提供了最常见语言的停用词列表，并且其使用是直接的：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A subset of English stopwords is shown in the following snippet:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段显示了英语停用词的一个子集：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To filter a sentence, it''s possible to adopt a functional approach:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要过滤一个句子，可以采用一种功能性的方法：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Language detection
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言检测
- en: 'Stopwords, like other important features, are strictly related to a specific
    language, so it''s often necessary to detect the language before moving on to
    any other step. A simple, free, and reliable solution is provided by the `langdetect` library, which
    has been ported from Google''s language detection system. Its usage is immediate:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词，像其他重要特征一样，与特定语言密切相关，因此在进行任何其他步骤之前，通常有必要检测语言。由`langdetect`库提供的一个简单、免费且可靠的解决方案，该库已从谷歌的语言检测系统移植过来。其使用是直接的：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The function returns the ISO 639-1 codes ([https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)),
    which can be used as keys in a dictionary to get the complete language name. Where
    the text is more complex, the detection can more difficult and it''s useful to
    know whether there are any ambiguities. It''s possible to get the probabilities
    for the expected languages through the `detect_langs()` method:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回ISO 639-1代码（[https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)），这些代码可以用作字典中的键来获取完整的语言名称。当文本更复杂时，检测可能更困难，了解是否存在任何歧义是有用的。可以通过`detect_langs()`方法获取预期语言的概率：
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: langdetect can be installed using pip (`pip install --upgrade langdetect`).
    Further information is available at [https://pypi.python.org/pypi/langdetect](https://pypi.python.org/pypi/langdetect).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用pip安装langdetect（`pip install --upgrade langdetect`）。更多信息可在[https://pypi.python.org/pypi/langdetect](https://pypi.python.org/pypi/langdetect)找到。
- en: Stemming
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取
- en: 'Stemming is a process that is used to transform particular words (such as verbs
    or plurals) into their radical form so as to preserve the semantics without increasing
    the number of unique tokens. For example, if we consider the three expressions `I
    run`, `He runs`, and `Running`, they can be reduced into a useful (though grammatically
    incorrect) form: `I run`, `He run`, `Run`. In this way, we have a single token
    that defines the same concept (`run`), which, for clustering or classification
    purposes, can be used without any precision loss. There are many stemmer implementations
    provided by NLTK. The most common (and flexible) is `SnowballStemmer`, based on
    a multilingual algorithm:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一个将特定单词（如动词或复数形式）转换为它们的根形式的过程，以便在不增加唯一标记数量的情况下保留语义。例如，如果我们考虑三个表达式“我跑”、“他跑”和“跑步”，它们可以被简化为一个有用的（尽管语法上不正确）形式：“我跑”、“他跑”、“跑”。这样，我们就有一个定义相同概念（“跑”）的单个标记，在聚类或分类目的上，可以无任何精度损失地使用。NLTK提供了许多词干提取器的实现。最常见（且灵活）的是基于多语言算法的`SnowballStemmer`：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `ignore_stopwords` parameter informs the stemmer not to process the stopwords.
    Other implementations are `PorterStemmer` and `LancasterStemmer`. Very often the
    result is the same, but in some cases, a stemmer can implement more selective
    rules. For example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`ignore_stopwords`参数通知词干提取器不要处理停用词。其他实现包括`PorterStemmer`和`LancasterStemmer`。结果通常相同，但在某些情况下，词干提取器可以实施更选择性的规则。例如：'
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see, Snowball and Porter algorithms keep the word unchanged, while
    Lancaster extracts a radix (which is meaningless). On the other hand, the latter
    algorithm implements many specific English rules, which can really reduce the
    number of unique tokens:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Snowball和Porter算法保持单词不变，而Lancaster算法提取一个根（这在语义上是无意义的）。另一方面，后者算法实现了许多特定的英语规则，这实际上可以减少唯一标记的数量：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Unfortunately, both Porter and Lancaster stemmers are available in NLTK only
    in English; so the default choice is often Snowball, which is available in many
    languages and can be used in conjunction with an appropriate stopword set.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Porter和Lancaster词干提取器在NLTK中仅适用于英语；因此，默认选择通常是Snowball，它在许多语言中都可用，并且可以与适当的停用词集一起使用。
- en: Vectorizing
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化
- en: This is the last step of the bag-of-words pipeline and it is necessary for transforming
    text tokens into numerical vectors. The most common techniques are based on a
    count or frequency computation, and they are both available in scikit-learn with
    sparse matrix representations (this is a choice that can save a lot of space considering
    that many tokens appear only a few times while the vectors must have the same
    length).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是词袋模型管道的最后一步，它是将文本标记转换为数值向量的必要步骤。最常见的技术是基于计数或频率计算，它们都在scikit-learn中可用，以稀疏矩阵表示（考虑到许多标记只出现几次而向量必须有相同的长度，这是一个可以节省大量空间的选择）。
- en: Count vectorizing
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数向量化
- en: 'The algorithm is very simple and it''s based on representing a token considering
    how many times it appears in a document. Of course, the whole corpus must be processed
    in order to determine how many unique tokens are present and their frequencies.
    Let''s see an example of the `CountVectorizer` class on a simple corpus:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法非常简单，它基于考虑一个标记在文档中出现的次数来表示一个标记。当然，整个语料库必须被处理，以确定有多少唯一的标记及其频率。让我们看看`CountVectorizer`类在简单语料库上的一个例子：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, each document has been transformed into a fixed-length vector,
    where 0 means that the corresponding token is not present, while a positive number
    represents the occurrences. If we need to exclude all tokens whose document frequency
    is less than a predefined value, we can set it through the parameter `min_df`
    (the default value is 1). Sometimes it can be useful to avoid terms that are very
    common; however, the next strategy will manage this problem in a more reliable
    and complete way.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每个文档都已转换为固定长度的向量，其中0表示相应的标记不存在，而正数表示出现的次数。如果我们需要排除所有文档频率低于预定义值的标记，我们可以通过参数`min_df`（默认值为1）来设置它。有时避免非常常见的术语可能是有用的；然而，下一个策略将以更可靠和完整的方式解决这个问题。
- en: 'The vocabulary can be accessed through the instance variable `vocabulary_`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表可以通过实例变量`vocabulary_`访问：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Given a generic vector, it''s possible to retrieve the corresponding list of
    tokens with an inverse transformation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个通用向量，可以通过逆变换检索相应的标记列表：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Both this and the following method can also use an external tokenizer (through
    the parameter `tokenizer`), it can be customized using the techniques discussed
    in previous sections:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都可以使用外部分词器（通过参数 `tokenizer`），可以通过前几节中讨论的技术进行自定义：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With our tokenizer (using stopwords and stemming), the vocabulary is shorter
    and so are the vectors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的分词器（使用停用词和词干提取），词汇表更短，向量也更短。
- en: N-grams
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-grams
- en: 'So far, we have considered only single tokens (also called unigrams), but in
    many contexts, it''s useful to consider short sequences of words (bigrams or trigrams)
    as atoms for our classifiers, just like all the other tokens. For example, if
    we are analyzing the sentiment of some texts, it could be a good idea to consider
    bigrams such as `pretty good`, `very bad`, and so on. From a semantic viewpoint,
    in fact, it''s important to consider not just the adverbs but the whole compound
    form. It''s possible to inform our vectorizers about the range of n-grams we want
    to consider. For example, if we need unigrams and bigrams, we can use this snippet:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了单个标记（也称为单语素），但在许多情况下，考虑单词的短序列（双词组或三词组）作为我们的分类器的原子是有用的，就像所有其他标记一样。例如，如果我们正在分析某些文本的情感，考虑双词组如
    `pretty good`、`very bad` 等可能是个好主意。从语义角度来看，实际上，考虑不仅仅是副词，而是整个复合形式很重要。我们可以向向量器告知我们想要考虑的
    n-grams 范围。例如，如果我们需要单语素和双语素，我们可以使用以下代码片段：
- en: '[PRE20]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, the vocabulary now contains the bigrams, and the vectors include
    their relative frequencies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，词汇表现在包含了双词组，并且向量包括了它们的相对频率。
- en: Tf-idf vectorizing
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tf-idf 向量化
- en: 'The most common limitation of count vectorizing is that the algorithm doesn''t
    consider the whole corpus while considering the frequency of each token. The goal
    of vectorizing is normally preparing the data for a classifier; therefore it''s
    necessary to avoid features that are present very often, because their information
    decreases when the number of global occurrences increases. For example, in a corpus
    about a sport, the word `match` could be present in a huge number of documents;
    therefore it''s almost useless as a classification feature. To address this issue,
    we need a different approach. If we have a corpus `C` with `n` documents, we define
    **term-frequency**, the number of times a token occurs in a document, as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 计数向量化的最常见限制是，算法在考虑每个标记的频率时没有考虑整个语料库。向量化的目标通常是准备数据供分类器使用；因此，避免非常常见的特征是必要的，因为当全局出现次数增加时，它们的信息量会减少。例如，在一个关于体育的语料库中，单词
    `match` 可能会在大量文档中出现；因此，它几乎作为一个分类特征是无用的。为了解决这个问题，我们需要不同的方法。如果我们有一个包含 `n` 个文档的语料库
    `C`，我们定义**词频**，即一个标记在文档中出现的次数，如下所示：
- en: '![](img/073280b0-172d-4084-b4c2-1d40623143df.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/073280b0-172d-4084-b4c2-1d40623143df.png)'
- en: 'We define **inverse-document-frequency**, as the following measure:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义**逆文档频率**，如下所示：
- en: '![](img/1cc49044-032f-48a3-920a-264f1e888436.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1cc49044-032f-48a3-920a-264f1e888436.png)'
- en: 'In other words, `idf(t,C)` measures how much information is provided by every
    single term. In fact, if `count(D,t) = n`, it means that a token is always present
    and *idf(t, C)* comes close to 0, and vice-versa. The term 1 in the denominator
    is a correction factor, which avoids null idf for count`(D,t) = n`. So, instead
    of considering only the term frequency, we weigh each token by defining a new
    measure:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '换句话说，`idf(t,C)` 衡量每个单个术语提供的信息量。实际上，如果 `count(D,t) = n`，这意味着一个标记总是存在，且 `idf(t,
    C)` 接近 0，反之亦然。分母中的术语 1 是一个校正因子，它避免了当 `(D,t) = n` 时的 null idf。因此，我们不仅考虑术语频率，还通过定义一个新的度量来权衡每个标记： '
- en: '![](img/4ab0e24e-ca65-432e-aac9-888fc5c68b2e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4ab0e24e-ca65-432e-aac9-888fc5c68b2e.png)'
- en: 'scikit-learn provides the `TfIdfVectorizer` class, which we can apply to the
    same toy corpus used in the previous paragraph:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了 `TfIdfVectorizer` 类，我们可以将其应用于前一段中使用的相同玩具语料库：
- en: '[PRE21]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s now check the vocabulary to make a comparison with simple count vectorizing:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查词汇表，以便与简单的计数向量化进行比较：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The term `documents` is the sixth feature in both vectorizers and appears in
    the last three documents. As you can see, it''s weight is about 0.3, while the
    term `the` is present twice only in the third document and its weight is about
    0.64\. The general rule is: if a term is representative of a document, its weight
    becomes close to 1.0, while it decreases if finding it in a sample document doesn''t
    allow us to easily determine its category.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 术语`documents`在两个向量器中都是第六个特征，并且出现在最后三个文档中。正如你所看到的，它的权重大约是0.3，而术语`the`只在第三个文档中出现了两次，其权重大约是0.64。一般规则是：如果一个术语代表一个文档，那么它的权重会接近1.0，而如果在一个样本文档中找到它不能轻易确定其类别，那么它的权重会降低。
- en: 'Also in this case, it''s possible to use an external tokenizer and specify
    the desired n-gram range. Moreover, it''s possible to normalize the vectors (through
    the parameter `norm`) and decide whether to include or exclude the addend 1 to
    the denominator of idf (through the parameter `smooth_idf`). It''s also possible
    to define the range of accepted document frequencies using the parameters `min_df`
    and `max_df` so as to exclude tokens whose occurrences are below or beyond a minimum/maximum
    threshold. They accept both integers (number of occurrences) or floats in the
    range of [0.0, 1.0] (proportion of documents). In the next example, we use some
    of these parameters:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，也可以使用外部标记化器并指定所需的n-gram范围。此外，还可以通过参数`norm`对向量进行归一化，并决定是否将1添加到idf分母中（通过参数`smooth_idf`）。还可以使用参数`min_df`和`max_df`定义接受的文档频率范围，以便排除出现次数低于或高于最小/最大阈值的标记。它们接受整数（出现次数）或范围在[0.0,
    1.0]内的浮点数（文档比例）。在下一个示例中，我们将使用这些参数中的一些：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In particular, normalizing vectors is always a good choice if they must be used
    as input for a classifier, as we'll see in the next chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，如果向量必须作为分类器的输入，那么归一化向量总是一个好的选择，正如我们将在下一章中看到的。
- en: A sample text classifier based on the Reuters corpus
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于路透社语料库的文本分类器示例
- en: 'We are going to build a sample text classifier based on the NLTK Reuters corpus.
    This one is made of up thousands of news lines divided into 90 categories:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于NLTK路透社语料库构建一个文本分类器示例。这个分类器由成千上万的新闻行组成，分为90个类别：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To simplify the process, we''ll take only two categories, which have a similar
    number of documents:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化过程，我们将只选取两个具有相似文档数量的类别：
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As each document is already split into tokens and we want to apply our custom
    tokenizer (with stopword removal and stemming), we need to rebuild the full sentences:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个文档已经被分割成标记，并且我们想要应用我们的自定义标记化器（带有停用词去除和词干提取），我们需要重建完整的句子：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we need to prepare the label vector, by assigning 0 to `rubber` and 1 to
    `cotton`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要准备标签向量，将`rubber`分配为0，将`cotton`分配为1：
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At this point, we can vectorize our corpus:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以对语料库进行向量化：
- en: '[PRE28]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now the dataset is ready, and we can proceed by splitting it into train and
    test subsets and finally train our classifier. I''ve decided to adopt a random
    forest because it''s particularly efficient for this kind of task, but the reader
    can try different classifiers and compare the results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经准备好了，我们可以通过将其分为训练集和测试集来继续，并最终训练我们的分类器。我决定采用随机森林，因为它特别适合这类任务，但读者可以尝试不同的分类器并比较结果：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The score is about 88%, which is a quite good result, but let''s try a prediction
    with a fake news line:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 得分大约是88%，这是一个相当好的结果，但让我们尝试用一条假新闻进行预测：
- en: '[PRE30]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The classification result is correct; however, by adopting some techniques that
    we're going to discuss in the next chapter, it's also possible to get better performance
    in more complex real-life problems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 分类结果正确；然而，通过采用我们将在下一章中讨论的一些技术，在更复杂的现实生活问题中也可以获得更好的性能。
- en: References
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Perkins J., Python 3 Text Processing with NLTK 3 Cookbook, Packt.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Perkins J.，《Python 3 文本处理与 NLTK 3 烹饪书》，Packt。
- en: Hardeniya N., NLTK Essentials, Packt
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hardeniya N.，《NLTK基础》，Packt
- en: Bonaccorso G., BBC News classification algorithm comparison, [https://github.com/giuseppebonaccorso/bbc_news_classification_comparison](https://github.com/giuseppebonaccorso/bbc_news_classification_comparison).
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bonaccorso G.，《BBC新闻分类算法比较》，[https://github.com/giuseppebonaccorso/bbc_news_classification_comparison](https://github.com/giuseppebonaccorso/bbc_news_classification_comparison)。
- en: Summary
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed all the basic NLP techniques, starting from the
    definition of a corpus up to the final transformation into feature vectors. We
    analyzed different tokenizing methods to address particular problems or situations
    of splitting a document into words. Then we introduced some filtering techniques
    that are necessary to remove all useless elements (also called stopwords) and
    to convert the inflected forms into standard tokens.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了所有基本的自然语言处理技术，从语料库的定义开始，直到最终将其转换为特征向量。我们分析了不同的分词方法，以解决将文档分割成单词的特定问题或情况。然后我们介绍了一些过滤技术，这些技术是必要的，以去除所有无用的元素（也称为停用词）并将屈折形式转换为标准标记。
- en: These steps are important in order to increase the information content by removing
    frequently used terms. When the documents have been successfully cleaned, it is
    possible to vectorize them using a simple approach such as the one implemented
    by the count-vectorizer, or a more complex one that takes into account the global
    distribution of terms, such as tf-idf. The latter was introduced to complete the
    work done by the stemming phase; in fact, it's purpose is to define vectors where
    each component will be close to 1 when the amount of information is high and vice-versa.
    Normally a word that is present in many documents isn't a good marker for a classifier;
    therefore, if not already removed by the previous steps, tf-idf will automatically
    reduce its weight. At the end of the chapter, we built a simple text classifier
    that implements the whole bag-of-words pipeline and uses a random forest to classify
    news lines.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤对于通过去除常用术语来增加信息内容非常重要。当文档被成功清理后，可以使用简单的方法如计数向量器实现的方法，或者更复杂的方法，如考虑术语全局分布的方法，例如tf-idf。后者是为了补充词干处理阶段的工作；实际上，它的目的是定义向量，其中每个分量在信息量高时接近1，反之亦然。通常，一个在许多文档中都存在的单词不是一个好的分类器标记；因此，如果在前面的步骤中没有被去除，tf-idf将自动降低其权重。在本章结束时，我们构建了一个简单的文本分类器，该分类器实现了整个词袋模型管道，并使用随机森林对新闻行进行分类。
- en: In the next chapter, we're going to complete this introduction with a brief
    discussion of advanced techniques such as topic modeling, latent semantic analysis,
    and sentiment analysis.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过简要讨论高级技术，如主题建模、潜在语义分析和情感分析，来完成这个介绍。
