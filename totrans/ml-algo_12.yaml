- en: Introduction to Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural language processing is a set of machine learning techniques that allows
    working with text documents, considering their internal structure and the distribution
    of words. In this chapter, we're going to discuss all common methods to collect
    texts, split them into atoms, and transform them into numerical vectors. In particular,
    we'll compare different methods to tokenize documents (separate each word), to
    filter them, to apply special transformations to avoid inflected or conjugated
    forms, and finally to build a common vocabulary. Using the vocabulary, it will
    be possible to apply different vectorization approaches to build feature vectors
    that can easily be used for classification or clustering purposes. To show how
    to implement the whole pipeline, at the end of the chapter, we're going to set
    up a simple classifier for news lines.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK and built-in corpora
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural Language Toolkit** (**NLTK**) is a very powerful Python framework
    that implements most NLP algorithms and will be adopted in this chapter together
    with scikit-learn. Moreover, NLTK provides some built-in corpora that can be used
    to test algorithms. Before starting to work with NLTK, it''s normally necessary
    to download all the additional elements (corpora, dictionaries, and so on) using
    a specific graphical interface. This can be done in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will launch the user interface, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9adf74f-9ab4-4b3f-856e-7aeae1f3c586.png)'
  prefs: []
  type: TYPE_IMG
- en: It's possible to select every single feature or download all elements (I suggest
    this option if you have enough free space) to immediately exploit all NLTK functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK can be installed using pip (`pip install -U nltk`) or with one of the binary
    distributions available at [http://www.nltk.org](http://www.nltk.org). On the
    same website, there's complete documentation that can be useful for going deeper
    into each topic.
  prefs: []
  type: TYPE_NORMAL
- en: Corpora examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A subset of the Gutenberg project is provided and can be freely accessed in
    this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A single document can be accessed as a raw version or split into sentences
    or words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we''re going to discuss, in many cases, it can be useful to have the raw
    text so as to split it into words using a custom strategy. In many other situations,
    accessing sentences directly allows working with the original structural subdivision.
    Other corpora include web texts, Reuters news lines, the Brown corpus, and many
    more. For example, the Brown corpus is a famous collection of documents divided
    by genre:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Further information about corpora can be found at [http://www.nltk.org/book/ch02.html](http://www.nltk.org/book/ch02.html).
  prefs: []
  type: TYPE_NORMAL
- en: The bag-of-words strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In NLP, a very common pipeline can be subdivided into the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting a document into a corpus.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizing, stopword (articles, prepositions and so on) removal, and stemming
    (reduction to radix-form).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building a common vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vectorizing the documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classifying or clustering the documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pipeline is called **bag-of-words** and will be discussed in this chapter.
    A fundamental assumption is that the order of each single word in a sentence is
    not important. In fact, when defining a feature vector, as we're going to see,
    the measures taken into account are always related to frequencies and therefore
    they are insensitive to the local positioning of all elements. From some viewpoints,
    this is a limitation because in a natural language the internal order of a sentence
    is necessary to preserve the meaning; however, there are many models that can
    work efficiently with texts without the complication of local sorting. When it's
    absolutely necessary to consider small sequences, it will be done by adopting
    groups of tokens (called n-grams) but considering them as a single atomic element
    during the vectorization step.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, there''s a schematic representation of this process
    (without the fifth step) for a sample document (sentence):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a3fb850-b3c2-48b1-9d30-21db8e60009a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are many different methods used to carry out each step and some of them
    are context-specific. However, the goal is always the same: maximizing the information
    of a document and reducing the size of the common vocabulary by removing terms
    that are too frequent or derived from the same radix (such as verbs). The information
    content of a document is in fact determined by the presence of specific terms
    (or group of terms) whose frequency in the corpus is limited. In the example shown
    in the previous figure, **fox** and **dog** are important terms, while **the**
    is useless (often called a **stopword**). Moreover, **jumps** can be converted
    to the standard form **jump**, which expresses a specific action when present
    in different forms (like jumping or jumped). The last step is transforming into
    a numerical vector, because our algorithms work with numbers, and it''s important
    to limit the length of the vectors so as to improve the learning speed and the
    memory consumption. In the following sections, we''re going to discuss each step
    in detail, and at the end, we''re going to build a sample classifier for news
    lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in processing a piece of text or a corpus is splitting it into
    atoms (sentences, words, or parts of words), normally defined as **tokens**. Such
    a process is quite simple; however, there can be different strategies to solve
    particular problems.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence tokenizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In many cases, it''s useful to split large text into sentences, which are normally
    delimited by a full stop or another equivalent mark. As every language has its
    own orthographic rules, NLTK offers a method called `sent_tokenize()`that accepts
    a language (the default is English) and splits the text according to the specific
    rules. In the following example, we show the usage of this function with different
    languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Word tokenizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way to tokenize a sentence into words is provided by the class `TreebankWordTokenizer`,
    which, however, has some limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, in the first case the sentence has been correctly split into
    words, keeping the punctuation separate (this is not a real issue because it can
    be removed in a second step). However, in the complex example, the contraction
    `isn''t` has been split into `is` and `n''t`. Unfortunately, without a further
    processing step, it''s not so easy converting a token with a contraction into
    its normal form (like `not` ), therefore, another strategy must be employed. A
    good way to solve the problem of separate punctuation is provided by the class `RegexpTokenizer`,
    which offers a flexible way to split words according to a regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the common problems can be easily solved using this class, so I suggest
    you learn how to write simple regular expressions that can match specific patterns.
    For example, we can remove all numbers, commas, and other punctuation marks from
    a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Even if there are other classes provided by NLTK, they can always be implemented
    using a customized `RegexpTokenizer`, which is powerful enough to solve almost
    every particular problem; so I prefer not to go deeper in this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Stopword removal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stopwords are part of a normal speech (articles, conjunctions, and so on),
    but their occurrence frequency is very high and they don''t provide any useful
    semantic information. For these reasons, it''s a good practice to filter sentences
    and corpora by removing them all. NLTK provides lists of stopwords for the most
    common languages and their usage is immediate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'A subset of English stopwords is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To filter a sentence, it''s possible to adopt a functional approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Language detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stopwords, like other important features, are strictly related to a specific
    language, so it''s often necessary to detect the language before moving on to
    any other step. A simple, free, and reliable solution is provided by the `langdetect` library, which
    has been ported from Google''s language detection system. Its usage is immediate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The function returns the ISO 639-1 codes ([https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes)),
    which can be used as keys in a dictionary to get the complete language name. Where
    the text is more complex, the detection can more difficult and it''s useful to
    know whether there are any ambiguities. It''s possible to get the probabilities
    for the expected languages through the `detect_langs()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: langdetect can be installed using pip (`pip install --upgrade langdetect`).
    Further information is available at [https://pypi.python.org/pypi/langdetect](https://pypi.python.org/pypi/langdetect).
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Stemming is a process that is used to transform particular words (such as verbs
    or plurals) into their radical form so as to preserve the semantics without increasing
    the number of unique tokens. For example, if we consider the three expressions `I
    run`, `He runs`, and `Running`, they can be reduced into a useful (though grammatically
    incorrect) form: `I run`, `He run`, `Run`. In this way, we have a single token
    that defines the same concept (`run`), which, for clustering or classification
    purposes, can be used without any precision loss. There are many stemmer implementations
    provided by NLTK. The most common (and flexible) is `SnowballStemmer`, based on
    a multilingual algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ignore_stopwords` parameter informs the stemmer not to process the stopwords.
    Other implementations are `PorterStemmer` and `LancasterStemmer`. Very often the
    result is the same, but in some cases, a stemmer can implement more selective
    rules. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, Snowball and Porter algorithms keep the word unchanged, while
    Lancaster extracts a radix (which is meaningless). On the other hand, the latter
    algorithm implements many specific English rules, which can really reduce the
    number of unique tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, both Porter and Lancaster stemmers are available in NLTK only
    in English; so the default choice is often Snowball, which is available in many
    languages and can be used in conjunction with an appropriate stopword set.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the last step of the bag-of-words pipeline and it is necessary for transforming
    text tokens into numerical vectors. The most common techniques are based on a
    count or frequency computation, and they are both available in scikit-learn with
    sparse matrix representations (this is a choice that can save a lot of space considering
    that many tokens appear only a few times while the vectors must have the same
    length).
  prefs: []
  type: TYPE_NORMAL
- en: Count vectorizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The algorithm is very simple and it''s based on representing a token considering
    how many times it appears in a document. Of course, the whole corpus must be processed
    in order to determine how many unique tokens are present and their frequencies.
    Let''s see an example of the `CountVectorizer` class on a simple corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, each document has been transformed into a fixed-length vector,
    where 0 means that the corresponding token is not present, while a positive number
    represents the occurrences. If we need to exclude all tokens whose document frequency
    is less than a predefined value, we can set it through the parameter `min_df`
    (the default value is 1). Sometimes it can be useful to avoid terms that are very
    common; however, the next strategy will manage this problem in a more reliable
    and complete way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vocabulary can be accessed through the instance variable `vocabulary_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a generic vector, it''s possible to retrieve the corresponding list of
    tokens with an inverse transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Both this and the following method can also use an external tokenizer (through
    the parameter `tokenizer`), it can be customized using the techniques discussed
    in previous sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With our tokenizer (using stopwords and stemming), the vocabulary is shorter
    and so are the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have considered only single tokens (also called unigrams), but in
    many contexts, it''s useful to consider short sequences of words (bigrams or trigrams)
    as atoms for our classifiers, just like all the other tokens. For example, if
    we are analyzing the sentiment of some texts, it could be a good idea to consider
    bigrams such as `pretty good`, `very bad`, and so on. From a semantic viewpoint,
    in fact, it''s important to consider not just the adverbs but the whole compound
    form. It''s possible to inform our vectorizers about the range of n-grams we want
    to consider. For example, if we need unigrams and bigrams, we can use this snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the vocabulary now contains the bigrams, and the vectors include
    their relative frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Tf-idf vectorizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most common limitation of count vectorizing is that the algorithm doesn''t
    consider the whole corpus while considering the frequency of each token. The goal
    of vectorizing is normally preparing the data for a classifier; therefore it''s
    necessary to avoid features that are present very often, because their information
    decreases when the number of global occurrences increases. For example, in a corpus
    about a sport, the word `match` could be present in a huge number of documents;
    therefore it''s almost useless as a classification feature. To address this issue,
    we need a different approach. If we have a corpus `C` with `n` documents, we define
    **term-frequency**, the number of times a token occurs in a document, as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/073280b0-172d-4084-b4c2-1d40623143df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We define **inverse-document-frequency**, as the following measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cc49044-032f-48a3-920a-264f1e888436.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, `idf(t,C)` measures how much information is provided by every
    single term. In fact, if `count(D,t) = n`, it means that a token is always present
    and *idf(t, C)* comes close to 0, and vice-versa. The term 1 in the denominator
    is a correction factor, which avoids null idf for count`(D,t) = n`. So, instead
    of considering only the term frequency, we weigh each token by defining a new
    measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ab0e24e-ca65-432e-aac9-888fc5c68b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'scikit-learn provides the `TfIdfVectorizer` class, which we can apply to the
    same toy corpus used in the previous paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now check the vocabulary to make a comparison with simple count vectorizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The term `documents` is the sixth feature in both vectorizers and appears in
    the last three documents. As you can see, it''s weight is about 0.3, while the
    term `the` is present twice only in the third document and its weight is about
    0.64\. The general rule is: if a term is representative of a document, its weight
    becomes close to 1.0, while it decreases if finding it in a sample document doesn''t
    allow us to easily determine its category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Also in this case, it''s possible to use an external tokenizer and specify
    the desired n-gram range. Moreover, it''s possible to normalize the vectors (through
    the parameter `norm`) and decide whether to include or exclude the addend 1 to
    the denominator of idf (through the parameter `smooth_idf`). It''s also possible
    to define the range of accepted document frequencies using the parameters `min_df`
    and `max_df` so as to exclude tokens whose occurrences are below or beyond a minimum/maximum
    threshold. They accept both integers (number of occurrences) or floats in the
    range of [0.0, 1.0] (proportion of documents). In the next example, we use some
    of these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In particular, normalizing vectors is always a good choice if they must be used
    as input for a classifier, as we'll see in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A sample text classifier based on the Reuters corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to build a sample text classifier based on the NLTK Reuters corpus.
    This one is made of up thousands of news lines divided into 90 categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To simplify the process, we''ll take only two categories, which have a similar
    number of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As each document is already split into tokens and we want to apply our custom
    tokenizer (with stopword removal and stemming), we need to rebuild the full sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to prepare the label vector, by assigning 0 to `rubber` and 1 to
    `cotton`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can vectorize our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the dataset is ready, and we can proceed by splitting it into train and
    test subsets and finally train our classifier. I''ve decided to adopt a random
    forest because it''s particularly efficient for this kind of task, but the reader
    can try different classifiers and compare the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The score is about 88%, which is a quite good result, but let''s try a prediction
    with a fake news line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The classification result is correct; however, by adopting some techniques that
    we're going to discuss in the next chapter, it's also possible to get better performance
    in more complex real-life problems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perkins J., Python 3 Text Processing with NLTK 3 Cookbook, Packt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hardeniya N., NLTK Essentials, Packt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bonaccorso G., BBC News classification algorithm comparison, [https://github.com/giuseppebonaccorso/bbc_news_classification_comparison](https://github.com/giuseppebonaccorso/bbc_news_classification_comparison).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed all the basic NLP techniques, starting from the
    definition of a corpus up to the final transformation into feature vectors. We
    analyzed different tokenizing methods to address particular problems or situations
    of splitting a document into words. Then we introduced some filtering techniques
    that are necessary to remove all useless elements (also called stopwords) and
    to convert the inflected forms into standard tokens.
  prefs: []
  type: TYPE_NORMAL
- en: These steps are important in order to increase the information content by removing
    frequently used terms. When the documents have been successfully cleaned, it is
    possible to vectorize them using a simple approach such as the one implemented
    by the count-vectorizer, or a more complex one that takes into account the global
    distribution of terms, such as tf-idf. The latter was introduced to complete the
    work done by the stemming phase; in fact, it's purpose is to define vectors where
    each component will be close to 1 when the amount of information is high and vice-versa.
    Normally a word that is present in many documents isn't a good marker for a classifier;
    therefore, if not already removed by the previous steps, tf-idf will automatically
    reduce its weight. At the end of the chapter, we built a simple text classifier
    that implements the whole bag-of-words pipeline and uses a random forest to classify
    news lines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to complete this introduction with a brief
    discussion of advanced techniques such as topic modeling, latent semantic analysis,
    and sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
