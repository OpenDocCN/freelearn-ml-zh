<html><head></head><body>
<div class="book" title="Chapter&#xA0;10.&#xA0;Ensembling Survival Models"><div class="book" id="22O7C2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10" class="calibre1"/>Chapter 10. Ensembling Survival Models</h1></div></div></div><p class="calibre7">The primary biliary cirrhosis data was introduced in first two chapters using the jackknife method. Observations in clinical trials are usually subject to censoring, and the jackknife method helps to complete incomplete observations through the idea of pseudo values. Since pseudo values are likely to be dependent on each other, the generalized estimating equation framework made it possible to estimate the impact of twelve covariates at the time of interest. The idea of pseudo values and the generalized estimating equation framework makes it easy for practitioners to interpret the results. However, this method might not be useful if the number of censored observations is exceptionally high. Furthermore, it is also preferable to have statistical methods that preserve the incompleteness of the observations and yet make good use of them. The general (linear) regression framework with time as the dependent variable and the error term following appropriate lifetime distribution can be set up in the usual regression framework. However, it turns out to be unreliable, and in many cases, it is known to be unstable, or the convergence simply does not take place. In <span class="strong"><em class="calibre9">Regression Models and Life-Tables</em></span> (<a class="calibre1" href="http://www.stat.cmu.edu/~ryantibs/journalclub/cox_1972.pdf">http://www.stat.cmu.edu/~ryantibs/journalclub/cox_1972.pdf</a>), Cox (1972) achieved a breakthrough in the regression modeling of the survival data when he proposed the proportional hazards model. So, what is a hazards model?</p><p class="calibre7">This chapter will open with core survival analysis concepts such as hazard rate, cumulative hazard function, and survival function. A few parametric lifetime models are also discussed and visualized through R programs. For the given data, we will then study how to carry out inference for the lifetime distribution through nonparametric methods. An estimation of the survival function and cumulative hazard function is then illustrated for the time to event of interest for the <code class="literal">pbc</code> dataset. Hypothesis testing through the use of the logrank test is demonstrated for different segments of the <code class="literal">pbc</code> data. Regression models will begin with a simple illustration of the parametric regression model, using exponential distribution as an example. It is known that the parametric models are not very useful for clinical trials data. This leads to an<a id="id450" class="calibre1"/> important variant in the Cox proportional hazards regression model, which is a <span class="strong"><strong class="calibre8">semiparametric</strong></span> model in the sense that the baseline hazard rate is left completely unspecified and the impact of covariates is modeled through an exponential linear term on the hazard rate.</p><p class="calibre7">Survival trees are an important variant of the decision tree applicable to the survival data. The split criteria are based on the <span class="strong"><strong class="calibre8">logrank</strong></span> test. Naturally, we will be interested in ensemble methods for the survival data, and <a id="id451" class="calibre1"/>hence we develop the survival random forests in the concluding section.</p><p class="calibre7">We will cover the following topics in this chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Essential concepts of survival analysis, such as hazard rate, cumulative hazard function, and survival function</li><li class="listitem">The Nelson-Aalen and Kaplan Meier estimators as respective estimators of the cumulative hazard function and the survival function</li><li class="listitem">Logrank tests for the comparison of survival curves</li><li class="listitem">Parametric and semiparametric methods analyzing the impact of independent covariates on the hazard rate</li><li class="listitem">Survival tree based on logrank test</li><li class="listitem">Random forests as ensemble methods for the survival data</li></ul></div></div>

<div class="book" title="Chapter&#xA0;10.&#xA0;Ensembling Survival Models">
<div class="book" title="Core concepts of survival analysis"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch10lvl1sec74" class="calibre1"/>Core concepts of survival analysis</h1></div></div></div><p class="calibre7">Survival analysis deals with <a id="id452" class="calibre1"/>censored data, and it is very common that parametric models are unsuitable for explaining the lifetimes observed in clinical trials.</p><p class="calibre7">Let <span class="strong"><em class="calibre9">T</em></span> denote the survival time, or the time to the event of interest, and we will naturally have <span class="strong"><img src="../images/00399.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>, which is a continuous random variable. Suppose that the lifetime cumulative distribution is <span class="strong"><em class="calibre9">F</em></span> and the associated density function is <span class="strong"><em class="calibre9">f</em></span>. We define important concepts as required for further analysis. We will explore the concept of <span class="strong"><em class="calibre9">survival function</em></span> next.</p><p class="calibre7">Suppose that <span class="strong"><em class="calibre9">T</em></span> is the continuous random variable of a lifetime and that the associated cumulative distribution function is <span class="strong"><em class="calibre9">F</em></span>. The survival function at time <span class="strong"><em class="calibre9">t</em></span> is the probability the observation is still alive at the time, and it is defined by the following:</p><div class="mediaobject"><img src="../images/00400.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The survival function can take different forms. Let's go through some examples for each of the distributions to get a clearer picture of the difference in survival functions.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exponential Distribution</strong></span>: Suppose that the lifetime distribution of an electronic component follows exponential distribution <a id="id453" class="calibre1"/>with rate <span class="strong"><img src="../images/00401.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>. Then, its density function is as follows:</p><div class="mediaobject"><img src="../images/00402.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The cumulative distribution function is as follows:</p><div class="mediaobject"><img src="../images/00403.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The mean and variance of the exponential distribution are, respectively, <span class="strong"><img src="../images/00404.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> and <span class="strong"><img src="../images/00405.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>. The survival function of the exponential distribution is as follows:</p><div class="mediaobject"><img src="../images/00406.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The mean of exponential distribution is <span class="strong"><img src="../images/00407.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>. The exponential distribution is driven by a single parameter and it also enjoys an elegant property, which is known as a memoryless property (see Chapter 6, Tattar, et al. (2016)).</p><p class="calibre7"><span class="strong"><strong class="calibre8">Gamma Distribution</strong></span>: We say that the<a id="id454" class="calibre1"/> lifetime random variable follows a gamma<a id="id455" class="calibre1"/> distribution with rate <span class="strong"><img src="../images/00408.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> and shape <span class="strong"><img src="../images/00409.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>, if its probability density function <code class="literal">f</code> is of the following form:</p><div class="mediaobject"><img src="../images/00410.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The mean and variance of gamma distribution are, respectively, <span class="strong"><img src="../images/00411.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> and <span class="strong"><img src="../images/00412.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>. A closed form of the cumulative distribution functions, and hence the survival function, does not exist.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Weibull Distribution</strong></span>: A lifetime random<a id="id456" class="calibre1"/> variable is said to follow a Weibull distribution with rate <span class="strong"><img src="../images/00413.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> and shape <span class="strong"><img src="../images/00414.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span>, if its probability density function <code class="literal">f</code> is of the following form:</p><div class="mediaobject"><img src="../images/00415.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The cumulative distribution function of the Weibull distribution is demonstrated as follows:</p><div class="mediaobject"><img src="../images/00416.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The survival function is as follows:</p><div class="mediaobject"><img src="../images/00417.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Next, we will define the concept of hazard rate, which is also known as the instantaneous failure rate.</p><p class="calibre7">Let <span class="strong"><em class="calibre9">T</em></span> denote the lifetime random variable and <span class="strong"><em class="calibre9">F</em></span> denote the associated cumulative distribution function, then the hazard rate at time <code class="literal">t</code> is defined as follows:</p><div class="mediaobject"><img src="../images/00418.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00419.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00420.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The problem of estimating the hazard rate is as difficult as that of the density function, and hence the cumulative function concept will be useful.</p><p class="calibre7">Let <span class="strong"><em class="calibre9">T</em></span> denote the<a id="id457" class="calibre1"/> lifetime random variable and <span class="strong"><img src="../images/00421.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> be the associated hazard rate, then the cumulative hazard function is defined using the following:</p><div class="mediaobject"><img src="../images/00422.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The following relationship exists between these three quantities: </p><div class="mediaobject"><img src="../images/00423.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><div class="mediaobject"><img src="../images/00424.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The expected value is related to the survival function as follows: </p><div class="mediaobject"><img src="../images/00425.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">In the next R program, we will visualize the three survival quantities for the three probability distributions. First, we will set up a graphical device for nine plots with the par and <code class="literal">mfrow</code> functions. The program is explained for the exponential distribution. Consider the time period 0-100 and create a numeric object <code class="literal">Time</code> in the program. We will begin with the computation of the values of the density functions using the <code class="literal">dexp</code> function for the Time object. This means that <code class="literal">dexp(Time)</code> will calculate the value of the density function <span class="strong"><em class="calibre9">f(t)</em></span> for each point between 0–100. Since the survival function is related to the cumulative distribution by <span class="strong"><img src="../images/00426.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> and <code class="literal">pexp</code> gives us the values of <span class="strong"><em class="calibre9">F </em></span>at the time point <span class="strong"><em class="calibre9">t</em></span>, the survival function for the exponential distribution is computed as <span class="strong"><em class="calibre9">1-pexp()</em></span>. The hazard rate, density function, and survival function are related by <span class="strong"><img src="../images/00427.jpeg" alt="Core concepts of survival analysis" class="calibre15"/></span> and can be easily obtained. The cumulative hazard function is obtained by using the values of the survival function and the relationship as follows:</p><div class="mediaobject"><img src="../images/00428.jpeg" alt="Core concepts of survival analysis" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The program is then repeated for the gamma and Weibull distribution with changes in the appropriate specification of the parameters, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; par(mfrow=c(3,3))
&gt; Time &lt;- seq(0,100,1)
&gt; lambda &lt;- 1/20
&gt; expdens &lt;- dexp(Time,rate=lambda)
&gt; expsurv &lt;- 1-pexp(Time,rate=lambda)
&gt; exphaz &lt;- expdens/expsurv
&gt; expcumhaz &lt;- -log(expsurv)
&gt; plot(Time,exphaz,"l",xlab="Time",ylab="Hazard Rate",ylim=c(0,0.1))
&gt; plot(Time,expcumhaz,"l",xlab="Time",ylab="Cumulative Hazard Function")
&gt; mtext("Exponential Distribution")
&gt; plot(Time,expsurv,"l",xlab="Time",ylab="Survival Function")
&gt; 
&gt; # Gamma Distribution
&gt; lambda &lt;- 1/10; k &lt;- 2
&gt; gammadens &lt;- dgamma(Time,rate=lambda,shape=k)
&gt; gammasurv &lt;- 1-pgamma(Time,rate=lambda,shape=k)
&gt; gammahaz &lt;- gammadens/gammasurv
&gt; gammacumhaz &lt;- -log(gammasurv)
&gt; plot(Time,gammahaz,"l",xlab="Time",ylab="Hazard Rate")
&gt; plot(Time,gammacumhaz,"l",xlab="Time",ylab="Cumulative Hazard Function")
&gt; mtext("Gamma Distribution")
&gt; plot(Time,gammasurv,"l",xlab="Time",ylab="Survival Function")
&gt; 
&gt; # Weibull Distribution
&gt; lambda &lt;- 25; k &lt;- 2
&gt; Weibulldens &lt;- dweibull(Time,scale=lambda,shape=k)
&gt; Weibullsurv &lt;- 1-pweibull(Time,scale=lambda,shape=k)
&gt; Weibullhaz &lt;- Weibulldens/Weibullsurv
&gt; Weibullcumhaz &lt;- -log(Weibullsurv)
&gt; plot(Time,Weibullhaz,"l",xlab="Time",ylab="Hazard Rate")
&gt; plot(Time,Weibullcumhaz,"l",xlab="Time",ylab="Cumulative Hazard Function")
&gt; mtext("Weibull Distribution")
&gt; plot(Time,Weibullsurv,"l",xlab="Time",ylab="Survival Function")</pre></div><div class="mediaobject"><img src="../images/00429.jpeg" alt="Core concepts of survival analysis" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Hazard rate, Cumulative hazard function, and Survival function for Exponential, Gamma, and Weibull distributions</p></div></div><p class="calibre11"> </p><p class="calibre7">Repeat the preceding program<a id="id458" class="calibre1"/> for different parameter values and prepare a summary of your observations for the change in the hazard function, cumulative hazard function, and the survival function. Summarize your observations separately for the three distributions of exponential, gamma, and Weibull.</p><p class="calibre7">Now, we need to see how well the model fits to the <code class="literal">pbc</code> dataset. Here, we will fit the exponential, gamma, and Weibull distributions to the lifetimes of interest in the <code class="literal">pbc</code> dataset. Note that, since we have censored data, the incomplete observations can't simply be thrown away, as 257 out of 418 are incomplete observations. Though we can't go into the mathematics of the maximum likelihood estimation for survival data, it is important to note here that the contribution of a complete observation to the likelihood is <span class="strong"><em class="calibre9">f(t)</em></span> and if it is incomplete/censored, it is <span class="strong"><em class="calibre9">S(t)</em></span>. Consequently, it is important for the software to know which observation is complete and which is incomplete. Here we will use the <code class="literal">Surv</code> function from the <code class="literal">survival</code> package to specify this, and then use the <code class="literal">flexsurvreg</code> function from the <code class="literal">flexsurv</code> package to fit an appropriate lifetime distribution. The option of <code class="literal">dist</code> helps set up the appropriate distributions, as can be seen in the following program:</p><div class="informalexample"><pre class="programlisting">&gt; pbc &lt;- survival::pbc
&gt; Surv(pbc$time,pbc$status==2)
  [1]  400  4500+ 1012  1925  1504+ 2503  1832+ 2466  2400    51  3762 
 [12]  304  3577+ 1217  3584  3672+  769   131  4232+ 1356  3445+  673  

 [397] 1328+ 1375+ 1260+ 1223+  935   943+ 1141+ 1092+ 1150+  703  1129+
[408] 1086+ 1067+ 1072+ 1119+ 1097+  989+  681  1103+ 1055+  691+  976+
&gt; pbc_exp &lt;- flexsurvreg(Surv(time,status==2)~1,data=pbc,dist="exponential")
&gt; pbc_exp
Call:
flexsurvreg(formula = Surv(time, status == 2) ~ 1, data = pbc, 
    dist = "exponential")

Estimates: 
      est       L95%      U95%      se      
rate  2.01e-04  1.72e-04  2.34e-04  1.58e-05

N = 418,  Events: 161,  Censored: 257
Total time at risk: 801633
Log-likelihood = -1531.593, df = 1
AIC = 3065.187

&gt; windows(height=100,width=100)
&gt; plot(pbc_exp,ylim=c(0,1),col="black")
&gt; pbc_gamma &lt;- flexsurvreg(Surv(time,status==2)~1,data=pbc,dist="gamma")
&gt; pbc_gamma
Call:
flexsurvreg(formula = Surv(time, status == 2) ~ 1, data = pbc, 
    dist = "gamma")

Estimates: 
       est       L95%      U95%      se      
shape  1.10e+00  9.21e-01  1.30e+00  9.68e-02
rate   2.33e-04  1.70e-04  3.21e-04  3.78e-05

N = 418,  Events: 161,  Censored: 257
Total time at risk: 801633
Log-likelihood = -1531.074, df = 2
AIC = 3066.147

&gt; plot(pbc_gamma,col="blue",add=TRUE)
&gt; pbc_Weibull &lt;- flexsurvreg(Surv(time,status==2)~1,data=pbc,dist="weibull")
&gt; pbc_Weibull
Call:
flexsurvreg(formula = Surv(time, status == 2) ~ 1, data = pbc, 
    dist = "weibull")

Estimates: 
       est       L95%      U95%      se      
shape  1.08e+00  9.42e-01  1.24e+00  7.48e-02
scale  4.71e+03  3.96e+03  5.59e+03  4.13e+02

N = 418,  Events: 161,  Censored: 257
Total time at risk: 801633
Log-likelihood = -1531.017, df = 2
AIC = 3066.035

&gt; plot(pbc_Weibull,col="orange",add=TRUE)
&gt; legend(3000,1,c("Exponential","Gamma","Weibull"),
+        col=c("black","blue","orange"),merge=TRUE,lty=2)</pre></div><p class="calibre7">The<a id="id459" class="calibre1"/> resulting diagram is given here:</p><div class="mediaobject"><img src="../images/00430.jpeg" alt="Core concepts of survival analysis" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Fitting exponential, gamma, and Weibull distributions for censored data</p></div></div><p class="calibre11"> </p><p class="calibre7">The AIC value for the fitted exponential model is 3065.187, for the fitted gamma model it is 3066.147, and for Weibull it is 3066.035. The lower the criteria, the better. Consequently, the exponential is the best fit according to the AIC criteria. This is then followed by the Weibull and gamma distributions. The exponential distribution with a single parameter is a better fit here than the more complex models of gamma and Weibull.</p><p class="calibre7">Some explanation of the R program is in order now. The <code class="literal">pbc</code> dataset is loaded with <code class="literal">survival::pbc</code> since the R package <code class="literal">randomForestSRC</code> also has a dataset with the same name and it is a slightly different version. Consequently, the <code class="literal">survival::pbc</code> code ensures that we continue to load the <code class="literal">pbc</code> dataset as we did in earlier instances. The event of interest for us is indicated by <code class="literal">status==2</code> and <code class="literal">Surv(pbc$time,pbc$status==2)</code>, which creates a survival object that has complete observations mentioned in the numeric object. If the <code class="literal">status</code> is anything other than <code class="literal">2</code>, the observation is censored and this is indicated by the number followed by the <code class="literal">+</code> sign. The <code class="literal">Surv(time,status==2)~1</code> code creates the necessary formula, which is useful for applying survival functions. The <code class="literal">dist="exponential"</code> option ensures that exponential distribution is fitted on the survival data. When the fitted model <code class="literal">pbc_exp</code> is run on the console, we get a summary of the fitted model and it returns the estimates of the parameters of the model, the 95% confidence interval, and the standard error of the parameter's estimate. We also get the count of complete and censored observations, the total time at risk for all patients, the likelihood function value, and the AIC. Note how the degrees of freedom vary across the three fitted distributions.</p><p class="calibre7">The parametric<a id="id460" class="calibre1"/> models detailed here give an idea of the survival concepts. When we don't have enough evidence to construct a parametric model, we resort to nonparametric and semiparametric models for carrying out the statistical inference. In the following section, we will continue our analysis of the <code class="literal">pbc</code> data.</p></div></div>
<div class="book" title="Nonparametric inference"><div class="book" id="23MNU2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec75" class="calibre1"/>Nonparametric inference</h1></div></div></div><p class="calibre7">Survival data is subject to <a id="id461" class="calibre1"/>censoring and we need to introduce a new quantity to capture this information. Suppose that we have a <span class="strong"><em class="calibre9">n</em></span> IID random sample of lifetime random variables in <span class="strong"><img src="../images/00431.jpeg" alt="Nonparametric inference" class="calibre15"/></span>, and we know that the event of interest might have occurred or that it will occur sometime in the future. The additional information is captured by the Kronecker indicator variable, <span class="strong"><img src="../images/00432.jpeg" alt="Nonparametric inference" class="calibre15"/></span>:</p><div class="mediaobject"><img src="../images/00433.jpeg" alt="Nonparametric inference" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Thus, we have n pairs of random observations in the <span class="strong"><em class="calibre9">Ts</em></span> and <span class="strong"><img src="../images/00434.jpeg" alt="Nonparametric inference" class="calibre15"/></span>s, <span class="strong"><img src="../images/00435.jpeg" alt="Nonparametric inference" class="calibre15"/></span>. To obtain the estimates of the cumulative hazard function and the survival function, we will need an additional notation. Let <span class="strong"><img src="../images/00436.jpeg" alt="Nonparametric inference" class="calibre15"/></span> denote the unique times of Ts at which the event of interest is observed. Next, we denote <span class="strong"><img src="../images/00437.jpeg" alt="Nonparametric inference" class="calibre15"/></span> to represent the number of observations that are at risk just before times <span class="strong"><img src="../images/00438.jpeg" alt="Nonparametric inference" class="calibre15"/></span> and <span class="strong"><img src="../images/00439.jpeg" alt="Nonparametric inference" class="calibre15"/></span> the number of events that occur at that time. Using these quantities, we now propose to estimate the cumulative hazard function using the following:</p><div class="mediaobject"><img src="../images/00440.jpeg" alt="Nonparametric inference" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The estimator <span class="strong"><img src="../images/00441.jpeg" alt="Nonparametric inference" class="calibre15"/></span> is the famous Nelson-Aalen estimator. The Nelson-Aalen estimator enjoys statistical properties<a id="id462" class="calibre1"/> including the fact that (i) it is the nonparametric maximum likelihood estimator for the cumulative hazard function, and (ii) it follows an asymptotically normal distribution. An estimator of the survival function is given by the following:</p><div class="mediaobject"><img src="../images/00442.jpeg" alt="Nonparametric inference" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The estimator <span class="strong"><img src="../images/00443.jpeg" alt="Nonparametric inference" class="calibre15"/></span> is the well-known Kaplan-Meier estimator. The properties of the Nelson-Aalen estimator get carried<a id="id463" class="calibre1"/> over to the Kaplan-Meier estimator by an application of the functional-delta theorem. It should be noted that the Kaplan-Meier estimator<a id="id464" class="calibre1"/> is again the nonparametric maximum likelihood estimator and asymptotically, it follows the normal distribution. We will now look at how to obtain the estimates for a given dataset using R software.</p><p class="calibre7">We have already <a id="id465" class="calibre1"/>created the survival object using the <code class="literal">Surv(pbc$time, pbc$status==2)</code> code. Now, applying the <code class="literal">survfit</code> function on the survival object, we set up the Kaplan-Meier estimator in the <code class="literal">pbc_sf survfit</code> object:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_sf &lt;- survfit(Surv(time,status==2)~1,pbc)
&gt; pbc_sf
Call: survfit(formula = Surv(time, status == 2) ~ 1, data = pbc)
      n  events  median 0.95LCL 0.95UCL 
    418     161    3395    3090    3853 </pre></div><p class="calibre7">The output shows that we have <code class="literal">418</code> observations. Out of these, <code class="literal">161</code> experience the event of interest. We would like to obtain the survival function at different time points of interest. The median survival time is shown as <code class="literal">3395</code> and the confidence interval for this point estimate is <code class="literal">3090</code> and <code class="literal">3853</code>. However, if you find the median of the overall times, the median time for complete observations, and for censored observations, none of that will come closer to the displayed value of <code class="literal">3395</code>. A quick code reveals the results as follows:</p><div class="informalexample"><pre class="programlisting">&gt; median(pbc$time)
[1] 1730
&gt; median(pbc$time[pbc$status==2])
[1] 1083
&gt; median(pbc$time[pbc$status!=2])
[1] 2157</pre></div><p class="calibre7">You may ask yourself, Why is there so much of a difference between the estimated median survival time and these medians? The answer will become clear soon enough.</p><p class="calibre7">The <code class="literal">summary</code> function will be used to obtain that answer. For the ten deciles of the observed time, inclusive of censored times, we will obtain the Kaplan-Meier estimates and the associated 95% confidence interval, which is based on the variance estimate, which in turn is based on Greenwood's formula:</p><div class="informalexample"><pre class="programlisting">&gt; summary(pbc_sf,times=as.numeric(quantile(pbc$time,seq(0,1,0.1))))
Call: survfit(formula = Surv(time, status == 2) ~ 1, data = pbc)

 time n.risk n.event survival std.err lower 95% CI upper 95% CI
   41    418       2    0.995 0.00338        0.989        1.000
  607    376      39    0.902 0.01455        0.874        0.931
  975    334      31    0.827 0.01860        0.791        0.864
 1218    292      19    0.778 0.02061        0.738        0.819
 1435    251       8    0.755 0.02155        0.714        0.798
 1730    209      13    0.713 0.02323        0.669        0.760
 2107    167      12    0.668 0.02514        0.621        0.719
 2465    126       9    0.628 0.02702        0.577        0.683
 2852     84      10    0.569 0.03032        0.512        0.632
 3524     42      10    0.478 0.03680        0.411        0.556
 4795      1       8    0.353 0.04876        0.270        0.463</pre></div><p class="calibre7">We have now obtained the<a id="id466" class="calibre1"/> Kaplan-Meier estimates at each of the decile time points, the standard error, and the confidence interval at each of the points. Using the <code class="literal">plot</code> function, we will now visualize the fitted Kaplan-Meier estimator for the <code class="literal">pbc</code> dataset: </p><div class="informalexample"><pre class="programlisting">&gt; plot(pbc_sf,xlab="Time",ylab="Survival Function Confidence Bands")</pre></div><div class="mediaobject"><img src="../images/00444.jpeg" alt="Nonparametric inference" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: Kaplan-Meier estimator for the PBC dataset</p></div></div><p class="calibre11"> </p><p class="calibre7">Now, if you look at the time at which the survival time becomes nearly 0.5, the earlier answer of median survival time being 3395 becomes clear enough. Next, we look at the cumulative hazard function.</p><p class="calibre7">To obtain the<a id="id467" class="calibre1"/> cumulative hazard function, we will apply the <code class="literal">coxph</code> function on the survival object and use the <code class="literal">basehaz</code> function to get the baseline cumulative hazard function, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_na &lt;- basehaz(coxph(Surv(time,status==2)~1,pbc))
&gt; pbc_na
         hazard time
1   0.004790426   41
2   0.007194272   43
3   0.009603911   51
4   0.012019370   71

396 1.030767970 4509
397 1.030767970 4523
398 1.030767970 4556
399 1.030767970 4795</pre></div><p class="calibre7">We will use the following code to create a visual display of the Nelson-Aalen estimator: </p><div class="informalexample"><pre class="programlisting">&gt; plot(pbc_na$time,pbc_na$hazard,"l",xlab="Time",ylab="Cumulative Hazard Function")</pre></div><p class="calibre7">The following graph illustrates the Nelson-Aalen estimator:</p><div class="mediaobject"><img src="../images/00445.jpeg" alt="Nonparametric inference" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: The Nelson-Aalen estimator for the cumulative hazard function</p></div></div><p class="calibre11"> </p><p class="calibre7">Note that <a id="id468" class="calibre1"/>one might be tempted to use the <span class="strong"><img src="../images/00446.jpeg" alt="Nonparametric inference" class="calibre15"/></span> relationship to obtain the Kaplan-Meier estimator, or vice versa. Let us check it out using the following code:</p><div class="informalexample"><pre class="programlisting">&gt; str(exp(-pbc_na$hazard))
 num [1:399] 0.995 0.993 0.99 0.988 0.986 ...
&gt; str(summary(pbc_sf,times=pbc_na$time)$surv)
 num [1:399] 0.995 0.993 0.99 0.988 0.986 ...</pre></div><p class="calibre7">Everything seems to be right here, so let's check it in its entirety:</p><div class="informalexample"><pre class="programlisting">&gt; round(exp(-pbc_na$hazard),4)==round(summary(pbc_sf,
+ times=pbc_na$time)$surv,4)
  [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
 [12]  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE FALSE
 [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
 [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
 
[375] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[386] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[397] FALSE FALSE FALSE</pre></div><p class="calibre7">After a certain length of time, the estimates will differ vastly, and hence we compute these two quantities separately. Next, we will look at how to carry out statistical tests to compare the equality of survival curves.</p><p class="calibre7">As noted<a id="id469" class="calibre1"/> previously, we are feeding a formula to the <code class="literal">survfit</code> function. It appeared as an extra specification <code class="literal">'~1'</code> in the <code class="literal">Surv</code> formula. As the formula is essential for further analysis of survival data, we can now make good use of this framework. If we replace 1 with a categorical variable such as sex, then we will then obtain survival curves for each level of the categorical variable. For the <code class="literal">pbc</code> data, we will plot the survival curves. The Kaplan-Meier estimates for males and females separately.</p><div class="informalexample"><pre class="programlisting">&gt;plot(survfit(Surv(time,status==2)~sex,pbc),conf.int=TRUE,xlab="Time",+      ylab="Survival Probability", col=c("red","blue"))</pre></div><div class="mediaobject"><img src="../images/00447.jpeg" alt="Nonparametric inference" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Gender-wise comparison of the survival curves for the PBC data</p></div></div><p class="calibre11"> </p><p class="calibre7">The <a id="id470" class="calibre1"/>survival curves (indicated by the blue and red continuous lines) clearly show differences and we need to evaluate whether the observed difference is statistically significant. To that end, we apply the <code class="literal">survdiff</code> function and check if the difference is significant, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; survdiff(Surv(time,status==2)~sex,pbc)
Call:
survdiff(formula = Surv(time, status == 2) ~ sex, data = pbc)

        N Observed Expected (O-E)^2/E (O-E)^2/V
sex=m  44       24     17.3     2.640      2.98
sex=f 374      137    143.7     0.317      2.98
 Chisq= 3  on 1 degrees of freedom, p= 0.0845 </pre></div><p class="calibre7">The p-value is <code class="literal">0.0845</code>, and hence if the chosen significance is 95%, we conclude that the difference is insignificant.</p><p class="calibre7">A note to the reader: the significance level is pre-determined. If you have fixed it at 95% before carrying out the analysis and then look at the p-value and find that it is between 0.05 and 0.10, don't change the level. Stick to what was agreed on earlier.</p><p class="calibre7">In the analysis<a id="id471" class="calibre1"/> thus far, we have looked at parametric and nonparametric methods, and now we need to develop a larger framework. The impact of covariates needs to be evaluated clearly, and we will explore this topic in the following section.</p></div>
<div class="book" title="Regression models &#x2013; parametric and Cox proportional hazards models"><div class="book" id="24L8G2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec76" class="calibre1"/>Regression models – parametric and Cox proportional hazards models</h1></div></div></div><p class="calibre7">You may recall that the survival data <a id="id472" class="calibre1"/>consists of complete as well as censored observations, and we saw that the lifetimes look like 400, 4500+, 1012, 1925, 1504+, … for the <code class="literal">pbc</code> dataset. Although<a id="id473" class="calibre1"/> the lifetimes are continuous random variables, a regression model of the form <span class="strong"><img src="../images/00448.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> will not be appropriate here. In fact, there were many attempts to correct and improvise on models of this form in the 1970s, and most often the results<a id="id474" class="calibre1"/> were detrimental. We will define<a id="id475" class="calibre1"/> a generic <span class="strong"><strong class="calibre8">hazards regression model</strong></span> as follows:</p><div class="mediaobject"><img src="../images/00449.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Here, <span class="strong"><em class="calibre9">t</em></span> is the lifetime, <span class="strong"><img src="../images/00450.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is the lifetime indicator, <span class="strong"><img src="../images/00451.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is the covariate vector, <span class="strong"><img src="../images/00452.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is the vector of regression coefficients, and <span class="strong"><img src="../images/00453.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is the baseline hazard rate. A relative risks model that is of specific interest is the following:</p><div class="mediaobject"><img src="../images/00454.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">We will focus solely on this class of model. First, the parametric hazards regression is considered. This means that we will specify the hazard rate <span class="strong"><img src="../images/00455.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> through a parametric model, for example, through exponential distribution. But what does this mean? It means that the baseline hazard function is of the following form:</p><div class="mediaobject"><img src="../images/00456.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Consequently, the hazards regression model is as follows:</p><div class="mediaobject"><img src="../images/00457.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The estimation problem is then to find <span class="strong"><img src="../images/00458.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> and <span class="strong"><img src="../images/00459.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span>. The R function <code class="literal">survreg</code> from the <code class="literal">flexsurv</code> package will be useful to fit the parametric hazards regression model. It is demonstrated as continuity on the <code class="literal">pbc</code> dataset. The <code class="literal">survival</code> formula will be extended to include all the covariates in the model, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_Exp &lt;- survreg(Surv(time,status==2)~trt + age + sex + ascites 
+                      hepato + spiders + edema + bili + chol + albumin
+                      copper + alk.phos + ast + trig + platelet + 
+                      protime + stage,
+                    dist="exponential", pbc)
&gt; pbc_Exp_summary &lt;- summary(pbc_Exp)
&gt; pbc_Exp_summary
Call:
survreg(formula = Surv(time, status == 2) ~ trt + age + sex + 
    ascites + hepato + spiders + edema + bili + chol + albumin + 
    copper + alk.phos + ast + trig + platelet + protime + stage, 
    data = pbc, dist = "exponential")
                Value Std. Error      z        p
(Intercept)  1.33e+01   1.90e+00  7.006 2.46e-12
trt          6.36e-02   2.08e-01  0.305 7.60e-01
age         -2.81e-02   1.13e-02 -2.486 1.29e-02
sexf         3.42e-01   3.00e-01  1.140 2.54e-01
ascites     -1.01e-01   3.70e-01 -0.273 7.85e-01
hepato      -7.76e-02   2.43e-01 -0.320 7.49e-01
spiders     -1.21e-01   2.36e-01 -0.513 6.08e-01
edema       -8.06e-01   3.78e-01 -2.130 3.32e-02
bili        -4.80e-02   2.49e-02 -1.929 5.37e-02
chol        -4.64e-04   4.35e-04 -1.067 2.86e-01
albumin      4.09e-01   2.87e-01  1.427 1.54e-01
copper      -1.63e-03   1.11e-03 -1.466 1.43e-01
alk.phos    -4.51e-05   3.81e-05 -1.182 2.37e-01
ast         -3.68e-03   1.92e-03 -1.917 5.52e-02
trig         1.70e-04   1.37e-03  0.124 9.01e-01
platelet    -2.02e-04   1.16e-03 -0.174 8.62e-01
protime     -2.53e-01   9.78e-02 -2.589 9.61e-03
stage       -3.66e-01   1.66e-01 -2.204 2.75e-02
Scale fixed at 1 

Exponential distribution
Loglik(model)= -984.1   Loglik(intercept only)= -1054.6
Chisq= 141.17 on 17 degrees of freedom, p= 0 
Number of Newton-Raphson Iterations: 5 
n=276 (142 observations deleted due to missingness)</pre></div><p class="calibre7">Here, the <a id="id476" class="calibre1"/>convergence occurs after five iterations. The p-value is nearly equal to zero, which implies that the fitted model is significant. However, not all the p-values associated with the covariates indicate significance. We will use the following code to find it:</p><div class="informalexample"><pre class="programlisting">&gt; round(pbc_Exp_summary$table[,4],4)
(Intercept)         trt         age        sexf     ascites      hepato 
     0.0000      0.7601      0.0129      0.2541      0.7846      0.7491 
    spiders       edema        bili        chol     albumin      copper 
     0.6083      0.0332      0.0537      0.2859      0.1536      0.1426 
   alk.phos         ast        trig    platelet     protime       stage 
     0.2372      0.0552      0.9010      0.8621      0.0096      0.0275
&gt; AIC(pbc_exp)
[1] 3065.187</pre></div><p class="calibre7">The AIC value is also very high, and<a id="id477" class="calibre1"/> we try to see if this can be improved on. Hence, we slap the <code class="literal">step</code> function on the fitted exponential hazards regression model and eliminate the covariates that are insignificant, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_Exp_eff &lt;- step(pbc_Exp)
Start:  AIC=2004.12
Surv(time, status == 2) ~ trt + age + sex + ascites + hepato + 
    spiders + edema + bili + chol + albumin + copper + alk.phos + 
    ast + trig + platelet + protime + stage

           Df    AIC
- ascites   1 2002.2
- trt       1 2002.2
- hepato    1 2002.2
- spiders   1 2002.4
- chol      1 2003.2
- sex       1 2003.4
- alk.phos  1 2003.4
- albumin   1 2004.1
&lt;none&gt;        2004.1
- ast       1 2005.5
- bili      1 2005.5
- edema     1 2006.5
- stage     1 2007.2
- protime   1 2008.0
- age       1 2008.3
- trig      1 2020.4
- copper    1 2020.7
- platelet  1 2021.8

Step:  AIC=2002.19
Surv(time, status == 2) ~ trt + age + sex + hepato + spiders + 
    edema + bili + chol + albumin + copper + alk.phos + ast + 
    trig + platelet + protime + stage


Step:  AIC=1994.61
Surv(time, status == 2) ~ age + edema + bili + albumin + copper + 
    ast + trig + platelet + protime + stage

           Df    AIC
&lt;none&gt;        1994.6
- albumin   1 1995.5
- edema     1 1996.3
- ast       1 1996.9
- bili      1 1998.8
- protime   1 1999.2
- stage     1 1999.7
- age       1 2000.9
- platelet  1 2012.1
- copper    1 2014.9
- trig      1 2198.7
There were 50 or more warnings (use warnings() to see the first 50)
&gt; pbc_Exp_eff_summary &lt;- summary(pbc_Exp_eff)
&gt; round(pbc_Exp_eff_summary$table[,4],4)
(Intercept)         age       edema        bili     albumin      copper 
     0.0000      0.0037      0.0507      0.0077      0.0849      0.0170 
        ast        trig    platelet     protime       stage 
     0.0281      0.9489      0.7521      0.0055      0.0100 
&gt; AIC(pbc_Exp_eff)
[1] 1994.607</pre></div><p class="calibre7">We see here that all the covariates in the current model are significant, except the <code class="literal">trig</code> and <code class="literal">platelet</code> variables. The AIC value has also decreased drastically.</p><p class="calibre7">Parametric models are often not acceptable in life sciences. A flexible framework for <span class="strong"><img src="../images/00460.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is the famous Cox proportional hazards model. It is a semi-parametric regression model in that the baseline<a id="id478" class="calibre1"/> hazard function <span class="strong"><img src="../images/00461.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is completely unspecified. Cox (1972) proposed this model, which is one of the most important models in statistics. The only requirement of the baseline hazard function <span class="strong"><img src="../images/00462.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is that it must be non-negative and the associated probability distribution with it must be a <span class="strong"><em class="calibre9">proper</em></span> probability distribution. In this model, the regression coefficient vector <span class="strong"><img src="../images/00463.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre15"/></span> is estimated by treating the baseline hazard function as a nuisance factor. Its inference is based on the<a id="id479" class="calibre1"/> important notion of partial likelihood function; see Cox (1975) for complete details. Here, we will only specify the form of the Cox proportional hazards <a id="id480" class="calibre1"/>model and refer the interested reader to Kalbfleisch and Prentice (2002):</p><div class="mediaobject"><img src="../images/00464.jpeg" alt="Regression models – parametric and Cox proportional hazards models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">We will use the <code class="literal">coxph</code> function from the <code class="literal">survival</code> package to fit the proportional hazards regression model. For <a id="id481" class="calibre1"/>some technical reason, we will have to omit all rows from the <code class="literal">pbc</code> dataset which have missing values, and the remaining steps parallel the fitting of the exponential hazards regression model:</p><div class="informalexample"><pre class="programlisting">&gt; pbc2 &lt;- na.omit(pbc)
&gt; pbc_coxph &lt;- coxph(Surv(time,status==2)~trt + age + sex + ascites 
+                      hepato + spiders + edema + bili + chol + albumin 
+                      copper + alk.phos + ast + trig + platelet + 
+                      protime + stage,                   pbc2)
&gt; pbc_coxph_summary &lt;- summary(pbc_coxph)
&gt; pbc_coxph_summary
Call:
coxph(formula = Surv(time, status == 2) ~ trt + age + sex + ascites + 
    hepato + spiders + edema + bili + chol + albumin + copper + 
    alk.phos + ast + trig + platelet + protime + stage, data = pbc2)

  n= 276, number of events= 111 

               coef  exp(coef)   se(coef)      z Pr(&gt;|z|)   
trt      -1.242e-01  8.832e-01  2.147e-01 -0.579  0.56290   
age       2.890e-02  1.029e+00  1.164e-02  2.482  0.01305 * 
sexf     -3.656e-01  6.938e-01  3.113e-01 -1.174  0.24022   
ascites   8.833e-02  1.092e+00  3.872e-01  0.228  0.81955   
hepato    2.552e-02  1.026e+00  2.510e-01  0.102  0.91900   
spiders   1.012e-01  1.107e+00  2.435e-01  0.416  0.67760   
edema     1.011e+00  2.749e+00  3.941e-01  2.566  0.01029 * 
bili      8.001e-02  1.083e+00  2.550e-02  3.138  0.00170 **
chol      4.918e-04  1.000e+00  4.442e-04  1.107  0.26829   
albumin  -7.408e-01  4.767e-01  3.078e-01 -2.407  0.01608 * 
copper    2.490e-03  1.002e+00  1.170e-03  2.128  0.03337 * 
alk.phos  1.048e-06  1.000e+00  3.969e-05  0.026  0.97893   
ast       4.070e-03  1.004e+00  1.958e-03  2.078  0.03767 * 
trig     -9.758e-04  9.990e-01  1.333e-03 -0.732  0.46414   
platelet  9.019e-04  1.001e+00  1.184e-03  0.762  0.44629   
protime   2.324e-01  1.262e+00  1.061e-01  2.190  0.02850 * 
stage     4.545e-01  1.575e+00  1.754e-01  2.591  0.00958 ** ---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

         exp(coef) exp(-coef) lower .95 upper .95
trt         0.8832     1.1323    0.5798    1.3453
age         1.0293     0.9715    1.0061    1.0531
sexf        0.6938     1.4414    0.3769    1.2771
ascites     1.0924     0.9155    0.5114    2.3332
hepato      1.0259     0.9748    0.6273    1.6777
spiders     1.1066     0.9037    0.6865    1.7835
edema       2.7487     0.3638    1.2697    5.9505
bili        1.0833     0.9231    1.0305    1.1388
chol        1.0005     0.9995    0.9996    1.0014
albumin     0.4767     2.0977    0.2608    0.8714
copper      1.0025     0.9975    1.0002    1.0048
alk.phos    1.0000     1.0000    0.9999    1.0001
ast         1.0041     0.9959    1.0002    1.0079
trig        0.9990     1.0010    0.9964    1.0016
platelet    1.0009     0.9991    0.9986    1.0032
protime     1.2617     0.7926    1.0247    1.5534
stage       1.5754     0.6348    1.1170    2.2219

Concordance= 0.849  (se = 0.031 )
Rsquare= 0.455   (max possible= 0.981 )
Likelihood ratio test= 167.7  on 17 df,   p=0
Wald test            = 174.1  on 17 df,   p=0
Score (logrank) test = 283.7  on 17 df,   p=0

&gt; round(pbc_coxph_summary$coefficients[,5],4)
     trt      age     sexf  ascites   hepato  spiders    edema     bili 
  0.5629   0.0131   0.2402   0.8195   0.9190   0.6776   0.0103   0.0017 
    chol  albumin   copper alk.phos      ast     trig platelet  protime 
  0.2683   0.0161   0.0334   0.9789   0.0377   0.4641   0.4463   0.0285 
   stage 
  0.0096
&gt; AIC(pbc_coxph)
[1] 966.6642</pre></div><p class="calibre7">Since we find that a lot of <a id="id482" class="calibre1"/>variables are insignificant, we will attempt to improve on it by using the <code class="literal">step</code> function and then calculating the improved AIC value, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_coxph_eff &lt;- step(pbc_coxph)
Start:  AIC=966.66
Surv(time, status == 2) ~ trt + age + sex + ascites + hepato + 
    spiders + edema + bili + chol + albumin + copper + alk.phos + 
    ast + trig + platelet + protime + stage
           Df    AIC
- alk.phos  1 964.66
- hepato    1 964.67
- ascites   1 964.72
- spiders   1 964.84
- trt       1 965.00
- trig      1 965.22
- platelet  1 965.24
- chol      1 965.82
- sex       1 965.99
&lt;none&gt;        966.66
- ast       1 968.69
- copper    1 968.85
- protime   1 968.99
- albumin   1 970.35
- age       1 970.84
- edema     1 971.00
- stage     1 971.83
- bili      1 973.34


Step:  AIC=952.58
Surv(time, status == 2) ~ age + edema + bili + albumin + copper + 
    ast + protime + stage

          Df    AIC
&lt;none&gt;       952.58
- protime  1 955.06
- ast      1 955.79
- edema    1 955.95
- albumin  1 957.27
- copper   1 958.18
- age      1 959.97
- stage    1 960.11
- bili     1 966.57
&gt; pbc_coxph_eff_summary &lt;- summary(pbc_coxph_eff)
&gt; pbc_coxph_eff_summary
Call:
coxph(formula = Surv(time, status == 2) ~ age + edema + bili + 
    albumin + copper + ast + protime + stage, data = pbc2)
  n= 276, number of events= 111 
              coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    
age      0.0313836  1.0318812  0.0102036  3.076  0.00210 ** 
edema    0.8217952  2.2745795  0.3471465  2.367  0.01792 *  
bili     0.0851214  1.0888492  0.0193352  4.402 1.07e-05 ***
albumin -0.7185954  0.4874364  0.2724486 -2.638  0.00835 ** 
copper   0.0028535  1.0028576  0.0009832  2.902  0.00370 ** 
ast      0.0043769  1.0043865  0.0018067  2.423  0.01541 *  
protime  0.2275175  1.2554794  0.1013729  2.244  0.02481 *  
stage    0.4327939  1.5415584  0.1456307  2.972  0.00296 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

        exp(coef) exp(-coef) lower .95 upper .95
age        1.0319     0.9691    1.0114    1.0527
edema      2.2746     0.4396    1.1519    4.4915
bili       1.0888     0.9184    1.0484    1.1309
albumin    0.4874     2.0515    0.2858    0.8314
copper     1.0029     0.9972    1.0009    1.0048
ast        1.0044     0.9956    1.0008    1.0079
protime    1.2555     0.7965    1.0292    1.5314
stage      1.5416     0.6487    1.1588    2.0508

Concordance= 0.845  (se = 0.031 )
Rsquare= 0.448   (max possible= 0.981 )
Likelihood ratio test= 163.8  on 8 df,   p=0
Wald test            = 176.1  on 8 df,   p=0
Score (logrank) test = 257.5  on 8 df,   p=0
&gt; round(pbc_coxph_eff_summary$coefficients[,5],4)
    age   edema    bili albumin  copper     ast protime   stage 
 0.0021  0.0179  0.0000  0.0084  0.0037  0.0154  0.0248  0.0030 
&gt; AIC(pbc_coxph_eff)
[1] 952.5814</pre></div><p class="calibre7">We can now see that almost all the variables in the <code class="literal">pbc_coxph_eff</code> model are significant. The AIC value has also decreased from its earlier value.</p><p class="calibre7">In most survival data analyses, the<a id="id483" class="calibre1"/> purpose is to find the effective covariates and their impact on the hazard rate. Accuracy measures, such as AUC with classification problems, do not exist in the context of survival data. On similar (though important) lines, the prediction of the lifetime for a given choice of covariates might not again be of importance. In <a class="calibre1" title="Chapter 2. Bootstrapping" href="part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee">Chapter 2</a>, <span class="strong"><em class="calibre9">Bootstrapping</em></span>, we looked at the application of pseudo values and the jackknife methods that provided ease of interpretation. We will look at a different approach in the following section.</p></div>
<div class="book" title="Survival tree" id="25JP21-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec77" class="calibre1"/>Survival tree</h1></div></div></div><p class="calibre7">The parametric hazards<a id="id484" class="calibre1"/> regression model is sometimes seen as a restrictive class of models by practitioners, and the Cox proportional hazards regression is sometimes preferred over its parametric counterpart. Compared with the parametric models, the interpretation is sometimes lost, and the regular practitioner finds it difficult to connect with the hazards regression model. Of course, an alternative is to build a survival tree over the pseudo observations. Such an attempt can be seen in Tattar's (2016) unpublished paper. Gordon and Olshen (1985) made the first attempt to build a survival tree and many scientists have continued constructing it. LeBlanc and Crowley (1992) are among the most important contributors to set up a survival tree. Zhang and Singer (2010) have also given a systematic development of related methods, and chapters 7-10 of their book deal with survival trees. The basic premise remains the same, and we need good splitting criteria in order to create the survival tree.</p><p class="calibre7">LeBlanc and Crowley<a id="id485" class="calibre1"/> proposed the splitting criteria based on the node deviance measure between a saturated model log-likelihood and the maximum log-likelihood, and then the unknown full likelihood is approximated by replacing the baseline cumulative hazard function as estimated by the Nelson-Aalen estimator. As suggested by Hamad et al. (2011), note that the advantage of this method is that any software that can carry out the implementation of Poisson trees would be able to create the survival tree. This approach has been exploited in the <code class="literal">rpart</code> R package created by Therneau and Atkinson (2017). The terminal nodes of the survival tree can be summarized by the Kaplan-Meier survival function for the observations belonging in the node.</p><p class="calibre7">We will now set up the survival tree for the <code class="literal">pbc</code> dataset using the <code class="literal">rpart</code> library:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_stree &lt;- rpart(Surv(time,status==2)~trt + age + sex + ascites + 
+                      hepato + spiders + edema + bili + chol + albumin + 
+                      copper + alk.phos + ast + trig + platelet + 
+                      protime + stage,
+                    pbc)
&gt; pbc_stree
n= 418 

node), split, n, deviance, yval
      * denotes terminal node

 1) root 418 555.680700 1.0000000  
   2) bili&lt; 2.25 269 232.626500 0.4872828  
     4) age&lt; 51.24162 133  76.376990 0.2395736  
       8) alk.phos&lt; 1776 103  30.340440 0.1036750 *
       9) alk.phos&gt;=1776 30  33.092480 0.6135695 *
     5) age&gt;=51.24162 136 136.800300 0.7744285  
      10) protime&lt; 10.85 98  80.889260 0.5121123 *
      11) protime&gt;=10.85 38  43.381670 1.4335430  
        22) age&lt; 65.38125 26  24.045820 0.9480269  
          44) bili&lt; 0.75 8   5.188547 0.3149747 *
          45) bili&gt;=0.75 18  12.549130 1.3803650 *
        23) age&gt;=65.38125 12   8.392462 3.2681510 *
   3) bili&gt;=2.25 149 206.521900 2.6972690  
     6) protime&lt; 11.25 94  98.798830 1.7717830  
      12) stage&lt; 3.5 57  56.734150 1.2620350  
        24) age&lt; 43.5332 25  16.656000 0.6044998 *
        25) age&gt;=43.5332 32  32.986760 1.7985470 *
      13) stage&gt;=3.5 37  32.946240 2.8313470 *
     7) protime&gt;=11.25 55  76.597760 5.1836880  
      14) ascites&lt; 0.5 41  52.276360 4.1601690  
        28) age&lt; 42.68172 7   6.829564 1.4344660 *
        29) age&gt;=42.68172 34  37.566600 5.1138380 *
      15) ascites&gt;=0.5 14  17.013010 7.9062910 *</pre></div><p class="calibre7">A text display of the survival tree is displayed by running <code class="literal">pbc_stree</code> at the console. The asterisk (<code class="literal">*</code>) at the end of the <code class="literal">yval</code> for a node indicates censorship.</p><p class="calibre7">We will<a id="id486" class="calibre1"/> now look at the <code class="literal">cptable</code> and variable importance associated with the survival tree, as shown in the following program:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_stree$cptable
           CP nsplit rel error    xerror       xstd
1  0.20971086      0 1.0000000 1.0026398 0.04761402
2  0.05601298      1 0.7902891 0.8179390 0.04469359
3  0.03500063      2 0.7342762 0.8198147 0.04731189
4  0.02329408      3 0.6992755 0.8247148 0.04876085
5  0.02254786      4 0.6759815 0.8325230 0.05132829
6  0.01969366      5 0.6534336 0.8363125 0.05160507
7  0.01640950      6 0.6337399 0.8373375 0.05268262
8  0.01366665      7 0.6173304 0.8336743 0.05406099
9  0.01276163      9 0.5899971 0.8209714 0.05587843
10 0.01135209     10 0.5772355 0.8248827 0.05612403
11 0.01000000     11 0.5658834 0.8255873 0.05599763
&gt; pbc_stree$variable.importance
      bili    protime        age    albumin      edema      stage 
138.007841  70.867308  54.548224  32.239919  25.576170  15.231256 
   ascites   alk.phos   platelet        sex     copper        ast 
 14.094208  13.440866  10.017966   2.452776   2.114888   1.691910 </pre></div><p class="calibre7">The variable importance shows that the <code class="literal">bili</code> variable is the most important, followed by <code class="literal">protime</code> and <code class="literal">age</code>. We conclude this section with a visual depiction of the survival tree, using the following code:</p><div class="informalexample"><pre class="programlisting">&gt; windows(height=100,width=60)
&gt; plot(pbc_stree,uniform = TRUE)
&gt; text(pbc_stree,use.n=TRUE)</pre></div><div class="mediaobject"><img src="../images/00465.jpeg" alt="Survival tree" class="calibre10"/><div class="caption"><p class="calibre14">Figure 6: Survival tree for the PBC dataset</p></div></div><p class="calibre11"> </p><p class="calibre7">As with the problem of overfitting with a single tree, we need to look at an important alternative method of ensembles of survival trees.</p></div>
<div class="book" title="Ensemble survival models" id="26I9K1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec78" class="calibre1"/>Ensemble survival models</h1></div></div></div><p class="calibre7">The random forest package <code class="literal">randomForestSRC</code> will continue to be useful for creating the random forests <a id="id487" class="calibre1"/>associated with the survival data. In fact, the <code class="literal">S</code> of <code class="literal">SRC</code> in the package name stands for survival! The usage of the <code class="literal">rfsrc</code> function remains the same as in previous chapters, and we will now give it a <code class="literal">Surv</code> object, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_rf &lt;- rfsrc(Surv(time,status==2)~trt + age + sex + ascites + 
+                 hepato + spiders + edema + bili + chol + albumin+
+                 copper + alk.phos + ast +trig + platelet + protime+
+                 stage, ntree=500, tree.err = TRUE, pbc)</pre></div><p class="calibre7">We will find some of the basic settings that have gone into setting up this random forest:</p><div class="informalexample"><pre class="programlisting">&gt; pbc_rf$splitrule
[1] "logrankCR"
&gt; pbc_rf$nodesize
[1] 6
&gt; pbc_rf$mtry
[1] 5</pre></div><p class="calibre7">Thus, the splitting criteria is based on the log-rank test, the minimum number of observations in a terminal node is six, and the number of variables considered at random for each split is five.</p><p class="calibre7">Next, we will find the variable importance for both events and apply the function <code class="literal">var.select</code> as we have done in earlier chapters. We will then show a part of the iteration run using the<a id="id488" class="calibre1"/> following code:</p><div class="informalexample"><pre class="programlisting">&gt; vimp(pbc_rf)$importance
               event.1      event.2
trt       0.0013257796 0.0002109143
age       0.0348848966 0.0166352983
sex       0.0007755091 0.0004011303
ascites   0.0008513276 0.0107212361
hepato    0.0050666763 0.0015445001
spiders  -0.0001136547 0.0003552415
edema     0.0006227470 0.0147982184
bili      0.0696654202 0.0709713627
chol      0.0002483833 0.0107024051
albumin  -0.0106392917 0.0115188264
copper    0.0185417386 0.0255099568
alk.phos  0.0041407841 0.0022297323
ast       0.0029317937 0.0063469825
trig     -0.0040190463 0.0014371745
platelet  0.0021021396 0.0002388797
protime   0.0057968358 0.0133710339
stage    -0.0023944666 0.0042808034
&gt; var.select(pbc_rf, method = "vh.vimp", nrep = 50)
---------------------  Iteration: 1   ---------------------
 selecting variables using Variable Hunting (VIMP) ...
 PE: 38.8889      dim: 2                                        
---------------------  Iteration: 2   ---------------------
 selecting variables using Variable Hunting (VIMP) ...
 PE: 23.6842      dim: 2                                        
---------------------  Iteration: 3   ---------------------
 selecting variables using Variable Hunting (VIMP) ...
 PE: 50.6667      dim: 1                                  
      
      

 
---------------------  Iteration: 48   ---------------------
 selecting variables using Variable Hunting (VIMP) ...
 PE: 38.3562      dim: 2                                        
---------------------  Iteration: 49   ---------------------
 selecting variables using Variable Hunting (VIMP) ...
 PE: 19.4737      dim: 2                                        
---------------------  Iteration: 50   ---------------------
 selecting variables using Variable Hunting (VIMP) ...
 PE: 55.914      dim: 2                                         
fitting forests to final selected variables ...


-----------------------------------------------------------
family             : surv-CR 
var. selection     : Variable Hunting (VIMP) 
conservativeness   : medium 
dimension          : 17 
sample size        : 276 
K-fold             : 5 
no. reps           : 50 
nstep              : 1 
ntree              : 500 
nsplit             : 10 
mvars              : 4 
nodesize           : 2 
refitted forest    : TRUE 
model size         : 2.16 +/- 0.6809 
PE (K-fold)        : 43.3402 +/- 17.7472 


Top variables:
       rel.freq
bili         28
hepato       24
ast          24
-----------------------------------------------------------</pre></div><p class="calibre7">Next, the error rate plot as the number of trees increases will be depicted using the following code:</p><div class="informalexample"><pre class="programlisting">&gt; plot(pbc_rf,plots.one.page = TRUE)</pre></div><div class="mediaobject"><img src="../images/00466.jpeg" alt="Ensemble survival models" class="calibre10"/><div class="caption"><p class="calibre14">Figure 7: Random forest error rates for the two events</p></div></div><p class="calibre11"> </p><p class="calibre7">Consequently, we have<a id="id489" class="calibre1"/> now seen the construction and setup of random forests for the survival data.</p></div>
<div class="book" title="Summary" id="27GQ61-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch10lvl1sec79" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">Survival data is different to typical regression data, and the incomplete observations pose a challenge. Since the data structure is completely different, we need specialized techniques to handle the incomplete observations and to that end, we introduced core survival concepts, such as hazard rate and survival function. We then introduced parametric lifetime models, which gives us a brief peek at how the lifetime distribution should look. We even fitted these lifetime distributions into the pbc dataset.</p><p class="calibre7">We also learned that the parametric setup might be very restrictive, and hence considered the nonparametric methods of the estimation of survival quantities. We also demonstrated the utility of the Nelson-Aalen estimator, the Kaplan-Meier survival function, and the log-rank test. The parametric hazards regression model was backed with the Cox proportional hazards regression model and applied to the pbc dataset. The logrank test can also help in the splitting criteria, and it has also been seen as the criteria for the random forest. Survival trees were demonstrated in the earlier section.</p><p class="calibre7">In the next chapter, we will look at another kind of data structure: time series data. The reader does not need to be familiar with time series analysis in order to follow the ensemble methods applied in it.</p></div></body></html>