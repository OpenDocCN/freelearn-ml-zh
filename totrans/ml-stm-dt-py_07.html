<html><head></head><body>
		<div id="_idContainer069">
			<h1 id="_idParaDest-98"><em class="italic"><a id="_idTextAnchor097"/>Chapter 5</em>: Online Anomaly Detection</h1>
			<p><a id="_idTextAnchor098"/>Anomaly detection is a good starting point for machine learning on streaming data. As streaming data delivers a continuous stream of data points, use cases of monitoring live solutions are among the first that come to mind.</p>
			<p>There are many domains in which monitoring is essential. In IT solutions, there is generally continuous logging of what happens in the systems, and those logs can be analyzed as streaming data. </p>
			<p>In the <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>), sensor data is being collected on sometimes a large number of sensors. This data is then analyzed and used in real time.</p>
			<p>Real-time and online anomaly detection can be of great added value in such use cases by finding values that are far from the expected range of measurements, or otherwise unexpected. Detecting them on time can have great value.</p>
			<p>In this chapter, you will first get an in-depth overview of anomaly detection and the theoretical considerations to take into account when implementing it. You will then see how to implement online anomaly detection using the River package in Python.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Defining anomaly detection</li>
				<li>Use cases of anomaly detection</li>
				<li>Comparing anomaly detection and imbalanced classification</li>
				<li>Algorithms for detecting anomalies in River</li>
				<li>Going further with anomaly detection</li>
			</ul>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor099"/>Technical requirements</h1>
			<p>You can find all the code for this book on GitHub at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python">https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python</a>. If you are not yet familiar with Git and GitHub, the easiest way to download the notebooks and code samples is the following:</p>
			<ol>
				<li>Go to the link of the repository.</li>
				<li>Go to the green <strong class="bold">Code</strong> button.</li>
				<li>Select <strong class="bold">Download ZIP</strong>.</li>
			</ol>
			<p>When you download the ZIP file, unzip it in your local environment, and you will be able to access the code through your preferred Python editor.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor100"/>Python environment</h2>
			<p>To <a id="_idIndexMarker265"/><a id="_idIndexMarker266"/>follow along with this book, you can download the code in the repository and execute it using your preferred Python editor.</p>
			<p>If you are not yet familiar with Python environments, I would advise you to check out Anaconda (<a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>), which comes with Jupyter Notebooks and JupyterLabs, which are both great for executing notebooks. It also comes with Spyder and VSCode for editing scripts and programs.</p>
			<p>If you have difficulty installing Python or the associated programs on your machine, you can check out Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or Kaggle Notebooks (<a href="https://www.kaggle.com/code">https://www.kaggle.com/code</a>), which both allow you to run Python code in online notebooks for free, without any setup to do.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor101"/>Defining anomaly detection</h1>
			<p>Let's start by<a id="_idIndexMarker267"/> creating an understanding of what <strong class="bold">anomaly detection</strong> is. Also called outlier detection, anomaly detection is the process of identifying rare observations in a dataset. Those<a id="_idIndexMarker268"/> rare observations are called <strong class="bold">outliers</strong> or <strong class="bold">anomalies</strong>.</p>
			<p>The goal of <a id="_idIndexMarker269"/>anomaly detection is to build models that can automatically detect outliers using statistical methods and/or machine learning. Such models can use multiple variables to see whether an observation should be considered an outlier or not.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Are outliers a problem?</h2>
			<p>Outliers <a id="_idIndexMarker270"/>occur in many datasets. After all, if you consider a variable that follows a<a id="_idIndexMarker271"/> normal distribution, it is normal to see data points far away from the mean. Let's consider a standard normal distribution (a normal distribution with mean <strong class="source-inline">0</strong> and standard deviation <strong class="source-inline">1</strong>):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-1</p>
			<pre class="source-code">import matplotlib.pyplot as plt</pre>
			<pre class="source-code">import numpy as np</pre>
			<pre class="source-code">import scipy.stats as stats</pre>
			<pre class="source-code">x = np.linspace(-4,4, 100)</pre>
			<pre class="source-code">plt.plot(x, stats.norm.pdf(x, 0, 1))</pre>
			<p>You can see the resulting figure as follows:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18335_05_1.jpg" alt="Figure 5.1 – The normal distribution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – The normal distribution</p>
			<p>This standard normal distribution has most of its observations around <strong class="source-inline">0</strong>. However, it is normal to observe some observations in the tails of the distribution. If you have a variable that really follows this distribution, and your sample size is big enough, having some observations far away from the center cannot really be considered something bad.</p>
			<p>In the<a id="_idIndexMarker272"/> following code, you see how a sample of 10 million observations is drawn from a standard normal distribution:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-2</p>
			<pre class="source-code">import numpy as np</pre>
			<pre class="source-code">import matplotlib.pyplot as plt</pre>
			<pre class="source-code">data = np.random.normal(size=10000000)</pre>
			<pre class="source-code">plt.hist(data, bins=25)</pre>
			<p>The data follows the normal curve quite well. You can see this in the following graph:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18335_05_2.jpg" alt="Figure 5.2 – The normal distribution histogram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – The normal distribution histogram</p>
			<p>Now, let's see what the highest and lowest values of this sample are by using the following code:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-3</p>
			<pre class="source-code">min(data), max(data)</pre>
			<p>In the current draw, a minimum of <strong class="source-inline">5.11</strong> and a maximum of <strong class="source-inline">5.12</strong> were observed. Now, are those outliers or not? The answer is complicated. Of course, the two values are perfectly<a id="_idIndexMarker273"/> within the range of the normal distribution. On the other hand, they are extreme values. </p>
			<p>This example illustrates that defining an outlier is not always easy, and needs careful consideration for your specific use case. We will now see a number of use cases of anomaly dete<a id="_idTextAnchor103"/>ction.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor104"/>Exploring use cases of anomaly detection</h1>
			<p>Before <a id="_idIndexMarker274"/>moving on to some specific algorithms for anomaly detection, let's first consider some use cases that are often done with anomaly detection.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/>Fraud detection in financial institutions</h2>
			<p>A very common<a id="_idIndexMarker275"/> use case for anomaly detection is the detection of fraud in financial institutions. Banks generally have a lot of data, as almost everyone has one or more bank accounts that are used on a regular basis. All these usages generate a huge amount of data that can help banks to improve their services and their profits. Fraud detection is a key component of data science applications in banks, together with many other use cases.</p>
			<p>A common use case for fraud detection is to automatically detect credit card fraud. Imagine that your card or card details have been stolen and someone is fraudulently using them. This leads to fraudulent transactions, which could be automatically detected by a machine learning algorithm. The bank could then automatically block your card and ask you to validate whether it was you, or someone fraudulently making these payments.</p>
			<p>This is both in the interest of the bank and of the user, so it is a great use case for anomaly detection. Other companies that work with credit card and payment data may also use these methods.</p>
			<p>Streaming models are great for fraud detection. There is generally a huge amount of data that comes in in a continuous stream of payments and other data. Streaming models <a id="_idIndexMarker276"/>allow you to take action directly when a fraud situation occurs, rather than waiting for the next batch to be launched.</p>
			<p>If you want to read more about fraud detection in financial institutions, you can check out the following links:</p>
			<ul>
				<li><a href="https://www.miteksystems.com/blog/how-does-machine-learning-help-with-fraud-detection-in-banks">https://www.miteksystems.com/blog/how-does-machine-learning-help-with-fraud-detection-in-banks</a> </li>
				<li><a href="https://www.sas.com/en_us/software/detection-investigation-for-banking.html">https://www.sas.com/en_us/software/detection-investigation-for-banking.html</a></li>
			</ul>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor106"/>Anomaly detection on your log data</h2>
			<p>A <a id="_idIndexMarker277"/>second use case for anomaly detection is log analysis. Many software applications generate huge amounts of logs containing all types of information on the execution of programs. These logs are often stored temporarily or long-term for further analysis. In some cases, these analyses may be manual searches of specific information about what happened at some point in software, but at other times they may be automated log treatment programs.</p>
			<p>One of the difficulties with anomaly detection in logs is that log data is generally very unstructured. Often, they are just a bunch of printed statements one after the other in a text file. It is very hard to make sense of this data.</p>
			<p>If you succeed in the challenge of structuring and categorizing your log data correctly, you can then use machine learning techniques to automatically detect problems with the execution of your software. This allows you to take action straight away.</p>
			<p>Using streaming analysis rather than batch analysis is important here as well. Some software is mission-critical, and downtime often means problems for the company. These can be different types of problems, including contractual problems and loss of revenue. If a company can automatically detect bugs, this allows them to move fast and quickly repair the problems. The faster a problem is repaired, the fewer problems for the company.</p>
			<p>For deeper use case literature on anomaly detection on log data, you can have a look at the following links:</p>
			<ul>
				<li><a href="https://www.zebrium.com/blog/using-machine-learning-to-detect-anomalies-in-logs">https://www.zebrium.com/blog/using-machine-learning-to-detect-anomalies-in-logs</a></li>
				<li><a href="https://arxiv.org/abs/2202.04301">https://arxiv.org/abs/2202.04301</a></li>
			</ul>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/>Fault detection in manufacturing and production lines</h2>
			<p>An<a id="_idIndexMarker278"/> example of fault detection in production lines is the business of industrial food production. Many production lines are almost fully automated, meaning that there is almost no human intervention between the input of raw products and the output of finalized products. The risk of this is that defects might be occurring that cannot be accepted as final products.</p>
			<p>The use of sensor data on production lines can strongly help in detecting anomalies in production. When a production line has some parameters that go wrong, sensors, in combination with streaming systems and real-time alerting systems, can allow you to stop the production of faulty products immediately. This can save a lot of money, as producing waste is very costly.</p>
			<p>Using streaming and real-time analytics here is also important. The longer you take to respond to a problem, the more waste you produce and the more money is lost. There is a huge return on investment to gain from implementing real-time and streaming analytics systems in manufacturing and production lines.</p>
			<p>The following links will allow you to learn more about this use case:</p>
			<ul>
				<li>https://www.scienced irect.com/science/article/pii/S2212827119301908</li>
				<li><a href="https://www.merl.com/publications/docs/TR2018-097.pdf">https://www.merl.com/publications/docs/TR2018-097.pdf</a></li>
			</ul>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/>Hacking detection in computer networks (cyber security)</h2>
			<p>Automated<a id="_idIndexMarker279"/> threat detection for cyber security is another great use case of anomaly detection. Just like the other use cases, positive occurrences are very rare compared to negative cases. The importance of those positive cases, however, is far more impactful than the negative ones.</p>
			<p>With recent developments, there is a much higher impact of cyber security problems and leaks for companies than before. Personal data can be sold for a large amount of money and hackers often try to steal this information thinking that they can remain anonymous behind their computers.</p>
			<p>Threat and anomaly detection <a id="_idIndexMarker280"/>systems are automated systems using machine learning to detect behavior that is not normal and that may represent intrusions. If companies can react quickly to such events happening, they can avoid large public shaming campaigns and potential lawsuits costing lots of money.</p>
			<p>Streaming and real-time systems are crucial here as well, as leaving as little time as possible for intruders to act will strongly reduce the risk of any cyber criminality happening in your organization.</p>
			<p>The following two articles give a good deep dive into such use cases:</p>
			<ul>
				<li><a href="https://securityboulevard.com/2021/07/what-is-anomaly-detection-in-cybersecurity/">https://securityboulevard.com/2021/07/what-is-anomaly-detection-in-cybersecurity/</a></li>
				<li><a href="https://www.xenonstack.com/insights/cyber-network-security">https://www.xenonstack.com/insights/cyber-network-security</a></li>
			</ul>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor109"/>Medical risks in health data</h2>
			<p>The <a id="_idIndexMarker281"/>medical world has seen a large number of inventions over the last years. Part of this is in personal tools such as smart watches and other connected health devices that allow you to measure your own health KPIs in real time. Other use cases can be found in hospitals and other professional health care applications.</p>
			<p>When anomalies occur in your health KPIs, it is often of utmost importance to intervene straight away. Health KPI signals can often occur even before we, as humans, start to notice that our health is deteriorating. Even if it is shortly after an event happens, the information will be able to get you the right care without spending much time looking for resources on the causes of your problem.</p>
			<p>In general, most of your health metrics will be good, or at least acceptable, until that one metric tells you that something is really going wrong and you need help. In such scenarios, it is important to work with streaming analytics rather than batch analytics. After all, if the data arrives the next hour or the next day, it may well be too late for you. This is another strong argument for using streaming analytics rather than batch analytics.</p>
			<p>You can read more about this over here:</p>
			<ul>
				<li><a href="https://medinform.jmir.org/2021/5/e27172/">https://medinform.jmir.org/2021/5/e27172/</a></li>
				<li><a href="https://arxiv.org/pdf/2012.02364.pdf">https://arxiv.org/pdf/2012.02364.pdf</a></li>
			</ul>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor110"/>Predictive maintenance and sensor data</h2>
			<p>The<a id="_idIndexMarker282"/> last use case that will be discussed here is the use case of predictive maintenance. Many companies have critical systems that need preventive maintenance; if something breaks, this will cost a lot of money or even worse.</p>
			<p>An example is the aviation industry. If an airplane crashes, this costs a lot of lives. Of course, no company can predict all anomalies, but any anomaly that could be detected before a crash happens would be a great win.</p>
			<p>Anomaly detections can be used for predictive maintenance in many sectors that have comparable problems; if you can predict that your critical system will fail soon, you can have just enough time to do maintenance on the part that needs it and avoid larger problems.</p>
			<p>Predictive maintenance can sometimes be done in batch, but it can also benefit from streaming. It all depends on the amount of time you have between detecting an anomaly and the intervention being needed.</p>
			<p>If you have a predictive maintenance model that predicts airplane engine failure between now and 30 minutes, you have a large need to get this data to your pilot as soon as possible. If you have predictive systems that tell you that a part needs changing in the coming month, you can probably use batch analytics as well.</p>
			<p>To read more about this use case, you can check out the following links:</p>
			<ul>
				<li><a href="https://www.knime.com/blog/anomaly-detection-for-predictive-maintenance-EDA">https://www.knime.com/blog/anomaly-detection-for-predictive-maintenance-EDA</a></li>
				<li><a href="https://www.e3s-conferences.org/articles/e3sconf/pdf/2020/30/e3sconf_evf2020_02007.pdf">https://www.e3s-conferences.org/articles/e3sconf/pdf/2020/30/e3sconf_evf2020_02007.pdf</a></li>
			</ul>
			<p>In the next section, you will see how anomaly detection models compare to imbalanced classification.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/>Comparing anomaly detection and imbalanced classification</h1>
			<p>For <a id="_idIndexMarker283"/>detecting positive cases against negative cases, the standard go-to family of methods would be classification. For the problems described, as long as you have historical data on at least a few positive and negative cases, you<a id="_idIndexMarker284"/> can use classification algorithms. However, you have a very common problem: there are only very few observations that are anomalies. This is a problem that is generally known as the problem of <strong class="bold">imbalanced data</strong>.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor112"/>The problem of imbalanced data</h2>
			<p>Imbalanced datasets <a id="_idIndexMarker285"/>are datasets in which the target class has very unevenly distributed occurrences. An often-occurring example is website sales: among 1,000 visitors, you often have at least 900 visitors that are just watching and browsing, as opposed to maybe 100 who actually buy something.</p>
			<p>Using classification methods carelessly on imbalanced data is prone to errors. Imagine that you fit a classification model that needs to predict for each website visitor whether they will buy something. If you create a very bad model that only predicts non-buying for every visitor, then you will still be right for 900 out of the 1,000 visitors and your accuracy metric will be 90%.</p>
			<p>There are a number of standard approaches against this imbalanced data, including using the F1 score and using SMOTE oversampling.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor113"/>The F1 score</h2>
			<p>The F1 score<a id="_idIndexMarker286"/> is a great replacement for the accuracy score in cases of unbalanced data. Accuracy is computed as the number of correct predictions divided by the total number of predictions made.</p>
			<p>This is the formula for accuracy:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/Formula_05_001.jpg" alt=""/>
				</div>
			</div>
			<p>The F1 score, however, takes <a id="_idIndexMarker287"/>into account the precision and recall of your model. The precision of a model is the percentage of predicted positives that are actually correct. The recall of your model shows the percentage of positives that you were actually able to detect.</p>
			<p>This is the formula of precision:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/Formula_05_002.jpg" alt=""/>
				</div>
			</div>
			<p>This is the formula of recall:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/Formula_05_003.jpg" alt=""/>
				</div>
			</div>
			<p>The F1 score combines those two into one metric, using the following formula:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/Formula_05_004.jpg" alt=""/>
				</div>
			</div>
			<p>Using this metric for evaluation, you will avoid interpreting very bad models as good models, especially in the case of imbalanced data.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/>SMOTE oversampling</h2>
			<p>SMOTE oversampling is<a id="_idIndexMarker288"/> the second method that you can use for counteracting imbalance in your data. It is a method that will create <em class="italic">fake</em> data points that strongly resemble the data points in your positive class. By creating a number of data points, your model will be able to learn much better about the positive class, and by using the original positives as the source, you guarantee that the newly generated data points are not too far off.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor115"/>Anomaly detection versus classification</h2>
			<p>Although<a id="_idIndexMarker289"/> imbalanced classification problems can sometimes work well for anomaly detection problems, there is a reason that anomaly detection is treated as a separate category of machine learning.</p>
			<p>The main difference is in the importance of understanding what the positive (anomaly) class looks like. In classification models, you want a model that is easily able to distinguish between two (positives and negatives) or more classes. For this to work, you want your model to learn what each class looks like. The model will search for variables that describe one class, and for other variables or values that describe the other class.</p>
			<p>In anomaly detection, you don't really care what the anomaly class looks like. What you need much more, is your model to learn what is <em class="italic">normal</em>. As long as your model has a very good understanding of the normal, negative class, it will be able to state normal versus abnormal quite well. This can be an anomaly in any direction and in any sense of the word. It is not needed for the model to have seen such a type of anomaly before, just to know that it is not normal.</p>
			<p>In the case of a first anomaly, a standard classification model would not know what this observation should be classified into. If you're lucky, it could go into the anomaly class, but you have no reason to believe it will. However, an anomaly detection model that focuses on what it knows versus what it does not know would be able to detect this anomaly as something that it has not seen before and, therefore, class it as an anomaly.</p>
			<p>In the next section, you will see a number of algorithms for anomaly detection that are available in Python's River package.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor116"/>Algorithms for detecting anomalies in River</h1>
			<p>In this chapter, you will again use River for online machine learning algorithms. There are other<a id="_idIndexMarker290"/> libraries out there, but River is a very promising candidate for being the go-to Python package for online learning (except for reinforcement learning).</p>
			<p>You will see two of<a id="_idIndexMarker291"/> the online machine learning algorithms for anomaly detection that River currently (version 0.9.0) contains, as follows:</p>
			<ul>
				<li><strong class="source-inline">OneClassSVM</strong>: An<a id="_idIndexMarker292"/> online adaptation of the offline version of One-Class SVM</li>
				<li><strong class="source-inline">HalfSpaceTrees</strong>: An <a id="_idIndexMarker293"/>online adaptation of Isolation Forests</li>
			</ul>
			<p>You will also see how to work with the constant thresholder and the quantile thresholder.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/>The use of thresholders in River anomaly detection</h2>
			<p>Let's first look at <a id="_idIndexMarker294"/>the use of thresholders, as they will be wrapped around the actual anomaly detection algorithms. </p>
			<p>Anomaly detection algorithms<a id="_idIndexMarker295"/> will generally return a score between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> to indicate to the model to what extent the observation is an anomaly. Scores closer to <strong class="source-inline">1</strong> are more likely to be an outlier, and scores closer to <strong class="source-inline">0</strong> are considered more normal.</p>
			<p>In practice, you need to decide on a threshold to state for each observation whether you expect it to be an outlier. To convert the continuous <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong> scale into a yes/no answer, you use a thresholder. </p>
			<h3>Constant thresholder</h3>
			<p>The <a id="_idIndexMarker296"/>constant thresholder is the simplest approach that you would intuitively come up with. You will give a <a id="_idIndexMarker297"/>constant value that will split observations with a continuous (<strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>) anomaly score into yes/no anomalies based on being higher or lower than the constant.</p>
			<p>As an example, if you specify a value of <strong class="source-inline">0.95</strong> to be your constant threshold, every observation with an anomaly higher than that will be considered an anomaly, and every data point that is scored lower than that is not considered an anomaly.</p>
			<h3>Quantile thresholder</h3>
			<p>The<a id="_idIndexMarker298"/> quantile thresholder<a id="_idIndexMarker299"/> is slightly more advanced. Rather than a constant, you specify a quantile. You have seen quantiles before in the chapter on descriptive statistics. A <strong class="source-inline">0.95</strong> quantile means that 95% of the observations are below this value and 5% of the observations are above it.</p>
			<p>Imagine<a id="_idIndexMarker300"/> that you used a constant threshold of <strong class="source-inline">0.95</strong>, but the model has detected no points above <strong class="source-inline">0.95</strong>. In this case, the <a id="_idIndexMarker301"/>constant thresholder would split no observations at all into the anomaly class. The quantile thresholder of <strong class="source-inline">0.95</strong> would still give you exactly 5% of your observations as anomalies.</p>
			<p>The preferred behavior will depend on your use case, but at least you have the two options at the ready for your anomaly detectio<a id="_idTextAnchor118"/>n in River.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor119"/>Anomaly detection algorithm 1 – One-Class SVM</h2>
			<p>Let's now <a id="_idIndexMarker302"/>move on to the first anomaly detection <a id="_idIndexMarker303"/>algorithm: One-Class SVM. You'll first see a general overview of how One-Class SVM works for anomaly detection. After that, you'll see how it is adapted for an online context in River and you'll do a Python use case using One-Class S<a id="_idTextAnchor120"/>VM in Python.</p>
			<h3>General use of One-Class SVM on anomaly detection</h3>
			<p>One-Class SVM is an unsupervised <a id="_idIndexMarker304"/>outlier detection algorithm based on<a id="_idIndexMarker305"/> the <strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>) classification algorithm.</p>
			<p>SVMs are commonly used models for classification or other supervised learning. In supervised learning, they are known to be great for using the kernel trick, which maps the inputs into high-dimensional feature spaces. With this process, SVMs are able to generate non-linear classification.</p>
			<p>As described earlier, anomaly detection algorithms need to understand what is normal, but they don't have to understand the non-normal classes. The One-Class SVM is, therefore, an adaptation of the regular SVMs. In regular, supervised SVMs, you need to specify the classes (target variable), but in One-Class SVM, you act like all the data is in a single class.</p>
			<p>Basically, the One-Class SVM will just fit an SVM in which it tries to fit a model that best predicts all of the variables as the same target class. When the model fits well, the maximum of individuals will have a low error in their prediction. </p>
			<p>Individuals with a high error score for the best-fitting model are difficult to predict using the same model as for the other individuals. You could consider that they may need another model and, therefore, hypothesize that the individuals do not come from the same data-generating process. They may well, therefore, be anomalies.</p>
			<p>The error is <a id="_idIndexMarker306"/>used as a thresholding score to split individuals. Individuals with a high error score can be classified as anomalies and individuals with a low error score can be considered normal. This split is generally done with a quantile threshold, which was int<a id="_idTextAnchor121"/>roduced earlier.</p>
			<h3>Online One-Class SVM in River</h3>
			<p>The <strong class="source-inline">OneClassSVM</strong> model<a id="_idIndexMarker307"/> in River is described in the documentation as a stochastic implementation of the One-Class SVM and it will not, unfortunately, perfectly match the offline definition of the algorithm. If it is important for your use case to find exact results, you could try out online and offline implementations and see how much they differ. </p>
			<p>In general, outlier detection is an unsupervised task, and it is hard to be totally sure about the final answer and precision of your models. This is not a problem as long as you monitor results and take KPI selection and tracking of your business r<a id="_idTextAnchor122"/>esults seriously.</p>
			<h3>Application on a use case</h3>
			<p>Let's now apply the <a id="_idIndexMarker308"/>online training process of a One-Class SVM using River.</p>
			<p>For this example, let's create our own dataset so that we can be sure of the data that should be considered an outlier or not:</p>
			<ol>
				<li value="1">Let's create a uniform distribution variable with 1,000 observations between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-4</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">normal_data = np.random.rand(1000)</p>
			<ol>
				<li value="2">The histogram of the current run can be prepared as follows, but it will change due to randomness:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-5</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">plt.hist(normal_data)</p>
			<p>The<a id="_idIndexMarker309"/> resulting plot will show the following histogram:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18335_05_3.jpg" alt="Figure 5.3 – Plot of the normal data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Plot of the normal data</p>
			<ol>
				<li value="3">As we know this distribution very well, we know what to expect: any data point between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> is normal and every data point outside <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong> is an outlier. Let's now add 1% of outliers to the data. Let's make 0.5% of easy-to-detect outliers (random int between <strong class="source-inline">2</strong> and <strong class="source-inline">3</strong> and between <strong class="source-inline">-1</strong> and <strong class="source-inline">-2</strong>), which is very far away from our normal distribution. Let's also make 0.5% of our outliers a bit harder to detect (between <strong class="source-inline">0</strong> and <strong class="source-inline">-1</strong> and between <strong class="source-inline">1</strong> and <strong class="source-inline">2</strong>).</li>
			</ol>
			<p>This way we can challenge the model and see how well it performs:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-6</p>
			<p class="source-code">hard_to_detect = list(np.random.uniform(1,2,size=int(0.005*1000))) + \</p>
			<p class="source-code">                  list(np.random.uniform(0,-1,size=int(0.005*1000)))</p>
			<p class="source-code">easy_to_detect = list(np.random.uniform(2,3,size=int(0.005*1000))) + \</p>
			<p class="source-code">                  list(np.random.uniform(-1,-2,size=int(0.005*1000)))</p>
			<ol>
				<li value="4">Let's <a id="_idIndexMarker310"/>put all that data together and write code to deliver it to the model in a streaming fashion, as follows:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-7</p>
			<p class="source-code">total_data = list(normal_data) + hard_to_detect + easy_to_detect</p>
			<p class="source-code">import random</p>
			<p class="source-code">random.shuffle(total_data)</p>
			<p class="source-code">for datapoint in total_data:</p>
			<p class="source-code">  pass</p>
			<ol>
				<li value="5">Now, the only thing remaining to do is to add the model into the loop:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-8</p>
			<p class="source-code"># Anomaly percentage for the quantile thresholder</p>
			<p class="source-code">expected_percentage_anomaly = 20/1020</p>
			<p class="source-code">expected_percentage_normal = 1 - expected_percentage_anomaly</p>
			<ol>
				<li value="6">Here, you <a id="_idIndexMarker311"/>can fit the model:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-9</p>
			<p class="source-code">!pip install river</p>
			<p class="source-code">from river import anomaly</p>
			<p class="source-code">model = anomaly.QuantileThresholder(</p>
			<p class="source-code">    anomaly.OneClassSVM(),</p>
			<p class="source-code">    q=expected_percentage_normal</p>
			<p class="source-code">    )</p>
			<p class="source-code">for datapoint in total_data:</p>
			<p class="source-code">    model = model.learn_one({'x': datapoint})</p>
			<p>When running this code, you have now trained an online One-Class SVM on our synthetic data points! </p>
			<ol>
				<li value="7">Let's try to get an idea of how well it worked. In this following code, you see how to obtain the scores of each individual and the assignment to the classes:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-10</p>
			<p class="source-code">scores = []</p>
			<p class="source-code">for datapoint in total_data:</p>
			<p class="source-code">    scores.append(model.score_one({'x': datapoint}))</p>
			<ol>
				<li value="8">As we know the actual result, we can now compare whether the answers were right. You can use the following code for that:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-11</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">results = pd.DataFrame({'data': total_data , 'score': scores})</p>
			<p class="source-code">results['actual_outlier'] = (results['data'] &gt; 1 ) | (results ['data'] &lt; 0)</p>
			<p class="source-code"># there are 20 actual outliers</p>
			<p class="source-code">results['actual_outlier'].value_counts()</p>
			<p>The results are shown here:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B18335_05_4.jpg" alt="Figure 5.4 – The results of Code Block 5-11&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – The results of Code Block 5-11</p>
			<ol>
				<li value="9">The <a id="_idIndexMarker312"/>following code block will compute the value counts of what the algorithm has detected:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-12</p>
			<p class="source-code"># the algo detected 22 outliers</p>
			<p class="source-code">results['score'].value_counts()</p>
			<p>The following figure shows that 22 outliers were detected:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B18335_05_5.jpg" alt="Figure 5.5 – The results of Code Block 5-12&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – The results of Code Block 5-12</p>
			<ol>
				<li value="10">We should now compute how many of the detected outliers are actual outliers and how <a id="_idIndexMarker313"/>many are not actual outliers. This is done in the following code block:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-13</p>
			<p class="source-code"># in the 22 detected otuliuers, 10 are actual outliers, but 12 are not actually outliers</p>
			<p class="source-code">results.groupby('actual_outlier')['score'].sum()</p>
			<p>The result is that out of the 22 detected outliers, 10 are actual outliers, but 12 are not actually outliers. This can be seen in the following figure:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B18335_05_6.jpg" alt="Figure 5.6 – The results of Code Block 5-13&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – The results of Code Block 5-13</p>
			<p>The obtained result is not too bad: at least some of the outliers were detected correctly, and this could be a good minimum viable product to start automating anomaly detection for this particular use case. Let's see whether we can beat it with a different anomaly d<a id="_idTextAnchor123"/>etection algorithm!</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor124"/>Anomaly detection algorithm 2 – Half-Space-Trees</h2>
			<p>The second <a id="_idIndexMarker314"/>main anomaly<a id="_idIndexMarker315"/> detection algorithm that you'll see here is the online alternative to Isolation Forests, a commonly used and performant outlier detection algorithm.</p>
			<h3>General use of Isolation Forests in anomaly detection</h3>
			<p>Isolation Forests work a bit differently than most anomaly detection algorithms. As described throughout this chapter, many models do anomaly detection by first understanding the <em class="italic">normal</em> data <a id="_idIndexMarker316"/>points and then deciding whether a data point is relatively similar to the other normal points or not. If not, it is considered an outlier.</p>
			<p>Isolation Forests <a id="_idIndexMarker317"/>are a great invention, as they work the other way around. They try to model everything that is not normal, and they try to isolate those points from the rest.</p>
			<p>In order to isolate observations, the Isolation Forest will randomly select features and then split the feature between the minimum and the maximum. The number of splits required to isolate a sample is considered a good description <a id="_idIndexMarker318"/>of the <strong class="bold">isolation score</strong> of an observation. </p>
			<p>If it is easy to isolate it (short path to isolation, equivalent to having little splits to isolate the point), then it is probably a relatively isolated data point, and we could class it as an outlier.</p>
			<h3>How does it change with River?</h3>
			<p>In River, the model<a id="_idIndexMarker319"/> has to train online, and they had to make some adaptations to make it work. The fact that some adaptations have been made is the reason for callling the model <strong class="source-inline">HalfSpaceTrees</strong> in River.</p>
			<p>As something to keep in mind, the anomalies have to be spread out in the dataset in order for the model to work well. Also, the model needs all values to be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>.</p>
			<h3>Application of Half-Space-Trees on an anomaly detection use case</h3>
			<p>We will implement<a id="_idIndexMarker320"/> this as follows:</p>
			<ol>
				<li value="1">Let's now apply Half-Space-Trees to the same, univariate use case and see what happens:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-14</p>
			<p class="source-code">from river import anomaly</p>
			<p class="source-code">model2 = anomaly.QuantileThresholder(</p>
			<p class="source-code">    anomaly.HalfSpaceTrees(),</p>
			<p class="source-code">    q=expected_percentage_normal</p>
			<p class="source-code">    )</p>
			<p class="source-code">for datapoint in total_data:</p>
			<p class="source-code">    model2 = model2.learn_one({'x': datapoint})</p>
			<p class="source-code">scores2 = []</p>
			<p class="source-code">for datapoint in total_data:</p>
			<p class="source-code">    scores2.append(model2.score_one({'x': datapoint}))</p>
			<p class="source-code">    </p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">results2 = pd.DataFrame({'data': total_data, 'score': scores2})</p>
			<p class="source-code">results2['actual_outlier'] = (results2 ['data'] &gt; 1 ) | (results2['data'] &lt; 0)</p>
			<p class="source-code"># there are 20 actual outliers</p>
			<p class="source-code">results2['actual_outlier'].value_counts()</p>
			<p>The results <a id="_idIndexMarker321"/>of this code block can be seen in the following figure. It appears that there are 20 actual outliers:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B18335_05_7.jpg" alt="Figure 5.7 – The results of Code Block 5-14&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – The results of Code Block 5-14</p>
			<ol>
				<li value="2">You can now compute how many outliers the model detected using the following code:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-15</p>
			<p class="source-code"># the algo detected 29 outliers</p>
			<p class="source-code">results2['score'].value_counts()</p>
			<p>It appears that the algorithm detected 29 outliers. This can be seen in the following figure:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B18335_05_8.jpg" alt="Figure 5.8 – The results of Code Block 5-15&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – The results of Code Block 5-15</p>
			<ol>
				<li value="3">We will <a id="_idIndexMarker322"/>now compute how many of those 29 detected outliers were actually outliers to see whether our model is any good:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Code Block 5-16</p>
			<p class="source-code"># the 29 detected outliers are not actually outliers</p>
			<p class="source-code">results2.groupby('actual_outlier')['score'].sum()</p>
			<p>The results show that our 29 detected outliers were not really outliers, indicating that this model is not a good choice for this task. There is really no problem with that. After all, this is the exact reason to do model benchmarking:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B18335_05_9.jpg" alt="Figure 5.9 – The results of Code Block 5-16&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – The results of Code Block 5-16</p>
			<p>As you can see, this model is less performant in the current use case. In conclusion, the One-Class SVM performed better at identifying anomalies in our sample of 1,000 draws of a uniform distribution on the interval <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor125"/>Going further with anomaly detection</h1>
			<p>To go further with<a id="_idIndexMarker323"/> anomaly detection use cases, you can try out using different datasets or even a dataset of your own use case. As you have seen in the example, data points are inputted as a dictionary. In the current example, you used univariate data points: only one entry in the dictionary.</p>
			<p>In practice, you generally have multivariate problems, and you would have multiple variables in your input. Models may be able to fit<a id="_idTextAnchor126"/> better in such use cases.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor127"/>Summary</h1>
			<p>In this chapter, you have learned how anomaly detection works, both in streaming and non-streaming contexts. This category of machine learning models takes a number of variables about a situation and uses this information to detect whether specific data points or observations are likely to be different from the others.</p>
			<p>You have gotten an overview of different use cases for this. Some of those are the monitoring of IT systems, or production line sensor data in manufacturing. Whenever it is problematic to have a data point that is too different from the others, anomaly detection is of great added value.</p>
			<p>You have finished the chapter by implementing a model benchmark in which you have benchmarked two online anomaly detection models from the River library. You have seen one model being able to detect a part of the anomalies, and the other model having much worse performances. This has introduced you not only to anomaly detection but also to model benchmarking and model evaluation.</p>
			<p>In the next chapter, you will see even more on those topics. You will be working on online classification models, and you will again see how to implement model benchmarking and metrics, but this time, for classification rather than anomaly detection. As you have seen in this chapter, classification can sometimes be used for anomaly detection as well, making the two use cases related to each other.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor128"/>Further reading</h1>
			<ul>
				<li><em class="italic">Anomaly Detection</em>: <a href="https://en.wikipedia.org/wiki/Anomaly_detection">https://en.wikipedia.org/wiki/Anomaly_detection</a></li>
				<li><em class="italic">River ML Constant Thresholder</em>: <a href="https://riverml.xyz/latest/api/anomaly/ConstantThresholder/">https://riverml.xyz/latest/api/anomaly/ConstantThresholder/</a></li>
				<li><em class="italic">River ML Quantile Thresholder</em>: <a href="https://riverml.xyz/latest/api/anomaly/QuantileThresholder/">https://riverml.xyz/latest/api/anomaly/QuantileThresholder/</a></li>
				<li><em class="italic">Support Vector Machine</em>: <a href="https://en.wikipedia.org/wiki/Support-vector_machine">https://en.wikipedia.org/wiki/Support-vector_machine</a></li>
				<li><em class="italic">Scikit Learn One Class SVM</em>: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html</a></li>
				<li><em class="italic">River ML One Class SVM</em>: <a href="https://riverml.xyz/latest/api/anomaly/OneClassSVM/">https://riverml.xyz/latest/api/anomaly/OneClassSVM/</a></li>
				<li><em class="italic">Isolation Forest</em>: <a href="https://en.wikipedia.org/wiki/Isolation_forest">https://en.wikipedia.org/wiki/Isolation_forest</a></li>
				<li><em class="italic">River ML Half-Space Trees</em>: <a href="https://riverml.xyz/latest/api/anomaly/HalfSpaceTrees/">https://riverml.xyz/latest/api/anomaly/HalfSpaceTrees/</a></li>
			</ul>
		</div>
	</body></html>