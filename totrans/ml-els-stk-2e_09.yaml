- en: '*Chapter 7*: AIOps and Root Cause Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this point, we have extensively explained the value of detecting anomalies
    across metrics and logs separately. This is extremely valuable, of course. In
    some cases, however, the knowledge that a particular metric or log file has gone
    awry may not tell the whole story of what is going on. It may, for example, be
    pointing to a symptom and not the cause of the problem. To have a better understanding
    of the full scope of an emerging problem, it is often helpful to look holistically
    at many aspects of a system or situation. This involves smartly analyzing multiple
    kinds of related datasets together.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the term ''AIOps''
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the importance and limitations of KPIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving beyond KPIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizing data for better analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging the contextual information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bringing it all together for RCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The information and examples demonstrated in this chapter are relevant as of
    v7.11 of the Elastic Stack and utilize sample datasets from the GitHub repo found
    at [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition).
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the term ''AIOps''
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned in [*Chapter 1*](B17040_01_Epub_AM.xhtml#_idTextAnchor016), *Machine
    Learning for IT*, that many companies are drowning in an ever-increasing cascade
    of IT data while simultaneously being asked to ''''do more with less'''' (fewer
    people, fewer costs, and so on). Some of that data is collected and/or stored
    in specialized tools, but some may be collected in general-purpose data platforms
    such as the Elastic Stack. But the question still remains: what percentage of
    that data is being paid attention to? By this, we mean the percentage of collected
    data that is actively inspected by humans or being watched by some type of automated
    means (defined alarms based on rules, thresholds, and so on). Even generous estimates
    might put the percentage in the range of single digits. So, with 90% or more data
    being collected going unwatched, what''s being missed? The proper answer might
    be that we don''t actually know.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we admonish IT organizations for the sin of collecting piles of data
    but not watching it, we need to understand the magnitude of the challenge associated
    with such an operation. A typical user-facing application may do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Span hundreds of physical servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have dozens (if not hundreds) of microservices, each of which may have dozens
    or hundreds of operational metrics or log entries that describe its operation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combinatorics of this can easily rise to a six- or seven-figure range of
    unique measurement points. Additionally, there may be dozens or even hundreds
    of such applications under the umbrella of management by the IT organization.
    It's no wonder that the amount of data being collected by these systems per day
    can easily be measured in terabytes.
  prefs: []
  type: TYPE_NORMAL
- en: So, it is quite natural that the desired solution could involve a combination
    of automation and artificial intelligence to lessen the burden on human analysts.
    Some clever marketing person somewhere figured out that coining the term ''AIOps''
    encapsulated a projected solution to the problem – augment what humans can't (or
    don't have the time or capacity to do manually) with some amount of intelligent
    automation. Now, what an AIOps solution actually does to accomplish that goal
    is often left to a discerning user to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s demystify the term by not focusing on the term itself (let''s leave
    that to the marketing folks), but rather articulating the kinds of things we would
    want to have this intelligent technology do to help us in our situation:'
  prefs: []
  type: TYPE_NORMAL
- en: Autonomously inspect data and assess its relevance, importance, and notability
    based upon an automatically learned set of constraints, rules, and behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter out the noise of irrelevant behaviors so as to not distract human analysts
    from the things that actually matter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obtain a certain amount of proactive early warnings regarding problems that
    may be brewing but have not necessarily caused an outage yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically gather related/correlated evidence around a problem to assist
    with **Root** **Cause** **Analysis** (**RCA**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncover operational inefficiencies in order to maximize infrastructure performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suggest an action or next step for remediation, based upon past remediations
    and their effectiveness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this list is in no way comprehensive, we can see the gist of what we're
    getting at here – which is that intelligent automation and analysis can pay big
    dividends and allow IT departments to drive efficiencies and thus maximize business
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Except for the suggested remediations mentioned in number six in the preceding
    list (at least at this moment), Elastic **Machine Learning** (**ML**) can very
    much be an important part of all the other goals on this list. We've seen already
    how Elastic ML can automatically find anomalous behavior, forecast trends, proactively
    alert, and so on. But we must also recognize that Elastic ML is a generic ML platform
    – it is not purpose-built for IT operations/observability or security analytics.
    As such, there still needs to be an orientation of how Elastic ML is used in the
    context of operations, and that will be discussed throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to note that there are still a large number of IT operation
    groups that currently use no intelligent automation and analysis. They often claim
    that they would like to employ an AI-based approach to improve their current situation,
    but that they are not quite ready to take the plunge. So, let's challenge the
    notion that the only way to benefit from AI is to do every single thing that is
    possible on day 1\. Let's instead build up some practical applications of Elastic
    ML in the context of IT operations and how it can be used to satisfy most of the
    goals articulated in the preceding list.
  prefs: []
  type: TYPE_NORMAL
- en: We will first start with the notion of the **Key** **Performance** **Indicator**
    (**KPI**) and why it is the logical choice for the best place to get started with
    Elastic ML.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance and limitations of KPIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Because of the problem of scale and the desire to make some amount of progress
    in making the collected data actionable, it is natural that some of the first
    metrics to be tackled for active inspection are those that are the best indicators
    of performance or operation. The KPIs that an IT organization chooses for measurement,
    tracking, and flagging can span diverse indicators, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer experience**: These metrics measure customer experience, such as
    application response times or error rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: Metrics such as uptime or **Mean** **Time** **to** **Repair**
    (**MTTR**) are often important to track.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business**: Here we may have metrics that directly measure business performance,
    such as orders per minute or number of active users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As such, these types of metrics are usually displayed, front and center, on
    most high-level operational dashboards or on staff reports for employees ranging
    from technicians to executives. A quick Google image search for a KPI dashboard
    will return countless examples of charts, gauges, dials, maps, and other eye candy.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there is great value in such displays of information that can be consumed
    with a mere glance, there are still fundamental challenges with manual inspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretation**: There may be difficulty in understanding the difference
    between normal operation and abnormal, unless that difference is already intrinsically
    understood by the human.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Challenges of scale**: Despite the fact that KPIs are already a distillation
    of all metrics down to a set of important ones, there still may be more KPIs to
    display than is feasible given the real estate of the screen that the dashboard
    is displayed upon. The end result may be crowded visualizations or lengthy dashboards
    that require scrolling/paging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of proactivity**: Many dashboards such as this do not have their metrics
    also tied to alerts, thus requiring constant supervision if it''s proactively
    known that a KPI that is faltering is important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom line is that KPIs are an extremely important step in the process
    of identifying and tracking meaningful indicators of the health and behavior of
    an IT system. However, it should be obvious that the mere act of identifying and
    tracking a set of KPIs with a visual-only paradigm is going to leave some significant
    deficiencies in the strategy of a successful IT operations plan.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It should be obvious that KPIs are a great candidate for metrics that can be
    tracked with Elastic ML''s anomaly detection. For example, say we have some data
    that looks like the following (from the `it_ops_kpi` sample dataset in the GitHub
    repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the KPI (the field called `events_per_min`) represents the summarized
    total number of purchases per minute for some online transaction processing system.
    We could easily track this KPI over time with an anomaly detection job with a
    `sum` function on the `events_per_min` field and a bucket span of 15 minutes.
    An unexpected dip in online sales (to a value of `921`) is detected and flagged
    as anomalous:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A KPI being analyzed with a typical anomaly detection job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – A KPI being analyzed with a typical anomaly detection job
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the KPI is just a single, overall metric. If there was another
    categorical field in the data that allowed it to be segmented (for example, sales
    by product ID, product category, geographical region, and so on), then ML could
    easily split the analysis along that field to expand the analysis in a parallel
    fashion (as we saw in [*Chapter 3*](B17040_03_Epub_AM.xhtml#_idTextAnchor049),
    *Anomaly Detection*). But let''s not lose sight of what we''re accomplishing here:
    a proactive analysis of a key metric that someone likely cares about. The number
    of online sales per unit of time is directly tied to incoming revenue and thus
    is an obvious KPI.'
  prefs: []
  type: TYPE_NORMAL
- en: However, despite the importance of knowing that something unusual is happening
    with our KPI, there is still no insight as to *why* it is happening. Is there
    an operational problem with one of the backend systems that supports this customer-facing
    application? Was there a user interface coding error in the latest release that
    makes it harder for users to complete the transaction? Is there a problem with
    the third-party payment processing provider that is relied upon? None of these
    questions can be answered by merely scrutinizing the KPI.
  prefs: []
  type: TYPE_NORMAL
- en: To get that kind of insight, we will need to broaden our analysis to include
    other sets of relevant and related information.
  prefs: []
  type: TYPE_NORMAL
- en: Moving beyond KPIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of selecting KPIs, in general, should be relatively easy, as it
    is likely obvious what metrics are the best indicators (if online sales are down,
    then the application is likely not working). But if we want to get a more holistic
    view of what may be contributing to an operational problem, we must expand our
    analysis beyond the KPIs to indicators that emanate from the underlying systems
    and technology that support the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there are a plethora of ways to collect all kinds of data for
    centralization in the Elastic Stack. The **Elastic Agent**, for example, is a
    single, unified agent that you can deploy to hosts or containers to collect data
    and send it to the Elastic Stack. Behind the scenes, the Elastic Agent runs the
    Beats shippers or Elastic Endpoint required for your configuration. Starting from
    version 7.11, the Elastic Agent is managed in Kibana in the **Fleet** user interface
    and can be used to add and manage integrations for popular services and platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – The Integrations section of the Fleet user interface in Kibana'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – The Integrations section of the Fleet user interface in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these different integrations, the user can easily collect data and centralize
    it in the Elastic Stack. While this chapter is not meant to be a tutorial on Fleet
    and the Elastic Agent, the important point is that regardless of what tools you
    use to gather the underlying application and system data, one thing is likely
    true: there will be a lot of data when all is said and done. Remember that our
    ultimate goal is to proactively and holistically pay attention to a larger percentage
    of the overall dataset. To do that, we must first organize this data so that we
    can effectively analyze it with Elastic ML.'
  prefs: []
  type: TYPE_NORMAL
- en: Organizing data for better analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the nicest things about ingesting data via the Elastic Agent is that
    by default, the data collected is normalized using the **Elastic Common Schema**
    (**ECS**). ECS is an open source specification that defines a common taxonomy
    and naming conventions across data that is stored in the Elastic Stack. As such,
    the data becomes easier to manage, analyze, visualize, and correlate across disparate
    data types – including across both performance metrics and log files.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you are not using the Elastic Agent or other legacy Elastic ingest tools
    (such as Beats and Logstash) and are instead relying on other, third-party data
    collection or ingest pipelines, it is still recommended that you conform your
    data to ECS because it will pay big dividends when users expect to use this data
    for queries, dashboards, and, of course, ML jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: More information on ECS can be found in the reference section of the website
    at [https://www.elastic.co/guide/en/ecs/current/ecs-reference.html](https://www.elastic.co/guide/en/ecs/current/ecs-reference.html).
  prefs: []
  type: TYPE_NORMAL
- en: Among many of the important fields within ECS is the `host.name` field, which
    defines which host the data was collected from. By default, most data collection
    strategies in the Elastic Stack involve putting data in indices that are oriented
    around the data type, and thus potentially contain interleaved documents from
    many different hosts. Perhaps some of our hosts in our environment support one
    application (that is, online purchases), but other hosts support a different application
    (such as invoice processing). With all hosts reporting their data into a single
    index, if we are interested in orienting our reporting and analysis of the data
    for one or both applications, it is obviously inappropriate to orient the analysis
    based solely on the index – we will need our analysis to be application-centric.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to accomplish this, we have a few options:'
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the base query of the anomaly detection job so that it filters the
    data for only the hosts associated with the application of interest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying the data on ingest to enrich it, to insert additional contextual information
    into each document, which will later be used to filter the query made by the anomaly
    detection job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both require customization of the datafeed query that the anomaly detection
    job makes to the raw data in the source indices. The first option may result in
    a relatively complex query and the second option requires an interstitial step
    of data enrichment using custom ingest pipelines. Let's briefly discuss each.
  prefs: []
  type: TYPE_NORMAL
- en: Custom queries for anomaly detection datafeeds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a new job is created in the anomaly detection UI, the first step is to
    choose either an index pattern or a Kibana saved search. If the former is chosen,
    then a `{''match_all'':{}}` Elasticsearch query (return every record in the index)
    is invoked. If the job is created via the API or the advanced job wizard, then
    the user can specify just about any valid Elasticsearch DSL for filtering the
    data. Free-form composing Elasticsearch DSL can be a little error-prone for non-expert
    users. Therefore, a more intuitive way would be to approach this from Kibana via
    saved searches.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say that we have an index of log files and the appropriate
    hosts associated with the application we would like to monitor and analyze consist
    of two servers, `esxserver1.acme.com` and `esxserver2.acme.com`. On Kibana''s
    **Discover** page, we can build a filtered query using **KQL** using the search
    box at the top of the user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Building a filtered query using KQL'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – Building a filtered query using KQL
  prefs: []
  type: TYPE_NORMAL
- en: 'The text of this KQL query would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you were curious about the actual Elasticsearch DSL that is invoked by Kibana
    to get this filtered query, you could click the **Inspect** button in the top
    right and select the **Request** tab to see the Elasticsearch DSL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Inspecting the Elasticsearch DSL that runs for the KQL filter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Inspecting the Elasticsearch DSL that runs for the KQL filter
  prefs: []
  type: TYPE_NORMAL
- en: It is probably worth noting that despite the way the KQL query gets translated
    to Elasticsearch DSL in this specific example (using `match_phrase`, for example),
    it is not the only way to achieve the desired results. A query filter using `terms`
    is yet another way, but assessing the merits of one over the other is beyond the
    scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the Elasticsearch DSL that runs behind the scenes, the key thing
    is that we have a query that filters the raw data to identify only the servers
    of interest for the application we would like to analyze with Elastic ML. To keep
    this filtered search, a click of the **Save** button in the top right and naming
    the search is necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Saving the search for later use in Elastic ML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – Saving the search for later use in Elastic ML
  prefs: []
  type: TYPE_NORMAL
- en: 'Later on, you could then select this saved search when configuring a new anomaly
    detection job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Leveraging a saved search in an anomaly detection job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Leveraging a saved search in an anomaly detection job
  prefs: []
  type: TYPE_NORMAL
- en: As such, our ML job will now only run for the hosts of interest for this specific
    application. Thus, we have been able to effectively limit and segment the data
    analysis to the hosts that we've defined to have made a contribution to this application.
  prefs: []
  type: TYPE_NORMAL
- en: Data enrichment on ingest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another option is to move the decision-making about which hosts belong to which
    applications further upstream to the time of ingest. If Logstash was part of the
    ingest pipeline, you could use a filter plugin to add additional fields to the
    data based upon a lookup against an asset list (file, database, and so on). Consult
    the Logstash documentation at [https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html](https://www.elastic.co/guide/en/logstash/current/lookup-enrichment.html),
    which shows you how to dynamically enrich the indexed documents with additional
    fields to provide context. If you were not using Logstash (merely using Beats/Elastic
    Agent and the ingest node), perhaps a simpler way would be to use the enrich processor
    instead. Consult the documentation at [https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you could have this enrichment add an `application_name` field
    and dynamically populate the value of this field with the appropriate name of
    the application, such as the following (truncated JSON here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Or you could have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once the value of this field is set and inserted into the indexed documents,
    then you would use the `application_name` field, along with the ability to filter
    the query for the anomaly detection job (as previously described), to limit your
    data analysis to the pertinent application of interest. The addition of the data
    enrichment step may seem like a little more up-front effort, but it should pay
    dividends in the long term as it will be easier to maintain as asset names change
    or evolve, since the first method requires hardcoding the asset names into the
    searches of the ML jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have organized our data and perhaps even enriched it, let's now
    see how we can leverage that contextual information to make our anomaly detection
    jobs more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the contextual information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our data organized and/or enriched, the two primary ways we can leverage
    contextual information is via analysis **splits** and statistical **influencers**.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis splits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already seen that an **anomaly** **detection** job can be split based
    on any categorical field. As such, we can individually model behavior separately
    for each instance of that field. This could be extremely valuable, especially
    in a case where each instance needs its own separate model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take, for example, the case where we have data for different regions of the
    world:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Differing data behaviors based on region'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – Differing data behaviors based on region
  prefs: []
  type: TYPE_NORMAL
- en: Whatever data this is (sales KPIs, utilization metrics, and so on), clearly
    it has very distinctive patterns that are unique to each region. In this case,
    it makes sense to split any analysis we do with anomaly detection for each region
    to capitalize on this uniqueness. We would be able to detect anomalies in the
    behavior that are specific to each region.
  prefs: []
  type: TYPE_NORMAL
- en: Let's also imagine that, within each region, a fleet of servers support the
    application and transaction processing, but they are load-balanced and contribute
    equally to the performance/operation. In that way, there's nothing unique about
    each server's contribution to a region. As such, it probably doesn't make sense
    to split the analysis per server.
  prefs: []
  type: TYPE_NORMAL
- en: We've naturally come to the conclusion that splitting by region is more effective
    than splitting by server. But what if a particular server within a region is having
    problems contributing to the anomalies that are being detected? Wouldn't we want
    to have this information available immediately, instead of having to manually
    diagnose further? This is possible to know via influencers.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical influencers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduced the concept of influencers in [*Chapter 5*](B17040_05_Epub_AM.xhtml#_idTextAnchor090),
    *Interpreting Results*. As a reminder, an influencer is a field that describes
    an entity where you would like to know whether it ''influences'' (is to blame
    for) the existence of the anomaly or at least had a significant contribution.
    Remember that any field chosen as a candidate to be an influencer doesn't need
    to be part of the detection logic, although it is natural to pick fields that
    are used as splits to also be influencers. It is also important that influencers
    are chosen when the anomaly detection jobs are created as they cannot be added
    to the configuration later.
  prefs: []
  type: TYPE_NORMAL
- en: It is also key to understand that the process of finding potential influencers
    happens after the anomaly detection job finds the anomaly. In other words, it
    does not affect any of the probability calculations that are made as part of the
    detection. Once the anomaly has been determined, ML will systematically go through
    all instances of each candidate influencer field and remove that instance's contribution
    to the data in that time bucket. If, once removed, the remaining data is no longer
    anomalous, then via counterfactual reasoning, that instance's contribution must
    have been influential and is scored accordingly (with an `influencer_score` value
    in the results).
  prefs: []
  type: TYPE_NORMAL
- en: What we will see in the next section, however, is how influencers can be leveraged
    when viewing the results of not just a single anomaly detection job, but potentially
    several related jobs. Let's now move on to discuss the process of grouping and
    viewing jobs together to assist with RCA.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing it all together for RCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are at the point now where we can now discuss how we can bring everything
    together. In our desire to increase our effectiveness in IT operations and look
    more holistically at application health, we now need to operationalize what we've
    prepared in the prior sections and configure our anomaly detection jobs accordingly.
    To that end, let's work through a real-life scenario in which Elastic ML helped
    us get to the root cause of an operational problem.
  prefs: []
  type: TYPE_NORMAL
- en: Outage background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This scenario is loosely based on a real application outage, although the data
    has been somewhat simplified and sanitized to obfuscate the original customer.
    The problem was with a retail application that processed gift card transactions.
    Occasionally, the app would stop working and transactions could not be processed.
    This would only be discovered when individual stores called headquarters to complain.
    The root cause of the issue was unknown and couldn't be ascertained easily by
    the customer. Because they never got to the root cause, and because the problem
    could be fixed by simply rebooting the application servers, the problem would
    randomly reoccur and plagued them for months.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following data was collected and included in the analysis to help understand
    the origins of the problem. This data included the following (and is supplied
    in the GitHub repo):'
  prefs: []
  type: TYPE_NORMAL
- en: A summarized (1-minute) count of transaction volume (the main KPI)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application logs (semi-structured text-based messages) from the transaction
    processing engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SQL Server performance metrics from the database that backed the transaction
    processing engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network utilization performance metrics from the network the transaction processing
    engine operates on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As such, four ML jobs were configured against the data. They were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sum` on the number of transactions processed per minute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count` by the `mlcategory` detector to count the number of log messages by
    type, but using dynamic ML-based categorization to delineate different message
    types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean` analysis of every SQL Server metric in the index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean` analysis of every network performance metric in the index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These four jobs were configured and run on the data when the problem occurred
    in the application. Anomalies were found, especially in the KPI that tracked the
    number of transactions being processed. In fact, this is the same KPI that we
    saw at the beginning of this chapter, where an unexpected dip in order processing
    was the main indicator that a problem was occurring:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – The KPI of the number of transactions processed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – The KPI of the number of transactions processed
  prefs: []
  type: TYPE_NORMAL
- en: However, the root cause wasn't understood until this KPI's anomaly was correlated
    with the anomalies in the other three ML jobs that were looking at the data in
    the underlying technology and infrastructure. Let's see how the power of visual
    correlation and shared influencers allowed the underlying cause to be discovered.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation and shared influencers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the anomaly in the transactions processed KPI (in which an unexpected
    dip occurs), the other three anomaly detection jobs (for the network metrics,
    the application logs, and the SQL database metrics) were superimposed onto the
    same time frame in the **Anomaly Explorer**. The following screenshot shows the
    results of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Anomaly Explorer showing results of multiple jobs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Anomaly Explorer showing results of multiple jobs
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, notice that the day the KPI was exhibiting problems (February
    8, 2021, as shown in *Figure 7.8*), the three other jobs in *Figure 7.9* exhibit
    correlated anomalies, shown by the circled area. Upon closer inspection (by clicking
    on the red tile for the `it_ops_sql` job), you can see that there were issues
    with several of the SQL Server metrics going haywire at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Anomaly Explorer showing anomalies for SQL Server'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Anomaly Explorer showing anomalies for SQL Server
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The shaded area of the charts is highlighting the window of time associated
    with the width of the selected tile in the swim lane. This window of time might
    be larger than the bucket span of the analysis (as is the case here) and therefore
    the shaded area can contain many individual anomalies during that time frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the anomalies in the anomaly detection job for the application
    log, there is an influx of errors all referencing the database (further corroborating
    an unstable SQL server):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Anomaly Explorer showing anomalies for the application log'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.11 – Anomaly Explorer showing anomalies for the application log
  prefs: []
  type: TYPE_NORMAL
- en: 'However, interesting things were also happening on the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – Anomaly Explorer showing anomalies for the network data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – Anomaly Explorer showing anomalies for the network data
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, there was a large spike in network traffic (shown by the **Out_Octets**
    metric), and a high spike in packets getting dropped at the network interface
    (shown by the **Out_Discards** metric).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, there was clear suspicion that this network spike might have
    something to do with the database problem. And, while correlation is not always
    causation, it was enough of a clue to entice the operations team to look back
    over some historical data from prior outages. In every other outage, this large
    network spike and packet drops pattern also existed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ultimate cause of the network spike was VMware''s action of moving VMs
    to new ESX servers. Someone had misconfigured the network switch and VMware was
    sending this massive burst of traffic over the application VLAN instead of the
    management VLAN. When this occurred (randomly, of course), the transaction processing
    app would temporarily lose connection to the database and attempt to reconnect.
    However, there was a critical flaw in this reconnection code in that it would
    not attempt the reconnection to the database at the remote IP address that belonged
    to SQL Server. Instead, it attempted the reconnection to localhost (IP address
    `127.0.01`), where, of course, there was no such database. The clue to this bug
    was seen in one of the example log lines that Elastic ML displayed in the **Examples**
    section (circled in the following screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Anomaly Explorer showing the root cause of the reconnection
    problem'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Anomaly Explorer showing the root cause of the reconnection problem
  prefs: []
  type: TYPE_NORMAL
- en: Once the problem occurred, the connection to SQL Server was therefore only possible
    if the application server was completely rebooted, the startup configuration files
    were reread, and the IP address of SQL Server was relearned. This was why a full
    reboot always fixed the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'One key thing to notice is how the influencers in the user interface also assist
    with narrowing down the scope of who''s at fault for the anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Anomaly Explorer showing the top influencers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Anomaly Explorer showing the top influencers
  prefs: []
  type: TYPE_NORMAL
- en: The top-scoring influencers over the time span selected in the dashboard are
    listed in the **Top influencers** section on the left. For each influencer, the
    maximum influencer score (in any bucket) is displayed, together with the total
    influencer score over the dashboard time range (summed across all buckets). And,
    if multiple jobs are being displayed together, then those influencers that are
    common across jobs have higher sums, thus pushing their ranking higher.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very key point because now it is very easy to see commonalities in
    offending entities across jobs. If `esxserver1.acme.com` is the only physical
    host that surfaces as an influencer when viewing multiple jobs, then we immediately
    know which machine to focus on; we know it is not a widespread problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the customer was able to mitigate the system by both correcting
    the network misconfiguration and addressing the bug in the database reconnection
    code. They were able to narrow in on this root cause quite quickly because Elastic
    ML allowed them to narrow the focus of their investigation, thus saving time and
    preventing future occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Elastic ML can certainly boost the amount of data that IT organizations *pay
    attention to*, and thus get more insight and proactive value out of their data.
    The ability to organize, correlate, and holistically view related anomalies across
    data types is critical to problem isolation and root cause identification. It
    reduces application downtime and limits the possibility of problem recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how other apps within the Elastic Stack (APM,
    Security, and Logs) take advantage of Elastic ML to provide an out-of-the-box
    experience that's custom-tailored for specific use cases.
  prefs: []
  type: TYPE_NORMAL
