- en: Market Basket Analysis, Recommendation Engines, and Sequential Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's much easier to double your business by doubling your conversion rate than
    by doubling your traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '- Jeff Eisenberg, CEO of BuyerLegends.com'
  prefs: []
  type: TYPE_NORMAL
- en: I don't see smiles on the faces of people at Whole Foods.
  prefs: []
  type: TYPE_NORMAL
- en: '- Warren Buffett'
  prefs: []
  type: TYPE_NORMAL
- en: One would have to live on the dark side of the moon in order to not observe
    each and every day the results of the techniques that we are about to discuss
    in this chapter. If you visit [www.amazon.com](http://www.amazon.com), watch movies
    on [www.netflix.com](http://www.netflix.com), or visit any retail website, you
    will be exposed to terms such as "related products", "because you watched...",
    "customers who bought *x* also bought *y*", or "recommended for you", at every
    twist and turn. With large volumes of historical real-time or near real-time information,
    retailers utilize the algorithms discussed here to attempt to increase both the
    buyer's quantity and value of their purchases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The techniques to do this can be broken down into two categories: association
    rules and recommendation engines. Association rule analysis is commonly referred
    to as market basket analysis as one is trying to understand what items are purchased
    together. With recommendation engines, the goal is to provide a customer with
    other items that they will enjoy based on how they have rated previously viewed
    or purchased items.'
  prefs: []
  type: TYPE_NORMAL
- en: Another technique a business can use is to understand the sequence in which
    you purchase or use their products and services. This is called sequential analysis.
    A very common implementation of this methodology is to understand how customers
    click through various webpages and/or links.
  prefs: []
  type: TYPE_NORMAL
- en: In the examples coming up, we will endeavor to explore how R can be used to
    develop such algorithms. We will not cover their implementation, as that is outside
    the scope of this book. We will begin with a market basket analysis of purchasing
    habits at a grocery store, then dig into building a recommendation engine on website
    reviews, and finally, analyze the sequence of web pages.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of a market basket analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Market basket analysis is a data mining technique that has the purpose of finding
    the optimal combination of products or services and allows marketers to exploit
    this knowledge to provide recommendations, optimize product placement, or develop
    marketing programs that take advantage of cross-selling. In short, the idea is
    to identify which items go well together, and profit from it.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of the results of the analysis as an `if...then` statement. If
    a customer buys an airplane ticket, then there is a 46 percent probability that
    they will buy a hotel room, and if they go on to buy a hotel room, then there
    is a 33 percent probability that they will rent a car.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is not just for sales and marketing. It is also being used in fraud
    detection and healthcare; for example, if a patient undergoes treatment A, then
    there is a 26 percent probability that they might exhibit symptom X. Before going
    into the details, we should have a look at some terminology, as it will be used
    in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Itemset**: This is a collection of one or more items in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Support**: This is the proportion of the transactions in the data that contain
    an itemset of interest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confidence**: This is the conditional probability that if a person purchases
    or does x, they will purchase or do y; the act of doing x is referred to as the
    *antecedent* or Left-Hand Side (LHS), and y is the *consequence* or Right-Hand
    Side (RHS).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lift**: This is the ratio of the support of x occurring together with y divided
    by the probability that x and y occur if they are independent. It is the **confidence**
    divided by the probability of x times the probability of y; for example, say that
    we have the probability of x and y occurring together as 10 percent and the probability
    of x is 20 percent and y is 30 percent, then the lift would be 10 percent (20
    percent times 30 percent) or 16.67 percent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The package in R that you can use to perform a market basket analysis is **arules:
    Mining Association Rules and Frequent Itemsets**. The package offers two different
    methods of finding rules. Why would one have different methods? Quite simply,
    if you have massive datasets, it can become computationally expensive to examine
    all the possible combinations of the products. The algorithms that the package
    supports are **apriori** and **ECLAT**. There are other algorithms to conduct
    a market basket analysis, but apriori is used most frequently, and so, that will
    be our focus.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With apriori, the principle is that, if an itemset is frequent, then all of
    its subsets must also be frequent. A minimum frequency (support) is determined
    by the analyst prior to executing the algorithm, and once established, the algorithm
    will run as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let *k=1* (the number of items)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate itemsets of a length that are equal to or greater than the specified
    support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate *k + (1...n)*, pruning those that are infrequent (less than the support)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop the iteration when no new frequent itemsets are identified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have an ordered summary of the most frequent itemsets, you can continue
    the analysis process by examining the confidence and lift in order to identify
    the associations of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our business case, we will focus on identifying the association rules for
    a grocery store. The dataset will be from the `arules` package and is called `Groceries`.
    This dataset consists of actual transactions over a 30-day period from a real-world
    grocery store and consists of 9,835 different purchases. All the items purchased
    are put into one of 169 categories, for example, bread, wine, meat, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we are a start-up microbrewery trying to make a headway in this
    grocery outlet and want to develop an understanding of what potential customers
    will purchase along with beer. This knowledge may just help us in identifying
    the right product placement within the store or support a cross-selling campaign.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this analysis, we will only need to load two packages, as well as the `Groceries`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This dataset is structured as a sparse matrix object, known as the `transaction`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: So, once the structure is that of the class transaction, our standard exploration
    techniques will not work, but the `arules` package offers us other techniques
    to explore the data. On a side note, if you have a data frame or matrix and want
    to convert it to the `transaction` class, you can do this with a simple syntax,
    using the `as()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is for illustrative purposes only, so do not run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`> # transaction.class.name <- as(current.data.frame,"transactions")`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to explore this data is with an item frequency plot using the
    `itemFrequencyPlot()` function in the `arules` package. You will need to specify
    the transaction dataset, the number of items with the highest frequency to plot,
    and whether or not you want the relative or absolute frequency of the items. Let''s
    first look at the absolute frequency and the top `10` items only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The top item purchased was **whole milk** with roughly **2**,**500** of the
    9,836 transactions in the basket. For a relative distribution of the top 15 items,
    let''s run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: Alas, here we see that beer shows up as the 13th and 15th most purchased item
    at this store. Just under 10 percent of the transactions had purchases of **bottled
    beer** and/or **canned beer**.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of this exercise, this is all we really need to do, therefore,
    we can move right on to the modeling and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by mining the data for the overall association rules before moving
    on to our rules for beer specifically. Throughout the modeling process, we will
    use the apriori algorithm, which is the appropriately named `apriori()` function
    in the `arules` package. The two main things that we will need to specify in the
    function is the dataset and parameters. As for the parameters, you will need to
    apply judgment when specifying the minimum support, confidence, and the minimum
    and/or maximum length of basket items in an itemset. Using the item frequency
    plots, along with trial and error, let''s set the minimum support at 1 in 1,000
    transactions and minimum confidence at 90 percent. Additionally, let''s establish
    the maximum number of items to be associated as four. The following is the code
    to create the object that we will call `rules`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the object shows how many rules the algorithm produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a number of ways to examine the rules. The first thing that I recommend
    is to set the number of displayed digits to only two, with the `options()` function
    in base R. Then, sort and inspect the top five rules based on the lift that they
    provide, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Lo and behold, the rule that provides the best overall lift is the purchase
    of `liquor` and `red wine` on the probability of purchasing `bottled beer`. I
    have to admit that this is pure chance and not intended on my part. As I always
    say, it is better to be lucky than good. Although, it is still not a very common
    transaction with a support of only 1.9 per 1,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also sort by the support and confidence, so let''s have a look at the
    first `5` `rules` `by="confidence"` in descending order, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see in the table that `confidence` for these transactions is 100 percent.
    Moving on to our specific study of beer, we can utilize a function in `arules`
    to develop cross tabulations--the `crossTable()` function--and then examine whatever
    suits our needs. The first step is to create a table with our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'With `tab` created, we can now examine the joint occurrences between the items.
    Here, we will look at just the first three rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might imagine, shoppers only selected liver loaf 50 times out of the
    9,835 transactions. Additionally, of the `924` times, people gravitated toward
    `sausage`, `10` times they felt compelled to grab `liver loaf`. (Desperate times
    call for desperate measures!) If you want to look at a specific example, you can
    either specify the row and column number or just spell that item out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells us that there were `792` transactions of `bottled beer`. Let''s
    see what the joint occurrence between `bottled beer` and `canned beer` is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: I would expect this to be low as it supports my idea that people lean toward
    drinking beer from either a bottle or a can. I strongly prefer a bottle. It also
    makes a handy weapon to protect oneself from all these ruffian protesters like
    Occupy Wallstreet and the like.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now move on and derive specific rules for `bottled beer`. We will again
    use the `apriori()` function, but this time, we will add a syntax around `appearance`.
    This means that we will specify in the syntax that we want the left-hand side
    to be items that increase the probability of a purchase of `bottled beer`, which
    will be on the right-hand side. In the following code, notice that I''ve adjusted
    the `support` and `confidence` numbers. Feel free to experiment with your own
    settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We find ourselves with only `4` association rules. We have seen one of them
    already; now let''s bring in the other three rules in descending order by lift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In all of the instances, the purchase of `bottled beer` is associated with
    booze, either `liquor` and/or `red wine` , which is no surprise to anyone. What
    is interesting is that `white wine` is not in the mix here. Let''s take a closer
    look at this and compare the joint occurrences of `bottled beer` and types of
    wine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It's interesting that 25 percent of the time, when someone purchased `red wine`,
    they also purchased `bottled beer`; but with `white wine`, a joint purchase only
    happened in 12 percent of the instances. We certainly don't know why in this analysis,
    but this could potentially help us to determine how we should position our product
    in this grocery store. Another thing before we move on is to look at a plot of
    the rules. This is done with the `plot()` function in the `arulesViz` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many graphic options available. For this example, let''s specify
    that we want a `graph`, showing `lift`, and the rules provided and shaded by `confidence`.
    The following syntax will provide this accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: This graph shows that **liquor**/**red wine** provides the best **lift** and
    the highest level of **confidence** with both the **size** of the circle and its
    shading.
  prefs: []
  type: TYPE_NORMAL
- en: What we've just done in this simple exercise is show how easy it is with R to
    conduct a market basket analysis. It doesn't take much imagination to figure out
    the analytical possibilities that one can include with this technique, for example,
    in corporate customer segmentation, longitudinal purchase history, and so on,
    as well as how to use it in ad displays, co-promotions, and so on. Now let's move
    on to a situation where customers rate items, and learn how to build and test
    recommendation engines.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of a recommendation engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now focus on situations where users have provided rankings or ratings
    on previously viewed or purchased items. There are two primary categories of designing
    recommendation systems: *collaborative filtering and content-based* (Ansari, Essegaier,
    and Kohli, 2000). The former category is what we will concentrate on, as this
    is the focus of the `recommenderlab` R package that we will be using.'
  prefs: []
  type: TYPE_NORMAL
- en: For content-based approaches, the concept is to link user preferences with item
    attributes. These attributes may be things such as the genre, cast, or storyline
    for a movie or TV show recommendation. As such, recommendations are based entirely
    on what the user provides as ratings; there is no linkage to what anyone else
    recommends. This has the advantage over content-based approaches in that when
    a new item is added, it can be recommended to a user if it matches their profile,
    instead of relying on other users to rate it first (the so-called "first rater
    problem"). However, content-based methods can suffer when limited content is available,
    either because of the domain or when a new user enters the system. This can result
    in non-unique recommendations, that is, poor recommendations (Lops, Gemmis, and
    Semeraro, 2011).
  prefs: []
  type: TYPE_NORMAL
- en: In collaborative filtering, the recommendations are based on the many ratings
    provided by some or all of the individuals in the database. Essentially, it tries
    to capture the wisdom of the crowd.
  prefs: []
  type: TYPE_NORMAL
- en: 'For collaborative filtering, we will focus on the following four methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-based collaborative filtering** (**UBCF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Item-based collaborative filtering** (**IBCF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular value decomposition** (**SVD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Principal components analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at these methods briefly before moving on to the business case.
    It is also important to understand that `recommenderlab` was not designed to be
    used as a real-world implementation tool, but rather as a laboratory tool in order
    to research algorithms provided in the package as well as algorithms that you
    wish to experiment with on your own.
  prefs: []
  type: TYPE_NORMAL
- en: User-based collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In UBCF, the algorithm finds *missing ratings for a user by first finding a
    neighborhood of similar users and then aggregating the ratings of these users
    to form a prediction* (Hahsler, 2011). The neighborhood is determined by selecting
    either the KNN that is the most similar to the user we are making predictions
    for or by some similarity measure with a minimum threshold. The two similarity
    measures available in `recommenderlab` are **pearson correlation coefficient**
    and **cosine similarity**. I will skip the formulas for these measures as they
    are readily available in the package documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Once the neighborhood method is decided on, the algorithm identifies the neighbors
    by calculating the similarity measure between the individual of interest and their
    neighbors on only those items that were rated by both. Through a scoring scheme,
    say, a simple average, the ratings are aggregated in order to make a predicted
    score for the individual and item of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a simple example. In the following matrix, there are six individuals
    with ratings on four movies, with the exception of my rating for *Mad Max*. Using
    *k=1*, the nearest neighbor is **Homer**, with **Bart** a close second; even though
    **Flanders** hated the **Avengers** as much as I did. So, using Homer''s rating
    for **Mad Max**, which is **4**, the predicted rating for me would also be a **4**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: There are a number of ways to weigh the data and/or control the bias. For instance,
    **Flanders** is quite likely to have lower ratings than the other users, so normalizing
    the data where the new rating score is equal to the user rating for an item minus
    the average for that user for all the items is likely to improve the rating accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The weakness of UBCF is that, to calculate the similarity measure for all the
    possible users, the entire database must be kept in memory, which can be quite
    computationally expensive and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Item-based collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you might have guessed, IBCF uses the similarity between the items and not
    users to make a recommendation. *The assumption behind this approach is that users
    will prefer items that are similar to other items they like* (Hahsler, 2011).
    The model is built by calculating a pairwise similarity matrix of all the items.
    The popular similarity measures are Pearson correlation and cosine similarity.
    To reduce the size of the similarity matrix, one can specify to retain only the
    k-most similar items. However, limiting the size of the neighborhood may significantly
    reduce the accuracy, leading to poorer performance versus UCBF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing with our simplified example, if we examine the following matrix,
    with *k=1* the item most similar to **Mad Max** is **American Sniper**, and we
    can thus take that rating as the prediction for **Mad Max**, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: Singular value decomposition and principal components analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is quite common to have a dataset where the number of users and items number
    in the millions. Even if the rating matrix is not that large, it may be beneficial
    to reduce the dimensionality by creating a smaller (lower-rank) matrix that captures
    most of the information in the higher-dimension matrix. This may potentially allow
    you to capture important latent factors and their corresponding weights in the
    data. Such factors could lead to important insights, such as the movie genre or
    book topics in the rating matrix. Even if you are unable to discern meaningful
    factors, the techniques may filter out the noise in the data.
  prefs: []
  type: TYPE_NORMAL
- en: One issue with large datasets is that you will likely end up with a sparse matrix
    that has many ratings missing. One weakness of these methods is that they will
    not work on a matrix with missing values, which must be imputed. As with any data
    imputation task, there are a number of techniques that one can try and experiment
    with, such as using the mean, median, or code as zeroes. The default for `recommenderlab`
    is to use the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what is SVD? It is simply a method for matrix factorization, and can help
    transform a set of correlated features to a set of uncorrelated features. Say
    that you have a matrix called **A**. This matrix will factor into three matrices:
    **U**, **D**, and **V^T**. U is an orthogonal matrix, D is a non-negative, diagonal
    matrix, and V^T is a transpose of an orthogonal matrix. Now, let''s look at our
    rating matrix and walk through an example using R.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing that we will do is recreate the rating matrix (think of it
    as matrix **A**, as shown in the following code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use the `svd()` function in base R to create the three matrices
    described above, which R calls `$d`, `$u`, and `$v`. You can think of the `$u`
    values as an individual''s loadings on that factor and `$v` as a movie''s loadings
    on that dimension. For example, `Mad Max` loads on dimension one at -0.116 (1st
    row, 4th column):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'It is easy to explore how much variation is explained by reducing the dimensionality.
    Let''s sum the diagonal numbers of `$d`, then look at how much of the variation
    we can explain with just two factors, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With two of the four factors, we are able to capture just over 85 percent of
    the total variation in the full matrix. You can see the scores that the reduced
    dimensions would produce. To do this, we will create a function. (Many thanks
    to the [www.stackoverflow.com](http://www.stackoverflow.com) respondents who helped
    me put this function together.) This function will allow us to specify the number
    of factors that are to be included for a prediction. It calculates a rating value
    by multiplying the `$u` matrix times the `$v` matrix times the `$d` matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying `n=4` and calling the function, we can recreate the original
    rating matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can specify `n=2` and examine the resulting matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: So, with SVD, you can reduce the dimensionality and possibly identify the meaningful
    latent factors.
  prefs: []
  type: TYPE_NORMAL
- en: If you went through the prior chapter, you will see the similarities with PCA.
    In fact, the two are closely related and often used interchangeably as they both
    utilize matrix factorization. You may be asking what is the difference? In short,
    PCA is based on the covariance matrix, which is symmetric. This means that you
    start with the data, compute the covariance matrix of the centered data, diagonalize
    it, and create the components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply a portion of the PCA code from the prior chapter to our data in
    order to see how the difference manifests itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: You can see that PCA is easier to interpret. Notice how `American Sniper` and
    `Mad Max` have high loadings on the first component, while only `Avengers` has
    a high loading on the second component. Additionally, these two components account
    for 94 percent of the total variance in the data.  It is noteworthy to include
    that, in the time between the first and second editions of this book, PCA has
    become unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Having applied a simplistic rating matrix to the techniques of collaborative
    filtering, let's move on to a more complex example using real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding and recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This business case is a joke, literally. Maybe it is more appropriate to say
    a bunch of jokes, as we will use the `Jester5k` data from the `recommenderlab`
    package. This data consists of 5,000 ratings on 100 jokes sampled from the Jester
    Online Joke Recommender System. It was collected between April 1999 and May 2003,
    and all the users have rated at least 36 jokes (Goldberg, Roeder, Gupta, and Perkins,
    2001). Our goal is to compare the recommendation algorithms and select the best
    one.
  prefs: []
  type: TYPE_NORMAL
- en: As such, I believe it is important to lead off with a statistical joke to put
    one in the proper frame of mind. I'm not sure of how to properly provide attribution
    for this one, but it is popular all over the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: A statistician's wife had twins. He was delighted. He rang the minister who
    was also delighted. "Bring them to church on Sunday and we'll baptize them", said
    the minister. "No", replied the statistician. "Baptize one. We'll keep the other
    as a control."
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding, preparation, and recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The one library that we will need for this exercise is `recommenderlab`. The
    package was developed by the Southern Methodist University''s Lyle Engineering
    Lab, and they have an excellent website with supporting documentation at [https://lyle.smu.edu/IDA/recommenderlab/](https://lyle.smu.edu/IDA/recommenderlab/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The rating matrix contains `362106` total ratings. It is quite easy to get
    a list of a user''s ratings. Let''s look at user number `10`. The following output
    is abbreviated for the first five jokes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also look at the mean rating for a user (user `10`) and/or the mean
    rating for a specific joke (joke `1`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'One method to get a better understanding of the data is to plot the ratings
    as a histogram, both the raw data and after normalization. We will do this with
    the `getRating()` function from `recommenderlab`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `normalize()` function in the package centers the data by subtracting the
    mean of the ratings of the joke from that joke''s rating. As the preceding distribution
    is slightly biased towards the positive ratings, normalizing the data can account
    for this, thus yielding a more normal distribution but still showing a slight
    skew towards the positive ratings, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before modeling and evaluation, it is quite easy to create the `train` and
    `test` datasets with the `recommenderlab` package with the `evaluationScheme()`
    function. Let''s do an 80/20 split of the data for the `train` and `test` sets.
    You can also choose k-fold cross-validation and bootstrap resampling if you desire.
    We will also specify that for the `test` set, the algorithm will be given 15 ratings.
    This means that the other rating items will be used to compute the error. Additionally,
    we will specify what the threshold is for a good rating; in our case, greater
    than or equal to `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `train` and `test` data established, we will now begin to model and
    evaluate the different recommenders: user-based, item-based, popular, SVD, PCA,
    and random.'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling, evaluation, and recommendations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to build and test our recommendation engines, we can use the same
    function, `Recommender()`, merely changing the specification for each technique.
    In order to see what the package can do and explore the parameters available for
    all six techniques, you can examine the registry. Looking at the following IBCF,
    we can see that the default is to find 30 neighbors using the cosine method with
    the centered data while the missing data is not coded as a zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is how you can put together the algorithms based on the `train` data.
    For simplicity, let''s use the default algorithm settings. You can adjust the
    parameter settings by simply including your changes in the function as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, using the `predict()` and `getData()` functions, we will get the predicted
    ratings for the 15 items of the `test` data for each of the algorithms, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will examine the error between the predictions and unknown portion of the
    `test` data using the `calcPredictionAccuracy()` function. The output will consist
    of `RMSE`, `MSE`, and `MAE` for all the methods. We''ll examine `UBCF` by itself.
    After creating the objects for all five methods, we can build a table by creating
    an object with the `rbind()` function and giving names to the rows with the `rownames()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can see in the output that the user-based and popular algorithms slightly
    outperform IBCF and SVD and all outperform random predictions.
  prefs: []
  type: TYPE_NORMAL
- en: There is another way to compare methods using the `evaluate()` function. Making
    comparisons with `evaluate()` allows one to examine additional performance metrics
    as well as performance graphs. As the UBCF and Popular algorithms performed the
    best, we will look at them along with IBCF.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first task in this process is to create a list of the algorithms that we
    want to compare, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, let''s compare the top `5`, `10`, and `15` joke recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that by executing the command, you will receive an output on how long
    it took to run the algorithm. We can now examine the performance using the `avg()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the performance metrics for `POPULAR` and `UBCF` are nearly the same.
    One could say that the simpler-to-implement popular-based algorithm is probably
    the better choice for a model selection.  We can plot and compare the results
    as **Receiver Operating Characteristic Curves** (**ROC**), comparing `TPR` and
    `FPR` or precision/recall, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To get the precision/recall curve plot you only need to specify `"prec"` in
    the `plot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: You can clearly see in the plots that the popular-based and user-based algorithms
    are almost identical and outperform the item-based one. The `annotate=TRUE` parameter
    provides numbers next to the point that corresponds to the number of recommendations
    that we called for in our evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was simple, but what are the actual recommendations from a model for a
    specific individual? This is quite easy to code as well. First, let''s build a
    `"popular"` recommendation engine on the full dataset. Then, we will find the
    top five recommendations for the first two raters. We will use the `Recommend()`
    function and apply it to the whole dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just need to get the top five recommendations for the first two raters
    and produce them as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'It is also possible to see a rater''s specific rating score for each of the
    jokes by specifying this in the `predict()` syntax and then putting it in a matrix
    for review. Let''s do this for ten individuals (raters `300` through `309`) and
    three jokes (`71` through `73`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The numbers in the matrix indicate the predicted rating scores for the jokes
    that the individual rated, while the NAs indicate those that the user did not
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final effort on this data will show how to build recommendations for those
    situations where the ratings are binary, that is, good or bad or 1 or 0\. We will
    need to turn the ratings into this binary format with 5 or greater as a 1 and
    less than 5 as 0\. This is quite easy to do with `Recommenderlab` using the `binarize()`
    function and specifying `minRating=5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will need to have our data reflect the number of ratings equal to one
    in order to match what we need the algorithm to use for the training. For argument''s
    sake, let''s go with greater than 10\. The code to create the subset of the necessary
    data is shown in the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to create `evaluationScheme`. In this instance, we will go with
    `cross-validation`. The default k-fold in the function is `10`, but we can also
    safely go with `k=5`, which will reduce our computation time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparison purposes, the algorithms under evaluation will include `random`,
    `popular`, and `UBCF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'It is now time to build our model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Forgoing the table of performance metrics, let''s take a look at the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: The user-based algorithm slightly outperforms the popular-based one, but you
    can clearly see that they are both superior to any random recommendation. In our
    business case, it will come down to the judgment of the decision-making team as
    to which algorithm to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are known knowns. These are things we know that we know. There are known
    unknowns. That is to say, there are things that we know we don't know. But there
    are also unknown unknowns. There are things we don't know we don't know.
  prefs: []
  type: TYPE_NORMAL
- en: '- Donald Rumsfeld, Former Secretary of Defense'
  prefs: []
  type: TYPE_NORMAL
- en: The very first business question I came across after the 1st edition was published
    revolved around product sequential analysis. The team worked on complicated Excel
    spreadsheets and pivot tables, along with a bunch of SAS code, to produce insights.
    After coming across this problem, I explored what could be done with R and was
    pleasantly surprised to stumble into the `TraMineR` package, specifically designed
    for just such a task. I believe the application of R to the problem would have
    greatly simplified the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The package was designed for the social sciences, but it can be used in just
    about every situation where you want to mine and learn how observation's states
    evolve over discrete periods or events (longitudinal data). A classic use would
    be as in the case mentioned above where you want to understand the order in which
    customers purchase products. This would facilitate a recommendation engine of
    sorts where you can create the probability of the next purchase, as I've heard
    it being referred to as a next logical product offer. Another example could be
    in healthcare, examining the order that a patient receives treatments and/or medications,
    or even physician prescribing habits. I've worked on such tasks, creating simple
    and complex Markov chains to build models and create forecasts. Indeed, `TraMineR`
    allows the creation of Markov chain transition matrices to support such models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we will examine does the hard work of creating, counting, and plotting
    the various combinations of transitions over time, also incorporating covariates.
    That will be our focus, but keep in mind that one can also build a dissimilarity
    matrix for clustering. The core features covered in the practical exercise will
    consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Transition rates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: duration within each state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential analysis applied
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this exercise, I''ve created an artificial dataset; to follow along, you
    can download it from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/datameister66/data/blob/master/sequential.csv](https://github.com/datameister66/data/blob/master/sequential.csv)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also datasets available with the package and tutorials are available.
    My intent was to create something new that mirrored situations I have encountered.
    I developed it completely from random (with some supervision), so it does not
    match any real world data. It consists of 5,000 observations, with each observation,
    the history of a customer and nine variables:'
  prefs: []
  type: TYPE_NORMAL
- en: Cust_segment--a factor variable indicating the customer's assigned segment (see
    [Chapter 8](f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml), *Cluster Analysis*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eight discrete purchase events named `Purchase1` through `Purchase8`; remember,
    these are events and not time-based, which is to say that a customer could have
    purchased all eight products at the same time, but in a specific order
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within each purchase variable are the generic names of the product, seven possible
    products to be exact. They are named `Product_A` through `Product_G`.  What are
    these products? Doesn't matter! Use your imagination or apply it to your own situation.
    If a customer only purchased one product, then `Purchase1` would contain the name
    of that product and the other variables would be NULL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we load the file as a dataframe. The structure output is abbreviated for
    clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Time for some exploration of the data, starting with a table of the customer
    segment counts and a count of the first product purchased:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`Segment1` is the largest segment, and the most purchased initial product is
    `Product A`. However, is it the most purchased product overall? This code will
    provide the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Yes, `ProductA` is the most purchased. The count of NULL values is 22,390.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you may be wondering if we can just build some summaries without much trouble,
    and that is surely the case. Here, I put the `count()` and `arrange()` functions
    from the `dplyr` package to good use to examine the frequency of sequences between
    the first and second purchase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We see that the most frequent sequences are the purchase of `ProductA` followed
    by another purchase of `ProductA`, along with the purchase of `ProductD` followed
    by no additional purchases. What is interesting is the frequency of similar product
    purchases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin further examination using the `TraMineR` package. To begin,
    the data needs to be put into an object of class sequence with the `seqdef()`
    function. This should consist of only the sequences and not any covariates. Also,
    you can specify the distance of tick marks in plotting functions with `xstep =
    n`. In our case, we will have a tick mark for every event:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now explore the data further. Let''s look at the index plot, which produces
    the sequences of the first 10 observations. You can use indices with the data
    to examine as many observations and event periods as you wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One can plot all observations with `seqIplot()`, but given the size of the
    data, it doesn''t produce anything meaningful. A plot of distribution by state
    is more meaningful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With this plot, it is easy to see the distribution of product purchases by
    state. We can also group this plot by segments and determine whether there are
    differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we clearly see that `Segment2` has a higher proportion of `ProductA`
    purchases than the other segments. Another way to see that insight is with the
    modal plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is interesting. Around 50% of `Segment2` purchased `ProductA` first, while
    segment 4''s most frequent initial purchase was `ProductD`. Another plot that
    may be of interest, but I believe not in this case, is the mean time plot. It
    plots the average "time" spent in each state. Since we are not time-based, it
    doesn''t make sense, but I include for your consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s supplement our preceding code and look further at the transition of
    sequences. This code creates an object of sequences, then narrows that down to
    those sequences with an occurrence of at least 5%, then plots the top 10 sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_17.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the plot shows the percentage frequency of the sequences through the
    eight transition states. If you want to narrow that down to, say, the first two
    transitions, you would do that in the `seqecreate()` function using indices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s see how you can use the data to create a transition matrix.
    This matrix shows the probability of transitioning from one state to the next.
    In our case, it provides the probability of purchasing the next product. As I
     mentioned before, this can also be used in a Markov chain simulation to develop
    a forecast. That is outside the scope of this chapter, but if you are interested
    I recommend having a look at the `markovchain` package in R and its tutorial on
    how to implement the procedure. Two possible transition matrices are available.
    One that incorporates the overall probability through all states and another that
    develops a transition matrix from one state to the next, that is, time-varying
    matrices. This code shows how to develop the former. To produce the latter, just
    specify `"time.varying = TRUE"` in the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows rows 2 through 4 and columns 1 through 3\. The matrix shows
    us that the probability of having Product A and the next purchase being `ProductA`
    is almost 42%, while it is 19% to not purchase another product, and 17% to purchase
    `ProductB`. The final output we will examine is the probability of not purchasing
    another product for each prior purchase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the matrix shows that the probability of not purchasing another product
    after not purchasing is 100%. Also notice that the probability of not purchasing
    after acquiring Product D is 33%. Implications for Segment4? Perhaps.
  prefs: []
  type: TYPE_NORMAL
- en: What is fascinating is that this analysis was done with only a few lines of
    code and didn't require the use of Excel or some expensive visualization software.
    Have longitudinal data? Give sequential analysis a try!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the goal was to provide an introduction to how to use R in
    order to build and test association rule mining (market basket analysis) and recommendation
    engines. Market basket analysis is trying to understand what items are purchased
    together. With recommendation engines, the goal is to provide a customer with
    other items that they will enjoy based on how they have rated previously viewed
    or purchased items. It is important to understand the R package that we used (`recommenderlab`)
    for recommendation is not designed for implementation, but to develop and test
    algorithms. The other thing examined here was longitudinal data and mining it
    to learn valuable insights, in our case, the order in which customers purchased
    our products. Such an analysis has numerous applications, from marketing campaigns
    to healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: We are now going to shift gears back to supervised learning. In the next chapter,
    we are going to cover some of the most exciting and important methods in practical
    machine learning, that is multi-class classification and creating ensemble models,
    something that is very easy to do in R with recent package releases.
  prefs: []
  type: TYPE_NORMAL
