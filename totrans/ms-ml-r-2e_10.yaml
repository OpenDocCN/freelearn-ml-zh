- en: Market Basket Analysis, Recommendation Engines, and Sequential Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 购物篮分析、推荐引擎和序列分析
- en: It's much easier to double your business by doubling your conversion rate than
    by doubling your traffic.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加倍转换率来加倍你的业务，比通过加倍流量要容易得多。
- en: '- Jeff Eisenberg, CEO of BuyerLegends.com'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- BuyerLegends.com 首席执行官 杰夫·艾森伯格'
- en: I don't see smiles on the faces of people at Whole Foods.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我没在 Whole Foods 的人脸上看到笑容。
- en: '- Warren Buffett'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '- 沃伦·巴菲特'
- en: One would have to live on the dark side of the moon in order to not observe
    each and every day the results of the techniques that we are about to discuss
    in this chapter. If you visit [www.amazon.com](http://www.amazon.com), watch movies
    on [www.netflix.com](http://www.netflix.com), or visit any retail website, you
    will be exposed to terms such as "related products", "because you watched...",
    "customers who bought *x* also bought *y*", or "recommended for you", at every
    twist and turn. With large volumes of historical real-time or near real-time information,
    retailers utilize the algorithms discussed here to attempt to increase both the
    buyer's quantity and value of their purchases.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要不观察我们即将在本章讨论的每种技术每天的结果，人们就得住在月亮的阴暗面。如果你访问 [www.amazon.com](http://www.amazon.com)，在
    [www.netflix.com](http://www.netflix.com) 观看电影，或者访问任何零售网站，你都会在每个角落遇到诸如“相关产品”、“因为你观看了...”、“购买
    *x* 的顾客也购买了 *y*”或“为您推荐”等术语。有了大量历史实时或接近实时信息，零售商利用这里讨论的算法试图增加买家的购买数量和价值。
- en: 'The techniques to do this can be broken down into two categories: association
    rules and recommendation engines. Association rule analysis is commonly referred
    to as market basket analysis as one is trying to understand what items are purchased
    together. With recommendation engines, the goal is to provide a customer with
    other items that they will enjoy based on how they have rated previously viewed
    or purchased items.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这些技巧的方法可以分为两类：关联规则和推荐引擎。关联规则分析通常被称为购物篮分析，因为人们试图了解哪些商品是共同购买的。在推荐引擎中，目标是根据客户之前评分的观看或购买的商品提供他们可能会喜欢的其他商品。
- en: Another technique a business can use is to understand the sequence in which
    you purchase or use their products and services. This is called sequential analysis.
    A very common implementation of this methodology is to understand how customers
    click through various webpages and/or links.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种企业可以使用的技巧是了解你购买或使用他们产品和服务的时间顺序。这被称为序列分析。这种方法的非常常见的实现方式是了解客户如何点击各种网页和/或链接。
- en: In the examples coming up, we will endeavor to explore how R can be used to
    develop such algorithms. We will not cover their implementation, as that is outside
    the scope of this book. We will begin with a market basket analysis of purchasing
    habits at a grocery store, then dig into building a recommendation engine on website
    reviews, and finally, analyze the sequence of web pages.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的例子中，我们将努力探索如何使用 R 开发这样的算法。我们不会涵盖它们的实现，因为这超出了本书的范围。我们将从一个杂货店的购买习惯的购物篮分析开始，然后深入构建基于网站评论的推荐引擎，最后分析网页的顺序。
- en: An overview of a market basket analysis
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 购物篮分析概述
- en: Market basket analysis is a data mining technique that has the purpose of finding
    the optimal combination of products or services and allows marketers to exploit
    this knowledge to provide recommendations, optimize product placement, or develop
    marketing programs that take advantage of cross-selling. In short, the idea is
    to identify which items go well together, and profit from it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 购物篮分析是一种数据挖掘技术，其目的是找到产品或服务的最佳组合，并允许营销人员利用这种知识提供推荐、优化产品摆放或开发利用交叉销售的营销计划。简而言之，想法是识别哪些商品搭配得好，并从中获利。
- en: You can think of the results of the analysis as an `if...then` statement. If
    a customer buys an airplane ticket, then there is a 46 percent probability that
    they will buy a hotel room, and if they go on to buy a hotel room, then there
    is a 33 percent probability that they will rent a car.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把分析的结果看作一个 `if...then` 语句。如果一个顾客购买了飞机票，那么他们购买酒店房间的概率是 46%，如果他们继续购买酒店房间，那么他们租车的概率是
    33%。
- en: 'However, it is not just for sales and marketing. It is also being used in fraud
    detection and healthcare; for example, if a patient undergoes treatment A, then
    there is a 26 percent probability that they might exhibit symptom X. Before going
    into the details, we should have a look at some terminology, as it will be used
    in the example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，它不仅用于销售和营销。它还被用于欺诈检测和医疗保健；例如，如果一个患者接受了治疗A，那么他们可能会表现出症状X的概率为26%。在进入细节之前，我们应该看看一些术语，因为它们将在示例中使用：
- en: '**Itemset**: This is a collection of one or more items in the dataset.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项集**：这是数据集中一个或多个项目的集合。'
- en: '**Support**: This is the proportion of the transactions in the data that contain
    an itemset of interest.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持度**：这是数据中包含感兴趣项集的交易比例。'
- en: '**Confidence**: This is the conditional probability that if a person purchases
    or does x, they will purchase or do y; the act of doing x is referred to as the
    *antecedent* or Left-Hand Side (LHS), and y is the *consequence* or Right-Hand
    Side (RHS).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置信度**：这是如果一个人购买了或做了x，他们将会购买或做y的条件概率；做x的行为被称为*前提*或左侧（LHS），而y是*结果*或右侧（RHS）。'
- en: '**Lift**: This is the ratio of the support of x occurring together with y divided
    by the probability that x and y occur if they are independent. It is the **confidence**
    divided by the probability of x times the probability of y; for example, say that
    we have the probability of x and y occurring together as 10 percent and the probability
    of x is 20 percent and y is 30 percent, then the lift would be 10 percent (20
    percent times 30 percent) or 16.67 percent.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升度**：这是x和y同时发生的支持度与它们独立发生的概率之比。它是**置信度**除以x的概率乘以y的概率；例如，如果我们有x和y同时发生的概率为10%，x的概率为20%，y的概率为30%，那么提升度将是10%（20%乘以30%）或16.67%。'
- en: 'The package in R that you can use to perform a market basket analysis is **arules:
    Mining Association Rules and Frequent Itemsets**. The package offers two different
    methods of finding rules. Why would one have different methods? Quite simply,
    if you have massive datasets, it can become computationally expensive to examine
    all the possible combinations of the products. The algorithms that the package
    supports are **apriori** and **ECLAT**. There are other algorithms to conduct
    a market basket analysis, but apriori is used most frequently, and so, that will
    be our focus.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '你可以在R中使用的用于执行市场篮子分析的包是**arules: Mining Association Rules and Frequent Itemsets**。该包提供两种不同的查找规则的方法。为什么会有不同的方法？简单地说，如果你有大量数据集，检查所有可能的产品组合可能会变得计算成本高昂。该包支持的算法是**apriori**和**ECLAT**。还有其他算法可以进行市场篮子分析，但apriori使用得最频繁，因此，我们将重点关注它。'
- en: 'With apriori, the principle is that, if an itemset is frequent, then all of
    its subsets must also be frequent. A minimum frequency (support) is determined
    by the analyst prior to executing the algorithm, and once established, the algorithm
    will run as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在apriori中，原则是，如果一个项集是频繁的，那么它的所有子集也必须是频繁的。最小频率（支持度）是在执行算法之前由分析师确定的，一旦确定，算法将按以下方式运行：
- en: Let *k=1* (the number of items)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 令 *k=1*（项目数量）
- en: Generate itemsets of a length that are equal to or greater than the specified
    support
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成等于或大于指定支持度的项集长度
- en: Iterate *k + (1...n)*, pruning those that are infrequent (less than the support)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代 *k + (1...n)*，剪枝那些不频繁的（小于支持度）
- en: Stop the iteration when no new frequent itemsets are identified
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当没有新的频繁项集被识别时停止迭代
- en: Once you have an ordered summary of the most frequent itemsets, you can continue
    the analysis process by examining the confidence and lift in order to identify
    the associations of interest.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了最频繁项集的有序摘要，你可以通过检查置信度和提升度来继续分析过程，以识别感兴趣的关联。
- en: Business understanding
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: For our business case, we will focus on identifying the association rules for
    a grocery store. The dataset will be from the `arules` package and is called `Groceries`.
    This dataset consists of actual transactions over a 30-day period from a real-world
    grocery store and consists of 9,835 different purchases. All the items purchased
    are put into one of 169 categories, for example, bread, wine, meat, and so on.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的业务案例，我们将专注于识别杂货店的关联规则。数据集将来自`arules`包，称为`Groceries`。这个数据集包含了一个现实世界杂货店30天内的实际交易，包括9,835种不同的购买。所有购买的物品都被放入169个类别中的一个，例如，面包、酒、肉类等等。
- en: Let's say that we are a start-up microbrewery trying to make a headway in this
    grocery outlet and want to develop an understanding of what potential customers
    will purchase along with beer. This knowledge may just help us in identifying
    the right product placement within the store or support a cross-selling campaign.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是一家初创精酿啤酒厂，试图在这家杂货店取得突破，并希望了解潜在顾客会与啤酒一起购买什么。这种知识可能正好帮助我们确定店内正确的产品摆放位置，或者支持交叉销售活动。
- en: Data understanding and preparation
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'For this analysis, we will only need to load two packages, as well as the `Groceries`
    dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这次分析，我们只需要加载两个包，以及`Groceries`数据集：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This dataset is structured as a sparse matrix object, known as the `transaction`
    class.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的结构是一个稀疏矩阵对象，称为`transaction`类。
- en: So, once the structure is that of the class transaction, our standard exploration
    techniques will not work, but the `arules` package offers us other techniques
    to explore the data. On a side note, if you have a data frame or matrix and want
    to convert it to the `transaction` class, you can do this with a simple syntax,
    using the `as()` function.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦结构变成交易类，我们的标准探索技术将不再适用，但`arules`包为我们提供了其他探索数据的技术。顺便提一下，如果你有一个数据框或矩阵，并想将其转换为`transaction`类，你可以使用简单的语法，通过`as()`函数实现。
- en: 'The following code is for illustrative purposes only, so do not run it:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码仅用于说明，请勿运行：
- en: '`> # transaction.class.name <- as(current.data.frame,"transactions")`.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`> # transaction.class.name <- as(current.data.frame,"transactions")`。'
- en: 'The best way to explore this data is with an item frequency plot using the
    `itemFrequencyPlot()` function in the `arules` package. You will need to specify
    the transaction dataset, the number of items with the highest frequency to plot,
    and whether or not you want the relative or absolute frequency of the items. Let''s
    first look at the absolute frequency and the top `10` items only:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 探索这些数据最好的方式是使用`arules`包中的`itemFrequencyPlot()`函数制作项目频率图。你需要指定交易数据集、要绘制频率最高的项目数量，以及是否需要绘制项目的相对或绝对频率。让我们首先查看绝对频率和前`10`个商品：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_10_01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_10_01.png)'
- en: 'The top item purchased was **whole milk** with roughly **2**,**500** of the
    9,836 transactions in the basket. For a relative distribution of the top 15 items,
    let''s run the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 购买最多的商品是**全脂牛奶**，在9,836笔交易中有大约**2,500**笔。为了显示前15个商品的相对分布，让我们运行以下代码：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output of the preceding command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个命令的输出如下：
- en: '![](img/image_10_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_10_02.png)'
- en: Alas, here we see that beer shows up as the 13th and 15th most purchased item
    at this store. Just under 10 percent of the transactions had purchases of **bottled
    beer** and/or **canned beer**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 殊不知，在这里我们看到啤酒作为第13和第15大购买商品出现在这家商店。大约10%的交易包含了**瓶装啤酒**和/或**罐装啤酒**的购买。
- en: For the purpose of this exercise, this is all we really need to do, therefore,
    we can move right on to the modeling and evaluation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习的目的，我们实际上需要做的就这么多，因此，我们可以直接进入建模和评估阶段。
- en: Modeling and evaluation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模和评估
- en: 'We will start by mining the data for the overall association rules before moving
    on to our rules for beer specifically. Throughout the modeling process, we will
    use the apriori algorithm, which is the appropriately named `apriori()` function
    in the `arules` package. The two main things that we will need to specify in the
    function is the dataset and parameters. As for the parameters, you will need to
    apply judgment when specifying the minimum support, confidence, and the minimum
    and/or maximum length of basket items in an itemset. Using the item frequency
    plots, along with trial and error, let''s set the minimum support at 1 in 1,000
    transactions and minimum confidence at 90 percent. Additionally, let''s establish
    the maximum number of items to be associated as four. The following is the code
    to create the object that we will call `rules`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先挖掘数据以获取整体关联规则，然后再转向针对啤酒的具体规则。在整个建模过程中，我们将使用apriori算法，这是`arules`包中名为`apriori()`的适当命名的函数。在函数中，我们需要指定的主要两件事是数据集和参数。至于参数，在指定最小支持度、置信度以及项目集的最小和/或最大长度时，你需要运用判断力。使用项目频率图，结合试错法，我们将最小支持度设置为1000笔交易中的1，最小置信度设置为90%。此外，我们将关联的项目数量上限设置为四个。以下是我们将创建的名为`rules`的对象的代码：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Calling the object shows how many rules the algorithm produced:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 调用对象显示了算法产生的规则数量：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are a number of ways to examine the rules. The first thing that I recommend
    is to set the number of displayed digits to only two, with the `options()` function
    in base R. Then, sort and inspect the top five rules based on the lift that they
    provide, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以检查规则。我首先推荐的是，使用基础R中的`options()`函数将显示的数字位数设置为仅两位。然后，根据它们提供的提升度对前五条规则进行排序和检查，如下所示：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Lo and behold, the rule that provides the best overall lift is the purchase
    of `liquor` and `red wine` on the probability of purchasing `bottled beer`. I
    have to admit that this is pure chance and not intended on my part. As I always
    say, it is better to be lucky than good. Although, it is still not a very common
    transaction with a support of only 1.9 per 1,000.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 看看，提供最佳整体提升度的规则是购买`酒`和`红酒`的概率，基于购买`瓶装啤酒`。我必须承认，这完全是巧合，并不是我故意为之。正如我经常说的，幸运比好更重要。尽管如此，这仍然不是一个非常常见的交易，支持率仅为每1,000次交易中有1.9次。
- en: 'You can also sort by the support and confidence, so let''s have a look at the
    first `5` `rules` `by="confidence"` in descending order, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以按支持和置信度排序，所以让我们看看按`confidence`降序排列的前`5`条`规则`，如下所示：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can see in the table that `confidence` for these transactions is 100 percent.
    Moving on to our specific study of beer, we can utilize a function in `arules`
    to develop cross tabulations--the `crossTable()` function--and then examine whatever
    suits our needs. The first step is to create a table with our dataset:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在表中看到，这些交易的`confidence`为100%。继续到我们具体的啤酒研究，我们可以利用`arules`中的函数来开发交叉表——`crossTable()`函数——然后检查任何适合我们需求的内容。第一步是创建一个包含我们的数据集的表：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With `tab` created, we can now examine the joint occurrences between the items.
    Here, we will look at just the first three rows and columns:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tab` 创建后，我们现在可以检查项目之间的联合出现情况。在这里，我们将只查看前三个行和列：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you might imagine, shoppers only selected liver loaf 50 times out of the
    9,835 transactions. Additionally, of the `924` times, people gravitated toward
    `sausage`, `10` times they felt compelled to grab `liver loaf`. (Desperate times
    call for desperate measures!) If you want to look at a specific example, you can
    either specify the row and column number or just spell that item out:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，购物者在9,835次交易中只选择了50次肝肉饼。此外，在`924`次中，人们倾向于选择`香肠`，`10`次他们感到不得不抓取`肝肉饼`。（在绝望的时刻需要采取绝望的措施！）如果你想查看一个具体的例子，你可以指定行和列号，或者只拼写那个项目：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This tells us that there were `792` transactions of `bottled beer`. Let''s
    see what the joint occurrence between `bottled beer` and `canned beer` is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们有`792`次交易是`瓶装啤酒`。让我们看看`瓶装啤酒`和`罐装啤酒`之间的联合出现情况：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I would expect this to be low as it supports my idea that people lean toward
    drinking beer from either a bottle or a can. I strongly prefer a bottle. It also
    makes a handy weapon to protect oneself from all these ruffian protesters like
    Occupy Wallstreet and the like.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计这会很低，因为它支持我的观点，即人们倾向于从瓶装或罐装中喝酒。我强烈偏好瓶装。这也使得它成为一件方便的武器，可以用来保护自己免受所有这些流氓抗议者，如占领华尔街和类似的人。
- en: 'We can now move on and derive specific rules for `bottled beer`. We will again
    use the `apriori()` function, but this time, we will add a syntax around `appearance`.
    This means that we will specify in the syntax that we want the left-hand side
    to be items that increase the probability of a purchase of `bottled beer`, which
    will be on the right-hand side. In the following code, notice that I''ve adjusted
    the `support` and `confidence` numbers. Feel free to experiment with your own
    settings:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续前进，并推导出针对`瓶装啤酒`的具体规则。我们再次使用`apriori()`函数，但这次，我们将在`appearance`周围添加语法。这意味着我们将在语法中指定，我们想要左侧是增加购买`瓶装啤酒`概率的项目，这些项目将位于右侧。在下面的代码中，请注意我已经调整了`support`和`confidence`数字。请随意尝试你自己的设置：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We find ourselves with only `4` association rules. We have seen one of them
    already; now let''s bring in the other three rules in descending order by lift:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现自己只有`4`条关联规则。我们已经看到了其中之一；现在让我们按提升度降序引入其他三条规则：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In all of the instances, the purchase of `bottled beer` is associated with
    booze, either `liquor` and/or `red wine` , which is no surprise to anyone. What
    is interesting is that `white wine` is not in the mix here. Let''s take a closer
    look at this and compare the joint occurrences of `bottled beer` and types of
    wine:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些实例中，购买 `瓶装啤酒` 与酒精饮料相关联，无论是 `烈酒` 和/或 `红酒`，这对任何人来说都不足为奇。有趣的是，这里没有 `白葡萄酒`。让我们更仔细地看看这一点，并比较
    `瓶装啤酒` 和葡萄酒类型的联合出现：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It's interesting that 25 percent of the time, when someone purchased `red wine`,
    they also purchased `bottled beer`; but with `white wine`, a joint purchase only
    happened in 12 percent of the instances. We certainly don't know why in this analysis,
    but this could potentially help us to determine how we should position our product
    in this grocery store. Another thing before we move on is to look at a plot of
    the rules. This is done with the `plot()` function in the `arulesViz` package.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，25% 的时间，当有人购买 `红酒` 时，他们也购买了 `瓶装啤酒`；但与 `白葡萄酒` 相比，联合购买只发生在 12% 的情况下。我们当然不知道在这个分析中为什么，但这可能有助于我们确定如何在这个杂货店定位我们的产品。在我们继续之前，还有一个事情要看看规则的图。这是通过
    `arulesViz` 包中的 `plot()` 函数完成的。
- en: 'There are many graphic options available. For this example, let''s specify
    that we want a `graph`, showing `lift`, and the rules provided and shaded by `confidence`.
    The following syntax will provide this accordingly:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多图形选项可用。对于这个例子，让我们指定我们想要一个 `图形`，显示 `提升` 和由 `信心` 提供和阴影的规则。以下语法将相应地提供：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output of the preceding command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面命令的结果：
- en: '![](img/image_10_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_10_03.png)'
- en: This graph shows that **liquor**/**red wine** provides the best **lift** and
    the highest level of **confidence** with both the **size** of the circle and its
    shading.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示，**烈酒**/**红酒**在圆圈的大小及其阴影方面提供了最佳的**提升**和最高的**信心**水平。
- en: What we've just done in this simple exercise is show how easy it is with R to
    conduct a market basket analysis. It doesn't take much imagination to figure out
    the analytical possibilities that one can include with this technique, for example,
    in corporate customer segmentation, longitudinal purchase history, and so on,
    as well as how to use it in ad displays, co-promotions, and so on. Now let's move
    on to a situation where customers rate items, and learn how to build and test
    recommendation engines.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个简单的练习中所做的是展示使用 R 进行市场篮子分析是多么容易。想象一下，使用这种技术可以包括哪些分析可能性，例如，在企业客户细分、纵向购买历史等方面，以及如何将其用于广告展示、联合促销等。现在让我们转向一个客户对商品进行评分的情况，并学习如何构建和测试推荐引擎。
- en: An overview of a recommendation engine
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐引擎概述
- en: 'We will now focus on situations where users have provided rankings or ratings
    on previously viewed or purchased items. There are two primary categories of designing
    recommendation systems: *collaborative filtering and content-based* (Ansari, Essegaier,
    and Kohli, 2000). The former category is what we will concentrate on, as this
    is the focus of the `recommenderlab` R package that we will be using.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将关注用户对先前查看或购买的商品提供排名或评分的情况。设计推荐系统有两个主要类别：*协同过滤和基于内容的*（Ansari, Essegaier,
    和 Kohli, 2000）。我们将重点关注前者，因为这是我们将要使用的 `recommenderlab` R 包的重点。
- en: For content-based approaches, the concept is to link user preferences with item
    attributes. These attributes may be things such as the genre, cast, or storyline
    for a movie or TV show recommendation. As such, recommendations are based entirely
    on what the user provides as ratings; there is no linkage to what anyone else
    recommends. This has the advantage over content-based approaches in that when
    a new item is added, it can be recommended to a user if it matches their profile,
    instead of relying on other users to rate it first (the so-called "first rater
    problem"). However, content-based methods can suffer when limited content is available,
    either because of the domain or when a new user enters the system. This can result
    in non-unique recommendations, that is, poor recommendations (Lops, Gemmis, and
    Semeraro, 2011).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于内容的推荐方法，其概念是将用户偏好与项目属性相联系。这些属性可能包括电影或电视剧推荐的类型、演员阵容或剧情。因此，推荐完全基于用户提供的评分；没有与其他人推荐的关联。这比基于内容的推荐方法有优势，因为当添加新项目时，如果它与用户的个人资料匹配，就可以向用户推荐，而不是依赖其他用户首先对其进行评分（所谓的“第一个评分者问题”）。然而，当可用的内容有限时，基于内容的方法可能会受到影响，无论是由于领域限制还是当新用户进入系统时。这可能导致非唯一的推荐，即较差的推荐（Lops,
    Gemmis, and Semeraro, 2011）。
- en: In collaborative filtering, the recommendations are based on the many ratings
    provided by some or all of the individuals in the database. Essentially, it tries
    to capture the wisdom of the crowd.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在协同过滤中，推荐基于数据库中某些或所有个体的许多评分。本质上，它试图捕捉大众的智慧。
- en: 'For collaborative filtering, we will focus on the following four methods:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于协同过滤，我们将关注以下四种方法：
- en: '**User-based collaborative filtering** (**UBCF**)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于用户的协同过滤**（**UBCF**）'
- en: '**Item-based collaborative filtering** (**IBCF**)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于项目的协同过滤**（**IBCF**）'
- en: '**Singular value decomposition** (**SVD**)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）'
- en: '**Principal components analysis** (**PCA**)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: We will look at these methods briefly before moving on to the business case.
    It is also important to understand that `recommenderlab` was not designed to be
    used as a real-world implementation tool, but rather as a laboratory tool in order
    to research algorithms provided in the package as well as algorithms that you
    wish to experiment with on your own.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续研究案例之前，我们将简要地探讨这些方法。重要的是要理解，`recommenderlab`并非设计为用于实际应用工具，而是一个实验室工具，用于研究包中提供的算法以及您希望自行实验的算法。
- en: User-based collaborative filtering
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于用户的协同过滤
- en: In UBCF, the algorithm finds *missing ratings for a user by first finding a
    neighborhood of similar users and then aggregating the ratings of these users
    to form a prediction* (Hahsler, 2011). The neighborhood is determined by selecting
    either the KNN that is the most similar to the user we are making predictions
    for or by some similarity measure with a minimum threshold. The two similarity
    measures available in `recommenderlab` are **pearson correlation coefficient**
    and **cosine similarity**. I will skip the formulas for these measures as they
    are readily available in the package documentation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在UBCF中，算法首先找到与目标用户最相似的用户邻域，然后汇总这些用户的评分以形成预测（Hahsler, 2011）。邻域是通过选择与目标用户最相似的KNN或通过某种最小阈值的相似度度量来确定的。`recommenderlab`中可用的两种相似度度量是**皮尔逊相关系数**和**余弦相似度**。我将跳过这些度量的公式，因为它们在包的文档中很容易找到。
- en: Once the neighborhood method is decided on, the algorithm identifies the neighbors
    by calculating the similarity measure between the individual of interest and their
    neighbors on only those items that were rated by both. Through a scoring scheme,
    say, a simple average, the ratings are aggregated in order to make a predicted
    score for the individual and item of interest.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦决定了邻域方法，算法通过仅计算感兴趣个体与其邻居在共同评分的项目上的相似度来识别邻居。通过评分方案，例如简单的平均，汇总评分以对感兴趣的个人和项目做出预测评分。
- en: 'Let''s look at a simple example. In the following matrix, there are six individuals
    with ratings on four movies, with the exception of my rating for *Mad Max*. Using
    *k=1*, the nearest neighbor is **Homer**, with **Bart** a close second; even though
    **Flanders** hated the **Avengers** as much as I did. So, using Homer''s rating
    for **Mad Max**, which is **4**, the predicted rating for me would also be a **4**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的例子。在以下矩阵中，有六个人对四部电影进行了评分，除了我对 *Mad Max* 的评分。使用 *k=1*，最近的邻居是 **Homer**，其次是
    **Bart**；尽管 **弗兰德斯**和我一样讨厌 **复仇者联盟**。因此，使用Homer对 **Mad Max** 的评分，即 **4**，对我的预测评分也将是
    **4**：
- en: '![](img/image_10_04.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片10_04](img/image_10_04.png)'
- en: There are a number of ways to weigh the data and/or control the bias. For instance,
    **Flanders** is quite likely to have lower ratings than the other users, so normalizing
    the data where the new rating score is equal to the user rating for an item minus
    the average for that user for all the items is likely to improve the rating accuracy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以衡量数据并/或控制偏差。例如，**弗兰德斯**的用户评分很可能比其他用户低，因此，在数据标准化时，将新的评分分数设置为用户对某项物品的评分减去该用户对所有物品的平均评分，可能会提高评分的准确性。
- en: The weakness of UBCF is that, to calculate the similarity measure for all the
    possible users, the entire database must be kept in memory, which can be quite
    computationally expensive and time-consuming.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: UBCF的弱点在于，为了计算所有可能用户的相似度度量，必须将整个数据库保留在内存中，这可能会非常计算量大且耗时。
- en: Item-based collaborative filtering
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于物品的协同过滤
- en: As you might have guessed, IBCF uses the similarity between the items and not
    users to make a recommendation. *The assumption behind this approach is that users
    will prefer items that are similar to other items they like* (Hahsler, 2011).
    The model is built by calculating a pairwise similarity matrix of all the items.
    The popular similarity measures are Pearson correlation and cosine similarity.
    To reduce the size of the similarity matrix, one can specify to retain only the
    k-most similar items. However, limiting the size of the neighborhood may significantly
    reduce the accuracy, leading to poorer performance versus UCBF.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，IBCF使用物品之间的相似性而不是用户之间的相似性来做出推荐。*这种方法的假设是，用户将更喜欢与他们喜欢的其他物品相似的物品*（Hahsler，2011）。模型是通过计算所有物品的双边相似度矩阵来构建的。流行的相似度度量包括皮尔逊相关性和余弦相似性。为了减少相似度矩阵的大小，可以指定仅保留k个最相似的物品。然而，限制邻域的大小可能会显著降低准确性，导致性能不如UCBF。
- en: 'Continuing with our simplified example, if we examine the following matrix,
    with *k=1* the item most similar to **Mad Max** is **American Sniper**, and we
    can thus take that rating as the prediction for **Mad Max**, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的简化示例，如果我们检查以下矩阵，当 *k=1* 时，与 **Mad Max** 最相似的项目是 **美国狙击手**，因此我们可以将那个评分作为
    **Mad Max** 的预测，如下所示：
- en: '![](img/image_10_05.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片10_05](img/image_10_05.png)'
- en: Singular value decomposition and principal components analysis
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单值分解和主成分分析
- en: It is quite common to have a dataset where the number of users and items number
    in the millions. Even if the rating matrix is not that large, it may be beneficial
    to reduce the dimensionality by creating a smaller (lower-rank) matrix that captures
    most of the information in the higher-dimension matrix. This may potentially allow
    you to capture important latent factors and their corresponding weights in the
    data. Such factors could lead to important insights, such as the movie genre or
    book topics in the rating matrix. Even if you are unable to discern meaningful
    factors, the techniques may filter out the noise in the data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，用户和物品的数量数以百万计的情况相当常见。即使评分矩阵不是那么大，通过创建一个较小的（低秩）矩阵来捕捉高维矩阵中的大部分信息，可能也有利于降低维度。这可能会潜在地允许你捕捉数据中的重要潜在因素及其对应的权重。这些因素可能导致重要的见解，例如评分矩阵中的电影类型或书籍主题。即使你无法识别有意义的因素，这些技术也可能过滤掉数据中的噪声。
- en: One issue with large datasets is that you will likely end up with a sparse matrix
    that has many ratings missing. One weakness of these methods is that they will
    not work on a matrix with missing values, which must be imputed. As with any data
    imputation task, there are a number of techniques that one can try and experiment
    with, such as using the mean, median, or code as zeroes. The default for `recommenderlab`
    is to use the median.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集的一个问题是，你可能会得到一个稀疏矩阵，其中包含许多缺失的评分。这些方法的弱点是它们不能在包含缺失值的矩阵上工作，这些缺失值必须被估计。与任何数据估计任务一样，有几种技术可以尝试和实验，例如使用平均值、中位数或用零编码。`recommenderlab`的默认值是使用中位数。
- en: 'So, what is SVD? It is simply a method for matrix factorization, and can help
    transform a set of correlated features to a set of uncorrelated features. Say
    that you have a matrix called **A**. This matrix will factor into three matrices:
    **U**, **D**, and **V^T**. U is an orthogonal matrix, D is a non-negative, diagonal
    matrix, and V^T is a transpose of an orthogonal matrix. Now, let''s look at our
    rating matrix and walk through an example using R.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，SVD是什么？它是一种矩阵分解的方法，可以帮助将一组相关特征转换为不相关特征的一组。比如说，你有一个名为**A**的矩阵。这个矩阵将分解为三个矩阵：**U**、**D**和**V^T**。U是一个正交矩阵，D是一个非负的对角矩阵，V^T是一个正交矩阵的转置。现在，让我们看看我们的评分矩阵，并使用R来通过一个例子进行说明。
- en: 'The first thing that we will do is recreate the rating matrix (think of it
    as matrix **A**, as shown in the following code):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的是重新创建评分矩阵（可以将其视为矩阵**A**，如下面的代码所示）：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will use the `svd()` function in base R to create the three matrices
    described above, which R calls `$d`, `$u`, and `$v`. You can think of the `$u`
    values as an individual''s loadings on that factor and `$v` as a movie''s loadings
    on that dimension. For example, `Mad Max` loads on dimension one at -0.116 (1st
    row, 4th column):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用基础R中的`svd()`函数来创建上述三个矩阵，R将其称为`$d`、`$u`和`$v`。你可以将`$u`值视为个体在该因子上的载荷，而`$v`值则视为电影在该维度上的载荷。例如，`Mad
    Max`在第一个维度上的载荷为-0.116（第1行，第4列）：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It is easy to explore how much variation is explained by reducing the dimensionality.
    Let''s sum the diagonal numbers of `$d`, then look at how much of the variation
    we can explain with just two factors, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 探索通过降低维度可以解释多少变化是很容易的。让我们先求出`$d`的对角线数字之和，然后看看我们只用两个因子就能解释多少变化，如下所示：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With two of the four factors, we are able to capture just over 85 percent of
    the total variation in the full matrix. You can see the scores that the reduced
    dimensions would produce. To do this, we will create a function. (Many thanks
    to the [www.stackoverflow.com](http://www.stackoverflow.com) respondents who helped
    me put this function together.) This function will allow us to specify the number
    of factors that are to be included for a prediction. It calculates a rating value
    by multiplying the `$u` matrix times the `$v` matrix times the `$d` matrix:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用四个因子中的两个，我们能够捕捉到整个矩阵中超过85%的总变化。你可以看到减少维度将产生的分数。为此，我们将创建一个函数。（非常感谢[www.stackoverflow.com](http://www.stackoverflow.com)上的响应者，他们帮助我把这个函数组合起来。）这个函数将允许我们指定要包含在预测中的因子数量。它通过将`$u`矩阵乘以`$v`矩阵再乘以`$d`矩阵来计算评分值：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'By specifying `n=4` and calling the function, we can recreate the original
    rating matrix:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定`n=4`并调用该函数，我们可以重新创建原始的评分矩阵：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Alternatively, we can specify `n=2` and examine the resulting matrix:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以指定`n=2`并检查得到的矩阵：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So, with SVD, you can reduce the dimensionality and possibly identify the meaningful
    latent factors.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用SVD，你可以降低维度，并可能识别出有意义的潜在因子。
- en: If you went through the prior chapter, you will see the similarities with PCA.
    In fact, the two are closely related and often used interchangeably as they both
    utilize matrix factorization. You may be asking what is the difference? In short,
    PCA is based on the covariance matrix, which is symmetric. This means that you
    start with the data, compute the covariance matrix of the centered data, diagonalize
    it, and create the components.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了前一章，你会看到与PCA的相似之处。事实上，这两个方法是密切相关的，并且经常可以互换使用，因为它们都利用矩阵分解。你可能想知道它们之间的区别？简而言之，PCA基于协方差矩阵，它是对称的。这意味着你从数据开始，计算中心化数据的协方差矩阵，对其进行对角化，并创建成分。
- en: 'Let''s apply a portion of the PCA code from the prior chapter to our data in
    order to see how the difference manifests itself:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将前一章中的一部分PCA代码应用到我们的数据中，以查看差异如何体现出来：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can see that PCA is easier to interpret. Notice how `American Sniper` and
    `Mad Max` have high loadings on the first component, while only `Avengers` has
    a high loading on the second component. Additionally, these two components account
    for 94 percent of the total variance in the data.  It is noteworthy to include
    that, in the time between the first and second editions of this book, PCA has
    become unavailable.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Having applied a simplistic rating matrix to the techniques of collaborative
    filtering, let's move on to a more complex example using real-world data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding and recommendations
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This business case is a joke, literally. Maybe it is more appropriate to say
    a bunch of jokes, as we will use the `Jester5k` data from the `recommenderlab`
    package. This data consists of 5,000 ratings on 100 jokes sampled from the Jester
    Online Joke Recommender System. It was collected between April 1999 and May 2003,
    and all the users have rated at least 36 jokes (Goldberg, Roeder, Gupta, and Perkins,
    2001). Our goal is to compare the recommendation algorithms and select the best
    one.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: As such, I believe it is important to lead off with a statistical joke to put
    one in the proper frame of mind. I'm not sure of how to properly provide attribution
    for this one, but it is popular all over the Internet.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: A statistician's wife had twins. He was delighted. He rang the minister who
    was also delighted. "Bring them to church on Sunday and we'll baptize them", said
    the minister. "No", replied the statistician. "Baptize one. We'll keep the other
    as a control."
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding, preparation, and recommendations
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The one library that we will need for this exercise is `recommenderlab`. The
    package was developed by the Southern Methodist University''s Lyle Engineering
    Lab, and they have an excellent website with supporting documentation at [https://lyle.smu.edu/IDA/recommenderlab/](https://lyle.smu.edu/IDA/recommenderlab/):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The rating matrix contains `362106` total ratings. It is quite easy to get
    a list of a user''s ratings. Let''s look at user number `10`. The following output
    is abbreviated for the first five jokes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can also look at the mean rating for a user (user `10`) and/or the mean
    rating for a specific joke (joke `1`), as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'One method to get a better understanding of the data is to plot the ratings
    as a histogram, both the raw data and after normalization. We will do this with
    the `getRating()` function from `recommenderlab`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_06.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
- en: 'The `normalize()` function in the package centers the data by subtracting the
    mean of the ratings of the joke from that joke''s rating. As the preceding distribution
    is slightly biased towards the positive ratings, normalizing the data can account
    for this, thus yielding a more normal distribution but still showing a slight
    skew towards the positive ratings, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following is the output of the preceding command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_07.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: 'Before modeling and evaluation, it is quite easy to create the `train` and
    `test` datasets with the `recommenderlab` package with the `evaluationScheme()`
    function. Let''s do an 80/20 split of the data for the `train` and `test` sets.
    You can also choose k-fold cross-validation and bootstrap resampling if you desire.
    We will also specify that for the `test` set, the algorithm will be given 15 ratings.
    This means that the other rating items will be used to compute the error. Additionally,
    we will specify what the threshold is for a good rating; in our case, greater
    than or equal to `5`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'With the `train` and `test` data established, we will now begin to model and
    evaluate the different recommenders: user-based, item-based, popular, SVD, PCA,
    and random.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Modeling, evaluation, and recommendations
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to build and test our recommendation engines, we can use the same
    function, `Recommender()`, merely changing the specification for each technique.
    In order to see what the package can do and explore the parameters available for
    all six techniques, you can examine the registry. Looking at the following IBCF,
    we can see that the default is to find 30 neighbors using the cosine method with
    the centered data while the missing data is not coded as a zero:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here is how you can put together the algorithms based on the `train` data.
    For simplicity, let''s use the default algorithm settings. You can adjust the
    parameter settings by simply including your changes in the function as a list:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, using the `predict()` and `getData()` functions, we will get the predicted
    ratings for the 15 items of the `test` data for each of the algorithms, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We will examine the error between the predictions and unknown portion of the
    `test` data using the `calcPredictionAccuracy()` function. The output will consist
    of `RMSE`, `MSE`, and `MAE` for all the methods. We''ll examine `UBCF` by itself.
    After creating the objects for all five methods, we can build a table by creating
    an object with the `rbind()` function and giving names to the rows with the `rownames()`
    function:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can see in the output that the user-based and popular algorithms slightly
    outperform IBCF and SVD and all outperform random predictions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: There is another way to compare methods using the `evaluate()` function. Making
    comparisons with `evaluate()` allows one to examine additional performance metrics
    as well as performance graphs. As the UBCF and Popular algorithms performed the
    best, we will look at them along with IBCF.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The first task in this process is to create a list of the algorithms that we
    want to compare, as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For this example, let''s compare the top `5`, `10`, and `15` joke recommendations:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Note that by executing the command, you will receive an output on how long
    it took to run the algorithm. We can now examine the performance using the `avg()`
    function:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that the performance metrics for `POPULAR` and `UBCF` are nearly the same.
    One could say that the simpler-to-implement popular-based algorithm is probably
    the better choice for a model selection.  We can plot and compare the results
    as **Receiver Operating Characteristic Curves** (**ROC**), comparing `TPR` and
    `FPR` or precision/recall, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following is the output of the preceding command:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_08.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'To get the precision/recall curve plot you only need to specify `"prec"` in
    the `plot` function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_09.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: You can clearly see in the plots that the popular-based and user-based algorithms
    are almost identical and outperform the item-based one. The `annotate=TRUE` parameter
    provides numbers next to the point that corresponds to the number of recommendations
    that we called for in our evaluation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'This was simple, but what are the actual recommendations from a model for a
    specific individual? This is quite easy to code as well. First, let''s build a
    `"popular"` recommendation engine on the full dataset. Then, we will find the
    top five recommendations for the first two raters. We will use the `Recommend()`
    function and apply it to the whole dataset, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we just need to get the top five recommendations for the first two raters
    and produce them as a list:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It is also possible to see a rater''s specific rating score for each of the
    jokes by specifying this in the `predict()` syntax and then putting it in a matrix
    for review. Let''s do this for ten individuals (raters `300` through `309`) and
    three jokes (`71` through `73`):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The numbers in the matrix indicate the predicted rating scores for the jokes
    that the individual rated, while the NAs indicate those that the user did not
    rate.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final effort on this data will show how to build recommendations for those
    situations where the ratings are binary, that is, good or bad or 1 or 0\. We will
    need to turn the ratings into this binary format with 5 or greater as a 1 and
    less than 5 as 0\. This is quite easy to do with `Recommenderlab` using the `binarize()`
    function and specifying `minRating=5`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, we will need to have our data reflect the number of ratings equal to one
    in order to match what we need the algorithm to use for the training. For argument''s
    sake, let''s go with greater than 10\. The code to create the subset of the necessary
    data is shown in the following lines:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You will need to create `evaluationScheme`. In this instance, we will go with
    `cross-validation`. The default k-fold in the function is `10`, but we can also
    safely go with `k=5`, which will reduce our computation time:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'For comparison purposes, the algorithms under evaluation will include `random`,
    `popular`, and `UBCF`:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'It is now time to build our model, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Forgoing the table of performance metrics, let''s take a look at the plots:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_10.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: '[PRE46]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_11.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: The user-based algorithm slightly outperforms the popular-based one, but you
    can clearly see that they are both superior to any random recommendation. In our
    business case, it will come down to the judgment of the decision-making team as
    to which algorithm to implement.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Sequential data analysis
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are known knowns. These are things we know that we know. There are known
    unknowns. That is to say, there are things that we know we don't know. But there
    are also unknown unknowns. There are things we don't know we don't know.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '- Donald Rumsfeld, Former Secretary of Defense'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The very first business question I came across after the 1st edition was published
    revolved around product sequential analysis. The team worked on complicated Excel
    spreadsheets and pivot tables, along with a bunch of SAS code, to produce insights.
    After coming across this problem, I explored what could be done with R and was
    pleasantly surprised to stumble into the `TraMineR` package, specifically designed
    for just such a task. I believe the application of R to the problem would have
    greatly simplified the analysis.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The package was designed for the social sciences, but it can be used in just
    about every situation where you want to mine and learn how observation's states
    evolve over discrete periods or events (longitudinal data). A classic use would
    be as in the case mentioned above where you want to understand the order in which
    customers purchase products. This would facilitate a recommendation engine of
    sorts where you can create the probability of the next purchase, as I've heard
    it being referred to as a next logical product offer. Another example could be
    in healthcare, examining the order that a patient receives treatments and/or medications,
    or even physician prescribing habits. I've worked on such tasks, creating simple
    and complex Markov chains to build models and create forecasts. Indeed, `TraMineR`
    allows the creation of Markov chain transition matrices to support such models.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we will examine does the hard work of creating, counting, and plotting
    the various combinations of transitions over time, also incorporating covariates.
    That will be our focus, but keep in mind that one can also build a dissimilarity
    matrix for clustering. The core features covered in the practical exercise will
    consist of the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Transition rates
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: duration within each state
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence frequency
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Sequential analysis applied
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this exercise, I''ve created an artificial dataset; to follow along, you
    can download it from GitHub:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/datameister66/data/blob/master/sequential.csv](https://github.com/datameister66/data/blob/master/sequential.csv)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also datasets available with the package and tutorials are available.
    My intent was to create something new that mirrored situations I have encountered.
    I developed it completely from random (with some supervision), so it does not
    match any real world data. It consists of 5,000 observations, with each observation,
    the history of a customer and nine variables:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Cust_segment--a factor variable indicating the customer's assigned segment (see
    [Chapter 8](f3f7c511-1b7f-4500-ba78-dd208b227ae0.xhtml), *Cluster Analysis*)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eight discrete purchase events named `Purchase1` through `Purchase8`; remember,
    these are events and not time-based, which is to say that a customer could have
    purchased all eight products at the same time, but in a specific order
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within each purchase variable are the generic names of the product, seven possible
    products to be exact. They are named `Product_A` through `Product_G`.  What are
    these products? Doesn't matter! Use your imagination or apply it to your own situation.
    If a customer only purchased one product, then `Purchase1` would contain the name
    of that product and the other variables would be NULL.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we load the file as a dataframe. The structure output is abbreviated for
    clarity:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Time for some exploration of the data, starting with a table of the customer
    segment counts and a count of the first product purchased:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`Segment1` is the largest segment, and the most purchased initial product is
    `Product A`. However, is it the most purchased product overall? This code will
    provide the answer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Yes, `ProductA` is the most purchased. The count of NULL values is 22,390.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you may be wondering if we can just build some summaries without much trouble,
    and that is surely the case. Here, I put the `count()` and `arrange()` functions
    from the `dplyr` package to good use to examine the frequency of sequences between
    the first and second purchase:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We see that the most frequent sequences are the purchase of `ProductA` followed
    by another purchase of `ProductA`, along with the purchase of `ProductD` followed
    by no additional purchases. What is interesting is the frequency of similar product
    purchases.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now begin further examination using the `TraMineR` package. To begin,
    the data needs to be put into an object of class sequence with the `seqdef()`
    function. This should consist of only the sequences and not any covariates. Also,
    you can specify the distance of tick marks in plotting functions with `xstep =
    n`. In our case, we will have a tick mark for every event:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now explore the data further. Let''s look at the index plot, which produces
    the sequences of the first 10 observations. You can use indices with the data
    to examine as many observations and event periods as you wish:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_12.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'One can plot all observations with `seqIplot()`, but given the size of the
    data, it doesn''t produce anything meaningful. A plot of distribution by state
    is more meaningful:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_13.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'With this plot, it is easy to see the distribution of product purchases by
    state. We can also group this plot by segments and determine whether there are
    differences:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_14.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
- en: 'Here, we clearly see that `Segment2` has a higher proportion of `ProductA`
    purchases than the other segments. Another way to see that insight is with the
    modal plot:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_15.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'This is interesting. Around 50% of `Segment2` purchased `ProductA` first, while
    segment 4''s most frequent initial purchase was `ProductD`. Another plot that
    may be of interest, but I believe not in this case, is the mean time plot. It
    plots the average "time" spent in each state. Since we are not time-based, it
    doesn''t make sense, but I include for your consideration:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let''s supplement our preceding code and look further at the transition of
    sequences. This code creates an object of sequences, then narrows that down to
    those sequences with an occurrence of at least 5%, then plots the top 10 sequences:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output of the preceding command is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_10_17.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
- en: Note that the plot shows the percentage frequency of the sequences through the
    eight transition states. If you want to narrow that down to, say, the first two
    transitions, you would do that in the `seqecreate()` function using indices.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s see how you can use the data to create a transition matrix.
    This matrix shows the probability of transitioning from one state to the next.
    In our case, it provides the probability of purchasing the next product. As I
     mentioned before, this can also be used in a Markov chain simulation to develop
    a forecast. That is outside the scope of this chapter, but if you are interested
    I recommend having a look at the `markovchain` package in R and its tutorial on
    how to implement the procedure. Two possible transition matrices are available.
    One that incorporates the overall probability through all states and another that
    develops a transition matrix from one state to the next, that is, time-varying
    matrices. This code shows how to develop the former. To produce the latter, just
    specify `"time.varying = TRUE"` in the function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output shows rows 2 through 4 and columns 1 through 3\. The matrix shows
    us that the probability of having Product A and the next purchase being `ProductA`
    is almost 42%, while it is 19% to not purchase another product, and 17% to purchase
    `ProductB`. The final output we will examine is the probability of not purchasing
    another product for each prior purchase:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Of course, the matrix shows that the probability of not purchasing another product
    after not purchasing is 100%. Also notice that the probability of not purchasing
    after acquiring Product D is 33%. Implications for Segment4? Perhaps.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: What is fascinating is that this analysis was done with only a few lines of
    code and didn't require the use of Excel or some expensive visualization software.
    Have longitudinal data? Give sequential analysis a try!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the goal was to provide an introduction to how to use R in
    order to build and test association rule mining (market basket analysis) and recommendation
    engines. Market basket analysis is trying to understand what items are purchased
    together. With recommendation engines, the goal is to provide a customer with
    other items that they will enjoy based on how they have rated previously viewed
    or purchased items. It is important to understand the R package that we used (`recommenderlab`)
    for recommendation is not designed for implementation, but to develop and test
    algorithms. The other thing examined here was longitudinal data and mining it
    to learn valuable insights, in our case, the order in which customers purchased
    our products. Such an analysis has numerous applications, from marketing campaigns
    to healthcare.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: We are now going to shift gears back to supervised learning. In the next chapter,
    we are going to cover some of the most exciting and important methods in practical
    machine learning, that is multi-class classification and creating ensemble models,
    something that is very easy to do in R with recent package releases.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
