- en: Decision Trees and Ensemble Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和集成学习
- en: In this chapter, we're going to discuss binary decision trees and ensemble methods.
    Even if they're probably not the most common methods for classification, they
    offer a good level of simplicity and can be adopted in many tasks that don't require
    a high level of complexity. They're also quite useful when it's necessary to show
    how a decision process works because they are based on a structure that can be
    shown easily in presentations and described step by step.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论二进制决策树和集成方法。尽管它们可能不是最常见的分类方法，但它们提供了良好的简单性，并且可以在许多不需要高复杂性的任务中应用。当需要展示决策过程的工作原理时，它们也非常有用，因为它们基于一种可以在演示中轻松展示并逐步描述的结构。
- en: Ensemble methods are a powerful alternative to complex algorithms because they
    try to exploit the statistical concept of majority vote. Many weak learners can
    be trained to capture different elements and make their own predictions, which
    are not globally optimal, but using a sufficient number of elements, it's statistically
    probable that a majority will evaluate correctly. In particular, we're going to
    discuss random forests of decision trees and some boosting methods that are slightly
    different algorithms that can optimize the learning process by focusing on misclassified
    samples or by continuously minimizing a target loss function.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法是复杂算法的有力替代品，因为它们试图利用多数投票的统计概念。可以训练许多弱学习器来捕捉不同的元素并做出自己的预测，这些预测不是全局最优的，但使用足够数量的元素，从统计上讲，大多数预测将是正确的。特别是，我们将讨论决策树的随机森林和一些提升方法，这些方法稍微不同的算法可以通过关注误分类样本或通过持续最小化目标损失函数来优化学习过程。
- en: Binary decision trees
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二进制决策树
- en: 'A binary decision tree is a structure based on a sequential decision process.
    Starting from the root, a feature is evaluated and one of the two branches is
    selected. This procedure is repeated until a final leaf is reached, which normally
    represents the classification target we’re looking for. Considering other algorithms,
    decision trees seem to be simpler in their dynamics; however, if the dataset is
    splittable while keeping an internal balance, the overall process is intuitive
    and rather fast in its predictions. Moreover, decision trees can work efficiently
    with unnormalized datasets because their internal structure is not influenced
    by the values assumed by each feature. In the following figure, there are plots
    of an unnormalized bidimensional dataset and the cross-validation scores obtained
    using a logistic regression and a decision tree:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制决策树是一种基于顺序决策过程的结构。从根节点开始，评估一个特征并选择两个分支中的一个。这个过程会重复进行，直到达到一个最终的叶子节点，它通常代表我们寻找的分类目标。与其他算法相比，决策树在动态上似乎更简单；然而，如果数据集在保持内部平衡的同时可以分割，整个过程在预测上既直观又相对快速。此外，决策树可以有效地处理未归一化的数据集，因为它们的内部结构不受每个特征所取值的影响。在下图中，有未归一化的二维数据集的图和用逻辑回归和决策树获得的交叉验证分数：
- en: '![](img/2a649bb6-16ae-498f-8793-ec09a532f5a7.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a649bb6-16ae-498f-8793-ec09a532f5a7.png)'
- en: The decision tree always achieves a score close to 1.0, while the logistic regression
    has an average slightly greater than 0.6\. However, without proper limitations,
    a decision tree could potentially grow until a single sample (or a very low number)
    is present in every node. This situation drives to overfit the model, and the
    tree becomes unable to generalize correctly. Using a consistent test set or cross-validation
    can help in avoiding this problem; however, in the section dedicated to scikit-learn
    implementation, we're going to discuss how to limit the growth of the tree.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树始终达到接近 1.0 的分数，而逻辑回归的平均分数略大于 0.6。然而，如果没有适当的限制，决策树可能会潜在地生长到每个节点中只有一个样本（或非常少的样本）。这种情况会导致模型过拟合，并且树无法正确泛化。使用一致的测试集或交叉验证可以帮助避免这个问题；然而，在关于
    scikit-learn 实现的部分，我们将讨论如何限制树的增长。
- en: Binary decisions
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二进制决策
- en: 'Let''s consider an input dataset *X*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个输入数据集 *X*：
- en: '![](img/b6bcf22c-1fc0-43e8-9754-45fff47b2740.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6bcf22c-1fc0-43e8-9754-45fff47b2740.png)'
- en: 'Every vector is made up of *m* features, so each of them can be a good candidate
    to create a node based on the (feature, threshold) tuple:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个向量由 *m* 个特征组成，因此每个特征都可以作为基于（特征，阈值）元组的节点的好候选：
- en: '![](img/c271f7b4-3755-4aa5-895b-99fe6e8e5125.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c271f7b4-3755-4aa5-895b-99fe6e8e5125.png)'
- en: According to the feature and the threshold, the structure of the tree will change.
    Intuitively, we should pick the feature that best separates our data in other
    words, a perfect separating feature will be present only in a node and the two
    subsequent branches won't be based on it anymore. In real problems, this is often
    impossible, so it's necessary to find the feature that minimizes the number of
    following decision steps.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特征和阈值，树的结构将发生变化。直观上，我们应该选择最能分离我们的数据的特征，换句话说，一个完美的分离特征将只存在于一个节点，接下来的两个分支将不再基于它。在现实问题中，这往往是不可行的，因此需要找到最小化后续决策步骤数量的特征。
- en: 'For example, let''s consider a class of students where all males have dark
    hair and all females have blonde hair, while both subsets have samples of different
    sizes. If our task is to determine the composition of the class, we can start
    with the following subdivision:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑一个学生群体，其中所有男生都有深色头发，所有女生都有金色头发，而这两个子集都有不同大小的样本。如果我们的任务是确定班级的组成，我们可以从以下细分开始：
- en: '![](img/394a80d4-1e15-4d40-adeb-fd7118942aec.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/394a80d4-1e15-4d40-adeb-fd7118942aec.png)'
- en: 'However, the block **Dark color?** will contain both males and females (which
    are the targets we want to classify). This concept is expressed using the term
    **purity** (or, more often, its opposite concept, **impurity**). An ideal scenario
    is based on nodes where the impurity is null so that all subsequent decisions
    will be taken only on the remaining features. In our example, we can simply start
    from the color block:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，包含**深色？**的块将包含男性和女性（这是我们想要分类的目标）。这个概念用术语**纯净度**（或者更常见的是其对立概念，**杂质**）来表示。一个理想的场景是基于杂质为零的节点，这样后续的所有决策都只基于剩余的特征。在我们的例子中，我们可以简单地从颜色块开始：
- en: '![](img/c956d527-24cf-45c5-8370-675d3938117d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c956d527-24cf-45c5-8370-675d3938117d.png)'
- en: The two resulting sets are now pure according to the color feature, and this
    can be enough for our task. If we need further details, such as hair length, other
    nodes must be added; their impurity won't be null because we know that there are,
    for example, both male and female students with long hair.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 根据颜色特征，现在得到的两个集合是纯净的，这足以满足我们的任务。如果我们需要更详细的细节，例如发长，必须添加其他节点；它们的杂质不会为零，因为我们知道，例如，既有长发男生也有长发女生。
- en: 'More formally, suppose we define the selection tuple as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，假设我们定义选择元组如下：
- en: '![](img/cf6063f4-1921-4999-9c64-1e323aa8332e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf6063f4-1921-4999-9c64-1e323aa8332e.png)'
- en: Here, the first element is the index of the feature we want to use to split
    our dataset at a certain node (it will be the entire dataset only at the beginning;
    after each step, the number of samples decreases), while the second is the threshold
    that determines left and right branches. The choice of the best threshold is a
    fundamental element because it determines the structure of the tree and, therefore,
    its performance. The goal is to reduce the residual impurity in the least number
    of splits so as to have a very short decision path between the sample data and
    the classification result.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一个元素是我们想要在某个节点上分割数据集所使用的特征的索引（它只会在开始时是整个数据集；每一步之后，样本数都会减少），而第二个是确定左右分支的阈值。最佳阈值的选择是一个基本元素，因为它决定了树的结构，因此也决定了其性能。目标是减少分割中剩余的杂质，以便在样本数据和分类结果之间有非常短的决策路径。
- en: 'We can also define a total impurity measure by considering the two branches:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过考虑两个分支来定义总杂质度量：
- en: '![](img/c8f37002-3e38-4d4e-b11e-177273d11e6c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c8f37002-3e38-4d4e-b11e-177273d11e6c.png)'
- en: Here, *D* is the whole dataset at the selected node, *D[left]* and *D[right]*
    are the resulting subsets (by applying the selection tuple), and the *I* are impurity
    measures.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*D* 是所选节点上的整个数据集，*D[left]* 和 *D[right]* 是通过应用选择元组得到的结果子集，而 *I* 是杂质度量。
- en: Impurity measures
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 杂质度量
- en: 'To define the most used impurity measures, we need to consider the total number
    of target classes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义最常用的杂质度量，我们需要考虑目标类别的总数：
- en: '![](img/0ddfc3e6-2e30-4dd5-8552-7417a5a52162.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ddfc3e6-2e30-4dd5-8552-7417a5a52162.png)'
- en: In a certain node *j*, we can define the probability *p(i|j)*where *i* is an
    index [1, *n*] associated with each class. In other words, according to a frequentist
    approach, this value is the ratio between the number of samples belonging to class
    *i* and the total number of samples belonging to the selected node.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个节点 *j*，我们可以定义概率 *p(i|j)*，其中 *i* 是与每个类别关联的索引 [1, *n*]。换句话说，根据频率主义方法，这个值是属于类别
    *i* 的样本数与属于所选节点的总样本数之间的比率。
- en: Gini impurity index
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gini不纯度指数
- en: 'The Gini impurity index is defined as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Gini不纯度指数定义为：
- en: '![](img/756bb2ca-86ba-4bb4-a005-ad55c466998b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/756bb2ca-86ba-4bb4-a005-ad55c466998b.png)'
- en: Here, the sum is always extended to all classes. This is a very common measure
    and it's used as a default value by scikit-learn. Given a sample, the Gini impurity
    measures the probability of a misclassification if a label is randomly chosen
    using the probability distribution of the branch. The index reaches its minimum
    (0.0) when all the samples of a node are classified into a single category.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，总和总是扩展到所有类别。这是一个非常常见的度量，并且被scikit-learn用作默认值。给定一个样本，Gini不纯度衡量的是如果使用分支的概率分布随机选择标签时发生错误分类的概率。当节点中所有样本都被分类到单个类别时，该指标达到最小值（0.0）。
- en: Cross-entropy impurity index
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵不纯度指数
- en: 'The cross-entropy measure is defined as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵度量定义为：
- en: '![](img/8e62e0f8-d0d9-408c-98e6-af159ab08fae.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8e62e0f8-d0d9-408c-98e6-af159ab08fae.png)'
- en: This measure is based on information theory, and assumes null values only when
    samples belonging to a single class are present in a split, while it is maximum
    when there's a uniform distribution among classes (which is one of the worst cases
    in decision trees because it means that there are still many decision steps until
    the final classification). This index is very similar to the Gini impurity, even
    though, more formally, the cross-entropy allows you to select the split that minimizes
    the uncertainty about the classification, while the Gini impurity minimizes the
    probability of misclassification.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个度量基于信息理论，并且仅在分割中存在属于单个类别的样本时假设为空值，而在类别之间有均匀分布时达到最大值（这是决策树中最坏的情况之一，因为它意味着还有许多决策步骤直到最终分类）。这个指标与Gini不纯度非常相似，尽管更正式地说，交叉熵允许你选择最小化关于分类不确定性的分割，而Gini不纯度最小化错误分类的概率。
- en: 'In [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml), *Important Elements
    in Machine Learning*, we defined the concept of mutual information *I(X; Y) =
    H(X) - H(X|Y)* as the amount of information shared by both variables, thereby
    reducing the uncertainty about *X* provided by the knowledge of *Y*. We can use
    this to define the information gain provided by a split:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml)《机器学习中的重要元素》中，我们定义了互信息的概念
    *I(X; Y) = H(X) - H(X|Y)*，作为两个变量共享的信息量，从而减少了由 *Y* 的知识提供的关于 *X* 的不确定性。我们可以使用这个来定义分割提供的信息增益：
- en: '![](img/7cf49f95-bbd3-48d1-95ff-58eb3b5069b6.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7cf49f95-bbd3-48d1-95ff-58eb3b5069b6.png)'
- en: 'When growing a tree, we start by selecting the split that provides the highest
    information gain and proceed until one of the following conditions is verified:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当生长树时，我们首先选择提供最高信息增益的分割，并继续进行，直到满足以下条件之一：
- en: All nodes are pure
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有节点都是纯净的
- en: The information gain is null
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息增益为零
- en: The maximum depth has been reached
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已达到最大深度
- en: Misclassification impurity index
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误分类不纯度指数
- en: 'The misclassification impurity is the simplest index, defined as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分类不纯度是最简单的指标，定义为：
- en: '![](img/656ff3b2-b969-48be-a787-c2f969543a8c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/656ff3b2-b969-48be-a787-c2f969543a8c.png)'
- en: In terms of quality performance, this index is not the best choice because it's
    not particularly sensitive to different probability distributions (which can easily
    drive the selection to a subdivision using Gini or cross-entropy indexes).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在质量性能方面，这个指标并不是最佳选择，因为它对不同的概率分布（这可以很容易地驱动选择使用Gini或交叉熵指标进行细分）并不特别敏感。
- en: Feature importance
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'When growing a decision tree with a multidimensional dataset, it can be useful
    to evaluate the importance of each feature in predicting the output values. In
    [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml), *Feature Selection and
    Feature Engineering*, we discussed some methods to reduce the dimensionality of
    a dataset by selecting only the most significant features. Decision trees offer
    a different approach based on the impurity reduction determined by every single
    feature. In particular, considering a feature *x[i]*, its importance can be determined
    as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用多维数据集生长决策树时，评估每个特征在预测输出值中的重要性可能很有用。在[第3章](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml)《特征选择和特征工程》中，我们讨论了一些通过仅选择最显著的特征来降低数据集维度的方法。决策树提供了一种基于每个特征确定的不纯度减少的不同方法。特别是，考虑一个特征
    *x[i]*，其重要性可以确定如下：
- en: '![](img/e20b55ba-7249-45f6-abce-66899ac81638.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e20b55ba-7249-45f6-abce-66899ac81638.png)'
- en: The sum is extended to all nodes where *x[i]* is used, and *N[k]* is the number
    of samples reaching the node *k*. Therefore, the importance is a weighted sum
    of all impurity reductions computed considering only the nodes where the feature
    is used to split them. If the Gini impurity index is adopted, this measure is
    also called **Gini importance**.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 求和扩展到所有使用*x[i]*的节点，而*N[k]*是达到节点*k*的样本数量。因此，重要性是所有仅考虑使用特征分割的节点的杂质减少的加权总和。如果采用Gini杂质指数，这个度量也称为**Gini重要性**。
- en: Decision tree classification with scikit-learn
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行决策树分类
- en: 'scikit-learn contains the `DecisionTreeClassifier` class, which can train a
    binary decision tree with Gini and cross-entropy impurity measures. In our example,
    let''s consider a dataset with three features and three classes:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn包含`DecisionTreeClassifier`类，它可以训练具有Gini和交叉熵杂质度量的二叉决策树。在我们的例子中，让我们考虑一个具有三个特征和三个类别的数据集：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s first consider a classification with default Gini impurity:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先考虑一个默认Gini杂质度量的分类：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A very interesting feature is given by the possibility of exporting the tree
    in `Graphviz` format and converting it into a PDF.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有趣的功能是能够将树以`Graphviz`格式导出，并将其转换为PDF。
- en: Graphviz is a free tool that can be downloaded from [http://www.graphviz.org](http://www.graphviz.org).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Graphviz是一个免费工具，可以从[http://www.graphviz.org](http://www.graphviz.org)下载。
- en: 'To export a trained tree, it is necessary to use the built-in function `export_graphviz()`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要导出训练好的树，必须使用内置函数`export_graphviz()`：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this case, we have used `A`, `B`, and `C` as feature names and `C1`, `C2`,
    and `C3` as class names. Once the file has been created, it''s possible converting
    to PDF using the command-line tool:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用了`A`、`B`和`C`作为特征名称，`C1`、`C2`和`C3`作为类别名称。一旦文件创建完成，可以使用命令行工具将其转换为PDF：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The graph for our example is rather large, so in the following feature you
    can see only a part of a branch:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例图相当大，所以在下图中您只能看到分支的一部分：
- en: '![](img/a660ff28-558c-4cb6-99bf-fe44060afc50.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a660ff28-558c-4cb6-99bf-fe44060afc50.png)'
- en: 'As you can see, there are two kinds of nodes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，有两种类型的节点：
- en: Nonterminal, which contains the splitting tuple (as feature <= threshold) and
    a positive impurity measure
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非终端，它包含分割元组（作为特征 <= 阈值）和正杂质度量
- en: Terminal, where the impurity measure is null and a final target class is present
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 终端，其中杂质度量值为空且存在一个最终的目标类别
- en: 'In both cases, you can always check the number of samples. This kind of graph
    is very useful in understanding how many decision steps are needed. Unfortunately,
    even if the process is quite simple, the dataset structure can lead to very complex
    trees, while other methods can immediately find out the most appropriate class.
    Of course, not all features have the same importance. If we consider the root
    of the tree and the first nodes, we find features that separate a lot of samples;
    therefore, their importance must be higher than that of all terminal nodes, where
    the residual number of samples is minimum. In scikit-learn, it''s possible to
    assess the Gini importance of each feature after training a model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，您都可以始终检查样本数量。这种类型的图在理解需要多少决策步骤非常有用。不幸的是，即使过程相当简单，数据集的结构可能导致非常复杂的树，而其他方法可以立即找到最合适的类别。当然，不是所有特征的重要性都相同。如果我们考虑树的根和第一个节点，我们会发现能够分离大量样本的特征；因此，它们的重要性必须高于所有终端节点的重要性，在终端节点中剩余的样本数量最少。在scikit-learn中，在训练模型后可以评估每个特征的Gini重要性：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following figure shows a plot of the importances:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了重要性的绘图：
- en: '![](img/d4af9918-ddb0-45ff-86df-e7713741a7fe.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d4af9918-ddb0-45ff-86df-e7713741a7fe.png)'
- en: The most important features are 6, 3, 4, and 7, while feature 2, for example,
    separates a very small number of samples, and can be considered noninformative
    for the classification task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的特征是6、3、4和7，而例如特征2将非常少的样本分开，可以认为对于分类任务来说是非信息的。
- en: 'In terms of efficiency, a tree can also be pruned using the `max_depth` parameter;
    however, it''s not always so simple to understand which value is the best (grid
    search can help in this task). On the other hand, it''s easier to decide what
    the maximum number of features to consider at each split should be. The parameter
    `max_features` can be used for this purpose:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在效率方面，也可以使用`max_depth`参数对树进行剪枝；然而，理解哪个值是最好的并不总是那么简单（网格搜索可以帮助完成这项任务）。另一方面，决定在每个分割点考虑的最大特征数更容易。可以使用`max_features`参数来完成这个目的：
- en: If it's a number, the value is directly taken into account at each split
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是一个数字，该值将在每个分割时直接考虑
- en: If it's `'auto'` or `'sqrt'`, the square root of the number of features will
    be adopted
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是 `'auto'` 或 `'sqrt'`，将采用特征数量的平方根
- en: If it's `'log2'`, the logarithm (base 2) will be used
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是 `'log2'`，将使用以2为底的对数
- en: If it's `'None'`, all the features will be used (this is the default value)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是 `'None'`，将使用所有特征（这是默认值）
- en: 'In general, when the number of total features is not too high, the default
    value is the best choice, although it''s useful to introduce a small compression
    (via `sqrt` or `log2`) when too many features can interfere among themselves,
    reducing the efficiency. Another parameter useful for controlling both performance
    and efficiency is `min_samples_split`, which specifies the minimum number of samples
    to consider for a split. Some examples are shown in the following snippet:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当总特征数量不是太高时，默认值是最好的选择，尽管当太多特征可能相互干扰时，引入小的压缩（通过 `sqrt` 或 `log2`）是有用的。另一个有助于控制性能和效率的参数是
    `min_samples_split`，它指定了考虑分割的最小样本数。以下是一些示例：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As already explained, finding the best parameters is generally a difficult task,
    and the best way to carry it out is to perform a grid search while including all
    the values that could affect the accuracy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，找到最佳参数通常是一项困难的任务，而执行它的最佳方式是在包括所有可能影响准确性的值的同时进行网格搜索。
- en: 'Using logistic regression on the previous set (only for comparison), we get:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个集合上使用逻辑回归（仅用于比较），我们得到：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So the score is higher, as expected. However, the original dataset was quite
    simple, and based on the concept of having a single cluster per class. This allows
    a simpler and more precise linear separation. If we consider a slightly different
    scenario with more variables and a more complex structure (which is hard to capture
    by a linear classifier), we can compare an ROC curve for both linear regression
    and decision trees:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如预期的那样，得分更高。然而，原始数据集相当简单，基于每个类别只有一个簇的概念。这允许更简单、更精确的线性分离。如果我们考虑一个具有更多变量和更复杂结构（线性分类器难以捕捉）的略微不同的场景，我们可以比较线性回归和决策树的ROC曲线：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting ROC curve is shown in the following figure:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的ROC曲线显示在下图中：
- en: '![](img/bf28527f-cb09-4f43-9f80-f3d3cec8c2e9.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bf28527f-cb09-4f43-9f80-f3d3cec8c2e9.png)'
- en: 'Using a grid search with the most common parameters on the MNIST digits dataset,
    we can get:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST数字数据集上使用最常见的参数进行网格搜索，我们可以得到：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case, the element that impacted accuracy the most is the minimum number
    of samples to consider for a split. This is reasonable, considering the structure
    of this dataset and the need to have many branches to capture even small changes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，影响准确率最大的因素是考虑分割的最小样本数。考虑到这个数据集的结构和需要有许多分支来捕捉甚至微小的变化，这是合理的。
- en: Ensemble learning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: 'Until now, we have trained models on single instances, iterating an algorithm
    in order to minimize a target loss function. This approach is based on so-called
    strong learners, or methods that are optimized to solve a specific problem by
    looking for the best possible solution. Another approach is based on a set of
    weak learners that can be trained in parallel or sequentially (with slight modifications
    on the parameters) and used as an ensemble based on a majority vote or the averaging
    of results. These methods can be classified into two main categories:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在单个实例上训练模型，通过迭代算法来最小化目标损失函数。这种方法基于所谓的强学习器，或通过寻找最佳可能解决方案来优化解决特定问题的方法。另一种方法是基于一组弱学习器，这些学习器可以并行或顺序（对参数进行轻微修改）训练，并基于多数投票或结果平均作为集成使用。这些方法可以分为两大类：
- en: '**Bagged (or Bootstrap) trees**: In this case, the ensemble is built completely.
    The training process is based on a random selection of the splits and the predictions
    are based on a majority vote. Random forests are an example of bagged tree ensembles.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagged（或Bootstrap）树**：在这种情况下，集成是完整构建的。训练过程基于随机选择的分割，预测基于多数投票。随机森林是Bagged树集成的一个例子。'
- en: '**Boosted trees**: The ensemble is built sequentially, focusing on the samples
    that have been previously misclassified. Examples of boosted trees are AdaBoost
    and gradient tree boosting.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boosted树**：集成是按顺序构建的，专注于先前被错误分类的样本。Boosted树的例子包括AdaBoost和梯度提升树。'
- en: Random forests
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'A random forest is a set of decision trees built on random samples with a different
    policy for splitting a node: Instead of looking for the best choice, in such a
    model, a random subset of features (for each tree) is used, trying to find the
    threshold that best separates the data. As a result, there will be many trees
    trained in a weaker way and each of them will produce a different prediction.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一组基于随机样本构建的决策树，其分割节点的策略不同：在这种模型中，不是寻找最佳选择，而是使用随机特征子集（对于每棵树），试图找到最佳的数据分割阈值。因此，将训练出许多以较弱方式训练的树，并且每棵树都会产生不同的预测。
- en: There are two ways to interpret these results; the more common approach is based
    on a majority vote (the most voted class will be considered correct). However,
    scikit-learn implements an algorithm based on averaging the results, which yields
    very accurate predictions. Even if they are theoretically different, the probabilistic
    average of a trained random forest cannot be very different from the majority
    of predictions (otherwise, there should be different stable points); therefore
    the two methods often drive to comparable results.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 解释这些结果有两种方式；更常见的方法是基于多数投票（得票最多的类别将被认为是正确的）。然而，scikit-learn实现了一个基于平均结果的算法，这产生了非常准确的预测。即使它们在理论上不同，训练好的随机森林的概率平均也不可能与多数预测相差很大（否则，应该有不同的稳定点）；因此，这两种方法通常会导致可比的结果。
- en: 'As an example, let''s consider the MNIST dataset with random forests made of
    a different number of trees:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑由不同数量的树组成的随机森林MNIST数据集：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting plot is shown in the following figure:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了生成的图表：
- en: '![](img/f68b4c9a-a179-49c2-a94d-4eac0c13aee3.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f68b4c9a-a179-49c2-a94d-4eac0c13aee3.png)'
- en: 'As expected, the accuracy is low when the number of trees is under a minimum
    threshold; however, it starts increasing rapidly with fewer than 10 trees. A value
    between 20 and 30 trees yields the optimal result (95%), which is higher than
    for a single decision tree. When the number of trees is low, the variance of the
    model is very high and the averaging process produces many incorrect results;
    however, increasing the number of trees reduces the variance and allows the model
    to converge to a very stable solution. scikit-learn also offers a variance that
    enhances the randomness in selecting the best threshold. Using the `ExtraTreesClassifier`
    class, it''s possible to implement a model that randomly computes thresholds and
    picks the best one. As discussed in the official documentation, this allows us
    to further reduce the variance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，当树的数量低于最小阈值时，准确性较低；然而，当树的数量少于10棵时，它开始迅速增加。在20到30棵树之间可以获得最佳结果（95%），这比单棵决策树要高。当树的数量较少时，模型的方差非常高，平均过程会产生许多错误的结果；然而，增加树的数量可以减少方差，并使模型收敛到一个非常稳定的解。scikit-learn还提供了一个方差，它增强了选择最佳阈值时的随机性。使用`ExtraTreesClassifier`类，可以实现一个随机计算阈值并选择最佳值的模型。正如官方文档中讨论的那样，这使我们能够进一步减少方差：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The results (with the same number of trees) in terms of accuracy are slightly
    better, as shown in the following figure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在准确性方面，具有相同树数量的结果略好，如下图所示：
- en: '![](img/11da180a-5d20-4db6-be64-ecf3af8a4271.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/11da180a-5d20-4db6-be64-ecf3af8a4271.png)'
- en: Feature importance in random forests
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林中的特征重要性
- en: 'The concept of feature importance that we previously introduced can also be
    applied to random forests, computing the average over all trees in the forest:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前介绍的特征重要性概念也可以应用于随机森林，通过计算森林中所有树的平均值：
- en: '![](img/1018e9d6-c672-4a27-9924-dd0ba2ba4108.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1018e9d6-c672-4a27-9924-dd0ba2ba4108.png)'
- en: 'We can easily test the importance evaluation with a dummy dataset containing
    50 features with 20 noninformative elements:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地使用包含50个特征和20个非信息元素的虚拟数据集来测试重要性评估：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The importance of the first 50 features according to a random forest with 20
    trees is plotted in the following figure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了由20棵树组成的随机森林计算出的前50个特征的重要性：
- en: '![](img/29c72b5b-4378-47c0-862c-29260e8218a4.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/29c72b5b-4378-47c0-862c-29260e8218a4.png)'
- en: As expected, there are a few *very* important features, a block of features
    with a medium importance, and a tail containing features that have quite a low
    influence on the predictions. This type of plot is also useful during the analysis
    stage to better understand how the decision process is structured. With multidimensional
    datasets, it's rather difficult to understand the influence of every factor, and
    sometimes many important business decisions are made without a complete awareness
    of their potential impact. Using decision trees or random forests, it's possible
    to assess the "real" importance of all features and exclude all the elements under
    a fixed threshold. In this way, a complex decision process can be simplified and,
    at the same time, be partially denoised.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，有几个**非常重要**的特征，一个中等重要性的特征块，以及一个包含对预测影响相当低的特征的尾部。这种类型的图表在分析阶段也很有用，可以帮助更好地理解决策过程是如何构建的。对于多维数据集，理解每个因素的影响相当困难，有时许多重要的商业决策在没有完全意识到它们潜在影响的情况下就被做出了。使用决策树或随机森林，可以评估所有特征的“真实”重要性，并排除所有低于固定阈值的元素。这样，复杂的决策过程就可以简化，同时部分去噪。
- en: AdaBoost
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost
- en: Another technique is called **AdaBoost** (short for **Adaptive Boosting**) and
    works in a slightly different way than many other classifiers. The basic structure
    behind this can be a decision tree, but the dataset used for training is continuously
    adapted to force the model to focus on those samples that are misclassified. Moreover,
    the classifiers are added sequentially, so a new one boosts the previous one by
    improving the performance in those areas where it was not as accurate as expected.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术被称为**AdaBoost**（即**自适应提升**），其工作方式与许多其他分类器略有不同。其背后的基本结构可以是决策树，但用于训练的数据集会持续适应，迫使模型专注于那些被错误分类的样本。此外，分类器是按顺序添加的，因此新的一个通过提高那些它不如预期准确的地方的性能来增强前一个。
- en: At each iteration, a weight factor is applied to each sample so as to increase
    the importance of the samples that are wrongly predicted and decrease the importance
    of others. In other words, the model is repeatedly boosted, starting as a very
    weak learner until the maximum `n_estimators` number is reached. The predictions,
    in this case, are always obtained by majority vote.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，都会对每个样本应用一个权重因子，以增加错误预测样本的重要性并降低其他样本的重要性。换句话说，模型会反复增强，从一个非常弱的学习者开始，直到达到最大的`n_estimators`数量。在这种情况下，预测总是通过多数投票获得。
- en: 'In the scikit-learn implementation, there''s also a parameter called `learning_rate`
    that weighs the effect of each classifier. The default value is 1.0, so all estimators
    are considered to have the same importance. However, as we can see with the MNIST
    dataset, it''s useful to decrease this value so that each contribution is weakened:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn实现中，还有一个名为`learning_rate`的参数，它衡量每个分类器的影响。默认值是1.0，因此所有估计器都被认为是同等重要的。然而，正如我们从MNIST数据集中看到的那样，降低这个值是有用的，这样每个贡献都会减弱：
- en: '[PRE12]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result is shown in the following figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下图中：
- en: '![](img/f8b2c4ac-f53a-47a2-a06d-734e1138572f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8b2c4ac-f53a-47a2-a06d-734e1138572f.png)'
- en: 'The accuracy is not so high as in the previous examples; however, it''s possible
    to see that when the boosting adds about 20-30 trees, it reaches a stable value.
    A grid search on `learning_rate` could allow you to find the optimal value; however,
    the sequential approach in this case is not preferable. A classic random forest,
    which works with a fixed number of trees since the first iteration, performs better.
    This may well be due to the strategy adopted by AdaBoost; in this set, increasing
    the weight of the correctly classified samples and decreasing the strength of misclassifications
    can produce an oscillation in the loss function, with a final result that is not
    the optimal minimum point. Repeating the experiment with the Iris dataset (which
    is structurally much simpler) yields better results:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率不如前面的例子高；然而，可以看到当提升添加大约20-30棵树时，它达到了一个稳定值。对`learning_rate`进行网格搜索可以让你找到最佳值；然而，在这种情况下，顺序方法并不理想。一个经典的随机森林，从第一次迭代开始就使用固定数量的树，表现更好。这很可能是由于AdaBoost采用的策略；在这个集合中，增加正确分类样本的权重并降低错误分类的强度可能会在损失函数中产生振荡，最终结果不是最优的最小点。用Iris数据集（结构上要简单得多）重复实验可以得到更好的结果：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this case, a learning rate of 1.0 is the best choice, and it''s easy to
    understand that the boosting process can be stopped after a few iterations. In
    the following figure, you can see a plot showing the accuracy for this dataset:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，学习率为1.0是最好的选择，很容易理解提升过程可以在几次迭代后停止。在下图中，你可以看到显示此数据集准确率的图表：
- en: '![](img/d10abc62-9d30-48a7-b8bd-4f0ebe340f9b.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d10abc62-9d30-48a7-b8bd-4f0ebe340f9b.png)'
- en: After about 10 iterations, the accuracy becomes stable (the residual oscillation
    can be discarded), reaching a value that is compatible with this dataset. The
    advantage of using AdaBoost can be appreciated in terms of resources; it doesn't
    work with a fully configured set of classifiers and the whole set of samples.
    Therefore, it can help save time when training on large datasets.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 经过大约10次迭代后，准确率变得稳定（残差振荡可以被忽略），达到与这个数据集兼容的值。使用AdaBoost的优势在于资源利用；它不与一组完全配置好的分类器和整个样本集一起工作。因此，在大型数据集上训练时，它可以帮助节省时间。
- en: Gradient tree boosting
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度树提升
- en: 'Gradient tree boosting is a technique that allows you to build a tree ensemble
    step by step with the goal of minimizing a target loss function. The generic output
    of the ensemble can be represented as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度树提升是一种技术，允许你逐步构建一个树集成，目标是最小化目标损失函数。集成的一般输出可以表示为：
- en: '![](img/a294a9fd-b0b4-43c7-a5e5-3fa85a71f887.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a294a9fd-b0b4-43c7-a5e5-3fa85a71f887.png)'
- en: 'Here, *f[i](x)* is a function representing a weak learner. The algorithm is
    based on the concept of adding a new decision tree at each step so as to minimize
    the global loss function using the steepest gradient descent method (see [https://en.wikipedia.org/wiki/Method_of_steepest_descent](https://en.wikipedia.org/wiki/Method_of_steepest_descent),
    for further information):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f[i](x)*是一个表示弱学习者的函数。该算法基于在每个步骤添加一个新的决策树的概念，以使用最速下降法（参见[https://en.wikipedia.org/wiki/Method_of_steepest_descent](https://en.wikipedia.org/wiki/Method_of_steepest_descent)，获取更多信息）来最小化全局损失函数：
- en: '![](img/7b2d39b4-17ea-47a0-ab11-0799233a1087.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7b2d39b4-17ea-47a0-ab11-0799233a1087.png)'
- en: 'After introducing the gradient, the previous expression becomes:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入梯度后，前面的表达式变为：
- en: '![](img/4c2818e8-e0f9-4770-805e-2a12349731ac.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4c2818e8-e0f9-4770-805e-2a12349731ac.png)'
- en: 'scikit-learn implements the `GradientBoostingClassifier` class, supporting
    two classification loss functions:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn实现了`GradientBoostingClassifier`类，支持两种分类损失函数：
- en: Binomial/multinomial negative log-likelihood (which is the default choice)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二项式/多项式负对数似然（这是默认选择）
- en: Exponential (such as AdaBoost)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指数（例如 AdaBoost）
- en: 'Let''s evaluate the accuracy of this method using a more complex dummy dataset
    made up of 500 samples with four features (three informative and one redundant)
    and three classes:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个由500个样本组成、具有四个特征（三个信息性和一个冗余）和三个类别的更复杂的虚拟数据集来评估此方法的准确率：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can collect the cross-validation average accuracy for a number of estimators
    in the range (1, 50). The loss function is the default one (multinomial negative
    log-likelihood):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以收集一定范围内（1, 50）的多个估计器的交叉验证平均准确率。损失函数是默认的（多项式负对数似然）：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'While increasing the number of estimators (parameter `n_estimators`), it''s
    important to decrease the learning rate (parameter `learning_rate`). The optimal
    value cannot be easily predicted; therefore, it''s often useful to perform a grid
    search. In our example, I''ve set a very high learning rate at the beginning (5.0),
    which converges to 0.05 when the number of estimators is equal to 100\. This is
    not a perfect choice (unacceptable in most real cases!), and it has been made
    only to show the different accuracy performances. The results are shown in the
    following figure:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在增加估计器数量（参数`n_estimators`）时，重要的是要降低学习率（参数`learning_rate`）。最佳值难以预测；因此，进行网格搜索通常很有用。在我们的例子中，我一开始设置了非常高的学习率（5.0），当估计器数量达到100时，收敛到0.05。这并不是一个完美的选择（在大多数实际情况下都是不可接受的！），这样做只是为了展示不同的准确率性能。结果如下所示：
- en: '![](img/fb8c7747-9a50-4406-9e86-646860d593f4.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fb8c7747-9a50-4406-9e86-646860d593f4.png)'
- en: As it's possible to see, the optimal number of estimators is about 50, with
    a learning rate of 0.1\. The reader can try different combinations and compare
    the performances of this algorithm with the other ensemble methods.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，最佳估计器数量约为50，学习率为0.1。读者可以尝试不同的组合，并比较此算法与其他集成方法的性能。
- en: Voting classifier
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票分类器
- en: 'A very interesting ensemble solution is offered by the class `VotingClassifier`,
    which isn''t an actual classifier but a wrapper for a set of different ones that
    are trained and evaluated in parallel. The final decision for a prediction is
    taken by majority vote according to two different strategies:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类`VotingClassifier`提供了一个非常有趣的集成解决方案，它不是一个实际的分类器，而是一组不同分类器的包装，这些分类器是并行训练和评估的。预测的最终决策是根据两种不同的策略通过多数投票来确定的：
- en: '**Hard voting**: In this case, the class that received the major number of
    votes, *N[c](y[t])*, will be chosen:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬投票**：在这种情况下，获得最多投票的类别，即*N[c](y[t])*，将被选择：'
- en: '![](img/50ee8383-9a24-45df-bcf6-452d5f607f28.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/50ee8383-9a24-45df-bcf6-452d5f607f28.png)'
- en: '**Soft voting**: In this case, the probability vectors for each predicted class
    (for all classifiers) are summed up and averaged. The winning class is the one
    corresponding to the highest value:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软投票**：在这种情况下，每个预测类（对于所有分类器）的概率向量被相加并平均。获胜的类别是对应最高值的类别：'
- en: '![](img/5a9b01d2-6cf5-4792-97a0-8ca60ad399c2.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5a9b01d2-6cf5-4792-97a0-8ca60ad399c2.png)'
- en: 'Let''s consider a dummy dataset and compute the accuracy with a hard voting
    strategy:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个虚拟数据集，并使用硬投票策略计算准确率：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For our examples, we are going to consider three classifiers: logistic regression,
    decision tree (with default Gini impurity), and an SVM (with a polynomial kernel
    and `probability=True` in order to generate the probability vectors). This choice
    has been made only for didactic purposes and may not be the best one. When creating
    an ensemble, it''s useful to consider the different features of each involved
    classifier and avoid "duplicate" algorithms (for example, a logistic regression
    and a linear SVM or a perceptron are likely to yield very similar performances).
    In many cases, it can be useful to mix nonlinear classifiers with random forests
    or AdaBoost classifiers. The reader can repeat this experiment with other combinations,
    comparing the performance of each single estimator and the accuracy of the voting
    classifier:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将考虑三个分类器：逻辑回归、决策树（默认使用Gini不纯度），以及一个SVM（使用多项式核，并将`probability=True`设置为生成概率向量）。这个选择仅出于教学目的，可能不是最佳选择。在创建集成时，考虑每个涉及分类器的不同特征并避免“重复”算法（例如，逻辑回归和线性SVM或感知器可能会产生非常相似的性能）是有用的。在许多情况下，将非线性分类器与随机森林或AdaBoost分类器混合可能很有用。读者可以用其他组合重复此实验，比较每个单一估计器的性能和投票分类器的准确率：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Computing the cross-validation accuracies, we get:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 计算交叉验证准确率，我们得到：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The accuracies of each single classifier and of the ensemble are plotted in
    the following figure:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单一分类器和集成方法的准确率在以下图中展示：
- en: '![](img/9cd9bfad-be11-4f1c-b9c0-64bb0a5ec04a.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9cd9bfad-be11-4f1c-b9c0-64bb0a5ec04a.png)'
- en: 'As expected, the ensemble takes advantage of the different algorithms and yields
    better performance than any single one. We can now repeat the experiment with
    soft voting, considering that it''s also possible to introduce a weight vector
    (through the parameter `weights`) to give more or less importance to each classifier:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，集成方法利用了不同的算法，其性能优于任何单一算法。现在我们可以用软投票重复实验，考虑到也可以通过参数`weights`引入权重向量，以给予每个分类器更多或更少的重视：
- en: '![](img/bef91f33-3792-4e56-bfbc-974b1f8ef89a.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bef91f33-3792-4e56-bfbc-974b1f8ef89a.png)'
- en: 'For example, considering the previous figure, we can decide to give more importance
    to the logistic regression and less to the decision tree and SVM:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑前面的图，我们可以决定给予逻辑回归更多的重视，而给予决策树和SVM较少的重视：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Repeating the same calculations for the cross-validation accuracies, we get:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 重复相同的计算用于交叉验证准确率，我们得到：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The resulting plot is shown in the following figure:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下所示：
- en: '![](img/e9d252d4-2b89-4a6d-b0de-599c98eaadfa.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e9d252d4-2b89-4a6d-b0de-599c98eaadfa.png)'
- en: Weighting is not limited to the soft strategy. It can also be applied to hard
    voting, but in that case, it will be used to filter (reduce or increase) the number
    of actual occurrences.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分配不仅限于软策略。它也可以应用于硬投票，但在此情况下，它将用于过滤（减少或增加）实际发生次数的数量。
- en: '![](img/412a2a2a-34f5-43ed-8605-f6537b94992d.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/412a2a2a-34f5-43ed-8605-f6537b94992d.png)'
- en: Here, *N[c](y[t],w)* is the number of votes for each target class, each of them
    multiplied by the corresponding classifier weighting factor.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N[c](y[t],w)*是每个目标类别的投票数，每个投票数都乘以相应的分类器权重因子。
- en: A voting classifier can be a good choice whenever a single strategy is not able
    to reach the desired accuracy threshold; while exploiting the different approaches,
    it's possible to capture many microtrends using only a small set of strong (but
    sometimes limited) learners.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当单一策略无法达到所需的准确度阈值时，投票分类器可以是一个不错的选择；在利用不同的方法的同时，仅使用一小组强大（但有时有限）的学习器就可以捕捉到许多微观趋势。
- en: References
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Louppe G., Wehenkel L., Sutera A., and Geurts P., *Understanding variable importances
    in forests of randomized trees*, NIPS Proceedings 2013.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Louppe G.，Wehenkel L.，Sutera A.，和Geurts P.，*在随机树森林中理解变量重要性*，NIPS进程2013。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced decision trees as a particular kind of classifier.
    The basic idea behind their concept is that a decision process can become sequential
    by using splitting nodes, where, according to the sample, a branch is chosen until
    we reach a final leaf. In order to build such a tree, the concept of impurity
    was introduced; starting from a complete dataset, our goal is to find a split
    point that creates two distinct sets that should share the minimum number of features
    and, at the end of the process, should be associated with a single target class.
    The complexity of a tree depends on the intrinsic purity—in other words, when
    it's always easy to determine a feature that best separates a set, the depth will
    be lower. However, in many cases, this is almost impossible, so the resulting
    tree needs many intermediate nodes to reduce the impurity until it reaches the
    final leaves.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了决策树作为一种特定的分类器。其概念背后的基本思想是，通过使用分裂节点，决策过程可以变成一个顺序过程，其中根据样本，选择一个分支，直到我们达到最终的叶子节点。为了构建这样的树，引入了不纯度的概念；从完整的数据集开始，我们的目标是找到一个分割点，创建两个具有最小特征数且在过程结束时应与单个目标类别相关联的独立集合。树的复杂性取决于内在的不纯度——换句话说，当总是容易确定一个最佳分离集合的特征时，深度会较低。然而，在许多情况下，这几乎是不可行的，因此生成的树需要许多中间节点来减少不纯度，直到达到最终的叶子节点。
- en: 'We also discussed some ensemble learning approaches: random forests, AdaBoost,
    gradient tree boosting and voting classifiers. They are all based on the idea
    of training several weak learners and evaluating their predictions using a majority
    vote or an average. However, while a random forest creates a set of decision trees
    that are partially randomly trained, AdaBoost and gradient boost trees adopt the
    technique of boosting a model by adding a new one, step after step, and focusing
    only on those samples that have been previously misclassified or by focusing on
    the minimization of a specific loss function. A voting classifier, instead, allows
    the mixing of different classifiers, adopting a majority vote to decide which
    class must be considered as the winning one during a prediction.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了一些集成学习方法：随机森林、AdaBoost、梯度树提升和投票分类器。它们都基于训练多个弱学习器并使用多数投票或平均来评估其预测的想法。然而，虽然随机森林创建了一组部分随机训练的决策树，AdaBoost和梯度提升树则采用逐步添加新模型的技术，并专注于那些先前被错误分类的样本，或者专注于最小化特定的损失函数。相反，投票分类器允许混合不同的分类器，在预测期间采用多数投票来决定哪个类别必须被视为获胜者。
- en: In the next chapter, we're going to introduce the first unsupervised learning
    approach, k-means, which is one of most diffused clustering algorithms. We will
    concentrate on its strengths and weaknesses, and explore some alternatives offered
    by scikit-learn.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍第一种无监督学习方法，k-means，这是最广泛使用的聚类算法之一。我们将集中讨论其优势和劣势，并探索scikit-learn提供的某些替代方案。
