- en: Decision Trees and Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to discuss binary decision trees and ensemble methods.
    Even if they're probably not the most common methods for classification, they
    offer a good level of simplicity and can be adopted in many tasks that don't require
    a high level of complexity. They're also quite useful when it's necessary to show
    how a decision process works because they are based on a structure that can be
    shown easily in presentations and described step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods are a powerful alternative to complex algorithms because they
    try to exploit the statistical concept of majority vote. Many weak learners can
    be trained to capture different elements and make their own predictions, which
    are not globally optimal, but using a sufficient number of elements, it's statistically
    probable that a majority will evaluate correctly. In particular, we're going to
    discuss random forests of decision trees and some boosting methods that are slightly
    different algorithms that can optimize the learning process by focusing on misclassified
    samples or by continuously minimizing a target loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Binary decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A binary decision tree is a structure based on a sequential decision process.
    Starting from the root, a feature is evaluated and one of the two branches is
    selected. This procedure is repeated until a final leaf is reached, which normally
    represents the classification target we’re looking for. Considering other algorithms,
    decision trees seem to be simpler in their dynamics; however, if the dataset is
    splittable while keeping an internal balance, the overall process is intuitive
    and rather fast in its predictions. Moreover, decision trees can work efficiently
    with unnormalized datasets because their internal structure is not influenced
    by the values assumed by each feature. In the following figure, there are plots
    of an unnormalized bidimensional dataset and the cross-validation scores obtained
    using a logistic regression and a decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a649bb6-16ae-498f-8793-ec09a532f5a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision tree always achieves a score close to 1.0, while the logistic regression
    has an average slightly greater than 0.6\. However, without proper limitations,
    a decision tree could potentially grow until a single sample (or a very low number)
    is present in every node. This situation drives to overfit the model, and the
    tree becomes unable to generalize correctly. Using a consistent test set or cross-validation
    can help in avoiding this problem; however, in the section dedicated to scikit-learn
    implementation, we're going to discuss how to limit the growth of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Binary decisions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider an input dataset *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6bcf22c-1fc0-43e8-9754-45fff47b2740.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Every vector is made up of *m* features, so each of them can be a good candidate
    to create a node based on the (feature, threshold) tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c271f7b4-3755-4aa5-895b-99fe6e8e5125.png)'
  prefs: []
  type: TYPE_IMG
- en: According to the feature and the threshold, the structure of the tree will change.
    Intuitively, we should pick the feature that best separates our data in other
    words, a perfect separating feature will be present only in a node and the two
    subsequent branches won't be based on it anymore. In real problems, this is often
    impossible, so it's necessary to find the feature that minimizes the number of
    following decision steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s consider a class of students where all males have dark
    hair and all females have blonde hair, while both subsets have samples of different
    sizes. If our task is to determine the composition of the class, we can start
    with the following subdivision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/394a80d4-1e15-4d40-adeb-fd7118942aec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the block **Dark color?** will contain both males and females (which
    are the targets we want to classify). This concept is expressed using the term
    **purity** (or, more often, its opposite concept, **impurity**). An ideal scenario
    is based on nodes where the impurity is null so that all subsequent decisions
    will be taken only on the remaining features. In our example, we can simply start
    from the color block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c956d527-24cf-45c5-8370-675d3938117d.png)'
  prefs: []
  type: TYPE_IMG
- en: The two resulting sets are now pure according to the color feature, and this
    can be enough for our task. If we need further details, such as hair length, other
    nodes must be added; their impurity won't be null because we know that there are,
    for example, both male and female students with long hair.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, suppose we define the selection tuple as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf6063f4-1921-4999-9c64-1e323aa8332e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the first element is the index of the feature we want to use to split
    our dataset at a certain node (it will be the entire dataset only at the beginning;
    after each step, the number of samples decreases), while the second is the threshold
    that determines left and right branches. The choice of the best threshold is a
    fundamental element because it determines the structure of the tree and, therefore,
    its performance. The goal is to reduce the residual impurity in the least number
    of splits so as to have a very short decision path between the sample data and
    the classification result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also define a total impurity measure by considering the two branches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8f37002-3e38-4d4e-b11e-177273d11e6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *D* is the whole dataset at the selected node, *D[left]* and *D[right]*
    are the resulting subsets (by applying the selection tuple), and the *I* are impurity
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: Impurity measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To define the most used impurity measures, we need to consider the total number
    of target classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ddfc3e6-2e30-4dd5-8552-7417a5a52162.png)'
  prefs: []
  type: TYPE_IMG
- en: In a certain node *j*, we can define the probability *p(i|j)*where *i* is an
    index [1, *n*] associated with each class. In other words, according to a frequentist
    approach, this value is the ratio between the number of samples belonging to class
    *i* and the total number of samples belonging to the selected node.
  prefs: []
  type: TYPE_NORMAL
- en: Gini impurity index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Gini impurity index is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/756bb2ca-86ba-4bb4-a005-ad55c466998b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the sum is always extended to all classes. This is a very common measure
    and it's used as a default value by scikit-learn. Given a sample, the Gini impurity
    measures the probability of a misclassification if a label is randomly chosen
    using the probability distribution of the branch. The index reaches its minimum
    (0.0) when all the samples of a node are classified into a single category.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy impurity index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The cross-entropy measure is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e62e0f8-d0d9-408c-98e6-af159ab08fae.png)'
  prefs: []
  type: TYPE_IMG
- en: This measure is based on information theory, and assumes null values only when
    samples belonging to a single class are present in a split, while it is maximum
    when there's a uniform distribution among classes (which is one of the worst cases
    in decision trees because it means that there are still many decision steps until
    the final classification). This index is very similar to the Gini impurity, even
    though, more formally, the cross-entropy allows you to select the split that minimizes
    the uncertainty about the classification, while the Gini impurity minimizes the
    probability of misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml), *Important Elements
    in Machine Learning*, we defined the concept of mutual information *I(X; Y) =
    H(X) - H(X|Y)* as the amount of information shared by both variables, thereby
    reducing the uncertainty about *X* provided by the knowledge of *Y*. We can use
    this to define the information gain provided by a split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7cf49f95-bbd3-48d1-95ff-58eb3b5069b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When growing a tree, we start by selecting the split that provides the highest
    information gain and proceed until one of the following conditions is verified:'
  prefs: []
  type: TYPE_NORMAL
- en: All nodes are pure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The information gain is null
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum depth has been reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misclassification impurity index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The misclassification impurity is the simplest index, defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/656ff3b2-b969-48be-a787-c2f969543a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: In terms of quality performance, this index is not the best choice because it's
    not particularly sensitive to different probability distributions (which can easily
    drive the selection to a subdivision using Gini or cross-entropy indexes).
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When growing a decision tree with a multidimensional dataset, it can be useful
    to evaluate the importance of each feature in predicting the output values. In
    [Chapter 3](7323017f-66c2-4b18-af1e-5d4dc28f9031.xhtml), *Feature Selection and
    Feature Engineering*, we discussed some methods to reduce the dimensionality of
    a dataset by selecting only the most significant features. Decision trees offer
    a different approach based on the impurity reduction determined by every single
    feature. In particular, considering a feature *x[i]*, its importance can be determined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e20b55ba-7249-45f6-abce-66899ac81638.png)'
  prefs: []
  type: TYPE_IMG
- en: The sum is extended to all nodes where *x[i]* is used, and *N[k]* is the number
    of samples reaching the node *k*. Therefore, the importance is a weighted sum
    of all impurity reductions computed considering only the nodes where the feature
    is used to split them. If the Gini impurity index is adopted, this measure is
    also called **Gini importance**.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree classification with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'scikit-learn contains the `DecisionTreeClassifier` class, which can train a
    binary decision tree with Gini and cross-entropy impurity measures. In our example,
    let''s consider a dataset with three features and three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s first consider a classification with default Gini impurity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A very interesting feature is given by the possibility of exporting the tree
    in `Graphviz` format and converting it into a PDF.
  prefs: []
  type: TYPE_NORMAL
- en: Graphviz is a free tool that can be downloaded from [http://www.graphviz.org](http://www.graphviz.org).
  prefs: []
  type: TYPE_NORMAL
- en: 'To export a trained tree, it is necessary to use the built-in function `export_graphviz()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we have used `A`, `B`, and `C` as feature names and `C1`, `C2`,
    and `C3` as class names. Once the file has been created, it''s possible converting
    to PDF using the command-line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph for our example is rather large, so in the following feature you
    can see only a part of a branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a660ff28-558c-4cb6-99bf-fe44060afc50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, there are two kinds of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: Nonterminal, which contains the splitting tuple (as feature <= threshold) and
    a positive impurity measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Terminal, where the impurity measure is null and a final target class is present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In both cases, you can always check the number of samples. This kind of graph
    is very useful in understanding how many decision steps are needed. Unfortunately,
    even if the process is quite simple, the dataset structure can lead to very complex
    trees, while other methods can immediately find out the most appropriate class.
    Of course, not all features have the same importance. If we consider the root
    of the tree and the first nodes, we find features that separate a lot of samples;
    therefore, their importance must be higher than that of all terminal nodes, where
    the residual number of samples is minimum. In scikit-learn, it''s possible to
    assess the Gini importance of each feature after training a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows a plot of the importances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4af9918-ddb0-45ff-86df-e7713741a7fe.png)'
  prefs: []
  type: TYPE_IMG
- en: The most important features are 6, 3, 4, and 7, while feature 2, for example,
    separates a very small number of samples, and can be considered noninformative
    for the classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of efficiency, a tree can also be pruned using the `max_depth` parameter;
    however, it''s not always so simple to understand which value is the best (grid
    search can help in this task). On the other hand, it''s easier to decide what
    the maximum number of features to consider at each split should be. The parameter
    `max_features` can be used for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: If it's a number, the value is directly taken into account at each split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it's `'auto'` or `'sqrt'`, the square root of the number of features will
    be adopted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it's `'log2'`, the logarithm (base 2) will be used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it's `'None'`, all the features will be used (this is the default value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, when the number of total features is not too high, the default
    value is the best choice, although it''s useful to introduce a small compression
    (via `sqrt` or `log2`) when too many features can interfere among themselves,
    reducing the efficiency. Another parameter useful for controlling both performance
    and efficiency is `min_samples_split`, which specifies the minimum number of samples
    to consider for a split. Some examples are shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As already explained, finding the best parameters is generally a difficult task,
    and the best way to carry it out is to perform a grid search while including all
    the values that could affect the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using logistic regression on the previous set (only for comparison), we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'So the score is higher, as expected. However, the original dataset was quite
    simple, and based on the concept of having a single cluster per class. This allows
    a simpler and more precise linear separation. If we consider a slightly different
    scenario with more variables and a more complex structure (which is hard to capture
    by a linear classifier), we can compare an ROC curve for both linear regression
    and decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting ROC curve is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf28527f-cb09-4f43-9f80-f3d3cec8c2e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a grid search with the most common parameters on the MNIST digits dataset,
    we can get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the element that impacted accuracy the most is the minimum number
    of samples to consider for a split. This is reasonable, considering the structure
    of this dataset and the need to have many branches to capture even small changes.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, we have trained models on single instances, iterating an algorithm
    in order to minimize a target loss function. This approach is based on so-called
    strong learners, or methods that are optimized to solve a specific problem by
    looking for the best possible solution. Another approach is based on a set of
    weak learners that can be trained in parallel or sequentially (with slight modifications
    on the parameters) and used as an ensemble based on a majority vote or the averaging
    of results. These methods can be classified into two main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagged (or Bootstrap) trees**: In this case, the ensemble is built completely.
    The training process is based on a random selection of the splits and the predictions
    are based on a majority vote. Random forests are an example of bagged tree ensembles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosted trees**: The ensemble is built sequentially, focusing on the samples
    that have been previously misclassified. Examples of boosted trees are AdaBoost
    and gradient tree boosting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A random forest is a set of decision trees built on random samples with a different
    policy for splitting a node: Instead of looking for the best choice, in such a
    model, a random subset of features (for each tree) is used, trying to find the
    threshold that best separates the data. As a result, there will be many trees
    trained in a weaker way and each of them will produce a different prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to interpret these results; the more common approach is based
    on a majority vote (the most voted class will be considered correct). However,
    scikit-learn implements an algorithm based on averaging the results, which yields
    very accurate predictions. Even if they are theoretically different, the probabilistic
    average of a trained random forest cannot be very different from the majority
    of predictions (otherwise, there should be different stable points); therefore
    the two methods often drive to comparable results.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let''s consider the MNIST dataset with random forests made of
    a different number of trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f68b4c9a-a179-49c2-a94d-4eac0c13aee3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the accuracy is low when the number of trees is under a minimum
    threshold; however, it starts increasing rapidly with fewer than 10 trees. A value
    between 20 and 30 trees yields the optimal result (95%), which is higher than
    for a single decision tree. When the number of trees is low, the variance of the
    model is very high and the averaging process produces many incorrect results;
    however, increasing the number of trees reduces the variance and allows the model
    to converge to a very stable solution. scikit-learn also offers a variance that
    enhances the randomness in selecting the best threshold. Using the `ExtraTreesClassifier`
    class, it''s possible to implement a model that randomly computes thresholds and
    picks the best one. As discussed in the official documentation, this allows us
    to further reduce the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The results (with the same number of trees) in terms of accuracy are slightly
    better, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11da180a-5d20-4db6-be64-ecf3af8a4271.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature importance in random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of feature importance that we previously introduced can also be
    applied to random forests, computing the average over all trees in the forest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1018e9d6-c672-4a27-9924-dd0ba2ba4108.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can easily test the importance evaluation with a dummy dataset containing
    50 features with 20 noninformative elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The importance of the first 50 features according to a random forest with 20
    trees is plotted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29c72b5b-4378-47c0-862c-29260e8218a4.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, there are a few *very* important features, a block of features
    with a medium importance, and a tail containing features that have quite a low
    influence on the predictions. This type of plot is also useful during the analysis
    stage to better understand how the decision process is structured. With multidimensional
    datasets, it's rather difficult to understand the influence of every factor, and
    sometimes many important business decisions are made without a complete awareness
    of their potential impact. Using decision trees or random forests, it's possible
    to assess the "real" importance of all features and exclude all the elements under
    a fixed threshold. In this way, a complex decision process can be simplified and,
    at the same time, be partially denoised.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another technique is called **AdaBoost** (short for **Adaptive Boosting**) and
    works in a slightly different way than many other classifiers. The basic structure
    behind this can be a decision tree, but the dataset used for training is continuously
    adapted to force the model to focus on those samples that are misclassified. Moreover,
    the classifiers are added sequentially, so a new one boosts the previous one by
    improving the performance in those areas where it was not as accurate as expected.
  prefs: []
  type: TYPE_NORMAL
- en: At each iteration, a weight factor is applied to each sample so as to increase
    the importance of the samples that are wrongly predicted and decrease the importance
    of others. In other words, the model is repeatedly boosted, starting as a very
    weak learner until the maximum `n_estimators` number is reached. The predictions,
    in this case, are always obtained by majority vote.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the scikit-learn implementation, there''s also a parameter called `learning_rate`
    that weighs the effect of each classifier. The default value is 1.0, so all estimators
    are considered to have the same importance. However, as we can see with the MNIST
    dataset, it''s useful to decrease this value so that each contribution is weakened:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8b2c4ac-f53a-47a2-a06d-734e1138572f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The accuracy is not so high as in the previous examples; however, it''s possible
    to see that when the boosting adds about 20-30 trees, it reaches a stable value.
    A grid search on `learning_rate` could allow you to find the optimal value; however,
    the sequential approach in this case is not preferable. A classic random forest,
    which works with a fixed number of trees since the first iteration, performs better.
    This may well be due to the strategy adopted by AdaBoost; in this set, increasing
    the weight of the correctly classified samples and decreasing the strength of misclassifications
    can produce an oscillation in the loss function, with a final result that is not
    the optimal minimum point. Repeating the experiment with the Iris dataset (which
    is structurally much simpler) yields better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, a learning rate of 1.0 is the best choice, and it''s easy to
    understand that the boosting process can be stopped after a few iterations. In
    the following figure, you can see a plot showing the accuracy for this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d10abc62-9d30-48a7-b8bd-4f0ebe340f9b.png)'
  prefs: []
  type: TYPE_IMG
- en: After about 10 iterations, the accuracy becomes stable (the residual oscillation
    can be discarded), reaching a value that is compatible with this dataset. The
    advantage of using AdaBoost can be appreciated in terms of resources; it doesn't
    work with a fully configured set of classifiers and the whole set of samples.
    Therefore, it can help save time when training on large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient tree boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Gradient tree boosting is a technique that allows you to build a tree ensemble
    step by step with the goal of minimizing a target loss function. The generic output
    of the ensemble can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a294a9fd-b0b4-43c7-a5e5-3fa85a71f887.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *f[i](x)* is a function representing a weak learner. The algorithm is
    based on the concept of adding a new decision tree at each step so as to minimize
    the global loss function using the steepest gradient descent method (see [https://en.wikipedia.org/wiki/Method_of_steepest_descent](https://en.wikipedia.org/wiki/Method_of_steepest_descent),
    for further information):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b2d39b4-17ea-47a0-ab11-0799233a1087.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After introducing the gradient, the previous expression becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c2818e8-e0f9-4770-805e-2a12349731ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'scikit-learn implements the `GradientBoostingClassifier` class, supporting
    two classification loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: Binomial/multinomial negative log-likelihood (which is the default choice)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential (such as AdaBoost)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s evaluate the accuracy of this method using a more complex dummy dataset
    made up of 500 samples with four features (three informative and one redundant)
    and three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can collect the cross-validation average accuracy for a number of estimators
    in the range (1, 50). The loss function is the default one (multinomial negative
    log-likelihood):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'While increasing the number of estimators (parameter `n_estimators`), it''s
    important to decrease the learning rate (parameter `learning_rate`). The optimal
    value cannot be easily predicted; therefore, it''s often useful to perform a grid
    search. In our example, I''ve set a very high learning rate at the beginning (5.0),
    which converges to 0.05 when the number of estimators is equal to 100\. This is
    not a perfect choice (unacceptable in most real cases!), and it has been made
    only to show the different accuracy performances. The results are shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb8c7747-9a50-4406-9e86-646860d593f4.png)'
  prefs: []
  type: TYPE_IMG
- en: As it's possible to see, the optimal number of estimators is about 50, with
    a learning rate of 0.1\. The reader can try different combinations and compare
    the performances of this algorithm with the other ensemble methods.
  prefs: []
  type: TYPE_NORMAL
- en: Voting classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A very interesting ensemble solution is offered by the class `VotingClassifier`,
    which isn''t an actual classifier but a wrapper for a set of different ones that
    are trained and evaluated in parallel. The final decision for a prediction is
    taken by majority vote according to two different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hard voting**: In this case, the class that received the major number of
    votes, *N[c](y[t])*, will be chosen:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/50ee8383-9a24-45df-bcf6-452d5f607f28.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Soft voting**: In this case, the probability vectors for each predicted class
    (for all classifiers) are summed up and averaged. The winning class is the one
    corresponding to the highest value:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5a9b01d2-6cf5-4792-97a0-8ca60ad399c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s consider a dummy dataset and compute the accuracy with a hard voting
    strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For our examples, we are going to consider three classifiers: logistic regression,
    decision tree (with default Gini impurity), and an SVM (with a polynomial kernel
    and `probability=True` in order to generate the probability vectors). This choice
    has been made only for didactic purposes and may not be the best one. When creating
    an ensemble, it''s useful to consider the different features of each involved
    classifier and avoid "duplicate" algorithms (for example, a logistic regression
    and a linear SVM or a perceptron are likely to yield very similar performances).
    In many cases, it can be useful to mix nonlinear classifiers with random forests
    or AdaBoost classifiers. The reader can repeat this experiment with other combinations,
    comparing the performance of each single estimator and the accuracy of the voting
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Computing the cross-validation accuracies, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracies of each single classifier and of the ensemble are plotted in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cd9bfad-be11-4f1c-b9c0-64bb0a5ec04a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the ensemble takes advantage of the different algorithms and yields
    better performance than any single one. We can now repeat the experiment with
    soft voting, considering that it''s also possible to introduce a weight vector
    (through the parameter `weights`) to give more or less importance to each classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bef91f33-3792-4e56-bfbc-974b1f8ef89a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, considering the previous figure, we can decide to give more importance
    to the logistic regression and less to the decision tree and SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeating the same calculations for the cross-validation accuracies, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9d252d4-2b89-4a6d-b0de-599c98eaadfa.png)'
  prefs: []
  type: TYPE_IMG
- en: Weighting is not limited to the soft strategy. It can also be applied to hard
    voting, but in that case, it will be used to filter (reduce or increase) the number
    of actual occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/412a2a2a-34f5-43ed-8605-f6537b94992d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N[c](y[t],w)* is the number of votes for each target class, each of them
    multiplied by the corresponding classifier weighting factor.
  prefs: []
  type: TYPE_NORMAL
- en: A voting classifier can be a good choice whenever a single strategy is not able
    to reach the desired accuracy threshold; while exploiting the different approaches,
    it's possible to capture many microtrends using only a small set of strong (but
    sometimes limited) learners.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Louppe G., Wehenkel L., Sutera A., and Geurts P., *Understanding variable importances
    in forests of randomized trees*, NIPS Proceedings 2013.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced decision trees as a particular kind of classifier.
    The basic idea behind their concept is that a decision process can become sequential
    by using splitting nodes, where, according to the sample, a branch is chosen until
    we reach a final leaf. In order to build such a tree, the concept of impurity
    was introduced; starting from a complete dataset, our goal is to find a split
    point that creates two distinct sets that should share the minimum number of features
    and, at the end of the process, should be associated with a single target class.
    The complexity of a tree depends on the intrinsic purity—in other words, when
    it's always easy to determine a feature that best separates a set, the depth will
    be lower. However, in many cases, this is almost impossible, so the resulting
    tree needs many intermediate nodes to reduce the impurity until it reaches the
    final leaves.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also discussed some ensemble learning approaches: random forests, AdaBoost,
    gradient tree boosting and voting classifiers. They are all based on the idea
    of training several weak learners and evaluating their predictions using a majority
    vote or an average. However, while a random forest creates a set of decision trees
    that are partially randomly trained, AdaBoost and gradient boost trees adopt the
    technique of boosting a model by adding a new one, step after step, and focusing
    only on those samples that have been previously misclassified or by focusing on
    the minimization of a specific loss function. A voting classifier, instead, allows
    the mixing of different classifiers, adopting a majority vote to decide which
    class must be considered as the winning one during a prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to introduce the first unsupervised learning
    approach, k-means, which is one of most diffused clustering algorithms. We will
    concentrate on its strengths and weaknesses, and explore some alternatives offered
    by scikit-learn.
  prefs: []
  type: TYPE_NORMAL
