["```py\nimport numpy as np\nimport random\nimport cv2\nfrom imutils import paths\nimport os\n# SkLearn Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\n# TensorFlow Libraries\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import backend as K\n```", "```py\ndef load_mnist_data(dataroot):\n    X = list()\n    y = list()\n    for label in os.listdir(dataroot):\n      label_dir_path = dataroot + \"/\"+label\n      for imgFile in os.listdir(label_dir_path):\n        img_file_path = label_dir_path + \"/\" + imgFile\n        image_gray = cv2.imread(img_file_path, cv2.IMREAD_GRAYSCALE)\n        image = np.array(image_gray).flatten()\n        X.append(image/255)\n        y.append(label)\n    return X, y\n```", "```py\ndef create_client_nodes(X,y,num_clients=10,\nprefix='CLIENT_'):\n    #create a list of client names\n    client_names = []\n    for i in range(num_clients):\n      client_names.append(prefix + str(i))\n    #randomize the data\n    data = list(zip(X, y))\n    random.shuffle(data)\n    #shard data and place at each client\n    per_client = len(data)//num_clients\n    client_chunks = []\n    start = 0\n    end = 0\n    for i in range(num_clients):\n      end = start + per_client\n      if end > len(data):\n        client_chunks.append(data[start:])\n      else:\n        client_chunks.append(data[start:end])\n        start = end\n    return {client_names[i] : client_chunks[i] for i in range(len(client_names))}\n```", "```py\ndef collapse_chunk(chunk, batch_size=32):\n    X, y = zip(*chunk)\n    dataset = tf.data.Dataset.from_tensor_slices((list(X), list(y)))\n    return dataset.shuffle(len(y)).batch(batch_size)\n```", "```py\ndef MNIST_DeepLearning_Model(hidden_layer_sizes = [200, 200, 200]):\n  input_dim = 784\n  num_classes = 10\n  model = Sequential()\n  model.add(Dense(200, input_shape=(input_dim,)))\n  model.add(Activation(\"relu\"))\n  for hidden in hidden_layer_sizes:\n    model.add(Dense(hidden))\n    model.add(Activation(\"relu\"))\n  model.add(Dense(num_classes))\n  model.add(Activation(\"softmax\"))\n  return model\n```", "```py\ndef scale_weights(all_clients,this_client,weights):\n  # First calculate scaling factor\n  # Obtain batch size\n  batch_size = list(all_clients[this_client])[0][0].shape[0]\n# Compute global data size\n  sizes = []\n  for client in all_clients.keys():\n    sizes.append(tf.data.experimental.cardinality(all_clients[client]).numpy())\n  global_data_size = np.sum(sizes)*batch_size\n  # Compute data size in this client\n  this_client_size = tf.data.experimental.cardinality(all_clients[this_client]).numpy()*batch_size\n  # Scaling factor is the ratio of the two\n  scaling_factor = this_client_size / global_data_size\n  scaled_weights = []\n  for weight in weights:\n    scaled_weights.append(scaling_factor * weight)\n  return scaled_weights\n```", "```py\nglobal_model = MNIST_DeepLearning_Model(hidden_layer_sizes = [200, 200, 200])\nglobal_model.summary()\n```", "```py\ndataroot = './trainingSet'\nX, y = load_mnist_data(dataroot)\ny_binarized = LabelBinarizer().fit_transform(y)\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                      y_binarized,\n                                      test_size=0.2,\n                                      random_state=123)\n```", "```py\nclients = create_client_nodes(X_train, y_train, num_clients=10)\nclients_data = {}\nfor client_name in clients.keys():\n    clients_data[client_name] = collapse_chunk(clients[client_name])\n#process and batch the test set\ntest_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n```", "```py\nlearn_rate = 0.01\nnum_rounds = 40\nloss='categorical_crossentropy'\nmetrics = ['accuracy']\n```", "```py\n for round in range(num_rounds):\n    # Get the weights of the global model\n    global_weights = global_model.get_weights()\n    scaled_local_weights = []\n    # Shuffle the clients\n    # This will remove any inherent bias\n    client_names= list(clients_data.keys())\n    random.shuffle(client_names)\n    # Create initial local models\n    for client in client_names:\n        # Create the model\n        local_client_model = MNIST_DeepLearning_Model(hidden_layer_sizes = [200])\n        # Compile the model\n        local_client_model.compile(loss=loss,\n        optimizer=\n             tf.keras.optimizers.Adam(learning_rate=0.01),\n        metrics=metrics)\n        # The model will have random weights\n        # We need to reset it to the weights of the current global model\n        local_client_model.set_weights(global_weights)\n        # Train local model\n        local_client_model.fit(clients_data[client],\nepochs=1,verbose = 0)\n        # Scale model weights\n        # Based on this client model's local weights\n        scaled_weights = scale_weights(clients_data, client,local_client_model.get_weights())\n        # Record the value\n        scaled_local_weights.append(scaled_weights)\n        # Memory management\n        K.clear_session()\n    # Communication round has ended\n    # Need to compute the average gradients from all local models\n    average_weights = []\n    for gradients in zip(*scaled_local_weights):\n        # Calculate mean per-layer weights\n        layer_mean = tf.math.reduce_sum(gradients, axis=0)\n        # This becomes new weight for that layer\n        average_weights.append(layer_mean)\n    # Update global model with newly computed gradients\n    global_model.set_weights(average_weights)\n    # Evaluate performance of model at end of round\n    losses = []\n    accuracies = []\n    for(X_test, Y_test) in test_batched:\n        # Use model for inference\n        Y_pred = global_model.predict(X_test)\n        # Calculate loss based on actual and predicted value\n        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n        loss_value = loss_fn(Y_test, Y_pred)\n        losses.append(loss_value)\n        # Calculate accuracy based on actual and predicted value\n        accuracy_value = accuracy_score(tf.argmax(Y_pred, axis=1),tf.argmax(Y_test, axis=1))\n        accuracies.append(accuracy_value)\n    # Print Information\n    print(\"ROUND: {} ---------- GLOBAL ACCURACY: {:.2%}\".format(round, accuracy_value))\n```", "```py\nimport matplotlib.pyplot as plt\nplt.plot(range(num_rounds), losses)\nplt.xlabel(\"Communication Rounds\")\nplt.ylabel(\"Loss\")\n```", "```py\nimport matplotlib.pyplot as plt\nplt.plot(range(num_rounds), accuracies)\nplt.xlabel(\"Communication Rounds\")\nplt.ylabel(\"Accuracy\")\n```", "```py\n# Initialize global model\nglobal_model = MNIST_DeepLearning_Model(hidden_layer_sizes = [200, 200, 200])\nglobal_model.compile(loss=loss,optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),metrics=metrics)\n# Create dataset from entire data\nfull_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\.shuffle(len(y_train))\\.batch(32)\n# Fit the model\nglobal_model.fit(full_dataset, epochs = 10)\n```", "```py\n# Initialize local client\nlocal_client_model = MNIST_DeepLearning_Model(hidden_layer_sizes = [200, 200, 200])\nlocal_client_model.compile(loss=loss,optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),metrics=metrics)\n# Train on only one client data\nlocal_client_model.fit(clients_data['CLIENT_8'],epochs=10)\n```"]