- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature Engineering for Natural Language Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored how to extract features from numerical
    data and images. We explored a few algorithms that are used for that purpose.
    In this chapter, we’ll continue with the algorithms that extract features from
    natural language data.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language is a special kind of data source in software engineering. With
    the introduction of GitHub Copilot and ChatGPT, it became evident that machine
    learning and artificial intelligence tools for software engineering tasks are
    no longer science fiction. Therefore, in this chapter, we’ll explore the first
    steps that made these technologies so powerful – feature extraction from natural
    language data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizers and their role in feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bag-of-words as a simple technique for processing natural language data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embeddings as more advanced methods that can capture contexts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Natural language data in software engineering and the rise of GitHub Copilot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Programming has always been a mixture of science, engineering, and creativity.
    Creating new programs and being able to instruct computers to do something has
    always been something that was considered worth paying for – that’s how all programmers
    make their living. There have been attempts to automate programming and to support
    smaller tasks – for example, provide programmers with suggestions on how to use
    a specific function or library method.
  prefs: []
  type: TYPE_NORMAL
- en: Good programmers, however, can make programs that last and that are readable
    for others. They can also make reliable programs that work without maintenance
    for a long period. The best programmers are the ones who can solve very difficult
    tasks and follow the principles and best practices of software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: In 2020, something happened – GitHub Copilot entered the stage and showed that
    automated tools, based on **large language models** (**LLMs**), can provide much
    more than just suggestions for simple function calls. It has demonstrated that
    these language models are capable of providing suggestions for entire solutions
    and algorithms and even solving programming competitions. This opened up completely
    new possibilities for programmers – the best ones became extremely productive,
    and have been provided with the tools that allow them to focus on the complex
    parts of their programming tasks. The simple ones are now solved by GitHub Copilot
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why these tools are so good is because they are based on LLMs, which
    are capable of finding and quantifying contexts of programs. Just like a great
    chess player can foresee several moves in advance, these tools can foresee what
    the programmers may need in advance and provide useful suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few simple tricks that make these tools so effective, and one of
    them is feature engineering. Feature engineering for natural language tasks, including
    programming, is a process where a piece of text is transformed into a vector (or
    matrix) of numbers. These vectors can be simple – for example, quantifying the
    tokens – and also very complex – for example, finding an atomic piece of text
    linked to other tasks. We’ll explore these techniques in this chapter. We’ll start
    with a bit of a repetition of the bag-of-words technique (seen in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038)
    and [*Chapter 5*](B19548_05.xhtml#_idTextAnchor060)). We do not need to repeat
    the entire code, but we do need to provide a small re-cap to understand the limitations
    of these approaches. However, here is my best practice for choosing whether I
    need a tokenizer or embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #44'
  prefs: []
  type: TYPE_NORMAL
- en: Use tokenizers for LLMs such as BERT and word embeddings for simple tasks.
  prefs: []
  type: TYPE_NORMAL
- en: For simple tasks, such as basic tokenization of the text for sentiment analysis
    or quickly understanding the dependencies in the text, I often use word embeddings.
    However, I usually use different tokenizers for working with LLMs such as BERT,
    RoBERTa, or AlBERT since these models are very good at finding dependencies on
    their own. However, for designing classifiers, I use word embeddings since they
    provide a fast way of creating feature vectors that are compatible with the “classical”
    machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a tokenizer needs to be done based on the task. We’ll look at this
    closer in this chapter, but the topic itself could occupy an entire book. For
    example, for tasks that require information about the part of speech (or, in many
    cases, part of the abstract syntax tree of a program), we need to use a tokenizer
    that is designed to capture that information – for example, from a programming
    language parser. These tokenizers provide more information to the model, but they
    impose more requirements on the data – an abstract syntax tree-based tokenizer
    requires the program to be well formed.
  prefs: []
  type: TYPE_NORMAL
- en: What a tokenizer is and what it does
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in feature engineering text data is to decide on the tokenization
    of the text. The tokenization of text is a process of extracting parts of words
    that capture the meaning of the text without too many extra details.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to extract tokens, which we’ll explore in this chapter,
    but to illustrate the problem of extracting tokens, let’s look at one word that
    can take different forms – *print*. The word by itself can be a token, but it
    can be in different forms, such as *printing*, *printed*, *printer*, *prints*,
    *imprinted*, and many others. If we use a simple tokenizer, each of these words
    will be one token – which means quite a few tokens. However, all these tokens
    capture some sort of meaning related to printing, so maybe we do not need so many
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: This is where tokenizers come in. Here, we can decide how to treat these different
    forms of the word. We could take the main part only – *print* – and then all the
    other forms would be counted as that, so both *imprinted* and *printing* would
    be counted as *print*. It decreases the number of tokens, but we reduce the expressiveness
    of our feature vector – some information is lost as we do not have the same number
    of tokens to use. We could pre-design the set of tokens – that is, use both *print*
    and *imprint* to distinguish between different contexts. We could also use bigrams
    (two words together) as tokens (for example, *is_going* versus is, *going* – the
    first one requires both words to be in the specific sequence, where the other
    one allows them to be in two different sequences), or we could add the information
    about whether the word is an object of the subject in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-words and simple tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapters 3* and *5*, we saw the use of the bag-of-words feature extraction
    technique. This technique takes the text and counts the number of tokens, which
    were words in *Chapters 3* and *5*. It is simple and computationally efficient,
    but it has a few problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'When instantiating the bag-of-words tokenizer, we can use several parameters
    that strongly impact the results, as we did in the following fragment of code
    in the previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `max_features` parameter is a cut-off value that reduces the number of features,
    but it also can introduce noise where two (or more) distinct sentences have the
    same feature vector (we saw an example of such a sentence in [*Chapter 2*](B19548_02.xhtml#_idTextAnchor023)).
    Since we discussed noise and the problems related to it, we could be tempted to
    use other parameters – `max_df` and `min_df`. These two parameters determine how
    often a word should appear in the document to be considered a token. The tokens
    that are too rare can (`min_df`) result in a sparse matrix – a lot of 0s in the
    feature matrix – but they can be a very good discriminant between data points.
    Maybe these rare words are just what we are looking for. The other parameter (`max_df`)
    results in more dense feature matrices, but they may not discriminate the data
    points completely. This means that it is not so simple to select these parameters
    – we need experiments and we need to use machine learning model training (and
    validation) to find the right vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also another way – we can perform a recursive search for such a feature
    vector that would discriminate all data points without adding too much noise.
    My team has experimented with such algorithms, which yield very good performance
    for model training and validation but are computationally very expensive. Such
    an algorithm is presented in *Figure 8**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – An algorithm for finding a set of features that discriminate
    all data points in a text file. The flow has been simplified to illustrate the
    main points](img/B19548_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – An algorithm for finding a set of features that discriminate all
    data points in a text file. The flow has been simplified to illustrate the main
    points
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm works by adding new tokens if a data point has the same feature
    vector as any of the previous ones. It starts by taking the first token from the
    first line, then the second line. If the token can discriminate between these
    two lines, then it proceeds to the third line. Once the algorithm discovers that
    two different lines have the same feature vector, it finds out whether there is
    a token that can discriminate between these lines and adds it to the set of features.
    It continues until there are no new tokens to add or all lines have been analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm guarantees that the set of tokens that best discriminates the
    analyzed dataset is found. However, it has one large disadvantage – it is slow
    (as it must start from the first line once a new token is found/needed). The resulting
    feature matrix is also not optimal – it contains a lot of 0s since most of the
    tokens can only be found in one line. The feature matrix, in turn, can be much
    larger than the actual raw dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This is where my next best practice comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #45'
  prefs: []
  type: TYPE_NORMAL
- en: Use bag-of-words tokenizers together with dictionaries when your task requires
    a pre-defined set of words.
  prefs: []
  type: TYPE_NORMAL
- en: I use bag-of-words tokenizers quite often when analyzing programming language
    code. I use a pre-defined set of keywords from the programming language to boost
    the tokenizer and then the standard `CountVectorizer`. This allows me to control
    part of the vocabulary that I am interested in – keywords – and allows the tokenizer
    to adjust to the text.
  prefs: []
  type: TYPE_NORMAL
- en: WordPiece tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A better way to tokenize and extract features from text documents is to use
    a WordPiece tokenizer. This tokenizer works in such a way that it finds the most
    common pieces of text that it can discriminate, and also the ones that are the
    most common. This kind of tokenizer needs to be trained – that is, we need to
    provide a set of representative texts to get the right vocabulary (tokens).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example where we use a simple program, a module from an open
    source project, to train such a tokenizer and then apply this tokenizer to the
    famous “Hello World” program in C. Let’s start by creating the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re using the WordPiece tokenizer from the Hugging Face library,
    specifically the one that is prepared to work with LLMs such as BERT. There are
    several parameters that we can use, but let’s settle with the parameters that
    show that we are only interested in lowercase characters; we do not want to handle
    Chinese characters and want to start from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to find a piece of text that we can train the tokenizer on. In
    this example, I’ll use one of the files from an open source project – AzureOS
    NetX. It’s a component written in C that handles parts of the internet HTTP protocol.
    We create a new variable – `path` – and add the path to that file there. Once
    we’ve prepared the text, we can train the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We’ve set the tokenizer to a similar set of parameters, similar to `CountVectorizer`
    in the previous examples. This preceding code fragment finds the set of the most
    common pieces of words and uses them as tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the list of tokens through the `tokenizer.get_vocab()` statement,
    which results in a long dictionary of tokens. Here are the first few ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The first token is a piece of a word, which is denoted by the fact that it has
    two hashtags at the beginning of it. This token is mapped to the number `183`
    in the vocabulary. This mapping is important as the numbers are used later on
    by the machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting observation is that some of the tokens, such as `'disable'`,
    are not pieces of words but entire words. This means that this token does not
    appear as a piece of the word anywhere and it does not contain any other pieces
    of other words in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve trained the WordPiece tokenizer, we can check how the tokenizer
    extracts features from a simple C program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding piece of code tokenizes the program. The result is the following
    list of tokens (only the first 10 tokens of 50 are shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line, which starts with the `int` token, has been tokenized in the
    following way. The first word – `int` – is split into two tokens: `"in"` and `"##t"`.
    This is because these two parts were used in the training program. We can also
    see that the second token – `main` – is split into two tokens: `"ma"` and `"##in"`.
    The IDs for these tokens are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This means that this list of numbers is the feature vector for our simple C
    program.
  prefs: []
  type: TYPE_NORMAL
- en: WordPiece tokenization is very effective, but it depends a lot on the training
    data. If we use training data that is very different from the tokenized text,
    the set of tokens will not be very helpful. Therefore, my next best practice is
    about training this tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #46'
  prefs: []
  type: TYPE_NORMAL
- en: Use the WordPiece tokenizer as your first choice.
  prefs: []
  type: TYPE_NORMAL
- en: I usually use this tokenizer as my first choice. It is relatively flexible but
    quite fast. It allows us to capture a vocabulary that does the job most of the
    time and does not require a lot of setup. For simple tasks with straightforward
    language and a well-defined vocabulary, traditional word-level tokenization or
    other subword tokenization methods such as **byte-pair encoding** (**BPE**) may
    suffice. WordPiece tokenization can increase the size of the input data due to
    the introduction of subword tokens. This can impact memory and computational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: BPE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A more advanced method for tokenizing text is the BPE algorithm. This algorithm
    is based on the same premises as the compression algorithm that was created in
    the 1990s by Gage. The algorithm compresses a series of bytes by the bytes not
    used in the compressed data. The BPE tokenizer does a similar thing, except that
    it replaces a series of tokens with new bytes that are not used in the text. In
    this way, the algorithm can create a much larger vocabulary than `CountVectorizer`
    and the WordPiece tokenizer. BPE is very popular both for its ability to handle
    large vocabulary and for its efficient implementation through the fastBPE library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore how to apply this tokenizer to the same data and check the difference
    between the previous two. The following code fragment shows how to instantiate
    this tokenizer from the Hugging Face library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This tokenizer requires training as it needs to find the optimal set of pairs
    of tokens. Therefore, we need to instantiate a trainer class and train it. The
    following piece of code does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The important part of this training process is the use of a special pre-tokenizer.
    The pre-tokenizer is how we initially split words into tokens. In our case, we
    use the standard whitespaces, but we could use something more advanced. For example,
    we could use semicolons and therefore use entire lines of code as tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the preceding fragment of code, our tokenizer is trained and
    ready to use. We can check the tokens by writing `tokenizer.get_vocab()`. The
    set of tokens is as follows (the first 10 tokens):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This set of tokens is very different from the set of tokens in previous cases.
    It contains a mix of words such as “will” and subwords such as “ol.” This is because
    the BPE tokenizer found some replicated tokens and replaced them with dedicated
    bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #47'
  prefs: []
  type: TYPE_NORMAL
- en: Use BPE when working with LLMs and large corpora of text.
  prefs: []
  type: TYPE_NORMAL
- en: I use BPE as my go-to when I analyze large pieces of text, such as large code
    bases. It is blazingly fast for this task and can capture complex dependencies.
    It is also heavily used in models such as BERT or GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in our case, the program’s source code that we used to train the BPE tokenizer
    was small, so a lot of words did not repeat themselves and the optimization does
    not make much sense. Therefore, a WordPiece tokenizer would do an equally (if
    not better) job. However, for larger text corpora, this tokenizer is much more
    effective and efficient than WordPiece or bag-of-words. It is also the basis for
    the next tokenizer – SentencePiece.
  prefs: []
  type: TYPE_NORMAL
- en: The SentencePiece tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SentencePiece is a more general option than BPE for one more reason: it allows
    us to treat whitespaces as regular tokens. This allows us to find more complex
    dependencies and therefore train models that understand more than just pieces
    of words. Hence the name – SentencePiece. This tokenizer was originally introduced
    to enable the tokenization of languages such as Japanese, which do not use whitespaces
    in the same way as, for example, English. The tokenizer can be installed by running
    the `pip install -q` `sentencepiece` command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code example, we’re instantiating and training the SentencePiece
    tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We’ve trained it on the same file as the previous tokenizers. The text was a
    programming file, so we could expect the tokenizer to give us a better understanding
    of what’s in a programming language than what’s in a normal piece of text. Something
    worth noting is the size of the vocabulary, which is 200, unlike 30,000 in the
    previous examples. This is important because this tokenizer tries to find as many
    tokens as this parameter. Since our input program is very short – one file with
    a few functions in it – the tokenizer cannot create more than about 300 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following fragment encodes the “Hello World” program using this tokenizer
    and prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first 10 tokens are represented in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The new element in this tokenizer is the underscore character (`_`). It denotes
    whitespace in the text. This is unique and it allows us to use this tokenizer
    more effectively in programming language comprehension because it allows us to
    capture such programming constructs as nesting – that is, using tabs instead of
    spaces or writing several statements in the same line. This is all because this
    tokenizer treats whitespaces as something important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #48'
  prefs: []
  type: TYPE_NORMAL
- en: Use the SentencePiece tokenizer when no clear word boundaries are present.
  prefs: []
  type: TYPE_NORMAL
- en: I use SentencePiece when analyzing programming language code with a focus on
    programming styles – for example, when we focus on things such as camel-case variable
    naming. For this task, it is important to understand how programmers use spaces,
    formatting, and other compiler-transparent elements. Therefore, this tokenizer
    is perfect for such tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenizers are one way of extracting features from text. They are powerful and
    can be trained to create complex tokens and capture statistical dependencies of
    words. However, they are limited by the fact that they are completely unsupervised
    and do not capture any meaning or relationship between words. This means that
    the tokenizers are great at providing input to neural network models, such as
    BERT, but sometimes, we would like to have features that are more aligned with
    a certain task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where word embeddings come to the rescue. The following code shows
    how to instantiate the word embedding model, which is imported from the `gensim`
    library. First, we need to prepare the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code fragment prepares the file differently compared to the tokenizers.
    It creates a list of lines, and each line is a list of tokens, separated by whitespaces.
    Now, we are ready to create the `word2vec` model and train it on this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is that the model is trained on the corpus that we provided – the
    C program implementing a part of the HTTP protocol. We can look at the first 10
    tokens that have been extracted by writing `model.wv.key_to_index`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In total, `word2vec` extracted 259 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'This word embedding model is different from the tokenizers that we used before.
    It embeds the values of the words (tokens) into a latent space, which allows us
    to utilize the lexical properties of these words more smartly. For example, we
    can check the similarity of words using `model.wv.most_similar(positive=[''add''])`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also pretend that these words are vectors and their similarity is captured
    in this vector. Therefore, we can write something like `model.wv.most_similar(positive=
    [''file'', ''function''], negative=[''found''])` and obtain a result like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The expression will be similar if we use mathematics to express it: *result
    = file + function – found*. The resulting list of similar words is the list of
    words that are the closest to the vector that was captured as the result of this
    calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings are very powerful when we want to capture the similarity of
    the words and expressions. However, the original implementation of this model
    has certain limitations – for example, it does not allow us to use words that
    are not part of the original vocabulary. Asking for a word that is similar to
    an unknown token (for example, `model.wv.most_similar(positive=['return'])`) results
    in an error.
  prefs: []
  type: TYPE_NORMAL
- en: FastText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Luckily for us, there is an extension of the `word2vec` model that can approximate
    the unknown tokens – FastText. We can use it in a very similar way as we use `word2vec`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code fragment, the model is trained on the same set of data
    as `word2vec`. `model = FastText(vector_size=4, window=3, min_count=1)` creates
    an instance of the FastText model with three hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`vector_size`: The number of elements in the resulting feature vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window`: The size of the window used to capture context words'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_count`: The minimum frequency of a word to be included in the vocabulary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.build_vocab(corpus_iterable=tokenized_sentences)` builds the vocabulary
    of the model by iterating through the `tokenized_sentences` iterable (which should
    contain a list of lists, with each inner list representing a sentence tokenized
    into individual words) and adding each word to the vocabulary if it meets the
    `min_count` threshold. `model.train(corpus_iterable=tokenized_sentences, total_examples=len(tokenized_sentences),
    epochs=10)` trains the FastText model using the `tokenized_sentences` iterable
    for a total of 10 epochs. During each epoch, the model iterates through the corpus
    again and updates its internal weights based on the context words surrounding
    each target word. The `total_examples` parameter tells the model how many total
    examples (that is, sentences) are in the corpus, which is used to calculate the
    learning rate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input is the same. However, if we invoke the similarity for the unknown
    token, such as `model.wv.most_similar(positive=[''return''])`, we get the following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The set of three similar words indicates that the model can approximate an unknown
    token.
  prefs: []
  type: TYPE_NORMAL
- en: My next best practice is about the use of FastText.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #49'
  prefs: []
  type: TYPE_NORMAL
- en: Use word embeddings, such as FastText, as a valuable feature representation
    for text classification tasks, but consider incorporating them into more comprehensive
    models for optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Unless we need to use an LLM, this kind of feature extraction is a great alternative
    to the simple bag-of-words technique and powerful LLMs. It captures some parts
    of the meaning and allows us to design classifiers based on text data. It can
    also handle unknown tokens, which makes it very flexible.
  prefs: []
  type: TYPE_NORMAL
- en: From feature extraction to models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The feature extraction methods presented in this chapter are not the only ones
    we can use. Quite a few more exist (to say the least). However, they all work
    similarly. Unfortunately, no silver bullet exists, and all models have advantages
    and disadvantages. For the same task, but a different dataset, simpler models
    may be better than complex ones.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen how to extract features from text, images, and numerical
    data, it’s time we start training the models. This is what we’ll do in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Al-Sabbagh, K.W., et al. Selective regression testing based on big data: comparing
    feature extraction techniques. in 2020 IEEE International Conference on Software
    Testing, Verification and Validation Workshops (ICSTW).* *2020\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Staron, M., et al. Improving Quality of Code Review Datasets–Token-Based Feature
    Extraction Method. in Software Quality: Future Perspectives on Software Engineering
    Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21,
    2021, Proceedings 13\.* *2021\. Springer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sennrich, R., B. Haddow, and A. Birch, Neural machine translation of rare
    words with subword units. arXiv preprint* *arXiv:1508.07909, 2015.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gage, P., A new algorithm for data compression. C Users Journal, 1994\. 12(2):*
    *p. 23-38.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kudo, T. and J. Richardson, SentencePiece: A simple and language independent
    subword tokenizer and detokenizer for neural text processing. arXiv preprint*
    *arXiv:1808.06226, 2018.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
