- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Feature Engineering for Natural Language Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言数据的特征工程
- en: In the previous chapter, we explored how to extract features from numerical
    data and images. We explored a few algorithms that are used for that purpose.
    In this chapter, we’ll continue with the algorithms that extract features from
    natural language data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何从数值数据和图像中提取特征，并探讨了用于此目的的几个算法。在本章中，我们将继续探讨用于从自然语言数据中提取特征的算法。
- en: Natural language is a special kind of data source in software engineering. With
    the introduction of GitHub Copilot and ChatGPT, it became evident that machine
    learning and artificial intelligence tools for software engineering tasks are
    no longer science fiction. Therefore, in this chapter, we’ll explore the first
    steps that made these technologies so powerful – feature extraction from natural
    language data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是软件工程中的一种特殊数据源。随着GitHub Copilot和ChatGPT的引入，变得明显的是，用于软件工程任务的机器学习和人工智能工具不再是科幻。因此，在本章中，我们将探讨使这些技术变得如此强大的第一步——从自然语言数据中提取特征。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Tokenizers and their role in feature extraction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记化器及其在特征提取中的作用
- en: Bag-of-words as a simple technique for processing natural language data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋作为处理自然语言数据的一种简单技术
- en: Word embeddings as more advanced methods that can capture contexts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为更高级方法，词嵌入可以捕捉上下文
- en: Natural language data in software engineering and the rise of GitHub Copilot
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件工程中的自然语言数据和GitHub Copilot的兴起
- en: Programming has always been a mixture of science, engineering, and creativity.
    Creating new programs and being able to instruct computers to do something has
    always been something that was considered worth paying for – that’s how all programmers
    make their living. There have been attempts to automate programming and to support
    smaller tasks – for example, provide programmers with suggestions on how to use
    a specific function or library method.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 编程一直是科学、工程和创造力的结合。创建新的程序和能够指导计算机执行某些操作一直是被认为值得付费的事情——这就是所有程序员谋生的手段。人们尝试过自动化编程和支持较小的任务——例如，为程序员提供如何使用特定函数或库方法的建议。
- en: Good programmers, however, can make programs that last and that are readable
    for others. They can also make reliable programs that work without maintenance
    for a long period. The best programmers are the ones who can solve very difficult
    tasks and follow the principles and best practices of software engineering.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，优秀的程序员可以编写出持久且易于他人阅读的程序。他们还可以编写出长期无需维护的可靠程序。最好的程序员是那些能够解决非常困难任务并遵循软件工程原则和最佳实践的程序员。
- en: In 2020, something happened – GitHub Copilot entered the stage and showed that
    automated tools, based on **large language models** (**LLMs**), can provide much
    more than just suggestions for simple function calls. It has demonstrated that
    these language models are capable of providing suggestions for entire solutions
    and algorithms and even solving programming competitions. This opened up completely
    new possibilities for programmers – the best ones became extremely productive,
    and have been provided with the tools that allow them to focus on the complex
    parts of their programming tasks. The simple ones are now solved by GitHub Copilot
    and others.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在2020年，发生了某件事情——GitHub Copilot登上了舞台，并展示了基于**大型语言模型**（**LLMs**）的自动化工具不仅能提供简单的函数调用建议，还能提供更多。它已经证明，这些语言模型能够提供整个解决方案和算法的建议，甚至能够解决编程竞赛。这为程序员开辟了全新的可能性——最优秀的程序员变得极其高效，并得到了允许他们专注于编程任务复杂部分的工具。简单的任务现在由GitHub
    Copilot和其他工具解决。
- en: The reason why these tools are so good is because they are based on LLMs, which
    are capable of finding and quantifying contexts of programs. Just like a great
    chess player can foresee several moves in advance, these tools can foresee what
    the programmers may need in advance and provide useful suggestions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具之所以如此出色，是因为它们基于LLMs，能够找到并量化程序的上下文。就像一位伟大的棋手可以提前预见几步棋一样，这些工具可以提前预见程序员可能需要什么，并提供有用的建议。
- en: There are a few simple tricks that make these tools so effective, and one of
    them is feature engineering. Feature engineering for natural language tasks, including
    programming, is a process where a piece of text is transformed into a vector (or
    matrix) of numbers. These vectors can be simple – for example, quantifying the
    tokens – and also very complex – for example, finding an atomic piece of text
    linked to other tasks. We’ll explore these techniques in this chapter. We’ll start
    with a bit of a repetition of the bag-of-words technique (seen in [*Chapter 3*](B19548_03.xhtml#_idTextAnchor038)
    and [*Chapter 5*](B19548_05.xhtml#_idTextAnchor060)). We do not need to repeat
    the entire code, but we do need to provide a small re-cap to understand the limitations
    of these approaches. However, here is my best practice for choosing whether I
    need a tokenizer or embeddings.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些简单的技巧使得这些工具如此有效，其中之一就是特征工程。自然语言任务（包括编程）的特征工程是一个将文本片段转换为数字向量（或矩阵）的过程。这些向量可以是简单的——例如，量化标记——也可以是非常复杂的——例如，找到与其他任务相关联的原子文本片段。我们将在本章中探讨这些技术。我们将从稍微重复一下第3章和第5章中看到的词袋技术（见[*第3章*](B19548_03.xhtml#_idTextAnchor038)和[*第5章*](B19548_05.xhtml#_idTextAnchor060)）开始。我们不需要重复整个代码，但我们确实需要提供一个小的总结来理解这些方法的局限性。然而，这是我选择是否需要分词器或嵌入的最佳实践。
- en: 'Best practice #44'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #44'
- en: Use tokenizers for LLMs such as BERT and word embeddings for simple tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM（如BERT）使用分词器，对于简单任务使用词嵌入。
- en: For simple tasks, such as basic tokenization of the text for sentiment analysis
    or quickly understanding the dependencies in the text, I often use word embeddings.
    However, I usually use different tokenizers for working with LLMs such as BERT,
    RoBERTa, or AlBERT since these models are very good at finding dependencies on
    their own. However, for designing classifiers, I use word embeddings since they
    provide a fast way of creating feature vectors that are compatible with the “classical”
    machine learning algorithms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的任务，例如文本的基本分词用于情感分析或快速理解文本中的依赖关系，我通常使用词嵌入。然而，当与LLM（如BERT、RoBERTa或AlBERT）一起工作时，我通常使用不同的分词器，因为这些模型在自身寻找依赖关系方面非常出色。然而，在设计分类器时，我使用词嵌入，因为它们提供了一种快速创建与“经典”机器学习算法兼容的特征向量的方法。
- en: Choosing a tokenizer needs to be done based on the task. We’ll look at this
    closer in this chapter, but the topic itself could occupy an entire book. For
    example, for tasks that require information about the part of speech (or, in many
    cases, part of the abstract syntax tree of a program), we need to use a tokenizer
    that is designed to capture that information – for example, from a programming
    language parser. These tokenizers provide more information to the model, but they
    impose more requirements on the data – an abstract syntax tree-based tokenizer
    requires the program to be well formed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 选择分词器需要根据任务来决定。我们将在本章中更详细地探讨这个问题，但这个主题本身可能需要一整本书来阐述。例如，对于需要关于词性（或者在很多情况下，程序抽象语法树的某一部分）的信息的任务，我们需要使用专门设计来捕获这些信息的分词器——例如，从编程语言解析器中获取。这些分词器为模型提供了更多信息，但它们对数据的要求也更高——基于抽象语法树的分词器要求程序具有良好的格式。
- en: What a tokenizer is and what it does
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器是什么以及它做什么
- en: The first step in feature engineering text data is to decide on the tokenization
    of the text. The tokenization of text is a process of extracting parts of words
    that capture the meaning of the text without too many extra details.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程文本数据的第一个步骤是决定文本的分词。文本分词是一个提取能够捕捉文本意义而不包含太多额外细节的词的部分的过程。
- en: There are different ways to extract tokens, which we’ll explore in this chapter,
    but to illustrate the problem of extracting tokens, let’s look at one word that
    can take different forms – *print*. The word by itself can be a token, but it
    can be in different forms, such as *printing*, *printed*, *printer*, *prints*,
    *imprinted*, and many others. If we use a simple tokenizer, each of these words
    will be one token – which means quite a few tokens. However, all these tokens
    capture some sort of meaning related to printing, so maybe we do not need so many
    of them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来提取标记，我们将在本章中探讨这些方法，但为了说明提取标记的问题，让我们看看一个可以采取不同形式的单词——*print*。这个单词本身可以是一个标记，但它可以有不同的形式，如*printing*、*printed*、*printer*、*prints*、*imprinted*等等。如果我们使用简单的分词器，这些单词中的每一个都将是一个标记——这意味着有很多标记。然而，所有这些标记都捕捉到与打印相关的某种意义，所以可能我们不需要这么多。
- en: This is where tokenizers come in. Here, we can decide how to treat these different
    forms of the word. We could take the main part only – *print* – and then all the
    other forms would be counted as that, so both *imprinted* and *printing* would
    be counted as *print*. It decreases the number of tokens, but we reduce the expressiveness
    of our feature vector – some information is lost as we do not have the same number
    of tokens to use. We could pre-design the set of tokens – that is, use both *print*
    and *imprint* to distinguish between different contexts. We could also use bigrams
    (two words together) as tokens (for example, *is_going* versus is, *going* – the
    first one requires both words to be in the specific sequence, where the other
    one allows them to be in two different sequences), or we could add the information
    about whether the word is an object of the subject in a sentence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是分词器发挥作用的地方。在这里，我们可以决定如何处理这些不同的词形。我们可以只取主要部分——*print*——然后所有其他形式都会被计为那样，所以*imprinted*和*printing*都会被计为*print*。这减少了标记的数量，但我们也减少了特征向量的表达性——一些信息丢失了，因为我们没有相同数量的标记可以使用。我们可以预先设计一组标记——也就是说，使用*print*和*imprint*来区分不同的上下文。我们也可以使用双词（两个词一起）作为标记（例如，*is_going*与*is*，*going*——第一个需要两个词以特定顺序出现，而第二个允许它们出现在两个不同的序列中），或者我们可以添加关于单词是否为句子中主语对象的信息。
- en: Bag-of-words and simple tokenizers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋和简单分词器
- en: In *Chapters 3* and *5*, we saw the use of the bag-of-words feature extraction
    technique. This technique takes the text and counts the number of tokens, which
    were words in *Chapters 3* and *5*. It is simple and computationally efficient,
    but it has a few problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*和*第5章*中，我们看到了词袋特征提取技术的应用。这种技术对文本进行计数，统计标记的数量，在*第3章*和*第5章*中是单词。它简单且计算效率高，但有几个问题。
- en: 'When instantiating the bag-of-words tokenizer, we can use several parameters
    that strongly impact the results, as we did in the following fragment of code
    in the previous chapters:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当实例化词袋分词器时，我们可以使用几个参数，这些参数会强烈影响结果，就像我们在前几章的代码片段中所做的那样：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `max_features` parameter is a cut-off value that reduces the number of features,
    but it also can introduce noise where two (or more) distinct sentences have the
    same feature vector (we saw an example of such a sentence in [*Chapter 2*](B19548_02.xhtml#_idTextAnchor023)).
    Since we discussed noise and the problems related to it, we could be tempted to
    use other parameters – `max_df` and `min_df`. These two parameters determine how
    often a word should appear in the document to be considered a token. The tokens
    that are too rare can (`min_df`) result in a sparse matrix – a lot of 0s in the
    feature matrix – but they can be a very good discriminant between data points.
    Maybe these rare words are just what we are looking for. The other parameter (`max_df`)
    results in more dense feature matrices, but they may not discriminate the data
    points completely. This means that it is not so simple to select these parameters
    – we need experiments and we need to use machine learning model training (and
    validation) to find the right vector.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features`参数是一个截止值，它减少了特征的数量，但它也可能在两个（或更多）不同句子具有相同特征向量时引入噪声（我们在[*第2章*](B19548_02.xhtml#_idTextAnchor023)中看到了这样一个句子的例子）。由于我们已经讨论了噪声及其相关问题，我们可能会倾向于使用其他参数——`max_df`和`min_df`。这两个参数决定了单词在文档中应该出现多少次才能被认为是标记。过于罕见的标记（`min_df`）可能导致稀疏矩阵——特征矩阵中有许多0——但它们可以在数据点之间提供很好的区分。也许这些罕见的单词正是我们所寻找的。另一个参数（`max_df`）导致更密集的特征矩阵，但它们可能无法完全区分数据点。这意味着选择这些参数并不简单——我们需要实验，并使用机器学习模型训练（和验证）来找到正确的向量。'
- en: 'There is also another way – we can perform a recursive search for such a feature
    vector that would discriminate all data points without adding too much noise.
    My team has experimented with such algorithms, which yield very good performance
    for model training and validation but are computationally very expensive. Such
    an algorithm is presented in *Figure 8**.1*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还有一种方法——我们可以执行递归搜索以找到这样一个特征向量，它能够区分所有数据点而不会添加太多噪声。我的团队已经尝试过这样的算法，这些算法在模型训练和验证方面表现出色，但计算成本非常高。这种算法在*图8.1*中展示：
- en: '![Figure 8.1 – An algorithm for finding a set of features that discriminate
    all data points in a text file. The flow has been simplified to illustrate the
    main points](img/B19548_08_1.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 一种在文本文件中找到一组能够区分所有数据点的特征算法。流程已被简化以说明主要点](img/B19548_08_1.jpg)'
- en: Figure 8.1 – An algorithm for finding a set of features that discriminate all
    data points in a text file. The flow has been simplified to illustrate the main
    points
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 一个用于在文本文件中找到一组能够区分所有数据点的特征的算法。流程已被简化以说明主要观点
- en: The algorithm works by adding new tokens if a data point has the same feature
    vector as any of the previous ones. It starts by taking the first token from the
    first line, then the second line. If the token can discriminate between these
    two lines, then it proceeds to the third line. Once the algorithm discovers that
    two different lines have the same feature vector, it finds out whether there is
    a token that can discriminate between these lines and adds it to the set of features.
    It continues until there are no new tokens to add or all lines have been analyzed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过添加新标记来工作，如果数据点具有与之前任何数据点相同的特征向量。它首先从第一行取第一个标记，然后是第二行。如果标记可以区分这两行，则继续到第三行。一旦算法发现两行具有不同的特征向量，它就会找出是否存在可以区分这些行的标记，并将其添加到特征集。它继续添加，直到没有新标记可以添加或所有行都已分析。
- en: This algorithm guarantees that the set of tokens that best discriminates the
    analyzed dataset is found. However, it has one large disadvantage – it is slow
    (as it must start from the first line once a new token is found/needed). The resulting
    feature matrix is also not optimal – it contains a lot of 0s since most of the
    tokens can only be found in one line. The feature matrix, in turn, can be much
    larger than the actual raw dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法保证找到最佳区分分析数据集的标记集。然而，它有一个很大的缺点——它很慢（因为它必须从找到/需要新标记的第一行开始）。生成的特征矩阵也不是最优的——它包含很多0，因为大多数标记只能在一行中找到。反过来，特征矩阵可能比实际原始数据集大得多。
- en: This is where my next best practice comes in.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我的下一个最佳实践发挥作用的地方。
- en: 'Best practice #45'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #45'
- en: Use bag-of-words tokenizers together with dictionaries when your task requires
    a pre-defined set of words.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的任务需要预定义的单词集时，请使用词袋模型分词器并结合字典。
- en: I use bag-of-words tokenizers quite often when analyzing programming language
    code. I use a pre-defined set of keywords from the programming language to boost
    the tokenizer and then the standard `CountVectorizer`. This allows me to control
    part of the vocabulary that I am interested in – keywords – and allows the tokenizer
    to adjust to the text.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析编程语言代码时，我经常使用词袋模型分词器。我使用编程语言中预定义的关键词集来增强分词器，然后使用标准的`CountVectorizer`。这使我能够控制我感兴趣的部分词汇量——关键词——并允许分词器适应文本。
- en: WordPiece tokenizer
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WordPiece分词器
- en: A better way to tokenize and extract features from text documents is to use
    a WordPiece tokenizer. This tokenizer works in such a way that it finds the most
    common pieces of text that it can discriminate, and also the ones that are the
    most common. This kind of tokenizer needs to be trained – that is, we need to
    provide a set of representative texts to get the right vocabulary (tokens).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本文档中分词和提取特征的一个更好的方法是使用WordPiece分词器。这种分词器以这样的方式工作，即它找到它可以区分的最常见的文本片段，以及最常见的那些。这种类型的分词器需要训练——也就是说，我们需要提供一组代表性文本以获得正确的词汇（标记）。
- en: 'Let’s look at an example where we use a simple program, a module from an open
    source project, to train such a tokenizer and then apply this tokenizer to the
    famous “Hello World” program in C. Let’s start by creating the tokenizer:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子，我们使用一个简单的程序，一个开源项目中的模块，来训练这样的分词器，然后将这个分词器应用于著名的“Hello World”C语言程序。让我们首先创建分词器：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this example, we’re using the WordPiece tokenizer from the Hugging Face library,
    specifically the one that is prepared to work with LLMs such as BERT. There are
    several parameters that we can use, but let’s settle with the parameters that
    show that we are only interested in lowercase characters; we do not want to handle
    Chinese characters and want to start from scratch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用Hugging Face库中的WordPiece分词器，特别是为与BERT等LLM一起工作而准备的分词器。我们可以使用几个参数，但让我们只使用显示我们只对小写字母感兴趣；我们不希望处理中文字符，并希望从头开始。
- en: 'Now, we need to find a piece of text that we can train the tokenizer on. In
    this example, I’ll use one of the files from an open source project – AzureOS
    NetX. It’s a component written in C that handles parts of the internet HTTP protocol.
    We create a new variable – `path` – and add the path to that file there. Once
    we’ve prepared the text, we can train the tokenizer:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要找到一个可以用来训练分词器的文本片段。在这个例子中，我将使用开源项目中的一个文件 – AzureOS NetX。它是一个用C语言编写的组件，用于处理互联网HTTP协议的部分。我们创建一个新的变量
    – `path` – 并将文件的路径添加到那里。一旦我们准备好了文本，我们就可以训练分词器：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We’ve set the tokenizer to a similar set of parameters, similar to `CountVectorizer`
    in the previous examples. This preceding code fragment finds the set of the most
    common pieces of words and uses them as tokens.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将分词器设置成与之前示例中的`CountVectorizer`相似的一组参数。这个前代码片段找到了最常见的单词片段并将它们用作标记。
- en: 'We can get the list of tokens through the `tokenizer.get_vocab()` statement,
    which results in a long dictionary of tokens. Here are the first few ones:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`tokenizer.get_vocab()`语句获取标记列表，这将产生一个长的标记字典。以下是前几个标记：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The first token is a piece of a word, which is denoted by the fact that it has
    two hashtags at the beginning of it. This token is mapped to the number `183`
    in the vocabulary. This mapping is important as the numbers are used later on
    by the machine learning models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个标记是单词的一部分，这通过它开头有两个井号的事实来表示。这个标记在词汇表中映射到数字`183`。这种映射很重要，因为数字在后续的机器学习模型中会被使用。
- en: Another interesting observation is that some of the tokens, such as `'disable'`,
    are not pieces of words but entire words. This means that this token does not
    appear as a piece of the word anywhere and it does not contain any other pieces
    of other words in the vocabulary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的观察是，一些标记，如`'disable'`，不是单词的一部分，而是整个单词。这意味着这个标记在任何地方都没有作为单词的一部分出现，并且它不包含词汇表中其他单词的任何部分。
- en: 'Once we’ve trained the WordPiece tokenizer, we can check how the tokenizer
    extracts features from a simple C program:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了WordPiece分词器，我们可以检查分词器如何从一个简单的C程序中提取特征：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding piece of code tokenizes the program. The result is the following
    list of tokens (only the first 10 tokens of 50 are shown):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段对程序进行了分词。结果是以下标记列表（只显示了50个标记中的前10个）：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first line, which starts with the `int` token, has been tokenized in the
    following way. The first word – `int` – is split into two tokens: `"in"` and `"##t"`.
    This is because these two parts were used in the training program. We can also
    see that the second token – `main` – is split into two tokens: `"ma"` and `"##in"`.
    The IDs for these tokens are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行，以`int`标记开始，已经被以下方式分词。第一个单词 – `int` – 被分割成两个标记：`"in"` 和 `"##t"`。这是因为这两个部分被用于训练程序中。我们还可以看到第二个标记
    – `main` – 被分割成两个标记：`"ma"` 和 `"##in"`。这些标记的ID如下：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This means that this list of numbers is the feature vector for our simple C
    program.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这个数字列表是我们简单C程序的特征向量。
- en: WordPiece tokenization is very effective, but it depends a lot on the training
    data. If we use training data that is very different from the tokenized text,
    the set of tokens will not be very helpful. Therefore, my next best practice is
    about training this tokenizer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: WordPiece分词非常有效，但它很大程度上依赖于训练数据。如果我们使用与分词文本非常不同的训练数据，标记集将不会很有帮助。因此，我的下一个最佳实践是关于训练这个分词器。
- en: 'Best practice #46'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #46'
- en: Use the WordPiece tokenizer as your first choice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将WordPiece分词器作为首选。
- en: I usually use this tokenizer as my first choice. It is relatively flexible but
    quite fast. It allows us to capture a vocabulary that does the job most of the
    time and does not require a lot of setup. For simple tasks with straightforward
    language and a well-defined vocabulary, traditional word-level tokenization or
    other subword tokenization methods such as **byte-pair encoding** (**BPE**) may
    suffice. WordPiece tokenization can increase the size of the input data due to
    the introduction of subword tokens. This can impact memory and computational requirements.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常将这个分词器作为首选。它相对灵活但相当快速。它允许我们捕获一个词汇表，大多数时候都能完成任务，并且不需要很多设置。对于具有直接语言和明确定义词汇的简单任务，传统的词级分词或其他子词分词方法，如**字节对编码**（**BPE**）可能就足够了。WordPiece分词可能会由于引入子词标记而增加输入数据的大小。这可能会影响内存和计算需求。
- en: BPE
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BPE
- en: A more advanced method for tokenizing text is the BPE algorithm. This algorithm
    is based on the same premises as the compression algorithm that was created in
    the 1990s by Gage. The algorithm compresses a series of bytes by the bytes not
    used in the compressed data. The BPE tokenizer does a similar thing, except that
    it replaces a series of tokens with new bytes that are not used in the text. In
    this way, the algorithm can create a much larger vocabulary than `CountVectorizer`
    and the WordPiece tokenizer. BPE is very popular both for its ability to handle
    large vocabulary and for its efficient implementation through the fastBPE library.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标记化的一个更高级的方法是BPE算法。这个算法基于与20世纪90年代由Gage创建的压缩算法相同的原理。该算法通过压缩数据中未使用的字节来压缩一系列字节。BPE标记化程序做的是类似的事情，只不过它用未在文本中使用的新的字节替换了一系列标记。这样，该算法可以创建比`CountVectorizer`和WordPiece标记化程序更大的词汇表。BPE因其处理大型词汇表的能力和通过fastBPE库的高效实现而非常受欢迎。
- en: 'Let’s explore how to apply this tokenizer to the same data and check the difference
    between the previous two. The following code fragment shows how to instantiate
    this tokenizer from the Hugging Face library:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何将这个标记化程序应用于相同的数据，并检查与前两种方法的差异。以下代码片段展示了如何从Hugging Face库中实例化这个标记化程序：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This tokenizer requires training as it needs to find the optimal set of pairs
    of tokens. Therefore, we need to instantiate a trainer class and train it. The
    following piece of code does just that:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标记化程序需要训练，因为它需要找到最优的标记对集合。因此，我们需要实例化一个训练类并对其进行训练。以下代码片段正是这样做的：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The important part of this training process is the use of a special pre-tokenizer.
    The pre-tokenizer is how we initially split words into tokens. In our case, we
    use the standard whitespaces, but we could use something more advanced. For example,
    we could use semicolons and therefore use entire lines of code as tokens.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练过程中的重要部分是使用一个特殊的预标记化程序。预标记化程序是我们最初将单词分割成标记的方式。在我们的案例中，我们使用标准的空白字符，但我们可以使用更高级的方法。例如，我们可以使用分号，因此可以将整行代码作为标记。
- en: 'After executing the preceding fragment of code, our tokenizer is trained and
    ready to use. We can check the tokens by writing `tokenizer.get_vocab()`. The
    set of tokens is as follows (the first 10 tokens):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码片段后，我们的标记化程序已经训练完毕，可以使用了。我们可以通过编写`tokenizer.get_vocab()`来检查标记。以下是一些标记（前10个标记）：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This set of tokens is very different from the set of tokens in previous cases.
    It contains a mix of words such as “will” and subwords such as “ol.” This is because
    the BPE tokenizer found some replicated tokens and replaced them with dedicated
    bytes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这组标记与之前案例中的标记集非常不同。它包含了一些单词，如“will”，和一些子词，如“ol.”。这是因为BPE标记化程序发现了一些重复的标记，并用专门的字节替换了它们。
- en: 'Best practice #47'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #47'
- en: Use BPE when working with LLMs and large corpora of text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理大型语言模型和大量文本语料库时使用BPE。
- en: I use BPE as my go-to when I analyze large pieces of text, such as large code
    bases. It is blazingly fast for this task and can capture complex dependencies.
    It is also heavily used in models such as BERT or GPT.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我分析大量文本时，例如大型代码库，我会首选使用BPE。这项任务对BPE来说非常快速，并且能够捕捉复杂的依赖关系。它也在BERT或GPT等模型中被大量使用。
- en: Now, in our case, the program’s source code that we used to train the BPE tokenizer
    was small, so a lot of words did not repeat themselves and the optimization does
    not make much sense. Therefore, a WordPiece tokenizer would do an equally (if
    not better) job. However, for larger text corpora, this tokenizer is much more
    effective and efficient than WordPiece or bag-of-words. It is also the basis for
    the next tokenizer – SentencePiece.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们的案例中，我们用来训练BPE标记化程序的源代码很小，所以很多单词没有重复出现，优化并没有太多意义。因此，WordPiece标记化程序可以完成同样（如果不是更好）的工作。然而，对于更大的文本语料库，这个标记化程序比WordPiece或词袋模型更有效率和高效。它也是下一个标记化程序——SentencePiece的基础。
- en: The SentencePiece tokenizer
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SentencePiece标记化程序
- en: 'SentencePiece is a more general option than BPE for one more reason: it allows
    us to treat whitespaces as regular tokens. This allows us to find more complex
    dependencies and therefore train models that understand more than just pieces
    of words. Hence the name – SentencePiece. This tokenizer was originally introduced
    to enable the tokenization of languages such as Japanese, which do not use whitespaces
    in the same way as, for example, English. The tokenizer can be installed by running
    the `pip install -q` `sentencepiece` command.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分割（SentencePiece）比BPE更通用，还有一个原因：它允许我们将空白视为常规标记。这使我们能够找到更复杂的依赖关系，因此可以训练出理解不仅仅是单词片段的模型。因此得名——句子分割。这个分词器最初是为了使像日语这样的语言（例如，与英语不同，日语不使用空白）的标记化成为可能。可以通过运行`pip
    install -q sentencepiece`命令来安装这个分词器。
- en: 'In the following code example, we’re instantiating and training the SentencePiece
    tokenizer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们实例化和训练了SentencePiece分词器：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’ve trained it on the same file as the previous tokenizers. The text was a
    programming file, so we could expect the tokenizer to give us a better understanding
    of what’s in a programming language than what’s in a normal piece of text. Something
    worth noting is the size of the vocabulary, which is 200, unlike 30,000 in the
    previous examples. This is important because this tokenizer tries to find as many
    tokens as this parameter. Since our input program is very short – one file with
    a few functions in it – the tokenizer cannot create more than about 300 tokens.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在与其他分词器相同的文件上对其进行了训练。文本是一个编程文件，因此我们可以预期分词器能比正常文本更好地理解编程语言的内容。值得注意的是词汇表的大小，它是200，而之前的例子中是30,000。这是因为这个分词器试图找到尽可能多的标记。由于我们的输入程序非常短——一个包含几个函数的文件——分词器不能创建超过大约300个标记。
- en: 'The following fragment encodes the “Hello World” program using this tokenizer
    and prints the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下片段使用这个分词器对“Hello World”程序进行编码，并打印以下输出：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first 10 tokens are represented in the following way:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前十个标记的表示方式如下：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The new element in this tokenizer is the underscore character (`_`). It denotes
    whitespace in the text. This is unique and it allows us to use this tokenizer
    more effectively in programming language comprehension because it allows us to
    capture such programming constructs as nesting – that is, using tabs instead of
    spaces or writing several statements in the same line. This is all because this
    tokenizer treats whitespaces as something important.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个分词器中引入的新元素是下划线字符（`_`）。它在文本中表示空白。这是独特的，它使我们能够更有效地在编程语言理解中使用这个分词器，因为它允许我们捕获诸如嵌套之类的编程结构——也就是说，使用制表符而不是空格，或者在同一行中编写多个语句。这一切都是因为这个分词器将空白视为重要的事物。
- en: 'Best practice #48'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #48'
- en: Use the SentencePiece tokenizer when no clear word boundaries are present.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有明显的单词边界时，请使用SentencePiece分词器。
- en: I use SentencePiece when analyzing programming language code with a focus on
    programming styles – for example, when we focus on things such as camel-case variable
    naming. For this task, it is important to understand how programmers use spaces,
    formatting, and other compiler-transparent elements. Therefore, this tokenizer
    is perfect for such tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当分析编程语言代码并关注编程风格时，我会使用SentencePiece——例如，当我们关注诸如驼峰式变量命名等问题时。对于这个任务，理解程序员如何使用空格、格式化和其他编译器透明的元素非常重要。因此，这个分词器非常适合这样的任务。
- en: Word embeddings
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Tokenizers are one way of extracting features from text. They are powerful and
    can be trained to create complex tokens and capture statistical dependencies of
    words. However, they are limited by the fact that they are completely unsupervised
    and do not capture any meaning or relationship between words. This means that
    the tokenizers are great at providing input to neural network models, such as
    BERT, but sometimes, we would like to have features that are more aligned with
    a certain task.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器是从文本中提取特征的一种方法。它们功能强大，可以训练以创建复杂的标记并捕获单词的统计依赖关系。然而，它们受限于它们是完全无监督的，并且不捕获任何单词之间的意义或关系。这意味着分词器非常适合为神经网络模型，如BERT，提供输入，但有时我们希望有与特定任务更对齐的特征。
- en: 'This is where word embeddings come to the rescue. The following code shows
    how to instantiate the word embedding model, which is imported from the `gensim`
    library. First, we need to prepare the dataset:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是词嵌入发挥作用的地方。以下代码展示了如何实例化从`gensim`库导入的词嵌入模型。首先，我们需要准备数据集：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code fragment prepares the file differently compared to the tokenizers.
    It creates a list of lines, and each line is a list of tokens, separated by whitespaces.
    Now, we are ready to create the `word2vec` model and train it on this data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的标记化器相比，前面的代码片段以不同的方式准备文件。它创建了一个行列表，每行是一个由空格分隔的标记列表。现在，我们已经准备好创建`word2vec`模型并在这些数据上训练它：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result is that the model is trained on the corpus that we provided – the
    C program implementing a part of the HTTP protocol. We can look at the first 10
    tokens that have been extracted by writing `model.wv.key_to_index`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，该模型是在我们提供的语料库上训练的——实现HTTP协议一部分的C程序。我们可以通过编写`model.wv.key_to_index`来查看已提取的前10个标记：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In total, `word2vec` extracted 259 tokens.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总共，`word2vec`提取了259个标记。
- en: 'This word embedding model is different from the tokenizers that we used before.
    It embeds the values of the words (tokens) into a latent space, which allows us
    to utilize the lexical properties of these words more smartly. For example, we
    can check the similarity of words using `model.wv.most_similar(positive=[''add''])`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前使用的标记化器不同，这个词嵌入模型将词（标记）的值嵌入到一个潜在空间中，这使得我们可以更智能地利用这些词的词汇属性。例如，我们可以使用`model.wv.most_similar(positive=['add'])`来检查词的相似性：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also pretend that these words are vectors and their similarity is captured
    in this vector. Therefore, we can write something like `model.wv.most_similar(positive=
    [''file'', ''function''], negative=[''found''])` and obtain a result like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以假设这些词是向量，它们的相似性被这个向量捕捉。因此，我们可以写一些类似的东西，比如 `model.wv.most_similar(positive=
    ['file', 'function'], negative=['found'])` 并获得如下结果：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The expression will be similar if we use mathematics to express it: *result
    = file + function – found*. The resulting list of similar words is the list of
    words that are the closest to the vector that was captured as the result of this
    calculation.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用数学来表示这个表达式，那么结果将是：*result = file + function – found*。这个相似词列表是距离这个计算结果捕获的向量最近的词列表。
- en: Word embeddings are very powerful when we want to capture the similarity of
    the words and expressions. However, the original implementation of this model
    has certain limitations – for example, it does not allow us to use words that
    are not part of the original vocabulary. Asking for a word that is similar to
    an unknown token (for example, `model.wv.most_similar(positive=['return'])`) results
    in an error.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要捕捉词和表达式的相似性时，词嵌入非常强大。然而，该模型的原始实现存在某些限制——例如，它不允许我们使用原始词汇表之外的词。请求与未知标记（例如，`model.wv.most_similar(positive=['return'])`）相似的词会导致错误。
- en: FastText
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FastText
- en: 'Luckily for us, there is an extension of the `word2vec` model that can approximate
    the unknown tokens – FastText. We can use it in a very similar way as we use `word2vec`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个`word2vec`模型的扩展可以近似未知标记——FastText。我们可以用与使用`word2vec`非常相似的方式使用它：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code fragment, the model is trained on the same set of data
    as `word2vec`. `model = FastText(vector_size=4, window=3, min_count=1)` creates
    an instance of the FastText model with three hyperparameters:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，模型是在与`word2vec`相同的 数据集上训练的。`model = FastText(vector_size=4, window=3,
    min_count=1)` 创建了一个具有三个超参数的FastText模型实例：
- en: '`vector_size`: The number of elements in the resulting feature vector'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_size`：结果特征向量中的元素数量'
- en: '`window`: The size of the window used to capture context words'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window`：用于捕捉上下文词的窗口大小'
- en: '`min_count`: The minimum frequency of a word to be included in the vocabulary'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count`：要包含在词汇表中的单词的最小频率'
- en: '`model.build_vocab(corpus_iterable=tokenized_sentences)` builds the vocabulary
    of the model by iterating through the `tokenized_sentences` iterable (which should
    contain a list of lists, with each inner list representing a sentence tokenized
    into individual words) and adding each word to the vocabulary if it meets the
    `min_count` threshold. `model.train(corpus_iterable=tokenized_sentences, total_examples=len(tokenized_sentences),
    epochs=10)` trains the FastText model using the `tokenized_sentences` iterable
    for a total of 10 epochs. During each epoch, the model iterates through the corpus
    again and updates its internal weights based on the context words surrounding
    each target word. The `total_examples` parameter tells the model how many total
    examples (that is, sentences) are in the corpus, which is used to calculate the
    learning rate.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.build_vocab(corpus_iterable=tokenized_sentences)`通过遍历`tokenized_sentences`可迭代对象（该对象应包含一个列表的列表，其中每个内部列表代表一个句子被分解成单个单词）并将每个单词添加到词汇表中，如果它满足`min_count`阈值。`model.train(corpus_iterable=tokenized_sentences,
    total_examples=len(tokenized_sentences), epochs=10)`使用`tokenized_sentences`可迭代对象训练FastText模型，总共10个epoch。在每个epoch中，模型再次遍历语料库，并根据每个目标词周围的上下文单词更新其内部权重。`total_examples`参数告诉模型语料库中有多少个总示例（即句子），这用于计算学习率。'
- en: 'The input is the same. However, if we invoke the similarity for the unknown
    token, such as `model.wv.most_similar(positive=[''return''])`, we get the following
    result:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是相同的。然而，如果我们调用未知标记的相似度，例如`model.wv.most_similar(positive=['return'])`，我们会得到以下结果：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The set of three similar words indicates that the model can approximate an unknown
    token.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个相似词的集合表明模型可以近似未知标记。
- en: My next best practice is about the use of FastText.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我接下来的最佳实践是关于FastText的使用。
- en: 'Best practice #49'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #49'
- en: Use word embeddings, such as FastText, as a valuable feature representation
    for text classification tasks, but consider incorporating them into more comprehensive
    models for optimal performance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词嵌入，如FastText，作为文本分类任务的有价值特征表示，但考虑将其纳入更全面的模型以实现最佳性能。
- en: Unless we need to use an LLM, this kind of feature extraction is a great alternative
    to the simple bag-of-words technique and powerful LLMs. It captures some parts
    of the meaning and allows us to design classifiers based on text data. It can
    also handle unknown tokens, which makes it very flexible.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们需要使用LLM，这种特征提取是简单词袋技术以及强大的LLM的绝佳替代方案。它捕捉到一些含义的部分，并允许我们基于文本数据设计分类器。它还可以处理未知标记，这使得它非常灵活。
- en: From feature extraction to models
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从特征提取到模型
- en: The feature extraction methods presented in this chapter are not the only ones
    we can use. Quite a few more exist (to say the least). However, they all work
    similarly. Unfortunately, no silver bullet exists, and all models have advantages
    and disadvantages. For the same task, but a different dataset, simpler models
    may be better than complex ones.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中提出的特征提取方法并非我们唯一能使用的。至少还有更多（更不用说其他方法了）。然而，它们的工作原理相似。不幸的是，没有一劳永逸的解决方案，所有模型都有其优势和劣势。对于同一任务，但不同的数据集，简单的模型可能比复杂的模型更好。
- en: Now that we have seen how to extract features from text, images, and numerical
    data, it’s time we start training the models. This is what we’ll do in the next
    chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何从文本、图像和数值数据中提取特征，现在是时候开始训练模型了。这就是我们在下一章将要做的。
- en: References
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Al-Sabbagh, K.W., et al. Selective regression testing based on big data: comparing
    feature extraction techniques. in 2020 IEEE International Conference on Software
    Testing, Verification and Validation Workshops (ICSTW).* *2020\. IEEE.*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Al-Sabbagh, K.W., et al. Selective regression testing based on big data: comparing
    feature extraction techniques. in 2020 IEEE International Conference on Software
    Testing, Verification and Validation Workshops (ICSTW).* *2020\. IEEE.*'
- en: '*Staron, M., et al. Improving Quality of Code Review Datasets–Token-Based Feature
    Extraction Method. in Software Quality: Future Perspectives on Software Engineering
    Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21,
    2021, Proceedings 13\.* *2021\. Springer.*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Staron, M., et al. Improving Quality of Code Review Datasets–Token-Based Feature
    Extraction Method. in Software Quality: Future Perspectives on Software Engineering
    Quality: 13th International Conference, SWQD 2021, Vienna, Austria, January 19–21,
    2021, Proceedings 13\.* *2021\. Springer.*'
- en: '*Sennrich, R., B. Haddow, and A. Birch, Neural machine translation of rare
    words with subword units. arXiv preprint* *arXiv:1508.07909, 2015.*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sennrich, R., B. Haddow, and A. Birch, Neural machine translation of rare
    words with subword units. arXiv preprint* *arXiv:1508.07909, 2015.*'
- en: '*Gage, P., A new algorithm for data compression. C Users Journal, 1994\. 12(2):*
    *p. 23-38.*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gage, P., A new algorithm for data compression. C Users Journal, 1994\. 12(2):*
    *p. 23-38.*'
- en: '*Kudo, T. and J. Richardson, SentencePiece: A simple and language independent
    subword tokenizer and detokenizer for neural text processing. arXiv preprint*
    *arXiv:1808.06226, 2018.*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kudo, T. 和 J. Richardson, SentencePiece：一种简单且语言无关的子词分词和去分词器，用于神经文本处理。arXiv预印本*
    *arXiv:1808.06226, 2018.*'
