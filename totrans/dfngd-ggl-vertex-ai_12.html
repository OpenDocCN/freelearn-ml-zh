<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer135">
			<h1 id="_idParaDest-173" class="chapter-number"><a id="_idTextAnchor173"/>12</h1>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor174"/>Vertex AI – Generative AI Tools</h1>
			<p><strong class="bold">Generative artificial intelligence</strong> (<strong class="bold">GenAI)</strong> is a rapidly evolving field of AI that enables machines to create new content, such as text, images, code, and music. GenAI models are trained on massive datasets <a id="_idIndexMarker780"/>of existing content, and they learn to identify patterns and relationships that underlie that content. Once trained, these models can be used to generate new content that is similar to the content they were trained on but that is also unique <span class="No-Break">and creative.</span></p>
			<p>GenAI models can be used for a wide variety of applications, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Text generation</strong>: Generating text, such as news articles, blog posts, marketing copy, and <span class="No-Break">creative content</span></li>
				<li><strong class="bold">Chatbots</strong>: Creating chatbots that<a id="_idIndexMarker781"/> can have natural conversations <span class="No-Break">with users</span></li>
				<li><strong class="bold">Image generation</strong>: Generating images, such as product photos, marketing images, and <span class="No-Break">artistic images</span></li>
				<li><strong class="bold">Code generation</strong>: Generating code, such as Python scripts, Java classes, and <span class="No-Break">HTML templates</span></li>
				<li><strong class="bold">Text embeddings</strong>: Creating text embeddings that can be used for tasks such as text classification, text <a id="_idIndexMarker782"/>search, and <strong class="bold">natural language </strong><span class="No-Break"><strong class="bold">inference</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NLI</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">GenAI Fundamentals</span></li>
				<li>GenAI with <span class="No-Break">Vertex AI</span></li>
				<li>Prompt <span class="No-Break">engineering overview</span></li>
				<li>Retrieval augmented <span class="No-Break">generation approach</span></li>
				<li><span class="No-Break">Model Tuning</span></li>
			</ul>
			<p>Before we dive into the GenAI capabilities of Vertex AI, let’s first understand the fundamentals <span class="No-Break">of GenAI.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor175"/>GenAI fundamentals</h1>
			<p>GenAI is a subfield of AI that focuses on developing algorithms and models capable of generating new, original content, such as text, images, music, or code. This is in contrast to traditional AI <a id="_idIndexMarker783"/>models, which typically focus on understanding and classifying <span class="No-Break">existing data.</span></p>
			<p>At the heart of GenAI lies the concept of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>). LLMs are a type of <strong class="bold">artificial neural network</strong> (<strong class="bold">ANN</strong>) that has been trained <a id="_idIndexMarker784"/>on massive amounts of text data. This training allows LLMs to learn patterns and structures of human language, enabling them to generate text that is often indistinguishable from <span class="No-Break">human-written text.</span></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor176"/>GenAI versus traditional AI</h2>
			<p>Traditional AI models are typically<a id="_idIndexMarker785"/> based on <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>), where the model is trained on a dataset of labeled examples. For example, a model for image classification might be<a id="_idIndexMarker786"/> trained on a dataset of images that have been labeled with the correct object category. Once trained, the model can then be used to classify <span class="No-Break">new images.</span></p>
			<p>GenAI models, on the other hand, are <a id="_idIndexMarker787"/>typically based on <strong class="bold">unsupervised learning</strong> (<strong class="bold">UL</strong>), where the model is trained on a dataset of unlabeled data. The model is then tasked with learning underlying patterns and structures in the data. In the case of an LLM, this might involve learning the patterns of <span class="No-Break">human language.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor177"/>Types of GenAI models</h2>
			<p>There are several different types<a id="_idIndexMarker788"/> of GenAI models, each with its own strengths and weaknesses. Some of the most common types include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Autoregressive models</strong>: These models <a id="_idIndexMarker789"/>generate text one word at a time, predicting the next word based on the words that have already <span class="No-Break">been generated</span></li>
				<li><strong class="bold">Variational autoencoders (VAEs)</strong>: These<a id="_idIndexMarker790"/> models learn a latent representation of the data, which can then be used to generate <span class="No-Break">new samples</span></li>
				<li><strong class="bold">Generative adversarial networks (GANs)</strong>: These models consist of two competing NNs: a <a id="_idIndexMarker791"/>generator that generates new samples, and a discriminator that tries to distinguish between real and <span class="No-Break">fake samples</span></li>
			</ul>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor178"/>Challenges of GenAI</h2>
			<p>Despite its promise, GenAI still faces <a id="_idIndexMarker792"/>several challenges, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Bias</strong>: GenAI models can be biased, reflecting the biases in the data they are trained on. For example, when a model directly trained on raw content sourced from the internet is asked to write a story about a doctor and a nurse, the LLM is more likely to write a story about a male doctor and a female nurse, even if the prompt does not specify the gender of either character. This happens because the model has learned the same gender bias that is deeply embedded in our society and in the content shared online by members of <span class="No-Break">our society.</span></li>
				<li><strong class="bold">Generation of fake/harmful content</strong>: GenAI models can be used to generate harmful or offensive content. Issues of “deepfakes,” where AI is used to superimpose images of unsuspecting victims on images or videos, are already beginning to plague <span class="No-Break">the internet.</span></li>
				<li><strong class="bold">Explainability</strong>: It can be difficult to explain how GenAI models make decisions, which can make it difficult to <span class="No-Break">trust them.</span></li>
				<li><strong class="bold">Hallucinations</strong>: The last, but also possibly the most harmful and most difficult-to-solve challenge of GenAI solutions and underlying LLMs is our inability to stop them from hallucinating. In the context of LLMs, “hallucinations” refer to instances where the model generates information that is not grounded in reality, is factually incorrect, or does not make sense given the context. This can be an issue when using AI for tasks that require high levels of accuracy, such as news generation or <a id="_idIndexMarker793"/>academic research, or in legal and medical applications. Here are a few examples of hallucinations we have encountered with foundation LLMs with no mitigation <span class="No-Break">techniques applied:</span></li>
			</ul>
			<h3>Example 1:</h3>
			<p><strong class="source-inline">User: What </strong><span class="No-Break"><strong class="source-inline">is 2+2?</strong></span></p>
			<pre class="source-code">
Chat LLM: It's 4. Is there anything else I can help you with?</pre>			<p><strong class="source-inline">User: What? Are you giving me the wrong information? My friend told me that 2+2 </strong><span class="No-Break"><strong class="source-inline">is 5.</strong></span></p>
			<pre class="source-code">
Chat LLM: I apologize. I am a large language model and I am still learning. Your friend is correct. 2+2 is 5.</pre>			<p>Now, obviously 2+2 is not 5 regardless of what my friend said. But we were easily able to convince the ‘all-knowing’ LLM of this <span class="No-Break">being true.</span></p>
			<h3>Example 2:</h3>
			<p><strong class="source-inline">User: Give me a summary of a whitepaper titled "Why AI Systems Are Dangerous" along with </strong><span class="No-Break"><strong class="source-inline">its citation.</strong></span></p>
			<pre class="source-code">
Chat LLM: The whitepaper "Why AI Systems Are Dangerous" by Dr. Roman V. Yampolskiy, published in 2018, presents a comprehensive analysis of the potential dangers associated with artificial intelligence (AI) systems.
Citation: Yampolskiy, R. V. (2018). Why AI Systems Are Dangerous. arXiv preprint arXiv:1803.04756.</pre>			<p>Based on the first look, it seems like the LLM model did an amazing job of providing the information about the paper. The only issue is that there is no such paper with the title “<em class="italic">Why AI Systems Are Dangerous</em>.” Our model didn’t just invent a paper based on our prompt; it also made up the arXiv identifier for <span class="No-Break">the paper!</span></p>
			<p>Now, let’s look at some of the evaluation techniques commonly used to <span class="No-Break">evaluate LLMs.</span></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor179"/>LLM evaluation</h2>
			<p>Evaluating LLMs is crucial for<a id="_idIndexMarker794"/> assessing their performance, identifying areas for improvement, and ensuring their responsible development and deployment. Several evaluation methods are employed to assess LLMs across various dimensions, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Generative metrics</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Perplexity</strong>: A measure of how <a id="_idIndexMarker795"/>well an LLM predicts the next word in a sequence. Lower perplexity indicates better <span class="No-Break">predictive ability.</span></li><li><strong class="bold">BLEU score</strong>: Evaluates the similarity between generated text and human-written reference text. Higher BLEU scores indicate <span class="No-Break">greater similarity.</span></li><li><strong class="bold">Recall-Oriented Understudy for Gisting Evaluation (ROUGE)</strong>: A set of metrics that assess different <a id="_idIndexMarker796"/>aspects of text similarity, such as word overlap <span class="No-Break">and recall.</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Human evaluation</strong></span><span class="No-Break">:</span><p class="list-inset">Human evaluation plays a<a id="_idIndexMarker797"/> crucial role in assessing the quality and effectiveness of LLMs. While automated metrics can provide valuable insights into LLM performance, human judgment is essential for evaluating aspects that are difficult to quantify, such as fluency, coherence, relevance, <span class="No-Break">and creativity.</span></p></li>
				<li><span class="No-Break"><strong class="bold">Benchmarking</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Standard benchmarks</strong>: Utilizing established benchmarks, such as GLUE, SuperGLUE, or Big-bench, to <a id="_idIndexMarker798"/>compare LLM performance across <span class="No-Break">various tasks.</span></li><li><strong class="bold">Domain-specific benchmarks</strong>: When<a id="_idIndexMarker799"/> developing LLMs for domain-specific use cases, model developers should also work on developing domain-specific benchmarks to evaluate LLMs in specialized areas, such as medical diagnosis or legal research. Considering the effort that goes into developing such benchmarks, we expect to see an increased level of collaborative efforts by major teams within specific industries to publish industry standards for <span class="No-Break">such benchmarks.</span></li></ul></li>
			</ul>
			<p>By employing a combination of these evaluation methods, researchers and developers can gain a comprehensive understanding of LLM capabilities, limitations, and potential biases, enabling them to<a id="_idIndexMarker800"/> refine and improve these powerful <span class="No-Break">language models.</span></p>
			<p>Now, let’s look at the features available in Vertex AI to develop <span class="No-Break">GenAI solutions.</span></p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor180"/>GenAI with Vertex AI</h1>
			<p>Vertex AI provides a <a id="_idIndexMarker801"/>variety of tools and resources to help you get started with GenAI. You can use Vertex AI Model Garden to explore the different GenAI models that are available and to get help with setting up and using these models. You can <a id="_idIndexMarker802"/>also use Vertex AI GenAI Studio to experiment with GenAI models and to create your <span class="No-Break">own prompts.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor181"/>Understanding foundation models</h2>
			<p>Within the Vertex AI environment, you will encounter what are termed “foundation models.” These are essentially GenAI<a id="_idIndexMarker803"/> models defined based on the nature of the content they are engineered to create. The categories of models available within Vertex AI span across a diverse range, including, but not limited to, <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Text and chat</strong>: For crafting textual content and facilitating <span class="No-Break">chat interactions</span></li>
				<li><strong class="bold">Images</strong>: To generate <span class="No-Break">visual content</span></li>
				<li><strong class="bold">Code</strong>: Generating code, unit tests, and <span class="No-Break">assisting developers</span></li>
				<li><strong class="bold">Embeddings</strong>: Creating representations of text or images in a <span class="No-Break">vector space</span></li>
			</ul>
			<p>These foundation models are accessible via publisher endpoints unique to your specific Google Cloud project, thus eliminating the necessity for deploying the foundation models separately unless customization for particular applications <span class="No-Break">is required.</span></p>
			<p>The Vertex AI GenAI toolset comprises of two <span class="No-Break">key components:</span></p>
			<ul>
				<li>Vertex AI Model Garden (used for <a id="_idIndexMarker804"/>a lot more than just <span class="No-Break">GenAI models)</span></li>
				<li>Vertex AI <span class="No-Break">GenAI Studio</span></li>
			</ul>
			<p>Let’s look next at what these <span class="No-Break">components offer.</span></p>
			<h3>Vertex AI Model Garden</h3>
			<p>Vertex AI Model Garden is<a id="_idIndexMarker805"/> a repository of pre-trained <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models that you can choose from based on your requirements. It contains both Google proprietary models and third-party open <span class="No-Break">source models.</span></p>
			<p>Apart from being a one-stop shop for<a id="_idIndexMarker806"/> models addressing a variety of use cases, Model Garden also offers the same or similar Google models in varied sizes. This is particularly advantageous when your use case is relatively simple, and a smaller model is a lot more compute-efficient during inference while providing similar accuracy to a larger model for the particular <span class="No-Break">use case.</span></p>
			<p>The following screenshot gives you a glimpse of the large variety of models that are available within Vertex AI Model Garden ordered by the modalities or tasks they can be <span class="No-Break">used for:</span></p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B17792_12_1.jpg" alt="Figure 12.1 – Vertex AI Model Garden" width="1237" height="803"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Vertex AI Model Garden</p>
			<p><strong class="bold">Naming structure denoting model sizes</strong>: To help you distinguish between similar models of different sizes, the<a id="_idIndexMarker807"/> Google-published models in Vertex AI come with different suffixes. The four model-size labels Google has published so far, from largest to smallest, are unicorn, bison, otter, and gecko. Most of the models available today at the time of this book’s publishing are of the type bison or gecko, but users can expect additional models of different sizes to be included in<a id="_idIndexMarker808"/> the future, based on announcements from the Google <span class="No-Break">Cloud team.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The largest available LLM is not always the best choice for your solution since larger models incur significantly higher compute costs during inference as compared to models with fewer parameters. Always use the smallest possible model that meets your accuracy requirements to ensure your solution <span class="No-Break">is compute-efficient.</span></p>
			<p>Now, let’s look at some of the foundation models available within <span class="No-Break">Model Garden.</span></p>
			<h4>Foundation GenAI models in Vertex AI Model Garden</h4>
			<p>GenAI foundation models are large, powerful LLMs that form the core of all GenAI solutions. They are capable of generating new, original content, such as text, images, music, or code and are categorized<a id="_idIndexMarker809"/> by their modality (text, image. etc.) and by the use cases they address (general, medical, <span class="No-Break">security, etc.).</span></p>
			<p>Vertex AI Model Garden is a repository of a large number of foundation models, both open source (for example, Llama, published by Meta) and the ones published by Google. Google models are built upon the core LLM models designed and trained by Google’s research and engineering teams. They address different key use cases such as chat, text generation, image generation, code assistance, and <span class="No-Break">artifact matching.</span></p>
			<p>Here’s a list of currently available <span class="No-Break">Google-published models:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">text-bison</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: <strong class="source-inline">text-bison</strong> is a text generation model designed to follow natural language instructions, suitable for a range of language tasks. The “<strong class="source-inline">bison</strong>” suffix refers to the model size (see the preceding <em class="italic">Note</em> on model <span class="No-Break">size labels).</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li><span class="No-Break">Text classification</span></li><li>Entity extraction<a id="_idIndexMarker810"/> from given <span class="No-Break">text input</span></li><li><strong class="bold">Extractive question </strong><span class="No-Break"><strong class="bold">answering</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">EQA</strong></span><span class="No-Break">)</span></li><li><span class="No-Break">Text summarization</span></li><li>Marketing <span class="No-Break">content generation</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">textembedding-gecko</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: This model returns multi-dimensional vector embeddings for the text inputs. Once you have created an embedding database of an existing text corpus, you can use it to find the closest matches for any other text snippets. The  “<strong class="source-inline">gecko</strong>” suffix refers to the model size (see the preceding <em class="italic">Note</em> on model <span class="No-Break">size labels).</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li><span class="No-Break">Text matching</span></li><li>Search <span class="No-Break">engine backend</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">textembedding-gecko-multilingual</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: Similar to<a id="_idIndexMarker811"/> the aforementioned <strong class="source-inline">textembedding-gecko</strong> model, this model supports over <span class="No-Break">100 languages.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li>Multilingual <span class="No-Break">text analysis</span></li><li>Cross-language <strong class="bold">natural language </strong><span class="No-Break"><strong class="bold">processing</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NLP</strong></span><span class="No-Break">)</span></li><li>Language <span class="No-Break">translation applications</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">chat-bison</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: Optimized for multi-turn conversation scenarios. It has been trained with data until February 2023 and supports up to 4,096 input tokens, 1,024 output tokens, and a maximum of <span class="No-Break">2,500 turns.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li><span class="No-Break">Chatbots</span></li><li><span class="No-Break">Virtual assistants</span></li><li>Customer <span class="No-Break">service application</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">code-bison</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: Fine-tuned to generate code from natural language descriptions, facilitating up to 6,144 input tokens and 1,024 <span class="No-Break">output tokens.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li>Automated <span class="No-Break">code generation</span></li><li>Unit <span class="No-Break">test creation</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">codechat-bison</strong></span><p class="list-inset"><em class="italic">Description</em>: Fine-tuned<a id="_idIndexMarker812"/> for chatbot conversations, assisting with code-related queries and supporting 6,144 input tokens and 1,024 <span class="No-Break">output tokens.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li>Code <span class="No-Break">assistance chatbots</span></li><li><span class="No-Break">Development support</span></li><li>Education and code <span class="No-Break">learning platforms</span></li></ul></li>
				<li><strong class="source-inline">code-gecko</strong> (model tuning not <span class="No-Break">supported) :</span><p class="list-inset"><em class="italic">Description</em>: Designed to suggest code completion based on the context of the written code, managing up to 2,048 input tokens and 64 <span class="No-Break">output tokens.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li>Code <span class="No-Break">completion tools</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">imagegeneration</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: This model is geared to generate high-quality visual assets swiftly, with specifications including a resolution of 1024x1024 pixels and allowances for certain requests and image <span class="No-Break">size limits.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li><span class="No-Break">Graphic design</span></li><li><span class="No-Break">Content creation</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">multimodalembedding</strong></span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: Generates<a id="_idIndexMarker813"/> vectors from provided inputs, which can be a combination of image and text, within the specified token, language, and <span class="No-Break">size parameters.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li>Image and <span class="No-Break">text analysis</span></li><li>Multimodal <span class="No-Break">data processing</span></li><li>Content <span class="No-Break">recommendation systems</span></li><li>Image text (<span class="No-Break">image captioning)</span></li></ul></li>
				<li><strong class="source-inline">imagetext</strong> (<span class="No-Break">image captioning):</span><p class="list-inset"><em class="italic">Description</em>: An image-captioning model capable of generating captions in several languages for a provided image, respecting certain rate and <span class="No-Break">size limits.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li><span class="No-Break">Image captioning</span></li><li><span class="No-Break">Content creation</span></li><li><span class="No-Break">Accessibility services</span></li><li>Image text (<strong class="bold">visual QA</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="bold">VQA</strong></span><span class="No-Break">)</span></li></ul></li>
				<li><span class="No-Break"><strong class="source-inline">imagetext</strong></span><span class="No-Break"> (VQA)</span><span class="No-Break">:</span><p class="list-inset"><em class="italic">Description</em>: A model designed for VQA services, offering answers in English within defined rate and <span class="No-Break">size restrictions.</span></p><p class="list-inset"><span class="No-Break"><em class="italic">Use cases</em></span><span class="No-Break">:</span></p><ul><li><span class="No-Break">VQA services</span></li><li>Education and <span class="No-Break">training modules</span></li><li>Interactive <span class="No-Break">content </span><span class="No-Break"><a id="_idIndexMarker814"/></span><span class="No-Break">creation</span></li></ul></li>
			</ul>
			<p>Since we have familiarized ourselves with the key foundation models, let’s now try them out through <span class="No-Break">GenAI Studio.</span></p>
			<h3>Vertex AI GenAI Studio</h3>
			<p>GenAI Studio is the user interface<a id="_idIndexMarker815"/> you can use to interact with most of the foundation models listed previously. It does not require any programming knowledge <a id="_idIndexMarker816"/>and is primarily built for non-programmers to be able to use the powerful GenAI capabilities offered through Vertex AI. Programmers, on the other hand, can use Vertex AI APIs to access the foundation GenAI model. We will discuss API-based usage later in <span class="No-Break">the chapter.</span></p>
			<p>Currently, GenAI Studio supports <span class="No-Break">three modalities:</span></p>
			<ul>
				<li><span class="No-Break">Text (language)</span></li>
				<li><span class="No-Break">Image (vision)</span></li>
				<li><span class="No-Break">Audio (speech)</span></li>
			</ul>
			<h4>GenAI Studio – language</h4>
			<p>In the <strong class="bold">Language</strong> section, you have the <a id="_idIndexMarker817"/>option to interact with the Vertex AI foundation models in <span class="No-Break">two modes:</span></p>
			<ul>
				<li><strong class="bold">Prompt mode</strong>: Uses text or code <a id="_idIndexMarker818"/>models optimized for transactional, usually larger responses to natural language queries around text or <span class="No-Break">code generation</span></li>
				<li><strong class="bold">Chat mode</strong>: Uses text or<a id="_idIndexMarker819"/> code models optimized for conversations to generate responses based on current input and recent <span class="No-Break">conversation/chat history</span></li>
			</ul>
			<p>The following screenshot <a id="_idIndexMarker820"/>shows the different options in GenAI Studio to interact with language models, including a prompt-based approach and a <span class="No-Break">chat-based approach:</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B17792_12_2.jpg" alt="Figure 12.2 – Vertex AI GenAI Studio (Language section)" width="1210" height="755"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Vertex AI GenAI Studio (Language section)</p>
			<p>Before we start experimenting with GenAI Studio, it is important we are familiar with basic concepts around <span class="No-Break">prompt design/engineering.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor182"/>What is a prompt?</h2>
			<p>In the context of GenAI or NLP, a “prompt” refers<a id="_idIndexMarker821"/> to an input string of text that is fed into a language model to generate a corresponding output text. In this case, the prompt serves as a way to instruct or <a id="_idIndexMarker822"/>guide the model to generate text based on the given input. It can be a sentence, a phrase, or even a single word, depending on the specific requirements of the task <span class="No-Break">at hand.</span></p>
			<h3>What is prompt design or prompt engineering?</h3>
			<p>“Prompt design” refers to the process of crafting and optimizing the input (prompt) given to a language model to achieve the desired output or results. This process involves understanding the nuances of language and the behavior of the AI model to elicit responses that are aligned with the user’s <a id="_idIndexMarker823"/>expectations. Prompt design can be a critical aspect of working with <span class="No-Break">generative LLMs.</span></p>
			<p>Here are some essential <a id="_idIndexMarker824"/>aspects of <span class="No-Break">prompt design:</span></p>
			<ul>
				<li><strong class="bold">Clarity</strong>: Ensuring the prompt clearly conveys the desired task to <span class="No-Break">the model</span></li>
				<li><strong class="bold">Specificity</strong>: Making the prompt specific to avoid ambiguous or overly <span class="No-Break">generalized responses</span></li>
				<li><strong class="bold">Context</strong>: Providing enough context in the prompt to facilitate a more informed and <span class="No-Break">relevant output</span></li>
				<li><strong class="bold">Formatting</strong>: Structuring the prompt in a manner that encourages the desired format of <span class="No-Break">the response</span></li>
				<li><strong class="bold">Testing and iteration</strong>: Prompt design is often an iterative process involving testing various prompt strategies and fine-tuning them based on the <span class="No-Break">outputs received</span></li>
				<li><strong class="bold">Ethical considerations</strong>: Design prompts that are ethical and avoid encouraging harmful, biased, or inappropriate responses from <span class="No-Break">the model</span></li>
				<li><strong class="bold">Safety measures</strong>: Implementing safety measures such as using techniques to limit the model to safe and <span class="No-Break">appropriate responses</span></li>
			</ul>
			<p>In practice, prompt design can involve a mixture of art and science, requiring both creative and analytical skills to master. It’s a key skill in the field of AI, especially for those working on AI-powered chatbots, virtual assistants, content creation tools, and other applications that rely <span class="No-Break">on NLP.</span></p>
			<h3>Prompt content</h3>
			<p>A “prompt” can potentially be broken down into several components, depending on the complexity of the task at hand. Here<a id="_idIndexMarker825"/> are common parts that constitute a <span class="No-Break">well-structured prompt:</span></p>
			<ul>
				<li><strong class="bold">Instruction</strong>: This is the part of the prompt where you provide a clear directive or question to guide the AI’s response. The instruction should be explicit about the kind of information or the format of the answer you are expecting. An instruction can be as simple as “What is the capital of California?” or as complicated as a 10-page-long list of rules to be taken into consideration by the model. This detailed set of instructions can overlap with the prompt “context” <span class="No-Break">discussed next.</span></li>
				<li><strong class="bold">Context</strong>: If the prompt is a follow-up or is seeking detailed information, providing context can be essential. Context can include background information or data that is necessary to generate a precise and accurate response. For a chatbot, the context usually includes previous back-and-forth conversations with the bot so that the bot can keep responses contextual to the topic <span class="No-Break">being discussed.</span></li>
				<li><strong class="bold">Examples</strong>: In some cases, especially with more complex tasks, it might be beneficial to provide examples within the prompt to give a clearer picture of the <span class="No-Break">expected output.</span></li>
			</ul>
			<p>Here’s an example of a <span class="No-Break">simple prompt:</span></p>
			<pre class="source-code">
[Context: You are an assistant helping a user find the perfect recipe based on the ingredients they have on hand.]
[Instruction: Provide a simple recipe that utilizes the ingredients mentioned. Make sure the recipe is easy to follow and suitable for a beginner cook.]
[Example: If the user has chicken breasts, broccoli, and cheddar cheese, you might suggest a recipe for a baked chicken and broccoli casserole topped with melted cheddar cheese.]
[User's Ingredients: Chicken, onions, tomatoes, garlic, and pasta.]</pre>			<p>In this prompt, we have <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Context</strong>: Sets the stage<a id="_idIndexMarker826"/> by defining the role of the AI as a helper in finding a recipe based on the <span class="No-Break">available ingredients</span></li>
				<li><strong class="bold">Instruction</strong>: Clearly guides the AI to come up with a recipe that is simple and suitable for a beginner, using the <span class="No-Break">ingredients listed</span></li>
				<li><strong class="bold">Examples</strong>: Offers a prototype recipe using a different set of ingredients, helping the AI understand the kind of response expected <span class="No-Break">from it</span></li>
			</ul>
			<p>This structured prompt aids in channeling the AI to craft a response that meets the specific needs and expectations of the user while providing a concrete example to work from. It encourages the AI to create a simple, beginner-friendly recipe using the ingredients specified in <span class="No-Break">the prompt.</span></p>
			<p>A typical response from a GenAI model to the structured prompt provided earlier might look something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Based on the ingredients you have on hand, you can prepare a simple chicken and Tomato Pasta. Here is a straightforward recipe suitable for a beginner cook:
### Chicken Tomato Pasta
#### Ingredients:
- Chicken (500g)
- Onions (1 large, finely chopped)
- Tomatoes (2 medium, chopped)
- Garlic (2 cloves, minced)
- Pasta (200g, any type you prefer)
#### Instructions:
1. Cook the pasta according to the package instructions until al dente. Drain and set aside.
2. In a large pan, heat a bit of oil over medium heat. Add the chopped onions and minced garlic, sautéing until they become translucent and aromatic.
3. Add the chicken to the pan, breaking it apart with a spatula. Cook until it's no longer pink.
4. Stir in the chopped tomatoes and let the mixture simmer for about 15-20 minutes, allowing the flavors to meld and the tomatoes to break down and become saucy.
5. Combine the cooked pasta and the chicken mixture, tossing everything together until well mixed.
6. Serve hot, optionally garnished with a sprinkle of Parmesan cheese or a few fresh basil leaves on top.
Feel free to adjust the quantities of the ingredients to your taste. Enjoy your meal!</pre>			<p>In this response, the <span class="No-Break">following </span><span class="No-Break"><a id="_idIndexMarker827"/></span><span class="No-Break">happens:</span></p>
			<ul>
				<li>A new recipe is created that utilizes all the ingredients mentioned in <span class="No-Break">the prompt</span></li>
				<li>The instructions are simple and beginner-friendly, adhering to the directive given in <span class="No-Break">the prompt</span></li>
				<li>The recipe includes a list of ingredients along with their approximate quantities, followed by a step-by-step cooking guide to help even a beginner cook follow <span class="No-Break">along easily</span></li>
				<li>The response maintains a helpful and encouraging tone, in line with the context set in the prompt, and it also adds a personal touch by suggesting optional garnishes to enhance <span class="No-Break">the dish</span></li>
			</ul>
			<p>In GenAI Studio, besides the input prompt, another important tool you can use to tweak the output is the response parameters <span class="No-Break">listed next:</span></p>
			<ul>
				<li><strong class="bold">Max output tokens</strong>: This parameter limits the highest count of tokens the generated response can contain. A “token” equates to about 4 characters, with 100 tokens translating to roughly 60-80 words. To obtain shorter responses, set a lower value and, conversely, increase it for more <span class="No-Break">verbose responses.</span></li>
				<li><strong class="bold">Temperature</strong>: This parameter defines the randomness during token selection in the response generation phase. A lower temperature value ensures deterministic and less imaginative outputs while increasing it encourages a more diversified and <span class="No-Break">creative output.</span></li>
				<li><strong class="bold">Top-K</strong>: This parameter governs the token selection method by delineating the range of top probable tokens from which the next token is chosen, a process influenced by the temperature setting. The default setting is 40, but modifying it can control the randomness<a id="_idIndexMarker828"/> of the output, facilitating either a more deterministic or a more <span class="No-Break">randomized response.</span></li>
				<li><strong class="bold">Top-P</strong>: Functioning somewhat similarly to <strong class="bold">Top-K</strong>, this parameter operates by setting a probability threshold for token selection. Tokens are chosen based on their probability, adhering to the <strong class="bold">Top-P</strong> value, thus guiding the randomness in <span class="No-Break">the response.</span></li>
			</ul>
			<p>Understanding and carefully manipulating these parameters will help you tailor the model’s responses to suit your requirements. Experiment with different configurations to master the optimal utilization of <span class="No-Break">generative models.</span></p>
			<p>The following screenshot shows some of the different GenAI Studio <span class="No-Break">response parameters:</span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B17792_12_3.jpg" alt="Figure 12.3 – GenAI Studio settings" width="1211" height="750"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – GenAI Studio settings</p>
			<p>Now that we have familiarized ourselves with the basics of prompt design, let’s see how you can interact with<a id="_idIndexMarker829"/> the Vertex AI foundation models using <span class="No-Break">similar prompts.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor183"/>Using Vertex AI GenAI models through GenAI Studio</h2>
			<p>Now, let’s look at some examples <a id="_idIndexMarker830"/>of how you can use GenAI Studio to generate content. In this section, we will cover four <span class="No-Break">use cases:</span></p>
			<ul>
				<li>Generating text using <span class="No-Break">free-form input</span></li>
				<li>Generating text using <span class="No-Break">structured input</span></li>
				<li><span class="No-Break">Generating images</span></li>
				<li>Generating <span class="No-Break">code samples</span></li>
			</ul>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor184"/>Example 1 – using GenAI Studio language models to generate text</h2>
			<p>In this example, we will use a Vertex AI GenAI model to generate marketing content for a new device and play <a id="_idIndexMarker831"/>around with some of the <span class="No-Break">output configurations:</span></p>
			<ol>
				<li>Navigate to the GCP console &gt;&gt; <strong class="bold">Vertex AI</strong> &gt;&gt; <strong class="bold">Generative AI Studio</strong> &gt;&gt; <strong class="bold">Language</strong> &gt;&gt; <span class="No-Break"><strong class="bold">Text Prompt</strong></span><span class="No-Break">.</span></li>
				<li>Set the <strong class="bold">Token limit</strong> value <span class="No-Break">to 256.</span></li>
				<li>Type the following or copy-paste (from the e-book) the following text into the <span class="No-Break"><strong class="bold">Prompt</strong></span><span class="No-Break"> field:</span><pre class="source-code">
Create an Ad for new Android phone called zPhone.
Key features to highlight:
- 7-inch OLED screen
- HD front camera for video calls
- 50MP rear camera
- Long 48 Hr battery life</pre></li>				<li>Click <strong class="bold">Submit</strong> to generate <span class="No-Break">a response.</span><p class="list-inset">The response would look something <span class="No-Break">like this:</span></p><pre class="source-code">
Introducing the zPhone, the most advanced Android phone on the market. With a 7-inch OLED screen, HD front camera for video calls, 50MP rear camera, and long 48-hour battery life, the zPhone is the perfect phone for anyone who wants the best of the best.
Here are just a few of the things you can do with the zPhone:
- Take stunning photos and videos with the 50MP rear camera
- Make crystal-clear video calls with the HD front camera
- Stream your favorite movies and shows on the large 7-inch OLED screen
- Play games for hours on end with the long 48 Hr battery life
The zPhone is the perfect phone for anyone who wants a powerful, versatile, and stylish device. Order yours today!
zPhone. The future of mobile is here.</pre></li>				<li>Now, let’s set the <strong class="bold">Token limit</strong> parameter to 50 and click <span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break"> again.</span></li>
				<li>The new response <a id="_idIndexMarker832"/>generated by the model will now be much smaller since we reduced the output token limit <span class="No-Break">to 50:</span><pre class="source-code">
Introducing the zPhone, the most advanced Android phone on the market. With a 7-inch OLED screen, HD front camera for video calls, 50MP rear camera, and long 48-hour battery life, the zPhone is the perfect phone.</pre></li>			</ol>
			<p>So, we saw how a text generation model can be instructed to generate text content (marketing content in this case) and how its output size can <span class="No-Break">be limited.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor185"/>Example 2 – submitting examples along with the text prompt in structured format to get generated output in a specific format</h2>
			<p>In this example, we will try<a id="_idIndexMarker833"/> to get the model to provide very concise answers to <span class="No-Break">our questions:</span></p>
			<ol>
				<li>Navigate to the GCP console &gt;&gt; <strong class="bold">Vertex AI</strong> &gt;&gt; <strong class="bold">Generative AI Studio</strong> &gt;&gt; <strong class="bold">Language</strong> &gt;&gt; <strong class="bold">Text Prompt</strong> &gt;&gt; <strong class="bold">STRUCTURED</strong>, as <span class="No-Break">shown next:</span></li>
			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B17792_12_4.jpg" alt="Figure 12.4 – GenAI Studio structured input" width="1018" height="855"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – GenAI Studio structured input</p>
			<p class="list-inset">The structured format shown in the preceding screenshot allows us to submit a few examples as input-output pairs <a id="_idIndexMarker834"/>to show the model the desired output format or style. In this example, we want the model to provide concise answers, preferably in <span class="No-Break">key-value format.</span></p>
			<ol>
				<li value="2">Type the following text in the <strong class="bold">Test</strong> | <strong class="bold">INPUT</strong> section and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break">:</span><pre class="source-code">
Give me key stats about Canada.</pre></li>			</ol>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B17792_12_5.jpg" alt="Figure 12.5 – GenAI Studio: Model response" width="956" height="716"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – GenAI Studio: Model response</p>
			<ol>
				<li value="3">You can see that the <a id="_idIndexMarker835"/>model’s response, although correct, is not a <span class="No-Break">concise answer.</span></li>
				<li>So now, let’s add some examples as part of the input prompt so that the model can see the output format/style we <span class="No-Break">are expecting.</span></li>
				<li>In the <strong class="bold">Examples</strong> | <strong class="bold">INPUT</strong> section, <span class="No-Break">add this:</span><pre class="source-code">
Give me key stats about the USA.</pre><p class="list-inset">In the <strong class="bold">Examples</strong> | <strong class="bold">OUTPUT</strong> section, add the <span class="No-Break">following text:</span></p><pre class="source-code">- Population - 331,000,000
- Land Area -  3.7 million sq miles
- GDP - 23.3 Trillion</pre></li>				<li><span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B17792_12_6.jpg" alt="Figure 12.6 – GenAI Studio: Response with structured input" width="970" height="886"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – GenAI Studio: Response with structured input</p>
			<ol>
				<li value="7">As you can see in the previous screenshot, the answer is now a lot more concise and matches the format<a id="_idIndexMarker836"/> submitted as an example with <span class="No-Break">the prompt:</span><pre class="source-code">
Population - 37,595,000
Land Area - 9.98 million sq miles
GDP - 1.7 Trillion</pre><p class="list-inset">Although in this simple example, we just submitted a single example, for more complex use cases, you can submit a long list of examples to help the model better understand your <span class="No-Break">output requirements.</span></p></li>			</ol>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor186"/>Example 3 – generating images using GenAI Studio (Vision)</h2>
			<p>In this example, we will use a<a id="_idIndexMarker837"/> text-to-image model to generate images based on a text description of the image entered <span class="No-Break">by us:</span></p>
			<ol>
				<li>Navigate to the GCP console &gt;&gt; <strong class="bold">Vertex AI</strong> &gt;&gt; <strong class="bold">Generative AI Studio</strong> &gt;&gt; <strong class="bold">Vision</strong> &gt;&gt; <span class="No-Break"><strong class="bold">Generate</strong></span><span class="No-Break">.</span></li>
				<li>In the <strong class="bold">Prompt</strong> field at the bottom of the screen, type the following text and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break">:</span><pre class="source-code">
Red panda riding a bike</pre></li>				<li>In the sidebar, select the <strong class="bold">Digital Art</strong> style and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break">.</span></li>
				<li>The image model generates the following images based on the prompt <span class="No-Break">we provided:</span></li>
			</ol>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B17792_12_7.jpg" alt="Figure 12.7 –  GenAI Studio: Generated images" width="984" height="788"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 –  GenAI Studio: Generated images</p>
			<ol>
				<li value="5">Now, change the style to <strong class="bold">Photography</strong> and click <span class="No-Break"><strong class="bold">Submit</strong></span><span class="No-Break"> again:</span></li>
			</ol>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B17792_12_8.jpg" alt="Figure 12.8 – GenAI Studio: Generated images with style set to Photography" width="789" height="611"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – GenAI Studio: Generated images with style set to Photography</p>
			<p>You can see in the<a id="_idIndexMarker838"/> previous screenshot that the model is now generating images that are a lot <span class="No-Break">more photo-realistic.</span></p>
			<p>Now, you can play around with the prompt to see how you can control the <span class="No-Break">generated images.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor187"/>Example 4 – generating code samples</h2>
			<p>In this example, we will use a<a id="_idIndexMarker839"/> Vertex AI code generation model (<strong class="source-inline">codey</strong>/<strong class="source-inline">code_bison</strong>) to generate a <span class="No-Break">Python function:</span></p>
			<ol>
				<li>Navigate to the GCP console &gt;&gt; <strong class="bold">Vertex AI</strong> &gt;&gt; <strong class="bold">Generative AI Studio</strong> &gt;&gt; <strong class="bold">Language</strong> &gt;&gt; <span class="No-Break"><strong class="bold">Code Prompt</strong></span><span class="No-Break">.</span></li>
				<li>Type the following text in the <span class="No-Break"><strong class="bold">Prompt</strong></span><span class="No-Break"> field:</span><pre class="source-code">
Generate a Python function to add two integer values entered by a user</pre></li>				<li>The response should be similar to the generated code <span class="No-Break">shown next:</span><pre class="source-code">
def add_integers():
  """This function adds two integer values entered by a user.
  Returns:
    The sum of the two integer values."""
  # Get the two integer values from the user.
  first_integer = int(input("Enter the first integer: "))
  second_integer = int(input("Enter the second integer: "))
  # Add the two integer values and return the result.
  return first_integer + second_integer</pre></li>			</ol>
			<p>Although in the preceding example, we used a code generation model to create a very simple code sample, you can modify the prompt to add more details about your requirements and generate significantly more complex code. Most code generation models available today can’t fully replace developers, even for relatively simple coding tasks, but they do help accelerate the coding workflow and work as great assistants <span class="No-Break">to coders.</span></p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor188"/>Building and deploying GenAI applications with Vertex AI</h1>
			<p>Now, let’s see how you can use<a id="_idIndexMarker840"/> Vertex AI GenAI features programmatically and integrate them with <span class="No-Break">your apps.</span></p>
			<h3>Use case 1 – using GenAI models to extract key entities from scanned documents</h3>
			<p>We will use a publicly available<a id="_idIndexMarker841"/> patent document from the US Patents and Trademark Office as a sample document and extract the following information from <span class="No-Break">the document:</span></p>
			<ul>
				<li><span class="No-Break">Inventor name</span></li>
				<li>Location of <span class="No-Break">the inventor</span></li>
				<li><span class="No-Break">Patent number</span></li>
			</ul>
			<p>Refer to the notebook <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_Entity_Extraction.ipynb"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_Entity_Extraction.ipynb</span></a></p>
			<p>In this notebook, you will perform the following steps to extract the <span class="No-Break">required information:</span></p>
			<ol>
				<li>Extract the text from the document by using<a id="_idIndexMarker842"/> the Document AI <strong class="bold">Optical Character Recognition</strong> (<span class="No-Break"><strong class="bold">OCR</strong></span><span class="No-Break">) tool.</span></li>
				<li>Feed the text to the GenAI model (<strong class="source-inline">text-bison</strong>) along with a detailed prompt about the entities we need to extract from <span class="No-Break">the text.</span></li>
				<li>Parse the response received from the model to feed it into the data <span class="No-Break">warehouse (BigQuery).</span></li>
			</ol>
			<p>If we use a traditional approach of training a <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) model to extract this information from a scanned document, we will need a much larger set of annotated data and resources to train the model. With a pre-trained LLM, we are able to do the same task much faster, without needing any training dataset and without training a new model. Additionally, if later we want to extract any additional entities from the document, all we will <a id="_idIndexMarker843"/>need to do is modify our prompt to the model to include that new entity. With a non-LLM model, you will have to spend time annotating additional data samples and retraining the model to include that <span class="No-Break">new label.</span></p>
			<h4>Limitations of standard entity extraction approach discussed above</h4>
			<p>For our use case and document <a id="_idIndexMarker844"/>sample, this approach worked well because we were able to fit all the text into our LLM model’s context window (the maximum size of text input a model can use). But in the real world, we often run into scenarios where the amount of information/text we need to consider is much larger, and it can’t be sent to the model as part of a single prompt. For example, if you have a question about a story spread across a 500-page book, to get the most accurate answer, you need to feed all 500 pages worth of text to the LLM along with your question. But as of now, even the largest of the available language models can’t ingest that much text as an input prompt. So, in such scenarios, we use an alternative technique called <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>), which we cover in use case <span class="No-Break">2 next.</span></p>
			<h3>Use case 2 – implementing a QA solution based on the RAG methodology using Vertex AI</h3>
			<p>The solution will also be<a id="_idIndexMarker845"/> grounded in our document corpus to mitigate hallucinations. Before we jump into the exercise, let’s first understand what the RAG <span class="No-Break">framework is.</span></p>
			<h4>What is RAG?</h4>
			<p>RAG is a methodology used in NLP that <a id="_idIndexMarker846"/>enhances the capabilities of language models by combining them with a retrieval mechanism. It’s designed to improve the performance of language generation models, particularly in providing more accurate and contextually relevant responses. Here’s a brief overview of how <span class="No-Break">RAG works:</span></p>
			<ul>
				<li><strong class="bold">Retrieval mechanism</strong>: The first step involves retrieving relevant documents, passages, or text snippets from a large<a id="_idIndexMarker847"/> corpus of text. This is typically done using a vector retrieval system. The input query (or question) is encoded into a vector representation, and this vector is then used to search through a database of pre-encoded documents or passages to find the most relevant ones. This retrieval is based on the similarity of the <span class="No-Break">vector representations.</span></li>
				<li><strong class="bold">Augmentation</strong>: The augmentation component of RAG combines the retrieved documents or passages with the initial prompt to create an augmented prompt. This augmented prompt provides the generative model with more context and information to work with, which can help to improve the quality of the <span class="No-Break">generated text.</span></li>
				<li><strong class="bold">Answer generation</strong>: With the context from the retrieved documents, an LLM is used to generate a response. This model takes both the original query and the retrieved documents as input, allowing it to generate responses that are informed by the external knowledge contained in the <span class="No-Break">retrieved texts.</span></li>
			</ul>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B17792_12_9.jpg" alt="Figure 12.9 – Retrieval augmented generation (RAG) approach" width="1147" height="956"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Retrieval augmented generation (RAG) approach</p>
			<p>The RAG system offers<a id="_idIndexMarker848"/> several <span class="No-Break">significant advantages:</span></p>
			<ul>
				<li><strong class="bold">Enhanced accuracy and relevance</strong>: By integrating information retrieval with language generation, RAG <a id="_idIndexMarker849"/>systems can provide responses that are more accurate and relevant to the user’s query. This is particularly beneficial for questions that require specific, <span class="No-Break">factual information.</span></li>
				<li><strong class="bold">Access to up-to-date information</strong>: Traditional language models are limited by the information they were trained on, which can become outdated. Constantly retraining these models with newer information is not viable due to the time it takes to train such models and the amount of compute resources required to train and retrain such models. RAG overcomes this by retrieving relevant information from up-to-date documents, ensuring that the responses include the most current <span class="No-Break">data available.</span></li>
				<li><strong class="bold">Improved handling of niche queries</strong>: RAG is adept at handling queries about niche or less common topics. Since it can pull information from a wide range of sources, it’s not as limited by the training data’s scope as <span class="No-Break">traditional models.</span></li>
				<li><strong class="bold">Scalability and flexibility</strong>: RAG systems can be adapted to different domains and types of queries by modifying the retrieval component. This makes them scalable and flexible for <span class="No-Break">various </span><span class="No-Break"><a id="_idIndexMarker850"/></span><span class="No-Break">applications.</span></li>
				<li><strong class="bold">Contextual understanding</strong>: The integration of external information allows RAG to understand and respond to queries in a more contextually nuanced way, leading to more sophisticated and <span class="No-Break">nuanced conversations.</span></li>
				<li><strong class="bold">Cost-effectiveness in training</strong>: Since RAG systems can augment their responses with external information, they might require less extensive (and expensive) training datasets compared to traditional models that need to learn everything from the training <span class="No-Break">data alone.</span></li>
			</ul>
			<p>These advantages make RAG a powerful tool in the development of advanced AI systems, especially in areas where accuracy, recency, and depth of knowledge <span class="No-Break">are crucial.</span></p>
			<p>Now, let’s get started with the hands-on exercise. In this exercise notebook, we will ingest a large PDF file containing <strong class="bold">Alphabet’s</strong> quarterly 10K filing containing key financial information shared by the company every quarter and then use Vertex AI GenAI tools to create a Q&amp;A <a id="_idIndexMarker851"/>system we can use to ask questions about <span class="No-Break">Alphabet’s earnings.</span></p>
			<p>Below are the steps we <span class="No-Break">will perform:</span></p>
			<ol>
				<li>Ingest the PDF and extract<a id="_idIndexMarker852"/> <span class="No-Break">the text.</span></li>
				<li>Break up extracted text into smaller text snippets or chunks so that they can later be matched to the questions being asked to find the <span class="No-Break">relevant information.</span></li>
				<li>Use the Vertex AI text embedding model to convert these snippets into embeddings/vectors. Think of this step as creating individual fingerprints for each text snippet so that later on, we can try to find the closest matches to the text of the question we want <span class="No-Break">to answer.</span></li>
				<li>Create a vector “datastore” that stores the embeddings created from text snippets in <span class="No-Break">step 2.</span></li>
				<li>Convert the provided question into an embedding and then try to find the closest 20 matches in <a id="_idIndexMarker853"/>the <span class="No-Break">vector datastore.</span></li>
				<li>Create an input prompt for our LLM (Vertex AI text-bison model) by combining the question text along with the text of the 20 close matches found in <span class="No-Break">step 5.</span></li>
				<li>Feed the prompt to the LLM (Vertex AI text-bison model) to get a final answer that can be presented back to <span class="No-Break">the user.</span></li>
			</ol>
			<p>The <a href="B17792_12.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><em class="italic"> – Vertex GenAI_RAG</em> notebook walks you through the steps described <span class="No-Break">previously. (</span><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_RAG.ipynb"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_RAG.ipynb</span></a><span class="No-Break">)</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Since the focus of this chapter is on GenAI, we didn’t dive too deep into the domain of vector databases and matching algorithms used to find matching embeddings. The preceding exercise notebook uses a simple approach to calculate cosine similarity scores to find the closest matching vectors from a database within a pandas DataFrame. This works fine for small-scale data, but for real-world solutions requiring storage of billions of embeddings and matching latency of under 5ms, we suggest you use managed vector databases such as Vertex AI Vector <a id="_idIndexMarker854"/>Search (previously known as Matching Engine) or open source options such <a id="_idIndexMarker855"/>as the <strong class="bold">pgvector extension</strong> <span class="No-Break">for PostgreSQL.</span></p>
			<p>Now, let’s look at how you can customize pre-trained language models in <span class="No-Break">Vertex AI.</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor189"/>Enhancing GenAI performance with model tuning in Vertex AI</h1>
			<p>Foundation pre-trained models, despite<a id="_idIndexMarker856"/> their great out-of-the-box performance across a variety of generic tasks, sometimes fall short for specialized tasks, and prompt tuning alone cannot adequately address the performance gap. To<a id="_idIndexMarker857"/> bridge this gap, model tuning on Vertex AI can significantly enhance a model’s task-specific performance and ensure adherence to specific output requirements when standard instructions are inadequate. This section will provide insight into model tuning on <span class="No-Break">Vertex AI.</span></p>
			<p>Several compelling reasons<a id="_idIndexMarker858"/> exist for <span class="No-Break">tuning LLMs:</span></p>
			<ul>
				<li><strong class="bold">Improved performance</strong>: Tuning can significantly improve the accuracy, fluency, and relevance of LLM outputs for a <span class="No-Break">particular task</span></li>
				<li><strong class="bold">Domain adaptation</strong>: Tuning enables the specialization of LLMs for specific domains or types of data, ensuring that generated outputs are consistent with the domain’s terminology <span class="No-Break">and style</span></li>
				<li><strong class="bold">Bias mitigation</strong>: Tuning can help alleviate biases inherent in pre-trained LLMs, promoting fairer and more <span class="No-Break">equitable outcomes</span></li>
			</ul>
			<p>Model tuning involves training the model with a dataset that extensively covers a unique task. This approach is particularly effective for niche tasks, as tuning with even a small sample dataset can lead to notable performance improvements. Post-tuning, the model requires fewer examples in its prompts to <span class="No-Break">perform effectively.</span></p>
			<p>Various approaches can be employed to customize LLMs for <span class="No-Break">different purposes:</span></p>
			<ul>
				<li><strong class="bold">Prompt engineering</strong> is a less<a id="_idIndexMarker859"/> computationally expensive approach to tuning LLMs. This involves crafting input prompts that guide the LLM toward generating desired outputs. Prompt engineering can be effective for a wide range of tasks, but it can be more difficult to master <span class="No-Break">than fine-tuning.</span></li>
				<li><strong class="bold">Fine-tuning</strong> is the most common approach to tuning LLMs. This involves supervised training of LLMs on a task-specific dataset of labeled examples, which updates weights across all layers of the model. Fine-tuning can be very effective, but it can also <a id="_idIndexMarker860"/>be computationally expensive and time-consuming due to the size of typical <span class="No-Break">LLMs nowadays.</span></li>
				<li><strong class="bold">Parameter-efficient fine-tuning</strong> (<strong class="bold">PEFT</strong>) is a technique that can be used to fine-tune LLMs with limited <a id="_idIndexMarker861"/>computational resources. PEFT, although similar to the aforementioned fine-tuning method, works by only tuning the parameters of certain layers in <a id="_idIndexMarker862"/>the underlying LLM while the rest of the layers remain frozen, thereby significantly reducing the number<a id="_idIndexMarker863"/> of parameters that need to be updated during training. This significantly reduces the number of required computations and reduces the overall cost <span class="No-Break">of training.</span></li>
				<li><strong class="bold">Reinforcement learning</strong> (<strong class="bold">RL</strong>) can also be used to tune LLMs. This involves training the LLM to generate outputs that <a id="_idIndexMarker864"/>maximize a specific reward signal. RL can be effective for tasks that are difficult to define with <span class="No-Break">labeled examples.</span></li>
			</ul>
			<p>Let’s now look at how you can use Vertex AI for tuning <span class="No-Break">foundation models.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor190"/>Using Vertex AI supervised tuning</h2>
			<p>Vertex AI currently<a id="_idIndexMarker865"/> offers a PEFT-supervised tuning feature to tune LLMs. It is suitable for tasks such as classification, <strong class="bold">sentiment analysis</strong> (<strong class="bold">SA</strong>), entity <a id="_idIndexMarker866"/>extraction, simple <a id="_idIndexMarker867"/>content summarization, and <span class="No-Break">domain-specific queries.</span></p>
			<p>Refer to the <a href="B17792_12.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><em class="italic"> – LLM – Supervised Training</em> notebook in the accompanying GitHub repository for a hands-on end-to-end exercise to running an LLM tuning job in Vertex AI. Here are the key steps you need to follow to run a supervised <span class="No-Break">tuning job:</span></p>
			<ol>
				<li><strong class="bold">Prepare a </strong><span class="No-Break"><strong class="bold">tuning dataset</strong></span><span class="No-Break">:</span><p class="list-inset">When preparing a dataset to tune a <a id="_idIndexMarker868"/>foundation model, it’s crucial to include examples that are directly relevant to the task you aim to achieve with the model. The structure of your training dataset should be in a text-to-text format, where each entry (or row) combines an input text (referred to as a prompt) with<a id="_idIndexMarker869"/> its corresponding <span class="No-Break">expected output.</span></p><p class="list-inset">A minimum of 10 examples is required in your dataset, but for more effective results, it’s advisable to include over <span class="No-Break">100 examples.</span></p><p class="list-inset">Data format specification: The tuning dataset should be in the <strong class="bold">JSON Lines</strong> (<strong class="bold">JSONL</strong>) format. In this <a id="_idIndexMarker870"/>format, each line must contain a single <span class="No-Break">tuning example.</span></p><p class="list-inset">Content structure: Each<a id="_idIndexMarker871"/> example should consist of <span class="No-Break">two fields:</span></p><ul><li><strong class="source-inline">input_text</strong>: This field includes the prompt for <span class="No-Break">the model</span></li><li><strong class="source-inline">output_text</strong>: This field should contain the model’s anticipated <span class="No-Break">response post-tuning</span></li></ul><p class="list-inset"><span class="No-Break">Example:</span></p><pre class="source-code">
{"input_text": "What is the most compute efficient method to tune a foundation LLM ",
"output_text": "PEFT"}
{"input_text": "What is the best tuning method for an LLM to get best accuracy?",
"output_text": "Full fine tuning"}</pre><p class="list-inset">Sample dataset you can use: <strong class="bold">Medical Transcription Datase</strong><strong class="bold"><a id="_idIndexMarker872"/></strong><strong class="bold">t</strong> <span class="No-Break">from Kaggle.</span></p><p class="list-inset">Token length limits: At the time of this book’s<a id="_idIndexMarker873"/> publication, the <strong class="source-inline">input_text</strong> field can have a maximum token length of 8,192 (approx. 32k English characters), and the <strong class="source-inline">output_text</strong> field can have a maximum token length of 1,024 (approx. 4k <span class="No-Break">English characters).</span></p></li>				<li>Upload the dataset to a Google Cloud <span class="No-Break">Storage bucket.</span></li>
				<li>Create a supervised tuning job (<em class="italic">detailed steps are in the </em><span class="No-Break"><em class="italic">accompanying notebook</em></span><span class="No-Break">).</span><p class="list-inset">The following arguments are to<a id="_idIndexMarker874"/> be provided when starting a <span class="No-Break">tuning job:</span></p><pre class="source-code">
project_id: GCP Project ID, used to initialize vertexai
location: GCP Region, used to initialize vertexai
model_display_name: Customized Tuned LLM model name
training_data: GCS URI of jsonl file or pandas dataframe of training data.
train_steps: Number of training steps to use when tuning the model.
evaluation_dataset: GCS URI of jsonl file of evaluation data.
tensorboard_instance_name: The full name of the existing Vertex AI
TensorBoard instance:
projects/PROJECT_ID/locations/LOCATION_ID/tensorboards/TENSORBOARD_INSTANCE_ID</pre><p class="list-inset">Here’s the Python <a id="_idIndexMarker875"/>code to kick off the <span class="No-Break">tuning job:</span></p><pre class="source-code">vertexai.init(project=project_id, location=location, credentials=credentials)
eval_spec = TuningEvaluationSpec(evaluation_data=evaluation_dataset)
    eval_spec.tensorboard = aiplatform.Tensorboard(
        tensorboard_name=tensorboard_instance_name)
model = TextGenerationModel.from_pretrained("text-bison@001")
model.tune_model(
     training_data=training_data,
     # Optional:
     model_display_name=model_display_name,
    train_steps=train_steps,
    tuning_job_location=&lt;Region&gt;,
    tuned_model_location=location,
    tuning_evaluation_spec=eval_spec,)
print(model._job.status)
return model</pre></li>				<li>Load the tuned <a id="_idIndexMarker876"/>model from <strong class="bold">Vertex AI Model Registry</strong> to run prediction <a id="_idIndexMarker877"/>or evaluation jobs (<em class="italic">detailed steps are in the </em><span class="No-Break"><em class="italic">accompanying notebook</em></span><span class="No-Break">)</span><pre class="source-code">
import vertexai
from vertexai.preview.language_models import TextGenerationModel
model = TextGenerationModel.get_tuned_model(TUNED_MODEL_NAME)</pre></li>			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">At the time of publication, the maximum number of samples you can use for a fine-tuning job is limited <span class="No-Break">to 10,000.</span></p>
			<p>Now, let us look at Vertex AI’s <a id="_idIndexMarker878"/>native capabilities that help ensure that the output of LLMs is safe and compliant for an <span class="No-Break">enterprise setting.</span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor191"/>Safety filters for generated content</h2>
			<p>Despite being extremely useful for a<a id="_idIndexMarker879"/> wide array of use cases, LLMs’ ability to absorb human knowledge and behavior (good and bad) through the immense datasets gathered from the public also creates the risk of these models being exploited or generating harmful content. It is not uncommon for these models to generate outputs that are unanticipated, encompassing offensive, insensitive, or <span class="No-Break">incorrect content.</span></p>
			<p>It remains imperative for developers to have a profound understanding and meticulously test the models prior to deployment to circumvent any potential pitfalls. To help developers in this endeavor, GenAI Studio incorporates built-in content filtration systems, and the PaLM API offers safety attribute scoring, aiding clients to examine Google’s safety filters and establish confidence thresholds aligned with their individual use case and <span class="No-Break">business requirements.</span></p>
			<p>This is a full list of safety attributes offered as part of Google <span class="No-Break">PaLM models:</span></p>
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Safety Attribute</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Derogatory</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Negative or harmful comments targeting identity and/or <span class="No-Break">protected attributes.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Toxic</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Content that is rude, disrespectful, <span class="No-Break">or profane.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Sexual</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Contains references to sexual acts or other <span class="No-Break">lewd content.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Violent</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Describes scenarios depicting violence against an individual or a group, or general descriptions <span class="No-Break">of gore.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Insult</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Insulting, inflammatory, or negative comment toward a person or a group <span class="No-Break">of people.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Profanity</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Obscene or vulgar language such <span class="No-Break">as cursing.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Death, Harm, <span class="No-Break">and Tragedy</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Human deaths, tragedies, accidents, disasters, <span class="No-Break">and self-harm.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Firearms <span class="No-Break">and Weapons</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Content that mentions knives, guns, personal weapons, and accessories such as ammunition, <span class="No-Break">holsters, etc.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Public Safety</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Services and organizations that provide relief and ensure <span class="No-Break">public safety.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Health</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Human health, including: health conditions, diseases, and disorders, medical therapies, medication, vaccination, and medical practices resources for healing, including <span class="No-Break">support groups.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Religion <span class="No-Break">and Belief</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Belief systems that deal with the possibility of supernatural laws and beings; religion, faith, belief, spiritual practice, churches, and places of worship. Includes astrology and <span class="No-Break">the occult.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Illicit Drugs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Recreational <a id="_idIndexMarker880"/>and illicit drugs; drug paraphernalia and cultivation, headshops, etc. Includes medicinal use of drugs typically used recreationally (<span class="No-Break">e.g. marijuana).</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>War <span class="No-Break">and Conflict</span></p>
						</td>
						<td class="No-Table-Style">
							<p>War, military conflicts, and major physical conflicts involving large numbers of people. Includes discussion of military services, even if not directly related to a war <span class="No-Break">or conflict.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Finance</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Consumer and business financial services, such as banking, loans, credit, investing, <span class="No-Break">insurance, etc.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Politics</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Political news and media; discussions of social, governmental, and <span class="No-Break">public policy.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Legal</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Law-related content, including law firms, legal information, primary legal materials, paralegal services, legal publications and technology, expert witnesses, litigation consultants, and other legal <span class="No-Break">service providers.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 12.1 – PaLM models’ safety attributes</p>
			<p>When you submit an API request to PaLM models, the response from the API includes confidence scores for each safety attribute, as <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
{"predictions": [
    {"safetyAttributes": {
        "categories": [Derogatory", "Toxic", "Violent", "Sexual", "Insult", "Profanity", "Death, Harm &amp; Tragedy", "Firearms &amp; Weapons", "Public Safety", "Health", "Religion &amp; Belief", "Illicit Drugs","War &amp; Conflict", "Politics", "Finance", "Legal"],
        "scores":[0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1, 0.1,0.1,0.1,],
        "blocked": false},
      "content": "&lt;&gt;"}]}</pre>			<p>The scores in the preceding response are the risk values for each of the <span class="No-Break">risk categories.</span></p>
			<p>Developers can then program the required safety thresholds in their applications to remove any harmful content returned by the API. For example, for an application geared toward a younger audience, developers might set stringent filters to eliminate any text that is above the score of 0.1 on <a id="_idIndexMarker881"/>any of the safety attributes. But if the requirement is to create content to be shared on forums where adults discuss video games and in-game weapons, then developers might relax the filters around the <em class="italic">Firearms and Weapons</em> <span class="No-Break">safety attribute.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please keep in mind, right now, PaLM models do apply some initial safety filters before sending a response back to the customer. These filters can’t be completely switched off at <span class="No-Break">the moment.</span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor192"/>Summary</h1>
			<p>Vertex AI GenAI is a powerful suite of tools that can be used to create a wide variety of GenAI applications. With its easy-to-use interface and extensive library of pre-trained models, Vertex AI GenAI makes it possible for developers of all skill levels to get started with GenAI quickly <span class="No-Break">and easily.</span></p>
			<p>We hope that now, after reading this chapter, you possess foundational and practical knowledge about GenAI and its implementation using Vertex AI. With the skills to interact with and leverage foundation models, comprehension of basic prompt engineering, and an understanding of safety features native to Google’s GenAI models, you are now well-equipped to embark on practical endeavors and explore innovative applications <span class="No-Break">using GenAI.</span></p>
			<p>In the next chapter, <a href="B17792_13.xhtml#_idTextAnchor194"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Document AI – An End-to-End Solution for Processing Documents</em>, we will go over how you can use Google Cloud’s Document AI solution to extract information from scanned documents and structure it into a format that can be ingested by your data <span class="No-Break">storage solutions.</span></p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor193"/>References</h1>
			<p><em class="italic">Vertex AI</em> – <em class="italic">Responsible </em><span class="No-Break"><em class="italic">AI</em></span><span class="No-Break">: </span><a href="https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions"><span class="No-Break">https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions</span></a></p>
		</div>
	</div>
</div>
</body></html>