<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer234">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">10</span></h1>
<h1 class="chapterTitle" id="_idParaDest-137"><span class="koboSpan" id="kobo.2.1">Unleash the Power of Generative AI with Kaggle Models</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In the previous chapters, our primary focus was on mastering the analysis of diverse data types and developing strategies to tackle a variety of problems. </span><span class="koboSpan" id="kobo.3.2">We delved into an array of tools and methodologies for data exploration and visualization, enriching our skill set in these areas. </span><span class="koboSpan" id="kobo.3.3">A few of the earlier chapters were dedicated to constructing baseline models, notably for participation in competitive scenarios.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">Now, in this current chapter, we will pivot our attention toward leveraging Kaggle Models. </span><span class="koboSpan" id="kobo.4.2">Our objective is to integrate these models into Kaggle applications, in order to prototype the use of the newest Generative AI technologies in practical applications. </span><span class="koboSpan" id="kobo.4.3">A few examples of such real-world applications are personalized marketing, chatbots, content creation, targeted advertising, answering customers’ inquiries, fraud detection, medical diagnosis, patient monitoring, drug discovery, personalized medicine, financial analysis, risk evaluation, trading, document drafting, litigation support, legal analysis, personalized recommendations, and synthetic data generation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.5.1">This chapter will cover the following topics:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.6.1">An introduction </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.7.1">to Kaggle Models – how to access and use them</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.8.1">Prompting a </span><strong class="keyWord"><span class="koboSpan" id="kobo.9.1">Large Language Model</span></strong><span class="koboSpan" id="kobo.10.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.11.1">LLM</span></strong><span class="koboSpan" id="kobo.12.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.13.1">Using LLMs together with task-chaining solutions like Langchain to create a sequence (or chains) of prompts for LLMs</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.14.1">Building </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.15.1">a </span><strong class="keyWord"><span class="koboSpan" id="kobo.16.1">Retrieval Augmented Generation</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.18.1">RAG</span></strong><span class="koboSpan" id="kobo.19.1">) system using LangChain, LLMs, and a vector database</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-138"><span class="koboSpan" id="kobo.20.1">Introducing Kaggle Models</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.21.1">Kaggle Models represent one of the latest innovations on the Kaggle platform. </span><span class="koboSpan" id="kobo.21.2">This feature gained prominence in particular after the introduction of code competitions, where participants </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.22.1">often train models either on their local hardware or in the cloud. </span><span class="koboSpan" id="kobo.22.2">Post-training, they upload these models to Kaggle as a dataset. </span><span class="koboSpan" id="kobo.22.3">This practice allows Kagglers to utilize these pre-trained models in their inference notebooks, streamlining the process for code competition submissions. </span><span class="koboSpan" id="kobo.22.4">This method significantly reduces the runtime of the inference notebooks, fitting within the stringent time and memory constraints of the competition. </span><span class="koboSpan" id="kobo.22.5">Kaggle’s endorsement of this approach aligns well with real-world production systems, where model training and inference typically occur in separate pipelines.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.23.1">This strategy becomes indispensable with large-scale models, such as those based on Transformer architectures, considering the immense computational resources required for fine-tuning. </span><span class="koboSpan" id="kobo.23.2">Platforms like HuggingFace have further democratized access to large models, offering options to either utilize online or download collaboratively developed models. </span><span class="koboSpan" id="kobo.23.3">Kaggle’s introduction of the Models feature, which can be added to notebooks just like datasets, has been a significant advancement. </span><span class="koboSpan" id="kobo.23.4">These models can be directly used within a notebook for tasks like transfer learning or further fine-tuning. </span><span class="koboSpan" id="kobo.23.5">At the time of writing, however, Kaggle does not permit users to upload their models in the same manner as datasets.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.24.1">Kaggle’s model library offers a browsing and search functionality, allowing users to find models based on various criteria like name, metadata, task, data type, and more. </span><span class="koboSpan" id="kobo.24.2">At the time of writing, the library boasted 269 models with 1,997 variations, published by prominent organizations, including Google, TensorFlow, Kaggle, DeepMind, Meta, and Mistral.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.25.1">The field of Generative AI has seen a surge in interest following the introduction of models like GPT-3, ChatGPT, GPT-4, and various other </span><strong class="keyWord"><span class="koboSpan" id="kobo.26.1">LLMs</span></strong><span class="koboSpan" id="kobo.27.1"> or Foundation Models. </span><span class="koboSpan" id="kobo.27.2">Kaggle provides access to several powerful LLMs, such as Llama, Alpaca, and Llama 2. </span><span class="koboSpan" id="kobo.27.3">The platform’s integrated ecosystem allows users to swiftly test new models as they emerge. </span><span class="koboSpan" id="kobo.27.4">For instance, Meta’s Llama 2, available since July 18, 2023, is a series of generative text models, with variants ranging from 7 billion to 70 billion parameters. </span><span class="koboSpan" id="kobo.27.5">These models, including specialized versions for chat applications, are accessible on Kaggle with relative ease compared to other platforms.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.28.1">Kaggle further </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.29.1">simplifies the process by allowing users to start a notebook directly from the model page, akin to initiating a notebook from a competition or dataset. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.30.1">This streamlined approach, as illustrated in the following screenshot, enhances the user experience and fosters a more efficient workflow in model experimentation and application.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.31.1"><img alt="" role="presentation" src="../Images/B20963_10_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.32.1">Figure 10.1: Main page of the Mistral Model, with the button to Add a Notebook in the top-right corner</span></p>
<p class="normal"><span class="koboSpan" id="kobo.33.1">Once the notebook is open in the editor, the model is already added to it. </span><span class="koboSpan" id="kobo.33.2">One more step is needed in the case of models, and this is because a model has also variations, versions, and frameworks. </span><span class="koboSpan" id="kobo.33.3">In the right-hand panel of the notebook edit window, you can set these options. </span><span class="koboSpan" id="kobo.33.4">After these options are set, we are ready to use the model within the notebook. </span><span class="koboSpan" id="kobo.33.5">the following screenshot, we show the options for one model, Mistral, from Mistral AI (see </span><em class="italic"><span class="koboSpan" id="kobo.34.1">Reference 2</span></em><span class="koboSpan" id="kobo.35.1">), after everything was selected in the menu:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.36.1"><img alt="" role="presentation" src="../Images/B20963_10_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.37.1">Figure 10.2: Model Mistral from Mistral AI is added to a notebook, and all options are selected</span></p>
<h1 class="heading-1" id="_idParaDest-139"><span class="koboSpan" id="kobo.38.1">Prompting a foundation model</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.39.1">LLMs can be used directly, for example, for such tasks as summarization, question answering, and reasoning. </span><span class="koboSpan" id="kobo.39.2">Due to the very large amounts of data on which they were trained, they can </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.40.1">answer very well to a variety of questions on many subjects, since they have the context available in that training dataset.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.41.1">In many practical cases, such LLMs can correctly answer our questions on the first attempt. </span><span class="koboSpan" id="kobo.41.2">In other cases, we will need to provide a few clarifications or examples. </span><span class="koboSpan" id="kobo.41.3">The quality of the answers in these zero-shot or few-shot approaches highly depends on the ability of the user to craft the prompts for LLM. </span><span class="koboSpan" id="kobo.41.4">In this section, we will show the simplest way to interact with one LLM on Kaggle, using prompts.</span></p>
<h2 class="heading-2" id="_idParaDest-140"><span class="koboSpan" id="kobo.42.1">Model evaluation and testing</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.43.1">Before starting to use an LLM on Kaggle, we will need to perform a few preparation steps. </span><span class="koboSpan" id="kobo.43.2">We begin </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.44.1">by loading the model and then defining a tokenizer. </span><span class="koboSpan" id="kobo.44.2">Next, we create a model pipeline. </span><span class="koboSpan" id="kobo.44.3">In our first code example, we will use AutoTokenizer </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.45.1">from transformers as a tokenizer and create a pipeline, also using the transformers pipeline. </span><span class="koboSpan" id="kobo.45.2">The following code (excerpts from the notebook in </span><em class="italic"><span class="koboSpan" id="kobo.46.1">Reference 3</span></em><span class="koboSpan" id="kobo.47.1">) illustrates these steps described:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.48.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.49.1">load_model_tokenize_create_pipeline</span></span><span class="koboSpan" id="kobo.50.1">():
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.51.1">"""</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.52.1">    Load the model</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.53.1">    Create a </span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.54.1">    Args</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.55.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.56.1">        tokenizer</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.57.1">        pipeline</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.58.1">    """</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.59.1"># adapted from https://huggingface.co/blog/llama2#using-transformers</span></span><span class="koboSpan" id="kobo.60.1">
    time_1 = time()
    model = </span><span class="hljs-string"><span class="koboSpan" id="kobo.61.1">"/kaggle/input/llama-2/pytorch/7b-chat-hf/1"</span></span><span class="koboSpan" id="kobo.62.1">
    tokenizer = AutoTokenizer.from_pretrained(model)
    time_2 = time()
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.63.1">print</span></span><span class="koboSpan" id="kobo.64.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.65.1">f"Load model and init tokenizer: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.66.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.67.1">round</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.68.1">(time_2-time_1, </span></span><span class="hljs-number"><span class="koboSpan" id="kobo.69.1">3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.70.1">)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.71.1">"</span></span><span class="koboSpan" id="kobo.72.1">)
    pipeline = transformers.pipeline(
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.73.1">"text-generation"</span></span><span class="koboSpan" id="kobo.74.1">,
        model=model,
        torch_dtype=torch.float16,
        device_map=</span><span class="hljs-string"><span class="koboSpan" id="kobo.75.1">"auto"</span></span><span class="koboSpan" id="kobo.76.1">,)
    time_3 = time()
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.77.1">print</span></span><span class="koboSpan" id="kobo.78.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.79.1">f"Prepare pipeline: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.80.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.81.1">round</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.82.1">(time_3-time_2, </span></span><span class="hljs-number"><span class="koboSpan" id="kobo.83.1">3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.84.1">)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.85.1">"</span></span><span class="koboSpan" id="kobo.86.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.87.1">return</span></span><span class="koboSpan" id="kobo.88.1"> tokenizer, pipeline
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.89.1">The preceding code returns the tokenizer and the pipeline. </span><span class="koboSpan" id="kobo.89.2">We then implement a function to test the model. </span><span class="koboSpan" id="kobo.89.3">The function receives as parameters the tokenizer, the pipeline, and the prompt with which we would like to test the model. </span><span class="koboSpan" id="kobo.89.4">See the following code for the test function:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.90.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.91.1">test_model</span></span><span class="koboSpan" id="kobo.92.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.93.1">tokenizer, pipeline, prompt_to_test</span></span><span class="koboSpan" id="kobo.94.1">):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.95.1">"""</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.96.1">    Perform a query</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.97.1">    print the result</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.98.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.99.1">        tokenizer: the tokenizer</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.100.1">        pipeline: the pipeline</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.101.1">        prompt_to_test: the prompt</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.102.1">    Returns</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.103.1">        None</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.104.1">    """</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.105.1"># adapted from https://huggingface.co/blog/llama2#using-transformers</span></span><span class="koboSpan" id="kobo.106.1">
    time_1 = time()
    sequences = pipeline(
        prompt_to_test,
        do_sample=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.107.1">True</span></span><span class="koboSpan" id="kobo.108.1">,
        top_k=</span><span class="hljs-number"><span class="koboSpan" id="kobo.109.1">10</span></span><span class="koboSpan" id="kobo.110.1">,
        num_return_sequences=</span><span class="hljs-number"><span class="koboSpan" id="kobo.111.1">1</span></span><span class="koboSpan" id="kobo.112.1">,
        eos_token_id=tokenizer.eos_token_id,
        max_length=</span><span class="hljs-number"><span class="koboSpan" id="kobo.113.1">200</span></span><span class="koboSpan" id="kobo.114.1">,)
    time_2 = time()
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.115.1">print</span></span><span class="koboSpan" id="kobo.116.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.117.1">f"Test inference: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.118.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.119.1">round</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.120.1">(time_2-time_1, </span></span><span class="hljs-number"><span class="koboSpan" id="kobo.121.1">3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.122.1">)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.123.1">"</span></span><span class="koboSpan" id="kobo.124.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.125.1">for</span></span><span class="koboSpan" id="kobo.126.1"> seq </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.127.1">in</span></span><span class="koboSpan" id="kobo.128.1"> sequences:
        </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.129.1">print</span></span><span class="koboSpan" id="kobo.130.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.131.1">f"Result: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.132.1">{seq[</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.133.1">'generated_text'</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.134.1">]}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.135.1">"</span></span><span class="koboSpan" id="kobo.136.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.137.1">Now, we are </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.138.1">ready to prompt the model. </span><span class="koboSpan" id="kobo.138.2">The model we are using has the following characteristics: a Llama 2 model (7b), a chat version from </span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.139.1">HuggingFace (version 1), and thePyTorch framework. </span><span class="koboSpan" id="kobo.139.2">We will prompt the model with math questions. </span><span class="koboSpan" id="kobo.139.3">In the next code extract, we initialize the tokenizer and pipeline and then prompt the model with a simple arithmetic problem, formulated in plain language:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.140.1">tokenizer, pipeline = load_model_tokenize_create_pipeline()
prompt_to_test = </span><span class="hljs-string"><span class="koboSpan" id="kobo.141.1">'Prompt: Adrian has three apples. </span><span class="koboSpan" id="kobo.141.2">His sister Anne has ten apples more than him. </span><span class="koboSpan" id="kobo.141.3">How many apples has Anne?'</span></span><span class="koboSpan" id="kobo.142.1">
test_model(tokenizer, pipeline, prompt_to_test)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.143.1">Let’s see how the model reasons. </span><span class="koboSpan" id="kobo.143.2">the following screenshot, we plot the time for inference, the prompt, and the answer:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.144.1"><img alt="" role="presentation" src="../Images/B20963_10_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.145.1">Figure 10.3: Prompt, answer, and inference time for a math question with the Llama 2 model</span></p>
<p class="normal"><span class="koboSpan" id="kobo.146.1">For this simple math question, the reasoning of the model seems accurate. </span><span class="koboSpan" id="kobo.146.2">Let’s try again with a different question. </span><span class="koboSpan" id="kobo.146.3">In the following code excerpt, we ask a geometry question:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.147.1">prompt_to_test = </span><span class="hljs-string"><span class="koboSpan" id="kobo.148.1">'Prompt: A circle has the radius 5. </span><span class="koboSpan" id="kobo.148.2">What is the area of the circle?'</span></span><span class="koboSpan" id="kobo.149.1">
test_model(tokenizer, pipeline, prompt_to_test)
</span></code></pre>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.150.1">The following screenshot shows</span></em><span class="koboSpan" id="kobo.151.1"> the result of prompting the model with the preceding geometry question:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.152.1"><img alt="" role="presentation" src="../Images/B20963_10_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.153.1">Figure 10.4: Llama 2 model answer to a basic geometry question</span></p>
<p class="normal"><span class="koboSpan" id="kobo.154.1">The response </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.155.1">to simple mathematical questions is not correct </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.156.1">all of the time. </span><span class="koboSpan" id="kobo.156.2">In the following example, we prompted the model with a variation of the first algebraic problem. </span><span class="koboSpan" id="kobo.156.3">You can see that, in this case, the model took a convoluted and wrong path to reach an incorrect solution:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.157.1"><img alt="" role="presentation" src="../Images/B20963_10_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.158.1">Figure 10.5: Llama 2 model solution is wrong for an algebra problem</span></p>
<h2 class="heading-2" id="_idParaDest-141"><span class="koboSpan" id="kobo.159.1">Model quantization</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.160.1">In the preceding experiments, we engaged a model with a series of straightforward questions. </span><span class="koboSpan" id="kobo.160.2">This process underscored the crucial role of crafting clear, well-structured prompts to </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.161.1">elicit accurate and relevant responses. </span><span class="koboSpan" id="kobo.161.2">While Kaggle generously offers substantial computational resources at no cost, the sheer size of LLMs presents a challenge. </span><span class="koboSpan" id="kobo.161.3">These models demand significant RAM and CPU/GPU power for loading and inference.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.162.1">To mitigate these demands, we can employ a technique known as model quantization. </span><span class="koboSpan" id="kobo.162.2">This method effectively reduces the memory and computational requirements of a model. </span><span class="koboSpan" id="kobo.162.3">It achieves this by representing the model’s weights and activation functions using low-precision data types, such as 8-bit or 4-bit integers, instead of the standard 32-bit floating-point format. </span><span class="koboSpan" id="kobo.162.4">This approach not only conserves resources but also maintains a balance between efficiency and performance (see </span><em class="italic"><span class="koboSpan" id="kobo.163.1">Reference 4</span></em><span class="koboSpan" id="kobo.164.1">).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.165.1">In our upcoming example, we’ll demonstrate how to quantize a model from Kaggle using one of the available techniques, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.166.1">llama.cpp</span></code><span class="koboSpan" id="kobo.167.1"> library. </span><span class="koboSpan" id="kobo.167.2">We’ve chosen the Llama 2 model for this purpose. </span><span class="koboSpan" id="kobo.167.3">Llama 2 is, at the time of writing, one of the most successful LLMs that you can download (with Meta approval) and use freely. </span><span class="koboSpan" id="kobo.167.4">It also has demonstrable accuracy for a variety of tasks, on par with many other available models. </span><span class="koboSpan" id="kobo.167.5">The quantization will be executed using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.168.1">llama.cpp</span></code><span class="koboSpan" id="kobo.169.1"> library. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.170.1">The forthcoming code snippet details the steps to install </span><code class="inlineCode"><span class="koboSpan" id="kobo.171.1">llama.cpp</span></code><span class="koboSpan" id="kobo.172.1">, import the necessary functions from the package, execute the quantization process, and subsequently load the quantized model. </span><span class="koboSpan" id="kobo.172.2">It’s important to note that, in this instance, we will not utilize the latest, more advanced quantization option available in </span><code class="inlineCode"><span class="koboSpan" id="kobo.173.1">llama.cpp</span></code><span class="koboSpan" id="kobo.174.1">. </span><span class="koboSpan" id="kobo.174.2">This example serves as an introduction to model quantization on Kaggle and its practical implementation:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.175.1">!CMAKE_ARGS=</span><span class="hljs-string"><span class="koboSpan" id="kobo.176.1">"-DLLAMA_CUBLAS=on"</span></span><span class="koboSpan" id="kobo.177.1"> pip install llama-cpp-python
!git clone https://github.com/ggerganov/llama.cpp.git
!python llama.cpp/convert.py /kaggle/</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.178.1">input</span></span><span class="koboSpan" id="kobo.179.1">/llama-</span><span class="hljs-number"><span class="koboSpan" id="kobo.180.1">2</span></span><span class="koboSpan" id="kobo.181.1">/pytorch/7b-chat-hf/</span><span class="hljs-number"><span class="koboSpan" id="kobo.182.1">1</span></span><span class="koboSpan" id="kobo.183.1"> \
  --outfile llama-7b.gguf \
  --outtype q8_0
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.184.1">from</span></span><span class="koboSpan" id="kobo.185.1"> llama_cpp </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.186.1">import</span></span><span class="koboSpan" id="kobo.187.1"> Llama
llm = Llama(model_path=</span><span class="hljs-string"><span class="koboSpan" id="kobo.188.1">"/kaggle/working/llama-7b.gguf"</span></span><span class="koboSpan" id="kobo.189.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.190.1">Let’s look at a few examples of testing the quantized model. </span><span class="koboSpan" id="kobo.190.2">We will start by prompting it with a geography question:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.191.1">output = llm(</span><span class="hljs-string"><span class="koboSpan" id="kobo.192.1">"Q: Name three capital cities in Europe? </span><span class="koboSpan" id="kobo.192.2">A: "</span></span><span class="koboSpan" id="kobo.193.1">, max_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.194.1">38</span></span><span class="koboSpan" id="kobo.195.1">, stop=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.196.1">"Q:"</span></span><span class="koboSpan" id="kobo.197.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.198.1">"\n"</span></span><span class="koboSpan" id="kobo.199.1">], echo=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.200.1">True</span></span><span class="koboSpan" id="kobo.201.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.202.1">The result of the prompt is:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.203.1"><img alt="" role="presentation" src="../Images/B20963_10_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.204.1">Figure 10.6: Result of prompting the quantized Llama 2 model with a geography question</span></p>
<p class="normal"><span class="koboSpan" id="kobo.205.1">In the next screenshot, we show the answer of the model to a simple geometry question. </span><span class="koboSpan" id="kobo.205.2">The answer is </span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.206.1">quite straightforward and clearly formulated. </span><span class="koboSpan" id="kobo.206.2">The code to prompt the model and the one to print the result is:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.207.1">output = llm(</span><span class="hljs-string"><span class="koboSpan" id="kobo.208.1">"If a circle has the radius 3, what is its area?"</span></span><span class="koboSpan" id="kobo.209.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.210.1">print</span></span><span class="koboSpan" id="kobo.211.1">(output[</span><span class="hljs-string"><span class="koboSpan" id="kobo.212.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.213.1">choices'</span></span><span class="koboSpan" id="kobo.214.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.215.1">0</span></span><span class="koboSpan" id="kobo.216.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.217.1">'text'</span></span><span class="koboSpan" id="kobo.218.1">])
</span></code></pre>
<figure class="mediaobject"><span class="koboSpan" id="kobo.219.1"><img alt="" role="presentation" src="../Images/B20963_10_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.220.1">Figure 10.7: Result of prompting the quantized Llama 2 model with a geometry question</span></p>
<p class="normal"><span class="koboSpan" id="kobo.221.1">The notebook to illustrate the first method to quantize a Llama 2 model, from which we extracted the code and results, is given in </span><em class="italic"><span class="koboSpan" id="kobo.222.1">Reference 5</span></em><span class="koboSpan" id="kobo.223.1">. </span><span class="koboSpan" id="kobo.223.2">This notebook was run on the GPU. </span><span class="koboSpan" id="kobo.223.3">In another notebook given in </span><em class="italic"><span class="koboSpan" id="kobo.224.1">Reference 6</span></em><span class="koboSpan" id="kobo.225.1">, we run the same model but on the CPU. </span><span class="koboSpan" id="kobo.225.2">It is quite interesting to notice that the time to execute the inference on the CPU with the quantized model is much smaller than on the GPU (with the same quantized model). </span><span class="koboSpan" id="kobo.225.3">See the notebooks in </span><em class="italic"><span class="koboSpan" id="kobo.226.1">Reference 5</span></em><span class="koboSpan" id="kobo.227.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.228.1">6</span></em><span class="koboSpan" id="kobo.229.1"> for more details.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.230.1">We can also use alternative approaches for model quantization. </span><span class="koboSpan" id="kobo.230.2">For example, in </span><em class="italic"><span class="koboSpan" id="kobo.231.1">Reference 7</span></em><span class="koboSpan" id="kobo.232.1">, we used the </span><code class="inlineCode"><span class="koboSpan" id="kobo.233.1">bitsandbytes</span></code><span class="koboSpan" id="kobo.234.1"> library for model quantization. </span><span class="koboSpan" id="kobo.234.2">In order to use this quantization option, we need to install the accelerate library and the latest version of </span><code class="inlineCode"><span class="koboSpan" id="kobo.235.1">bitsandbytes</span></code><span class="koboSpan" id="kobo.236.1">. </span><span class="koboSpan" id="kobo.236.2">The following code excerpt shows how to initialize the model configuration </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.237.1">for quantization and load the model with this configuration:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.238.1">model_1_id = </span><span class="hljs-string"><span class="koboSpan" id="kobo.239.1">'/kaggle/input/llama-2/pytorch/7b-chat-hf/1'</span></span><span class="koboSpan" id="kobo.240.1">
device = </span><span class="hljs-string"><span class="koboSpan" id="kobo.241.1">f'cuda:</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.242.1">{cuda.current_device()}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.243.1">'</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.244.1">if</span></span><span class="koboSpan" id="kobo.245.1"> cuda.is_available() </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.246.1">else</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.247.1">'cpu'</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.248.1"># set quantization configuration to load large model with less GPU memory</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.249.1"># this requires the `bitsandbytes` library</span></span><span class="koboSpan" id="kobo.250.1">
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.251.1">True</span></span><span class="koboSpan" id="kobo.252.1">,
    bnb_4bit_quant_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.253.1">'nf4'</span></span><span class="koboSpan" id="kobo.254.1">,
    bnb_4bit_use_double_quant=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.255.1">True</span></span><span class="koboSpan" id="kobo.256.1">,
    bnb_4bit_compute_dtype=bfloat16
)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.257.1">We also define a pipeline:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.258.1">time_1 = time()
query_pipeline_1 = transformers.pipeline(
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.259.1">"text-generation"</span></span><span class="koboSpan" id="kobo.260.1">,
        model=model_1,
        tokenizer=tokenizer_1,
        torch_dtype=torch.float16,
        device_map=</span><span class="hljs-string"><span class="koboSpan" id="kobo.261.1">"auto"</span></span><span class="koboSpan" id="kobo.262.1">,)
time_2 = time()
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.263.1">print</span></span><span class="koboSpan" id="kobo.264.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.265.1">f"Prepare pipeline #1: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.266.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.267.1">round</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.268.1">(time_2-time_1, </span></span><span class="hljs-number"><span class="koboSpan" id="kobo.269.1">3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.270.1">)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.271.1"> sec."</span></span><span class="koboSpan" id="kobo.272.1">)
llm_1 = HuggingFacePipeline(pipeline=query_pipeline_1)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.273.1">We can test the model with a simple prompt:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.274.1">llm_1(prompt=</span><span class="hljs-string"><span class="koboSpan" id="kobo.275.1">"What is the most popular food in France for tourists? </span><span class="koboSpan" id="kobo.275.2">Just return the name of the food."</span></span><span class="koboSpan" id="kobo.276.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.277.1">The answer seems to be correct:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.278.1"><img alt="" role="presentation" src="../Images/B20963_10_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.279.1">Figure 10.8: Answer to a simple geography question (Llama 2 quantized with the bitsandbytes library)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.280.1">So far, we have </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.281.1">experimented with prompting models. </span><span class="koboSpan" id="kobo.281.2">We directly used the models from Kaggle Models, or after quantization. </span><span class="koboSpan" id="kobo.281.3">We performed the quantization with two different methods. </span><span class="koboSpan" id="kobo.281.4">In the next section, however, we will see how, using a task-chaining framework such as Langchain, we can extend the power of LLMs and create sequences of operations, where the answer of an initial query for the LLM is fed as input to the next task.</span></p>
<h1 class="heading-1" id="_idParaDest-142"><span class="koboSpan" id="kobo.282.1">Building a multi-task application with Langchain</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.283.1">Langchain is the most popular task-chaining framework (</span><em class="italic"><span class="koboSpan" id="kobo.284.1">Reference 8</span></em><span class="koboSpan" id="kobo.285.1">). </span><span class="koboSpan" id="kobo.285.2">Task chaining is an extension of the prompt engineering concept that we illustrated in the previous section. </span><span class="koboSpan" id="kobo.285.3">Chains serve as predetermined sequences of operations, designed to organize intricate processes in a format that is both more manageable and easier to understand. </span><span class="koboSpan" id="kobo.285.4">These chains </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.286.1">follow a distinct order of actions. </span><span class="koboSpan" id="kobo.286.2">They are well suited for workflows characterized by a consistent number of steps. </span><span class="koboSpan" id="kobo.286.3">With task chaining, you can create sequences of prompts, where the output of the previous task executed by the framework is fed as input for the next task.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.287.1">Besides Langchain, there are now several other options available for task chaining, like LlamaIndex or Semantic Kernel (from Microsoft). </span><span class="koboSpan" id="kobo.287.2">Langchain provides multiple functionalities, including specialized tools to ingest data or output results, intelligent agents, as well as the possibility to extend it by defining your own tasks, tools, or agents. </span><span class="koboSpan" id="kobo.287.3">An agent will select and execute tasks based on the perceived context, in order to achieve its objective. </span><span class="koboSpan" id="kobo.287.4">In order to execute tasks, it will use generic or custom tools.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.288.1">Let’s start working with Langchain by defining a two-step sequence. </span><span class="koboSpan" id="kobo.288.2">We will define the sequence in a custom function that will receive a parameter and formulate an initial prompt, parameterized with the input parameter. </span><span class="koboSpan" id="kobo.288.3">Based on the answer to the first prompt, we assemble the prompt for the next task. </span><span class="koboSpan" id="kobo.288.4">This way, we can create the dynamic behavior of </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.289.1">our mini-application. </span><span class="koboSpan" id="kobo.289.2">The code for defining this function is as follows (</span><em class="italic"><span class="koboSpan" id="kobo.290.1">Reference 7</span></em><span class="koboSpan" id="kobo.291.1">):</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.292.1">def sequential_chain(country, llm):
    </span><span class="hljs-string"><span class="koboSpan" id="kobo.293.1">"""</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.294.1">    Args:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.295.1">        country: country selected</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.296.1">    Returns:</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.297.1">        None</span></span>
<span class="hljs-string"><span class="koboSpan" id="kobo.298.1">    """</span></span><span class="koboSpan" id="kobo.299.1">
    time_1 = time()
    template = </span><span class="hljs-string"><span class="koboSpan" id="kobo.300.1">"What is the most popular food in {country} for tourists? </span><span class="koboSpan" id="kobo.300.2">Just return the name of the food."</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.301.1">#  first task in chain</span></span><span class="koboSpan" id="kobo.302.1">
    first_prompt = PromptTemplate(
    input_variables=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.303.1">"country"</span></span><span class="koboSpan" id="kobo.304.1">],
    template=template)
    chain_one = LLMChain(llm = llm, prompt = first_prompt)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.305.1"># second step in chain</span></span><span class="koboSpan" id="kobo.306.1">
    second_prompt = PromptTemplate(
    input_variables=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.307.1">"food"</span></span><span class="koboSpan" id="kobo.308.1">],
    template=</span><span class="hljs-string"><span class="koboSpan" id="kobo.309.1">"What are the top three ingredients in {food}. </span><span class="koboSpan" id="kobo.309.2">Just return the answer as three bullet points."</span></span><span class="koboSpan" id="kobo.310.1">,)
    chain_two = LLMChain(llm=llm, prompt=second_prompt)
    </span><span class="hljs-comment"><span class="koboSpan" id="kobo.311.1"># combine the two steps and run the chain sequence</span></span><span class="koboSpan" id="kobo.312.1">
    overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.313.1">True</span></span><span class="koboSpan" id="kobo.314.1">)
    overall_chain.run(country)
    time_2 = time()
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.315.1">print</span></span><span class="koboSpan" id="kobo.316.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.317.1">f"Run sequential chain: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.318.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.319.1">round</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.320.1">(time_2-time_1, </span></span><span class="hljs-number"><span class="koboSpan" id="kobo.321.1">3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.322.1">)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.323.1"> sec."</span></span><span class="koboSpan" id="kobo.324.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.325.1">The expected input parameter is the name of a country. </span><span class="koboSpan" id="kobo.325.2">The first prompt will get the most popular food in that country. </span><span class="koboSpan" id="kobo.325.3">The next prompt will use the answer to the first question to build the second question, which is about the top three ingredients for that food.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.326.1">Let’s check the functionality of the code with two examples. </span><span class="koboSpan" id="kobo.326.2">First, let’s try with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.327.1">France</span></code><span class="koboSpan" id="kobo.328.1"> parameter:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.329.1">final_answer = sequential_chain(</span><span class="hljs-string"><span class="koboSpan" id="kobo.330.1">"France"</span></span><span class="koboSpan" id="kobo.331.1">, llm_1)
</span></code></pre>
<figure class="mediaobject"><span class="koboSpan" id="kobo.332.1"><img alt="" role="presentation" src="../Images/B20963_10_09.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.333.1">Figure 10.9: Two-step sequential chain execution (ingredients of the most famous food in France)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.334.1">The answer looks quite convincing. </span><span class="koboSpan" id="kobo.334.2">Indeed, tourists in France prefer snails, and yes, the top three </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.335.1">ingredients for this tasty food are listed correctly. </span><span class="koboSpan" id="kobo.335.2">Let’s double-check, with another country famous for its delicious food, </span><code class="inlineCode"><span class="koboSpan" id="kobo.336.1">Italy</span></code><span class="koboSpan" id="kobo.337.1">. </span><span class="koboSpan" id="kobo.337.2">The prompt will be:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.338.1">final_answer = sequential_chain(</span><span class="hljs-string"><span class="koboSpan" id="kobo.339.1">"Italy"</span></span><span class="koboSpan" id="kobo.340.1">, llm_1)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.341.1">Consequently, the result will be:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.342.1"><img alt="" role="presentation" src="../Images/B20963_10_10.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.343.1">Figure 10.10: Most popular food in Italy and its ingredients</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">We illustrated with an intuitive example how we can use LangChain together with an LLM to </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.345.1">chain multiple prompts and extend the power of LLMs, for example, in the automatization of business processes. </span><span class="koboSpan" id="kobo.345.2">In the next section, we will see how we can use LLMs for another important task, the automatization of code generation, to increase the productivity in the coding process.</span></p>
<h1 class="heading-1" id="_idParaDest-143"><span class="koboSpan" id="kobo.346.1">Code generation with Kaggle Models</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.347.1">For code generation, we will experiment with the Code Llama model, the 13b version. </span><span class="koboSpan" id="kobo.347.2">From the LLMs </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.348.1">available on the Kaggle platform at </span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.349.1">the time of writing, this model was the most appropriate, in regards to its purpose (it is a model specialized for code generation) and size (i.e., we can use it with Kaggle Notebooks), for the task of code generation. </span><span class="koboSpan" id="kobo.349.2">The notebook used to demonstrate the code generation is given in </span><em class="italic"><span class="koboSpan" id="kobo.350.1">Reference 9</span></em><span class="koboSpan" id="kobo.351.1">. </span><span class="koboSpan" id="kobo.351.2">The model is loaded, quantized using </span><code class="inlineCode"><span class="koboSpan" id="kobo.352.1">bitsandbytes</span></code><span class="koboSpan" id="kobo.353.1">, and has a tokenizer initialized in the same way, as demonstrated in </span><em class="italic"><span class="koboSpan" id="kobo.354.1">Reference 7</span></em><span class="koboSpan" id="kobo.355.1">. </span><span class="koboSpan" id="kobo.355.2">We define a prompt and a pipeline (using the transformers function) with the following code:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.356.1">prompt = </span><span class="hljs-string"><span class="koboSpan" id="kobo.357.1">'Write the code for a function to compute the area of circle.'</span></span><span class="koboSpan" id="kobo.358.1">
sequences = pipeline(
    prompt,
    do_sample=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.359.1">True</span></span><span class="koboSpan" id="kobo.360.1">,
    top_k=</span><span class="hljs-number"><span class="koboSpan" id="kobo.361.1">10</span></span><span class="koboSpan" id="kobo.362.1">,
    temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.363.1">0.1</span></span><span class="koboSpan" id="kobo.364.1">,
    top_p=</span><span class="hljs-number"><span class="koboSpan" id="kobo.365.1">0.95</span></span><span class="koboSpan" id="kobo.366.1">,
    num_return_sequences=</span><span class="hljs-number"><span class="koboSpan" id="kobo.367.1">1</span></span><span class="koboSpan" id="kobo.368.1">,
    eos_token_id=tokenizer.eos_token_id,
    max_length=</span><span class="hljs-number"><span class="koboSpan" id="kobo.369.1">200</span></span><span class="koboSpan" id="kobo.370.1">,
)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.371.1">The result </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.372.1">of executing the preceding code is given in the following screenshot. </span><span class="koboSpan" id="kobo.372.2">The code looks functional, but the answer contains more information </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.373.1">than expected. </span><span class="koboSpan" id="kobo.373.2">We obtained this by printing all the sequences outputted. </span><span class="koboSpan" id="kobo.373.3">If we just select the first one, the answer will be correct (only the code for the circle area).</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.374.1"><img alt="" role="presentation" src="../Images/B20963_10_11.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.375.1">Figure 10.11: Code generation: a function to compute the area of a circle</span></p>
<p class="normal"><span class="koboSpan" id="kobo.376.1">In the notebook from </span><em class="italic"><span class="koboSpan" id="kobo.377.1">Reference 9</span></em><span class="koboSpan" id="kobo.378.1">, there are more examples; we will not give all the details here. </span><span class="koboSpan" id="kobo.378.2">You can modify the notebook and generate more answers, by changing the prompt.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.379.1">In the next section, let’s see how we can further extend the functionality of LLMs, by creating a system that retrieves information stored in a special database (a vector database), assembling a prompt by combining the initial query with the retrieved information (context), and prompting the LLM to answer to the initial query by only using the </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.380.1">context result from the retrieval step. </span><span class="koboSpan" id="kobo.380.2">Such a system is called </span><strong class="keyWord"><span class="koboSpan" id="kobo.381.1">Retrieval Augmented Generation</span></strong><span class="koboSpan" id="kobo.382.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.383.1">RAG</span></strong><span class="koboSpan" id="kobo.384.1">).</span></p>
<h1 class="heading-1" id="_idParaDest-144"><span class="koboSpan" id="kobo.385.1">Creating a RAG system</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.386.1">In the previous sections, we explored various approaches to interact with Foundation Models – more precisely, available LLMs from Kaggle Models. </span><span class="koboSpan" id="kobo.386.2">First, we experimented with prompting, directly using the models. </span><span class="koboSpan" id="kobo.386.3">Then, we quantized the models with two different approaches. </span><span class="koboSpan" id="kobo.386.4">We also showed that we can use models to generate code. </span><span class="koboSpan" id="kobo.386.5">A more complex </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.387.1">application included a combination of </span><code class="inlineCode"><span class="koboSpan" id="kobo.388.1">LangChain</span></code><span class="koboSpan" id="kobo.389.1"> with an LLM to create sequences of connected operations, or task sequences.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.390.1">In all these cases, the answers of the LLM are based on the information already available with the model at the time of training it. </span><span class="koboSpan" id="kobo.390.2">If we would like to have the LLM answer queries about information that was never presented to the LLM, the model might provide a deceiving answer by hallucinating. </span><span class="koboSpan" id="kobo.390.3">To counter this tendency of models to hallucinate when they do not have the right information, we can fine-tune models with our own data. </span><span class="koboSpan" id="kobo.390.4">The disadvantage to this is that it is costly, since the computational resources needed to fine-tune a large model are very large. </span><span class="koboSpan" id="kobo.390.5">It also doesn’t necessarily totally eliminate hallucination.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.391.1">An alternative to this approach is to combine vector databases, task-chaining frameworks, and LLMs to create a RAG system (see </span><em class="italic"><span class="koboSpan" id="kobo.392.1">Reference 10</span></em><span class="koboSpan" id="kobo.393.1">). </span><span class="koboSpan" id="kobo.393.2">In the following figure, we illustrate the functionality of such a system:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.394.1"><img alt="" role="presentation" src="../Images/B20963_10_12.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.395.1">Figure 10.12: RAG system explained</span></p>
<p class="normal"><span class="koboSpan" id="kobo.396.1">Before using the RAG system, we will have to ingest the documents into the vector database (</span><em class="italic"><span class="koboSpan" id="kobo.397.1">Step 1</span></em><span class="koboSpan" id="kobo.398.1"> in </span><em class="italic"><span class="koboSpan" id="kobo.399.1">Figure 10.12</span></em><span class="koboSpan" id="kobo.400.1">). </span><span class="koboSpan" id="kobo.400.2">The documents can be in any format, including Word, PowerPoint, text, Excel, images, video, email, etc. </span><span class="koboSpan" id="kobo.400.3">We first transform each modality in the text format (for example, using Tesseract to extract text from images, or OpenAI Whisper to convert video into text). </span><span class="koboSpan" id="kobo.400.4">After we transform all the formats/modalities into text, we will </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.401.1">have to split the larger texts into fixed-size chunks (partially superposed, to not lose context that might be distributed across multiple chunks).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.402.1">Then, we use one of the options to encode the information before adding the pre-processed documents to the vector database. </span><span class="koboSpan" id="kobo.402.2">The vector database stores the data encoded using text embeddings, and it also uses very efficient indexing for such an encoding type, which will allow us to perform a fast search and retrieval of information, based on a similarity search. </span><span class="koboSpan" id="kobo.402.3">We have multiple options for vector databases, like ChromaDB, Weaviate, Pinecone, and FAISS. </span><span class="koboSpan" id="kobo.402.4">In our application on Kaggle, we used ChromaDB, which has a simple interface, plugins with Langchain, is easy to integrate, has options to be used in memory as well as persistent storage.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.403.1">Once the data is transformed, chunked, encoded, and indexed in the vector database, we can start to query our system. </span><span class="koboSpan" id="kobo.403.2">Queries are passed through a Langchain specialized task – a question and answering retrieval (</span><em class="italic"><span class="koboSpan" id="kobo.404.1">Step 2</span></em><span class="koboSpan" id="kobo.405.1"> in </span><em class="italic"><span class="koboSpan" id="kobo.406.1">Figure 10.12</span></em><span class="koboSpan" id="kobo.407.1">). </span><span class="koboSpan" id="kobo.407.2">The query is used to perform similarity search in the vector database. </span><span class="koboSpan" id="kobo.407.3">The retrieved documents are used together with the query (</span><em class="italic"><span class="koboSpan" id="kobo.408.1">Step 3</span></em><span class="koboSpan" id="kobo.409.1"> in </span><em class="italic"><span class="koboSpan" id="kobo.410.1">Figure 10.12</span></em><span class="koboSpan" id="kobo.411.1">) to compose the prompt for LLM (</span><em class="italic"><span class="koboSpan" id="kobo.412.1">Step 4</span></em><span class="koboSpan" id="kobo.413.1"> in </span><em class="italic"><span class="koboSpan" id="kobo.414.1">Figure 10.12</span></em><span class="koboSpan" id="kobo.415.1">). </span><span class="koboSpan" id="kobo.415.2">The LLM will provide its answer to the query by referring only to the context we provided – context from the data stored in the vector database.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.416.1">The code to implement the RAG system is given in </span><em class="italic"><span class="koboSpan" id="kobo.417.1">Reference 11</span></em><span class="koboSpan" id="kobo.418.1">. </span><span class="koboSpan" id="kobo.418.2">We will use as documents </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.419.1">the text of the State of the Union 2023 (from Kaggle datasets). </span><span class="koboSpan" id="kobo.419.2">Let’s first use the LLM directly by prompting to answer a question about the State of the Union in general:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.420.1">llm = HuggingFacePipeline(pipeline=query_pipeline)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.421.1"># checking again that everything is working fine</span></span><span class="koboSpan" id="kobo.422.1">
llm(prompt=</span><span class="hljs-string"><span class="koboSpan" id="kobo.423.1">"Please explain what is the State of the Union address. </span><span class="koboSpan" id="kobo.423.2">Give just a definition. </span><span class="koboSpan" id="kobo.423.3">Keep it in 100 words."</span></span><span class="koboSpan" id="kobo.424.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.425.1">The answer is given in the following screenshot. </span><span class="koboSpan" id="kobo.425.2">We can observe that the LLM has the relevant information, and the answer is correct. </span><span class="koboSpan" id="kobo.425.3">Of course, if we had asked about recent information, the answer might have been wrong.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.426.1"><img alt="" role="presentation" src="../Images/B20963_10_13.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.427.1">Figure 10.13: Result of the prompt (a general question, without context)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.428.1">Let’s see now some answers to questions about the information we ingested in the vector database.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.429.1">Data transformation, chunking, and encoding are done with the following code. </span><span class="koboSpan" id="kobo.429.2">Since the data we ingest is plain text, we will use </span><code class="inlineCode"><span class="koboSpan" id="kobo.430.1">TextLoader</span></code><span class="koboSpan" id="kobo.431.1"> from Langchain. </span><span class="koboSpan" id="kobo.431.2">We will use </span><code class="inlineCode"><span class="koboSpan" id="kobo.432.1">ChromaDB</span></code><span class="koboSpan" id="kobo.433.1"> as the vector database, and Sentence Transformer for embeddings:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.434.1"># load file(s)</span></span><span class="koboSpan" id="kobo.435.1">
loader = TextLoader(</span><span class="hljs-string"><span class="koboSpan" id="kobo.436.1">"/kaggle/input/president-bidens-state-of-the-union-2023/biden-sotu-2023-planned-official.txt"</span></span><span class="koboSpan" id="kobo.437.1">,
                    encoding=</span><span class="hljs-string"><span class="koboSpan" id="kobo.438.1">"utf8"</span></span><span class="koboSpan" id="kobo.439.1">)
documents = loader.load()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.440.1"># data chunking</span></span><span class="koboSpan" id="kobo.441.1">
text_splitter = RecursiveCharacterTextSplitter(chunk_size=</span><span class="hljs-number"><span class="koboSpan" id="kobo.442.1">1000</span></span><span class="koboSpan" id="kobo.443.1">, chunk_overlap=</span><span class="hljs-number"><span class="koboSpan" id="kobo.444.1">20</span></span><span class="koboSpan" id="kobo.445.1">)
all_splits = text_splitter.split_documents(documents)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.446.1"># embeddings model: Sentence Transformer</span></span><span class="koboSpan" id="kobo.447.1">
model_name = </span><span class="hljs-string"><span class="koboSpan" id="kobo.448.1">"sentence-transformers/all-mpnet-base-v2"</span></span><span class="koboSpan" id="kobo.449.1">
model_kwargs = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.450.1">"device"</span></span><span class="koboSpan" id="kobo.451.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.452.1">"cuda"</span></span><span class="koboSpan" id="kobo.453.1">}
embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.454.1"># add documents to the ChromaDB database</span></span><span class="koboSpan" id="kobo.455.1">
vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=</span><span class="hljs-string"><span class="koboSpan" id="kobo.456.1">"chroma_db"</span></span><span class="koboSpan" id="kobo.457.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.458.1">We define </span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.459.1">the question and answering retrieval chain:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.460.1">retriever = vectordb.as_retriever()
qa = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.461.1">"stuff"</span></span><span class="koboSpan" id="kobo.462.1">, 
    retriever=retriever, 
    verbose=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.463.1">True</span></span><span class="koboSpan" id="kobo.464.1">
)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.465.1">We also define a function to test the preceding chain:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.466.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.467.1">test_rag</span></span><span class="koboSpan" id="kobo.468.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.469.1">qa, query</span></span><span class="koboSpan" id="kobo.470.1">):
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.471.1">print</span></span><span class="koboSpan" id="kobo.472.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.473.1">f"Query: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.474.1">{query}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.475.1">\n"</span></span><span class="koboSpan" id="kobo.476.1">)
    time_1 = time()
    result = qa.run(query)
    time_2 = time()
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.477.1">print</span></span><span class="koboSpan" id="kobo.478.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.479.1">f"Inference time: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.480.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.481.1">round</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.482.1">(time_2-time_1, </span></span><span class="hljs-number"><span class="koboSpan" id="kobo.483.1">3</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.484.1">)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.485.1"> sec."</span></span><span class="koboSpan" id="kobo.486.1">)
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.487.1">print</span></span><span class="koboSpan" id="kobo.488.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.489.1">"\nResult: "</span></span><span class="koboSpan" id="kobo.490.1">, result)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.491.1">Let’s test the functionality of this system. </span><span class="koboSpan" id="kobo.491.2">We will formulate queries about the subject – in this case, the State of the Union 2023:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.492.1">query = </span><span class="hljs-string"><span class="koboSpan" id="kobo.493.1">"What were the main topics in the State of the Union in 2023? </span><span class="koboSpan" id="kobo.493.2">Summarize. </span><span class="koboSpan" id="kobo.493.3">Keep it under 200 words."</span></span><span class="koboSpan" id="kobo.494.1">
test_rag(qa, query)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.495.1">The result </span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.496.1">of running the preceding query will be:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.497.1"><img alt="" role="presentation" src="../Images/B20963_10_14.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.498.1">Figure 10.14: Query and answer using the RAG system (example 1)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.499.1">Next, we show the answer to a different query on the same content (the query included in the printout):</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.500.1"><img alt="" role="presentation" src="../Images/B20963_10_15.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.501.1">Figure 10.15: Query and answer using the RAG system (example 2)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.502.1">We can also retrieve the documents that were used to create the context for the answer. </span><span class="koboSpan" id="kobo.502.2">The following code does just that:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.503.1">docs = vectordb.similarity_search(query)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.504.1">print</span></span><span class="koboSpan" id="kobo.505.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.506.1">f"Query: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.507.1">{query}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.508.1">"</span></span><span class="koboSpan" id="kobo.509.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.510.1">print</span></span><span class="koboSpan" id="kobo.511.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.512.1">f"Retrieved documents: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.513.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.514.1">len</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.515.1">(docs)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.516.1">"</span></span><span class="koboSpan" id="kobo.517.1">)
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.518.1">for</span></span><span class="koboSpan" id="kobo.519.1"> doc </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.520.1">in</span></span><span class="koboSpan" id="kobo.521.1"> docs:
    doc_details = doc.to_json()[</span><span class="hljs-string"><span class="koboSpan" id="kobo.522.1">'kwargs'</span></span><span class="koboSpan" id="kobo.523.1">]
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.524.1">print</span></span><span class="koboSpan" id="kobo.525.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.526.1">"Source: "</span></span><span class="koboSpan" id="kobo.527.1">, doc_details[</span><span class="hljs-string"><span class="koboSpan" id="kobo.528.1">'metadata'</span></span><span class="koboSpan" id="kobo.529.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.530.1">'source'</span></span><span class="koboSpan" id="kobo.531.1">])
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.532.1">print</span></span><span class="koboSpan" id="kobo.533.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.534.1">"Text: "</span></span><span class="koboSpan" id="kobo.535.1">, doc_details[</span><span class="hljs-string"><span class="koboSpan" id="kobo.536.1">'page_content'</span></span><span class="koboSpan" id="kobo.537.1">], </span><span class="hljs-string"><span class="koboSpan" id="kobo.538.1">"\n"</span></span><span class="koboSpan" id="kobo.539.1">)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.540.1">RAG is a </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.541.1">powerful method to leverage the capability of LLMs for reasoning, while controlling what the source of information is. </span><span class="koboSpan" id="kobo.541.2">The answer given by the LLM is only from the context extracted by similarity search (in the first step of a question-and-answer retrieval chain), from the vector database where we stored the information.</span></p>
<h1 class="heading-1" id="_idParaDest-145"><span class="koboSpan" id="kobo.542.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.543.1">In this chapter, we explored how we can leverage the potential of Generative AI, using LLMs from Kaggle Models. </span><span class="koboSpan" id="kobo.543.2">We started by focusing on the simplest way to use such Foundation Models – by directly prompting them. </span><span class="koboSpan" id="kobo.543.3">We learned that crafting a prompt is important and experimented with simple math questions. </span><span class="koboSpan" id="kobo.543.4">We used the models that were available in Kaggle Models as well as quantized ones and quantized models with two approaches: using </span><code class="inlineCode"><span class="koboSpan" id="kobo.544.1">Llama.cpp</span></code><span class="koboSpan" id="kobo.545.1"> and the </span><code class="inlineCode"><span class="koboSpan" id="kobo.546.1">bitsandbytes</span></code><span class="koboSpan" id="kobo.547.1"> library. </span><span class="koboSpan" id="kobo.547.2">We then combined Langchain with a LLM to create sequences of chained tasks, where the output of one task is used to craft (by the framework) the input (or prompt) for the next task. </span><span class="koboSpan" id="kobo.547.3">Using the Code Llama 2 model, we tested the feasibility of code generation on Kaggle. </span><span class="koboSpan" id="kobo.547.4">The results were less than perfect, with multiple sequences generated besides the expected one. </span><span class="koboSpan" id="kobo.547.5">Finally, we learned how to create a RAG system that combines the speed, versatility, and ease of using vector databases with the chaining functions of Langchain and the reasoning capabilities of LLMs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.548.1">In the next chapter, which is also the last chapter of our book, you will learn a few useful recipes that will help you to make your high-quality work on the platform more visible and appreciated.</span></p>
<h1 class="heading-1" id="_idParaDest-146"><span class="koboSpan" id="kobo.549.1">References</span></h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.550.1">Llama 2, Kaggle Models: </span><a href="https://www.kaggle.com/models/metaresearch/llama-2"><span class="url"><span class="koboSpan" id="kobo.551.1">https://www.kaggle.com/models/metaresearch/llama-2</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.552.1">Mistral, Kaggle Models: </span><a href="https://www.kaggle.com/models/mistral-ai/mistral/"><span class="url"><span class="koboSpan" id="kobo.553.1">https://www.kaggle.com/models/mistral-ai/mistral/</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.554.1">Gabriel Preda – Test Llama v2 with math, Kaggle Notebooks: </span><a href="https://www.kaggle.com/code/gpreda/test-llama-v2-with-math"><span class="url"><span class="koboSpan" id="kobo.555.1">https://www.kaggle.com/code/gpreda/test-llama-v2-with-math</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.556.1">Model Quantization, HuggingFace: </span><a href="https://huggingface.co/docs/optimum/concept_guides/quantization"><span class="url"><span class="koboSpan" id="kobo.557.1">https://huggingface.co/docs/optimum/concept_guides/quantization</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.558.1">Gabriel Preda – Test Llama 2 quantized with Llama.cpp, Kaggle Notebooks: </span><a href="https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp"><span class="url"><span class="koboSpan" id="kobo.559.1">https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.560.1">Gabriel Preda – Test of Llama 2 quantized with llama.cpp (on CPU), Kaggle Notebooks: </span><a href="https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu"><span class="url"><span class="koboSpan" id="kobo.561.1">https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.562.1">Gabriel Preda – Simple sequential chain with Llama 2 and Langchain, Kaggle Notebooks: </span><a href="https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain/"><span class="url"><span class="koboSpan" id="kobo.563.1">https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain/</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.564.1">Langchain, Wikipedia page: </span><a href="https://en.wikipedia.org/wiki/LangChain"><span class="url"><span class="koboSpan" id="kobo.565.1">https://en.wikipedia.org/wiki/LangChain</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.566.1">Gabriel Preda – Use Code Llama to generate Python code (13b), Kaggle Notebooks: </span><a href="https://www.kaggle.com/code/gpreda/use-code-llama-to-generate-python-code-13b"><span class="url"><span class="koboSpan" id="kobo.567.1">https://www.kaggle.com/code/gpreda/use-code-llama-to-generate-python-code-13b</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.568.1">Gabriel Preda – Retrieval Augmented Generation, Combining LLMs, Task-Chaining and Vector Databases, Endava Blog: </span><a href="https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases"><span class="url"><span class="koboSpan" id="kobo.569.1">https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases</span></span></a></li>
<li class="numberedList"><span class="koboSpan" id="kobo.570.1">Gabriel Preda – RAG using Llama 2, Langchain and ChromaDB, Kaggle Notebooks: </span><a href="https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb"><span class="url"><span class="koboSpan" id="kobo.571.1">https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb</span></span></a></li>
</ol>
<h1 class="heading-1"><span class="koboSpan" id="kobo.572.1">Join our book’s Discord space</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.573.1">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</span></p>
<p class="normal"><a href="https://packt.link/kaggle"><span class="url"><span class="koboSpan" id="kobo.574.1">https://packt.link/kaggle</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.575.1"><img alt="" role="presentation" src="../Images/QR_Code9220780366773140.png"/></span></p>
</div>
</body></html>