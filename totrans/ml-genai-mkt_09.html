<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer304">
    <h1 class="chapterNumber">9</h1>
    <h1 class="chapterTitle" id="_idParaDest-192">Creating Compelling Content with Zero-Shot Learning</h1>
    <p class="normal">Having introduced the promise of large language models in <em class="chapterRef">Chapter 5</em>, we will go deeper into related topics in this chapter, extending our analysis from their role in data augmentation and sentiment analysis to their broader impact across different domains. This chapter<a id="_idIndexMarker716"/> introduces <strong class="keyWord">zero-shot learning </strong>(<strong class="keyWord">ZSL</strong>), a method in machine learning where a model can correctly make predictions for new, unseen classes without having received any specific training examples for those classes. It discusses the potential of ZSL and its application within the area of generative AI to create marketing copy. The discussion highlights how ZSL, an efficient tool to complement traditional marketing content creation processes, can revolutionize the generation of marketing copy.</p>
    <p class="normal">We will start with an in-depth discussion of the core principles of generative AI and navigate through the capabilities and limitations of these technologies, which will set the stage for our subsequent exploration of the importance of pre-trained models. We will finish the chapter with a practical walkthrough of ZSL, using hands-on examples to illustrate the flexibility of this approach and how we can use it to generate marketing content. This will equip you with the skills to understand and leverage this technique to elevate your marketing strategies to new heights.</p>
    <p class="normal">By the end of the chapter, you will be well versed in:</p>
    <ul>
      <li class="bulletList">The fundamentals of generative AI and its versatile applications in marketing</li>
      <li class="bulletList">The principles of ZSL and its value in improving the efficiency of traditional content creation processes</li>
      <li class="bulletList">Practical strategies and considerations when applying ZSL to create marketing copy</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-193">Fundamentals of generative AI</h1>
    <p class="normal"><strong class="keyWord">Generative AI</strong> (<strong class="keyWord">GenAI</strong>) refers <a id="_idIndexMarker717"/>to a subset of AI capable of generating new content, be it text, images, videos, or even synthetic data, that mirrors real-world examples. Unlike traditional AI models, which are designed to interpret, classify, or predict data based on inputs, GenAI takes it a step further by creating new, previously unseen outputs. It does this by understanding and learning from existing data patterns to produce novel outputs that maintain a logical continuity with the input data.</p>
    <p class="normal">We were introduced to GenAI in <em class="chapterRef">Chapter 1</em>, and we further touched upon it and its applications for sentiment analysis in <em class="chapterRef">Chapter 5</em>. Before beginning our discussion of pre-trained models and ZSL, we will explore the fundamental technical considerations of GenAI, what it is (and is not), and why it’s so impactful for generating marketing content. While the focus of the hands-on examples in this chapter will involve text generation, important concepts that power GenAI’s capabilities in other applications such as images and video will also be discussed.</p>
    <h2 class="heading-2" id="_idParaDest-194">A probabilistic approach</h2>
    <p class="normal">At the heart of GenAI is a <a id="_idIndexMarker718"/>probabilistic approach to modeling data distributions. This involves learning the underlying probability distribution of a dataset to generate new samples from that same distribution. A cornerstone of this approach is Bayesian inference, a principle that updates the probability of a hypothesis as more evidence or information becomes available. For instance, consider a simple equation that forms the mathematical foundation on which Bayesian inference is built, Bayes’ Theorem:</p>
    <p class="center"><img alt="" src="../Images/B30999_09_001.png"/></p>
    <p class="normal">where:</p>
    <ul>
      <li class="bulletList"><em class="italic">P</em>(<em class="italic">A</em>∣<em class="italic">B</em>) is the posterior probability of hypothesis <em class="italic">A</em>, given the evidence <em class="italic">B</em></li>
      <li class="bulletList"><em class="italic">P</em>(<em class="italic">B</em>∣<em class="italic">A</em>) is the likelihood of observing evidence <em class="italic">B</em>, given that hypothesis <em class="italic">A</em> is true</li>
      <li class="bulletList"><em class="italic">P</em>(<em class="italic">A</em>) is the prior probability of hypothesis <em class="italic">A</em>, or how likely we believe <em class="italic">A</em> to be true before seeing the evidence</li>
      <li class="bulletList"><em class="italic">P</em>(<em class="italic">B</em>) is the probability of observing the evidence under all possible hypotheses<div class="note">
          <p class="normal"><strong class="keyWord">Bayes’ Theorem – a pillar of probabilistic reasoning</strong></p>
          <p class="normal">Bayes’ Theorem <a id="_idIndexMarker719"/>is not just a cornerstone of GenAI but also a fundamental principle across a wide range of disciplines, from statistics and computer science to philosophy and medicine. At its core, Bayes’ Theorem allows us to refine our hypotheses in light of new evidence, offering a rigorous mathematical approach to the concept of learning from experience.</p>
        </div>
      </li>
    </ul>
    <p class="normal">When extending the principles of Bayesian inference and incorporating deep learning models such <a id="_idIndexMarker720"/>as <strong class="keyWord">recurrent neural networks</strong> (<strong class="keyWord">RNNs</strong>), <strong class="keyWord">long short-term memory networks</strong> (<strong class="keyWord">LSTMs</strong>), or <a id="_idIndexMarker721"/>transformers to the generation of sequences, we enter the realm of conditional probabilities. This sequence generation process can be viewed through the lens of predicting <a id="_idIndexMarker722"/>each element based on its predecessors, a concept foundational not just to video and audio but also to time series modeling and other forms of sequential data generation applications, including text.</p>
    <p class="normal">Training GenAI models involves vast amounts of data, and in the case of text, these must be broken down into smaller units known as tokens. These tokens often consist of subword units or phrases, making the models more efficient in understanding and generating natural language. The process of tokenizing text is crucial because it allows a model to learn the probability distribution of different sequences of words or subwords.</p>
    <p class="normal">When we tokenize text, we break it down into manageable pieces that a model can process. Each token is then used as an input to the model during training. The model learns to predict the probability of the next token in a sequence, given the previous tokens. This probabilistic approach is where Bayesian principles come into play. By continuously updating the probability distribution of tokens as new data is introduced, the model becomes better at generating coherent and contextually relevant outputs.</p>
    <p class="normal">For example, in text generation, a model might predict the next word in a sentence based on the preceding words. This prediction process involves calculating the conditional probability:</p>
    <p class="center"><img alt="" src="../Images/B30999_09_002.png"/></p>
    <p class="normal">where <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub> represents the token at time <em class="italic">t</em> and <em class="italic">P</em>(<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>∣<em class="italic">x</em><sub class="subscript">1</sub>,<em class="italic">x</em><sub class="subscript">2</sub>,…,<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>) denotes the probability of generating <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, given the sequence of all preceding tokens.</p>
    <p class="normal">In the case of video and audio generation, leveraging deep learning models informed by Bayesian principles helps in understanding and predicting temporal progression. Each frame or audio sample at time <em class="italic">t</em>(<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>)is predicated on the sequence of all previous frames or samples (<em class="italic">x</em><sub class="subscript">1</sub>,<em class="italic">x</em><sub class="subscript">2</sub>,…,<em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t-1</sub>). Mathematically, this relationship is captured by the previous equation:</p>
    <p class="normal">where <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub> represents the frame or audio sample at time <em class="italic">t</em> and <img alt="" src="../Images/B30999_09_004.png"/> denotes the probability of <a id="_idIndexMarker723"/>generating <em class="italic">x</em><sub class="subscript-italic" style="font-style: italic;">t</sub>, given the sequence of all preceding samples.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Source code and data</strong>:</p>
      <p class="normal"><a href="https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.9"><span class="url">https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.9</span></a></p>
    </div>
    <h2 class="heading-2" id="_idParaDest-195">Foundational models</h2>
    <p class="normal">There are <a id="_idIndexMarker724"/>several important foundational models in GenAI, each contributing uniquely to applications in image, text, and sequence generation. This section will cover some of the most important and widely used cases, such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Generative adversarial networks</strong> (<strong class="keyWord">GANs</strong>)</li>
      <li class="bulletList"><strong class="keyWord">Variational autoencoders</strong> (<strong class="keyWord">VAEs</strong>)</li>
      <li class="bulletList"><strong class="keyWord">Long short-term memory networks</strong> (<strong class="keyWord">LSTMs</strong>)</li>
      <li class="bulletList">Transformer-based models like the <strong class="keyWord">Generative Pre-Trained Transformer</strong> (<strong class="keyWord">GPT</strong>)</li>
    </ul>
    <p class="normal">While comprehensive implementation examples and theory for each of these models are outside the scope of this chapter, we will discuss the core concepts for each model type, as well as provide simplified, illustrative examples of their architectures, in order to understand the importance of these models for marketing applications. In <em class="chapterRef">Chapter 12</em>, we will extend our discussion to mention further model advances that have garnered more recent <a id="_idIndexMarker725"/>attention for their promise in advancing the field of GenAI.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Exploring ML models with Google Colab notebooks</strong></p>
      <p class="normal">Training your<a id="_idIndexMarker726"/> own state-of-the-art ML model can be computationally expensive. However, using Google Colab notebooks, you can train and tweak models without any setup on your own machine. The following are links to get started:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">GANs</strong> for<strong class="keyWord"> </strong>high-quality image generation: <a href="https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu "><span class="url">https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu</span></a></li>
        <li class="bulletList"><strong class="keyWord">VAEs</strong> for image reconstruction: <a href="https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb "><span class="url">https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb</span></a></li>
        <li class="bulletList"><strong class="keyWord">GPTs</strong> for language processing: <a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing "><span class="url">https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing</span></a></li>
        <li class="bulletList"><strong class="keyWord">LSTMs</strong> for time series forecasting: <a href="https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/lstm.ipynb"><span class="url">https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/lstm.ipynb</span></a></li>
      </ul>
    </div>
    <h3 class="heading-3" id="_idParaDest-196">Generative adversarial networks</h3>
    <p class="normal">GANs have found <a id="_idIndexMarker727"/>applications across a <a id="_idIndexMarker728"/>wide range of domains, from image generation and style transfer to data augmentation and beyond. They are particularly impactful in applications where realistic image generation is crucial, and they have been used by NVIDIA and Adobe in their photo editing software to generate and modify images. Their applications include the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Content creation</strong>: GANs can generate high-quality, realistic images, artwork, and videos, enabling new forms of creative content production</li>
      <li class="bulletList"><strong class="keyWord">Image-to-image translation</strong>: Applications like photo enhancement, photo-realistic rendering from sketches, and domain adaptation, such as day-to-night and summer-to-winter transformations, leverage GANs to transform images from one domain to another while preserving contextual details</li>
    </ul>
    <p class="normal">At their core, GANs consist of two neural networks that are trained simultaneously through a competitive process:</p>
    <ul>
      <li class="bulletList">The generator (<em class="italic">G</em>) aims to generate data that is indistinguishable from real data</li>
      <li class="bulletList">The discriminator (<em class="italic">D</em>) aims to accurately classify data as real or generated</li>
    </ul>
    <p class="normal">This can be illustrated by the following figure:</p>
    <figure class="mediaobject"><img alt="A diagram of a generator  Description automatically generated" src="../Images/B30999_09_01.png"/></figure>
    <p class="packt_figref">Figure 9.1: GAN workflow</p>
    <p class="normal">The objective <a id="_idIndexMarker729"/>function for a GAN encapsulates the<a id="_idIndexMarker730"/> training dynamics between the generator and discriminator, creating a dynamic where both models improve in response to each other’s performance. This is similar to a two-player game where each player’s success is based on outsmarting their opponent. The game, in GAN’s case, reaches equilibrium when the generator produces perfect replicas of the real data, making it impossible for the discriminator to distinguish real from fake, which ideally results in a <code class="inlineCode">0.5</code> probability of guessing correctly by the discriminator.</p>
    <p class="normal">For GANs to be highly effective at tasks such as high-resolution image generation, both the generator and discriminator architectures must be carefully designed. This can involve incorporating advanced architectures that are effective at handling spatial hierarchy data, such as <strong class="keyWord">convolutional neural networks</strong> (<strong class="keyWord">CNNs</strong>).</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">What are CNNs?</strong></p>
      <p class="normal">CNNs are a<a id="_idIndexMarker731"/> cornerstone of machine learning for processing spatial data, such as images. They identify patterns using convolutional filters, excelling in tasks that require an understanding of spatial hierarchies. This makes CNNs indispensable in many GAN applications for image generation and recognition.</p>
    </div>
    <p class="normal">In image-based GAN applications, the generator uses techniques to expand latent representations into detailed images, while the discriminator applies methods to reduce the dimensionality of the input image to assess its authenticity efficiently. The latent dimension, serving as the seed to generate new data instances, is a compact, high-dimensional space, encapsulating potential data variations in a compressed format.</p>
    <p class="normal">The following code shows the process for building the core structure of a simplified GAN for images, using<a id="_idIndexMarker732"/> Python, with the key steps <a id="_idIndexMarker733"/>described here:</p>
    <ol>
      <li class="numberedList" value="1">Import the libraries needed to build the generator and discriminator:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense
</code></pre>
      </li>
      <li class="numberedList">Define the generator model, which takes a latent space vector and produces a 28x28 image through a series of dense layers:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_generator</span>(<span class="hljs-params">latent_dim</span>):
    model = Sequential([
        Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>, input_dim=latent_dim),  <span class="hljs-comment"># maps from latent space</span>
        Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>),  <span class="hljs-comment"># expands representation</span>
        Dense(<span class="hljs-number">784</span>, activation=<span class="hljs-string">'tanh'</span>)  <span class="hljs-comment"># produces 28x28 image</span>
    ])
    <span class="hljs-keyword">return</span> model
</code></pre>
      </li>
      <li class="numberedList">Define the discriminator model, which takes an image and classifies it as real or generated through a series of dense layers:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_discriminator</span>(<span class="hljs-params">input_shape</span>):
    model = Sequential([
        Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>, input_shape=(input_shape,)),  <span class="hljs-comment"># processes input image</span>
        Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>),  <span class="hljs-comment"># reduces dimensionality</span>
        Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)  <span class="hljs-comment"># classifies input as real or generated</span>
    ])
    <span class="hljs-keyword">return</span> model
Define the latent dimension <span class="hljs-keyword">and</span> initialize the models:
latent_dim = <span class="hljs-number">100</span>  <span class="hljs-comment"># size of input noise vector</span>
generator = build_generator(latent_dim)
discriminator = build_discriminator(<span class="hljs-number">784</span>)  <span class="hljs-comment"># for 28x28 images</span>
</code></pre>
      </li>
    </ol>
    <p class="normal">Further technical considerations are necessary to address common challenges, such as limited output diversity and ensuring that the generated data is varied while still closely mirroring the real data distribution. These considerations include the choice of the activation function (<code class="inlineCode">relu</code> for non-linear transformations), techniques to ensure consistent input distribution across layers, and strategies such as randomly omitting units during <a id="_idIndexMarker734"/>training to prevent <a id="_idIndexMarker735"/>overfitting. </p>
    <p class="normal">For more details on this, you can refer to the recent paper on GANs: <a href="https://www.researchgate.net/publication/380573076_Understanding_GANs_fundamentals_variants_training_challenges_applications_and_open_problems"><span class="url">https://www.researchgate.net/publication/380573076_Understanding_GANs_fundamentals_variants_training_challenges_applications_and_open_problems</span></a>.</p>
    <h3 class="heading-3" id="_idParaDest-197">Variational autoencoders</h3>
    <p class="normal">VAEs present a different <a id="_idIndexMarker736"/>approach to <a id="_idIndexMarker737"/>generative modeling as compared to GANs. VAEs offer a probabilistic way of learning latent representations of data and consist of two main components:</p>
    <ul>
      <li class="bulletList">The encoder compresses input data into a latent space representation</li>
      <li class="bulletList">The decoder reconstructs the data from this latent space</li>
    </ul>
    <p class="normal">Unlike traditional autoencoders, VAEs introduce a probabilistic twist where, rather than encoding input as a single point, they encode it as a distribution over the latent space.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">The versatile applications of VAEs</strong></p>
      <p class="normal">VAEs are instrumental <a id="_idIndexMarker738"/>in understanding and modeling complex distributions of data. One key area where VAEs excel is in data imputation, where they can predict missing information or forecast future trends in time-series data.</p>
    </div>
    <p class="normal">The loss function for VAEs combines <a id="_idIndexMarker739"/>reconstruction loss with the <strong class="keyWord">Kullback-Leibler</strong> (<strong class="keyWord">KL</strong>) divergence between the learned latent variable distribution and the prior distribution. The reconstruction loss measures how well the generated outputs match the original inputs, ensuring that a model creates accurate reproductions of the data, and the KL divergence acts as a form of regularization during training that ensures the model learns efficient and meaningful data representations. This regularization prevents the model from overfitting by encouraging it to generate outputs that are not just accurate but also generalize well to new, unseen data. By combining these two components, the VAE learns to produce high-quality, diverse outputs and encourages the model to learn efficient encodings of the data.</p>
    <p class="normal">The following is <a id="_idIndexMarker740"/>a simplified example of<a id="_idIndexMarker741"/> constructing a VAE using Keras:</p>
    <ol>
      <li class="numberedList" value="1">We will use a flattened 28x28 image input as another example, and we will first sample from the latent distribution in the <code class="inlineCode">sampling()</code> function:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Input, Lambda
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> Model, backend <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">def</span> <span class="hljs-title">sampling</span>(<span class="hljs-params">args</span>):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[<span class="hljs-number">0</span>]
    dim = K.int_shape(z_mean)[<span class="hljs-number">1</span>]
    epsilon = K.random_normal(shape=(batch, dim))
    <span class="hljs-keyword">return</span> z_mean + K.exp(<span class="hljs-number">0.5</span> * z_log_var) * epsilon
</code></pre>
      </li>
      <li class="numberedList">We then use the encoder to map the inputs into latent space:
        <pre class="programlisting code-one"><code class="hljs-code">inputs = Input(shape=(<span class="hljs-number">784</span>,)) <span class="hljs-comment"># Encoder</span>
h = Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>)(inputs)
z_mean = Dense(<span class="hljs-number">2</span>)(h)
z_log_var = Dense(<span class="hljs-number">2</span>)(h)
z = Lambda(sampling, output_shape=(<span class="hljs-number">2</span>,))([z_mean, z_log_var])
</code></pre>
      </li>
      <li class="numberedList">We then use the decoder to reconstruct the image from the latent space:
        <pre class="programlisting code-one"><code class="hljs-code">decoder_h = Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'relu'</span>) <span class="hljs-comment"># Decoder</span>
decoder_mean = Dense(<span class="hljs-number">784</span>, activation=<span class="hljs-string">'sigmoid'</span>)
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)
vae = Model(inputs, x_decoded_mean)
</code></pre>
      </li>
    </ol>
    <p class="normal">For the effective application of VAEs, selecting the right architecture for the encoder and decoder is crucial, often involving densely connected layers for basic tasks or more sophisticated structures, like CNNs, for image data. The dimensionality of the latent space is also vital – it must be large enough to capture relevant data variations but not so large that it leads to overfitting or meaningless reconstructions. When designed correctly, VAEs offer a principled approach to generative modeling, balancing the need for accurate data <a id="_idIndexMarker742"/>reconstruction with<a id="_idIndexMarker743"/> the flexibility to generate new, diverse samples from learned data distributions.</p>
    <h3 class="heading-3" id="_idParaDest-198">Long short-term memory networks</h3>
    <p class="normal">LSTMs are a specialized<a id="_idIndexMarker744"/> type of RNN<a id="_idIndexMarker745"/> designed to learn long-term dependencies in sequential data. RNNs are a class of neural networks that include loops, allowing information to persist by passing it from one step of the network to the next. This looping mechanism makes RNNs suitable for processing sequences of data such as time series or text. However, standard RNNs often struggle with learning long-range dependencies, due to issues like vanishing and exploding gradients. This occurs because, during backpropagation, gradients can become exponentially small (vanish) or large (explode), making it difficult to update the network weights effectively. LSTMs address these challenges with a more complex internal structure that allows them to remember information for longer periods effectively.</p>
    <p class="normal">The defining feature of LSTMs that enables them to remember information more effectively is their cell state, along with three types of gates: input, output, and forget gates. These components work together to regulate the flow of information, allowing a network to remember important information over long periods and to forget irrelevant data.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Key components of an LSTM</strong></p>
      <p class="normal">Core to the <a id="_idIndexMarker746"/>LSTM are its cell state and gates with the following functions:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Input gate</strong>: How much new information to store in the cell state</li>
        <li class="bulletList"><strong class="keyWord">Forget gate</strong>: What information is discarded from the cell state</li>
        <li class="bulletList"><strong class="keyWord">Output gate</strong>: The<a id="_idIndexMarker747"/> output of the cell state to the next layer</li>
      </ul>
    </div>
    <p class="normal">The following code sets up a simple LSTM network for time-series prediction that could be used to predict the next day’s sales, based on different features from the previous day. In this architecture, we allow for 10 days to be captured by <code class="inlineCode">sequence_length</code> and then 5 features by <code class="inlineCode">num_feature</code>, which could include data points such as web traffic or previous sales. The LSTM layer with 50 units learns to recognize patterns in the sequence data, while the <code class="inlineCode">Dense</code> layer outputs the prediction. Finally, the model is compiled with the Adam optimizer and mean squared error (<code class="inlineCode">mse</code>) loss function, common choices for regression tasks:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> LSTM
sequence_length = <span class="hljs-number">10</span>
num_features = <span class="hljs-number">5</span>
model = Sequential([
    LSTM(<span class="hljs-number">50</span>, activation=<span class="hljs-string">'relu'</span>, input_shape=(sequence_length, num_features)),
    Dense(<span class="hljs-number">1</span>)
])
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>)
</code></pre>
    <p class="normal">To demonstrate the training process, we can generate synthetic time-series data that mimics sales <a id="_idIndexMarker748"/>prediction data. The synthetic<a id="_idIndexMarker749"/> data includes a base sine wave with added noise, simulating daily patterns with random fluctuations:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">def</span> <span class="hljs-title">generate_synthetic_data</span>(<span class="hljs-params">num_samples, sequence_length, num_features</span>):
    X = []
    y = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_samples):
        base = np.array([np.sin(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(sequence_length)]) + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, sequence_length)
        features = np.tile(base, (num_features, <span class="hljs-number">1</span>)).T + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>, (sequence_length, num_features))
        target = np.<span class="hljs-built_in">sum</span>(base) + np.random.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.1</span>)
        X.append(features)
        y.append(target)
    <span class="hljs-keyword">return</span> np.array(X), np.array(y)
X_train, y_train = generate_synthetic_data(<span class="hljs-number">100</span>, sequence_length, num_features)
model.fit(X_train, y_train, epochs=<span class="hljs-number">10</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">After training, we can evaluate the model on test data by comparing the predicted sales values with the actual values:</p>
    <pre class="programlisting code"><code class="hljs-code">X_test, y_test = generate_synthetic_data(<span class="hljs-number">10</span>, sequence_length, num_features)
y_pred = model.predict(X_test)
plt.plot(y_test, label=<span class="hljs-string">'Actual'</span>)
plt.plot(y_pred, label=<span class="hljs-string">'</span><span class="hljs-string">Predicted'</span>)
plt.xlabel(<span class="hljs-string">'Days'</span>)
plt.ylabel(<span class="hljs-string">'Sales Prediction'</span>)
plt.title(<span class="hljs-string">'Actual vs Predicted Sales Over Time'</span>)
plt.legend()
plt.show()
</code></pre>
    <p class="normal">The code <a id="_idIndexMarker750"/>produces<a id="_idIndexMarker751"/> the following plot, showing how a simple LSTM model with training can quickly provide useful predictions for metrics, such as sales, if given appropriate input features:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_09_02.png"/></figure>
    <p class="packt_figref">Figure 9.2: Output of an LSTM sales prediction model</p>
    <h3 class="heading-3" id="_idParaDest-199">Transformers</h3>
    <p class="normal">Transformer-based<a id="_idIndexMarker752"/> models, such as<a id="_idIndexMarker753"/> the GPT series, have revolutionized natural language processing by introducing a model architecture that excels in capturing context and relationships within data. The core innovation of transformer models is the attention mechanism, which enables a model to weigh the importance of different parts of the input data differently. This mechanism allows GPT and similar models to understand the context and generate text that is coherent and contextually relevant.</p>
    <p class="normal">GPT models leverage the transformer architecture for generative tasks, trained on vast amounts of text data to understand language patterns, grammar, and context. This pre-training enables GPT models to generate text that is highly coherent and contextually relevant to the input prompts. While building a GPT from scratch is a formidable task, at the conceptual level, there are some key components within a GPT architecture:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Embedding layer</strong>: Converts <a id="_idIndexMarker754"/>token indices to dense vectors of fixed size</li>
      <li class="bulletList"><strong class="keyWord">Multi-head attention</strong>: Allows a model to focus on different parts of the input sequence simultaneously, capturing various contextual relationships</li>
      <li class="bulletList"><strong class="keyWord">Layer normalization and residual connections</strong>: Help stabilize and optimize the training process, ensuring that gradients flow smoothly throughout a network</li>
    </ul>
    <p class="normal">The following code shows how a simplified GPT-like architecture can be created using Keras with the <a id="_idIndexMarker755"/>aforementioned<a id="_idIndexMarker756"/> components:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Embedding, LayerNormalization, MultiHeadAttention
<span class="hljs-keyword">def</span> <span class="hljs-title">simplified_gpt_model</span>(<span class="hljs-params">vocab_size=</span><span class="hljs-number">10000</span><span class="hljs-params">, embed_dim=</span><span class="hljs-number">256</span><span class="hljs-params">, max_length=</span><span class="hljs-number">40</span><span class="hljs-params">, num_heads=</span><span class="hljs-number">4</span><span class="hljs-params">, ff_dim=</span><span class="hljs-number">512</span>)
    inputs = Input(shape=(max_length,))  <span class="hljs-comment"># input layer for sequences of tokens</span>
    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs) <span class="hljs-comment"># convert token indices to vectors</span>
    <span class="hljs-comment"># self-Attention layer with multiple heads</span>
    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(embedding_layer, embedding_layer)
    <span class="hljs-comment"># normalization and residual connection for the attention output</span>
    attn_output = LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)(attn_output + embedding_layer)
    <span class="hljs-comment"># feed Forward network to processes attention output</span>
    ff_network = Dense(ff_dim, activation=<span class="hljs-string">"relu"</span>)(attn_output)
    ff_network_output = Dense(embed_dim)(ff_network)
    <span class="hljs-comment"># second normalization and residual connection</span>
    sequence_output = LayerNormalization(epsilon=<span class="hljs-number">1e-6</span>)(ff_network_output + attn_output)
    <span class="hljs-comment"># output layer to predict the next token in the sequence</span>
    outputs = Dense(vocab_size, activation=<span class="hljs-string">"</span><span class="hljs-string">softmax"</span>)(sequence_output)
    model = Model(inputs=inputs, outputs=outputs)
    <span class="hljs-keyword">return</span> model
gpt_model = simplified_gpt_model()
gpt_model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>)
</code></pre>
    <div class="note">
      <p class="normal"><strong class="keyWord">The power of self-attention in GPT</strong></p>
      <p class="normal">Self-attention, the key innovation behind GPT models, allows a network to weigh the importance of different words in a sentence, enhancing its understanding<a id="_idIndexMarker757"/> of context<a id="_idIndexMarker758"/> and relationships between words.</p>
      <p class="normal">To learn more, check out the paper <em class="italic">Attention Is All You Need</em> by Illia Polosukhin et al. (<a href="https://arxiv.org/pdf/1706.03762"><span class="url">https://arxiv.org/pdf/1706.03762</span></a>).</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-200">When GenAI is the right fit</h2>
    <p class="normal">While GenAI brings<a id="_idIndexMarker759"/> new realms of possibility to content creation and digital marketing, understanding its limitations is also crucial. GenAI shines in environments that demand innovation, creativity, and the ability to scale personalized content dynamically. More generally, GenAI can be a great fit<strong class="keyWord"> </strong>for your marketing campaign in the following cases:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Brainstorming for creative campaigns</strong>: Generating unique and compelling content, be it text, image, or video, to facilitate brainstorming for creative marketing campaigns that stand out in a crowded digital landscape. For example, we will use GenAI to generate text for a new product launch in this chapter and <em class="chapterRef">Chapter 10</em>.</li>
      <li class="bulletList"><strong class="keyWord">Dynamic content personalization</strong>: Enabling marketers to tailor content at scale while still addressing individual user preferences and behaviors, in order to increase engagement and conversion rates. For instance, we will show how GenAI combined <a id="_idIndexMarker760"/>with <strong class="keyWord">retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) can be used to create personalized recommendations and email content, based on individual browsing history and purchase behavior, in <em class="chapterRef">Chapter 11</em>.</li>
      <li class="bulletList"><strong class="keyWord">Efficiency in content production</strong>: Automating the content generation process, significantly reducing the time and resources needed to produce marketing materials.</li>
    </ul>
    <p class="normal">However, GenAI isn’t a one-size-fits-all solution, particularly in marketing scenarios where accuracy and deep contextual understanding are needed. For example, its application in highly regulated industries or in sensitive campaigns around social issues, where a misstep can significantly impact a brand’s reputation, must be done with caution. While GenAI can help with brainstorming in these cases, the unpredictability of GenAI content could pose significant risks if deployed without careful monitoring.<strong class="keyWord"> </strong>Further discussion of this topic will be presented in <em class="chapterRef">Chapter 13</em>, along with strategies to improve its contextual<a id="_idIndexMarker761"/> understanding in <em class="chapterRef">Chapters 10</em> and <em class="chapterRef">11</em>.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">GenAI in highly regulated industries</strong></p>
      <p class="normal">When applying GenAI in sectors like healthcare, financial services, insurance, and legal services, adherence to stringent regulatory standards is crucial. Marketing content for these applications requires extra scrutiny, as they must not only be accurate and transparent but also align with industry-specific compliance measures.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-201">Introduction to pre-trained models and ZSL</h1>
    <p class="normal">Building on the <a id="_idIndexMarker762"/>foundations of GenAI discussed in the chapter so far, we will now introduce <a id="_idIndexMarker763"/>some core concepts related to pre-trained models and <strong class="keyWord">zero-shot learning</strong> (<strong class="keyWord">ZSL</strong>). These concepts underly how models can take vast amounts of existing data to create realistic, new outputs for scenarios that have not yet been encountered, with little to no additional training. With a focus on text data, we will discuss how contextual embeddings and semantic proximity are two key concepts that facilitate this capability. With this knowledge, you will be equipped to understand and apply these concepts in this chapter and the ones to come.</p>
    <h2 class="heading-2" id="_idParaDest-202">Contextual embeddings</h2>
    <p class="normal">Contextual<a id="_idIndexMarker764"/> embeddings, enabled by advancements such as the LSTM and GPT models discussed earlier, are fundamental to <a id="_idIndexMarker765"/>how <strong class="keyWord">large language models</strong> (<strong class="keyWord">LLM</strong>s) interpret and generate language. As discussed in <em class="chapterRef">Chapter 5</em>, embeddings are dense vector representations of data that capture key features in a high-dimensional space. Early models like Word2Vec and GloVe generate static embeddings where the same word always has the same vector. In contrast, advanced models like BERT and GPT create contextual embeddings, where word representations change based on their usage in context. Effective NLP embeddings preserve the semantic relationships of the original data, meaning similar vectors are closer together in vector space. This adaptability is foundational for applications such as ZSL, which rely on a model’s ability to apply learned knowledge to new tasks without specific training data.</p>
    <p class="normal">Earlier in the chapter in the exploration of GenAI’s probabilistic nature, we noted how text generation is analogous to sequence prediction in video or audio, in that the relevance of each piece of data depends on its predecessors. As an analogy, consider how Google’s auto-suggest feature adapts suggestions based on the context of the words already entered. This same concept underpins the transformative potential of models like BERT, which analyzes text from both preceding and subsequent contexts to enhance language comprehension and prediction accuracy, via their contextual embeddings.</p>
    <p class="normal">GPT models take it a step further and adopt an autoregressive framework. The term “autoregressive” means that a model makes predictions based on its own previous outputs, meaning that it anticipates the subsequent word based on all preceding outputs of the model as context. For example, when developing a content calendar for a marketing blog, a GPT model can analyze past articles and trending topics to suggest new posts that align with the brand’s voice and audience interests. This is unlike transformer-based models, which can look at both preceding and following words simultaneously, a<a id="_idIndexMarker766"/>s discussed earlier in the chapter. However, such autoregressive models can offer more nuanced text generation, enabling them to create narratives with a level of coherence that bidirectional models may not achieve as seamlessly.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">The importance of contextual embeddings</strong></p>
      <p class="normal">Contextual <a id="_idIndexMarker767"/>embeddings from LSTM or GPT models allow for nuanced understanding by evaluating more of the text in its entirety.</p>
      <p class="normal">For instance, in filling in the blank in “The stormy seas calmed as the ___ sailed into the harbor,” a model leveraging both prior and subsequent context could infer “ship” with greater accuracy, whereas a more naive model with only prior context might inaccurately predict the word “day.”</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-203">Semantic proximity</h2>
    <p class="normal">Transitioning from our<a id="_idIndexMarker768"/> discussion on contextual embeddings and their critical role in language models, we will now explore <strong class="keyWord">semantic proximity</strong>. Contextual embeddings not only enhance the understanding of text by considering its dynamic contexts; they also serve as a fundamental tool in evaluating the semantic relationships between words or phrases within that text. This nuanced understanding is pivotal when we examine the concept of semantic proximity, which involves quantifying how closely related or distant two linguistic items are in meaning.</p>
    <p class="normal">For example, consider the phrases “limited-time offer” and “exclusive deal.” These phrases have close semantic proximity because they both relate to targeted promotions for potential customers. Conversely, the phrases “limited-time offer” and “customer feedback” would have a much larger semantic distance.</p>
    <p class="normal">Semantic proximity is effectively assessed through methods such as cosine similarity, which measures the angle between vectors representing these items in a high-dimensional space. This metric, rooted in the geometry of vector spaces, provides a clear, mathematical way to capture and compare the meanings encoded by embeddings. Mathematically, cosine similarity is given by:</p>
    <p class="center"><img alt="" src="../Images/B30999_09_005.png"/></p>
    <p class="normal">where <em class="italic">A</em> and <em class="italic">B</em> are vectors (semantic embeddings for a word, phrase, document, etc), <em class="italic">A</em>⋅<em class="italic">B</em> denotes their dot product, and ∥<em class="italic">A</em>∥ and ∥<em class="italic">B</em>∥ represent their magnitudes. The value of cosine similarity ranges from -1 to 1. A value of 1 implies that the vectors are identical in direction, indicating maximum similarity. A value of 0 implies that the vectors are orthogonal, indicating no similarity.</p>
    <p class="normal">While, in practice, many sophisticated text embeddings are high-dimensional and can range from hundreds to thousands of dimensions, we can more easily illustrate the concept of cosine similarity in 2D space via two vectors. In the following code, we illustrate the calculation of cosine similarity using vectors <em class="italic">A</em> and <em class="italic">B</em>, with the angle associated with their cosine similarity:</p>
    <pre class="programlisting code"><code class="hljs-code">A, B = np.array([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]), np.array([<span class="hljs-number">3</span>, <span class="hljs-number">0</span>])
cos_sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))
angle = np.arccos(cos_sim)
plt.figure(figsize=(<span class="hljs-number">6</span>,<span class="hljs-number">6</span>))
<span class="hljs-keyword">for</span> vector, color, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([A, B], [<span class="hljs-string">'red'</span>, <span class="hljs-string">'green'</span>], [<span class="hljs-string">'Vector A'</span>, <span class="hljs-string">'Vector B'</span>]):
    plt.quiver(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, vector[<span class="hljs-number">0</span>], vector[<span class="hljs-number">1</span>], angles=<span class="hljs-string">'xy'</span>, scale_units=<span class="hljs-string">'xy'</span>, scale=<span class="hljs-number">1</span>, color=color, label=label)
plt.plot(<span class="hljs-number">0.5</span> * np.cos(np.linspace(<span class="hljs-number">0</span>, angle, <span class="hljs-number">100</span>)), <span class="hljs-number">0.5</span> * np.sin(np.linspace(<span class="hljs-number">0</span>, angle, <span class="hljs-number">100</span>)), color=<span class="hljs-string">'blue'</span>, label=<span class="hljs-string">f'Cosine Similarity = </span><span class="hljs-subst">{cos_sim:</span><span class="hljs-number">.2</span><span class="hljs-subst">f}</span><span class="hljs-string">'</span>)
plt.axis([-<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">4</span>])
plt.axhline(<span class="hljs-number">0</span>, color=<span class="hljs-string">'</span><span class="hljs-string">black'</span>, linewidth=<span class="hljs-number">0.5</span>)
plt.axvline(<span class="hljs-number">0</span>, color=<span class="hljs-string">'black'</span>, linewidth=<span class="hljs-number">0.5</span>)
plt.grid(color=<span class="hljs-string">'gray'</span>, linestyle=<span class="hljs-string">'--'</span>, linewidth=<span class="hljs-number">0.5</span>)
plt.title(<span class="hljs-string">'Cosine Similarity between Vectors'</span>)
plt.legend()
plt.show()
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_09_03.png"/></figure>
    <p class="packt_figref">Figure 9.3: Visualization of cosine similarity between two vectors, A and B, showing their angular relationship in 2D space</p>
    <p class="normal">Continuing from the <a id="_idIndexMarker769"/>mathematical foundation of cosine similarity, we can apply this same concept to explore the polysemous nature of words in different contexts. Consider the word “light” used in two different sentences:</p>
    <p class="normal"><em class="italic">He turned on the</em> <strong class="bold-italic" style="font-style: italic;">light</strong> <em class="italic">to read</em>.</p>
    <p class="normal"><em class="italic">The</em> <strong class="bold-italic" style="font-style: italic;">light</strong> <em class="italic">fabric was perfect for summer</em>.</p>
    <p class="normal">These sentences demonstrate different semantic instances of the word “light,” and by employing contextual embedding models like BERT, we can quantify the semantic differences of “light” in these cases using cosine similarity.</p>
    <p class="normal">As an example, in the following code, we:</p>
    <ol>
      <li class="numberedList" value="1">Import libraries and load the pre-trained BERT model and tokenizer.</li>
      <li class="numberedList">Tokenize the sentences, converting them into the format expected by the BERT model.</li>
      <li class="numberedList">Pass the tokenized sentences through the BERT model to obtain the embeddings.</li>
      <li class="numberedList">Extract embeddings for the word “light” by finding the index of the word in each tokenized sentence and extracting its corresponding embedding from the model output.</li>
      <li class="numberedList">Compute cosine similarity and print the result.</li>
    </ol>
    <p class="normal">To do this, use the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">'bert-base-uncased'</span>)
model = TFBertModel.from_pretrained(<span class="hljs-string">'bert-base-uncased'</span>)
sentence_1 = <span class="hljs-string">"</span><span class="hljs-string">He turned on the light to read."</span>
sentence_2 = <span class="hljs-string">"The light fabric was perfect for summer."</span>
tokens_1 = tokenizer(sentence_1, return_tensors=<span class="hljs-string">"tf"</span>)
tokens_2 = tokenizer(sentence_2, return_tensors=<span class="hljs-string">"tf"</span>)
outputs_1 = model(tokens_1)
outputs_2 = model(tokens_2)
light_index_1 = tokens_1[<span class="hljs-string">'input_ids'</span>][<span class="hljs-number">0</span>].numpy().tolist().index(tokenizer.convert_tokens_to_ids(<span class="hljs-string">'light'</span>))
light_index_2 = tokens_2[<span class="hljs-string">'input_ids'</span>][<span class="hljs-number">0</span>].numpy().tolist().index(tokenizer.convert_tokens_to_ids(<span class="hljs-string">'light'</span>))
embedding_1 = outputs_1.last_hidden_state[<span class="hljs-number">0</span>, light_index_1]
embedding_2 = outputs_2.last_hidden_state[<span class="hljs-number">0</span>, light_index_2]
cosine_similarity = tf.keras.losses.cosine_similarity(embedding_1, embedding_2, axis=<span class="hljs-number">0</span>)
cosine_similarity = -cosine_similarity.numpy()
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Cosine similarity between 'light' embeddings in the two sentences:"</span>, cosine_similarity)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Cosine similarity between 'light' embeddings in the two sentences: 0.47577658
</code></pre>
    <p class="normal">A cosine similarity value ranges from -1 to 1. As mentioned earlier, a value of 1 indicates identical vectors, while a value of 0 implies orthogonal vectors with no similarity. In this case, a cosine<a id="_idIndexMarker770"/> similarity of 0.48 suggests that the embeddings for “light” in the two sentences are somewhat similar but not identical.</p>
    <h2 class="heading-2" id="_idParaDest-204">Pre-trained models</h2>
    <p class="normal">Pre-trained models<a id="_idIndexMarker771"/> are machine learning algorithms that have been previously trained on large datasets to perform general tasks, such as understanding natural language or recognizing objects in images. The utility of pre-trained models is fundamentally rooted in their use of the embeddings they were trained on. For text, these embeddings not only enable models to grasp context dynamically but also serve as the foundation to adapt these models for specific tasks, such as sentiment analysis, as discussed in <em class="chapterRef">Chapter 5</em>, with minimal to no additional training. This adaptability is critical for applications such as ZSL.</p>
    <p class="normal">The advent of pre-trained models democratized access to state-of-the-art AI by offering a base model that can be fine-tuned or used for inference directly. These models not only reduce the cost and time to deployment for AI-driven solutions but also the need for computational resources and energy consumption, making AI both more accessible and environmentally friendly. In the marketing domain, pre-trained models offer significant advantages. They enable marketers to quickly deploy advanced AI solutions for tasks such as personalized content creation, customer sentiment analysis, and targeted advertising.</p>
    <p class="normal">Sophisticated AI models that were previously attainable only for large or highly specialized technology companies are now a possibility for smaller-sized businesses, and even individual consumers, at a fraction of the cost. For perspective, training a model such as GPT-3 from scratch was estimated to be in the millions of dollars at the time of its release in 2020, a figure that encapsulates computational costs and human expertise. Today, a user can perform inference to generate text using this same (or a more advanced) model through the company’s API, at the cost of a few cents for hundreds of words of content.</p>
    <p class="normal">The key components of a pre-trained model are:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Weights</strong>: They<a id="_idIndexMarker772"/> represent the learned parameters from training datasets and encode a wide range of knowledge and patterns needed for transfer learning</li>
      <li class="bulletList"><strong class="keyWord">Architecture</strong>: The model’s structure, detailing how inputs are processed through various layers to generate outputs</li>
      <li class="bulletList"><strong class="keyWord">Pre-processing steps</strong>: Procedures like tokenization and normalization to ensure data compatibility with the model’s training</li>
    </ul>
    <p class="normal">Let’s look at these components in detail.</p>
    <h3 class="heading-3" id="_idParaDest-205">Model weights</h3>
    <p class="normal">Model weights are at the <a id="_idIndexMarker773"/>core of neural networks, including pre-trained models. Weights are the refined parameters developed through extensive training to minimize loss, acting as a repository of a model’s learned knowledge. For instance, in language models like GPT, these weights capture the intricacies of language, such as grammar and context, enabling the generation of text that’s not only coherent but also contextually rich. The effectiveness of pre-trained models in tasks like ZSL stems from these weights, which enable you to generalize from the model’s training data to new, unseen data with remarkable accuracy.</p>
    <p class="normal">To better understand where the model weights come from, consider the case of an <strong class="keyWord">artificial neural network</strong> (<strong class="keyWord">ANN</strong>), as<a id="_idIndexMarker774"/> shown in the following figure:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_09_04.png"/></figure>
    <p class="packt_figref">Figure 9.4: Example ANN architecture from Chapter 6</p>
    <p class="normal">This consists of three main layers: the input layer, the hidden layer, and the output layer. During training, neural networks undergo forward and backward propagation. Forward propagation involves feeding data through a network to generate an output, while backward propagation adjusts weights based on the error of the predictions. Through these iterations, the network learns the optimal weights for each neuron, minimizing prediction <a id="_idIndexMarker775"/>error and enhancing a model’s performance.</p>
    <h3 class="heading-3" id="_idParaDest-206">Model architecture</h3>
    <p class="normal">The architecture of a<a id="_idIndexMarker776"/> pre-trained model goes hand in hand with its weights. It delineates the structure of how data is processed and transformed across the model’s layers and also guides the adaptability of the model to perform novel tasks. For example, deeper language model architectures might be better suited for complex reasoning tasks, while models with specially configured attention mechanisms can offer finer control over the focus of the model during inference. For image recognition, the intermediate representations produced by these models, such as features extracted at various layers by a pre-trained CNN, can also serve as a valuable starting point for classification tasks.</p>
    <p class="normal">When understanding a machine learning model’s architecture, it can be helpful to plot it and its parameters to visualize its key aspects. Here, we illustrate this using Keras’s <code class="inlineCode">plot_model()</code>, as a demonstration of a simple LSTM model composed of two LSTM layers:</p>
    <pre class="programlisting code"><code class="hljs-code">!conda install pydot
!conda install graphviz
<span class="hljs-keyword">from</span> tensorflow.keras.utils <span class="hljs-keyword">import</span> plot_model
model = Sequential([
    LSTM(<span class="hljs-number">64</span>, return_sequences=<span class="hljs-literal">True</span>, input_shape=(<span class="hljs-number">10</span>, <span class="hljs-number">128</span>)),
    LSTM(<span class="hljs-number">64</span>),
    Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)
])
model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'categorical_crossentropy'</span>, metrics=[<span class="hljs-string">'</span><span class="hljs-string">accuracy'</span>])
plot_model(model, show_shapes=<span class="hljs-literal">True</span>, show_layer_names=<span class="hljs-literal">True</span>)
</code></pre>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_09_05.png"/></figure>
    <p class="packt_figref">Figure 9.5: A simple LSTM-based neural network architecture</p>
    <p class="normal">This visualization clearly delineates the model’s structure, showing the progression from input through two LSTM layers, each with 64 units (or neurons), to a final dense output layer configured with 10 softmax units for class prediction. Based on the labels given in the figure, the structure can be<a id="_idIndexMarker777"/> further broken down as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Input layer (lstm_input)</strong>: Takes in data with a shape of <code class="inlineCode">(10, 128)</code>, processing sequences of 10 timesteps, each with 128 features</li>
      <li class="bulletList"><strong class="keyWord">First LSTM layer (lstm)</strong>: Contains 64 units and returns sequences, processing the input and passing on sequences of the same length to the next LSTM layer</li>
      <li class="bulletList"><strong class="keyWord">Second LSTM layer (lstm_1)</strong>: Also has 64 units but does not return sequences, compressing the output from the first LSTM layer into a single vector</li>
      <li class="bulletList"><strong class="keyWord">Dense output layer (dense)</strong>: The final layer is a dense layer with 10 units and a softmax activation function, outputting a probability distribution over 10 classes (categories or labels)</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-207">Preprocessing steps</h3>
    <p class="normal">Preprocessing steps <a id="_idIndexMarker778"/>are crucial for ensuring that data is compatible with a pre-trained model’s training procedure. Here are a couple of examples:</p>
    <ul>
      <li class="bulletList">NLP <strong class="keyWord">tokenization</strong> breaks down text into words or subwords that are consistent with what an LLM model expects</li>
      <li class="bulletList">For images, <strong class="keyWord">normalization</strong> can adjust image pixel values to a common scale to facilitate a model’s ability to learn from and generate predictions for data</li>
    </ul>
    <p class="normal">These preprocessing steps are essential for leveraging pre-trained models effectively, ensuring that input data mirrors the form of the data that the model was trained on.</p>
    <h2 class="heading-2" id="_idParaDest-208">Zero-shot learning</h2>
    <p class="normal">Following our <a id="_idIndexMarker779"/>exploration of pre-trained models and their fundamental components, we will now shift our focus to their application in ZSL. As we will discuss later, the fundamentals behind ZSL allow marketers to dynamically generate relevant content and even target their marketing campaigns to individual consumers in near real time. Before we get to those examples, this section will provide some background on ZSL and how it enables models to apply learned knowledge and infer information about tasks or classes that were not explicitly covered during their training. This capability extends the utility of pre-trained models, allowing them to generalize across unseen data and scenarios.</p>
    <h3 class="heading-3" id="_idParaDest-209">Mechanics of learning and prediction</h3>
    <p class="normal">At its core, ZSL operates<a id="_idIndexMarker780"/> by using transformations and mappings of the input and output in a high-dimensional embedding space, using two primary functions:</p>
    <ul>
      <li class="bulletList">An <strong class="keyWord">embedding function</strong> <em class="italic">f</em>: <img alt="" src="../Images/B30999_09_006.png"/> that transforms input – such as images or text – into feature vectors within the embedding space.</li>
      <li class="bulletList">A <strong class="keyWord">semantic attribute function</strong> <em class="italic">g</em>: <img alt="" src="../Images/B30999_09_007.png"/> that associates class labels with semantic attributes in the same embedding space. These attributes describe classes in terms of universal, distinguishable features, existing within an attribute space <em class="italic">A</em>.</li>
    </ul>
    <p class="normal">As a simplified example, take a case where ZSL is applied to distinguish between animals. The embedding function <em class="italic">f</em>: <img alt="" src="../Images/B30999_09_008.png"/> would transform visual images of the animal into high-dimensional feature vectors within the embedding space – for example, taking images of a sparrow and an eagle that are processed to show distinct feature vectors, representing their visual characteristics. Simultaneously, the semantic attribute function <em class="italic">g</em>: <img alt="" src="../Images/B30999_09_009.png"/> maps class labels, like <em class="italic">sparrow</em> and <em class="italic">eagle</em>, to vectors in the same embedding space based on semantic attributes. Attributes for <em class="italic">sparrow</em> might include <code class="inlineCode">[′small_size′, ′brown_color′, ′has_wings′]</code>, whereas <em class="italic">eagle</em> could be characterized by <code class="inlineCode">[′large_size′, ′sharp_beak′, ′has_wings′]</code>. These attributes are then<a id="_idIndexMarker781"/> quantified, where <code class="inlineCode">small_size</code> might be encoded as <code class="inlineCode">0.2</code> on a size scale, and <code class="inlineCode">large_size</code> as <code class="inlineCode">0.8</code>.</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_09_06.png"/></figure>
    <p class="packt_figref">Figure 9.6: Example mappings from an input image or class label to an embedding space</p>
    <p class="normal">By placing both images and class attributes within the same embedding space, as shown in <em class="italic">Figure 9.6</em>, a model can match an image’s feature vector to the closest class attribute vector. This matching is facilitated even if the model has never seen an image of a sparrow or eagle during training, by recognizing the overlap in their attributes with those of known classes. To find the best class match, the algorithm is tasked with the optimization of a<a id="_idIndexMarker782"/> compatibility function, which quantifies the match between an input’s feature vector and a class’s attribute vector. The ZSL prediction then involves selecting the class that maximizes compatibility for an unseen instance.</p>
    <h3 class="heading-3" id="_idParaDest-210">Output parameters</h3>
    <p class="normal">There are a number of<a id="_idIndexMarker783"/> key parameters that can be specified to influence the output of ZSL models. These parameters tailor the output by adjusting the behavior of the model during the sampling process. Three of the most common ones used in the context of text generation with models like GPT include:</p>
    <ul>
      <li class="bulletList">Temperature</li>
      <li class="bulletList">Top P (nucleus sampling)</li>
      <li class="bulletList">Frequency penalty</li>
    </ul>
    <p class="normal">By influencing how the model’s outputs are sampled from the probability distribution generated by the compatibility function, each of these parameters allows adjustments in the creativity, coherence, and diversity of the model’s outputs. The following sections provide further detail <a id="_idIndexMarker784"/>on the theory underlying each of these parameters.</p>
    <h4 class="heading-4">Temperature</h4>
    <p class="normal">The temperature parameter<a id="_idIndexMarker785"/> plays a crucial role in<a id="_idIndexMarker786"/> determining the level of randomness or confidence in the prediction distribution. Mathematically, adjusting the temperature modifies the <code class="inlineCode">softmax</code> function used to calculate the probabilities of the next word in the following manner:</p>
    <p class="center"><img alt="" src="../Images/B30999_09_010.png"/></p>
    <p class="normal">where <em class="italic">T</em> is the temperature and logit represents the raw outputs from the model. A lower temperature sharpens the distribution, making the model’s predictions more deterministic and less varied, whereas a higher temperature flattens the distribution, encouraging diversity in the predictions at the cost of potentially introducing less coherence, as the model becomes more likely to sample less probable words.</p>
    <p class="normal">To illustrate the concept, let’s consider a predictive text generation scenario of the next word in the sentence “<code class="inlineCode">The cat sat on the ___.</code>" For simplicity, assume our model considers five possible completions: “<code class="inlineCode">mat</code>,” “<code class="inlineCode">tree</code>,” “<code class="inlineCode">ball</code>,” “<code class="inlineCode">bed</code>,” and “<code class="inlineCode">tabl</code>e,” with the initial logits reflecting their probabilities (assigned manually in this case for simplicity). We can use the following code to produce a visualization, demonstrating how changing the temperature parameter <em class="italic">T</em> affects the <code class="inlineCode">softmax</code> function, altering the probability distribution of these potential next words:</p>
    <pre class="programlisting code"><code class="hljs-code">logits = np.array([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])
labels = ['mat<span class="hljs-string">' , 'tree', 'ball', '</span>bed<span class="hljs-string">', '</span>table<span class="hljs-string">'</span><span class="hljs-string">]</span>
<span class="hljs-string">def softmax_temperature_adjusted(logits, temperature):</span>
<span class="hljs-string">    exp_logits = np.exp(logits / temperature)</span>
<span class="hljs-string">    return exp_logits / np.sum(exp_logits)</span>
<span class="hljs-string">temperatures = [0.5, 1, 2, 4]</span>
<span class="hljs-string">plt.figure(figsize=(12, 8))</span>
<span class="hljs-string">for T in temperatures:</span>
<span class="hljs-string">    probabilities = softmax_temperature_adjusted(logits, T)</span>
<span class="hljs-string">    plt.plot(labels, probabilities, marker='</span>o<span class="hljs-string">', label=f'</span>Temperature = {T}<span class="hljs-string">')</span>
<span class="hljs-string">plt.title('</span>Effect of Temperature on Prediction Distribution<span class="hljs-string">')</span>
<span class="hljs-string">plt.ylabel('</span>Probability<span class="hljs-string">')</span>
<span class="hljs-string">plt.xlabel('</span>Words<span class="hljs-string">')</span>
<span class="hljs-string">plt.legend()</span>
<span class="hljs-string">plt.grid(True)</span>
<span class="hljs-string">plt.show()</span>
</code></pre>
    <p class="normal">This gives <a id="_idIndexMarker787"/>us the following output:</p>
    <figure class="mediaobject"><img alt="" src="../Images/B30999_09_07.png"/></figure>
    <p class="packt_figref">Figure 9.7: Visualization of the impact of the temperature parameter on softmax probability distributions for potential next words in the sentence “The cat sat on the ___”</p>
    <p class="normal">As illustrated in the graph, at lower temperatures the distribution is sharper, concentrating the probability mass on fewer, more likely outcomes, like “<code class="inlineCode">bed</code>.” As the temperature increases, the distribution becomes flatter, giving a higher probability to a broader set of<a id="_idIndexMarker788"/> potentially less likely word outcomes, such as <code class="inlineCode">table</code> or <code class="inlineCode">mat</code>.</p>
    <h4 class="heading-4">Top P</h4>
    <p class="normal">Top P, or <strong class="keyWord">nucleus sampling</strong>, offers a <a id="_idIndexMarker789"/>dynamic way to<a id="_idIndexMarker790"/> focus the generation process on the most plausible set of outcomes. Instead of considering the entire vocabulary, the model limits its choices to the smallest set of words whose cumulative probability exceeds the threshold P. </p>
    <p class="normal">This approach can be thought of as dynamically adjusting the breadth of consideration based on the model’s confidence, according to the formula:</p>
    <p class="center"><img alt="" src="../Images/B30999_09_011.png"/></p>
    <p class="normal">where <em class="italic">N</em> is the number of words considered and <em class="italic">P</em>(<em class="italic">w</em><sub class="subscript-italic" style="font-style: italic;">i</sub>) is the probability of the <em class="italic">i</em><sup class="superscript">th</sup> word. This technique helps maintain a balance between variety and relevance, ensuring that the generated text remains plausible without being overly constrained by the most likely predictions.</p>
    <h4 class="heading-4">Frequency penalty</h4>
    <p class="normal">The <strong class="keyWord">frequency penalty</strong> addresses<a id="_idIndexMarker791"/> the tendency of models <a id="_idIndexMarker792"/>to repeat the same words or phrases, enhancing the diversity of the output. It modifies the probability of each word based on its previous occurrences in the generated text:</p>
    <p class="center"><img alt="" src="../Images/B30999_09_012.png"/></p>
    <p class="normal">where <em class="italic">P</em>(<em class="italic">w</em><sub class="subscript-italic" style="font-style: italic;">i</sub>) is the original probability of the word <em class="italic">w</em><sub class="subscript-italic" style="font-style: italic;">i</sub> and <em class="italic">Occurrence</em> (<em class="italic">w</em><sub class="subscript-italic" style="font-style: italic;">i</sub>) is the number of times <em class="italic">w</em><sub class="subscript-italic" style="font-style: italic;">i</sub> has appeared in the text so far. This adjustment encourages the exploration of new vocabulary and ideas by penalizing words that the model has already used, promoting a <a id="_idIndexMarker793"/>richer and more varied output.</p>
    <h1 class="heading-1" id="_idParaDest-211">ZSL for marketing copy</h1>
    <p class="normal">We will now<a id="_idIndexMarker794"/> discuss the practical application of ZSL through the example of an e-commerce brand that is launching new lines of eco-friendly kitchenware and fashion products. Traditionally, creating compelling product descriptions and promotional content could require significant research and creative effort from writers familiar with a brand’s tone and the intricacies of sustainable design. However, with ZSL, the brand can input a concise description of the product line, emphasizing keywords like “sustainable” and “eco-friendly activewear,” into a pre-trained model, immediately producing an output of brand-appropriate content that is ready for consideration across digital platforms. By automating these initial stages of content generation, the brand can now focus more on higher value efforts like strategy, engagement, and analyzing the effectiveness of its marketing efforts.</p>
    <p class="normal">In general, to integrate ZSL effectively into your marketing strategy, consider the following iterative process as a template:</p>
    <ol>
      <li class="numberedList" value="1">Define your content goals and the key messages you want to communicate.</li>
      <li class="numberedList">Create concise prompts that encapsulate these goals and messages, incorporating relevant keywords and themes.</li>
      <li class="numberedList">Experiment with different parameters (<code class="inlineCode">temperature</code>, <code class="inlineCode">Top P</code>, and <code class="inlineCode">frequency penalty</code>) to adjust the style and diversity of the generated content.</li>
      <li class="numberedList">Generate multiple content variations to explore different angles and ideas.</li>
      <li class="numberedList">Review and refine the output, selecting the best options that align with your brand’s tone and objectives.</li>
      <li class="numberedList">Use the generated content as a starting point for further customization and optimization, based on audience feedback and performance metrics.</li>
    </ol>
    <p class="normal">In the following subsections, we will focus on steps 1–4 of the preceding workflow, with steps 5 and 6 covered by examples in the chapters that will follow.</p>
    <h2 class="heading-2" id="_idParaDest-212">Preparing for ZSL in Python</h2>
    <p class="normal">To demonstrate the<a id="_idIndexMarker795"/> Python setup process for ZSL, this<a id="_idIndexMarker796"/> section will walk through the basic steps of using both freely available, open-source models, as well as more advanced models requiring paid API-based implementations. We will demonstrate the former with models available in the Hugging Face Transformers library, and we will demonstrate the setup for the latter using OpenAI’s API service.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Staying ahead with Hugging Face updates</strong></p>
      <p class="normal">Hugging Face’s Transformers library frequently updates, making advanced models that were previously behind paid API services freely available. For the latest Hugging Face models available, consult their documentation at <code class="inlineCode">https://huggingface.co/models</code>.</p>
    </div>
    <p class="normal">The basic setup for ZSL tasks using the gpt2 model available on Hugging Face is:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
generator = pipeline(<span class="hljs-string">'text-generation'</span>, model=<span class="hljs-string">'gpt2'</span>)
prompt = <span class="hljs-string">"Write a compelling product description for eco-friendly kitchenware emphasizing sustainability:"</span>
completion = generator(prompt, max_length=<span class="hljs-number">100</span>, num_return_sequences=<span class="hljs-number">1</span>, temperature=<span class="hljs-number">0.7</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Generated text using HuggingFace's GPT model:"</span>, completion[<span class="hljs-number">0</span>][<span class="hljs-string">'generated_text'</span>])
</code></pre>
    <p class="normal">Running this<a id="_idIndexMarker797"/> prompt<a id="_idIndexMarker798"/> results in an output such as the following:</p>
    <pre class="programlisting code"><code class="hljs-code">"Your kitchen will save you time and energy. It will save you money. It will make you feel good. It will help you get the most out of your kitchen and minimize waste. It will help you get good results. You'll save money and your home will be healthier."
</code></pre>
    <p class="normal">We can contrast this with the output that is returned from a model with the same parameters but using the more advanced text generation capabilities of OpenAI’s GPT-4. Executing this model currently requires the creation of an OpenAI account and then generating an API key that can be substituted in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI
client = OpenAI(api_key = <span class="hljs-string">'XXX'</span>) <span class="hljs-comment">#insert your api key here</span>
completion = client.chat.completions.create(
    model=<span class="hljs-string">"gpt-4"</span>,
    messages=[{<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>,  <span class="hljs-string">"content"</span>: <span class="hljs-string">"Write a compelling product description for eco-friendly kitchenware emphasizing sustainability:"</span>}],
    max_tokens=<span class="hljs-number">100</span>, n=<span class="hljs-number">1</span>, temperature=<span class="hljs-number">0.7</span>)
<span class="hljs-built_in">print</span>(completion.choices[<span class="hljs-number">0</span>].message)
</code></pre>
    <p class="normal">This results in a response such as:</p>
    <pre class="programlisting code"><code class="hljs-code">"Introducing our premium range of eco-friendly kitchenware, designed for the conscious home cook. Every item in this collection is expertly created from sustainable materials, ensuring minimal impact on our environment. From bamboo cutting boards to recycled stainless-steel knives, each piece combines functionality with eco-friendly design. Our eco-friendly kitchenware is not only durable and high-performing, but also promotes a healthier lifestyle and a greener planet."
</code></pre>
    <p class="normal">It’s evident from these two responses that advanced models like GPT-4 significantly enhance the relevancy and quality of the generated content. This is because, compared to GPT-2, GPT-4 incorporates major advancements in deep learning architectures, larger training datasets, and more sophisticated fine-tuning techniques. For this reason, we will use results obtained from GPT-4 for the remainder of this chapter.</p>
    <p class="normal">However, the key to leveraging ZSL effectively lies not just in a model’s capabilities but also in effectively choosing the input information that shapes the output. The most critical aspects <a id="_idIndexMarker799"/>here<a id="_idIndexMarker800"/> include creating an effective prompt, as well as setting other parameters such as temperature and Top P, as discussed earlier in the chapter.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Use the outputs of LLMs with caution</strong></p>
      <p class="normal">Relying directly on the output of any LLM for critical campaign content without human review can be risky! Always consider treating the initial outputs of GenAI models as a creative starting point, and then iterate and refine them as needed to ensure alignment with your brand’s voice and objectives.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-213">Creating an effective prompt</h2>
    <p class="normal">Creating an effective<a id="_idIndexMarker801"/> prompt is the most crucial step in leveraging ZSL for marketing copy. In ZSL, the prompt effectively becomes the instruction manual for a model, telling it what kind of content to generate, as well as its style, tone, and substance.</p>
    <p class="normal">The following are some guidelines around how to formulate prompts that will elicit the best possible marketing copy content from the model:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Clarity</strong>: Ensure that your prompt is specific about what you want, whether it’s a product description, headline, or call to action.</li>
      <li class="bulletList"><strong class="keyWord">Contextual</strong>: Provide sufficient background to guide a model. For eco-friendly products, mention key selling points like sustainability or biodegradability.</li>
      <li class="bulletList"><strong class="keyWord">Creative</strong>: While clarity is crucial, leaving room for creativity can yield surprising and innovative results. Phrases like “Imagine...” or “Create a story where...” can be particularly powerful.</li>
      <li class="bulletList"><strong class="keyWord">Concise</strong>: Lengthy prompts can dilute the focus. Aim for brevity while including essential details, ensuring that a model stays on topic.</li>
    </ul>
    <p class="normal">In the following sections, we will illustrate the impact of prompt quality through examples, with different types of marketing copy. While good prompts elicit detailed, relevant, and engaging content, poor prompts can lead to vague and uninspiring outputs. To generate these responses, we will define the following function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_response</span>(<span class="hljs-params">prompt, model=</span><span class="hljs-string">"gpt-4"</span><span class="hljs-params">, max_tokens=</span><span class="hljs-number">100</span><span class="hljs-params">, temperature=</span><span class="hljs-number">0.7</span><span class="hljs-params">, n=</span><span class="hljs-number">1</span>):
    response = client.chat.completions.create(
        model=model,
        messages=[{<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: prompt}],
        max_tokens=max_tokens,
        n=n,
        temperature=temperature)
<span class="hljs-keyword">return</span> response.choices[<span class="hljs-number">0</span>].message.content
</code></pre>
    <p class="normal">This function will <a id="_idIndexMarker802"/>be used with different prompt types in the examples that follow.</p>
    <h3 class="heading-3" id="_idParaDest-214">Example 1: Product descriptions</h3>
    <p class="normal">In this example, we<a id="_idIndexMarker803"/> will generate product descriptions for our e-commerce brand, which is launching new lines of eco-friendly kitchenware.</p>
    <p class="normal">The following is an example of a poor prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">poor_product_description = <span class="hljs-string">"Talk about bamboo cutlery."</span>
generate_response(poor_product_description)
</code></pre>
    <p class="normal">This produces:</p>
    <pre class="programlisting code"><code class="hljs-code">Bamboo cutlery is a type of eating utensil made from bamboo, a fast-growing renewable resource. This form of cutlery includes items such as forks, knives, spoons, and chopsticks. They are an eco-friendly alternative to plastic disposable cutlery due to their reusable nature and biodegradable properties. Bamboo cutlery is lightweight and durable, making it a popular choice for camping, picnics, and travel.
</code></pre>
    <p class="normal">Now, let’s look at the following example of a good prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">good_product_description = <span class="hljs-string">"Write a captivating description for a bamboo cutlery set designed for eco-conscious consumers, emphasizing its sustainability and style."</span>
generate_response(good_product_description)
</code></pre>
    <p class="normal">This prompt produces the following output:</p>
    <pre class="programlisting code"><code class="hljs-code">Discover the perfect harmony of sophistication and sustainability with our Bamboo Cutlery Set. Designed for the eco-conscious consumer, this set is not only a statement of your commitment to the environment but also a reflection of your exquisite taste. Each piece is crafted from 100% organic bamboo, a renewable resource that grows significantly faster than traditional hardwoods. This set boasts a sleek, minimalist design that perfectly complements any table setting, from casual picnic to elegant dinner parties.
</code></pre>
    <p class="normal">From a marketing perspective, this example demonstrates the significance of creating detailed and audience-specific prompts with clear requirements when using ZSL for product descriptions, as well as how this leads to more specificity in the generated response. However, it is worth noting that older consumers may value more straightforward, factual <a id="_idIndexMarker804"/>information and, therefore, may favor the more generic prompt’s response from an engagement standpoint. Tailoring GenAI outputs at the level of the individual consumer can be crucial as well and is a topic discussed in <em class="chapterRef">Chapter 11</em>.</p>
    <h3 class="heading-3" id="_idParaDest-215">Example 2: Blog post titles</h3>
    <p class="normal">In our next example, we will focus on another type of marketing copy by generating blog post titles for our e-commerce brand.</p>
    <p class="normal">We’ll start by generating a poor prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">poor_blog_title = <span class="hljs-string">"Write a title about kitchenware benefits."</span>
generate_response(poor_blog_title)
</code></pre>
    <p class="normal">This produces the following:</p>
    <pre class="programlisting code"><code class="hljs-code">Exploring the Numerous Benefits of High-Quality Kitchenware
</code></pre>
    <p class="normal">Here’s an example of a good prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">good_blog_title = <span class="hljs-string">"Create an intriguing title for a blog post highlighting the top five benefits of biodegradable kitchenware for sustainable living."</span>
generate_response(good_blog_title)
</code></pre>
    <p class="normal">This gives us a more engaging result:</p>
    <pre class="programlisting code"><code class="hljs-code">Unlocking Sustainable Living: Top 5 Benefits of Biodegradable Kitchenware
</code></pre>
    <p class="normal">Comparing these prompts for blog post titles illustrates the impact of specificity and audience targeting on content effectiveness, where a specific prompt highlighting biodegradable kitchenware creates content aligned more with sustainability. In contrast, a vague prompt results in a generic title that would fail to differentiate itself amid a sea of similar content. To tailor the language produced by LLMs even further, we can also use <strong class="keyWord">few-shot learning</strong> (<strong class="keyWord">FSL</strong>), the topic of the next chapter. Used effectively, FSL can achieve the same specificity in language but in a way that’s aligned with a brand’s unique voice, in <a id="_idIndexMarker805"/>order to distinguish LLM outputs from what other LLMs might produce, even when given the same prompt.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Navigating topical content with AI</strong></p>
      <p class="normal">When generating blog posts, understanding your model’s training data recency is crucial. Without current data or web search capabilities, you risk creating content based on outdated trends that lack the necessary context for relevant outputs.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-216">Example 3: Social media captions</h3>
    <p class="normal">In this example, we will generate an Instagram caption for a post about our e-commerce brand.</p>
    <p class="normal">Let’s look at a poor prompt first:</p>
    <pre class="programlisting code"><code class="hljs-code">poor_social_media_caption = <span class="hljs-string">"Make an Instagram post for kitchenware."</span>
generate_response(poor_social_media_caption)
</code></pre>
    <p class="normal">This yields the following:</p>
    <pre class="programlisting code"><code class="hljs-code">'Spice up your kitchen game with our premium-quality kitchenware! 🍳🥄 From non-stick pans to sharp, durable knives, we have everything you need. Cook like a pro and serve in style! Get yours today at our store. 🍽️👩🍳👨🍳
</code></pre>
    <p class="normal">Next, we will look at an example of a good prompt:</p>
    <pre class="programlisting code"><code class="hljs-code">good_social_media_caption = <span class="hljs-string">"Create an engaging and witty Instagram caption for our latest eco-friendly kitchenware line, focusing on reducing plastic waste."</span>
generate_response(good_social_media_caption)
</code></pre>
    <p class="normal">This produces:</p>
    <pre class="programlisting code"><code class="hljs-code">Kiss plastic goodbye👋and say hello to our chic, eco-friendly kitchenware!🍃Because every meal should be a feast for the eyes and gentle on the earth. #SustainableGourmet"
</code></pre>
    <p class="normal">The difference between these two prompts for Instagram captions illustrates how a specific prompt generates a caption that not only engages with its witty language but also directly appeals to eco-conscious consumers, likely increasing likes, shares, and comments – all crucial metrics on social platforms. In contrast, the vague prompt results in a generic and <a id="_idIndexMarker806"/>broad caption that, while informative, lacks a focused appeal and may fail to capture the attention of certain potential customers looking for eco-friendly products.</p>
    <h2 class="heading-2" id="_idParaDest-217">Impact of parameter tuning</h2>
    <p class="normal">While creating an <a id="_idIndexMarker807"/>effective prompt lays the groundwork, fine-tuning the model’s parameters is equally essential to align the generated content with the desired marketing style. In this section, we will explore how adjusting parameters like temperature and Top P affect the output of a language model. Transitioning from kitchenware, we will demonstrate this by generating marketing slogans for an eco-friendly and sustainable fashion line launch.</p>
    <p class="normal">We will do this by defining the following Python function, outputting sets of three variants of the slogan for each alteration to one of these parameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_slogan</span>(<span class="hljs-params">prompt, temperature=</span><span class="hljs-number">1.0</span><span class="hljs-params">, top_p=</span><span class="hljs-number">1.0</span>):
    response = client.chat.completions.create(
        model=<span class="hljs-string">"gpt-4"</span>,
        messages=[{<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: prompt}],
        max_tokens=<span class="hljs-number">15</span>,
        temperature=temperature,
        top_p=top_p,        n=<span class="hljs-number">3</span>)
    <span class="hljs-keyword">return</span> (response.choices[<span class="hljs-number">0</span>].message.content, response.choices[<span class="hljs-number">1</span>].message.content, response.choices[<span class="hljs-number">2</span>].message.content)
</code></pre>
    <p class="normal">Let’s use the base case, with both <code class="inlineCode">temperature</code> and <code class="inlineCode">Top P</code> set to values of <code class="inlineCode">1.0</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">generate_slogan(prompt)
</code></pre>
    <p class="normal">This produces:</p>
    <pre class="programlisting code"><code class="hljs-code">(<span class="hljs-string">'"Dress with Purpose: Eco-Elegance Never Goes Out of Style."'</span>,
 <span class="hljs-string">'"Style with Substance, Fashion with Consciousness!"'</span>,
 <span class="hljs-string">'"Dressing the world, Preserving the planet."'</span>)
</code></pre>
    <p class="normal">Now, let’s tweak<a id="_idIndexMarker808"/> each parameter and see the output that we get:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Temperature</strong>: Increasing temperature makes responses more diverse and creative but potentially less coherent:
        <pre class="programlisting code-one"><code class="hljs-code">generate_slogan(prompt, temperature=<span class="hljs-number">1.8</span>)
</code></pre>
      </li>
    </ul>
    <p class="normal-one">This produces:</p>
    <pre class="programlisting code-one"><code class="hljs-code">(<span class="hljs-string">'"Drape Yourself in Earth Love: Stylishly Sustainable Fashion Favorites"'</span>,
 <span class="hljs-string">'"Wear the change, leave no trace."'</span>,
 <span class="hljs-string">'"Leave no carbon footprints, only stylish one - Wear Eco in Style"'</span>)
</code></pre>
    <p class="normal-one">Conversely, lowering the temperature results in more predictable and consistent outputs:</p>
    <pre class="programlisting code-one"><code class="hljs-code">generate_slogan(prompt, temperature=<span class="hljs-number">0.3</span>)
</code></pre>
    <p class="normal-one">This time, the prompt produces the following output:</p>
    <pre class="programlisting code-one"><code class="hljs-code">(<span class="hljs-string">'"Style with Sustainability: Dress Green, Live Clean!"'</span>,
 <span class="hljs-string">'"Style with Sustainability: Fashion for a Better Tomorrow"'</span>,
 <span class="hljs-string">'"Style with Sustainability: Fashion for a Future"'</span>)
</code></pre>
    <ul>
      <li class="bulletList"><strong class="keyWord">Top P</strong>: Reducing <code class="inlineCode">top_p</code> from its maximum value of 1.0 narrows the possible variety of generated slogans by making the model more conservative, as it tends to select only the most probable outputs:
        <pre class="programlisting code-one"><code class="hljs-code">generate_slogan(prompt, top_p=<span class="hljs-number">0.4</span>)
</code></pre>
      </li>
    </ul>
    <p class="normal-one">This produces:</p>
    <pre class="programlisting code-one"><code class="hljs-code">(<span class="hljs-string">'"Style with Sustainability: Fashion for a Better Future"'</span>,
 <span class="hljs-string">'"Style with Sustainability: Fashion for a Greener Tomorrow"'</span>,
 <span class="hljs-string">'"Style with Sustainability: Fashion for a Greener Tomorrow"'</span>)
</code></pre>
    <p class="normal">Through these examples, we can observe the significant impact that parameter adjustments can have on the nature of generated content. This demonstrates the importance of <a id="_idIndexMarker809"/>parameter tuning in creating marketing slogans that are not only relevant and engaging but also tailored to the style and message of a brand.</p>
    <h1 class="heading-1" id="_idParaDest-218">Summary</h1>
    <p class="normal">In this chapter, we went on a journey through GenAI, and ZSL, and their transformative potential in marketing content creation. We introduced foundational GenAI concepts and discussed the mechanisms that allow these models to generate text, images, and more, with a particular focus on text generation. Analyzing contextual embeddings and semantic proximity highlighted the nuances that pre-trained models like GPT and BERT bring to understanding and generating language with remarkable adaptability and accuracy.</p>
    <p class="normal">Central to our discussion was the application of ZSL in creating marketing copy, which allows brands to generate compelling content without the need for extensive training data. We outlined a strategic process to integrate ZSL into marketing strategies with the help of examples that emphasize the importance of creating clear, contextual, and creative prompts. This step-by-step approach – defining content goals, experimenting with parameters, and refining outputs – enables marketers to harness the power of LLMs effectively. We also learned how adjusting parameters such as temperature and Top P can help fine-tune the creativity, coherence, and diversity of the generated content. These practical insights will help you optimize marketing copy to align with brand messaging and campaign objectives.</p>
    <p class="normal">Looking ahead, the next chapter progresses into the more advanced territories of few-shot and transfer learning. Building on the ZSL foundation, we will explore how these techniques can further refine GenAI models for on-target messaging. This involves adapting models to new contexts with minimal examples (FSL) and updating them with brand or customer-specific information (transfer learning), ensuring consistency and relevance in the generated content.</p>
    <h1 class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 5000 members at:</p>
    <p class="normal"><a href="https://packt.link/genai"><span class="url">https://packt.link/genai</span></a></p>
    <p class="normal"><img alt="" src="../Images/QR_Code12856128601808671.png"/></p>
  </div>
</body></html>