- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Exploring Optional Parameters for H2O AutoML
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索H2O AutoML的可选参数
- en: As we explored in [*Chapter 2*](B17298_02.xhtml#_idTextAnchor038), *Working
    with H2O Flow (H2O’s Web UI)*, when training models using H2O AutoML, we had plenty
    of parameters to select. All these parameters gave us the capability to control
    how H2O AutoML should train our models. This control helps us get the best possible
    use of AutoML based on our requirements. Most of the parameters we explored were
    pretty straightforward to understand. However, there were some parameters whose
    purpose and effects were slightly complex to be understood at the very start of
    this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第二章*](B17298_02.xhtml#_idTextAnchor038)中探讨的，*使用H2O Flow（H2O的Web UI）*，当使用H2O
    AutoML训练模型时，我们有大量的参数可供选择。所有这些参数都赋予我们控制H2O AutoML如何训练我们的模型的能力。这种控制帮助我们根据我们的需求获得AutoML的最佳使用效果。我们探讨的大多数参数都很容易理解。然而，有些参数的目的和效果在本书一开始时理解起来稍微复杂一些。
- en: In this chapter, we shall explore these parameters by learning about the **Machine
    Learning** (**ML**) concepts behind them, and then understand how we can use them
    in an AutoML setting.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过了解其背后的**机器学习**（**ML**）概念来探索这些参数，然后了解我们如何在AutoML环境中使用它们。
- en: By the end of this chapter, you will not only be educated in some of the advanced
    ML concepts, but you will also be able to implement them using the parametric
    provisions made in H2O AutoML.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您不仅将了解一些高级ML概念，而且还将能够使用H2O AutoML中提供的参数规定来实现它们。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Experimenting with parameters that support imbalanced classes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试支持不平衡类别的参数
- en: Experimenting with parameters that support early stopping
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试支持提前停止的参数
- en: Experimenting with parameters that support cross-validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试支持交叉验证的参数
- en: We will start by understanding what imbalanced classes are.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先了解不平衡类别的概念。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You will require the following to complete this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章内容您需要以下条件：
- en: The latest version of your preferred web browser.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您首选的网页浏览器的最新版本。
- en: The H2O software installed on your system. Refer to [*Chapter 1*](B17298_01.xhtml#_idTextAnchor017),
    *Understanding H2O AutoML Basics*, for instructions on how to install H2O on your
    system.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装在您系统上的H2O软件。有关如何在您的系统上安装H2O的说明，请参阅[*第一章*](B17298_01.xhtml#_idTextAnchor017)，*理解H2O
    AutoML基础知识*。
- en: Tip
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: All the H2O AutoML function parameters shown in this chapter are shown using
    H2O Flow to keep things simple. The equivalent parameters are also available in
    the Python and R programming languages for software engineers to code into their
    services. You can find these details at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的所有H2O AutoML函数参数均使用H2O Flow进行展示，以保持内容简洁。等效参数也以Python和R编程语言的形式提供，供软件工程师将其编码到他们的服务中。您可以在[https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml)找到这些详细信息。
- en: Experimenting with parameters that support imbalanced classes
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试支持不平衡类别的参数
- en: One common problem you will often face in the field of ML is classifying rare
    events. Consider the case of large earthquakes. Large earthquakes of magnitude
    7 and higher occur about once every year. If you had a dataset containing the
    Earth’s tectonic activity of each day since the last decade with the response
    column containing whether or not an earthquake occurred, then you would have approximately
    3,650 rows of data; that is, one row for each day in the decade, with around 8-12
    rows showing large earthquakes. That is less than a 0.3% chance that this event
    will occur. 99.7% of the time, there will be no large earthquakes. This dataset,
    where the number of large earthquake events is so small, is called an **imbalanced
    dataset**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML领域，您经常会遇到的一个常见问题是分类罕见事件。考虑大地震的例子。每年大约发生一次7级以上的大地震。如果您有一个包含过去十年每天地球板块活动的数据集，其中响应列包含是否发生地震，那么您将有大约3,650行数据；也就是说，十年中的每一天一行，大约有8-12行显示大地震。这个事件发生的概率不到0.3%。99.7%的时间，不会有大地震。这个大型地震事件数量如此之小的数据集被称为**不平衡数据集**。
- en: The problem with the imbalanced dataset is that even if you write a simple `if-else`
    function that marks all tectonic events as not earthquakes and call this a model,
    it will still show the accuracy as 99.7% accuracy since the majority of the events
    are not earthquake-causing. However, in actuality, this so-called model is very
    bad as it is not correctly informing you whether it is an earthquake or not.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡数据集的问题在于，即使你编写一个简单的`if-else`函数来标记所有构造事件为非地震，并将其称为模型，它仍然会显示99.7%的准确率，因为大多数事件都不是地震引起的。然而，实际上，这个所谓的模型非常糟糕，因为它没有正确地告诉你是否是地震。
- en: Such imbalance in the **target class** creates a lot of issues when training
    ML models. The ML models are more likely to assume that these events are so rare
    that they will never occur and will not learn the distinction between those events.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在**目标类**中的不平衡会在训练机器学习模型时造成很多问题。机器学习模型更有可能假设这些事件非常罕见，永远不会发生，并且不会学习这些事件之间的区别。
- en: However, there are ways to tackle this issue. One way is to undersample the
    majority class and the other way is to oversample the minority class. We shall
    learn more about these techniques in the upcoming sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有方法可以解决这个问题。一种方法是对多数类进行下采样，另一种方法是对少数类进行过采样。我们将在接下来的章节中了解更多关于这些技术的内容。
- en: Understanding undersampling the majority class
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解对多数类进行下采样的方法
- en: In the scenario of predicting the occurrence of earthquakes, the dataset contains
    a large number of events that have been identified as *not-earthquake*. This event
    is known as the majority class. The few events that mark the activity as an *earthquake*
    are known as the minority class.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测地震发生的情况中，数据集中包含大量已被识别为*非地震*的事件。这个事件被称为多数类。标记活动为*地震*的少数事件被称为少数类。
- en: 'Let’s see how **undersampling the majority class** can solve the problems caused
    by an imbalance in the classes. Consider the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看**对多数类进行下采样**如何解决由类别不平衡引起的问题。考虑以下图表：
- en: '![Figure 8.1 – Undersampling an imbalanced dataset ](img/B17298_08_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 对不平衡数据集进行下采样](img/B17298_08_001.jpg)'
- en: Figure 8.1 – Undersampling an imbalanced dataset
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 对不平衡数据集进行下采样
- en: Let’s assume you have 3,640 data samples of tectonic activity that indicate
    no earthquakes happened and only 10 samples that indicate earthquakes happened.
    In this case, to tackle this imbalance issue, you must create a bootstrapped dataset
    containing all 10 samples of the minority class, and 10 samples of the majority
    class chosen at random from the 3,640 data samples. Then, you can feed this new
    dataset to H2O AutoML for training. In this case, we have undersampled the majority
    class and equalized the *earthquake* and *not-earthquake* data samples before
    training the model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有3,640个表示没有发生地震的构造活动数据样本，以及仅10个表示发生地震的样本。在这种情况下，为了解决这种不平衡问题，你必须创建一个包含所有10个少数类样本，以及从3,640个数据样本中随机选择的10个多数类样本的bootstrapped数据集。然后，你可以将这个新的数据集输入到H2O
    AutoML中进行训练。在这种情况下，我们在训练模型之前对多数类进行了下采样，并平衡了*地震*和*非地震*数据样本。
- en: The drawback of this approach is that we end up tossing away huge amounts of
    data, and the model won’t be able to learn a lot from the reduced data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是我们最终会丢弃大量数据，并且模型无法从减少的数据中学习到很多。
- en: Understanding oversampling the minority class
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解对少数类进行过采样的方法
- en: 'The second approach to tackling the imbalanced dataset issue is to **oversample
    the minority class**. One obvious way is to duplicate the minority class data
    samples and append them to the dataset so that the number of data samples between
    the majority and minority classes is equal. Refer to the following diagram for
    a better understanding:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 解决不平衡数据集问题的第二种方法是**对少数类进行过采样**。一种明显的方法是将少数类数据样本复制并附加到数据集中，使得多数类和少数类之间的数据样本数量相等。参考以下图表以获得更好的理解：
- en: '![Figure 8.2 – Oversampling an imbalanced dataset ](img/B17298_08_002.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 对不平衡数据集进行过采样](img/B17298_08_002.jpg)'
- en: Figure 8.2 – Oversampling an imbalanced dataset
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 对不平衡数据集进行过采样
- en: In the preceding diagram, you can see that we replicated the minority class
    data samples and appended them to the dataset so that we ended up with 3,640 rows
    for each of the classes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，你可以看到我们复制了少数类数据样本并将它们附加到数据集中，结果每个类别都有3,640行。
- en: This approach can work; however, oversampling will lead to an explosion in the
    size of the dataset. You need to make sure that it does not exceed your computation
    and memory limits and end up failing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以工作；然而，过采样会导致数据集大小的爆炸性增长。你需要确保它不会超过你的计算和内存限制，否则最终会失败。
- en: Now that we’ve covered the basics of class balancing using undersampling and
    oversampling, let’s see how H2O AutoML handles it using its class balancing parameters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了使用欠采样和过采样进行类别平衡的基本知识，让我们看看H2O AutoML是如何使用其类别平衡参数来处理它的。
- en: Working with class balancing parameters in H2O AutoML
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在H2O AutoML中处理类别平衡参数
- en: H2O AutoML has a parameter called `balance_classes` that accepts a boolean value.
    If set to *True*, H2O performs oversampling on the minority class and undersampling
    on the majority class. The balancing is performed in such a way that eventually,
    each class contains the same number of data samples.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: H2O AutoML有一个名为`balance_classes`的参数，它接受一个布尔值。如果设置为*True*，H2O将对少数类进行过采样，对多数类进行欠采样。平衡是按照这样的方式进行的，最终每个类别都包含相同数量的数据样本。
- en: Both undersampling and oversampling of the respective classes is done randomly.
    Additionally, oversampling of the minority class is done with replacement. This
    means that data samples from the minority class can be chosen and added to the
    new training dataset multiple times and can be repeated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对相应类别的欠采样和过采样都是随机进行的。此外，少数类的过采样是带替换的。这意味着可以从少数类中选择数据样本并将其多次添加到新的训练数据集中，并且可以重复。
- en: 'H2O AutoML has the following parameters that support class balancing functionality:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: H2O AutoML有以下参数支持类别平衡功能：
- en: '`balance_classes`: This parameter accepts a boolean value. It is *False* by
    default, but if you want to perform class balancing on your dataset before feeding
    it to H2O AutoML for training, then you can set the boolean value to *True*.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance_classes`：此参数接受一个布尔值。默认为*False*，但如果你想在将数据集输入H2O AutoML进行训练之前进行类别平衡，则可以将布尔值设置为*True*。'
- en: 'In H2O Flow, you get a checkbox besides the parameter. Refer to the following
    screenshot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O Flow中，你会在参数旁边得到一个复选框。请参考以下截图：
- en: '![Figure 8.3 – The balance_classes checkbox in H2O Flow ](img/B17298_08_003.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – H2O Flow中的balance_classes复选框](img/B17298_08_003.jpg)'
- en: Figure 8.3 – The balance_classes checkbox in H2O Flow
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – H2O Flow中的balance_classes复选框
- en: 'Checking it makes the `class_sampling_factors` and `max_after_balance_size`
    parameters available in the **EXPERT** section of the **Run AutoML** parameters,
    as shown in the following screenshot:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 选中它会使`class_sampling_factors`和`max_after_balance_size`参数在**运行AutoML**参数的**专家**部分中可用，如下截图所示：
- en: '![Figure 8.4 – The class_sampling_factors and max_after_balance_size parameters
    in the EXPERT section ](img/B17298_08_004.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 专家部分中的class_sampling_factors和max_after_balance_size参数](img/B17298_08_004.jpg)'
- en: Figure 8.4 – The class_sampling_factors and max_after_balance_size parameters
    in the EXPERT section
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 专家部分中的class_sampling_factors和max_after_balance_size参数
- en: '`class_sampling_factors`: This parameter requires `balance_classes` to be *True*.
    This parameter takes a list of float values as input that will represent the sampling
    rate for that class. A sampling rate of value *1.0* for a given class will not
    change its sample rate during class balancing. A sampling rate of *0.5* will halve
    the sample rate of a class during class balancing while a sampling rate of *2.0*
    will double it.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_sampling_factors`：此参数要求`balance_classes`为*True*。此参数接受一个浮点数列表作为输入，该列表将代表该类的采样率。对于给定类的值为*1.0*的采样率将在类别平衡期间不改变其采样率。采样率为*0.5*将使类别在类别平衡期间的采样率减半，而采样率为*2.0*将使其加倍。'
- en: '`max_after_balance_size`: This parameter requires `balance_classes` to be *True*
    and specifies the maximum relative size of the training dataset after balancing.
    This parameter accepts a `float` value as input, which would limit the size your
    training dataset can grow to. The default value is *5.0*, which indicates that
    the training dataset will grow a maximum of *5* times its size. This value can
    also be less than *1.0*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_after_balance_size`：此参数要求`balance_classes`为*True*，并指定平衡后的训练数据集的最大相对大小。此参数接受一个`float`值作为输入，这将限制你的训练数据集可以增长的大小。默认值是*5.0*，表示训练数据集将增长到其大小的最大*5*倍。此值也可以小于*1.0*。'
- en: 'In the Python programming language, you can set these parameters as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python编程语言中，你可以这样设置这些参数：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Similarly, in the R programming language, you can set these parameters as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在 R 编程语言中，你可以这样设置这些参数：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To perform class balancing when training models using AutoML, you can set the
    `balance_classes` parameter to true in the H2O AutoML estimator object. In that
    same object, you can specify your `class_sampling_factors` and `max_after_balance_size`
    parameters. Then, you can use this initialized AutoML estimator object to trigger
    AutoML on your training dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用AutoML训练模型进行类别平衡时，你可以在H2O AutoML估计器对象中将`balance_classes`参数设置为true。在同一个对象中，你可以指定你的`class_sampling_factors`和`max_after_balance_size`参数。然后，你可以使用这个初始化的AutoML估计器对象来在你的训练数据集上触发AutoML。
- en: Now that you understand how we can tackle the class imbalance issue using the
    `balance_classes`, `class_sampling_factors`, and `max_after_balance_size` parameters,
    let’s understand the next optional parameters in AutoML – that is, stopping criteria.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何使用`balance_classes`、`class_sampling_factors`和`max_after_balance_size`参数来解决类别不平衡问题，让我们了解AutoML中的下一个可选参数——那就是停止标准。
- en: Experimenting with parameters that support early stopping
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试支持提前停止的参数
- en: '**Overfitting** models is one of the common issues often faced when trying
    to solve an ML problem. Overfitting is said to have occurred when the ML model
    tries to adapt to your training set too much, so much so that it is only able
    to make predictions on values that it has seen before in the training set and
    is unable to make a generalized prediction on unseen data.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**模型是在尝试解决机器学习问题时经常遇到的一个常见问题。当机器学习模型试图过度适应训练集时，就会发生过拟合，以至于它只能对训练集中之前见过的值做出预测，而无法对未见过的数据进行泛化预测。'
- en: Overfitting occurs due to a variety of reasons, one of them being that the model
    learns so much from the dataset that it even incorporates and learns the noise
    in the dataset. This learning negatively impacts predictions on new data that
    may not have that noise. So, how do we tackle this issue and prevent the model
    from overfitting? Stop the model early before it learns the noise.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是由于多种原因造成的，其中之一是模型从数据集中学习得太多，以至于它甚至吸收并学习了数据集中的噪声。这种学习对可能没有那种噪声的新数据的预测产生了负面影响。那么，我们如何解决这个问题并防止模型过拟合呢？在模型学习噪声之前尽早停止模型。
- en: In the following sub-sections, we shall understand what early stopping is and
    how it is done. Then, we will learn how the early stopping parameters offered
    by H2O AutoML work.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子节中，我们将了解什么是提前停止以及它是如何实现的。然后，我们将学习H2O AutoML提供的提前停止参数是如何工作的。
- en: Understanding early stopping
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解提前停止
- en: '**Early stopping** is a form of **regularization** that stops a model’s training
    once it has achieved a satisfactory understanding of the data and further prevents
    it from overfitting. Early stopping aims to observe the model’s performance as
    it improves using an appropriate performance metric and stop the model’s training
    once deterioration is observed due to overfitting.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**提前停止**是一种**正则化**形式，一旦模型对数据有了令人满意的了解，就会停止模型的训练，并进一步防止它过拟合。提前停止旨在通过使用适当的性能指标观察模型性能的改善，一旦观察到由于过拟合导致的性能下降，就停止模型的训练。'
- en: When training a model using algorithms that use iterative optimization to minimize
    the loss function, the training dataset is passed through the algorithm during
    each iteration. Observations and understandings that pass are then used during
    the next iteration. This iteration of passing the training dataset through the
    algorithm is called an **epoch**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用迭代优化最小化损失函数的算法训练模型时，训练数据集会在每次迭代中通过算法。然后，通过观察和理解这些数据，在下一个迭代中使用。这种将训练数据集通过算法的迭代过程称为**一个epoch**。
- en: 'For early stopping, at the end of every epoch, we can calculate the performance
    of the model and note down the metric value. Comparing these values during every
    iteration helps us understand whether the model is improving its performance after
    every epoch or whether it is learning noise and losing performance. We can monitor
    this and stop the model training at the epoch where we start seeing a decrease
    in performance. Refer to the following diagram to gain a better understanding
    of early stopping:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提前停止，在每个epoch结束时，我们可以计算模型的表现并记录指标值。在每次迭代中比较这些值有助于我们了解模型是否在每个epoch后提高了性能，或者它是否在学习噪声并失去性能。我们可以监控这一点，并在性能开始下降的epoch停止模型训练。参考以下图表以更好地理解提前停止：
- en: '![Figure 8.5 – Early stopping to avoid model overfitting ](img/B17298_08_005.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 避免模型过拟合的提前停止](img/B17298_08_005.jpg)'
- en: Figure 8.5 – Early stopping to avoid model overfitting
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 避免模型过拟合的提前停止
- en: In the preceding diagram, on the *Y* axis, we have the **Performance** value
    of the model. On the *X*-axis, we have the **Epoch** value. So, as time goes on
    and we iterate through the number of epochs, we see that the performance of the
    model on the training set and the validation set continues to increase. But after
    a certain point, the performance of the model on the validation dataset starts
    decreasing, while the performance of the training dataset continues to increase.
    This is where overfitting starts. The model learns too much from the training
    dataset and starts incorporating noise into its learning. This might show high
    performance on the training dataset, but the model fails to generalize the predictions.
    This leads to bad predictions on unseen data, such as the ones in the validation
    dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，*Y*轴上我们有模型的**性能**值。在*X*-轴上，我们有**Epoch**值。因此，随着时间的推移，我们遍历epoch的数量，我们看到模型在训练集和验证集上的性能持续增加。但经过某个点后，模型在验证数据集上的性能开始下降，而训练数据集的性能继续增加。这就是过拟合开始的地方。模型从训练数据集中学习得太多，并开始将其学习中的噪声纳入其中。这可能在训练数据集上显示出高性能，但模型无法泛化预测。这导致对未见数据（如验证数据集中的数据）的预测不佳。
- en: So, the best thing to do is to stop the model at the exact point where the performance
    of the model is highest for both the training and validation dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最好的做法是在模型性能在训练和验证数据集上最高的确切点停止模型。
- en: Now that we have a basic understanding of how early stopping of model training
    works, let’s learn how we can perform it using the early stopping parameter offered
    by the H2O AutoML function.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对模型训练早期停止的工作原理有了基本了解，接下来让我们学习如何使用H2O AutoML函数提供的早期停止参数来执行它。
- en: Working with early stopping parameters in H2O AutoML
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在H2O AutoML中处理早期停止参数
- en: H2O AutoML has provisions for you to implement and control the early stopping
    of your models that it will auto train for you.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: H2O AutoML为您提供了实施和控制它将为您自动训练的模型早期停止的选项。
- en: 'You can use the following parameters to implement early stopping:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下参数来实现早期停止：
- en: '`stopping_rounds`: This parameter indicates the number of training rounds over
    which if the stopping metric fails to improve, we stop the model training.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_rounds`：此参数表示在停止指标未能改善的情况下，我们停止模型训练的训练轮数。'
- en: '`stopping_metric`: This parameter is used to select the performance metric
    to consider when early stopping. It is available if `stopping_rounds` is set and
    is greater than *0*. We studied performance metrics in [*Chapter 6*](B17298_06.xhtml#_idTextAnchor129),
    *Understanding H2O AutoML Leaderboard and Other Performance Metrics*, so kindly
    refer to that chapter if you wish to revise how the different metrics measure
    performance. The available options for this parameter are as follows:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_metric`：此参数用于选择在早期停止时考虑的性能指标。如果`stopping_rounds`被设置并且大于*0*，则此参数可用。我们在[*第6章*](B17298_06.xhtml#_idTextAnchor129)，“理解H2O
    AutoML排行榜和其他性能指标”中研究了性能指标，如果您想复习不同指标如何衡量性能，请参阅该章节。此参数的可用选项如下：'
- en: '`AUTO`: This is the default value and further defaults to the following values,
    depending on the type of ML problem:'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AUTO`：这是默认值，并且根据机器学习问题的类型进一步默认为以下值：'
- en: '`logloss`: The default stopping metric for classification problems.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logloss`：分类问题的默认停止指标。'
- en: '`deviance`: The default stopping metric for regression problems. This stands
    for mean residual deviance.'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deviance`：回归问题的默认停止指标。这代表平均残差偏差。'
- en: '`anomaly_score`: The default stopping metric for Isolation Forest models, which
    are a type of ensemble model.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`anomaly_score`：Isolation Forest模型（集成模型）的默认停止指标。'
- en: '`anomaly_score`: The default stopping metric for Isolation Forest models (ensemble
    models). It is the measure of normality of an observation equivalent to the number
    of splits in a decision tree needed to isolate a point in a given tree where that
    point is at max depth.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`anomaly_score`：Isolation Forest模型（集成模型）的默认停止指标。它是观察正常性的度量，相当于在决策树中隔离给定树中某点的最大深度所需的分割数。'
- en: '`deviance`: This stands for mean residual deviance. This value tells us how
    well the label value can be predicted by a model based on the number of features
    in the dataset.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deviance`：这代表平均残差偏差。此值告诉我们，基于数据集中的特征数量，模型可以有多好地预测标签值。'
- en: '`logloss`: Log loss is a metric that is a way of measuring the performance
    of a classification model that outputs classification results in the form of probability
    values.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logloss`：Log loss是一种衡量输出概率值形式的分类模型性能的指标。'
- en: '`MSE` (`RMSE` (`MAE` (`RMSLE` (`AUC` (`AUCPR` (`lift_top_group`: This parameter
    configures AutoML in such a way that the model being trained must improve its
    lift within the top 1% of the training data. Lift is nothing but the measure of
    performance of a model in making accurate predictions, compared to a model that
    randomly makes predictions. The top 1% of the dataset are the observations with
    the highest predicted values.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MSE`（`RMSE`（`MAE`（`RMSLE`（`AUC`（`AUCPR`（`lift_top_group`：此参数配置AutoML，使得正在训练的模型必须在训练数据的前1%内提高其提升度。提升度不过是衡量模型在做出准确预测方面的性能，与随机预测的模型相比。数据集的前1%是具有最高预测值的观测值。'
- en: '`misclassification`: This metric is used to measure the fraction of the predictions
    that were incorrectly predicted without distinguishing between positive and negative
    predictions.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`misclassification`：此指标用于衡量预测错误的预测比例，不区分正负预测。'
- en: '`mean_per_class_error`: This is a metric that calculates the average of all
    errors per class in a dataset containing multiple classes.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mean_per_class_error`：这是一个指标，它计算包含多个类别的数据集中每类的所有错误的平均值。'
- en: '`custom`: This parameter is used to set any custom metric as the stopping metric
    during AutoML training. The custom metric should be of the behavior *less is better*,
    meaning the lower the value of the custom metric, the better the performance of
    the model. The lower bound value of the custom metric is assumed to be 0.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`custom`：此参数用于在AutoML训练期间将任何自定义指标设置为停止指标。自定义指标的行为应为“越少越好”，意味着自定义指标值越低，模型性能越好。自定义指标的假设下限值为0。'
- en: '`custom_increasing`: This parameter is for custom performance metrics that
    have the behavior as *more is better*, meaning the higher the value of these metrics,
    the better the model performance. At the time of writing, this parameter is only
    supported in the Python client for GBM and DRF.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`custom_increasing`：此参数用于具有“越多越好”行为的自定义性能指标，意味着这些指标值越高，模型性能越好。在撰写本文时，此参数仅在GBM和DRF的Python客户端中受支持。'
- en: '`stopping_tolerance`: This parameter indicates the tolerance value by which
    the model’s performance metric must improve before stopping the model training.
    It is available if `stopping_rounds` is set and is greater than *0*. The default
    stopping tolerance for AutoML is *0.001* if the dataset contains at least 1 million
    rows; otherwise, the value is determined by the size of the dataset and the amount
    of non-NA data in the dataset, which leads to a value greater than *0.001*'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stopping_tolerance`：此参数表示模型性能指标必须改善的容忍值，以便停止模型训练。如果设置了`stopping_rounds`并且大于**0**，则此参数可用。如果数据集包含至少1百万行，AutoML的默认停止容忍度为**0.001**；否则，该值由数据集的大小和数据集中非NA数据量决定，导致值大于**0.001**。'
- en: 'In H2O Flow, these parameters are available in the **ADVANCED** section of
    the **Run AutoML** parameters, as shown in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O Flow中，这些参数可在**运行AutoML**参数的**高级**部分找到，如下面的截图所示：
- en: '![Figure 8.6 – Early stopping parameters in H2O Flow ](img/B17298_08_006.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – H2O Flow中的早期停止参数](img/B17298_08_006.jpg)'
- en: Figure 8.6 – Early stopping parameters in H2O Flow
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – H2O Flow中的早期停止参数
- en: 'In the Python programming language, you can set these parameters as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python编程语言中，您可以按以下方式设置这些参数：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the R programming language, you can set these parameters as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在R编程语言中，您可以按以下方式设置这些参数：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To better understand how AutoML will stop a model's training early, consider
    the same Python and R example values. We have `stopping_metric` as `stopping_rounds`
    as `stopping_tolerance` as **0.001**.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解AutoML如何提前停止模型训练，请考虑相同的Python和R示例值。我们将`stopping_metric`设置为`stopping_rounds`，将`stopping_tolerance`设置为**0.001**。
- en: When implementing early stopping, H2O will calculate the moving average of the
    last `stopping_tolerance` of *0.001*, then H2O will stop the model training. For
    performance metrics that have the *more is better* behavior, the ratio between
    the best moving average and reference moving average should be less than or equal
    to the stopping tolerance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现早期停止时，H2O将计算最后`stopping_tolerance`的**0.001**的移动平均值，然后H2O将停止模型训练。对于具有“越多越好”行为的性能指标，最佳移动平均值与参考移动平均值之比应小于或等于停止容忍度。
- en: Now that we understand how to stop model training early using the `stopping_rounds`,
    `stopping_metrics`, and `stopping tolerance` parameters, let’s understand the
    next optional parameter in AutoML – that is, cross-validation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用`stopping_rounds`、`stopping_metrics`和`stopping tolerance`参数来提前停止模型训练，接下来让我们了解AutoML中的下一个可选参数——那就是交叉验证。
- en: Experimenting with parameters that support cross-validation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试支持交叉验证的参数
- en: When performing model training on a dataset, we usually perform a train-test
    split on the dataset. Let’s assume we split it in the ratio of 70% and 30%, where
    70% is used to create the training dataset and the remaining 30% is used to create
    the test dataset. Then, we pass the training dataset to the ML system for training
    and use the test dataset to calculate the performance of the model. A train-test
    split is often performed in a random state, meaning 70% of the data that was used
    to create the training dataset is often chosen at random from the original dataset
    without replacement, except in the case of time-series data, where the order of
    the events needs to be maintained or in the case where we need to keep the classes
    stratified. Similarly, for the test dataset, 30% of the data is chosen at random
    from the original dataset to create the test dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集进行模型训练时，我们通常会对数据集进行训练-测试划分。假设我们按照70%和30%的比例进行划分，其中70%用于创建训练数据集，剩下的30%用于创建测试数据集。然后，我们将训练数据集传递给机器学习系统进行训练，并使用测试数据集来计算模型的性能。训练-测试划分通常在随机状态下进行，这意味着用于创建训练数据集的70%的数据通常是从原始数据集中随机选择，且不进行替换，除非是时间序列数据，其中事件的顺序需要保持，或者在我们需要保持类别分层的情况下。同样，对于测试数据集，30%的数据也是从原始数据集中随机选择来创建测试数据集。
- en: 'The following diagram shows how data from the dataset is randomly picked to
    create the training and testing datasets for their respective purposes:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了如何从数据集中随机选择数据来创建用于各自目的的训练和测试数据集：
- en: '![Figure 8.7 – Train-test split on the dataset ](img/B17298_08_007.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 数据集上的训练-测试划分](img/B17298_08_007.jpg)'
- en: Figure 8.7 – Train-test split on the dataset
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 数据集上的训练-测试划分
- en: Now, the issue with the train-test split is that when 30% of the data kept outside
    the testing dataset is not used to train the model, any missing knowledge that
    could be derived from this data isn’t available to train the model. This leads
    to a loss in the performance of the model. If you retrain a model using a different
    random state for the train-test split, then the model will end up having a different
    performance level as it has been trained on different data records. Thus, the
    performance of the models depends on the random assignment of the training dataset.
    So, how can we provide the test data for training as well as keeping some test
    data for performance measurement? This is where cross-validation comes into play.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练-测试划分的问题在于，当30%的数据被保留在测试数据集之外而没有用于训练模型时，从这些数据中可以推导出的任何知识都无法用于训练模型，这会导致模型性能的损失。如果你使用不同的随机状态重新训练模型进行训练-测试划分，那么模型最终将具有不同的性能水平，因为它是在不同的数据记录上训练的。因此，模型的性能取决于训练数据集的随机分配。那么，我们如何既为训练提供测试数据，又保留一些测试数据进行性能测量呢？这就是交叉验证发挥作用的地方。
- en: Understanding cross-validation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解交叉验证
- en: '**Cross-validation** is a model validation technique that resamples data to
    train and test models. The technique uses different parts of the dataset for training
    and testing during each iteration. Multiple iterations of model training and testing
    are performed using different parts of the dataset. The performance results are
    combined to give an average estimation of the model’s performance.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**是一种模型验证技术，它通过重新采样数据来训练和测试模型。该技术在每个迭代中使用数据集的不同部分进行训练和测试。通过使用数据集的不同部分进行多次模型训练和测试，然后将性能结果合并，以给出模型性能的平均估计。'
- en: Let’s try to understand this with an example. Let’s assume your dataset contains
    around 1,000 records. To perform cross-validation, you must split the dataset
    into a ratio – let’s assume a 1:9 ratio where we have 100 records for the test
    dataset and 900 records for the training dataset. Then, you perform model training
    on the training dataset. Once the model has been trained, you must test the model
    on the test dataset and note its performance. This is your first iteration of
    cross-validation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来尝试理解这个概念。假设你的数据集包含大约1,000条记录。为了进行交叉验证，你必须将数据集分成一个比例——假设是1:9的比例，其中测试数据集有100条记录，训练数据集有900条记录。然后，你在训练数据集上执行模型训练。一旦模型训练完成，你必须测试模型在测试数据集上的性能，并记录其性能。这是你的第一次交叉验证迭代。
- en: In the next iteration, you split the dataset in the same ratio of 1/9 records
    for the testing and training datasets, respectively, but this time, you choose
    different data records to form your test dataset and use the remaining records
    as the training dataset. Then, you perform model training on the training dataset
    and calculate the model’s performance on the testing dataset. You repeat the same
    experiment using different data records until all the dataset has been used for
    training as well as testing. You will need to perform around 10 iterations of
    cross-validation so that, during the entire cross-validation process, the model
    is trained and tested on the entire dataset each iteration while containing different
    data records in the testing DataFrame.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一次迭代中，你以相同的1/9记录比例分割数据集，用于测试和训练数据集，但这次，你选择不同的数据记录来形成你的测试数据集，并使用剩余的记录作为训练数据集。然后，你在训练数据集上执行模型训练，并计算模型在测试数据集上的性能。你将使用不同的数据记录重复相同的实验，直到所有数据集都被用于训练和测试。你需要进行大约10次交叉验证，这样在整个交叉验证过程中，模型在每个迭代中都会在全部数据集上训练和测试，同时测试数据框中包含不同的数据记录。
- en: Once all the iterations have finished, you must combine the performance results
    of the experiments and provide the average estimation of the model’s performance.
    This technique is called cross-validation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有迭代完成，你必须合并实验的性能结果，并提供模型性能的平均估计。这种技术称为交叉验证。
- en: 'You may have noticed that during cross-validation, we perform model training
    multiple times on the same dataset. This is expected to increase the overall ML
    process time. This is especially true when performing cross-validation on a large
    dataset with a very high ratio between the training and testing partition. For
    example, if we have a dataset that contains 30,000 rows and we split the dataset
    into 29,000 rows for training and 1,000 rows for testing, then this will lead
    to a total of 3,000 iterations of model training and testing. Hence, there is
    an alternative form of cross-validation that lets you choose how many iterations
    to perform: called **K-fold cross-validation**.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在交叉验证过程中，我们在同一个数据集上多次执行模型训练。这预计会增加整体机器学习过程的时间。这在执行具有非常高的训练和测试分区比的大数据集的交叉验证时尤其如此。例如，如果我们有一个包含30,000行的数据集，我们将数据集分成29,000行用于训练和1,000行用于测试，那么这将导致总共3,000次模型训练和测试的迭代。因此，有一种交叉验证的替代形式，允许你选择要执行多少次迭代：称为**K折交叉验证**。
- en: 'In K-fold cross-validation, you decide the value of **K**, which is used to
    determine the number of cross-validation iterations to perform. Depending on the
    value of K, the ML service will randomly partition the dataset into K equal subsamples
    that will be resampled over the cross-validation iterations. The following diagram
    will help you understand this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在K折交叉验证中，你决定K的值，该值用于确定要执行多少次交叉验证迭代。根据K的值，机器学习服务将随机将数据集分成K个相等的子集，这些子集将在交叉验证迭代中重采样。以下图表将帮助您理解这一点：
- en: '![Figure 8.8 – K-fold cross-validation where K=3 ](img/B17298_08_008.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – K折交叉验证，其中K=3](img/B17298_08_008.jpg)'
- en: Figure 8.8 – K-fold cross-validation where K=3
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – K折交叉验证，其中K=3
- en: As you can see, we have a dataset that contains 30,000 data records and the
    chosen value of K in the K-fold cross-validation is 3\. Accordingly, the dataset
    will be split into 20,000 records for the test dataset and 10,000 records for
    training, which will be resampled during the following iterations, leading to
    a total of three cross-validations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们有一个包含30,000条数据记录的数据集，K折交叉验证中选择的K值为3。因此，数据集将被分成20,000条记录用于测试数据集和10,000条记录用于训练，这些记录将在接下来的迭代中进行重采样，从而总共进行三次交叉验证。
- en: The benefit of using K-fold cross-validation to perform your model validation
    is that the model is trained on the entire dataset without missing out on data
    during training. This is especially beneficial in multi-class classification problems
    where there are chances that the model might miss out training on some of the
    prediction classes because it got split out from the training dataset to be used
    in the testing dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K折交叉验证进行模型验证的好处是模型在完整数据集上训练，而不会在训练过程中丢失数据。这在多类分类问题中尤其有益，因为模型可能会错过某些预测类别的训练，因为它被从训练数据集中分割出来用于测试数据集。
- en: Now that we have a better understanding of the basics of cross-validation and
    how it works, let’s see how we can perform it using special parameters in the
    H2O AutoML training function.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更好地理解了交叉验证的基本原理及其工作方式，让我们看看如何使用H2O AutoML训练函数中的特殊参数来执行它。
- en: Working with cross-validation parameters in H2O AutoML
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在H2O AutoML中处理交叉验证参数
- en: H2O AutoML has provisions for you to implement K-fold cross-validation on your
    data for all ML algorithms that support it, along with some additional information
    that may help support the implementation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: H2O AutoML为您提供了在支持它的所有ML算法上实现K折交叉验证的选项，以及一些可能有助于支持实现的其他信息。
- en: 'You can use the following parameters to implement cross-validation:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下参数来实现交叉验证：
- en: '`nfolds`: This parameter sets the number of folds to use for K-fold cross-validation.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nfolds`: 此参数设置用于K折交叉验证的折叠数。'
- en: 'In H2O Flow, this parameter will be available in the **ADVANCED** section of
    the **Run AutoML** parameters, as shown in the following screenshot:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O Flow中，此参数将在**运行AutoML**参数的**高级**部分中可用，如下面的截图所示：
- en: '![Figure 8.9 – The nfolds parameter in H2O Flow ](img/B17298_08_009.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – H2O Flow中的nfolds参数](img/B17298_08_009.jpg)'
- en: Figure 8.9 – The nfolds parameter in H2O Flow
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – H2O Flow中的nfolds参数
- en: '`fold_assignment`: This parameter is used to specify the fold assignment scheme
    to use to perform K-fold cross-validation. The various types of fold assignments
    you can set are as follows:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fold_assignment`: 此参数用于指定用于执行K折交叉验证的折叠分配方案。您可以设置的折叠分配类型如下：'
- en: '`AUTO`: This assignment value lets the model training algorithm choose the
    fold assignment to use. `AUTO` currently uses `Random` as the fold assignment.'
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AUTO`: 这个赋值允许模型训练算法选择要使用的折叠分配。`AUTO`当前使用`Random`作为折叠分配。'
- en: '`Random`: This assignment value is used to randomly split the dataset based
    on the `nfolds` value. This value is set by default if `nfolds > 0` and `fold_column`
    is not specified.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Random`: 这个赋值用于根据`nfolds`值随机分割数据集。如果`nfolds > 0`且未指定`fold_column`，则默认设置此值。'
- en: '`Modulo`: This assignment value is used to perform a modulo operation when
    splitting the folds based on the `nfolds` value.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Modulo`: 这个赋值用于在根据`nfolds`值分割折叠时执行模运算。'
- en: '`Stratified`: This assignment value is used to arrange the folds based on the
    response variable for classification problems.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Stratified`: 这个赋值用于根据分类问题的响应变量来安排折叠。'
- en: 'In the Python programming language, you can set these parameters as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python编程语言中，您可以如下设置这些参数：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the R programming language, you can set these parameters as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在R编程语言中，您可以如下设置这些参数：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`fold_column`: This parameter is used to specify the fold assignment based
    on the contents of a column rather than any procedural assignment technique. You
    can custom set the fold values per row in the dataset by creating a separate column
    containing the fold IDs and then setting `fold_column` to the custom column’s
    name.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fold_column`: 此参数用于根据列的内容而不是任何程序性分配技术来指定折叠分配。您可以通过创建一个包含折叠ID的单独列并设置`fold_column`为自定义列的名称来为数据集中的每一行自定义设置折叠值。'
- en: 'In H2O Flow, this parameter will be available in the **ADVANCED** section of
    the **Run AutoML** parameters, as shown in the following screenshot:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O Flow中，此参数将在**运行AutoML**参数的**高级**部分中可用，如下面的截图所示：
- en: '![Figure 8.10 – The fold_column parameter in H2O Flow ](img/B17298_08_010.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – H2O Flow中的fold_column参数](img/B17298_08_010.jpg)'
- en: Figure 8.10 – The fold_column parameter in H2O Flow
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – H2O Flow中的fold_column参数
- en: 'In the Python programming language, you can set these parameters as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python编程语言中，您可以如下设置这些参数：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the R programming language, you can set these parameters as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在R编程语言中，您可以如下设置这些参数：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`keep_cross_validation_predictions`: When performing K-fold cross-validation,
    H2O will train *K+1* number of models, where *K* number of models are trained
    as a part of cross-validation and *1* additional model is trained on the entire
    dataset. Each of the cross-validation models makes predictions on the test DataFrame
    for that iteration and the predicted values are stored in a prediction frame.
    You can save these prediction frames by setting this parameter to *True*. By default,
    this parameter is set to *False*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_cross_validation_predictions`：在进行K折交叉验证时，H2O将训练*K+1*个模型，其中*K*个模型作为交叉验证的一部分进行训练，另外*1*个模型在全部数据集上训练。每个交叉验证模型都会对那个迭代的测试DataFrame进行预测，预测值存储在预测框架中。您可以通过将此参数设置为*True*来保存这些预测框架。默认情况下，此参数设置为*False*。'
- en: '`keep_cross_validation_models`: Similar to `keep_cross_validation_predictions`,
    you can also choose to keep the models trained during cross-validation for further
    inspection and experimentation by enabling this parameter to *True*. By default,
    this parameter is set to *False*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_cross_validation_models`：类似于`keep_cross_validation_predictions`，您也可以选择通过将此参数设置为*True*来保留交叉验证期间训练的模型，以便进行进一步的检查和实验。默认情况下，此参数设置为*False*。'
- en: '`keep_cross_validation_fold_assignment`: During cross-validation, the data
    is split either by the `fold_cloumn` or `fold_assignment` parameter. You can save
    the fold assignment that was used in cross-validation by setting this parameter
    to *True*. By default, this parameter is set to *False*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keep_cross_validation_fold_assignment`：在交叉验证期间，数据是通过`fold_cloumn`或`fold_assignment`参数进行分割的。您可以通过将此参数设置为*True*来保存交叉验证中使用的分割分配。默认情况下，此参数设置为*False*。'
- en: In H2O Flow, these parameters will be available in the **EXPERT** section of
    the **Run AutoML** parameters, as shown in the following screenshot.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O Flow中，这些参数将在**运行AutoML**参数的**专家**部分中可用，如下面的截图所示。
- en: '![Figure 8.11 – Advanced cross-validation parameters in H2O Flow ](img/B17298_08_011.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11 – H2O Flow中的高级交叉验证参数](img/B17298_08_011.jpg)'
- en: Figure 8.11 – Advanced cross-validation parameters in H2O Flow
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – H2O Flow中的高级交叉验证参数
- en: 'In the Python programming language, you can set these parameters as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python编程语言中，您可以如下设置这些参数：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the R programming language, you can set these parameters as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在R编程语言中，您可以如下设置这些参数：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Congratulations – you have now understood a few more advanced ML concepts and
    how to use them in H2O AutoML!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您 – 您现在已经理解了更多高级机器学习概念以及如何在H2O AutoML中使用它们！
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about some of the optional parameters that are available
    to us in H2O AutoML. We started by understanding what imbalanced classes in a
    dataset are and how they can cause trouble when training models. Then, we understood
    oversampling and undersampling, which we can use to tackle this. After that, we
    learned how H2O AutoML provides parameters for us to control the sampling techniques
    so that we can handle imbalanced classes in datasets.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了H2O AutoML中可用的某些可选参数。我们首先理解了数据集中不平衡类别的含义以及它们在训练模型时可能引起的问题。然后，我们了解了过采样和欠采样，我们可以使用这些方法来解决这个问题。之后，我们学习了H2O
    AutoML如何为我们提供参数，以便我们控制采样技术，从而处理数据集中的不平衡类别。
- en: After that, we understood another concept, called early stopping. We understood
    how overtraining can lead to an overfitted ML model that performs very poorly
    against unseen new data. We also learned that early stopping is a method that
    we can use to stop model training once we start noticing that the model has started
    overfitting by monitoring the performance of the model against the validation
    dataset. We then learned about the various parameters that H2O AutoML has that
    we can use to automatically stop model training once overfitting occurs during
    model training.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们了解了一个名为早期停止的概念。我们明白了过拟合如何导致机器学习模型在未见过的新的数据上表现非常糟糕。我们还了解到，早期停止是一种方法，一旦我们开始注意到模型开始过拟合，通过监控模型在验证数据集上的性能，我们可以使用这种方法来停止模型训练。然后，我们学习了H2O
    AutoML提供的各种参数，这些参数可以在模型训练过程中一旦发生过拟合时自动停止模型训练。
- en: Next, we understood what cross-validation is and how it helps us train the model
    on the entire dataset, as well as validate the model’s performance as if the model
    had seen the data for the first time. We also learned how K-fold cross-validation
    helps us control the number of cross-validation iterations to be performed during
    model training. Then, we explored how H2O AutoML has various provisions for performing
    cross-validation during AutoML training. Finally, we learned how we can keep the
    cross-validation models and predictions if we wish to perform more experiments
    on them, as well as how we can store the cross-validation fold assignments.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们了解了交叉验证是什么，以及它是如何帮助我们在整个数据集上训练模型，并像模型第一次看到数据一样验证模型性能的。我们还学习了K折交叉验证是如何帮助我们控制模型训练期间要执行的交叉验证迭代次数的。然后，我们探讨了H2O
    AutoML在AutoML训练期间执行交叉验证的各种规定。最后，我们学习了如果我们想对它们进行更多实验，我们如何保留交叉验证模型和预测，以及我们如何存储交叉验证的折分配。
- en: In the next chapter, we shall explore some of the miscellaneous features that
    H2O AutoML has that can be useful to us in certain scenarios.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨H2O AutoML的一些杂项功能，这些功能在某些场景下对我们可能很有用。
