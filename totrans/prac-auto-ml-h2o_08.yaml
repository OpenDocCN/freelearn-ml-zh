- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Optional Parameters for H2O AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we explored in [*Chapter 2*](B17298_02.xhtml#_idTextAnchor038), *Working
    with H2O Flow (H2O’s Web UI)*, when training models using H2O AutoML, we had plenty
    of parameters to select. All these parameters gave us the capability to control
    how H2O AutoML should train our models. This control helps us get the best possible
    use of AutoML based on our requirements. Most of the parameters we explored were
    pretty straightforward to understand. However, there were some parameters whose
    purpose and effects were slightly complex to be understood at the very start of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we shall explore these parameters by learning about the **Machine
    Learning** (**ML**) concepts behind them, and then understand how we can use them
    in an AutoML setting.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will not only be educated in some of the advanced
    ML concepts, but you will also be able to implement them using the parametric
    provisions made in H2O AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with parameters that support imbalanced classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with parameters that support early stopping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with parameters that support cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by understanding what imbalanced classes are.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will require the following to complete this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of your preferred web browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The H2O software installed on your system. Refer to [*Chapter 1*](B17298_01.xhtml#_idTextAnchor017),
    *Understanding H2O AutoML Basics*, for instructions on how to install H2O on your
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: All the H2O AutoML function parameters shown in this chapter are shown using
    H2O Flow to keep things simple. The equivalent parameters are also available in
    the Python and R programming languages for software engineers to code into their
    services. You can find these details at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/parameters.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with parameters that support imbalanced classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common problem you will often face in the field of ML is classifying rare
    events. Consider the case of large earthquakes. Large earthquakes of magnitude
    7 and higher occur about once every year. If you had a dataset containing the
    Earth’s tectonic activity of each day since the last decade with the response
    column containing whether or not an earthquake occurred, then you would have approximately
    3,650 rows of data; that is, one row for each day in the decade, with around 8-12
    rows showing large earthquakes. That is less than a 0.3% chance that this event
    will occur. 99.7% of the time, there will be no large earthquakes. This dataset,
    where the number of large earthquake events is so small, is called an **imbalanced
    dataset**.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the imbalanced dataset is that even if you write a simple `if-else`
    function that marks all tectonic events as not earthquakes and call this a model,
    it will still show the accuracy as 99.7% accuracy since the majority of the events
    are not earthquake-causing. However, in actuality, this so-called model is very
    bad as it is not correctly informing you whether it is an earthquake or not.
  prefs: []
  type: TYPE_NORMAL
- en: Such imbalance in the **target class** creates a lot of issues when training
    ML models. The ML models are more likely to assume that these events are so rare
    that they will never occur and will not learn the distinction between those events.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are ways to tackle this issue. One way is to undersample the
    majority class and the other way is to oversample the minority class. We shall
    learn more about these techniques in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding undersampling the majority class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the scenario of predicting the occurrence of earthquakes, the dataset contains
    a large number of events that have been identified as *not-earthquake*. This event
    is known as the majority class. The few events that mark the activity as an *earthquake*
    are known as the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how **undersampling the majority class** can solve the problems caused
    by an imbalance in the classes. Consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Undersampling an imbalanced dataset ](img/B17298_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Undersampling an imbalanced dataset
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume you have 3,640 data samples of tectonic activity that indicate
    no earthquakes happened and only 10 samples that indicate earthquakes happened.
    In this case, to tackle this imbalance issue, you must create a bootstrapped dataset
    containing all 10 samples of the minority class, and 10 samples of the majority
    class chosen at random from the 3,640 data samples. Then, you can feed this new
    dataset to H2O AutoML for training. In this case, we have undersampled the majority
    class and equalized the *earthquake* and *not-earthquake* data samples before
    training the model.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of this approach is that we end up tossing away huge amounts of
    data, and the model won’t be able to learn a lot from the reduced data.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding oversampling the minority class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second approach to tackling the imbalanced dataset issue is to **oversample
    the minority class**. One obvious way is to duplicate the minority class data
    samples and append them to the dataset so that the number of data samples between
    the majority and minority classes is equal. Refer to the following diagram for
    a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Oversampling an imbalanced dataset ](img/B17298_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Oversampling an imbalanced dataset
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, you can see that we replicated the minority class
    data samples and appended them to the dataset so that we ended up with 3,640 rows
    for each of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can work; however, oversampling will lead to an explosion in the
    size of the dataset. You need to make sure that it does not exceed your computation
    and memory limits and end up failing.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the basics of class balancing using undersampling and
    oversampling, let’s see how H2O AutoML handles it using its class balancing parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Working with class balancing parameters in H2O AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: H2O AutoML has a parameter called `balance_classes` that accepts a boolean value.
    If set to *True*, H2O performs oversampling on the minority class and undersampling
    on the majority class. The balancing is performed in such a way that eventually,
    each class contains the same number of data samples.
  prefs: []
  type: TYPE_NORMAL
- en: Both undersampling and oversampling of the respective classes is done randomly.
    Additionally, oversampling of the minority class is done with replacement. This
    means that data samples from the minority class can be chosen and added to the
    new training dataset multiple times and can be repeated.
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O AutoML has the following parameters that support class balancing functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`balance_classes`: This parameter accepts a boolean value. It is *False* by
    default, but if you want to perform class balancing on your dataset before feeding
    it to H2O AutoML for training, then you can set the boolean value to *True*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In H2O Flow, you get a checkbox besides the parameter. Refer to the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The balance_classes checkbox in H2O Flow ](img/B17298_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The balance_classes checkbox in H2O Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking it makes the `class_sampling_factors` and `max_after_balance_size`
    parameters available in the **EXPERT** section of the **Run AutoML** parameters,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The class_sampling_factors and max_after_balance_size parameters
    in the EXPERT section ](img/B17298_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – The class_sampling_factors and max_after_balance_size parameters
    in the EXPERT section
  prefs: []
  type: TYPE_NORMAL
- en: '`class_sampling_factors`: This parameter requires `balance_classes` to be *True*.
    This parameter takes a list of float values as input that will represent the sampling
    rate for that class. A sampling rate of value *1.0* for a given class will not
    change its sample rate during class balancing. A sampling rate of *0.5* will halve
    the sample rate of a class during class balancing while a sampling rate of *2.0*
    will double it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_after_balance_size`: This parameter requires `balance_classes` to be *True*
    and specifies the maximum relative size of the training dataset after balancing.
    This parameter accepts a `float` value as input, which would limit the size your
    training dataset can grow to. The default value is *5.0*, which indicates that
    the training dataset will grow a maximum of *5* times its size. This value can
    also be less than *1.0*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Python programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, in the R programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To perform class balancing when training models using AutoML, you can set the
    `balance_classes` parameter to true in the H2O AutoML estimator object. In that
    same object, you can specify your `class_sampling_factors` and `max_after_balance_size`
    parameters. Then, you can use this initialized AutoML estimator object to trigger
    AutoML on your training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand how we can tackle the class imbalance issue using the
    `balance_classes`, `class_sampling_factors`, and `max_after_balance_size` parameters,
    let’s understand the next optional parameters in AutoML – that is, stopping criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with parameters that support early stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Overfitting** models is one of the common issues often faced when trying
    to solve an ML problem. Overfitting is said to have occurred when the ML model
    tries to adapt to your training set too much, so much so that it is only able
    to make predictions on values that it has seen before in the training set and
    is unable to make a generalized prediction on unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting occurs due to a variety of reasons, one of them being that the model
    learns so much from the dataset that it even incorporates and learns the noise
    in the dataset. This learning negatively impacts predictions on new data that
    may not have that noise. So, how do we tackle this issue and prevent the model
    from overfitting? Stop the model early before it learns the noise.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sub-sections, we shall understand what early stopping is and
    how it is done. Then, we will learn how the early stopping parameters offered
    by H2O AutoML work.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding early stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Early stopping** is a form of **regularization** that stops a model’s training
    once it has achieved a satisfactory understanding of the data and further prevents
    it from overfitting. Early stopping aims to observe the model’s performance as
    it improves using an appropriate performance metric and stop the model’s training
    once deterioration is observed due to overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: When training a model using algorithms that use iterative optimization to minimize
    the loss function, the training dataset is passed through the algorithm during
    each iteration. Observations and understandings that pass are then used during
    the next iteration. This iteration of passing the training dataset through the
    algorithm is called an **epoch**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For early stopping, at the end of every epoch, we can calculate the performance
    of the model and note down the metric value. Comparing these values during every
    iteration helps us understand whether the model is improving its performance after
    every epoch or whether it is learning noise and losing performance. We can monitor
    this and stop the model training at the epoch where we start seeing a decrease
    in performance. Refer to the following diagram to gain a better understanding
    of early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Early stopping to avoid model overfitting ](img/B17298_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Early stopping to avoid model overfitting
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, on the *Y* axis, we have the **Performance** value
    of the model. On the *X*-axis, we have the **Epoch** value. So, as time goes on
    and we iterate through the number of epochs, we see that the performance of the
    model on the training set and the validation set continues to increase. But after
    a certain point, the performance of the model on the validation dataset starts
    decreasing, while the performance of the training dataset continues to increase.
    This is where overfitting starts. The model learns too much from the training
    dataset and starts incorporating noise into its learning. This might show high
    performance on the training dataset, but the model fails to generalize the predictions.
    This leads to bad predictions on unseen data, such as the ones in the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: So, the best thing to do is to stop the model at the exact point where the performance
    of the model is highest for both the training and validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how early stopping of model training
    works, let’s learn how we can perform it using the early stopping parameter offered
    by the H2O AutoML function.
  prefs: []
  type: TYPE_NORMAL
- en: Working with early stopping parameters in H2O AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: H2O AutoML has provisions for you to implement and control the early stopping
    of your models that it will auto train for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following parameters to implement early stopping:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stopping_rounds`: This parameter indicates the number of training rounds over
    which if the stopping metric fails to improve, we stop the model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_metric`: This parameter is used to select the performance metric
    to consider when early stopping. It is available if `stopping_rounds` is set and
    is greater than *0*. We studied performance metrics in [*Chapter 6*](B17298_06.xhtml#_idTextAnchor129),
    *Understanding H2O AutoML Leaderboard and Other Performance Metrics*, so kindly
    refer to that chapter if you wish to revise how the different metrics measure
    performance. The available options for this parameter are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO`: This is the default value and further defaults to the following values,
    depending on the type of ML problem:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logloss`: The default stopping metric for classification problems.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deviance`: The default stopping metric for regression problems. This stands
    for mean residual deviance.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`anomaly_score`: The default stopping metric for Isolation Forest models, which
    are a type of ensemble model.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`anomaly_score`: The default stopping metric for Isolation Forest models (ensemble
    models). It is the measure of normality of an observation equivalent to the number
    of splits in a decision tree needed to isolate a point in a given tree where that
    point is at max depth.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deviance`: This stands for mean residual deviance. This value tells us how
    well the label value can be predicted by a model based on the number of features
    in the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logloss`: Log loss is a metric that is a way of measuring the performance
    of a classification model that outputs classification results in the form of probability
    values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSE` (`RMSE` (`MAE` (`RMSLE` (`AUC` (`AUCPR` (`lift_top_group`: This parameter
    configures AutoML in such a way that the model being trained must improve its
    lift within the top 1% of the training data. Lift is nothing but the measure of
    performance of a model in making accurate predictions, compared to a model that
    randomly makes predictions. The top 1% of the dataset are the observations with
    the highest predicted values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`misclassification`: This metric is used to measure the fraction of the predictions
    that were incorrectly predicted without distinguishing between positive and negative
    predictions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_per_class_error`: This is a metric that calculates the average of all
    errors per class in a dataset containing multiple classes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom`: This parameter is used to set any custom metric as the stopping metric
    during AutoML training. The custom metric should be of the behavior *less is better*,
    meaning the lower the value of the custom metric, the better the performance of
    the model. The lower bound value of the custom metric is assumed to be 0.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`custom_increasing`: This parameter is for custom performance metrics that
    have the behavior as *more is better*, meaning the higher the value of these metrics,
    the better the model performance. At the time of writing, this parameter is only
    supported in the Python client for GBM and DRF.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stopping_tolerance`: This parameter indicates the tolerance value by which
    the model’s performance metric must improve before stopping the model training.
    It is available if `stopping_rounds` is set and is greater than *0*. The default
    stopping tolerance for AutoML is *0.001* if the dataset contains at least 1 million
    rows; otherwise, the value is determined by the size of the dataset and the amount
    of non-NA data in the dataset, which leads to a value greater than *0.001*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In H2O Flow, these parameters are available in the **ADVANCED** section of
    the **Run AutoML** parameters, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Early stopping parameters in H2O Flow ](img/B17298_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Early stopping parameters in H2O Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Python programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the R programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To better understand how AutoML will stop a model's training early, consider
    the same Python and R example values. We have `stopping_metric` as `stopping_rounds`
    as `stopping_tolerance` as **0.001**.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing early stopping, H2O will calculate the moving average of the
    last `stopping_tolerance` of *0.001*, then H2O will stop the model training. For
    performance metrics that have the *more is better* behavior, the ratio between
    the best moving average and reference moving average should be less than or equal
    to the stopping tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to stop model training early using the `stopping_rounds`,
    `stopping_metrics`, and `stopping tolerance` parameters, let’s understand the
    next optional parameter in AutoML – that is, cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with parameters that support cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When performing model training on a dataset, we usually perform a train-test
    split on the dataset. Let’s assume we split it in the ratio of 70% and 30%, where
    70% is used to create the training dataset and the remaining 30% is used to create
    the test dataset. Then, we pass the training dataset to the ML system for training
    and use the test dataset to calculate the performance of the model. A train-test
    split is often performed in a random state, meaning 70% of the data that was used
    to create the training dataset is often chosen at random from the original dataset
    without replacement, except in the case of time-series data, where the order of
    the events needs to be maintained or in the case where we need to keep the classes
    stratified. Similarly, for the test dataset, 30% of the data is chosen at random
    from the original dataset to create the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how data from the dataset is randomly picked to
    create the training and testing datasets for their respective purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Train-test split on the dataset ](img/B17298_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Train-test split on the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now, the issue with the train-test split is that when 30% of the data kept outside
    the testing dataset is not used to train the model, any missing knowledge that
    could be derived from this data isn’t available to train the model. This leads
    to a loss in the performance of the model. If you retrain a model using a different
    random state for the train-test split, then the model will end up having a different
    performance level as it has been trained on different data records. Thus, the
    performance of the models depends on the random assignment of the training dataset.
    So, how can we provide the test data for training as well as keeping some test
    data for performance measurement? This is where cross-validation comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cross-validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Cross-validation** is a model validation technique that resamples data to
    train and test models. The technique uses different parts of the dataset for training
    and testing during each iteration. Multiple iterations of model training and testing
    are performed using different parts of the dataset. The performance results are
    combined to give an average estimation of the model’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand this with an example. Let’s assume your dataset contains
    around 1,000 records. To perform cross-validation, you must split the dataset
    into a ratio – let’s assume a 1:9 ratio where we have 100 records for the test
    dataset and 900 records for the training dataset. Then, you perform model training
    on the training dataset. Once the model has been trained, you must test the model
    on the test dataset and note its performance. This is your first iteration of
    cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next iteration, you split the dataset in the same ratio of 1/9 records
    for the testing and training datasets, respectively, but this time, you choose
    different data records to form your test dataset and use the remaining records
    as the training dataset. Then, you perform model training on the training dataset
    and calculate the model’s performance on the testing dataset. You repeat the same
    experiment using different data records until all the dataset has been used for
    training as well as testing. You will need to perform around 10 iterations of
    cross-validation so that, during the entire cross-validation process, the model
    is trained and tested on the entire dataset each iteration while containing different
    data records in the testing DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Once all the iterations have finished, you must combine the performance results
    of the experiments and provide the average estimation of the model’s performance.
    This technique is called cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may have noticed that during cross-validation, we perform model training
    multiple times on the same dataset. This is expected to increase the overall ML
    process time. This is especially true when performing cross-validation on a large
    dataset with a very high ratio between the training and testing partition. For
    example, if we have a dataset that contains 30,000 rows and we split the dataset
    into 29,000 rows for training and 1,000 rows for testing, then this will lead
    to a total of 3,000 iterations of model training and testing. Hence, there is
    an alternative form of cross-validation that lets you choose how many iterations
    to perform: called **K-fold cross-validation**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In K-fold cross-validation, you decide the value of **K**, which is used to
    determine the number of cross-validation iterations to perform. Depending on the
    value of K, the ML service will randomly partition the dataset into K equal subsamples
    that will be resampled over the cross-validation iterations. The following diagram
    will help you understand this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – K-fold cross-validation where K=3 ](img/B17298_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – K-fold cross-validation where K=3
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have a dataset that contains 30,000 data records and the
    chosen value of K in the K-fold cross-validation is 3\. Accordingly, the dataset
    will be split into 20,000 records for the test dataset and 10,000 records for
    training, which will be resampled during the following iterations, leading to
    a total of three cross-validations.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using K-fold cross-validation to perform your model validation
    is that the model is trained on the entire dataset without missing out on data
    during training. This is especially beneficial in multi-class classification problems
    where there are chances that the model might miss out training on some of the
    prediction classes because it got split out from the training dataset to be used
    in the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of the basics of cross-validation and
    how it works, let’s see how we can perform it using special parameters in the
    H2O AutoML training function.
  prefs: []
  type: TYPE_NORMAL
- en: Working with cross-validation parameters in H2O AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: H2O AutoML has provisions for you to implement K-fold cross-validation on your
    data for all ML algorithms that support it, along with some additional information
    that may help support the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following parameters to implement cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nfolds`: This parameter sets the number of folds to use for K-fold cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In H2O Flow, this parameter will be available in the **ADVANCED** section of
    the **Run AutoML** parameters, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The nfolds parameter in H2O Flow ](img/B17298_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The nfolds parameter in H2O Flow
  prefs: []
  type: TYPE_NORMAL
- en: '`fold_assignment`: This parameter is used to specify the fold assignment scheme
    to use to perform K-fold cross-validation. The various types of fold assignments
    you can set are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AUTO`: This assignment value lets the model training algorithm choose the
    fold assignment to use. `AUTO` currently uses `Random` as the fold assignment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Random`: This assignment value is used to randomly split the dataset based
    on the `nfolds` value. This value is set by default if `nfolds > 0` and `fold_column`
    is not specified.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Modulo`: This assignment value is used to perform a modulo operation when
    splitting the folds based on the `nfolds` value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Stratified`: This assignment value is used to arrange the folds based on the
    response variable for classification problems.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Python programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the R programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`fold_column`: This parameter is used to specify the fold assignment based
    on the contents of a column rather than any procedural assignment technique. You
    can custom set the fold values per row in the dataset by creating a separate column
    containing the fold IDs and then setting `fold_column` to the custom column’s
    name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In H2O Flow, this parameter will be available in the **ADVANCED** section of
    the **Run AutoML** parameters, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – The fold_column parameter in H2O Flow ](img/B17298_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – The fold_column parameter in H2O Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Python programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the R programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`keep_cross_validation_predictions`: When performing K-fold cross-validation,
    H2O will train *K+1* number of models, where *K* number of models are trained
    as a part of cross-validation and *1* additional model is trained on the entire
    dataset. Each of the cross-validation models makes predictions on the test DataFrame
    for that iteration and the predicted values are stored in a prediction frame.
    You can save these prediction frames by setting this parameter to *True*. By default,
    this parameter is set to *False*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_cross_validation_models`: Similar to `keep_cross_validation_predictions`,
    you can also choose to keep the models trained during cross-validation for further
    inspection and experimentation by enabling this parameter to *True*. By default,
    this parameter is set to *False*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keep_cross_validation_fold_assignment`: During cross-validation, the data
    is split either by the `fold_cloumn` or `fold_assignment` parameter. You can save
    the fold assignment that was used in cross-validation by setting this parameter
    to *True*. By default, this parameter is set to *False*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In H2O Flow, these parameters will be available in the **EXPERT** section of
    the **Run AutoML** parameters, as shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Advanced cross-validation parameters in H2O Flow ](img/B17298_08_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Advanced cross-validation parameters in H2O Flow
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Python programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the R programming language, you can set these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations – you have now understood a few more advanced ML concepts and
    how to use them in H2O AutoML!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about some of the optional parameters that are available
    to us in H2O AutoML. We started by understanding what imbalanced classes in a
    dataset are and how they can cause trouble when training models. Then, we understood
    oversampling and undersampling, which we can use to tackle this. After that, we
    learned how H2O AutoML provides parameters for us to control the sampling techniques
    so that we can handle imbalanced classes in datasets.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we understood another concept, called early stopping. We understood
    how overtraining can lead to an overfitted ML model that performs very poorly
    against unseen new data. We also learned that early stopping is a method that
    we can use to stop model training once we start noticing that the model has started
    overfitting by monitoring the performance of the model against the validation
    dataset. We then learned about the various parameters that H2O AutoML has that
    we can use to automatically stop model training once overfitting occurs during
    model training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we understood what cross-validation is and how it helps us train the model
    on the entire dataset, as well as validate the model’s performance as if the model
    had seen the data for the first time. We also learned how K-fold cross-validation
    helps us control the number of cross-validation iterations to be performed during
    model training. Then, we explored how H2O AutoML has various provisions for performing
    cross-validation during AutoML training. Finally, we learned how we can keep the
    cross-validation models and predictions if we wish to perform more experiments
    on them, as well as how we can store the cross-validation fold assignments.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall explore some of the miscellaneous features that
    H2O AutoML has that can be useful to us in certain scenarios.
  prefs: []
  type: TYPE_NORMAL
