- en: '*Chapter 9*: Hyperparameter Tuning via Optuna'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-optimize`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll be introduced to the `Optuna` package, starting with
    its numerous features, how to utilize it to perform hyperparameter tuning, and
    all of the other important things you need to know about `Optuna`. We’ll not only
    learn how to utilize `Optuna` to perform hyperparameter tuning with their default
    configurations but also discuss the available configurations along with their
    usage. Moreover, we’ll also discuss how the implementation of the hyperparameter
    tuning methods is related to the theory that we have learned in previous chapters,
    since there may be some minor differences or adjustments made in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand all of the important
    things you need to know about `Optuna` and implement various hyperparameter tuning
    methods available in this package. You’ll also be able to understand each of the
    important parameters of the classes and how they are related to the theory that
    we have learned in previous chapters. Finally, equipped with the knowledge from
    previous chapters, you will also be able to understand what’s happening if there
    are errors or unexpected results and understand how to set up the method configuration
    to match your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main topics that will be discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Optuna
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing TPE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Grid Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Simulated Annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Successive Halving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Hyperband
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will learn how to implement various hyperparameter tuning methods with `Optuna`.
    To ensure that you are able to reproduce the code examples in this chapter, you
    will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (version 3.7 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `pandas` package (version 1.3.4 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `NumPy` package (version 1.21.2 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `Matplotlib` package (version 3.5.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `scikit-learn` package (version 1.0.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `Tensorflow` package (version 2.4.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installed `Optuna` package (version 2.10.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Optuna
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Optuna` is a hyperparameter tuning package in Python that provides several
    hyperparameter tuning methods implementation, such as Grid Search, Random Search,
    Tree-structured Parzen Estimators (TPE), and many more. Unlike `Hyperopt`, which
    assumes we are always working with a minimization problem (see [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074)*,
    Hyperparameter Tuning via Hyperopt*), we can tell `Optuna` the type of optimization
    problem we are working on: minimization or maximization.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Optuna` has two main classes, namely **samplers** and **pruners**. Samplers
    are responsible for performing the hyperparameter tuning optimization, whereas
    pruners are responsible for judging whether we should prune the trials based on
    the reported values. In other words, pruners act like *early stopping methods*
    where we will stop a hyperparameter tuning iteration whenever it seems that there’s
    no additional benefit to continuing the process.'
  prefs: []
  type: TYPE_NORMAL
- en: The built-in implementation for samplers includes several hyperparameter tuning
    methods that we have learned in *Chapters 3 - 4*, namely Grid Search, Random Search,
    and TPE, and also other methods that are outside of the scope of this book, such
    as CMA-ES, NSGA-II, and many more. We can also define our own custom samplers,
    such as the Simulated Annealing (SA), which will be discussed in the upcoming
    section. Furthermore, `Optuna` also allows us to integrate samplers from another
    package, such as from the `scikit-optimize` (`skopt`) package where we can utilize
    many Bayesian optimization-based methods from there.
  prefs: []
  type: TYPE_NORMAL
- en: Integrations in Optuna
  prefs: []
  type: TYPE_NORMAL
- en: Besides `skopt`, there are also many other integrations provided by `Optuna,`
    including but not limited, to `scikit-learn`, `Keras`, `PyTorch`, `XGBoost`, `LightGBM`,
    `FastAI`, `MLflow`, and many more. For more information about the available integrations,
    please see the official documentation ([https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html](https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html)).
  prefs: []
  type: TYPE_NORMAL
- en: As for pruners, `Optuna` provides both statistics-based and multi-fidelity optimization
    (MFO)-based methods. There are `MedianPruner`, `PercentilePruner`, and `ThresholdPruner`
    for the statistics-based group. `MedianPruner` will prune the trials whenever
    the current trial’s best intermediate result is worse compared to the median of
    the result of the previous trial. `PercentilePruner` will perform pruning when
    the current best intermediate value is part of the bottom percentile from previous
    trials. `ThresholdPruner` will simply perform pruning whenever the predefined
    threshold is met. The MFO-based pruners implemented in `Optuna` are `SuccessiveHalvingPruner`
    and `HyperbandPruner`. Both of them *define the resource as the number of training
    steps or epochs*, not as the number of samples such as in the implementations
    of `scikit-learn`. We will learn how to utilize these MFO-based pruners in the
    upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform hyperparameter tuning with `Optuna`, we can simply perform the following
    simple steps (more detailed steps, including the code implementation, will be
    given through various examples in the upcoming sections):'
  prefs: []
  type: TYPE_NORMAL
- en: Define the `objective` function along with the hyperparameter space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initiate a `study` object via the `create_study()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform hyperparameter tuning by calling the `optimize()` method on the `study`
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on full training data using the best set of hyperparameters
    found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the final trained model on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `Optuna`, we can directly define the hyperparameter space within the `objective`
    function itself. There’s no need to define another dedicated separate object just
    to store the hyperparameter space. This means that implementing conditional hyperparameters
    in `Optuna` becomes very easy since we just need to put them within the corresponding
    `if-else` blocks in the `objective` function. `Optuna` also provides very handy
    hyperparameter sampling distribution methods including `suggest_categorical`,
    `suggest_discrete_uniform`, `suggest_int`, and `suggest_float`.
  prefs: []
  type: TYPE_NORMAL
- en: The `suggest_categorical` method will suggest value from a categorical type
    of hyperparameters, which works similarly with the `random.choice()` method. The
    `suggest_discrete_uniform` can be utilized for a discrete type of hyperparameters,
    which works very similar to the `hp.quniform` in Hyperopt (see [*Chapter 8*](B18753_08_ePub.xhtml#_idTextAnchor074)*,
    Hyperparameter Tuning via Hyperopt*) by sampling uniformly from the range of `[low,
    high]` with a `q` step of discretization. The `suggest_int` method works similarly
    to the `random.randint()` method. Finally, the `suggest_float` method. This method
    works for a floating type of hyperparameters and is actually a wrapper of two
    other sampling distribution methods, namely the `suggest_uniform` and `suggest_loguniform`.
    To utilize `suggest_loguniform`, simply set the `log` parameter in `suggest_float`
    as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have a better understanding of how we can define the hyperparameter space
    within the `objective` function, the following code shows an example of how to
    define an `objective` function using `objective` function, to ensure readability
    and to enable us to write the code in a modular fashion. However, you can also
    put all of the code within one single `objective` function directly. The data
    and preprocessing steps used in this example are the same as in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit*. However, in this example, we are using a **neural
    network** model instead of a random forest as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function to define the model architecture. Here, we create a binary
    classifier model where the number of hidden layers, number of units, dropout rate,
    and the `activation` function for each layer are part of the hyperparameter space,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to define the model’s optimizer. Notice that we define conditional
    hyperparameters in this function where we have a different set of hyperparameters
    for a different chosen optimizer as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `train` and `validation` functions. Note that the preprocessing
    code is not shown here, but you can see it in the GitHub repo mentioned in the
    *Technical requirements* section for the full code. As the case with the examples
    in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062), we are also using F1-score
    as the evaluation metric of the model as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create the `objective` function. Here, we split the original training data into
    training data for hyperparameter tuning, `df_train_hp`, and validation data, `df_val`.
    We won’t follow the k-fold cross-validation evaluation method since it will take
    too much time for the neural network model to go through several folds of evaluation
    within each tuning trial (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,Evaluating
    Machine Learning Models*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To perform hyperparameter tuning in `Optuna`, we need to initiate a `study`
    object via the `create_study()` function. The `study` object provides interfaces
    to run a new `Trial` object and access the trials’ history. The `Trial` object
    is simply an object that involves the process of evaluating an `objective` function.
    This object will be passed to the `objective` function and it is responsible for
    managing the trial’s state, providing interfaces upon receiving the parameter
    suggestion just as we saw earlier in the `objective` function. The following code
    shows how to utilize the `create_study()` function to initiate a `study` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: There are several important parameters in the `create_study()` function. The
    `direction` parameter allows us to tell `Optuna` what kind of optimization problem
    we are working on. There are two valid values for this parameter, namely *‘maximize’*
    and *‘minimize’*. By setting the `direction` parameter equal to *‘maximize’*,
    it means that we tell `Optuna` that we are currently working on a maximization
    problem. `Optuna` sets this parameter to *‘minimize’* by default. The `sampler`
    parameter refers to the hyperparameter tuning algorithm that we want to use. By
    default, `Optuna` will use TPE as the sampler. The `pruner` parameter refers to
    the pruning algorithm that we want to use, where `MedianPruner()` is used by default.
  prefs: []
  type: TYPE_NORMAL
- en: Pruning in Optuna
  prefs: []
  type: TYPE_NORMAL
- en: 'Although `MedianPruner()` is chosen by default, the pruning process will not
    be performed unless we explicitly tell `Optuna` to do so within the `objective`
    function. This example shows how to perform a simple pruning procedure with the
    default pruner in `Optuna` at the following link: [https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py](https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Besides the three preceding parameters, there are also other parameters in the
    `create_study()` function, namely `storage`, `study_name`, and `load_if_exists`.
    The `storage` parameter expects a database URL input, which will be handled with
    `Optuna`. If we do not pass a database URL, `Optuna` will use the in-memory storage
    instead. The `study_name` parameter is simply the name that we want to give to
    the current `study` object. If we do not pass a name, `Optuna` will automatically
    generate a random name for us. Last but not least, the `load_if_exists` parameter
    is a Boolean parameter that handles cases when there might be conflicting study
    names. If the study name is already generated in the storage, and we set `load_if_exists=False`,
    then `Optuna` will raise an error. On the other hand, if the study name is already
    generated in the storage, but we set `load_if_exists=True`, `Optuna` will just
    load the existing `study` object instead of creating a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `study` object is initiated along with the appropriate parameters,
    we can start performing the hyperparameter tuning by calling the `optimize()`
    method. The following code shows you how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are several important parameters in the `optimize()` method. The first
    and most important method is the `func` parameter. This parameter expects a callable
    that implements the `objective` function. Here, we don’t directly pass the `objective`
    function to the `func` parameter since our `objective` function expects two inputs,
    while by default, `Optuna` can only handle an `objective` function with one input,
    which is the `Trial` object itself. That’s why we need the help of Python’s built-in
    `lambda` function to pass the second input to our `objective` function. You can
    also utilize the same `lambda` function if your `objective` function has more
    than two inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The second most important parameter is `n_trials`, which refers to the number
    of trials or iterations for the hyperparameter tuning process. Another implemented
    parameter that can be used as the stopping criteria is the `timeout` parameter.
    This parameter expects the stopping criteria in the unit of seconds. By default,
    `Optuna` sets the `n_trials` and `timeout` parameters to `None`. If we leave it
    be, then `Optuna` will run the hyperparameter tuning process until it receives
    a termination signal, such as `Ctrl+C` or `SIGTERM`.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, `Optuna` also allows us to utilize the parallel resources
    through a parameter called `n_jobs`. By default, Optuna will set `n_jobs=1`, meaning
    that it will only utilize one job. Here, we set `n_jobs=-1`, meaning that we will
    use all of the CPU counts in our computer to perform parallel computation.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter’s Importance in Optuna
  prefs: []
  type: TYPE_NORMAL
- en: '`Optuna` provides a very nice module to measure the importance of each hyperparameter
    in the search space. As per version 2.10.0, there are two methods implemented,
    namely the **fANOVA** and **Mean Decrease Impurity** methods. Please see the official
    documentation on how to utilize this module and the theory behind the implemented
    methods, available at the following link: [https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html](https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned what `Optuna` is in general, the available features
    that we can utilize, and the general steps as to how to perform hyperparameter
    tuning with this package. `Optuna` also has various visualization modules that
    can help us track our hyperparameter tuning experiments, which will be discussed
    in [*Chapter 13*](B18753_13_ePub.xhtml#_idTextAnchor125), *Tracking Hyperparameter
    Tuning Experiments*. In the upcoming sections, we will learn how to perform various
    hyperparameter tuning methods with `Optuna` through examples.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing TPE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TPE is one of the variants of the Bayesian optimization hyperparameter tuning
    group (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)), which is the
    default sampler in `Optuna`. To perform hyperparameter tuning with TPE in `Optuna`,
    we can just simply pass the `optuna.samplers.TPESampler()` class to the sampler
    parameter of the `create_study()` function. The following example shows how to
    implement TPE in `Optuna`. We’ll use the same data as in the examples in [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062) and follow the steps introduced in
    the preceding section as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the `objective` function along with the hyperparameter space. Here, we’ll
    use the same function that we defined in the *Introducing Optuna* section. Remember
    that we use the train-validation split instead of the k-fold cross-validation
    method within the `objective` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initiate a `study` object via the `create_study()` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform hyperparameter tuning by calling the `optimize()` method on the `study`
    object as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.563` of F1-score evaluated in
    the validation data. We also get a dictionary consisting of the best set of hyperparameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model on full training data using the best set of hyperparameters
    found. Here, we define another function called `train_and_evaluate_final()` that
    has the purpose of training the model in the full training data based on the best
    set of hyperparameters found in the preceding step, as well as evaluating it on
    the test data. You can see the implemented function in the GitHub repo mentioned
    in the *Technical requirements* section. Define the function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Test the final trained model on the test data. Based on the results from the
    preceding step, we get around `0.604` in F1-score when testing our final trained
    neural network model with the best set of hyperparameters on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are several important parameters for the `TPESampler` class. First, there
    is the `gamma` parameter, which refers to the threshold used in TPE to divide
    good and bad samples (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)).The
    `n_startup_trials` parameter is responsible for controlling how many trials will
    utilize Random Search before starting to perform the TPE algorithm. The `n_ei_candidates`
    parameter is responsible for controlling how many candidate samples are used to
    calculate the `expected improvement acquisition` function. Last but not least,
    the `seed` parameter, which controls the random seed of the experiment. There
    are many other parameters available for the `TPESampler` class, so please see
    the original documentation for more information, available at the following link:
    [https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html](https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to perform hyperparameter tuning with TPE
    in `Optuna` using the same data as in the example in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062).
    As mentioned in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring
    Bayesian Optimization* `Optuna` also implements the multivariate TPE, which is
    able to capture the interdependencies among hyperparameters. To enable the multivariate
    TPE, we can just simply set the `multivariate` parameter in `optuna.samplers.TPESampler()`
    as `True`. In the next section, we will learn how to perform Random Search with
    `Optuna`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing Random Search in `Optuna` is very similar to implementing TPE
    in `Optuna`. We can just follow a similar procedure to the preceding section and
    change the `sampler` parameter in the `optimize()` method in *step 2*. The following
    code shows you how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the exact same data, preprocessing steps, hyperparameter space, and `objective`
    function, we get around `0.548` in the F1-score evaluated in the validation data.
    We also get a dictionary consisting of the best set of hyperparameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After the model is trained with full data using the best set of hyperparameters,
    we get around `0.596` in F1-score when we test the final neural network model
    trained on the test data. Notice that although we have defined many hyperparameters
    earlier, (see the `objective` function in the preceding section), here, we do
    not get all of them in the results. This is because most of the hyperparameters
    are conditional hyperparameters. For example, since the chosen value for the *’num_layers’*
    hyperparameter is zero, there will be no *’n_units_layer_{layer_i}’*, *’dropout_rate_layer_{layer_i}’*,
    or *‘actv_func _layer_{layer_i}’* since those hyperparameters will only exist
    when the *’num_layers’* hyperparameter is greater than zero.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have seen how to perform hyperparameter tuning using the
    Random Search method with `Optuna`. In the next section, we will learn how to
    implement Grid Search with the `Optuna` package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Grid Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing Grid Search in `Optuna` is a bit different from implementing TPE
    and Random Search. Here, we need to also define the search space object and pass
    it to `optuna.samplers.GridSampler()`. The search space object is just a Python
    dictionary data structure consisting of hyperparameters’ names as the keys and
    the possible values of the corresponding hyperparameter as the dictionary’s values.
    `GridSampler` will stop the hyperparameter tuning process if all of the combinations
    in the search space have already been evaluated, even though the number of trials,
    `n_trials`, passed to the `optimize()` method has not been reached yet. Furthermore,
    `GridSampler` will only get the value stated in the search space no matter the
    range we pass to the sampling distribution methods, such as `suggest_categorical`,
    `suggest_discrete_uniform`, `suggest_int`, and `suggest_float`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to perform Grid Search in `Optuna`. The overall
    procedure to implement Grid Search in `Optuna` is similar to the procedure stated
    in the *Implementing Tree-structured Parzen Estimators* section. The only difference
    is that we have to define the search space and change the `sampler` parameter
    to `optuna.samplers.GridSampler()` in the `optimize()` method in *step 2* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `0.574` of the F1-score evaluated
    in the validation data. We also get a dictionary consisting of the best set of
    hyperparameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.610` in F1-score when we test the final neural network model
    trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that `GridSampler` will rely on the search space to perform
    the hyperparameter sampling. For example, in the search space, we only define
    the valid values for `num_layers` as `[0,1]`. So, although within the `objective`
    function we set `trial.suggest_int(‘num_layers’,low=0,high=3)` (see the *Introducing
    Optuna* section), only `0` and `1` will be tested during the tuning process. Remember
    that, in `Optuna`, we can specify the stopping criterion through the `n_trials`
    or `timeout` parameters. If we specify either one of those criteria, `GridSampler`
    will not test all of the possible combinations in the search space; the tuning
    process will stop once the stopping criterion is met. In this example, we set
    `n_trials=50`, just like the example in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to perform hyperparameter tuning using
    the Grid Search method with `Optuna`. In the next section, we will learn how to
    implement SA with the `Optuna` package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Simulated Annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SA is not part of the built-in implementation of the hyperparameter tuning method
    in `Optuna`. However, as mentioned in the first section of this chapter, we can
    define our own custom sampler in `Optuna`. When creating a custom sampler, we
    need to create a class that inherits from the `BaseSampler` class. The most important
    method that we need to define within our custom class is the `sample_relative()`
    method. This method is responsible for sampling the corresponding hyperparameters
    from the search space based on the hyperparameter tuning algorithm we chose.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete custom `SimulatedAnnealingSampler()` class with geometric cooling
    annealing schedule (see [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047))
    has been defined and can be seen in the GitHub repo mentioned in the *Technical
    requirements* section. The following code shows only the implementation of the
    `sample_relative()` method within the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows how to perform hyperparameter tuning with SA in `Optuna`.
    The overall procedure to implement SA in `Optuna` is similar to the procedure
    stated in the *Implementing Tree-structured Parzen Estimators* section. The only
    difference is that we have to change the `sampler` parameter to `SimulatedAnnealingSampler()`
    in the `optimize()` method in *step 2* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the exact same data, preprocessing steps, hyperparameter space, and `objective`
    function, we get around `0.556` of the F1-score evaluated in the validation data.
    We also get a dictionary consisting of the best set of hyperparameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.559` in F1-score when we test the final neural network model
    trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to perform hyperparameter tuning using
    the SA algorithm with `Optuna`. In the next section, we will learn how to utilize
    Successive Halving as a pruning method in `Optuna`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Successive Halving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Optuna`, meaning that it is responsible for stopping hyperparameter tuning
    iterations whenever it seems that there’s no additional benefit to continuing
    the process. Since it is implemented as a pruner, the resource definition of SH
    (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)) in `Optuna` refers
    to the number of training steps or epochs of the model, instead of the number
    of samples, as it does in `scikit-learn`’s implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can utilize SH as a pruner along with any sampler that we use. This example
    shows you how to perform hyperparameter tuning with the Random Search algorithm
    as the sampler and SH as the pruner. The overall procedure is similar to the procedure
    stated in the *Implementing TPE* section. Since we are utilizing SH as a pruner,
    we have to edit our `objective` function so that it will utilize the pruner during
    the optimization process. In this example, we can use the callback integration
    with TFKeras provided by `Optuna` via `optuna.integration.TFKerasPruningCallback`.
    We simply need to pass this class to the `callbacks` parameter when fitting the
    model within the `train` function as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have told `Optuna` to utilize the pruner, we also need to set the `pruner`
    parameter in the `optimize()` method to `optuna.pruners.SuccessiveHalvingPruner()`
    in *step 2* of the *Implementing Tree-structured Parzen Estimators* sectionas
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we also increased the number of trials from `50` to `100`
    since most of the trials will be pruned anyway as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the exact same data, preprocessing steps, and hyperparameter space, we
    get around `0.582` of the F1-score evaluated in the validation data. Out of `100`
    trials performed, there are `87` trials pruned by SH, which implies only `13`
    completed trials. We also get a dictionary consisting of the best set of hyperparameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.597` in F1-score when we test the final neural network model
    trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that there are several parameters for `SuccessiveHalvingPruner`
    that we can customize based on our needs. The `reduction_factor` parameter refers
    to the multiplier factor of SH (see [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054)).
    The `min_resource` parameter refers to the minimum number of resources to be used
    at the first trial. This parameter is set to *‘auto’*, by default, where a heuristic
    is utilized to calculate the most appropriate value based on the number of required
    steps for the first trial to be completed. In other words, `Optuna` will only
    be able to start the tuning process after the `min_resource` training steps or
    epochs have been performed.
  prefs: []
  type: TYPE_NORMAL
- en: '`Optuna` also provides the `min_early_stopping_rate` parameter, which has the
    exact same meaning as we defined in [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054).
    Last but not least, the `bootstrap_count` parameter. This parameter is not part
    of the original SH algorithm. The purpose of this parameter is to control the
    minimum number of trials that need to be completed before the actual SH iterations
    start.'
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder, what about the parameter that controls the value of maximum
    resources and the number of candidates in SH? Here, in `Optuna`, the maximum resources
    definition will be automatically derived based on the total number of training
    steps or epochs within the defined `objective` function. As for the parameter
    that controls the number of candidates, `Optuna` delegates this responsibility
    to the `n_trials` parameter in the `study.optimize()` method.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to utilize SH as a pruner during the hyperparameter
    tuning process. In the next section, we will learn how to utilize Hyperband, the
    extended algorithm of SH, as a pruning method in `Optuna`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Hyperband
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing `Optuna` is very similar to implementing Successive Halving as
    a pruner. The only difference is that we have to set the `pruner` parameter in
    the `optimize()` method to `optuna.pruners.HyperbandPruner()` in *step 2* in the
    preceding section. The following code shows you how to perform hyperparameter
    tuning with the Random Search algorithm as the sampler and HB as the pruner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: All of the parameters of `HyperbandPruner` are the same as `SuccessiveHalvingPruner`’s,
    except that, here, there is no `min_early_stopping_rate` parameter and there is
    a `max_resource` parameter. The `min_early_stopping_rate` parameter is removed
    since it is set automatically based on the ID of each bracket. The `max_resource`
    parameter is responsible for setting the maximum resource allocated to a trial.
    By default, this parameter is set to *‘auto’*, which means that the value will
    be set as the largest step in the first completed trial.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the exact same data, preprocessing steps, and hyperparameter space, we
    get around `0.580` of the F1-score evaluated in the validation data. Out of `100`
    trials performed, there are `79` trials pruned by SH, which implies only `21`
    completed trials. We also get a dictionary consisting of the best set of hyperparameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: After the model is trained on full data using the best set of hyperparameters,
    we get around `0.609` in F1-score when we test the final neural network model
    trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned how to utilize HB as a pruner during the hyperparameter
    tuning process with `Optuna`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned all of the important aspects of the `Optuna`
    package. We have also learned how to implement various hyperparameter tuning methods
    using the help of this package, in addition to understanding each of the important
    parameters of the classes and how are they related to the theory that we have
    learned in previous chapters. From now on, you should be able to utilize the packages
    we have discussed in the last few chapters to implement your chosen hyperparameter
    tuning method, and ultimately, boost the performance of your ML model. Equipped
    with the knowledge from *Chapters 3 - 6*, you will also be able to debug your
    code if there are errors or unexpected results, and you will be able to craft
    your own experiment configuration to match your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the DEAP and Microsoft NNI packages
    and how to utilize them to perform various hyperparameter tuning methods. The
    goal of the next chapter is similar to this chapter, which is to be able to utilize
    the package for hyperparameter tuning purposes and understand each of the parameters
    of the implemented classes.
  prefs: []
  type: TYPE_NORMAL
