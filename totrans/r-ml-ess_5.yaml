- en: Chapter 5. Step 2 – Applying Machine Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focuses on applying the machine learning algorithm, and it is the
    core of developing the solution. There are different types of techniques that
    learn from the data. Depending on our target, we can use the data to identify
    similarities between objects or to estimate an attribute on new objects.
  prefs: []
  type: TYPE_NORMAL
- en: In order to show the machine learning techniques, we start from the flag data
    that we processed in the previous chapter. However, reading this chapter doesn't
    require you to know about the previous, although it is recommended to understand
    where the data came from.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter you will learn to:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify homogeneous groups of items
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore and visualize the item groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimate a new country language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the configuration of a machine learning technique
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying a homogeneous group of items
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our data describes each country flag. Is there any way to identify groups of
    countries with similar flag attributes? We can use some clustering techniques
    that are machine learning algorithms that define homogeneous clusters using the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from the flag attributes, in the previous chapter, we built a feature
    table and we stored it into the `dtFeatures.txt` file. In order to load the file
    into R, the first step is to define the directory containing the file using `setwd`.
    Then, we can load the file into the `dfFeatures` data frame using `read.table`,
    and we can convert it into the `dtFeatures` data table, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the data using `str`, similar to the previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The language column is a factor and there are 10 languages, called `levels`
    of the factor. All the other columns contain features describing the flags and
    they are factors with two levels: `yes` and `no`. The features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `colors` feature (for example, `red`) has a `yes` level if the flag contains
    the color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `patterns` feature (for example, `circle`) has a `yes` level if the flag
    contains the pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `nBars`/`nStrp`/`nCol` features followed by a number (for example, `nBars3`)
    have a `yes` level if the flag has 3 bars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `topleft`/`botright`/`mainhue` features followed by a color (for example,
    `topleftblue`) have a `yes` level if the top-left part is blue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the groups using k-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our target is to identify groups of similar flags. For this purpose, we can
    start using a basic clustering algorithm, that is, **k-means**.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means target is to identify *k* (for example, eight) homogeneous clusters
    of flags. Imagine dividing all the flags in eight clusters. One of them includes
    10 flags out of which seven contain the color red. Let's suppose that we have
    a `red` attribute that is `1` if the flag contains red and `0` otherwise. We can
    say that the `average flag` of this cluster contains `red` with a probability
    of 70 percent, so its `red` attribute is 0.7\. Doing the same with every other
    attribute, we can define `average flag`, whose attributes are the average within
    the group. Each cluster has an average flag that we can determine using the same
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm is based on an average object that is called the cluster
    center. At the beginning, the algorithm divides the flags into 8 random groups
    and determines their 8 centers. Then, k-means reassigns each flag to the group
    whose center is the most similar. In this way, the clusters are more homogeneous
    and the algorithm can recompute their centers. After a few iterations, we have
    8 groups containing homogeneous flags.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm is a very popular technique and R provides us with the
    `kmeans` function. In order to use it, we can take a look at its help:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We need two inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: A numeric data matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`centers`: The number of clusters (or the cluster centers to start with)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting from `dtFeatures`, we need to build a numeric feature matrix `dtFeaturesKm`.
    First, we can put the feature names into `arrayFeatures` and generate the `dtFeaturesKm`
    data table containing all the features. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define `arrayFeatures` that is a vector containing the feature name. The `dtFeatures`
    method contains the attribute in the first column and the features in the others,
    so we extract all the column names apart from the first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `dtFeaturesKm` containing the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert a generic column (for example, `red`) into the numeric format. We can
    use `as.numeric` to convert the column format from factor into numeric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The new vector contains `1` if the value is `no` and `2` if the value is `yes`.
    In order to use the same standards as our k-means descriptions, we prefer to have
    `0` if the attribute is `no` and `1` if the attribute is `yes`. In this way, when
    we are computing the average attribute within a group, it will be a number between
    0 and 1 that can be seen as a portion of flags whose attribute is `yes`. Then,
    in order to have 0 and 1, we can use `as.numeric(red) – 1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Alternatively, we could have done the same using the ifelse function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We need to convert each column format into 0-1\. The `arrayFeatures` data table
    contains names of all the features and we can process each of them using a `for`
    loop. If we want to transform a column whose name is contained in `nameCol`, we
    need to use the `eval`-`get` notation. With `eval(nameCol) :=` we redefine the
    column, and with `get(nameCol)` we use the current value of the column, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now convert all the features in the 0-1 format. Let''s visualize it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `kmeans` function requires the data to be in the matrix form. In order
    to convert `dtFeaturesKm` into a matrix, we can use `as.matrix`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `matrixFeatures` data table contains data to build the k-means algorithm
    and the other `kmeans` inputs are the parameters. The k-means algorithm doesn''t
    automatically detect the number of clusters, so we need to specify it through
    the `centers` input. Given the set of objects, we can identify any number of clusters
    out of them. Which is the number that reflects the data most? There are some techniques
    that allow us to define it, but they''re out of the scope of this chapter. We
    can just define a reasonable number of centers, for example, 8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `modelKm` function is a list containing different model components. The
    help of `kmeans` provides us with a detailed description of the output and we
    can use `names` to get the element names. Let''s see the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the cluster centers that are contained in `centers`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Each row defines a center and each column shows an attribute. All the attributes
    are between 0 and 1, and they represent the percentage of flags in the cluster
    with an attribute equal to `1`. For instance, if `red` is `0.5`, it means that
    half of the flags contain the color red.
  prefs: []
  type: TYPE_NORMAL
- en: The element that we will use is `cluster` and it contains a label specifying
    the cluster of each flag. For instance, if the first element of a cluster is `3`,
    this means that the first flag in `matrixFeatures` (and also in `dtFeatures`)
    belongs to the third cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the clusters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can take a look at each cluster in order to explore its flags. In order
    to do that, we can add the cluster to the initial table by defining the `clusterKm`
    column, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to explore a cluster, we can determine how many of its countries speak
    each language. Starting from `dtFeatures`, we can summarize the data about each
    cluster using data table aggregation. First, let''s define the column that contains
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to determine how many rows we have in each cluster. The data table
    command that allows us to determine the number of rows is `.N`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to have a different column name for the cluster size, we can specify
    it within the list, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to determine how many countries we have for each language, we can
    use `table`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to use `table` within an aggregation, the output should be a list.
    For this purpose, we can convert the table using `as.list`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can apply this operation to each group using `by`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we want to visualize the percentage of countries speaking each language?
    We can divide each value of the table by the number of countries in the cluster,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to generate `dtClusters` containing the number of countries in each
    group and the percentage of each language. In order to do this, we can generate
    two lists using the commands that we''ve just seen. In order to combine the two
    lists, we can just use `c(list1, list2)`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Each row of `dtClusters` represents a cluster. The `nCountries` column displays
    the number of countries in the cluster and all the other columns show the percentage
    of each language. In order to visualize this data, we can build a histogram with
    a bar for each cluster. Each bar is divided into segments representing the number
    of countries speaking each language. The `barplot` function allows us to build
    the desired chart, if we give a matrix as the input. Each matrix column corresponds
    to a bar and each row defines the chunks in which the bar is divided.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to define a matrix containing the language percentages. This can be
    done by carrying out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define `arrayLanguages` containing the `dtClusters` language column names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build `dtBarplot` containing the language columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `dtBarplot` into a matrix using `as.matrix`. In order to build the
    chart, we need to transpose the matrix (invert the rows and columns) using the
    R function `t`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a vector with the cluster sizes, that is, the number of countries. We
    will display the numbers under the columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the legend names as the country names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reduce the legend names'' length in order to avoid having a legend overlapping
    the chart. Using `substring`, we limit the names to 12 characters, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the colors using `rainbow`. We need to define a color for each element
    of `namesLegend`, so the number of colors is `length(namesLegend)`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the chart title using `paste`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we have all the `barplot` inputs, so we can build the chart. In order to
    be sure that the legend dosen''t overlap the bars, we include the `xlim` argument
    that specifies the plot boundaries, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring the clusters](img/7740OS_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The k-means algorithm performs a series of steps starting from the initial clusters
    that are defined by splitting the data randomly. The final output depends on the
    initial random split that is different every time we run the algorithm. So, if
    we run k-means more than once, we might obtain different results. However, this
    chart helps us identify some patterns within the language group. For instance,
    in the eighth cluster, almost all the countries speak English, so we can deduce
    that there are some English-speaking countries with a similar flag. In the fifth
    cluster, more than half of the countries speak French, so we can deduce the same.
    Some less relevant results are that Arabic has a high share in the first cluster
    and Spanish is quite relevant in the seventh cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using other clustering algorithms and we will visualize the results
    in a similar way. In order to have clean and compact code, we can define the `plotCluster`
    function. The inputs are the `dtFeatures` feature data table and the `nameCluster`
    cluster column name. The code is almost the same as the preceding one, shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This function should build the same histogram as the previous one. Let''s check
    it using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Another way to visualize the clusters is to build a world map using a different
    color for each cluster. In addition, we can visualize a world map for the languages.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to build the map, we need to install and load the `rworldmap` package,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This package builds a world map starting from the country names, that is, in
    our case the `dfFeatures` row names. We can add the `country` column to `dtFeatures`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Our data is quite old so Germany is still divided in two parts. In order to
    visualize it on the map, we can convert `Germany-FRG` into `Germany`. Similarly,
    we can convert `USSR` into `Russia`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can define a function to build a world map showing the clusters. The
    inputs are the `dtFeatures` data table and the `colPlot` column name of the feature
    to visualize (for example, `clusterKm`). The other argument is `colourPalette`
    and it determines the color to be used in the map. See `help(mapCountryData)`
    for more information, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `colPlot` column containing the cluster to visualize. In the
    case of a string, we use just the first 12 characters, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We build `mapFeatures` containing the data that we need to build the chart.
    See `help(joinCountryData2Map)` for more information. The `joinCode = ''NAME''`
    input specifies that the countries are defined by their names and not by an abbreviation.
    The `nameJoinColumn` specifies which column we have the country names in, shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can build the chart using `mapCountryData`. We specify that we are using
    the colors of the rainbow and that the country with the missing data will be gray,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use `plotMap` to visualize the k-means clusters on the world map,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Exploring the clusters](img/7740OS_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that many Asian countries belong to the fifth cluster. In addition,
    we can observe that Italy, France, and Ireland belong to the same cluster, since
    their flag is similar. Apart from that, it's hard to identify any other pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying a cluster's hierarchy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Other techniques to identify homogeneous groups are the hierarchic clustering
    algorithms. These techniques build the clusters, merging the objects iteratively.
    At the beginning, we have a cluster for each country. We define a measure of how
    *similar* two clusters are and, at each step, we identify the two clusters whose
    flag is the most *similar* and merge them into a unique cluster. In the end, we
    have a cluster including all the countries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R function that performs hierarchic clustering is `hclust`. Let''s take
    a look at its `help` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The first input is `d` and the documentation explains that it''s a dissimilarity
    structure, that is, a matrix containing all the distances between the objects.
    As suggested by the documentation, we can use the `dist` function to build the
    input, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The input of `dist` is a numeric matrix describing the flags. We already built
    `matrixDistances` for the k-means algorithm, so we can reuse it. The other relevant
    input is `method` and it specifies how `dist` measures the distance between two
    flags. Which method should we use? All the features are binary as they have two
    possible outcomes, that is, `0` and `1`. Then, the distance can be the number
    of attributes with a different value. The `method` object that determines the
    distance in this way is `manhattan`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `matrixDistances` function contains the dissimilarity between any two flags.
    The other input is `method` and it specifies the agglomeration method. In our
    case, we set the method as `complete`. There are other options for `method` and
    they define the linkage, that is, the way of computing the distance between clusters,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `modelHc` method contains the clustering model and we can visualize the
    cluster using `plot`. You can consult the help of `hclust` to understand the `plot`
    parameters, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![Identifying a cluster''s hierarchy](img/7740OS_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This chart shows the algorithm procedure. At the bottom, we have all the countries,
    and each flag belongs to a different cluster. Each line represents a cluster and
    the lines converge when the algorithm merges the clusters. On the left-hand side
    of the chart, you can see a scale representing the distance between the flags,
    and at each level the algorithm merges the clusters that are at a specific distance
    from each other. At the top, all the flags belong to the same cluster. This chart
    is called **dendrogram**. Consider the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The clusters that we want to identify are the ones above the red line. The
    function that identifies the cluster starting from `modelHc` is `cutree`, and
    we can specify the horizontal line height in the `h` argument, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can add the cluster to `dtFeatures`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we can see which languages are spoken in each cluster.
    We can reuse `plotCluster` and `plotMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![Identifying a cluster''s hierarchy](img/7740OS_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the eighth cluster, English is the predominant language. Apart from that,
    Arabic is relevant in the first cluster only, French and German are relevant in
    the second and third if taken together, and Spanish is relevant in the third.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also visualize the world map with the clusters, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Identifying a cluster''s hierarchy](img/7740OS_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Similar to k-means, the only continent with a predominant cluster is Asia.
  prefs: []
  type: TYPE_NORMAL
- en: This section described two popular clustering techniques that identify homogeneous
    flag clusters. They both allow us to understand the similarities between different
    flags, and we can use this information as support to solve some problems.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the k-nearest neighbor algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section shows you how to estimate a new country language starting from
    its flag, using a simple supervised learning technique that is the **k-nearest
    neighbor** (**KNN**). In this case, we estimate the language, which is a `categoric`
    attribute so we use a classification technique. If the attribute was numeric,
    we would have used a regression technique. The reason I chose KNN is that it's
    simple to explain, and there are some options to modify its parameters in order
    to improve the result's accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how the KNN works. We know the flag and the language of 150 countries
    and we want to determine the language of a new country starting from its flag.
    First, we identify the 10 countries whose flag is the most similar to the new
    one. Out of them, we have six Spanish-speaking countries, two English-speaking
    countries, one French-speaking country, and one Arabic-speaking country.
  prefs: []
  type: TYPE_NORMAL
- en: Out of these 10 countries, the most common language is Spanish, so we can expect
    that the new flag belongs to a Spanish-speaking country.
  prefs: []
  type: TYPE_NORMAL
- en: The KNN is based upon this approach. In order to estimate a new country language,
    we identify the *K* countries whose flag is the most similar. Then, we estimate
    that the new country speaks the most common language among them.
  prefs: []
  type: TYPE_NORMAL
- en: We have a table describing 194 flags through 37 binary attributes whose value
    can be `Yes` or `No`. For instance, the `mainhuegreen` attribute is `yes`, if
    the predominant flag color is green and `no` otherwise. All the attributes describe
    the flag's colors and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the previous section, before modifying `dtFeatures`, we define `arrayFeatures`
    containing the feature names. As we added some columns to `dtFeatures`, we extract
    the feature names from `dfFeatures`. Then, we add the `country` column with the
    country names coming from `dfFeatures`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Starting from `dtFeatures`, we can apply KNN. Given a new flag, how do we determine
    which are the 10 most similar flags? Given any two flags, we can measure how *similar*
    they are. The easiest way is to count how many features have the same value across
    the two flags. The more attributes they have in common, the more similar they
    are.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we already explored and transformed the features,
    so we don''t need to process them. However, we haven''t explored the language
    column yet. For each language, we can determine how many countries speak the language
    using `table`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of countries varies a lot from one language to another. The most
    popular language is `English`, with 43 countries, and there are some languages
    with just four countries. In order to have an overview of all the languages, we
    can visualize the table by building a chart. In the previous section, we defined
    `plotMap`, which shows the groups on the world map. We can use it to show the
    countries speaking each language, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the k-nearest neighbor algorithm](img/7740OS_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s nice to see a map showing countries that speak each language, but it''s
    still a bit hard to understand how big the groups are. A better option is to generate
    a pie chart whose slices are proportional to the number of countries in each group.
    The R function is `pie`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pie` function requires an input, that is, a vector containing the number
    of countries speaking each language. If the input vector fields have a name, it''ll
    be displayed in the chart. We can build the required vector using `table`, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Fortunately, `pie` doesn''t require any other argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the k-nearest neighbor algorithm](img/7740OS_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are some languages that are spoken in just a few countries. For instance,
    there are just 4 Slavic countries. Given a new country, we want to determine its
    language starting from its flag. Let's pretend that we don't know which language
    is spoken in one of the 4 Slavic countries. If we take into account its 10 nearest
    neighbors, there cannot be more than 3 other Slavic countries. What if there are
    4 English-speaking countries out of its 10 neighbors? Despite all the remaining
    Slavic countries that are in its neighborhood, there are more English countries
    just because the English group is bigger. Therefore, the algorithm will estimate
    that the country is English. Similarly, we have the same issue with any other
    small group. Like almost all the machine learning algorithm, the KNN won't be
    able to classify the countries that belong to any other smaller group.
  prefs: []
  type: TYPE_NORMAL
- en: While dealing with any classification problem, if some groups are small, we
    don't have enough related information. In this context, even a good technique
    won't be able to classify the new objects that belong to a small group. In addition,
    given a new country that belongs to a medium-sized group, it likely has a lot
    of neighbors that belong to the big groups. Therefore, a new country speaking
    one of these languages might be assigned to the big groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'By knowing the model limitations, we can define a feasible machine learning
    problem. In order to avoid having small groups, we can merge some groups. The
    clustering techniques allowed us to identify which language groups are more well-defined,
    and accordingly, we can split the languages in these groups: `English`, `Spanish`,
    `French and German`, `Slavic and other Indo-European`, `Arabic`, and `Other`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the language groups to build `listGroups` whose elements contain
    the language spoken by the groups. For instance, we can define the `indoEu` group
    containing `Slavic` and `Other Indo-European` language, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can redefine the `language` column containing the language groups. For
    each element of `listGroups`, we convert all the languages into the element name.
    For instance, we convert `Slavic` and `Other Indo-European` into `indoEu`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform this operation within a `for` loop. All the group names are
    contained in the list names, so we can iterate over the elements of `names(listGroups)`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `nameGroup` defines a group name and `listGroups[[nameGroup]]` contains
    its languages. We can extract the rows of `dtFeatures` speaking any of the group
    languages, using `language %in% listGroups[[nameGroup]]`. Then, we can reassign
    the language column to the `nameGroup` group name using the `:=` data table notation,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We redefined the `language` column grouping the languages. Let''s take a look
    at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `language` is a factor and there are just six possible levels that are
    our language groups. However, you can see that R has printed `16 Levels: Arabic
    Chinese English French ... Other` in the console. The reason is that the `language`
    column format is `factor` and it keeps track of the 10 initial values. In order
    to display just the six language groups, we can redefine the `language` column
    using `factor`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have just six levels. Just like we did earlier, we can visualize the
    group sizes data using `plotMap`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The map obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the k-nearest neighbor algorithm](img/7740OS_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the countries of each category are geographically close to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to visualize the new group sizes, we can use `pie`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Applying the k-nearest neighbor algorithm](img/7740OS_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All the six groups contain enough countries. The **english** and **other** groups
    are a bit bigger than the others, but the sizes are comparable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can build the KNN model. R provides us with the `kknn` package containing
    the KNN algorithm. Let''s install and load the package, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The function that builds the KNN is called `kknn`, such as the package. Let''s
    see its help function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The first input is formula and it defines the features and the output. Then,
    we have to define a training set, containing the data to be used to build the
    model, and a test set, containing the data upon which we are applying the model.
    We use all the information about the training set and pretend not to know the
    language of the test set countries. There are other optional inputs defining some
    model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the feature names are contained in `arrayFeatures`. In order to define
    how the output depends on the features, we need to build a string in the `output
    ~ feature1 + feature2 + …`. format. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the first part of the string: `output ~` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each feature, add `+ feature` using `paste`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the string into the `formula` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We built `formulaKnn` containing the relationship to put into `kknn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to define the training set and the test set starting from `dtFeatures`.
    A fair split is putting 80 percent of the data in the training set, and for this
    purpose we can add each country to the training set with a probability of 80 percent
    and to the test set otherwise. We can define the `indexTrain` vector whose length
    is equal to the number of lines in `dtFeatures`. The R function is `sample`, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The arguments are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: The values to be put into the vector that are `TRUE` and `FALSE` in this
    case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size`: The length of the vector that is the number of rows in `dtFeatures`
    in our case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`replace`: In order to sample the values more than once, it''s `TRUE`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prob`: The probability of choosing the elements of `x`. In our case, we pick
    `TRUE` with a probability of 80 percent and `FALSE` with a probability of 20 percent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using our arguments, we can build `indexTrain`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to add the rows, for which `indexTrain` is `TRUE`, to the training
    set and the remaining rows to the testing set. We extract all the rows, for which
    `indexTrain` is `TRUE`, using a simple data table operation, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to extract the test rows, we have to switch `TRUE` and `FALSE` using
    the `NOT` operator that in R is `!`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have all the basic arguments for using `kknn`. The other parameters
    that we set are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`k`: The number of neighbors is `10`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: KNN has the option of assigning a different relevance to the features,
    but we''re not using this feature at the moment. Setting the `kernel` parameter
    as `rectangular`, we use the basic KNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distance`: We want to compute the distance between two flags as the number
    of attributes that they don''t have in common (similar to the previous chapter).
    In order to do this, we set the distance parameter equal to `1`. For more information,
    you can learn about **Minkowski distance**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s build the KNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The model has learned from `dtTrain` and estimated the language of the countries
    in `dtTest`. As we can see in the `kknn` help, `modelKnn` is a list containing
    a description of the model. The component showing the predicted language is `fitted.valued`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We can add the predicted language to `dtTest` in order to compare it with the
    real language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'For the countries in `dtTest`, we know the real and the predicted languages.
    We can count how many times they are the same using `sum(language == languagePred)`.
    We can measure the model accuracy by dividing the number of correct predictions
    by the total, that is, `.N` (the number of rows), as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Here, `percCorrect` varies a lot depending on the training/test dataset split.
    As we have different language groups, `percCorrect` is not particularly high.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the k-nearest neighbor algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We built our KNN model using 37 features that have a different relevance to
    the language. Given a new flag, its neighbors are the flags sharing a lot of attributes,
    regardless of their relevance. If a flag has different common attributes that
    are irrelevant to the language, we erroneously include it in the neighborhood.
    On the other hand, if a flag shares a few highly-relevant attributes, it won't
    be included.
  prefs: []
  type: TYPE_NORMAL
- en: KNN performs worse in the presence of irrelevant attributes. This fact is called
    the curse of dimensionality and it's quite common in machine learning algorithms.
    A solution to the curse of dimensionality is to rank the features on the basis
    of their relevance and to select the most relevant. Another option that we won't
    see in this chapter is using dimensionality reduction techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, in the *Ranking the features using a filter or a dimensionality
    reduction* section, we measured the feature''s relevance using the information
    gain ratio. Now, we can compute the `dtGains` table, similar to the previous chapter,
    starting from `dtTrain`. We cannot use the whole `dtFeatures` because we''re pretending
    not to know the language of the test set countries. If you want to see how `information.gain`
    works, you can take a look at [Chapter 4](ch04.html "Chapter 4. Step 1 – Data
    Exploration and Feature Engineering"), *Step 1 – Data Exploration and Feature
    Engineering*. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The `feature` column contains the feature names and the `attr_importance` column
    displays the feature gain, which expresses its relevance. In order to select the
    most relevant features, we can first rebuild `arrayFeatures` with the sorted features.
    Then, we''ll be able to select the top, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting from `arrayFeatures` and given a `nFeatures` number, we want to build
    the formula using the top `nFeatures` features. In order to be able to do this
    for any `nFeatures`, we can define a function to build the formula, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the top `nFeatures` features and put them into `arrayFeaturesTop`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the first part of the formula string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the features to the formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `formulaKnn` into a `formula` format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Return the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using our function, we can build `formulaKnnTop` using the top 10 features,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can build the model using the same inputs as before, with the exception
    of `formula input` that now contains `formulaKnnTop`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we can add the predicted language to `dtTest` in a new
    column called `languagePred10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compute the percentage of languages that we identified correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Have we achieved any improvement by selecting the top features? In order to
    determine which model is the most accurate, we can compare `percCorrect10` with
    `percCorrect` and determine which is the highest. We randomly defined the split
    between `dtTrain` and `dtTest`, so the result changes every time we run the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There is another option to avoid the curse of dimensionality. The flags are
    described by 37 features with different relevancies and we selected the 10 most
    relevant. In this way, the similarity depends on the number of features that are
    in common out of the top 10\. What if we have two flags with just two out of the
    top 10 features and 20 out of the remaining features in common? Are they less
    similar than two flags with three out of the top 10 features in common? Instead
    of ignoring the other 27 features, we can use them giving them a lower relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a KNN variation, called **weighted KNN**, which identifies the relevance
    of each feature and builds the KNN accordingly. There are different KNN versions
    and the `kknn` function allows us to use some of them, specifying the `kernel`
    argument. In our case, we can set `kernel = ''optimal''`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned earlier, we can measure the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the training/test split, `percCorrectWeighted` can be higher or
    lower than `percCorrect`.
  prefs: []
  type: TYPE_NORMAL
- en: We saw different options to build a supervised machine learning model. In order
    to identify which performs best, we need to evaluate each option and optimize
    the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to identify homogeneous clusters and visualize
    the clustering process and results. You defined a feasible supervised machine
    learning problem and solved it using KNN. You evaluated the model, accuracy and
    modified its parameters. You also ranked the features and selected the most relevant.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see a better approach to evaluating the accuracy
    of a supervised learning model. You will see a structured approach to optimizing
    the model parameters and selecting the most relevant features.
  prefs: []
  type: TYPE_NORMAL
