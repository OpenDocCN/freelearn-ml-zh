<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer161">
<h1 class="chapter-number" id="_idParaDest-145"><a id="_idTextAnchor202"/>11</h1>
<h1 id="_idParaDest-146"><a id="_idTextAnchor203"/>Case Study – The Qatar Blockade</h1>
<p>In this chapter, we will<a id="_idIndexMarker851"/> look at what happens when we apply one of our classifiers to real data that has not been carefully curated, that we don’t have a Gold Standard for, and that was not the data that we trained the classifier on. This is a real-life situation. You’ve trained a classifier; now, you want to use it. How well do the classifiers that we have looked at so far work in this situation? In this chapter, we will compare the output of a classifier on data collected over an extended period with events in an ongoing news story to see whether changes in the pattern of emotions can be linked to developments in the story and whether it is possible to detect long-term changes in public attitudes as well as immediate responses to key events. This analysis will be divided into <span class="No-Break">three parts:</span></p>
<ul>
<li>We will look at how specific events give rise to short-term changes in the pattern of emotions expressed <span class="No-Break">in tweets</span></li>
<li>We will investigate whether it is possible to detect long-term changes in public attitudes by tracking trends in the way emotions change <span class="No-Break">over time</span></li>
<li>We will look at the proportionality scores for several classifiers over the period to see whether proportionality is a good metric – if the proportionality scores for all our classifiers show the same trends, then it is likely to be safe to use this as a metric when looking for spikes and long-term trends in the emotions expressed <span class="No-Break">by tweets</span></li>
</ul>
<p>By the end of this chapter, you will have an appreciation of the extent to which emotion analysis tools can tell you interesting things about public attitudes, even when the data they have been trained on is not closely aligned with the data they are being <span class="No-Break">applied to.</span></p>
<p>In the previous chapters, we have seen that it is possible to use machine learning techniques to train tools that can assign emotions to tweets. These algorithms can perform well on data where one emotion is assigned to each tweet, with comparatively little variation in performance on this kind of data, but have more difficulty with datasets where zero or more labels can be assigned to a tweet. In the previous chapter, we suggested that proportionality might provide a useful metric <span class="No-Break">for assessing</span></p>
<p>whether a classifier provided a reliable overall view of a set of tweets even when there were large numbers of tweets with zero or multiple labels. Using multiple classifiers with either LEX or Naive Bayes for the base-level classifiers, and using <strong class="bold">neutral</strong> as an explicit label, gave acceptable F1 scores and quite high proportionality scores for the <span class="No-Break">key datasets.</span></p>
<p>However, this<a id="_idIndexMarker852"/> analysis was carried out on carefully curated datasets, where all the tweets were rigorously labeled and the training and test sets were drawn from the same overall collection split into sets of 10 separate 90:10 folds. The labels were assigned by multiple annotators, with a label assigned to a tweet if either all the annotators for that tweet agreed or the majority did. The folds were carefully constructed so that the tweets were randomly shuffled before being split into training and test sets, with no overlap between the training and test sets for a given fold and with every tweet appearing exactly once in a test set. Therefore, the results were as reliable as they could be for the <span class="No-Break">relevant data.</span></p>
<p>They do not, however, tell us how reliable the classifiers will be when applied to data other than the given training and test sets. It is almost inevitable that data collected using other criteria will have different characteristics. The overall distribution of emotions is likely to vary – after all, the obvious application of this technology is to analyze how attitudes to some event vary over time, which will affect, for instance, the prior probabilities used by Naive Bayes and hence is likely to affect the assignment of various labels. Suppose, for instance, that 10% of the tweets in the training set have the label <strong class="bold">anger</strong>, but that on some specific day, there is an upswell of anger over some issue and 50% of the tweets that day are angry. Consider some tweets containing the words <em class="italic">T1, …, Tn</em>. The formula for Naive Bayes says that <em class="italic">p(anger | T1 &amp; … &amp; Tn) = p(T1 | anger)×... × p(Tn | anger) × p(anger)/p(T1)×... × p(Tn)</em>. The term <em class="italic">p(anger)</em> here will come from the original data, where only 10% of the tweets express anger; but on the day in question, 50% do, so this part of the formula will lead to considerable underestimates for this emotion. If <em class="italic">T1, … Tn</em> are words that are linked to <strong class="bold">anger</strong>, <em class="italic">p(T1), …, P(Tn)</em> will also probably be underestimates of their frequency on this day, which may to some degree compensate for the error in <em class="italic">p(anger)</em>, but it is not reasonable to assume that they will exactly compensate <span class="No-Break">for it.</span></p>
<p>The other algorithms are likely to suffer from similar issues, though it is less easy to see exactly how this will play out for other cases. It is clear, however, that performance on test data extracted from one collection cannot be assumed to carry over to data collected <span class="No-Break">from another.</span></p>
<p>It is also not feasible to annotate all the new data to check the accuracy of a classifier. The whole point of building a classifier is that we want to collect information from new data that we do not have the resources to label by hand – if we could afford to label it by hand, then we would almost certainly get better results than can be achieved by any classifier since assigning labels to tweets is an essentially <span class="No-Break">subjective task.</span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor204"/>The case study</h1>
<p>To investigate the links <a id="_idIndexMarker853"/>between emotions expressed in texts and real-world events, we needed a set of tweets collected over an extended period, along with a picture of the major events that occurred during that period that could reasonably be expected to affect public opinion. We had two <span class="No-Break">goals here:</span></p>
<ul>
<li>To see whether emotions expressed in tweets could be matched to events that would be likely to affect <span class="No-Break">public opinion</span></li>
<li>To see whether classification algorithms that perform well on carefully curated datasets (where the test sets are drawn from the same collection of tweets as the training sets) continue to give good results on <span class="No-Break">brand-new data</span></li>
</ul>
<p>So, we carried out a longitudinal test on historic data to see how variations in the daily pattern of assignments matched real-world events. This provides a test of the utility of the overall strategy. If a classifier detects that labels corresponding to some emotion show variations that are explicable in terms of real-world events, then it is reasonable to use it for that kind of task, irrespective of how it performs on the data that it was originally trained and tested on. A classifier that performs well on test data but does not find changes in data collected as events unfold is less useful than one that does not do so well on the tests but does detect changes corresponding to <span class="No-Break">real-world events.</span></p>
<p>For our test set, we collected around 160,000 Arabic tweets that originated from Qatar between June 2017 and March 2018. This was a period during which Qatar was subject to a blockade by neighboring countries, with decisions made by the Qatari governments and those of its neighbors leading to substantial rapid changes in public opinion. We applied a version of LEX with a tokenizer and stemmer for Arabic and the standard set of 11 emotions (<strong class="bold">sadness</strong>, <strong class="bold">anger</strong>, <strong class="bold">fear</strong>, <strong class="bold">disgust</strong>, <strong class="bold">pessimism</strong>, <strong class="bold">love</strong>, <strong class="bold">joy</strong>, <strong class="bold">anticipation</strong>, <strong class="bold">trust</strong>, <strong class="bold">surprise</strong>, and <strong class="bold">optimism</strong>) to this data and looked at how the distribution changed in response to these decisions. The results revealed that the residents of Qatar experienced an emotional rollercoaster during the first 9 months of the blockade, with opinions as expressed in tweets following very rapidly after public events. This provides some validation of the use of a classifier trained on a specific dataset as a tool for extracting information <a id="_idIndexMarker854"/>from data that may be linguistically different from the training data (the training data for the classifier was general Gulf Arabic, which is not identical to Qatari dialect, even for Modern Standard Arabic) and where the distribution of emotions may be quite different from that in the <span class="No-Break">training data.</span></p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor205"/>Short-term changes</h2>
<p>The<a id="_idIndexMarker855"/> first thing to look for is correlations between spikes in the various emotions and events in the real world. Even if the actual distribution of labels is less than 100% accurate, if divergences from the baseline scores can be shown to correspond to real-world events, then the tool can be used to track public opinion. Being able to tell that people are angrier or more fearful today than they were yesterday is probably as useful as knowing that 56.7% of today’s tweets are angry where the level is usually 43.6% – messages on social media are, after all, not an accurate picture of public opinion in general. Twitter and similar outlets are echo chambers that tend to overestimate the strength of feeling in the general population. If 56.7% of tweets about some topic are angry, about 5% of the general population are likely mildly annoyed about it. But if twice as many tweets are angry about it today compared to yesterday, then the general population may well be twice as angry about it today than it <span class="No-Break">was yesterday.</span></p>
<p>Due to this, we looked for spikes in positive and negative sentiments and mapped them to the most relevant news in Qatar on that day. To identify the most relevant news, we looked at Google News (news.google.com) and used Google’s sorting feature to identify the most important blockade event of the day based on news relevance. We also looked at Al Jazeera’s dedicated page (<a href="https://www.aljazeera.com/news/2018/8/2/qatar-gulf-crisis-all-the-latest-updates">https://www.aljazeera.com/news/2018/8/2/qatar-gulf-crisis-all-the-latest-updates</a>) on events related to the blockade to identify the main events of the day. Our results showed that the spikes in sentiments that our algorithm identified from the collected tweets corresponded to significant blockade-related events, simultaneously confirming that the results that were obtained by controlled testing in the SemEval experiments transfer effectively to real-world scenarios for at least some classifiers and allowing us to probe the way that people reacted to <span class="No-Break">these events.</span></p>
<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em> shows a spike in optimism and a small drop in pessimism on August 17, 2017. The main event from the news outlook was that Saudi Arabia granted Qatari pilgrims permission to perform Hajj, which indicates a lessening of tension between the two countries. Accordingly, our data showed a spike in <strong class="bold">optimism</strong> (from 7% of tweets showing optimistic sentiments to 22%) and a decline in <strong class="bold">pessimism</strong> (from 5% to 2%). Note that while the change from 7% to 22% looks bigger than the change from 5% to 2%, they are both roughly threefold changes, which is reasonable given that these two are direct opposites. The spike is very short-lived, with both emotions returning to their baseline levels<a id="_idIndexMarker856"/> after a couple of days. The decline in <strong class="bold">optimism</strong> is slower than the initial spike, probably reflecting the fact that the initial tweets were retweeted and commented on so that the emotions that they expressed <a id="_idIndexMarker857"/>continue to be present even after the significance of the actual event has faded (the <strong class="bold">echo </strong><span class="No-Break"><strong class="bold">chamber</strong></span><span class="No-Break"> effect):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 11.1 – Change of sentiments due to Saudi Arabia allowing pilgrims to perform Hajj" height="699" src="image/B18714_11_01.jpg" width="945"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Change of sentiments due to Saudi Arabia allowing pilgrims to perform Hajj</p>
<p>Another example of a radical change in sentiment (July 3) relates to the change of position of the US government concerning Qatar. More specifically, President Trump reviewed his earlier position suggesting that Qatar supports terrorism and said that Qatar is a partner in fighting terrorism and encouraged GCC unity. Harmoniously, the sentiments associated with <strong class="bold">joy</strong> spiked from 7% to 20%, while <strong class="bold">sadness</strong> declined from 4% to 2% (see <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em>). A similar radical change occurred on July 9, when the chief prosecutor of the International Criminal Court praised Qatar and regretted the actions of the Quartet. Simultaneously joyful tweets increased to 14% and sad tweets declined to 2%. Again, the spikes are short-lived and are slightly steeper on the way up than on the way down, and the percentage changes in the upward and downward directions are similar (joy goes up between two- and three-fold, sadness goes <span class="No-Break">down two-fold):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 11.2 – Change of sentiments due to Trump’s review of position and the ICC chief prosecutor’s comments" height="702" src="image/B18714_11_02.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Change of sentiments due to Trump’s review of position and the ICC chief prosecutor’s comments</p>
<p>In <a id="_idIndexMarker858"/>these cases, the spike in one emotion (<strong class="bold">optimism</strong>, <strong class="bold">joy</strong>) was matched by a proportionally similar drop in the opposite emotion (<strong class="bold">pessimism</strong>, <strong class="bold">sadness</strong>). This does not always happen. If we look at two days when there is a substantial spike in fear, we will find that there is also a spike in one of the <span class="No-Break">positive emotions:</span></p>
<ul>
<li><strong class="bold">November 1, 2017</strong>: This day was observed to be the most fearful day for the residents of Qatar when 33% of tweets exhibited fearful sentiments (see <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em>). It was when Bahrain imposed entry visas on Qatar nationals and residents, which divided many families that had members in <span class="No-Break">both counties.</span></li>
<li><strong class="bold">November 4, 2017</strong>: This day was observed to be the third most fearful day for the residents of Qatar when 28% of tweets exhibited fearful sentiments (see <em class="italic">Figures 11.3</em> and <em class="italic">11.4</em>). It was when the foreign ministers of Saudi Arabia, Bahrain, UAE, and Egypt met in Abu Dhabi and reiterated freezing the membership of Qatar at the Gulf <span class="No-Break">Cooperation Council.</span></li>
</ul>
<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em> shows that Qatari residents expressed <strong class="bold">love</strong> simultaneously with <strong class="bold">fear</strong> on November 1 and 4. It is hard to be sure about how to interpret this, especially given the large spike in <strong class="bold">love</strong> on October 28, which corresponds to a drop in <strong class="bold">fear</strong>. There are several possible explanations: it may be that there was some other significant event around this time that independently led to an increase in tweets expressing <strong class="bold">love</strong>; it may be a consequence <a id="_idIndexMarker859"/>of the echo-chamber effect, though that seems unlikely since the initial spike on October 28 died away by October 31; or it may be that the rise in tweets expressing <strong class="bold">fear</strong> leads directly to a rise in ones expressing <strong class="bold">love</strong> (possibly, in fact, to tweets expressing both at the same time) as people contact their loved ones in times of stress. Whatever the underlying reason, this figure shows that conflicting emotions can interact in <span class="No-Break">surprising ways:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 11.3 – Fear and love cooccurring" height="1061" src="image/B18714_11_03.jpg" width="1364"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Fear and love cooccurring</p>
<p>In addition to the spikes in tweets expressing <strong class="bold">love</strong> over this period, Qatari residents also expressed <strong class="bold">joy</strong> at the same time as they expressed the <span class="No-Break">most </span><span class="No-Break"><strong class="bold">fear</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 11.4 – Fear and joy cooccurring" height="767" src="image/B18714_11_04.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Fear and joy cooccurring</p>
<p>The <a id="_idIndexMarker860"/>spikes in <strong class="bold">joy</strong> in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.4</em> match those in love from the previous figure, again including spikes in <strong class="bold">joy</strong> alongside the spikes in <strong class="bold">fear</strong>. <strong class="bold">love</strong> continues to increase up to November 7, where <strong class="bold">joy</strong> tails off at this point, which may again be an echo-chamber effect. What is clear is that, unsurprisingly, major real-world events are followed very rapidly by spikes in the emotions expressed in tweets, though the emotions that are affected can be more surprising and can occur in surprising combinations. Interpreting exactly what is going on may require detailed analysis (in the same way that gross measures of accuracy such as F1-measure and proportionality may require detailed analysis through finer-grained tools such as confusion matrices), but changes in the distribution of emotions in tweets do indicate changes in <span class="No-Break">public attitudes.</span></p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor206"/>Long-term changes</h2>
<p>Significant <a id="_idIndexMarker861"/>events, then, produce spikes in the emotions expressed on social media. This is hardly surprising, but it is worth confirming that even quite simple classification algorithms can detect these spikes and that, sometimes, they occur in unexpected combinations. What about <span class="No-Break">long-term trends?</span></p>
<p>It is harder to interpret these because they do not correspond to easily identifiable changes in the real world. The following figure shows general trends over the nine months of the blockade. Interpreting these is a challenge – do they track the progress of the political situation, or do they reflect a general weary acceptance of a situation that has not changed <span class="No-Break">very much?</span></p>
<p>These figures <a id="_idIndexMarker862"/>show eight of the emotions in matched pairs (<strong class="bold">anticipation</strong>, <strong class="bold">trust</strong>, and <strong class="bold">surprise</strong> tend to have low scores and not to change very much, either in response to specific events or over the long term, so we have omitted them). The given pairs seem reasonable – disgust and fear seem likely to form a matched pair; joy versus sadness, optimism versus pessimism, and anger versus love seem likely to behave as opposites – though as we have seen, opposites can show spikes at the same time, and it may be that pairing them off differently would give us a different <span class="No-Break">overall picture:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<img alt="" height="1093" src="image/B18714_11_05.jpg" width="1444"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Adaption over an extended period?</p>
<p>The trend lines in these figures are heavily smoothed – the data, as discussed previously, contains very large spikes, and fitting a trend line that obscures these may also obscure other gentler changes. Nonetheless, it is worth trying to see whether any general lessons can be learned <span class="No-Break">from them.</span></p>
<p>It does seem that <a id="_idIndexMarker863"/>there is a general upward trend in the positive emotions – optimism, joy, and love all increase fairly steadily over the period – and a downward trend in the negative ones, with anger and pessimism both falling steadily and disgust and fear both decreasing slightly. Exactly how to interpret this is unclear. It could be that the overall situation was improving over the period covered by the dataset and that these trends are simply a reasonable response to the changes in the political situation. Alternatively, it could reflect the fact that people adjust to whatever situation they find themselves in, so over a period of stress and anxiety, their attitude to the situation becomes more relaxed, and positive emotions simply become <span class="No-Break">more common.</span></p>
<p>The most surprising thing in this figure is that joy and sadness both show an increase toward the end of the period. It is hard to see why this would be the case – it cannot easily be explained in terms of growing adaptation to an unchanging situation, but at the same time, it is hard to see what changes in the situation could lead to an increase in both joy and sadness. Looking at trends over an extended period can be revealing, but it can also <span class="No-Break">be confusing!</span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor207"/>Proportionality revisited</h1>
<p>The preceding <a id="_idIndexMarker864"/>curves were obtained by running the multi-LEX classifier on the date-stamped tweets about the Qatar blockade. In the previous chapter, we looked at the notion that proportionality might provide a useful measure of how accurately a classifier that assigned a large number of false positives or false negatives might nonetheless provide an accurate general picture, even if its accuracy on individual tweets was flawed. We can approach this question by plotting the outputs of a range of different classifiers on the individual emotions: if these are generally similar, then we can at least hope that the places where they show peaks and troughs are <span class="No-Break">reasonably reliable.</span></p>
<p>Recall that the F1 and proportionality scores for the various classifiers, when trained and tested using 10-fold cross-validation on the SEM11-AR datasets, were <span class="No-Break">as follows:</span></p>
<table class="No-Table-Style" id="table001-11">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">DNN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LEX</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MULTI-DNN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MULTI-LEX</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MULTI-NB</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MULTI-SVM</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">NB</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SVM</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p>0.360 (<span class="No-Break">0.742)</span></p>
</td>
<td class="No-Table-Style">
<p>0.549 (<span class="No-Break">0.940)</span></p>
</td>
<td class="No-Table-Style">
<p>0.415 (<span class="No-Break">0.971)</span></p>
</td>
<td class="No-Table-Style">
<p>0.520 (<span class="No-Break">0.869)</span></p>
</td>
<td class="No-Table-Style">
<p>0.543 (<span class="No-Break">0.996)</span></p>
</td>
<td class="No-Table-Style">
<p>0.321 (<span class="No-Break">0.878)</span></p>
</td>
<td class="No-Table-Style">
<p>0.413 (<span class="No-Break">0.770)</span></p>
</td>
<td class="No-Table-Style">
<p>0.379 (<span class="No-Break">0.817)</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – F1 and proportionality for a range of classifiers on SEM11-AR</p>
<p>In the<a id="_idIndexMarker865"/> following figure, we have plotted the proportional scores for various emotions obtained by LEX, MULTI-LEX, and MULTI-NB, which are the classifiers with the best F1, scores over the period – that is, what proportion of the tweets at each date were labeled with the given emotion. Some of the data around the end of October 2017 is missing, and the plots are erratic over this period, but in general, there is a close correspondence between the plots for these <span class="No-Break">three classifiers:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="Figure 11.7 – Correlation between proportionality scores" height="1376" src="image/B18714_11_07.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Correlation between proportionality scores</p>
<p>The other<a id="_idIndexMarker866"/> plots for these classifiers are similar, with closely matching shapes, albeit sometimes with one lower than the other. It is inevitable that if one classifier assigns a higher proportion of tweets to a specific emotion than the others, then it will assign a lower proportion to another – MULTI-NB assigns more tweets to disgust and pessimism than the other two, and hence it has to assign fewer to the other labels. This also explains why all three have troughs for both joy and sadness around the beginning of October 2017 – they all have large peaks for disgust and pessimism at this point, and there simply is no room for either sadness <span class="No-Break">or joy.</span></p>
<p>For these classifiers to follow each other as closely as they do, they must be tracking something similar. Are they tracking the proportion of tweets that express each emotion day by day, or are they just tracking the same words? They are all lexicon-based, and LEX and MULTI-LEX collect their lexicons in much the same way, so it seems very likely that they are tracking words. But the fact that we can correlate the major peaks to specific blockade-related events suggests that the words they are paying attention to correspond to emotions – that is, they are tracking emotions. By looking at the words that are most strongly linked to <strong class="bold">anger</strong> and <strong class="bold">joy</strong> in these classifiers, they do look like words that express these <a id="_idIndexMarker867"/>emotions (the translations are taken word by word from Google Translate, so they might be a <span class="No-Break">bit strange).</span></p>
<p><span class="No-Break"><strong class="bold">ANGER</strong></span></p>
<p><span class="No-Break">MULTI-LEX</span></p>
<p>الغضب (anger) الكهرب (electrification) المسلمين (Muslims) 😠 (😠) حسبنا (we counted) ونعم (and yes) استياء (discontent) خيانة (betrayal) <span class="No-Break">الوكيل</span><span class="No-Break"> (agent)</span></p>
<p><span class="No-Break">LEX</span></p>
<p>الغضب (anger) الكهرب (electrification) 😠 (😠) 😑 (😑) ونعم  (and yes) وترهيب  (intimidation) حقير (despicable) جاب (gap) <span class="No-Break">نرفزة</span><span class="No-Break"> (willies)</span></p>
<p><span class="No-Break">MULTI-NB</span></p>
<p>😠 (😠) غضب (anger) 😡 (😡) قبحهم (ugliness) نرفزة (willies) الوكيل (agent) ونعم (and yes) حانق (grouchy) الدول (countries) غضبي (<span class="No-Break">my anger)</span></p>
<p><span class="No-Break"><strong class="bold">JOY</strong></span></p>
<p><span class="No-Break">MULTI-LEX</span></p>
<p>💃 (💃) اعلان (advertisement) 🙈 (🙈) 💪 (💪)  💝 (💝) 💘 (💘) 🎈(🎈) يعكرها (disturb) وسعاده (happiness)مبسوط  (<span class="No-Break">happy)</span></p>
<p><span class="No-Break">LEX</span></p>
<p>💝 (💝) #بهجة_أمل (#joy_of_hope)  💃 (💃) الفرحه (joy) #ريح_المدام (#the_wind_of_madam) #السعادة (#happiness) مبسوط (<span class="No-Break">happy)</span><span class="No-Break">💪</span><span class="No-Break"> (</span><span class="No-Break">💪</span><span class="No-Break">)</span></p>
<p><span class="No-Break">MULTI-NB</span></p>
<p>عام (general) #بهجة_أمل (#joy_of_hope) #السعادة (#happiness) 🎈(🎈) اعلان (adverti<a id="_idTextAnchor208"/>sement) سعيد (happy) يعكرها (disturb) 😍 (😍) 💝 (💝)</p>
<p>There are several things to <span class="No-Break">note here:</span></p>
<ul>
<li>Emojis appear quite high on the list of emotion markers. This really should not be surprising since that is exactly what emojis are for, but it is at least interesting to note that they do appear in the top 10 words for both sentiments for all <span class="No-Break">the classifiers.</span></li>
<li>A lot of words are shared by the three classifiers, though they are not always ranked the same. However, some words are emotion-bearing that are found by one but not the others (استياء (discontent),  حانق (grouchy)), so they are not just finding the <span class="No-Break">same sets.</span></li>
<li>Some words are not emotion-bearing – المسلمين (Muslims) seems unlikely to be a term for expressing anger, but if there are a large number of angry tweets about the way that Muslims are treated, then this word will come to be associated <span class="No-Break">with anger.</span></li>
</ul>
<p>So, we have<a id="_idIndexMarker868"/> several independent ways of checking that classifiers are doing what they are supposed to do in addition to re-annotating all <span class="No-Break">the data:</span></p>
<ul>
<li>If several classifiers assign the same peaks and troughs, they are likely to be identifying the same patterns in <span class="No-Break">the text</span></li>
<li>If these peaks and troughs can be aligned with external events, then the patterns probably do correspond <span class="No-Break">to emotions</span></li>
<li>If the words and emojis that are strongly linked to an emotion include words that would be expected to express that emotion, then the outputs probably do correspond to the <span class="No-Break">given emotions</span></li>
</ul>
<p>Not all classifiers behave as nicely as this. If we add, for instance, SVM and simple Naive Bayes to the mix, we get <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="Figure 11.8 – Naive Bayes and SVM don’t correlate with the others" height="1852" src="image/B18714_11_08.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Naive Bayes and SVM don’t correlate with the others</p>
<p>What has happened here is that the training of both Naive Bayes and SVM is dominated by the first label that is assigned to each tweet. So, if a tweet is assigned anger and disgust, it is simply treated as an instance of anger since there is no scope for assigning multiple labels <strong class="bold">during training</strong> for these classifiers, even if it is possible to assign thresholds to allow multiple labels in the test/application data. So, we get very high scores for <strong class="bold">anger</strong> and very low ones for other negative emotions. There are some peaks and troughs in roughly the right places for these classifiers, but they are swamped by the fact that they both assign anger to 90% or more of all tweets and less than 5% to all the other <span class="No-Break">negative emotions.</span></p>
<p>A number of <a id="_idIndexMarker869"/>the other classifiers (DNN, SVM) simply assign everything to neutral, which shows up as a flat line in every plot. This is despite the fact that they score moderately well on the test data (which is drawn from the same source as the training data, though of course the two are kept apart on each fold). This suggests that these classifiers are more easily influenced by properties of the training data that are specific to the way it was obtained and presented, rather than just to the gross distribution of words. In a sense, what makes them perform well in the original experiments – namely that they are sensitive to things other than the raw word counts – is exactly what makes them less robust when applied to <span class="No-Break">new data.</span></p>
<p>The two main lessons from this case study are <span class="No-Break">as follows:</span></p>
<ul>
<li>Looking at longitudinal data can be extremely revealing. By plotting the proportion of tweets that are assigned different labels and comparing these plots to external events and each other, we can obtain information about their reliability as indicators of emotions in tweets drawn from entirely new sources. This is not easily available if we just look at a single source. When testing with single sources, we obtained very high scores for classifiers that did not transfer well to the new data, despite the care that we took to carry out 10-fold cross-validation with properly separated training and <span class="No-Break">test sets.</span></li>
<li>The very high scores that were obtained by the more sophisticated classifiers fell away sharply when they were applied to the new data. It seems likely that these classifiers were responding to characteristics of the original data that are not replicated in the new material. The simple classifiers, which rely very heavily on simple statistics about the probability of a particular word being linked to a particular emotion, are more robust since that is all that they are sensitive to and that is typically <a id="_idIndexMarker870"/>carried over into new <span class="No-Break">text types.</span></li>
</ul>
<p>Let’s summarize everything we’ve covered <span class="No-Break">so far.</span></p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor209"/>Summary</h1>
<p>In the previous chapters, we looked at the performance of a collection of classifiers on a range of datasets, with datasets with varying numbers of emotions, varying sizes, and varying kinds of text and, most importantly, with some datasets that assigned exactly one label to each tweet and some that allowed zero or more labels per tweet. The conclusion at the end of <a href="B18714_10.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">, Multiclassifier</em> was that “different <span class="No-Break">tasks require</span></p>
<p>different classifiers.” This holds even more strongly now that we have tried our classifiers on data that does not match the data they were trained on, with the DNN and SVM classifiers that performed well on some of the previous datasets doing extremely poorly on the case <span class="No-Break">study data.</span></p>
<p>These two classifiers seem to have assigned <strong class="bold">neutral</strong> to almost all the tweets in this dataset. This seems likely because the clues that these classifiers are sensitive to are missing from, or at any rate rare in, the data, and hence they are not assigning any labels. It looks as though these classifiers assign very strong weights to specific cues, and if those cues are not present in the data that has been collected from different sources or following different guidelines, then the classifiers cannot make use of the information that <span class="No-Break"><em class="italic">is</em></span><span class="No-Break"> present.</span></p>
<p>It is, of course, difficult to assess the performance of data for which you do not have a Gold Standard. It is infeasible to construct a fresh Gold Standard every time you want to try your classifier on a new dataset – the data used in this case study, for instance, contains 160K tweets, and annotating all of these would be an extremely time-consuming task. Given that we do not have a Gold Standard for this data, how can we be sure that DNN and SVM are not correct in deciding that virtually none of the tweets in it express <span class="No-Break">any emotion?</span></p>
<p>We have two checks <span class="No-Break">on this:</span></p>
<ul>
<li>The fact that the other classifiers shadow each very closely suggests that they are indeed tracking something – it would be beyond coincidence that they all show peaks and troughs at the <span class="No-Break">same points.</span></li>
<li>The correlation between the major peaks and troughs and real-world events relating to the blockade further suggests that what they are tracking is public opinion. Again, it would be beyond coincidence that the largest peaks in <strong class="bold">fear</strong> occur following events such as the ban on entry visas to Bahrain and the meeting at which Qatar’s membership of the Gulf Cooperation Council was frozen, and the largest peaks in <strong class="bold">joy</strong> follow events such as permission being granted for pilgrims to <span class="No-Break">perform Hajj.</span></li>
</ul>
<p>So, yet again, you can see that you should be wary of assuming that a classifier that performs well on some dataset will perform equally well on another. Always, always, always try out different classifiers, with different settings, and make your own decision about which works best for your task on <span class="No-Break">your data.</span></p>
<p>The correlation between the scores for the various classifiers does, however, also suggest a further strategy: combine them. There are several ensemble learning strategies for using the results of a combination of classifiers – for example, by assigning a label if the majority of classifiers in the ensemble recommend assigning it, by assigning a label if the sum of the individual scores for that label exceeds some threshold, or even by training a new classifier whose input is the predictions of the members of the ensemble. Just as there is a wide range of classifiers to choose from, there is also a wide range of strategies for combining the results of classifiers. So, again, you should try different ensemble strategies and don’t believe what people say about <span class="No-Break">their favorites.</span></p>
<p><em class="italic">Always, always, always, try out different classifiers, with different settings, and make your own decision about which works best for your task on </em><span class="No-Break"><em class="italic">your data.</em></span></p>
</div>
</div></body></html>