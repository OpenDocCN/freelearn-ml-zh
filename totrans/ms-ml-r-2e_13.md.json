["```py\n    > library(tm)\n\n    > library(wordcloud)\n\n    > library(RColorBrewer)\n\n```", "```py\n > getwd()\n\n    > setwd(\".../data\") \n\n```", "```py\n    > name <- file.path(\".../text\")\n\n    > length(dir(name))\n    [1] 7\n\n    > dir(name)\n    [1] \"sou2010.txt\" \"sou2011.txt\" \"sou2012.txt\" \"sou2013.txt\"\n    [5] \"sou2014.txt\" \"sou2015.txt\" \"sou2016.txt\"\n\n```", "```py\n    > docs <- Corpus(DirSource(name))\n\n > docs\n    <<VCorpus>>\n    Metadata:  corpus specific: 0, document level (indexed): 0\n    Content:  documents: 7\n\n```", "```py\n\n    > docs <- tm_map(docs, tolower)\n\n    > docs <- tm_map(docs, removeNumbers)\n\n    > docs <- tm_map(docs, removePunctuation)\n\n    > docs <- tm_map(docs, removeWords, stopwords(\"english\"))\n\n    > docs <- tm_map(docs, stripWhitespace)\n\n```", "```py\n    > docs <- tm_map(docs, removeWords, c(\"applause\", \"can\", \"cant\", \n      \"will\",\n    \"that\", \"weve\", \"dont\", \"wont\", \"youll\", \"youre\"))\n\n```", "```py\n    > docs = tm_map(docs, PlainTextDocument)\n\n    > dtm = DocumentTermMatrix(docs)\n\n    > dim(dtm)\n    [1]    7 4738\n\n```", "```py\n    > dtm <- removeSparseTerms(dtm, 0.75)\n\n    > dim(dtm)\n    [1]    7 2254\n\n```", "```py\n    > rownames(dtm) <- c(\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \n       \"2015\", \"2016\")\n\n```", "```py\n > inspect(dtm[1:7, 1:5])\n Terms\n Docs abandon ability able abroad absolutely\n 2010       0       1    1      2          2\n 2011       1       0    4      3          0\n 2012       0       0    3      1          1\n 2013       0       3    3      2          1\n 2014       0       0    1      4          0\n 2015       1       0    1      1          0\n 2016       0       0    1      0          0 \n\n```", "```py\n    > freq <- colSums(as.matrix(dtm))\n\n    > ord <- order(-freq)\n\n```", "```py\n    > freq[head(ord)]\n        new  america  people   jobs    now  years \n 193      174     168    163    157    148 \n\n    > freq[tail(ord)]\n        wright written yearold youngest youngstown zero \n 2       2       2        2          2    2\n\n```", "```py\n    > head(table(freq))\n    freq\n 2   3   4   5   6  7 \n 596 354 230 141 137 89\n\n    > tail(table(freq))\n    freq\n 148 157 163 168 174 193 \n 1   1   1   1   1   1\n\n```", "```py\n    > findFreqTerms(dtm, 125)\n    [1] \"america\" \"american\" \"americans\" \"jobs\" \"make\" \"new\" \n     [7] \"now\"     \"people\"   \"work\"      \"year\" \"years\" \n\n```", "```py\n > findAssocs(dtm, \"jobs\", corlimit = 0.85)\n $jobs\n colleges serve market shouldnt defense  put  tax came \n 0.97  0.91   0.89     0.88    0.87 0.87 0.87 0.86\n\n```", "```py\n    > wordcloud(names(freq), freq, min.freq = 70, scale = c(3, .5),  \n      colors = brewer.pal(6, \"Dark2\"))\n\n```", "```py\n    > wordcloud(names(freq), freq, max.words = 25)\n\n```", "```py\n > freq <- sort(colSums(as.matrix(dtm)), decreasing = TRUE) \n > wf <- data.frame(word = names(freq), freq = freq) \n > wf <- wf[1:10, ] \n > barplot(wf$freq, names = wf$word, main = \"Word Frequency\",\n xlab = \"Words\", ylab = \"Counts\", ylim = c(0, 250))\n\n```", "```py\n    > library(topicmodels)\n\n    > set.seed(123)\n\n    > lda3 <- LDA(dtm, k = 3, method = \"Gibbs\")\n\n    > topics(lda3)\n    2010 2011 2012 2013 2014 2015 2016 \n       2    1    1    1    3    3    2\n\n```", "```py\n > terms(lda3, 25)\n Topic 1      Topic 2       Topic 3 \n [1,] \"jobs\"       \"people\"      \"america\" \n [2,] \"now\"        \"one\"         \"new\" \n [3,] \"get\"        \"work\"        \"every\" \n [4,] \"tonight\"    \"just\"        \"years\" \n [5,] \"last\"       \"year\"        \"like\" \n [6,] \"energy\"     \"know\"        \"make\" \n [7,] \"tax\"        \"economy\"     \"time\" \n [8,] \"right\"      \"americans\"   \"need\" \n [9,] \"also\"       \"businesses\"  \"american\" \n [10,] \"government\" \"even\"        \"world\" \n [11,] \"home\"       \"give\"        \"help\" \n [12,] \"well\"       \"many\"        \"lets\" \n [13,] \"american\"   \"security\"    \"want\" \n [14,] \"two\"        \"better\"      \"states\" \n [15,] \"congress\"   \"come\"        \"first\" \n [16,] \"country\"    \"still\"       \"country\" \n [17,] \"reform\"     \"workers\"     \"together\" \n [18,] \"must\"       \"change\"      \"keep\" \n [19,] \"deficit\"    \"take\"        \"back\" \n [20,] \"support\"    \"health\"      \"americans\"\n [21,] \"business\"   \"care\"        \"way\" \n [22,] \"education\"  \"families\"    \"hard\" \n [23,] \"companies\"  \"made\"        \"today\" \n [24,] \"million\"    \"future\"      \"working\" \n [25,] \"nation\"     \"small\"       \"good\" \n\n```", "```py\n    > library(qdap)\n    > speech16 <- paste(readLines(\"sou2016.txt\"), collapse=\" \")\n    Warning message:\n    In readLines(\"sou2016.txt\") : incomplete final line found on \n      'sou2016.txt'\n\n    > speech16 <- iconv(speech16, \"latin1\", \"ASCII\", \"\") \n\n```", "```py\n > prep16 <- qprep(speech16) \n\n```", "```py\n > prep16 <- replace_contraction(prep16)\n\n > prep16 <- rm_stopwords(prep16, Top100Words, separate = F)\n\n > prep16 <- strip(prep16, char.keep = c(\"?\", \".\")) \n\n```", "```py\n > sent16 <- data.frame(speech = prep16)\n\n > sent16 <- sentSplit(sent16, \"speech\")\n\n > sent16$year <- \"2016\"\n\n```", "```py\n > speech10 <- paste(readLines(\"sou2010.txt\"), collapse=\" \")\n > speech10 <- iconv(speech10, \"latin1\", \"ASCII\", \"\")\n > speech10 <- gsub(\"(Applause.)\", \"\", speech10)\n > prep10 <- qprep(speech10)\n > prep10 <- replace_contraction(prep10)\n > prep10 <- rm_stopwords(prep10, Top100Words, separate = F)\n > prep10 <- strip(prep10, char.keep = c(\"?\", \".\"))\n > sent10 <- data.frame(speech = prep10)\n > sent10 <- sentSplit(sent10, \"speech\")\n\n > sent10$year <- \"2010\" \n\n```", "```py\n > sentences <- data.frame(rbind(sent10, sent16)) \n\n```", "```py\n > plot(freq_terms(sentences$speech))\n\n```", "```py\n > wordMat <- wfm(sentences$speech, sentences$year)\n\n > head(wordMat[order(wordMat[, 1], wordMat[, 2],decreasing = \n      TRUE),])\n 2010 2016\n our        120   85\n us          33   33\n year        29   17\n americans   28   15\n why         27   10\n jobs        23    8\n\n```", "```py\n > trans_cloud(sentences$speech, sentences$year, min.freq = 10)\n\n```", "```py\n > ws <- word_stats(sentences$speech, sentences$year, rm.incomplete = T)\n\n > plot(ws, label = T, lab.digits = 2)\n\n```", "```py\n    > pol = polarity(sentences$speech, sentences$year)\n\n    > pol\n     year total.sentences total.words ave.polarity sd.polarity \n       stan.mean.polarity\n   1 2010             435        3900        0.052       0.432              \n      0.121\n   2 2016             299        2982        0.105       0.395              \n      0.267 \n\n```", "```py\n    > plot(pol)\n\n```", "```py\n    > pol.df <- pol$all\n\n    > which.min(pol.df$polarity)\n    [1] 12\n\n    > pol.df$text.var[12]\n\n    [1] \"One year ago, I took office amid two wars, an economy rocked \n       by a severe recession, a financial system on the verge of \n          collapse, and a government deeply in debt.\n\n```", "```py\n    > ari <- automated_readability_index(sentences$speech, \n      sentences$year) \n\n    > ari$Readability\n      year word.count sentence.count character.count\n    1 2010       3900            435           23859\n    2 2016       2982            299           17957\n      Automated_Readability_Index\n    1                    11.86709\n    2                    11.91929\n\n```", "```py\n    > form <- formality(sentences$speech, sentences$year)\n\n    > form\n      year word.count formality\n    1 2016       2983     65.61\n    2 2010       3900     63.88\n\n```", "```py\n    > form$form.prop.by\n      year word.count  noun   adj  prep articles pronoun\n    1 2010       3900 44.18 15.95  3.67     0        4.51\n    2 2016       2982 43.46 17.37  4.49     0        4.96\n       verb adverb interj other\n    1 23.49   7.77   0.05  0.38\n    2 21.73   7.41   0.00  0.57\n\n```", "```py\n    > div <- diversity(sentences$speech, sentences$year)\n\n    > div\n      year   wc simpson shannon collision berger_parker brillouin\n    1 2010 3900   0.998   6.825     5.970         0.031     6.326\n    2 2015 2982   0.998   6.824     6.008         0.029     6.248\n\n```", "```py\n > dispersion_plot(sentences$speech,\n     rm.vars = sentences$year,\n     c(\"security\", \"jobs\", \"economy\"),\n     color = \"black\", bg.color = \"white\")\n\n```"]