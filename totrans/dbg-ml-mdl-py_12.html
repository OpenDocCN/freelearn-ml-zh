<html><head></head><body>
<div id="_idContainer121">
<h1 class="chapter-number" id="_idParaDest-199"><a id="_idTextAnchor320"/><span class="koboSpan" id="kobo.1.1">12</span></h1>
<h1 id="_idParaDest-200"><a id="_idTextAnchor321"/><span class="koboSpan" id="kobo.2.1">Going Beyond ML Debugging with Deep Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">The most recent advancements in machine learning have been achieved through deep learning modeling. </span><span class="koboSpan" id="kobo.3.2">In this chapter, we will introduce deep learning and PyTorch as a framework to use for deep learning modeling. </span><span class="koboSpan" id="kobo.3.3">As the focus of this book is not on introducing different machine learning and deep learning algorithms in detail, we will focus on opportunities that deep learning provides for you to develop high-performance models, or use available ones, that can be built on top of the techniques reviewed in this chapter and the </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">next two.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Introduction to artificial </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">neural networks</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Frameworks for neural </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">network modeling</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.11.1">By the end of this chapter, you will have learned about some theoretical aspects of deep learning focusing on fully connected neural networks. </span><span class="koboSpan" id="kobo.11.2">You will have also practiced with PyTorch, a widely used deep </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">learning framework.</span></span></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor322"/><span class="koboSpan" id="kobo.13.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.14.1">The following requirements should be considered for this chapter as they will help you better understand the concepts, use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.16.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.18.1">torch</span></strong><span class="koboSpan" id="kobo.19.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">2.0.0</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.21.1">torchvision</span></strong><span class="koboSpan" id="kobo.22.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">0.15.1</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.24.1">You will also require basic knowledge of the difference between different types of machine learning models, such as classification, regression, </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">and clustering</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.26.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12"><span class="No-Break"><span class="koboSpan" id="kobo.28.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter12</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.29.1">.</span></span></p>
<h1 id="_idParaDest-202"><a id="_idTextAnchor323"/><span class="koboSpan" id="kobo.30.1">Introduction to artificial neural networks</span></h1>
<p><span class="koboSpan" id="kobo.31.1">Our natural networks </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.32.1">of neurons work as decision-making systems with information processing units called neurons that help us with, for example, recognizing the faces of our friends. </span><strong class="bold"><span class="koboSpan" id="kobo.33.1">Artificial neural networks</span></strong><span class="koboSpan" id="kobo.34.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.35.1">ANNs</span></strong><span class="koboSpan" id="kobo.36.1">) work similarly. </span><span class="koboSpan" id="kobo.36.2">Dissimilar to having a giant network of neurons, as in our bodies, that take care of all decision-making, active or reactive, ANNs are designed to be problem-specific. </span><span class="koboSpan" id="kobo.36.3">For example, we have ANNs for image classification, credit risk estimation, object detection, and more. </span><span class="koboSpan" id="kobo.36.4">We will use neural networks instead of ANNs for simplicity in </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">First, we want to </span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.39.1">focus on </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">fully connected neural networks</span></strong><span class="koboSpan" id="kobo.41.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.42.1">FCNNs</span></strong><span class="koboSpan" id="kobo.43.1">), which work on tabular data (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.44.1">Figure 12</span></em></span><em class="italic"><span class="koboSpan" id="kobo.45.1">.1</span></em><span class="koboSpan" id="kobo.46.1">). </span><span class="koboSpan" id="kobo.46.2">FCNNs</span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.47.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">multi-layer perceptrons</span></strong><span class="koboSpan" id="kobo.49.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.50.1">MLPs</span></strong><span class="koboSpan" id="kobo.51.1">) are used interchangeably in many resources. </span><span class="koboSpan" id="kobo.51.2">To be able to better compare different types of neural networks, we will use FCNNs instead of MLPs in </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">this book:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<span class="koboSpan" id="kobo.53.1"><img alt="Figure 12.1 – Schematic illustration of an FCNN and an individual neuron" src="image/B16369_12_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.54.1">Figure 12.1 – Schematic illustration of an FCNN and an individual neuron</span></p>
<p><span class="koboSpan" id="kobo.55.1">FCNNs for supervised learning have one input, one output, and one or multiple hidden (middle) layers. </span><span class="koboSpan" id="kobo.55.2">A neural network with more than three layers, inclusive of the input and the output layers in supervised models, is called a deep neural network, and deep learning refers to modeling with such networks (Hinton and </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">Salakhutdinov, 2006)</span><a id="_idTextAnchor324"/><span class="koboSpan" id="kobo.57.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.58.1">The input layer is nothing other than the features of data points used for modeling. </span><span class="koboSpan" id="kobo.58.2">The number of neurons</span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.59.1"> in the output layer is also determined based on the problem at hand. </span><span class="koboSpan" id="kobo.59.2">For example, in the case of binary classification, two neurons in the output layer represent two classes. </span><span class="koboSpan" id="kobo.59.3">The number and size of hidden layers are among the hyperparameters of an FCNN and can be optimized to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">FCNN performance.</span></span></p>
<p><span class="koboSpan" id="kobo.61.1">Each neuron in an FCNN receives a weighted sum of output values from neurons in the previous layer, applies a linear or nonlinear transformation to the received sum of values, and then outputs the resulting value to other neurons of the next layer. </span><span class="koboSpan" id="kobo.61.2">The weights used in the input value calculation of each neuron are the learned weights (parameters) in the training process. </span><span class="koboSpan" id="kobo.61.3">The nonlinear transformations are applied through predetermined activation functions (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.62.1">Figure 12</span></em></span><em class="italic"><span class="koboSpan" id="kobo.63.1">.2</span></em><span class="koboSpan" id="kobo.64.1">). </span><span class="koboSpan" id="kobo.64.2">FCNNs are known for coming up with complicated nonlinear relationships between input feature values and outputs, which makes them flexible in figuring out (maybe) different kinds of relationships between inputs and outputs. </span><span class="koboSpan" id="kobo.64.3">In FCNNs, activation functions that are applied to information that’s been received in neurons are responsible for that complexity </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">or flexibility:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.66.1"><img alt="Figure 12.2 – Widely used activation functions in neural network modeling" src="image/B16369_12_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.67.1">Figure 12.2 – Widely used activation functions in neural network modeling</span></p>
<p><span class="koboSpan" id="kobo.68.1">Each of these activation functions, such </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.69.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">rectified linear unit</span></strong><span class="koboSpan" id="kobo.71.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.72.1">ReLU</span></strong><span class="koboSpan" id="kobo.73.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">exponential linear unit</span></strong><span class="koboSpan" id="kobo.75.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.76.1">ELU</span></strong><span class="koboSpan" id="kobo.77.1">), transform</span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.78.1"> the values in a specific way, which makes them suitable for different layers and provides flexibility in neural network modeling. </span><span class="koboSpan" id="kobo.78.2">For </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.79.1">example, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.80.1">sigmoid</span></strong><span class="koboSpan" id="kobo.81.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">softmax</span></strong><span class="koboSpan" id="kobo.83.1"> functions are commonly used in output layers to transform the scores of the output neurons into values between zero and one for classification models; these are known as probabilities of predictions. </span><span class="koboSpan" id="kobo.83.2">There are also</span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.84.1"> other activation functions such as </span><strong class="bold"><span class="koboSpan" id="kobo.85.1">Gaussian error linear unit</span></strong><span class="koboSpan" id="kobo.86.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.87.1">GELU</span></strong><span class="koboSpan" id="kobo.88.1">) (Hendrycks and Gimpel, 2016) that have been used in more recent models such as </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">generative pre-trained transformer</span></strong><span class="koboSpan" id="kobo.90.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.91.1">GPT</span></strong><span class="koboSpan" id="kobo.92.1">), which </span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.93.1">will be explained in the next chapter. </span><span class="koboSpan" id="kobo.93.2">Here is the formula </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">for GELU:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.95.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.96.1">         </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.97.1">                              </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.98.1">G</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.99.1">E</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.100.1">L</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.101.1">U</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.102.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.103.1">z</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.104.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.105.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.106.1">0.5</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.107.1">z</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.108.1">(</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.109.1">1</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.110.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.111.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.112.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.113.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.114.1">h</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.115.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.116.1">√</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.117.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.118.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.119.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.120.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.121.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.122.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.123.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.124.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.125.1">π</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.126.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.127.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.128.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">z</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.130.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.131.1">0.044715</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.132.1">z</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.133.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.134.1">3</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.135.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.136.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.137.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.138.1">Supervised learning has two main processes: predicting outputs and learning from the incorrectness or correctness of predictions. </span><span class="koboSpan" id="kobo.138.2">In FCNNs, predictions happen in forward propagation. </span><span class="koboSpan" id="kobo.138.3">The weights of the FCNNs between the input and first hidden layer are used to calculate the input values of the neurons of the first hidden layer and similarly for other layers in the FCNN (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.139.1">Figure 12</span></em></span><em class="italic"><span class="koboSpan" id="kobo.140.1">.3</span></em><span class="koboSpan" id="kobo.141.1">). </span><span class="koboSpan" id="kobo.141.2">Going from input to output is called forward propagation or forward pass, which generates the output values (predictions) for each data point. </span><span class="koboSpan" id="kobo.141.3">Then, in the backward propagation (backpropagation) or backward pass, FCNN uses the predicted outputs and their differences with actual outputs to adjust its weights, resulting </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.142.1">in </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">better pred</span><a id="_idTextAnchor325"/><span class="koboSpan" id="kobo.144.1">ictions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<span class="koboSpan" id="kobo.145.1"><img alt="Figure 12.3 – Schematic illustration of forward propagation and backpropagation for output prediction and parameter update, respectively" src="image/B16369_12_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.146.1">Figure 12.3 – Schematic illustration of forward propagation and backpropagation for output prediction and parameter update, respectively</span></p>
<p><span class="koboSpan" id="kobo.147.1">The parameters of a neural network get determined in the training process using an optimization</span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.148.1"> algorithm. </span><span class="koboSpan" id="kobo.148.2">Now, we will review some widely used optimization algorithms in neural </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">network se</span><a id="_idTextAnchor326"/><span class="koboSpan" id="kobo.150.1">ttings.</span></span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor327"/><span class="koboSpan" id="kobo.151.1">Optimization algorithms</span></h2>
<p><span class="koboSpan" id="kobo.152.1">Optimization</span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.153.1"> algorithms </span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.154.1">work behind the scenes, trying to minimize the loss function to identify the optimal parameters when you train a machine learning model. </span><span class="koboSpan" id="kobo.154.2">At each step in the training process, an optimization algorithm decides how to update each of the weights or parameters in a neural network, or other machine learning models. </span><span class="koboSpan" id="kobo.154.3">Most optimization algorithms rely on the gradient vector of the cost function to update the weights. </span><span class="koboSpan" id="kobo.154.4">The main difference is how the gradient vector is used and what data points are used to </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">calculate it.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">In gradient descent, all the data points are used to calculate the gradient of the cost function; then, the weights of the model get updated in the direction of maximum decrease of cost. </span><span class="koboSpan" id="kobo.156.2">Despite the effectiveness of this method for small datasets, it can become computationally expensive and unsuitable for large datasets as for every iteration of learning, the cost needs to be calculated for all the data points simultaneously. </span><span class="koboSpan" id="kobo.156.3">The alternative</span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.157.1"> approach is </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">stochastic gradient descent</span></strong><span class="koboSpan" id="kobo.159.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.160.1">SGD</span></strong><span class="koboSpan" id="kobo.161.1">); instead of all data points, one data point gets selected in each iteration to calculate the cost and update the weights. </span><span class="koboSpan" id="kobo.161.2">But using one data point at a time causes a highly oscillating behavior in updating weights. </span><span class="koboSpan" id="kobo.161.3">Instead, we can use mini-batch gradient descent, which is commonly called SGD in tutorials and tools, in which instead of all data points or only one</span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.162.1"> in each iteration, it will use a batch of data points to update the weights. </span><span class="koboSpan" id="kobo.162.2">The</span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.163.1"> mathematics behind these three approaches is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.164.1">Figure 12</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.165.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<span class="koboSpan" id="kobo.167.1"><img alt="Figure 12.4 – Gradient descent, stochastic gradient descent, and mini-batch gradient descent optimization algorithms" src="image/B16369_12_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.168.1">Figure 12.4 – Gradient descent, stochastic gradient descent, and mini-batch gradient descent optimization algorithms</span></p>
<p><span class="koboSpan" id="kobo.169.1">Other optimization algorithms have been suggested in recent years to improve the performance of neural network models across a variety of applications, such as the Adam optimizer (Kingma and Ba, 2014). </span><span class="koboSpan" id="kobo.169.2">One of the intuitions behind this approach is to avoid diminishing gradients in the optimization process. </span><span class="koboSpan" id="kobo.169.3">Getting further into the details of different optimization algorithms is beyond the scope of </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">In neural network modeling, there are two important terms that you need to know the definition of: </span><em class="italic"><span class="koboSpan" id="kobo.172.1">epoch</span></em><span class="koboSpan" id="kobo.173.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.174.1">batch size</span></em><span class="koboSpan" id="kobo.175.1">. </span><span class="koboSpan" id="kobo.175.2">When training a neural network model using different frameworks, which we will review in the next section, you need to specify the </span><em class="italic"><span class="koboSpan" id="kobo.176.1">batch size</span></em><span class="koboSpan" id="kobo.177.1"> and the number of </span><em class="italic"><span class="koboSpan" id="kobo.178.1">epochs</span></em><span class="koboSpan" id="kobo.179.1">. </span><span class="koboSpan" id="kobo.179.2">In each iteration of optimization, a subset of data points, or a mini-batch as in mini-batch gradient descent (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.180.1">Figure 12</span></em></span><em class="italic"><span class="koboSpan" id="kobo.181.1">.4</span></em><span class="koboSpan" id="kobo.182.1">), gets used to calculate loss; then, the parameters of the model get updated using backpropagation. </span><span class="koboSpan" id="kobo.182.2">This process gets repeated to cover all the data points in the training data. </span><span class="koboSpan" id="kobo.182.3">Epoch is a term we use to specify how many times all the training data is used during the optimization process. </span><span class="koboSpan" id="kobo.182.4">For example, specifying an epoch of 5 means that the model gets trained until all the data points in the training process are used five times in the </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">optimization process.</span></span></p>
<p><span class="koboSpan" id="kobo.184.1">Now that</span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.185.1"> you </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.186.1">know the basics of neural network modeling, we are ready to introduce frameworks for neural </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">netw</span><a id="_idTextAnchor328"/><span class="koboSpan" id="kobo.188.1">ork modeling.</span></span></p>
<h1 id="_idParaDest-204"><a id="_idTextAnchor329"/><span class="koboSpan" id="kobo.189.1">Frameworks for neural network modeling</span></h1>
<p><span class="koboSpan" id="kobo.190.1">Multiple </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.191.1">frameworks have been used for neural </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">network modeling:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.193.1">PyTorch (</span></span><a href="https://pytorch.org/"><span class="No-Break"><span class="koboSpan" id="kobo.194.1">https://pytorch.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.195.1">)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.196.1">TensorFlow (</span></span><a href="https://www.tensorflow.org/learn"><span class="No-Break"><span class="koboSpan" id="kobo.197.1">https://www.tensorflow.org/learn</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.198.1">)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.199.1">Keras (</span></span><a href="https://keras.io/"><span class="No-Break"><span class="koboSpan" id="kobo.200.1">https://keras.io/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.201.1">)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.202.1">Caffe (</span></span><a href="https://caffe.berkeleyvision.org/"><span class="No-Break"><span class="koboSpan" id="kobo.203.1">https://caffe.berkeleyvision.org/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.204.1">)</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.205.1">MXNet (</span></span><a href="https://mxnet.apache.org/versions/1.9.1/"><span class="No-Break"><span class="koboSpan" id="kobo.206.1">https://mxnet.apache.org/versions/1.9.1/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.207.1">)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.208.1">In this book, we will focus on PyTorch in practicing deep learning, but the concepts we’ll introduce are independent of the framework you use in </span><a id="_idTextAnchor330"/><span class="No-Break"><span class="koboSpan" id="kobo.209.1">your projects.</span></span></p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor331"/><span class="koboSpan" id="kobo.210.1">PyTorch for deep learning modeling</span></h2>
<p><span class="koboSpan" id="kobo.211.1">PyTorch is an </span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.212.1">open source deep learning framework, based</span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.213.1"> on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">Torch</span></strong><span class="koboSpan" id="kobo.215.1"> library, developed by Meta AI. </span><span class="koboSpan" id="kobo.215.2">You can easily integrate PyTorch with Python’s scientific computing libraries in your deep learning projects. </span><span class="koboSpan" id="kobo.215.3">Here, we will practice using PyTorch by looking at a simple example of building an FCNN model using the MNIST digit dataset. </span><span class="koboSpan" id="kobo.215.4">It is a commonly used example and the objective is solely to understand how to train and test a deep learning model using PyTorch if you don’t have experience </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">with that.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">First, we will import the required libraries and load the dataset for training </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">and testing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.219.1">
import torchimport torchvision
import torchvision.transforms as transforms
torch.manual_seed(10)
# Device configuration
device = torch.device(
    'cuda' if torch.cuda.is_available() else 'cpu')
# MNIST dataset
batch_size = 100
train_dataset = torchvision.datasets.MNIST(
    root='../../data',train=True,
    transform=transforms.ToTensor(),download=True)
test_dataset = torchvision.datasets.MNIST(
    root='../../data', train=False,
    transform=transforms.ToTensor())
# Data loader
train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,batch_size=batch_size,
    shuffle=True)
test_loader = torch.utils.data.DataLoader(
    dataset=test_dataset,  batch_size=batch_size,
    shuffle=False)</span></pre>
<p><span class="koboSpan" id="kobo.220.1">Next, we will determine the hyperparameters of the model and its </span><strong class="source-inline"><span class="koboSpan" id="kobo.221.1">input_size</span></strong><span class="koboSpan" id="kobo.222.1">, which is the number of neurons in the input layer; this is the same as the number of features in our data. </span><span class="koboSpan" id="kobo.222.2">In this example, it is equal to the number of pixels in each image as we are considering each pixel as one feature to build an </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">FCNN model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.224.1">
input_size = 784# size of hidden layer
hidden_size = 256
# number of classes
num_classes = 10
# number of epochs
num_epochs = 10
# learning rate for the optimization process
learning_rate = 0.001</span></pre>
<p><span class="koboSpan" id="kobo.225.1">Then, we </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.226.1">will</span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.227.1"> import </span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">torch.nn</span></strong><span class="koboSpan" id="kobo.229.1">, from which we can add linear neural network layers for our FCNN model and write a class to determine the architecture of our network, which is a network with one hidden layer whose size is 256 (with </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">256 neurons):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.231.1">
import torch.nn as nnclass NeuralNet(nn.Module):
    def __init__(self, input_size, hidden_size,
        num_classes):
        super(NeuralNet, self).__init__()
        Self.fc_layer_1 = nn.Linear(input_size, hidden_size)
        self.fc_layer_2 = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        out = self.fc_layer_1(x)
        out = nn.ReLU()(out)
        out = self.fc_layer_2(out)
        return out
model = NeuralNet(input_size, hidden_size,
    num_classes).to(device)</span></pre>
<p><span class="koboSpan" id="kobo.232.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">torch.nn.Linear()</span></strong><span class="koboSpan" id="kobo.234.1"> class adds a linear layer and has two input arguments: the number of</span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.235.1"> neurons in</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.236.1"> the current and next layer, respectively. </span><span class="koboSpan" id="kobo.236.2">For the first, </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">nn.Linear()</span></strong><span class="koboSpan" id="kobo.238.1">, the first argument has to be equal to the number of features, while the second argument of the last </span><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">nn.Linear()</span></strong><span class="koboSpan" id="kobo.240.1"> input argument in the network initialization class needs to be equal to the number of classes in </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">Now, we must define our cross-entropy loss function and our optimizer object using the Adam optimizer </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">torch.optim()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.246.1">
criterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(),
    lr=learning_rate)</span></pre>
<p><span class="koboSpan" id="kobo.247.1">We are now ready to train our model. </span><span class="koboSpan" id="kobo.247.2">As you can see in the following code block, we have a loop over epochs and another internal loop over each batch. </span><span class="koboSpan" id="kobo.247.3">Within the internal loop, we have three important steps that are common across most supervised models that </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">use PyTorch:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.249.1">Get the output of the model for the data points within </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">the batch.</span></span></li>
<li><span class="koboSpan" id="kobo.251.1">Calculate the loss using the true labels and the predicted output for the data points of </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">that batch.</span></span></li>
<li><span class="koboSpan" id="kobo.253.1">Backpropagate and update the parameters of </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">the model.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.255.1">Next, we must</span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.256.1"> train </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.257.1">the model on the MNIST </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">training set:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.259.1">
total_step = len(train_loader)for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        images = images.reshape(-1, 28*28).to(device)
        labels = labels.to(device)
        # Forward pass to calculate output and loss
        outputs = model(images)
        loss = criterion(outputs, labels)
        # Backpropagation and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</span></pre>
<p><span class="koboSpan" id="kobo.260.1">At the end of epoch 10, we have a model with a loss of 0.0214 in the training set. </span><span class="koboSpan" id="kobo.260.2">Now, we can use the following code to calculate the accuracy of the model in the </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">test set:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.262.1">
with torch.no_grad():    correct = 0
    total = 0
    for images, labels in test_loader:
        images = images.reshape(-1, 28*28).to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    print('Accuracy of the network on the test images:
         {} %'.format(100 * correct / total))</span></pre>
<p><span class="koboSpan" id="kobo.263.1">This results in 98.4% for the model in the MNIST </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">test set.</span></span></p>
<p><span class="koboSpan" id="kobo.265.1">There are more than 10 different optimization algorithms, including the Adam optimization algorithm, available in PyTorch (</span><a href="https://pytorch.org/docs/stable/optim.html"><span class="koboSpan" id="kobo.266.1">https://pytorch.org/docs/stable/optim.html</span></a><span class="koboSpan" id="kobo.267.1">), which helps you in training your deep </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.269.1">Next, we will discuss </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.270.1">hyperparameter tuning, model </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.271.1">interpretability, and fairness in deep learning settings. </span><span class="koboSpan" id="kobo.271.2">We will also introduce PyTorch Lightning, which will help you in your</span><a id="_idTextAnchor332"/><span class="koboSpan" id="kobo.272.1"> deep </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">learning projects.</span></span></p>
<h3><span class="koboSpan" id="kobo.274.1">Hyperparameter tuning for deep learning</span></h3>
<p><span class="koboSpan" id="kobo.275.1">In deep learning </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.276.1">modeling, hyperparameters </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.277.1">are key factors in determining its performance. </span><span class="koboSpan" id="kobo.277.2">Here are some of the hyperparameters of FCNNs you can work with to improve the performance of your deep </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">learning models:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.279.1">Architecture</span></strong><span class="koboSpan" id="kobo.280.1">: The architecture</span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.281.1"> of an FCNN refers to the number of hidden layers and their sizes, or the number of neurons. </span><span class="koboSpan" id="kobo.281.2">More layers result in higher depth in a deep learning model and could result in more complex models. </span><span class="koboSpan" id="kobo.281.3">Although the depth of neural network models has been shown to improve performance on large datasets in many cases (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Szegedy et al., 2015; He et al., 2016), the majority of the success stories behind the positive effect of higher depth on performance are outside of FCNNs. </span><span class="koboSpan" id="kobo.281.4">But architecture is still an important hyperparameter that needs to be optimized to find a </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">high-performance model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.283.1">Activation functions</span></strong><span class="koboSpan" id="kobo.284.1">: Despite commonly used activation functions in each field and problem, you can still identify the best one for your problem. </span><span class="koboSpan" id="kobo.284.2">Remember that you don’t have to use the same function across all layers, although we usually stick </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">to one.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.286.1">Batch size</span></strong><span class="koboSpan" id="kobo.287.1">: Changing batch size changes both the performance and speed of convergence of your models. </span><span class="koboSpan" id="kobo.287.2">But usually, it doesn’t have a significant effect on performance, except in the steep part of the learning curve in the first </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">few epochs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.289.1">Learning rate</span></strong><span class="koboSpan" id="kobo.290.1">: The learning rate determines the speed of convergence. </span><span class="koboSpan" id="kobo.290.2">A higher learning rate </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.291.1">causes faster convergence but it might also cause oscillation around the local optimum point or even divergence. </span><span class="koboSpan" id="kobo.291.2">Algorithms such as the Adam optimizer</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.292.1"> control the diminishing </span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.293.1">convergence rate when we get closer to the local optima during the optimization process, but we can still play with the learning rate as a hyperparameter in deep </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">learning modeling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.295.1">Number of epochs</span></strong><span class="koboSpan" id="kobo.296.1">: Deep learning models have a steep learning curve for the first few epochs, depending on the learning rate and batch size, and then start plateauing on performance. </span><span class="koboSpan" id="kobo.296.2">Using enough epochs is important to make sure you get the best possible model out of </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">your training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.298.1">Regularization</span></strong><span class="koboSpan" id="kobo.299.1">: We talked about the importance of regulations in controlling overfitting and improving generalizability in </span><a href="B16369_05.xhtml#_idTextAnchor183"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.300.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.301.1">, </span><em class="italic"><span class="koboSpan" id="kobo.302.1">Improving the Performance of Machine Learning Models</span></em><span class="koboSpan" id="kobo.303.1">, by preventing the model from heavily relying on individual neurons and potentially improving generalizability. </span><span class="koboSpan" id="kobo.303.2">For example, if dropout is set to 0.2, each neuron has a 20% chance of getting zero out </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">during training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.305.1">Weight decay</span></strong><span class="koboSpan" id="kobo.306.1">: This is a form of L2 regularization that adds a penalty to the weights of the</span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.307.1"> neural network. </span><span class="koboSpan" id="kobo.307.2">We introduced L2 regularization in </span><a href="B16369_05.xhtml#_idTextAnchor183"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.308.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.309.1">, </span><em class="italic"><span class="koboSpan" id="kobo.310.1">Improving the Performance of Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.311.1">Learning Models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.313.1">You can use different hyperparameter optimization tools such as Ray Tune alongside PyTorch to train your deep learning models and optimize their hyperparameters. </span><span class="koboSpan" id="kobo.313.2">You can read more about it in this tutorial available on the PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">website: </span></span><a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html"><span class="No-Break"><span class="koboSpan" id="kobo.315.1">https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.316.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">In addition to </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.318.1">hyperparameter tuning, PyTorch</span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.319.1"> has different functionalities and associated libraries for tasks such as mod</span><a id="_idTextAnchor333"/><span class="koboSpan" id="kobo.320.1">el interpretability </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">and fairness.</span></span></p>
<h3><span class="koboSpan" id="kobo.322.1">Model interpretability in PyTorch</span></h3>
<p><span class="koboSpan" id="kobo.323.1">We introduced </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.324.1">multiple explainability techniques and </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.325.1">libraries in </span><a href="B16369_06.xhtml#_idTextAnchor201"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.326.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.327.1">, </span><em class="italic"><span class="koboSpan" id="kobo.328.1">Interpretability and Explainability in Machine Learning Modeling</span></em><span class="koboSpan" id="kobo.329.1">, that can help you in explaining complex machine learning and deep learning models. </span><span class="koboSpan" id="kobo.329.2">Captum AI (</span><a href="https://captum.ai/"><span class="koboSpan" id="kobo.330.1">https://captum.ai/</span></a><span class="koboSpan" id="kobo.331.1">) is another open source model interpretability library</span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.332.1"> developed by Meta AI for deep learning projects using PyTorch. </span><span class="koboSpan" id="kobo.332.2">You can easily integrate Captum into your existing or future PyTorch-based machine learning pipelines. </span><span class="koboSpan" id="kobo.332.3">You can benefit from different explainability and interpretability techniques such as integrated gradients, GradientSHAP, DeepLIF</span><a id="_idTextAnchor334"/><span class="koboSpan" id="kobo.333.1">T, and saliency maps </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">through Captum.</span></span></p>
<h3><span class="koboSpan" id="kobo.335.1">Fairness in deep learning models developed by PyTorch</span></h3>
<p><span class="koboSpan" id="kobo.336.1">We discussed the</span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.337.1"> importance</span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.338.1"> of fairness and introduced different notions, statistical measures, and techniques to help you in assessing and eliminating bias in your models as part of </span><a href="B16369_07.xhtml#_idTextAnchor218"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.339.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.340.1">, </span><em class="italic"><span class="koboSpan" id="kobo.341.1">Decreasing Bias and Achieving Fairness</span></em><span class="koboSpan" id="kobo.342.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.343.1">FairTorch</span></strong><span class="koboSpan" id="kobo.344.1"> (</span><a href="https://github.com/wbawakate/fairtorch"><span class="koboSpan" id="kobo.345.1">https://github.com/wbawakate/fairtorch</span></a><span class="koboSpan" id="kobo.346.1">) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">inFairness</span></strong><span class="koboSpan" id="kobo.348.1"> (</span><a href="https://github.com/IBM/inFairness"><span class="koboSpan" id="kobo.349.1">https://github.com/IBM/inFairness</span></a><span class="koboSpan" id="kobo.350.1">) are two other libraries you can use for fairness and bias assessment for your deep learning modeling using PyTorch. </span><span class="koboSpan" id="kobo.350.2">You can benefit from </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">inFairness</span></strong><span class="koboSpan" id="kobo.352.1"> in auditing, training, and post-processing your models for individual fairness. </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">Fairtorch</span></strong><span class="koboSpan" id="kobo.354.1"> also provides you with tools to mitigate bias in classification and regression, though</span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.355.1"> this</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.356.1"> is cur</span><a id="_idTextAnchor335"/><span class="koboSpan" id="kobo.357.1">rently limited to </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">binary classification.</span></span></p>
<h3><span class="koboSpan" id="kobo.359.1">PyTorch Lightning</span></h3>
<p><span class="koboSpan" id="kobo.360.1">PyTorch Lightning is an</span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.361.1"> open source, high-level framework that simplifies the process of developing and training deep learning models using PyTorch for you. </span><span class="koboSpan" id="kobo.361.2">Here are some of the features of </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">PyTorch Lightning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.363.1">Structured code</span></strong><span class="koboSpan" id="kobo.364.1">: PyTorch Lightning</span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.365.1"> organizes code into a Lightning Module that helps you in separating the model architecture, data handling, and training logic, making the code more modular and easier </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">to maintain</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.367.1">Training loop abstraction</span></strong><span class="koboSpan" id="kobo.368.1">: You can avoid repetitive code for the training, validation, and testing loops using </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">PyTorch Lightning</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.370.1">Distributed training</span></strong><span class="koboSpan" id="kobo.371.1">: PyTorch Lightning simplifies the process of scaling deep learning models across multiple GPUs </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">or nodes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.373.1">Experiment tracking and logging</span></strong><span class="koboSpan" id="kobo.374.1">: PyTorch Lightning integrates with experiment tracking and logging tools such as MLflow and Weights &amp; Biases, which make monitoring your deep learning model training easier </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">for you</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.376.1">Automatic optimization</span></strong><span class="koboSpan" id="kobo.377.1">: PyTorch Lightning automatically handles the optimization process, manages optimizers and learning rate schedulers, and makes it easier to switch between different </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">optimization algorithms</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.379.1">Despite all these factors, there is more to deep learning modeling tha</span><a id="_idTextAnchor336"/><span class="koboSpan" id="kobo.380.1">n FCNNs, as we’ll see in the </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">next chapter.</span></span></p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor337"/><span class="koboSpan" id="kobo.382.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.383.1">In this chapter, you learned about deep learning modeling with FCNNs. </span><span class="koboSpan" id="kobo.383.2">We practiced using PyTorch with a simple deep learning model to help you start performing deep learning modeling using PyTorch if you haven’t had that experience already. </span><span class="koboSpan" id="kobo.383.3">You also learned about the important hyperparameters of FCNNs, tools for model interpretability and fairness that you can use in deep learning settings, and PyTorch Lightning as an open source high-level framework to simplify deep learning modeling for you. </span><span class="koboSpan" id="kobo.383.4">You are now ready to learn more about PyTorch, PyTorch Lightning, and deep learning and start benefitting from them in </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">your problems.</span></span></p>
<p><span class="koboSpan" id="kobo.385.1">In the next chapter, you will learn about other more advanced types of deep learning models, including the convolutional neural network, transforme</span><a id="_idTextAnchor338"/><span class="koboSpan" id="kobo.386.1">r, and graph convolutional </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">network models.</span></span></p>
<h1 id="_idParaDest-207"><a id="_idTextAnchor339"/><span class="koboSpan" id="kobo.388.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.389.1">Do the parameters of a neural network model get updated </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">in backpropagation?</span></span></li>
<li><span class="koboSpan" id="kobo.391.1">What is the difference between stochastic and mini-batch </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">gradient descent?</span></span></li>
<li><span class="koboSpan" id="kobo.393.1">Can you explain the difference between a batch and </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">an epoch?</span></span></li>
<li><span class="koboSpan" id="kobo.395.1">Can you provide an example of where you need to use the sigmoid and softma</span><a id="_idTextAnchor340"/><span class="koboSpan" id="kobo.396.1">x functions in your neural </span><span class="No-Break"><span class="koboSpan" id="kobo.397.1">network models?</span></span></li>
</ol>
<h1 id="_idParaDest-208"><a id="_idTextAnchor341"/><span class="koboSpan" id="kobo.398.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.399.1">LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. </span><em class="italic"><span class="koboSpan" id="kobo.400.1">Deep learning</span></em><span class="koboSpan" id="kobo.401.1">. </span><span class="koboSpan" id="kobo.401.2">nature 521.7553 (</span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">2015): 436-444.</span></span></li>
<li><span class="koboSpan" id="kobo.403.1">Hinton, G. </span><span class="koboSpan" id="kobo.403.2">E., &amp; Salakhutdinov, R. </span><span class="koboSpan" id="kobo.403.3">R. </span><span class="koboSpan" id="kobo.403.4">(2006). </span><em class="italic"><span class="koboSpan" id="kobo.404.1">Reducing the Dimensionality of Data with Neural Networks</span></em><span class="koboSpan" id="kobo.405.1">. </span><span class="koboSpan" id="kobo.405.2">Science, </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">313(5786), 504-507.</span></span></li>
<li><span class="koboSpan" id="kobo.407.1">Abiodun, Oludare Isaac, et al. </span><em class="italic"><span class="koboSpan" id="kobo.408.1">State-of-the-art in artificial neural network applications: A survey</span></em><span class="koboSpan" id="kobo.409.1">. </span><span class="koboSpan" id="kobo.409.2">Heliyon 4.11 (</span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">2018): e00938.</span></span></li>
<li><span class="koboSpan" id="kobo.411.1">Hendrycks, D., &amp; Gimpel, K. </span><span class="koboSpan" id="kobo.411.2">(2016). </span><em class="italic"><span class="koboSpan" id="kobo.412.1">Gaussian Error Linear Units (GELUs)</span></em><span class="koboSpan" id="kobo.413.1">. </span><span class="koboSpan" id="kobo.413.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">preprint arXiv:1606.08415.</span></span></li>
<li><span class="koboSpan" id="kobo.415.1">Kingma, D. </span><span class="koboSpan" id="kobo.415.2">P., &amp; Ba, J. </span><span class="koboSpan" id="kobo.415.3">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.416.1">Adam: A Method for Stochastic Optimization</span></em><span class="koboSpan" id="kobo.417.1">. </span><span class="koboSpan" id="kobo.417.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">preprint arXiv:1412.6980.</span></span></li>
<li><span class="koboSpan" id="kobo.419.1">Kadra, Arlind, et al. </span><em class="italic"><span class="koboSpan" id="kobo.420.1">Well-tuned simple nets excel on tabular datasets</span></em><span class="koboSpan" id="kobo.421.1">. </span><span class="koboSpan" id="kobo.421.2">Advances in neural information processing systems 34 (</span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">2021): 23928-23941.</span></span></li>
<li><span class="koboSpan" id="kobo.423.1">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. </span><span class="koboSpan" id="kobo.423.2">E. </span><span class="koboSpan" id="kobo.423.3">(2012). </span><em class="italic"><span class="koboSpan" id="kobo.424.1">ImageNet classification with deep convolutional neural networks</span></em><span class="koboSpan" id="kobo.425.1">. </span><span class="koboSpan" id="kobo.425.2">In Advances in neural information processing systems (</span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">pp. </span><span class="koboSpan" id="kobo.426.2">1097-1105).</span></span></li>
<li><span class="koboSpan" id="kobo.427.1">Simonyan, K., &amp; Zisserman, A. </span><span class="koboSpan" id="kobo.427.2">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.428.1">Very deep convolutional networks for large-scale image recognition</span></em><span class="koboSpan" id="kobo.429.1">. </span><span class="koboSpan" id="kobo.429.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">preprint arXiv:1409.1556.</span></span></li>
<li><span class="koboSpan" id="kobo.431.1">He, K., Zhang, X., Ren, S., &amp; Sun, J. </span><span class="koboSpan" id="kobo.431.2">(2016). </span><em class="italic"><span class="koboSpan" id="kobo.432.1">Deep residual learning for image recognition</span></em><span class="koboSpan" id="kobo.433.1">. </span><span class="koboSpan" id="kobo.433.2">In Proceedings of the IEEE conference on computer vision and pattern recognition (</span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">pp. </span><span class="koboSpan" id="kobo.434.2">770-778).</span></span></li>
<li><span class="koboSpan" id="kobo.435.1">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., ... </span><span class="koboSpan" id="kobo.435.2">&amp; Rabinovich, A. </span><span class="koboSpan" id="kobo.435.3">(2015). </span><em class="italic"><span class="koboSpan" id="kobo.436.1">Going deeper with convolutions</span></em><span class="koboSpan" id="kobo.437.1">. </span><span class="koboSpan" id="kobo.437.2">In Proceedings of the IEEE conference on computer vision and pattern recognition (</span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">pp. </span><span class="koboSpan" id="kobo.438.2">1-9).</span></span></li>
</ul>
</div>
</body></html>