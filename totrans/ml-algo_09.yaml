- en: Clustering Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to introduce the basic concepts of clustering and
    the structure of k-means, a quite common algorithm that can solve many problems
    efficiently. However, its assumptions are very strong, in particular those concerning
    the convexity of the clusters, and this can lead to some limitations in its adoption.
    We're going to discuss its mathematical foundation and how it can be optimized.
    Moreover, we're going to analyze two alternatives that can be employed when k-means
    fails to cluster a dataset. These alternatives are DBSCAN, (which works by considering
    the differences of sample density), and spectral clustering, a very powerful approach
    based on the affinity among points.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a dataset of points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/166885de-3d63-48d5-a7a0-d08c5ec35c22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We assume that it''s possible to find a criterion (not unique) so that each
    sample can be associated with a specific group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8142ca3-03ec-4ebb-ac1a-42b0e66b3145.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Conventionally, each group is called a **cluster** and the process of finding
    the function *G* is called **clustering**. Right now, we are not imposing any
    restriction on the clusters; however, as our approach is unsupervised, there should
    be a similarity criterion to join some elements and separate other ones. Different
    clustering algorithms are based on alternative strategies to solve this problem,
    and can yield very different results. In the following figure, there''s an example
    of clustering based on four sets of bidimensional samples; the decision to assign
    a point to a cluster depends only on its features and sometimes on the position
    of a set of other points (neighborhood):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae488a1f-8aac-4104-94c1-81059dc3a61f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this book, we''re going to discuss **hard clustering** techniques, where
    each element must belong to a single cluster. The alternative approach, called
    **soft clustering** (or **fuzzy clustering**), is based on a membership score
    that defines how much the elements are "compatible" with each cluster. The generic
    clustering function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b957908-ca3d-4ae6-be64-a65745af825d.png)'
  prefs: []
  type: TYPE_IMG
- en: A vector *m[i]* represents the relative membership of *x[i]*, and it's often
    normalized as a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The k-means algorithm is based on the (strong) initial condition to decide
    the number of clusters through the assignment of k initial **centroids** or **means**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57f9c0a1-3a73-47ff-a35e-ee9e4fe2b787.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the distance between each sample and each centroid is computed and the
    sample is assigned to the cluster where the distance is minimum. This approach
    is often called **minimizing the inertia** of the clusters, which is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffa4f61c-5be4-48ea-87f1-52e27213501b.png)'
  prefs: []
  type: TYPE_IMG
- en: The process is iterative—once all the samples have been processed, a new set
    of centroids *K^((1))* is computed (now considering the actual elements belonging
    to the cluster), and all the distances are recomputed. The algorithm stops when
    the desired tolerance is reached, or in other words, when the centroids become
    stable and, therefore, the inertia is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this approach is quite sensitive to the initial conditions, and some
    methods have been studied to improve the convergence speed. One of them is called
    **k-means++** (Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V., and Sridhar
    G.R., *Robust Seed Selection Algorithm for K-Means Type Algorithms*, International
    Journal of Computer Science and Information Technology 3, no. 5, October 30, 2011),
    which selects the initial centroids so that they are statistically close to the
    final ones. The mathematical explanation is quite difficult; however, this method
    is the default choice for scikit-learn, and it's normally the best choice for
    any clustering problem solvable with this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a simple example with a dummy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We expect to have three clusters with bidimensional features and a partial overlap
    due to the standard deviation of each blob. In our example, we won't use the *Y*
    variable (which contains the expected cluster) because we want to generate only
    a set of locally coherent points to try our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resultant plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae60d470-3a18-4476-9072-f17138b8f586.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the problem is quite simple to solve, so we expect k-means to
    separate the three groups with minimum error in the region of *X* bounded between
    [-5, 0]. Keeping the default values, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Replotting the data using three different markers, it''s possible to verify
    how k-means successfully separated the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86c72a22-3ffd-4b06-ac81-6c4ff4ded9d3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the separation was very easy because k-means is based on Euclidean
    distance, which is radial, and therefore the clusters are expected to be convex.
    When this doesn't happen, the problem cannot be solved using this algorithm. Most
    of the time, even if the convexity is not fully guaranteed, k-means can produce
    good results, but there are several situations when the expected clustering is
    impossible and letting k-means find out the centroid can lead to completely wrong
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the case of concentric circles. scikit-learn provides a built-in
    function to generate such datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot of this dataset is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dba7d538-1ae1-491d-87c7-40988971e54c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We would like to have an internal cluster (corresponding to the samples depicted
    with triangular markers) and an external one (depicted by dots). However, such
    sets are not convex, and it''s impossible for k-means to separate them correctly
    (the means should be the same!). In fact, suppose we try to apply the algorithm
    to two clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the separation shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ce4374e-9ffc-40ae-86d2-250a23da572f.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, k-means converged on the two centroids in the middle of the two
    half-circles, and the resulting clustering is quite different from what we expected.
    Moreover, if the samples must be considered different according to the distance
    from the common center, this result will lead to completely wrong predictions.
    It's obvious that another method must be employed.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the optimal number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most common disadvantages of k-means is related to the choice of
    the optimal number of clusters. An excessively small value will determine large
    groupings that contain heterogeneous elements, while a large number leads to a
    scenario where it can be difficult to identify the differences among clusters.
    Therefore, we're going to discuss some methods that can be employed to determine
    the appropriate number of splits and to evaluate the corresponding performance.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the inertia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first method is based on the assumption that an appropriate number of clusters
    must produce a small inertia. However, this value reaches its minimum (0.0) when
    the number of clusters is equal to the number of samples; therefore, we can't
    look for the minimum, but for a value which is a trade-off between the inertia
    and the number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we have a dataset of 1,000 elements. We can compute and collect
    the inertias (scikit-learn stores these values in the instance variable `inertia_`)
    for a different number of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the values, we get the result shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09130468-38ab-4b44-a9ce-c7d700bb673d.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, there's a dramatic reduction between 2 and 3 and then the slope
    starts flattening. We want to find a value that, if reduced, leads to a great
    inertial increase and, if increased, produces a very small inertial reduction.
    Therefore, a good choice could be 4 or 5, while greater values are likely to produce
    unwanted intracluster splits (till the extreme situation where each point becomes
    a single cluster). This method is very simple, and can be employed as the first
    approach to determine a potential range. The next strategies are more complex,
    and can be used to find the final number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Silhouette score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The silhouette score is based on the principle of "maximum internal cohesion
    and maximum cluster separation". In other words, we would like to find the number
    of clusters that produce a subdivision of the dataset into dense blocks that are
    well separated from each other. In this way, every cluster will contain very similar
    elements and, selecting two elements belonging to different clusters, their distance
    should be greater than the maximum intracluster one.
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining a distance metric (Euclidean is normally a good choice), we
    can compute the average intracluster distance for each element:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52a32904-d907-4c36-9912-5d96f96b5af3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also define the average nearest-cluster distance (which corresponds
    to the lowest intercluster distance):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79c11243-250f-4c9c-b722-62686b2f9665.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The silhouette score for an element *x[i]* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33afd3c2-4b5b-44fb-8592-f83970dce3ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This value is bounded between -1 and 1, with the following interpretation:'
  prefs: []
  type: TYPE_NORMAL
- en: A value close to 1 is good (1 is the best condition) because it means that *a(x[i])
    << b(x[i])*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value close to 0 means that the difference between intra and inter cluster
    measures is almost null and therefore there's a cluster overlap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value close to -1 means that the sample has been assigned to a wrong cluster
    because *a(x[i]) >> b(x[i])*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'scikit-learn allows computing the average silhouette score to have an immediate
    overview for different numbers of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a1a75ed-58d7-457f-98f6-82a890aedc23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The best value is 3 (which is very close to 1.0), however, bearing in mind
    the previous method, 4 clusters provide a smaller inertia, together with a reasonable
    silhouette score. Therefore, a good choice could be 4 instead of 3\. However,
    the decision between 3 and 4 is not immediate and should be evaluated by also
    considering the nature of the dataset. The silhouette score indicates that there
    are 3 dense agglomerates, but the inertia diagram suggests that one of them (at
    least) can probably be split into two clusters. To have a better understanding
    of how the clustering is working, it''s also possible to graph the silhouette
    plots, showing the sorted score for each sample in all clusters. In the following
    snippet we create the plots for a number of clusters equal to 2, 3, 4, and 8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The silhouette coefficients for each sample are computed using the function
    `silhouette_values` (which are always bounded between -1 and 1). In this case,
    we are limiting the graph between -0.15 and 1 because there are no smaller values.
    However, it's important to check the whole range before restricting it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting graph is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94cbeff8-0256-4d3d-a26d-8945814fba7d.png)'
  prefs: []
  type: TYPE_IMG
- en: The width of each silhouette is proportional to the number of samples belonging
    to a specific cluster, and its shape is determined by the scores of each sample.
    An ideal plot should contain homogeneous and long silhouettes without peaks (they
    must be similar to trapezoids rather than triangles) because we expect to have
    a very low score variance among samples in the same cluster. For 2 clusters, the
    shapes are acceptable, but one cluster has an average score of 0.5, while the
    other has a value greater than 0.75; therefore, the first cluster has a low internal
    coherence. A completely different situation is shown in the plot corresponding
    to 8 clusters. All the silhouettes are triangular and their maximum score is slightly
    greater than 0.5\. It means that all the clusters are internally coherent, but
    the separation is unacceptable. With 3 clusters, the plot is almost perfect, except
    for the width of the second silhouette. Without further metrics, we could consider
    this number as the best choice (confirmed also by the average score), but the
    inertia is lower for a higher numbers of clusters. With 4 clusters, the plot is
    slightly worse, with two silhouettes having a maximum score of about 0.5\. This
    means that two clusters are perfectly coherent and separated, while the remaining
    two are rather coherent, but they aren't probably well separated. Right now, our
    choice should be made between 3 and 4\. The next methods will help us in banishing
    all doubts.
  prefs: []
  type: TYPE_NORMAL
- en: Calinski-Harabasz index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another method that is based on the concept of dense and well-separated clusters
    is the Calinski-Harabasz index. To build it, we need first to define the inter
    cluster dispersion. If we have k clusters with their relative centroids and the
    global centroid, the inter-cluster dispersion (BCD) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61698f0e-0e3d-430c-81e7-ca1ff5689493.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the above expression, *n[k]* is the number of elements belonging to the
    cluster k, *mu* (the Greek letter in the formula) is the global centroid, and
    *mu*[i] is the centroid of cluster *i*. The intracluster dispersion (WCD) is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/318e76f2-f257-43fa-b84c-873789494b11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Calinski-Harabasz index is defined as the ratio between *BCD(k)* and *WCD(k)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e3c3dde-0942-4d7d-a775-941a6683a9b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we look for a low intracluster dispersion (dense agglomerates) and a high
    intercluster dispersion (well-separated agglomerates), we need to find the number
    of clusters that maximizes this index. We can obtain a graph in a way similar
    to what we have already done for the silhouette score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bedfcc53-43b8-4b6c-be5c-4c35cf2ea966.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the highest value (5,500) is obtained with 3 clusters, while 4
    clusters yield a value slightly below 5,000\. Considering only this method, there's
    no doubt that the best choice is 3, even if 4 is still a reasonable value. Let's
    consider the last method, which evaluates the overall stability.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster instability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another approach is based on the concept of cluster instability defined in Von
    Luxburg U., *Cluster stability: an overview*, arXiv 1007:1075v1, 7 July 2010*. *Intuitively,
    we can say that a clustering approach is stable if perturbed versions of the same
    dataset produce very similar results. More formally, if we have a dataset *X*,
    we can define a set of *m* perturbed (or noisy) versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69ae3442-57db-4512-b20d-e6a63f7a1ff5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering a distance metric *d(C(X[1]), C(X[2]))* between two clusterings
    with the same number (k) of clusters, the instability is defined as the average
    distance between couples of clusterings of noisy versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fb57b45-0c9a-4d8d-9e6e-d10689759aee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our purposes, we need to find the value of k that minimizes *I(C)* (and
    therefore maximizes the stability). First of all, we need to produce some noisy
    versions of the dataset. Let''s suppose that *X* contains 1,000 bidimensional
    samples with a standard deviation of 10.0\. We can perturb *X* by adding a uniform
    random value (in the range [-2.0, 2.0]) with a probability of 0.25:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we are assuming to have four perturbed versions. As a metric, we adopt
    the Hamming distance, which is proportional (if normalized) to the number of output
    elements that disagree. At this point, we can compute the instabilities for various
    numbers of clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As the distances are symmetrical, we compute them only for the upper triangular
    part of the matrix. The result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f2485c9-b154-444a-8823-1a3adde2a9be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Excluding the configuration with 2 clusters, where the inertia is very high,
    we have a minimum for 3 clusters, a value that has already been confirmed by the
    three previous methods. Therefore, we can finally decide to set `n_clusters=3`, excluding
    the options of 4 or more clusters.  This method is very powerful, but it''s important
    to evaluate the stability with a reasonable number of noisy datasets, taking care
    not to excessively alter the original geometry. A good choice is to use Gaussian
    noise with a variance set to a fraction (for example 1/10) of the dataset variance.
    Alternative approaches are presented in Von Luxburg U., *Cluster stability: an
    overview*, arXiv 1007:1075v1, 7 July 2010.'
  prefs: []
  type: TYPE_NORMAL
- en: Even if we have presented these methods with k-means, they can be applied to
    any clustering algorithm to evaluate the performance and compare them.
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DBSCAN or **Density-Based Spatial Clustering of Applications with Noise** is
    a powerful algorithm that can easily solve non-convex problems where k-means fails.
    The idea is simple: A cluster is a high-density area (there are no restrictions
    on its shape) surrounded by a low-density one. This statement is generally true,
    and doesn''t need an initial declaration about the number of expected clusters.
    The procedure starts by analyzing a small area (formally, a point surrounded by
    a minimum number of other samples). If the density is enough, it is considered
    part of a cluster. At this point, the neighbors are taken into account. If they
    also have a high density, they are merged with the first area; otherwise, they
    determine a topological separation. When all the areas have been scanned, the
    clusters have also been determined because they are islands surrounded by empty
    space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn allows us to control this procedure with two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`eps`: Responsible for defining the maximum distance between two neighbors.
    Higher values will aggregate more points, while smaller ones will create more
    clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples`: This determines how many surrounding points are necessary to
    define an area (also known as the core-point).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s try DBSCAN with a very hard clustering problem, called half-moons. The
    dataset can be created using a built-in function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot of the dataset is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/252726e9-66ee-44b4-88cb-594b961906ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just to understand, k-means will cluster by finding the optimal convexity,
    and the result is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49b7dea9-457b-407f-bb37-a803af3e5a4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Of course, this separation is unacceptable, and there''s no way to improve
    the accuracy. Let''s try it with DBSCAN (with `eps` set to 0.1 and the default
    value of 5 for `min_samples`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In a different manner than other implementations, DBSCAN predicts the label
    during the training process, so we already have an array `Y` containing the cluster
    assigned to each sample. In the following figure, there''s a representation with
    two different markers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/281c2a7b-b947-49a7-b3c6-2fe0e88a43d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, the accuracy is very high and only three isolated points are
    misclassified (in this case, we know their class, so we can use this term even
    if it''s a clustering process). However, by performing a grid search, it''s easy
    to find the best values that optimize the clustering process. It''s important
    to tune up those parameters to avoid two common problems: few big clusters and
    many small ones. This problem can be easily avoided using the following method.'
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spectral clustering is a more sophisticated approach based on a symmetric affinity
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78106f0d-bdd9-47d8-9516-a352132c5fea.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, each element *a[ij]* represents a measure of affinity between two samples.
    The most diffused measures (also supported by scikit-learn) are radial basis function
    and nearest neighbors. However, any kernel can be used if it produces measures
    that have the same features of a distance (non-negative, symmetric, and increasing).
  prefs: []
  type: TYPE_NORMAL
- en: The Laplacian matrix is computed and a standard clustering algorithm is applied
    to a subset of eigenvectors (this element is strictly related to each single strategy).
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn implements the Shi-Malik algorithm (*Shi J., Malik J., Normalized
    Cuts and Image Segmentation, IEEE Transactions on Pattern Analysis and Machine
    Intelligence, Vol. 22, 08/2000*), also known as normalized-cuts, which partitions
    the samples into two sets (*G[1]* and *G[2]*, which are formally graphs where
    each point is a vertex and the edges are derived from the normalized Laplacian
    matrix) so that the weights corresponding to the points inside a cluster are quite
    higher than the one belonging to the cut. A complete mathematical explanation
    is beyond the scope of this book; however, in *Von Luxburg U., A Tutorial on Spectral
    Clustering, 2007*, you can read a full explanation of many alternative spectral
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the previous half-moon example. In this case, the affinity
    (just like for DBSCAN) should be based on the nearest neighbors function; however,
    it''s useful to compare different kernels. In the first experiment, we use an
    RBF kernel with different values for the `gamma` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this algorithm, we need to specify how many clusters we want, so we set
    the value to 2\. The resulting plots are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d730f90-a7b3-40fa-8363-db26464084b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, when the scaling factor gamma is increased the separation becomes
    more accurate; however, considering the dataset, using the nearest neighbors kernel
    is not necessary in any search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54637c5e-b476-439e-b043-9aefef953a45.png)'
  prefs: []
  type: TYPE_IMG
- en: As for many other kernel-based methods, spectral clustering needs a previous
    analysis to detect which kernel can provide the best values for the affinity matrix.
    scikit-learn also allows us to define custom kernels for those tasks that cannot
    easily be solved using the standard ones.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation methods based on the ground truth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we present some evaluation methods that require the knowledge
    of the ground truth. This condition is not always easy to obtain because clustering
    is normally applied as an unsupervised method; however, in some cases, the training
    set has been manually (or automatically) labeled, and it's useful to evaluate
    a model before predicting the clusters of new samples.
  prefs: []
  type: TYPE_NORMAL
- en: Homogeneity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An important requirement for a clustering algorithm (given the ground truth)
    is that each cluster should contain only samples belonging to a single class.
    In [Chapter 2](c34bc79b-3efe-436d-a511-39c6fc5f0df1.xhtml), *Important Elements
    in Machine Learning*, we have defined the concepts of entropy *H(X)* and conditional
    entropy *H(X|Y)*, which measures the uncertainty of *X* given the knowledge of
    *Y*. Therefore, if the class set is denoted as *C* and the cluster set as *K*,
    *H(C|K)* is a measure of the uncertainty in determining the right class after
    having clustered the dataset. To have a homogeneity score, it''s necessary to
    normalize this value considering the initial entropy of the class set *H(C)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/741df5c8-8901-4a85-b75e-d9c9f3c50316.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In scikit-learn, there''s the built-in function `homogeneity_score()` that
    can be used to compute this value. For this and the next few examples, we assume
    that we have a labeled dataset *X* (with true labels *Y*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: A value of 0.8 means that there's a residual uncertainty of about 20% because
    one or more clusters contain some points belonging to a secondary class. As with
    the other methods shown in the previous section, it's possible to use the homogeneity
    score to determine the optimal number of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Completeness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A complementary requirement is that each sample belonging to a class is assigned
    to the same cluster. This measure can be determined using the conditional entropy
    *H(K|C)*, which is the uncertainty in determining the right cluster given the
    knowledge of the class. Like for the homogeneity score, we need to normalize this
    using the entropy *H(K)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cb96f4f-63b6-4ca5-9f76-491a9c46d069.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute this score (on the same dataset) using the function `completeness_score()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Also, in this case, the value is rather high, meaning that the majority of samples
    belonging to a class have been assigned to the same cluster. This value can be
    improved using a different number of clusters or changing the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusted rand index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The adjusted rand index measures the similarity between the original class
    partitioning (*Y*) and the clustering. Bearing in mind the same notation adopted
    in the previous scores, we can define:'
  prefs: []
  type: TYPE_NORMAL
- en: '**a**: The number of pairs of elements belonging to the same partition in the
    class set *C* and to the same partition in the clustering set *K*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b**: The number of pairs of elements belonging to different partitions in
    the class set *C* and to different partitions in the clustering set **K**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we total number of samples in the dataset is *n*, the rand index is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc2c20a7-dd7c-4db5-b25a-621ecb046789.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The *Corrected for Chance* version is the adjusted rand index, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/492e610a-b5ee-4914-94d8-8a4f3a96fc36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute the adjusted rand score using the function `adjusted_rand_score()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As the adjusted rand score is bounded between -1.0 and 1.0, with negative values
    representing a bad situation (the assignments are strongly uncorrelated), a score
    of 0.83 means that the clustering is quite similar to the ground truth. Also,
    in this case, it's possible to optimize this value by trying different numbers
    of clusters or clustering strategies.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Karteeka Pavan K., Allam Appa Rao, Dattatreya Rao A. V., and Sridhar G.R., *Robust
    seed selection algorithm for k-means type algorithms*, International Journal of
    Computer Science and Information Technology 3, no. 5 (October 30, 2011)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi J., Malik J., *Normalized Cuts and Image Segmentation*, *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, Vol. 22 (08/2000)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Von Luxburg U., *A Tutorial on Spectral Clustering*, 2007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Von Luxburg U., *Cluster stability: an overview*, arXiv 1007:1075v1, 7 July
    2010'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the k-means algorithm, which is based on the
    idea of defining (randomly or according to some criteria) k centroids that represent
    the clusters and optimize their position so that the sum of squared distances
    for every point in each cluster and the centroid is minimum. As the distance is
    a radial function, k-means assumes clusters to be convex and cannot solve problems
    where the shapes have deep concavities (like the half-moon problem).
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve such situations, we presented two alternatives. The first
    one is called DBSCAN and is a simple algorithm that analyzes the difference between
    points surrounded by other samples and boundary samples. In this way, it can easily
    determine high-density areas (which become clusters) and low-density spaces among
    them. There are no assumptions about the shape or the number of clusters, so it's
    necessary to tune up the other parameters so as to generate the right number of
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering is a family of algorithms based on a measure of affinity
    among samples. They use a classical method (such as k-means) on subspaces generated
    by the Laplacian of the affinity matrix. In this way, it's possible to exploit
    the power of many kernel functions to determine the affinity between points, which
    a simple distance cannot classify correctly. This kind of clustering is particularly
    efficient for image segmentation, but it can also be a good choice whenever the
    other methods fail to separate a dataset correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we're going to discuss another approach called hierarchical
    clustering. It allows us to segment data by splitting and merging clusters until
    a final configuration is reached.
  prefs: []
  type: TYPE_NORMAL
