- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI – Generative AI Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative artificial intelligence** (**GenAI)** is a rapidly evolving field
    of AI that enables machines to create new content, such as text, images, code,
    and music. GenAI models are trained on massive datasets of existing content, and
    they learn to identify patterns and relationships that underlie that content.
    Once trained, these models can be used to generate new content that is similar
    to the content they were trained on but that is also unique and creative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GenAI models can be used for a wide variety of applications, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text generation**: Generating text, such as news articles, blog posts, marketing
    copy, and creative content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chatbots**: Creating chatbots that can have natural conversations with users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image generation**: Generating images, such as product photos, marketing
    images, and artistic images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: Generating code, such as Python scripts, Java classes,
    and HTML templates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text embeddings**: Creating text embeddings that can be used for tasks such
    as text classification, text search, and **natural language** **inference** (**NLI**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: GenAI Fundamentals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI with Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval augmented generation approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we dive into the GenAI capabilities of Vertex AI, let’s first understand
    the fundamentals of GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GenAI is a subfield of AI that focuses on developing algorithms and models capable
    of generating new, original content, such as text, images, music, or code. This
    is in contrast to traditional AI models, which typically focus on understanding
    and classifying existing data.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of GenAI lies the concept of **large language models** (**LLMs**).
    LLMs are a type of **artificial neural network** (**ANN**) that has been trained
    on massive amounts of text data. This training allows LLMs to learn patterns and
    structures of human language, enabling them to generate text that is often indistinguishable
    from human-written text.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI versus traditional AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional AI models are typically based on **supervised learning** (**SL**),
    where the model is trained on a dataset of labeled examples. For example, a model
    for image classification might be trained on a dataset of images that have been
    labeled with the correct object category. Once trained, the model can then be
    used to classify new images.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI models, on the other hand, are typically based on **unsupervised learning**
    (**UL**), where the model is trained on a dataset of unlabeled data. The model
    is then tasked with learning underlying patterns and structures in the data. In
    the case of an LLM, this might involve learning the patterns of human language.
  prefs: []
  type: TYPE_NORMAL
- en: Types of GenAI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several different types of GenAI models, each with its own strengths
    and weaknesses. Some of the most common types include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoregressive models**: These models generate text one word at a time, predicting
    the next word based on the words that have already been generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational autoencoders (VAEs)**: These models learn a latent representation
    of the data, which can then be used to generate new samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative adversarial networks (GANs)**: These models consist of two competing
    NNs: a generator that generates new samples, and a discriminator that tries to
    distinguish between real and fake samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges of GenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite its promise, GenAI still faces several challenges, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias**: GenAI models can be biased, reflecting the biases in the data they
    are trained on. For example, when a model directly trained on raw content sourced
    from the internet is asked to write a story about a doctor and a nurse, the LLM
    is more likely to write a story about a male doctor and a female nurse, even if
    the prompt does not specify the gender of either character. This happens because
    the model has learned the same gender bias that is deeply embedded in our society
    and in the content shared online by members of our society.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation of fake/harmful content**: GenAI models can be used to generate
    harmful or offensive content. Issues of “deepfakes,” where AI is used to superimpose
    images of unsuspecting victims on images or videos, are already beginning to plague
    the internet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainability**: It can be difficult to explain how GenAI models make decisions,
    which can make it difficult to trust them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucinations**: The last, but also possibly the most harmful and most difficult-to-solve
    challenge of GenAI solutions and underlying LLMs is our inability to stop them
    from hallucinating. In the context of LLMs, “hallucinations” refer to instances
    where the model generates information that is not grounded in reality, is factually
    incorrect, or does not make sense given the context. This can be an issue when
    using AI for tasks that require high levels of accuracy, such as news generation
    or academic research, or in legal and medical applications. Here are a few examples
    of hallucinations we have encountered with foundation LLMs with no mitigation
    techniques applied:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`User: What` `is 2+2?`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`User: What? Are you giving me the wrong information? My friend told me that
    2+2` `is 5.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, obviously 2+2 is not 5 regardless of what my friend said. But we were easily
    able to convince the ‘all-knowing’ LLM of this being true.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`User: Give me a summary of a whitepaper titled "Why AI Systems Are Dangerous"
    along with` `its citation.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Based on the first look, it seems like the LLM model did an amazing job of providing
    the information about the paper. The only issue is that there is no such paper
    with the title “*Why AI Systems Are Dangerous*.” Our model didn’t just invent
    a paper based on our prompt; it also made up the arXiv identifier for the paper!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at some of the evaluation techniques commonly used to evaluate
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: LLM evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Evaluating LLMs is crucial for assessing their performance, identifying areas
    for improvement, and ensuring their responsible development and deployment. Several
    evaluation methods are employed to assess LLMs across various dimensions, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative metrics**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perplexity**: A measure of how well an LLM predicts the next word in a sequence.
    Lower perplexity indicates better predictive ability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BLEU score**: Evaluates the similarity between generated text and human-written
    reference text. Higher BLEU scores indicate greater similarity.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall-Oriented Understudy for Gisting Evaluation (ROUGE)**: A set of metrics
    that assess different aspects of text similarity, such as word overlap and recall.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human evaluation**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human evaluation plays a crucial role in assessing the quality and effectiveness
    of LLMs. While automated metrics can provide valuable insights into LLM performance,
    human judgment is essential for evaluating aspects that are difficult to quantify,
    such as fluency, coherence, relevance, and creativity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Benchmarking**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard benchmarks**: Utilizing established benchmarks, such as GLUE, SuperGLUE,
    or Big-bench, to compare LLM performance across various tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain-specific benchmarks**: When developing LLMs for domain-specific use
    cases, model developers should also work on developing domain-specific benchmarks
    to evaluate LLMs in specialized areas, such as medical diagnosis or legal research.
    Considering the effort that goes into developing such benchmarks, we expect to
    see an increased level of collaborative efforts by major teams within specific
    industries to publish industry standards for such benchmarks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: By employing a combination of these evaluation methods, researchers and developers
    can gain a comprehensive understanding of LLM capabilities, limitations, and potential
    biases, enabling them to refine and improve these powerful language models.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the features available in Vertex AI to develop GenAI solutions.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI with Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI provides a variety of tools and resources to help you get started
    with GenAI. You can use Vertex AI Model Garden to explore the different GenAI
    models that are available and to get help with setting up and using these models.
    You can also use Vertex AI GenAI Studio to experiment with GenAI models and to
    create your own prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding foundation models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Within the Vertex AI environment, you will encounter what are termed “foundation
    models.” These are essentially GenAI models defined based on the nature of the
    content they are engineered to create. The categories of models available within
    Vertex AI span across a diverse range, including, but not limited to, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text and chat**: For crafting textual content and facilitating chat interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Images**: To generate visual content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code**: Generating code, unit tests, and assisting developers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings**: Creating representations of text or images in a vector space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These foundation models are accessible via publisher endpoints unique to your
    specific Google Cloud project, thus eliminating the necessity for deploying the
    foundation models separately unless customization for particular applications
    is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vertex AI GenAI toolset comprises of two key components:'
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Garden (used for a lot more than just GenAI models)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertex AI GenAI Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look next at what these components offer.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Garden
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vertex AI Model Garden is a repository of pre-trained **machine learning** (**ML**)
    models that you can choose from based on your requirements. It contains both Google
    proprietary models and third-party open source models.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from being a one-stop shop for models addressing a variety of use cases,
    Model Garden also offers the same or similar Google models in varied sizes. This
    is particularly advantageous when your use case is relatively simple, and a smaller
    model is a lot more compute-efficient during inference while providing similar
    accuracy to a larger model for the particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot gives you a glimpse of the large variety of models
    that are available within Vertex AI Model Garden ordered by the modalities or
    tasks they can be used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Vertex AI Model Garden](img/B17792_12_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Vertex AI Model Garden
  prefs: []
  type: TYPE_NORMAL
- en: '**Naming structure denoting model sizes**: To help you distinguish between
    similar models of different sizes, the Google-published models in Vertex AI come
    with different suffixes. The four model-size labels Google has published so far,
    from largest to smallest, are unicorn, bison, otter, and gecko. Most of the models
    available today at the time of this book’s publishing are of the type bison or
    gecko, but users can expect additional models of different sizes to be included
    in the future, based on announcements from the Google Cloud team.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The largest available LLM is not always the best choice for your solution since
    larger models incur significantly higher compute costs during inference as compared
    to models with fewer parameters. Always use the smallest possible model that meets
    your accuracy requirements to ensure your solution is compute-efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at some of the foundation models available within Model Garden.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation GenAI models in Vertex AI Model Garden
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GenAI foundation models are large, powerful LLMs that form the core of all GenAI
    solutions. They are capable of generating new, original content, such as text,
    images, music, or code and are categorized by their modality (text, image. etc.)
    and by the use cases they address (general, medical, security, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Model Garden is a repository of a large number of foundation models,
    both open source (for example, Llama, published by Meta) and the ones published
    by Google. Google models are built upon the core LLM models designed and trained
    by Google’s research and engineering teams. They address different key use cases
    such as chat, text generation, image generation, code assistance, and artifact
    matching.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a list of currently available Google-published models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text-bison`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: `text-bison` is a text generation model designed to follow natural
    language instructions, suitable for a range of language tasks. The “`bison`” suffix
    refers to the model size (see the preceding *Note* on model size labels).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity extraction from given text input
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extractive question** **answering** (**EQA**)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Marketing content generation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`textembedding-gecko`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: This model returns multi-dimensional vector embeddings for the
    text inputs. Once you have created an embedding database of an existing text corpus,
    you can use it to find the closest matches for any other text snippets. The “`gecko`”
    suffix refers to the model size (see the preceding *Note* on model size labels).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Text matching
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engine backend
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`textembedding-gecko-multilingual`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: Similar to the aforementioned `textembedding-gecko` model, this
    model supports over 100 languages.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multilingual text analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-language **natural language** **processing** (**NLP**)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation applications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chat-bison`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: Optimized for multi-turn conversation scenarios. It has been
    trained with data until February 2023 and supports up to 4,096 input tokens, 1,024
    output tokens, and a maximum of 2,500 turns.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chatbots
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtual assistants
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer service application
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`code-bison`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: Fine-tuned to generate code from natural language descriptions,
    facilitating up to 6,144 input tokens and 1,024 output tokens.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Automated code generation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Unit test creation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`codechat-bison`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: Fine-tuned for chatbot conversations, assisting with code-related
    queries and supporting 6,144 input tokens and 1,024 output tokens.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Code assistance chatbots
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Development support
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Education and code learning platforms
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`code-gecko` (model tuning not supported) :'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: Designed to suggest code completion based on the context of
    the written code, managing up to 2,048 input tokens and 64 output tokens.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Code completion tools
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagegeneration`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: This model is geared to generate high-quality visual assets
    swiftly, with specifications including a resolution of 1024x1024 pixels and allowances
    for certain requests and image size limits.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Graphic design
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Content creation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`multimodalembedding`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: Generates vectors from provided inputs, which can be a combination
    of image and text, within the specified token, language, and size parameters.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Image and text analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal data processing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Content recommendation systems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image text (image captioning)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagetext` (image captioning):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: An image-captioning model capable of generating captions in
    several languages for a provided image, respecting certain rate and size limits.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Content creation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessibility services
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Image text (**visual QA**, or **VQA**)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagetext` (VQA):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: A model designed for VQA services, offering answers in English
    within defined rate and size restrictions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Use cases*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VQA services
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Education and training modules
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Interactive content creation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since we have familiarized ourselves with the key foundation models, let’s now
    try them out through GenAI Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI GenAI Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GenAI Studio is the user interface you can use to interact with most of the
    foundation models listed previously. It does not require any programming knowledge
    and is primarily built for non-programmers to be able to use the powerful GenAI
    capabilities offered through Vertex AI. Programmers, on the other hand, can use
    Vertex AI APIs to access the foundation GenAI model. We will discuss API-based
    usage later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, GenAI Studio supports three modalities:'
  prefs: []
  type: TYPE_NORMAL
- en: Text (language)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image (vision)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio (speech)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAI Studio – language
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the **Language** section, you have the option to interact with the Vertex
    AI foundation models in two modes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt mode**: Uses text or code models optimized for transactional, usually
    larger responses to natural language queries around text or code generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chat mode**: Uses text or code models optimized for conversations to generate
    responses based on current input and recent conversation/chat history'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the different options in GenAI Studio to interact
    with language models, including a prompt-based approach and a chat-based approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Vertex AI GenAI Studio (Language section)](img/B17792_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Vertex AI GenAI Studio (Language section)
  prefs: []
  type: TYPE_NORMAL
- en: Before we start experimenting with GenAI Studio, it is important we are familiar
    with basic concepts around prompt design/engineering.
  prefs: []
  type: TYPE_NORMAL
- en: What is a prompt?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of GenAI or NLP, a “prompt” refers to an input string of text
    that is fed into a language model to generate a corresponding output text. In
    this case, the prompt serves as a way to instruct or guide the model to generate
    text based on the given input. It can be a sentence, a phrase, or even a single
    word, depending on the specific requirements of the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: What is prompt design or prompt engineering?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Prompt design” refers to the process of crafting and optimizing the input (prompt)
    given to a language model to achieve the desired output or results. This process
    involves understanding the nuances of language and the behavior of the AI model
    to elicit responses that are aligned with the user’s expectations. Prompt design
    can be a critical aspect of working with generative LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some essential aspects of prompt design:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clarity**: Ensuring the prompt clearly conveys the desired task to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity**: Making the prompt specific to avoid ambiguous or overly generalized
    responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: Providing enough context in the prompt to facilitate a more informed
    and relevant output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Formatting**: Structuring the prompt in a manner that encourages the desired
    format of the response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing and iteration**: Prompt design is often an iterative process involving
    testing various prompt strategies and fine-tuning them based on the outputs received'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical considerations**: Design prompts that are ethical and avoid encouraging
    harmful, biased, or inappropriate responses from the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety measures**: Implementing safety measures such as using techniques
    to limit the model to safe and appropriate responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, prompt design can involve a mixture of art and science, requiring
    both creative and analytical skills to master. It’s a key skill in the field of
    AI, especially for those working on AI-powered chatbots, virtual assistants, content
    creation tools, and other applications that rely on NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt content
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A “prompt” can potentially be broken down into several components, depending
    on the complexity of the task at hand. Here are common parts that constitute a
    well-structured prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Instruction**: This is the part of the prompt where you provide a clear directive
    or question to guide the AI’s response. The instruction should be explicit about
    the kind of information or the format of the answer you are expecting. An instruction
    can be as simple as “What is the capital of California?” or as complicated as
    a 10-page-long list of rules to be taken into consideration by the model. This
    detailed set of instructions can overlap with the prompt “context” discussed next.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context**: If the prompt is a follow-up or is seeking detailed information,
    providing context can be essential. Context can include background information
    or data that is necessary to generate a precise and accurate response. For a chatbot,
    the context usually includes previous back-and-forth conversations with the bot
    so that the bot can keep responses contextual to the topic being discussed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Examples**: In some cases, especially with more complex tasks, it might be
    beneficial to provide examples within the prompt to give a clearer picture of
    the expected output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this prompt, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context**: Sets the stage by defining the role of the AI as a helper in finding
    a recipe based on the available ingredients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction**: Clearly guides the AI to come up with a recipe that is simple
    and suitable for a beginner, using the ingredients listed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Examples**: Offers a prototype recipe using a different set of ingredients,
    helping the AI understand the kind of response expected from it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This structured prompt aids in channeling the AI to craft a response that meets
    the specific needs and expectations of the user while providing a concrete example
    to work from. It encourages the AI to create a simple, beginner-friendly recipe
    using the ingredients specified in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical response from a GenAI model to the structured prompt provided earlier
    might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this response, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: A new recipe is created that utilizes all the ingredients mentioned in the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The instructions are simple and beginner-friendly, adhering to the directive
    given in the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recipe includes a list of ingredients along with their approximate quantities,
    followed by a step-by-step cooking guide to help even a beginner cook follow along
    easily
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The response maintains a helpful and encouraging tone, in line with the context
    set in the prompt, and it also adds a personal touch by suggesting optional garnishes
    to enhance the dish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In GenAI Studio, besides the input prompt, another important tool you can use
    to tweak the output is the response parameters listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max output tokens**: This parameter limits the highest count of tokens the
    generated response can contain. A “token” equates to about 4 characters, with
    100 tokens translating to roughly 60-80 words. To obtain shorter responses, set
    a lower value and, conversely, increase it for more verbose responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature**: This parameter defines the randomness during token selection
    in the response generation phase. A lower temperature value ensures deterministic
    and less imaginative outputs while increasing it encourages a more diversified
    and creative output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-K**: This parameter governs the token selection method by delineating
    the range of top probable tokens from which the next token is chosen, a process
    influenced by the temperature setting. The default setting is 40, but modifying
    it can control the randomness of the output, facilitating either a more deterministic
    or a more randomized response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-P**: Functioning somewhat similarly to **Top-K**, this parameter operates
    by setting a probability threshold for token selection. Tokens are chosen based
    on their probability, adhering to the **Top-P** value, thus guiding the randomness
    in the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding and carefully manipulating these parameters will help you tailor
    the model’s responses to suit your requirements. Experiment with different configurations
    to master the optimal utilization of generative models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows some of the different GenAI Studio response
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – GenAI Studio settings](img/B17792_12_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – GenAI Studio settings
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have familiarized ourselves with the basics of prompt design, let’s
    see how you can interact with the Vertex AI foundation models using similar prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Using Vertex AI GenAI models through GenAI Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s look at some examples of how you can use GenAI Studio to generate
    content. In this section, we will cover four use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating text using free-form input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating text using structured input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating code samples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 1 – using GenAI Studio language models to generate text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will use a Vertex AI GenAI model to generate marketing
    content for a new device and play around with some of the output configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the GCP console >> **Vertex AI** >> **Generative AI Studio** >>
    **Language** >> **Text Prompt**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the **Token limit** value to 256.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following or copy-paste (from the e-book) the following text into
    the **Prompt** field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click **Submit** to generate a response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The response would look something like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s set the **Token limit** parameter to 50 and click **Submit** again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The new response generated by the model will now be much smaller since we reduced
    the output token limit to 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, we saw how a text generation model can be instructed to generate text content
    (marketing content in this case) and how its output size can be limited.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – submitting examples along with the text prompt in structured format
    to get generated output in a specific format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will try to get the model to provide very concise answers
    to our questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the GCP console >> **Vertex AI** >> **Generative AI Studio** >>
    **Language** >> **Text Prompt** >> **STRUCTURED**, as shown next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.4 – GenAI Studio structured input](img/B17792_12_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – GenAI Studio structured input
  prefs: []
  type: TYPE_NORMAL
- en: The structured format shown in the preceding screenshot allows us to submit
    a few examples as input-output pairs to show the model the desired output format
    or style. In this example, we want the model to provide concise answers, preferably
    in key-value format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the following text in the **Test** | **INPUT** section and click **Submit**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 12.5 – GenAI Studio: Model response](img/B17792_12_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5 – GenAI Studio: Model response'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the model’s response, although correct, is not a concise answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So now, let’s add some examples as part of the input prompt so that the model
    can see the output format/style we are expecting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Examples** | **INPUT** section, add this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the **Examples** | **OUTPUT** section, add the following text:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Click **Submit**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.6 – GenAI Studio: Response with structured input](img/B17792_12_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6 – GenAI Studio: Response with structured input'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the previous screenshot, the answer is now a lot more concise
    and matches the format submitted as an example with the prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although in this simple example, we just submitted a single example, for more
    complex use cases, you can submit a long list of examples to help the model better
    understand your output requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Example 3 – generating images using GenAI Studio (Vision)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will use a text-to-image model to generate images based
    on a text description of the image entered by us:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the GCP console >> **Vertex AI** >> **Generative AI Studio** >>
    **Vision** >> **Generate**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Prompt** field at the bottom of the screen, type the following text
    and click **Submit**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the sidebar, select the **Digital Art** style and click **Submit**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The image model generates the following images based on the prompt we provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.7 –  GenAI Studio: Generated images](img/B17792_12_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7 – GenAI Studio: Generated images'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, change the style to **Photography** and click **Submit** again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 12.8 – GenAI Studio: Generated images with style set to Photography](img/B17792_12_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8 – GenAI Studio: Generated images with style set to Photography'
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the previous screenshot that the model is now generating images
    that are a lot more photo-realistic.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can play around with the prompt to see how you can control the generated
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4 – generating code samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we will use a Vertex AI code generation model (`codey`/`code_bison`)
    to generate a Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the GCP console >> **Vertex AI** >> **Generative AI Studio** >>
    **Language** >> **Code Prompt**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type the following text in the **Prompt** field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response should be similar to the generated code shown next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Although in the preceding example, we used a code generation model to create
    a very simple code sample, you can modify the prompt to add more details about
    your requirements and generate significantly more complex code. Most code generation
    models available today can’t fully replace developers, even for relatively simple
    coding tasks, but they do help accelerate the coding workflow and work as great
    assistants to coders.
  prefs: []
  type: TYPE_NORMAL
- en: Building and deploying GenAI applications with Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s see how you can use Vertex AI GenAI features programmatically and
    integrate them with your apps.
  prefs: []
  type: TYPE_NORMAL
- en: Use case 1 – using GenAI models to extract key entities from scanned documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use a publicly available patent document from the US Patents and Trademark
    Office as a sample document and extract the following information from the document:'
  prefs: []
  type: TYPE_NORMAL
- en: Inventor name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location of the inventor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patent number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the notebook at [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_Entity_Extraction.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_Entity_Extraction.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: 'In this notebook, you will perform the following steps to extract the required
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the text from the document by using the Document AI **Optical Character
    Recognition** (**OCR**) tool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the text to the GenAI model (`text-bison`) along with a detailed prompt
    about the entities we need to extract from the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parse the response received from the model to feed it into the data warehouse
    (BigQuery).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we use a traditional approach of training a **deep learning** (**DL**) model
    to extract this information from a scanned document, we will need a much larger
    set of annotated data and resources to train the model. With a pre-trained LLM,
    we are able to do the same task much faster, without needing any training dataset
    and without training a new model. Additionally, if later we want to extract any
    additional entities from the document, all we will need to do is modify our prompt
    to the model to include that new entity. With a non-LLM model, you will have to
    spend time annotating additional data samples and retraining the model to include
    that new label.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of standard entity extraction approach discussed above
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For our use case and document sample, this approach worked well because we were
    able to fit all the text into our LLM model’s context window (the maximum size
    of text input a model can use). But in the real world, we often run into scenarios
    where the amount of information/text we need to consider is much larger, and it
    can’t be sent to the model as part of a single prompt. For example, if you have
    a question about a story spread across a 500-page book, to get the most accurate
    answer, you need to feed all 500 pages worth of text to the LLM along with your
    question. But as of now, even the largest of the available language models can’t
    ingest that much text as an input prompt. So, in such scenarios, we use an alternative
    technique called **Retrieval Augmented Generation** (**RAG**), which we cover
    in use case 2 next.
  prefs: []
  type: TYPE_NORMAL
- en: Use case 2 – implementing a QA solution based on the RAG methodology using Vertex
    AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The solution will also be grounded in our document corpus to mitigate hallucinations.
    Before we jump into the exercise, let’s first understand what the RAG framework
    is.
  prefs: []
  type: TYPE_NORMAL
- en: What is RAG?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'RAG is a methodology used in NLP that enhances the capabilities of language
    models by combining them with a retrieval mechanism. It’s designed to improve
    the performance of language generation models, particularly in providing more
    accurate and contextually relevant responses. Here’s a brief overview of how RAG
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval mechanism**: The first step involves retrieving relevant documents,
    passages, or text snippets from a large corpus of text. This is typically done
    using a vector retrieval system. The input query (or question) is encoded into
    a vector representation, and this vector is then used to search through a database
    of pre-encoded documents or passages to find the most relevant ones. This retrieval
    is based on the similarity of the vector representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmentation**: The augmentation component of RAG combines the retrieved
    documents or passages with the initial prompt to create an augmented prompt. This
    augmented prompt provides the generative model with more context and information
    to work with, which can help to improve the quality of the generated text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer generation**: With the context from the retrieved documents, an LLM
    is used to generate a response. This model takes both the original query and the
    retrieved documents as input, allowing it to generate responses that are informed
    by the external knowledge contained in the retrieved texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Retrieval augmented generation (RAG) approach](img/B17792_12_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Retrieval augmented generation (RAG) approach
  prefs: []
  type: TYPE_NORMAL
- en: 'The RAG system offers several significant advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced accuracy and relevance**: By integrating information retrieval with
    language generation, RAG systems can provide responses that are more accurate
    and relevant to the user’s query. This is particularly beneficial for questions
    that require specific, factual information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access to up-to-date information**: Traditional language models are limited
    by the information they were trained on, which can become outdated. Constantly
    retraining these models with newer information is not viable due to the time it
    takes to train such models and the amount of compute resources required to train
    and retrain such models. RAG overcomes this by retrieving relevant information
    from up-to-date documents, ensuring that the responses include the most current
    data available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved handling of niche queries**: RAG is adept at handling queries about
    niche or less common topics. Since it can pull information from a wide range of
    sources, it’s not as limited by the training data’s scope as traditional models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability and flexibility**: RAG systems can be adapted to different domains
    and types of queries by modifying the retrieval component. This makes them scalable
    and flexible for various applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextual understanding**: The integration of external information allows
    RAG to understand and respond to queries in a more contextually nuanced way, leading
    to more sophisticated and nuanced conversations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-effectiveness in training**: Since RAG systems can augment their responses
    with external information, they might require less extensive (and expensive) training
    datasets compared to traditional models that need to learn everything from the
    training data alone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These advantages make RAG a powerful tool in the development of advanced AI
    systems, especially in areas where accuracy, recency, and depth of knowledge are
    crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get started with the hands-on exercise. In this exercise notebook,
    we will ingest a large PDF file containing **Alphabet’s** quarterly 10K filing
    containing key financial information shared by the company every quarter and then
    use Vertex AI GenAI tools to create a Q&A system we can use to ask questions about
    Alphabet’s earnings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are the steps we will perform:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingest the PDF and extract the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Break up extracted text into smaller text snippets or chunks so that they can
    later be matched to the questions being asked to find the relevant information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the Vertex AI text embedding model to convert these snippets into embeddings/vectors.
    Think of this step as creating individual fingerprints for each text snippet so
    that later on, we can try to find the closest matches to the text of the question
    we want to answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a vector “datastore” that stores the embeddings created from text snippets
    in step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the provided question into an embedding and then try to find the closest
    20 matches in the vector datastore.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an input prompt for our LLM (Vertex AI text-bison model) by combining
    the question text along with the text of the 20 close matches found in step 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the prompt to the LLM (Vertex AI text-bison model) to get a final answer
    that can be presented back to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The [*Chapter 12*](B17792_12.xhtml#_idTextAnchor173) *– Vertex GenAI_RAG* notebook
    walks you through the steps described previously. ([https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_RAG.ipynb](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/blob/main/Chapter12/Chapter12_Vertex_GenAI_RAG.ipynb))
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since the focus of this chapter is on GenAI, we didn’t dive too deep into the
    domain of vector databases and matching algorithms used to find matching embeddings.
    The preceding exercise notebook uses a simple approach to calculate cosine similarity
    scores to find the closest matching vectors from a database within a pandas DataFrame.
    This works fine for small-scale data, but for real-world solutions requiring storage
    of billions of embeddings and matching latency of under 5ms, we suggest you use
    managed vector databases such as Vertex AI Vector Search (previously known as
    Matching Engine) or open source options such as the **pgvector extension** for
    PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at how you can customize pre-trained language models in Vertex
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing GenAI performance with model tuning in Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Foundation pre-trained models, despite their great out-of-the-box performance
    across a variety of generic tasks, sometimes fall short for specialized tasks,
    and prompt tuning alone cannot adequately address the performance gap. To bridge
    this gap, model tuning on Vertex AI can significantly enhance a model’s task-specific
    performance and ensure adherence to specific output requirements when standard
    instructions are inadequate. This section will provide insight into model tuning
    on Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several compelling reasons exist for tuning LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved performance**: Tuning can significantly improve the accuracy, fluency,
    and relevance of LLM outputs for a particular task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain adaptation**: Tuning enables the specialization of LLMs for specific
    domains or types of data, ensuring that generated outputs are consistent with
    the domain’s terminology and style'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias mitigation**: Tuning can help alleviate biases inherent in pre-trained
    LLMs, promoting fairer and more equitable outcomes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model tuning involves training the model with a dataset that extensively covers
    a unique task. This approach is particularly effective for niche tasks, as tuning
    with even a small sample dataset can lead to notable performance improvements.
    Post-tuning, the model requires fewer examples in its prompts to perform effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Various approaches can be employed to customize LLMs for different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering** is a less computationally expensive approach to tuning
    LLMs. This involves crafting input prompts that guide the LLM toward generating
    desired outputs. Prompt engineering can be effective for a wide range of tasks,
    but it can be more difficult to master than fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning** is the most common approach to tuning LLMs. This involves supervised
    training of LLMs on a task-specific dataset of labeled examples, which updates
    weights across all layers of the model. Fine-tuning can be very effective, but
    it can also be computationally expensive and time-consuming due to the size of
    typical LLMs nowadays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter-efficient fine-tuning** (**PEFT**) is a technique that can be used
    to fine-tune LLMs with limited computational resources. PEFT, although similar
    to the aforementioned fine-tuning method, works by only tuning the parameters
    of certain layers in the underlying LLM while the rest of the layers remain frozen,
    thereby significantly reducing the number of parameters that need to be updated
    during training. This significantly reduces the number of required computations
    and reduces the overall cost of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) can also be used to tune LLMs. This involves
    training the LLM to generate outputs that maximize a specific reward signal. RL
    can be effective for tasks that are difficult to define with labeled examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now look at how you can use Vertex AI for tuning foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: Using Vertex AI supervised tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vertex AI currently offers a PEFT-supervised tuning feature to tune LLMs. It
    is suitable for tasks such as classification, **sentiment analysis** (**SA**),
    entity extraction, simple content summarization, and domain-specific queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the [*Chapter 12*](B17792_12.xhtml#_idTextAnchor173) *– LLM – Supervised
    Training* notebook in the accompanying GitHub repository for a hands-on end-to-end
    exercise to running an LLM tuning job in Vertex AI. Here are the key steps you
    need to follow to run a supervised tuning job:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_text`: This field includes the prompt for the model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`output_text`: This field should contain the model’s anticipated response post-tuning'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample dataset you can use: `input_text` field can have a maximum token length
    of 8,192 (approx. 32k English characters), and the `output_text` field can have
    a maximum token length of 1,024 (approx. 4k English characters).'
  prefs: []
  type: TYPE_NORMAL
- en: Upload the dataset to a Google Cloud Storage bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a supervised tuning job (*detailed steps are in the* *accompanying notebook*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following arguments are to be provided when starting a tuning job:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the Python code to kick off the tuning job:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the tuned model from **Vertex AI Model Registry** to run prediction or
    evaluation jobs (*detailed steps are in the* *accompanying notebook*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of publication, the maximum number of samples you can use for a
    fine-tuning job is limited to 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us look at Vertex AI’s native capabilities that help ensure that the
    output of LLMs is safe and compliant for an enterprise setting.
  prefs: []
  type: TYPE_NORMAL
- en: Safety filters for generated content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite being extremely useful for a wide array of use cases, LLMs’ ability
    to absorb human knowledge and behavior (good and bad) through the immense datasets
    gathered from the public also creates the risk of these models being exploited
    or generating harmful content. It is not uncommon for these models to generate
    outputs that are unanticipated, encompassing offensive, insensitive, or incorrect
    content.
  prefs: []
  type: TYPE_NORMAL
- en: It remains imperative for developers to have a profound understanding and meticulously
    test the models prior to deployment to circumvent any potential pitfalls. To help
    developers in this endeavor, GenAI Studio incorporates built-in content filtration
    systems, and the PaLM API offers safety attribute scoring, aiding clients to examine
    Google’s safety filters and establish confidence thresholds aligned with their
    individual use case and business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a full list of safety attributes offered as part of Google PaLM models:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Safety Attribute** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Derogatory | Negative or harmful comments targeting identity and/or protected
    attributes. |'
  prefs: []
  type: TYPE_TB
- en: '| Toxic | Content that is rude, disrespectful, or profane. |'
  prefs: []
  type: TYPE_TB
- en: '| Sexual | Contains references to sexual acts or other lewd content. |'
  prefs: []
  type: TYPE_TB
- en: '| Violent | Describes scenarios depicting violence against an individual or
    a group, or general descriptions of gore. |'
  prefs: []
  type: TYPE_TB
- en: '| Insult | Insulting, inflammatory, or negative comment toward a person or
    a group of people. |'
  prefs: []
  type: TYPE_TB
- en: '| Profanity | Obscene or vulgar language such as cursing. |'
  prefs: []
  type: TYPE_TB
- en: '| Death, Harm, and Tragedy | Human deaths, tragedies, accidents, disasters,
    and self-harm. |'
  prefs: []
  type: TYPE_TB
- en: '| Firearms and Weapons | Content that mentions knives, guns, personal weapons,
    and accessories such as ammunition, holsters, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Public Safety | Services and organizations that provide relief and ensure
    public safety. |'
  prefs: []
  type: TYPE_TB
- en: '| Health | Human health, including: health conditions, diseases, and disorders,
    medical therapies, medication, vaccination, and medical practices resources for
    healing, including support groups. |'
  prefs: []
  type: TYPE_TB
- en: '| Religion and Belief | Belief systems that deal with the possibility of supernatural
    laws and beings; religion, faith, belief, spiritual practice, churches, and places
    of worship. Includes astrology and the occult. |'
  prefs: []
  type: TYPE_TB
- en: '| Illicit Drugs | Recreational and illicit drugs; drug paraphernalia and cultivation,
    headshops, etc. Includes medicinal use of drugs typically used recreationally
    (e.g. marijuana). |'
  prefs: []
  type: TYPE_TB
- en: '| War and Conflict | War, military conflicts, and major physical conflicts
    involving large numbers of people. Includes discussion of military services, even
    if not directly related to a war or conflict. |'
  prefs: []
  type: TYPE_TB
- en: '| Finance | Consumer and business financial services, such as banking, loans,
    credit, investing, insurance, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Politics | Political news and media; discussions of social, governmental,
    and public policy. |'
  prefs: []
  type: TYPE_TB
- en: '| Legal | Law-related content, including law firms, legal information, primary
    legal materials, paralegal services, legal publications and technology, expert
    witnesses, litigation consultants, and other legal service providers. |'
  prefs: []
  type: TYPE_TB
- en: Table 12.1 – PaLM models’ safety attributes
  prefs: []
  type: TYPE_NORMAL
- en: 'When you submit an API request to PaLM models, the response from the API includes
    confidence scores for each safety attribute, as shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The scores in the preceding response are the risk values for each of the risk
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can then program the required safety thresholds in their applications
    to remove any harmful content returned by the API. For example, for an application
    geared toward a younger audience, developers might set stringent filters to eliminate
    any text that is above the score of 0.1 on any of the safety attributes. But if
    the requirement is to create content to be shared on forums where adults discuss
    video games and in-game weapons, then developers might relax the filters around
    the *Firearms and Weapons* safety attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please keep in mind, right now, PaLM models do apply some initial safety filters
    before sending a response back to the customer. These filters can’t be completely
    switched off at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vertex AI GenAI is a powerful suite of tools that can be used to create a wide
    variety of GenAI applications. With its easy-to-use interface and extensive library
    of pre-trained models, Vertex AI GenAI makes it possible for developers of all
    skill levels to get started with GenAI quickly and easily.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that now, after reading this chapter, you possess foundational and practical
    knowledge about GenAI and its implementation using Vertex AI. With the skills
    to interact with and leverage foundation models, comprehension of basic prompt
    engineering, and an understanding of safety features native to Google’s GenAI
    models, you are now well-equipped to embark on practical endeavors and explore
    innovative applications using GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [*Chapter 13*](B17792_13.xhtml#_idTextAnchor194), *Document
    AI – An End-to-End Solution for Processing Documents*, we will go over how you
    can use Google Cloud’s Document AI solution to extract information from scanned
    documents and structure it into a format that can be ingested by your data storage
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Vertex AI* – *Responsible* *AI*: [https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai#safety_attribute_descriptions)'
  prefs: []
  type: TYPE_NORMAL
