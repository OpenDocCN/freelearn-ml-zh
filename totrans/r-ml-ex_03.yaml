- en: Chapter 2. Let's Help Machines Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning, when you first hear it, sounds more like a fancy word from
    a sci-fi movie than the latest trend in the tech industry. Talk about it to people
    in general and their responses are either related to being generally curious about
    the concept or being cautious and fearful about intelligent machines taking over
    our world in some sort of Terminator-Skynet way.
  prefs: []
  type: TYPE_NORMAL
- en: We live in a digital age and are constantly presented with all sorts of information
    all the time. As we will see in this and the coming chapters, machine learning
    is something that loves data. In fact, the recent hype and interest in this field
    has been fueled by not just the improvements in computing technology but also
    due to exponential growth in the amount of data being generated every second.
    The latest numbers stand at around 2.5 quintillion bytes of data every day (that's
    2.5 followed by 18 zeroes)!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Fun Fact**: More than 300 hours of video data is uploaded to YouTube every
    minute'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html](https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just take a deep breath and look around. Everything around you is generating
    data all the time, of all sorts; your phone, your car, the traffic signals, GPS,
    thermostats, weather systems, social networks, and on and on and on! There is
    data everywhere and we can do all sorts of interesting things with it and help
    the systems learn. Well, as fascinating as it sounds, let us start our journey
    on machine learning. Through this chapter we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms in machine learning and their application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Families of algorithms: supervised and unsupervised'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aren't we taught that computer systems have to be programmed to do certain tasks?
    They may be a million times faster at doing things but they have to be programmed.
    We have to code each and every step and only then do these systems work and complete
    a task. Isn't then the very notion of machine learning a very contradictory concept?
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest ways, machine learning refers to a method of teaching the systems
    to learn to do certain tasks, such as learning a function. As simple as it sounds,
    it is a bit confusing and difficult to digest. Confusing because our view of the
    way the systems (computer systems specifically) work and the way we learn are
    two concepts that hardly intersect. It is even more difficult to digest because
    learning, though an inherent capability of the human race, is difficult to put
    in to words, let alone teach to the systems.
  prefs: []
  type: TYPE_NORMAL
- en: Then what is machine learning? Before we even try to answer this question, we
    need to understand that at a philosophical level it is something more than just
    a way to program. Machine learning is a lot of things.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways in which machine learning can be described. Continuing
    from the high level definition we presented in the previous chapter, let us go
    through the definition given by Tom Mitchell in 1997:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"A computer program is said to learn from experience E with respect to some
    task T and some performance measure P, if its performance on T, as measured by
    P, improves with experience E."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Quick Note about Prof Tom Mitchell**'
  prefs: []
  type: TYPE_NORMAL
- en: Born in 1951, he is an American computer scientist and professor at Carnegie
    Mellon University (CMU). He is also the chair of the machine learning department
    at CMU. He is well known for his contributions in the fields of machine learning,
    artificial intelligence, and cognitive neuroscience. He is part of various institutions
    such as the Association for the Advancement of Artificial Intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us try to make sense out of this concise yet powerful definition with
    the help of an example. Let us say we want to build a system that predicts the
    weather. For the current example, the task (T) of the system would be to predict
    the weather for a certain place. To perform such a task, it needs to rely upon
    weather information from the past. We shall term it as experience E. Its performance
    (P) is measured on how well it predicts the weather at any given day. Thus, we
    can generalize that a system has successfully learned how to predict the weather
    (or task T) if it gets better at predicting it (or improves its performance P)
    utilizing the past information (or experience E).
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the preceding example, this definition not only helps us understand
    machine learning from an engineering point of view, it also gives us tools to
    quantify the terms. The definition helps us with the fact that learning a particular
    task involves understanding and processing of the data in the form of experience.
    It also mentions that if a computer program learns, its performance improves with
    experience, pretty similar to the way we learn.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have developed an abstract understanding of machine learning. We understand
    the definition of machine learning which states that a task T can be learned by
    a computer program utilizing data in the form of experience E when its performance
    P improves with it. We have also seen how machine learning is different from conventional
    programming paradigms because of the fact that we do not code each and every step,
    rather we let the program form an understanding of the problem space and help
    us solve it. It is rather surprising to see such a program work right in front
    of us.
  prefs: []
  type: TYPE_NORMAL
- en: All along while we learned about the concept of machine learning, we treated
    this magical computer program as a mysterious black box which learns and solves
    the problems for us. Now is the time we unravel its enigma and look under the
    hood and see these magical algorithms in full glory.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with some of the most commonly and widely used algorithms in machine
    learning, looking at their intricacies, usage, and a bit of mathematics wherever
    necessary. Through this chapter, you will be introduced to different families
    of algorithms. The list is by no means exhaustive and, even though the algorithms
    will be explained in fair detail, a deep theoretical understanding of each of
    them is beyond the scope of this book. There is tons of material easily available
    in the form of books, online courses, blogs, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is like the `Hello World` algorithm of the machine learning universe. It
    may be one of the easiest of the lot to understand and use but it is by no means
    any less powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Published in 1958 by Frank Rosenblatt, the perceptron algorithm gained much
    attention because of its guarantee to find a separator in a separable data set.
  prefs: []
  type: TYPE_NORMAL
- en: A perceptron is a function (or a simplified neuron to be precise) which takes
    a vector of real numbers as input and generates a real number as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, a perceptron can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where, `w1,…,wn` are weights, `b` is a constant termed as bias, `x1,…,xn` are
    inputs, and `y` is the output of the function `f`, which is called the activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize weight vector `w` and bias `b` to small random numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the output vector `y` based on the function `f` and vector `x`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weight vector `w` and bias `b` to counter the error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 and 3 until there is no error or the error drops below a certain
    threshold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm tries to find a separator which divides the input into two classes
    by using a labeled data set called the training data set (the training data set
    corresponds to the experience E as stated in the definition for machine learning
    in the previous section). The algorithm starts by assigning random weights to
    the weight vector `w` and the bias `b`. It then processes the input based on the
    function `f` and gives a vector `y`. This generated output is then compared with
    the correct output value from the training data set and the updates are made to
    `w` and `b` respectively. To understand the weight update process, let us consider
    a point, say `p1`, with a correct output value of `+1`. Now, suppose if the perceptron
    misclassifies `p1` as `-1`, it updates the weight `w` and bias `b` to move the
    perceptron by a small amount (movement is restricted by learning rate to prevent
    sudden jumps) in the direction of `p1` in order to correctly classify it. The
    algorithm stops when it finds the correct separator or when the error in classifying
    the inputs drops below a certain user defined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us see the algorithm in action with the help of small example.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the algorithm to work, we need a linearly separable data set. Let us assume
    the data is generated by the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the preceding equation, the correct separator will be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Generating an input vector `x` using uniformly distributed data in R is done
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the data, we need a function to classify it into one of the
    two categories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The helper function, `calculate_distance`, calculates the distance of each point
    from the separator, while `linear_classifier` classifies each point either as
    belonging to class `-1` or class `+1`.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron algorithm then uses the preceding classifier function to find
    the correct separator using the training data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It's now time to train the perceptron!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The perceptron working its way to find the correct classifier. The correct classifier
    is shown in green
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plots show the perceptron's training state. Each incorrect classifier
    is shown with a red line. As shown, the perceptron ends after finding the correct
    classifier marked in green.
  prefs: []
  type: TYPE_NORMAL
- en: 'A zoomed-in view of the final separator can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot looks as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The correct classifier function as found by perceptron
  prefs: []
  type: TYPE_NORMAL
- en: Families of algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are tons of algorithms in the machine learning universe and more are devised
    each year. There is tremendous research happening in this space and hence the
    ever increasing list of algorithms. It is also a fact that the more these algorithms
    are being used, the more improvements in them are being discovered. Machine learning
    is one space where industry and academia are running hand in hand.
  prefs: []
  type: TYPE_NORMAL
- en: But, as Spider-Man was told that *with great power comes great responsibility*,
    the reader should also understand the responsibility at hand. With so many algorithms
    available, it is necessary to understand what they are and where they fit. It
    can feel overwhelming and confusing at first but that is when categorizing them
    into families helps.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms can be categorized in many ways. The most common
    way is to group them into supervised learning algorithms and unsupervised learning
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised learning refers to algorithms which are trained on a predefined data
    set called the training data set. The training data set is usually a two element
    tuple consisting of an input element and a desired output element or signal. In
    general, the input element is a vector. The supervised learning algorithm uses
    the training data set to produce the desired function. The function so produced
    (or rather inferred) is then utilized to correctly map new data, better termed
    as the test data.
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm which has learnt well will be able to correctly determine the outputs
    for unseen data in a reasonable way. This brings in the concepts of generalization
    and overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly, generalization refers to the concept wherein an algorithm generalizes
    the desired function based upon the (limited) training data to handle unseen data
    in a correct manner. Overfitting is exactly the opposite concept of generalization,
    wherein an algorithm infers a function such that it maps exactly to the training
    data set (including the noise). This may result in huge errors when the function
    learnt by the algorithm is checked against new/unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Both generalization and overfitting revolve around the random errors or noise
    in the input data. While generalization tries to minimize the effect of noise,
    overfitting does the opposite by fitting in noise as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problems to be solved using supervised methods can be divided into the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prepare training data**: Data preparation is the most important step for
    all machine learning algorithms. Since supervised learning utilizes labeled input
    data sets (data sets which consist of corresponding outputs for given inputs),
    this step becomes even more important. This data is usually labeled by human experts
    or from measurements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prepare the model**: The model is the representation of the input data set
    and the learnt pattern. The model representation is affected by factors such as
    input features and the learning algorithm itself. The accuracy of the inferred
    function also depends on how this representation is formed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose an algorithm**: Based on the problem being solved and the input information,
    an algorithm is then chosen to learn and solve the problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Examine and fine tune**: This is an iterative step where the algorithm is
    run on the input data set and the parameters are fine tuned to achieve the desired
    level of output. The algorithm is then tested on the test data set to evaluate
    its performance and measure the error.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under supervised learning, two major sub categories are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression based machine learning**: Learning algorithms which help us answer
    quantitative questions such as how many? or how much? The outputs are generally
    continuous values. More formally, these algorithms predict the output values for
    unseen/new data based on the training data and the model formed. The output values
    are continuous in this case. Linear regression, multivariate regression, regression
    trees, and so on are a few supervised regression algorithms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Classification based machine learning**: Learning algorithms which help us
    answer objective questions or yes-or-no predictions. For example, questions such
    as is this component faulty? or can this tumor cause cancer? More formally, these
    algorithms predict the class labels for unseen or new data based upon the training
    data and model formed. **Support Vector Machines** (**SVM**), decision trees,
    random forests, and so on are a few commonly used supervised classification algorithms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us look at some supervised learning algorithms in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regression, as mentioned previously, helps us answer quantitative questions.
    Regression has its roots in the statistics domain. Researchers use a linear relationship
    to predict the output value `Y` for a given input value `X`. This linear relationship
    is called a linear regression or regression line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, linear regression is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where, `b[0]` is the intercept or the point where the line crosses the `y` axis.
  prefs: []
  type: TYPE_NORMAL
- en: '`b[1]` is the slope of the line, that is, the change in `y` over change in
    `x`.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding equation is pretty similar to how a straight line is represented
    and hence the name linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how do we decide which line we fit for our input so that it predicts well
    for unknown data? Well, for this we need an error measure. There can be various
    error measures; the most commonly used is the **least squares method**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we define the least squares method, we first need to understand the
    term residual. Residual is simply the deviation of Y from a fitted value. Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Where, `ŷ[i]` is the deviated value of `y`.
  prefs: []
  type: TYPE_NORMAL
- en: The least squares method states that the most optimal fit of a model to data
    occurs when the sum of the squares of residuals is minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We use calculus to minimize the sum of squares for residuals and find the corresponding
    coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand linear regression, let us take a real world example to
    see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have data related to the height and weight of school children. The
    data scientist in you suddenly starts thinking about whether there is any relation
    between the weights and heights of these children. Formally, could the weight
    of a child be predicted based upon his/her given height?
  prefs: []
  type: TYPE_NORMAL
- en: To fit in linear regression, the first step is to understand the data and see
    whether there is a correlation between the two variables (`weight` and `height`).
    Since in this case we are dealing with just two dimensions, visualizing the data
    using a scatter plot will help us understand it quickly. This will also enable
    us to determine if the variables have some linear relationship or not.
  prefs: []
  type: TYPE_NORMAL
- en: Let us prepare our data first and visualize it on a scatter plot along with
    the correlation coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The scatter plot looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Figure displaying data points across weight and height dimensions
  prefs: []
  type: TYPE_NORMAL
- en: The preceding scatter plot proves that our intuition about weight and height
    having a linear relationship was correct. This can further be confirmed using
    the correlation function, which gives us a value of `0.88`.
  prefs: []
  type: TYPE_NORMAL
- en: Time to prepare the model for our data set! We use the inbuilt utility `lm`
    or the linear model utility to find the coefficients `b[0]` and `b[1]`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can experiment a bit more to find out more details calculated by the `lm`
    utility using the following commands. We encourage you to go ahead and try these
    out.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As the final piece, let us visualize the regression line on our scatter plot
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The scatter plot looks like the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Scatter plot with regression calculated regression line
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we saw how the relationship between two variables can be identified and
    predictions can be made using a few lines of code. But we are not done yet. There
    are a couple of caveats which the reader must understand before deciding whether
    to use linear regression or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression can be used to predict the output values for given inputs,
    if and only if:'
  prefs: []
  type: TYPE_NORMAL
- en: The scatter plot forms a linear pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correlation between them is moderate to strong (beyond `0.5` or `-0.5`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cases where only one of the previous two conditions are met can lead to incorrect
    predictions or altogether invalid models. For example, if we only check the correlation
    and find it to be strong and skip the step where we look at the scatter plot,
    then that may lead to invalid predictions as you might have tried to fit a straight
    line when the data itself is following a curved shape (note that curved data sets
    can also have high correlation values and hence the mistake).
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that **correlation does not imply causation**. In
    simple words, correlation between two variables does not necessarily imply that
    one causes the other. There might be a case that cause and effect are indirectly
    related due to a third variable termed as the cofounding variable. The most common
    example used to describe this issue is the relationship between shoe size and
    reading ability. From the survey data (if one existed!) it could be inferred that
    larger shoe sizes relate to higher reading ability, but this clearly does not
    mean that large feet cause good reading skills to be developed. It might be rather
    interesting to note that young children have small feet and have not yet been
    taught to read. In such a case, the two variables are more accurately correlated
    with age.
  prefs: []
  type: TYPE_NORMAL
- en: You should now be coming to similar conclusion about the weight to height example
    we used earlier. Well yes, the earlier example also suffers from similar fallacy,
    yet it served as an easy to use scenario. Feel free to look around and model cases
    which do not suffer from this.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression finds application in the field of finance where it is used
    for things such as quantification of risks on investments. It is also used widely
    in the field of economics for trendline analysis and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from linear regression, **logistic regression**, **stepwise regression**,
    **Multivariate Adaptive** **Regression Splines** (**MARS**), and others are a
    few more supervised regression learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors (KNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K-Nearest Neighbors or KNN algorithms are amongst the simplest algorithms from
    an implementation and understanding point of view. They are yet another type of
    supervised learning algorithm which helps us classify data.
  prefs: []
  type: TYPE_NORMAL
- en: KNN can be easily described using the quote *like and like strike together—Plato*,
    that is, similar things are likely have similar properties. KNN utilizes this
    very concept to label a data point based upon its similarity with respect to its
    neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, KNN can be described as the process to classify unlabeled (or unseen)
    data points by assigning them the class of the most similar labeled data points
    (or training samples).
  prefs: []
  type: TYPE_NORMAL
- en: KNN is a supervised learning algorithm. Hence, it begins with a training data
    set of examples which are classified into different classes. The algorithm then
    picks each data point in the test data set and, based upon a chosen similarity
    measure, identifies its *k* nearest neighbors (where k is specified in advance).
    The data point is then assigned the class of the majority of the k nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick up the KNN''s sleeve is the similarity measure. There are various
    similarity measures available to us. The decision to choose one is based on the
    complexity of the problem, the type of data, and so on. Euclidean distance is
    one such measure which is widely used. Euclidean distance is the shortest direct
    route between two points. Mathematically it is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-Nearest Neighbors (KNN)](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Manhattan distance, Cosine distance, and Minkowski distance are some other types
    of distance measures which can be used for finding the nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: The next parameter for KNN algorithm is the `k` in the K-Nearest Neighbors.
    The value of k determines how well the KNN model generalizes to the test data.
    The balance between overfitting and underfitting the training data depends upon
    the value of `k`. With slight deliberation, it is easy to understand that a large
    `k` will minimize the impact of variance caused by noisy data, but at the same
    time, it will also undermine small but important patterns in the data. This problem
    is called the **bias-variance tradeoff**.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal value of k, even though hard to determine, lies between the extremes
    of `k=1` to `k=total number of training samples`. A common practice is to set
    the value of `k` equal to the square root of training instances, usually between
    3 and 10\. Though a common practice, the value of `k` is dependent on the complexity
    of the concept to be learned and the number of training examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step in pursuit of the KNN algorithm is preparation of data. Features
    used to prepare the input vectors should be on similar scales. The rationale for
    this step is that the distance formula is dependent on how the features are measured.
    For example, if certain features have large range of values as compared to others,
    the distance measurements will then be dominated by such measures. The method
    of scaling features to a similar scale is called **normalization**. Very much
    like the distance measure, there are various normalization methods available.
    One such method is min-max normalization, given mathematically as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-Nearest Neighbors (KNN)](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before we begin with our example to understand KNN, let us outline the steps
    to be performed to execute KNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect data and explore data**: We need to collect data relevant to the
    concept to be learnt. We also need to explore the data to understand various features,
    know the range of their values, and determine the class labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Normalize data**: As discussed previously, KNN''s dependence on distance
    measure makes it very important that we normalize the data to remove any inconsistency
    or bias in calculations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create training and test data sets**: Since it is important to learn a concept
    and prepare a model which generalizes to acceptable levels for unseen data, we
    need to prepare training and test data sets. The test data set, even though labeled,
    is used to determine the accuracy and the ability of the model to generalize the
    concept learnt. A usual practice is to divide the input sample into two-third
    and one-third portions for training and test data sets respectively. It is equally
    important that the two data sets are a good mix of all the class labels and data
    points, that is both the data sets should be representative subsets of the full
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Train the model**: Now that we have all the things in place, we can use the
    training data set, test data set, the labels, and the value of `k` to train our
    model and label the data points in the test data set.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate the model**: The final step is to evaluate the learnt pattern. In
    this step, we determine how well the algorithm has predicted the class labels
    of the test data set as compared to their known labels. Usually a confusion matrix
    is prepared for the same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let us see KNN in action. The problem at hand is to classify different
    species of flowers based upon certain features. For this particular example, we
    will be using the Iris data set. This data set comes built in with the default
    installation of R.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting and exploring data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Step one is to collect and explore the data. Let us first gather the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check if your system has the required dataset, type in just the name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not have the data set available, no worries! You can download it
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the data, it is time to explore and understand it. For exploring
    the data set and its attributes, we use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting and exploring data](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting and exploring data](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collecting and exploring data](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The summary command helps us understand the data in a better way. It clearly
    shows different attributes along with `min`, `max`, `median`, and other such statistics.
    These help us in the coming steps where we might have to scale or normalize the
    data or features.
  prefs: []
  type: TYPE_NORMAL
- en: During step one is where we usually label our input data. Since our current
    data set is already labeled, we can skip this step for this example problem. Let
    us visually see how the species are spread. We take help of the famous scatter
    plot again, but this time we use a package called `ggvis`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install `ggvis` as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'For visualizing petal widths and lengths for all 3 species, we use the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![Collecting and exploring data](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding plot clearly shows that there is a high correlation between petal
    widths and lengths for **Iris-setosa** flowers, while it is a little less for
    the other two species.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Try visualizing sepal width versus sepal length as well and see if you can spot
    any correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The next step is to normalize the data so that all the features are on the
    same scale. As seen from the data exploration step, the values of all the attributes
    are more or less in a comparable range. But, for the sake of this example, let
    us write a min-max normalization function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Remember, normalization does not alter the data, it simply scales it. Therefore,
    even though our data does not require normalization, doing so will not cause any
    harm.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: In the following steps, we will be using un-normalized data for clarity of output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a summary of the normalized data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normalizing data](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Creating training and test data sets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now that we have our data normalized, we can divide it into training and test
    data sets. We will follow the usual two-third one-third rule of splitting the
    data into two. As mentioned earlier, both data sets should be representative of
    the whole data and hence we need to pick proper samples. We will utilize R's `sample()`
    function to prepare our samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating training and test data sets](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Learning from data/training the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we have the data ready in our training and test data sets, we can proceed
    to the next step and learn from the data using KNN. The KNN implementation in
    R is present in the class library. The KNN function takes the following inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train`: The data frame containing the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test`: The data frame containing the test data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class`: A vector containing the class labels. Also called the factor vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`k`: The value of k-nearest neighbors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the current case, let us assume the value of `k` to be `3`. Odd numbers
    are usually good at breaking ties. KNN is executed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning from data/training the model](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A quick scan of the output shows everything correct except one versicolor label
    amongst virginica's. Though this one was easy to spot, there are better ways to
    evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This brings us to the last step where we evaluate the model. We do this by
    preparing a confusion matrix or a cross table which helps us understand how the
    predicted labels are with respect to the known labels of the test data. R provides
    us with yet another utility function called `CrossTable()` which is present in
    the `gmodels` library. Let us see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating the model](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output we can conclude that the model has labeled one instance
    of virginica as versicolor while all the other test data points have been correctly
    labeled. This also helps us infer that the choice of `k=3` was indeed good enough.
    We urge the reader to try the same example with different values of `k` and see
    the change in results.
  prefs: []
  type: TYPE_NORMAL
- en: KNN is a simple yet powerful algorithm which makes no assumptions about the
    underlying data distribution and hence can be used in cases where relationships
    between features and classes are complex or difficult to understand.
  prefs: []
  type: TYPE_NORMAL
- en: On the downside, KNN is a resource intensive algorithm as it requires a large
    amount of memory to process the data. Dependency on distance measures and missing
    data requires additional processing which is another overhead with this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Despite its limitations, KNN is used in a number of real life applications such
    as for text mining, predicting heart attacks, predicting cancer and so on. KNN
    also finds application in the field of finance and agriculture as well.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees, random forests, and support vector machines are some of the
    most popular and widely used supervised classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unsupervised learning refers to algorithms which learn a concept(s) on their
    own. Now that we are familiar with the concept of supervised learning, let us
    utilize our knowledge to understand unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supervised learning algorithms, which require a labeled input training
    data set, unsupervised learning algorithms are tasked with finding relationships
    and patterns in the data without any labeled training data set. These algorithms
    process the input data to mine for rules, detect patterns, summarize and group
    the data points which helps in deriving meaningful insights, and describing the
    data to the users. In the case of unsupervised learning algorithms, there is no
    concept of training and test data sets. Rather, as mentioned before, input data
    is analyzed and used to derive patterns and relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to supervised learning, unsupervised learning algorithms can also be
    divided into two main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Association rule based machine learning**: These algorithms mine the input
    data to identify patterns and rules. The rules explain interesting relationships
    between the variables in the data set to depict frequent itemsets and patterns
    which occur in the data. These rules in turn help discover useful insights for
    any business or organization from their huge data repositories. Popular algorithms
    include Apriori and FP-Growth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering based machine learning**: Similar to supervised learning based
    classification algorithms, the main objective of these algorithms is to cluster
    or group the input data points into different classes or categories using just
    features derived from the input data alone and no other external information.
    Unlike classification, the output labels are not known beforehand in clustering.
    Some popular clustering algorithms include k-means, k-medoids, and hierarchical
    clustering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us look at some unsupervised learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Apriori algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This algorithm which took the world by storm was proposed by Agarwal and Srikant
    in 1993\. The algorithm is designed to handle transactional data, where each transaction
    is a set of items, or itemset. The algorithm in short identifies the item sets
    which are subsets of at least C transactions in the data set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let `┬` be a set of items and `D` be a set of transactions, where
    each transaction `T` is a subset of `┬`. Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apriori algorithm](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then an association rule is an implication of the form `X → Y`, where a transaction
    `T` contains `X` as a subset of `┬` and:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apriori algorithm](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The implication `X → Y` holds true in the transaction set `D` with a confidence
    factor `c` if `c%` of the transactions in `D` that contain `X` also contain `Y`.
    The association rule `X → Y` is said to have a support factor of `s` if `s%` of
    transactions in `D` contain `X U Y`. Hence, given a set of transactions `D`, the
    task of identifying association rules implies generating all such rules that have
    confidence and support greater than a user defined thresholds called `minsup`
    (for minimum support threshold) and `minconf` (for minimum confidence threshold).
  prefs: []
  type: TYPE_NORMAL
- en: Broadly, the algorithm works in two steps. The first one being identification
    of the itemsets whose occurrence exceeds a predefined threshold. Such itemsets
    are called **Frequent Itemsets**. The second step is to generate association rules
    from the identified frequent itemsets which satisfy the constraints of minimum
    confidence and support.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same two steps can be explained better using the following pseudo-code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apriori algorithm](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now let us see the algorithm in action. The data set in consideration is the
    UCI machine learning repository's `Adult` data set. The data set contains census
    data with attributes such as gender, age, marital status, native country, and
    occupation, along with economic attributes such as work class, income, and so
    on. We will use this data set to identify if there are association rules between
    census information and the income of the person.
  prefs: []
  type: TYPE_NORMAL
- en: The Apriori algorithm is present in the `arules` library and the data set in
    consideration is named `Adult.` It is also available with default R installation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Time to explore our data set and see a few sample records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We get to know that the data set contains some `48k` transactions with 115 columns.
    We also get information regarding the distribution of itemsets based on their
    sizes. The `inspect` function gives us a peek into sample transactions and the
    values each of the columns hold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us build some relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The Apriori algorithm uses the `Adult` data set as input to identify rules
    and patterns in the transactional data. On viewing the summary, we can see that
    the algorithm successfully identified `84` rules meeting the support and confidence
    constraints of `50%` and `80%` respectively. Now that we have identified the rules,
    let us see what they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Apriori algorithm](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The rules are of the form `X→ Y` where `X` is the `lhs` or left-hand side and
    `Y` is the `rhs` or right-hand side. The preceding image displays corresponding
    confidence and support values as well. From the output we can infer that if people
    are working full time then their chances of facing capital loss is almost none
    (confidence factor 95.8%). Another rule helps us infer that people who work for
    a private employer also have close to no chance of facing a capital loss. Such
    rules can be used in preparing policies or schemes for social welfare, economic
    reforms, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from Apriori, there are other association rule mining algorithms such
    as FP Growth, ECLAT, and many others which have been used for various applications
    over the years.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the world of unsupervised clustering algorithms, the most simple and widely
    used algorithm is K-Means. As we have seen recently, unsupervised learning algorithms
    process the input data without any prior labels or training to come up with patterns
    and relationships. Clustering algorithms in particular help us cluster or partition
    the data points.
  prefs: []
  type: TYPE_NORMAL
- en: By definition, clustering refers to the task of grouping objects into groups
    such that elements of a group are more similar to each other than those in other
    groups. K-Means does the same in an unsupervised manner.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, given a set of `n` observations `{x1,x2,…,xn}`, where each observation
    is a *d* dimensional vector, the algorithm tries to partition these *n* observations
    into `k (≤ n)` sets by minimizing an objective function.
  prefs: []
  type: TYPE_NORMAL
- en: As with the other algorithms, there can be different objective functions. For
    the sake of simplicity, we will use the most widely used function called the **with-in
    cluster sum of squares** or **WCSS function**.
  prefs: []
  type: TYPE_NORMAL
- en: '![K-Means](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here `μ[i]` is the mean of points in the partition `S[i]`.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm follows a simple two step iterative process, where the first step
    is called the assignment step, followed by the update step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize by setting the means for `k` partitions: `m1,m2…mk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Until the mean does not change or the change is lower than a certain threshold:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assignment step**: Assign each observation to a partition for which the with-in
    cluster sum of squares value is minimum, that is, assign the observation to a
    partition whose mean is closest to the observation.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update step**: For `i` in `1 to k`, update each mean `mi` based on all the
    observations in that partition.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm can use different initialization methods. The most common ones
    are Forgy and Random Partition methods. I encourage you to read more on these.
    Also, apart from the input data set, the algorithm requires the value of `k`,
    that is the number of clusters to be formed. The optimal value may depend on various
    factors and is generally decided based on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see the algorithm in action.
  prefs: []
  type: TYPE_NORMAL
- en: We will again use the Iris flower data set which we already used for the KNN
    algorithm. For KNN we already had the species labeled and then tried to learn
    and classify the data points in the test data set into correct classes.
  prefs: []
  type: TYPE_NORMAL
- en: With K-Means, we also aim to achieve the same partitioning of the data but without
    any labeled training data set (or supervision).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the output from `k-means`, let us see how well it has partitioned
    the various species. Remember, `k-means` does not have partition labels and simply
    groups the data points.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Output:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![K-Means](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output shows that the species setosa matches cluster label 2, versicolor
    matches label 3, and so on. Visually, it is easy to make out how the data points
    are clustered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![K-Means](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: K-Means finds wide usage in areas like computer graphics for color quantization;
    it is combined with other algorithms and used for natural language processing,
    computer vision, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There are different variations of `k-means` (R itself provides three different
    variations). Apart from k-means, other unsupervised clustering algorithms are
    k-medoids, hierarchical clustering, and others.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through this chapter, we formally defined the concept of machine learning. We
    talked about how a machine learning algorithm actually learns a concept. We touched
    upon various other concepts such as generalization, overfitting, training, testing,
    frequent itemsets, and so on. We also learnt about the families of machine learning
    algorithms. We went through different machine learning algorithms to understand
    the magic under the hood, along with their areas of application.
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge we are ready to solve some real world problems and save
    the world.
  prefs: []
  type: TYPE_NORMAL
- en: The coming few chapters build on the concepts in this chapter to solve specific
    problems and use cases. Get ready for some action!
  prefs: []
  type: TYPE_NORMAL
