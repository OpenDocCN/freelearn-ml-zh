- en: Chapter 2. Let's Help Machines Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。让我们帮助机器学习
- en: Machine learning, when you first hear it, sounds more like a fancy word from
    a sci-fi movie than the latest trend in the tech industry. Talk about it to people
    in general and their responses are either related to being generally curious about
    the concept or being cautious and fearful about intelligent machines taking over
    our world in some sort of Terminator-Skynet way.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次听到机器学习这个词时，它听起来更像是一部科幻电影中的时髦词汇，而不是科技行业最新的趋势。和普通人谈论这个话题，他们的反应要么是对这个概念的一般好奇，要么是谨慎和恐惧，担心智能机器以某种终结者-天网的方式接管我们的世界。
- en: We live in a digital age and are constantly presented with all sorts of information
    all the time. As we will see in this and the coming chapters, machine learning
    is something that loves data. In fact, the recent hype and interest in this field
    has been fueled by not just the improvements in computing technology but also
    due to exponential growth in the amount of data being generated every second.
    The latest numbers stand at around 2.5 quintillion bytes of data every day (that's
    2.5 followed by 18 zeroes)!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在数字时代，时刻都被各种各样的信息包围。正如我们将在本章和接下来的章节中看到的，机器学习是一种喜欢数据的东西。事实上，这个领域最近的热潮和兴趣不仅是由计算技术的改进所推动，还由于每秒产生的数据量的指数级增长。最新的数据显示，每天大约有2.5亿亿字节的数据（即2.5后面跟着18个零）！
- en: Note
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Fun Fact**: More than 300 hours of video data is uploaded to YouTube every
    minute'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**有趣的事实**：每分钟有超过300小时的视频数据上传到YouTube'
- en: 'Source: [https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html](https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html](https://www-01.ibm.com/software/data/bigdata/what-is-big-data.html)
- en: 'Just take a deep breath and look around. Everything around you is generating
    data all the time, of all sorts; your phone, your car, the traffic signals, GPS,
    thermostats, weather systems, social networks, and on and on and on! There is
    data everywhere and we can do all sorts of interesting things with it and help
    the systems learn. Well, as fascinating as it sounds, let us start our journey
    on machine learning. Through this chapter we will cover:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深吸一口气，环顾四周。你周围的一切都在不断生成各种类型的数据；你的手机、你的汽车、交通信号灯、GPS、恒温器、气象系统、社交网络，等等！到处都是数据，我们可以用它做各种有趣的事情，并帮助系统学习。好吧，虽然听起来很吸引人，但让我们开始我们的机器学习之旅。通过本章，我们将涵盖：
- en: Understanding machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习
- en: Algorithms in machine learning and their application
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的算法及其应用
- en: 'Families of algorithms: supervised and unsupervised'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法系列：监督学习和无监督学习
- en: Understanding machine learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解机器学习
- en: Aren't we taught that computer systems have to be programmed to do certain tasks?
    They may be a million times faster at doing things but they have to be programmed.
    We have to code each and every step and only then do these systems work and complete
    a task. Isn't then the very notion of machine learning a very contradictory concept?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是被教导计算机系统必须被编程来完成某些任务吗？它们可能在做事情时快上百万倍，但它们必须被编程。我们必须编写每一个步骤，然后这些系统才能工作并完成任务。那么，机器学习的概念本身不是一个非常矛盾的概念吗？
- en: In the simplest ways, machine learning refers to a method of teaching the systems
    to learn to do certain tasks, such as learning a function. As simple as it sounds,
    it is a bit confusing and difficult to digest. Confusing because our view of the
    way the systems (computer systems specifically) work and the way we learn are
    two concepts that hardly intersect. It is even more difficult to digest because
    learning, though an inherent capability of the human race, is difficult to put
    in to words, let alone teach to the systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，机器学习是指一种教会系统学习执行某些任务的方法，比如学习一个函数。虽然听起来很简单，但它有点令人困惑，难以理解。之所以令人困惑，是因为我们对系统（特别是计算机系统）工作方式的看法和我们学习的方式这两个概念几乎不相交。它甚至更难以理解，因为学习虽然是人类的天赋能力，但很难用言语表达，更不用说教给系统了。
- en: Then what is machine learning? Before we even try to answer this question, we
    need to understand that at a philosophical level it is something more than just
    a way to program. Machine learning is a lot of things.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是机器学习？在我们甚至尝试回答这个问题之前，我们需要理解在哲学层面上，它不仅仅是一种编程方式。机器学习包含了很多东西。
- en: 'There are many ways in which machine learning can be described. Continuing
    from the high level definition we presented in the previous chapter, let us go
    through the definition given by Tom Mitchell in 1997:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 描述机器学习的方式有很多。继续从我们在上一章中提出的高级定义，让我们看看1997年汤姆·米切尔给出的定义：
- en: '*"A computer program is said to learn from experience E with respect to some
    task T and some performance measure P, if its performance on T, as measured by
    P, improves with experience E."*'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"如果计算机程序就某个任务T和某些性能指标P而言，从经验E中学习，那么其性能P，如P所测量的，随着经验E的提高而提高。"*'
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Quick Note about Prof Tom Mitchell**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于汤姆·米切尔教授的快速笔记**'
- en: Born in 1951, he is an American computer scientist and professor at Carnegie
    Mellon University (CMU). He is also the chair of the machine learning department
    at CMU. He is well known for his contributions in the fields of machine learning,
    artificial intelligence, and cognitive neuroscience. He is part of various institutions
    such as the Association for the Advancement of Artificial Intelligence.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1951年出生，他是美国计算机科学家，卡内基梅隆大学（CMU）的教授。他也是CMU机器学习系的主任。他在机器学习、人工智能和认知神经科学领域做出了贡献，因此广为人知。他是多个机构的成员，如人工智能协会。
- en: Now let us try to make sense out of this concise yet powerful definition with
    the help of an example. Let us say we want to build a system that predicts the
    weather. For the current example, the task (T) of the system would be to predict
    the weather for a certain place. To perform such a task, it needs to rely upon
    weather information from the past. We shall term it as experience E. Its performance
    (P) is measured on how well it predicts the weather at any given day. Thus, we
    can generalize that a system has successfully learned how to predict the weather
    (or task T) if it gets better at predicting it (or improves its performance P)
    utilizing the past information (or experience E).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个例子来理解这个简洁而有力的定义。假设我们想要构建一个预测天气的系统。对于当前示例，系统的任务（T）将是预测某个地方的天气。为了执行这个任务，它需要依赖于过去的天气信息。我们将它称为经验E。它的性能（P）是通过它在任何给定一天预测天气的好坏来衡量的。因此，我们可以概括地说，如果一个系统通过利用过去的信息（或经验E）在预测天气（或任务T）方面变得更好（或提高其性能P），那么它就成功地学会了如何预测天气（或任务T）。
- en: As seen in the preceding example, this definition not only helps us understand
    machine learning from an engineering point of view, it also gives us tools to
    quantify the terms. The definition helps us with the fact that learning a particular
    task involves understanding and processing of the data in the form of experience.
    It also mentions that if a computer program learns, its performance improves with
    experience, pretty similar to the way we learn.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，这个定义不仅帮助我们从工程角度理解机器学习，还为我们提供了量化这些术语的工具。这个定义帮助我们认识到，学习特定任务涉及以经验形式理解和处理数据。它还提到，如果一个计算机程序通过经验E学习，那么它的性能P会随着经验E的提高而提高，这与我们学习的方式非常相似。
- en: Algorithms in machine learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: So far we have developed an abstract understanding of machine learning. We understand
    the definition of machine learning which states that a task T can be learned by
    a computer program utilizing data in the form of experience E when its performance
    P improves with it. We have also seen how machine learning is different from conventional
    programming paradigms because of the fact that we do not code each and every step,
    rather we let the program form an understanding of the problem space and help
    us solve it. It is rather surprising to see such a program work right in front
    of us.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对机器学习有了抽象的理解。我们理解了机器学习的定义，即当计算机程序利用经验E中的数据来执行任务T时，如果其性能P随着它而提高，那么任务T可以被学习。我们还看到了机器学习与传统的编程范式不同的地方，因为我们不是编写每一个步骤，而是让程序形成对问题空间的理解，并帮助我们解决问题。看到这样的程序在我们面前工作，确实令人惊讶。
- en: All along while we learned about the concept of machine learning, we treated
    this magical computer program as a mysterious black box which learns and solves
    the problems for us. Now is the time we unravel its enigma and look under the
    hood and see these magical algorithms in full glory.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习机器学习概念的过程中，我们一直把这个神奇的计算机程序当作一个神秘的黑色盒子，它为我们学习和解决问题。现在是我们揭开它的神秘面纱，揭开盖子，看到这些神奇算法的全貌的时候了。
- en: We will begin with some of the most commonly and widely used algorithms in machine
    learning, looking at their intricacies, usage, and a bit of mathematics wherever
    necessary. Through this chapter, you will be introduced to different families
    of algorithms. The list is by no means exhaustive and, even though the algorithms
    will be explained in fair detail, a deep theoretical understanding of each of
    them is beyond the scope of this book. There is tons of material easily available
    in the form of books, online courses, blogs, and more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从机器学习中一些最常用和广泛使用的算法开始，探讨它们的复杂性、用法，并在必要时涉及一些数学知识。通过本章，你将了解不同算法家族。这个列表绝对不是详尽的，尽管算法将被相当详细地解释，但每个算法的深入理论理解超出了本书的范围。有大量的材料以书籍、在线课程、博客等形式轻松可用。
- en: Perceptron
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知器
- en: This is like the `Hello World` algorithm of the machine learning universe. It
    may be one of the easiest of the lot to understand and use but it is by no means
    any less powerful.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是机器学习宇宙中的 `Hello World` 算法。它可能是最容易理解和使用的一类，但绝对不比其他算法弱。
- en: Published in 1958 by Frank Rosenblatt, the perceptron algorithm gained much
    attention because of its guarantee to find a separator in a separable data set.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 1958年由弗兰克·罗森布拉特（Frank Rosenblatt）发表，感知器算法因其保证在可分数据集中找到分隔符而受到广泛关注。
- en: A perceptron is a function (or a simplified neuron to be precise) which takes
    a vector of real numbers as input and generates a real number as output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一个函数（或更确切地说是一个简化的神经元）它接受一个实数向量作为输入，并生成一个实数作为输出。
- en: 'Mathematically, a perceptron can be represented as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，感知器可以表示为：
- en: '![Perceptron](img/00050.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/00050.jpeg)'
- en: Where, `w1,…,wn` are weights, `b` is a constant termed as bias, `x1,…,xn` are
    inputs, and `y` is the output of the function `f`, which is called the activation
    function.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`w1,…,wn` 是权重，`b` 是称为偏置的常数，`x1,…,xn` 是输入，`y` 是函数 `f` 的输出，该函数被称为激活函数。
- en: 'The algorithm is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 算法如下：
- en: Initialize weight vector `w` and bias `b` to small random numbers.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重向量 `w` 和偏置 `b` 初始化为小的随机数。
- en: Calculate the output vector `y` based on the function `f` and vector `x`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据函数 `f` 和向量 `x` 计算输出向量 `y`。
- en: Update the weight vector `w` and bias `b` to counter the error.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重向量 `w` 和偏置 `b` 以抵消错误。
- en: Repeat steps 2 and 3 until there is no error or the error drops below a certain
    threshold.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 和 3，直到没有错误或错误低于某个阈值。
- en: The algorithm tries to find a separator which divides the input into two classes
    by using a labeled data set called the training data set (the training data set
    corresponds to the experience E as stated in the definition for machine learning
    in the previous section). The algorithm starts by assigning random weights to
    the weight vector `w` and the bias `b`. It then processes the input based on the
    function `f` and gives a vector `y`. This generated output is then compared with
    the correct output value from the training data set and the updates are made to
    `w` and `b` respectively. To understand the weight update process, let us consider
    a point, say `p1`, with a correct output value of `+1`. Now, suppose if the perceptron
    misclassifies `p1` as `-1`, it updates the weight `w` and bias `b` to move the
    perceptron by a small amount (movement is restricted by learning rate to prevent
    sudden jumps) in the direction of `p1` in order to correctly classify it. The
    algorithm stops when it finds the correct separator or when the error in classifying
    the inputs drops below a certain user defined threshold.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法试图通过使用称为训练数据集的标记数据集（训练数据集对应于前一小节中机器学习定义中的经验 E）来找到一个分隔符，将输入分为两类。算法首先随机分配权重向量
    `w` 和偏置 `b` 的权重。然后根据函数 `f` 处理输入，并给出一个向量 `y`。然后，将生成的输出与训练数据集中的正确输出值进行比较，并相应地更新
    `w` 和 `b`。为了理解权重更新过程，让我们考虑一个点，比如 `p1`，其正确输出值为 `+1`。现在，假设如果感知器错误地将 `p1` 分类为 `-1`，它将更新权重
    `w` 和偏置 `b`，以通过一个小量（移动受到学习率的限制，以防止突然跳跃）在 `p1` 的方向上移动感知器，以便正确分类它。当找到正确的分隔符或分类输入的错误低于某个用户定义的阈值时，算法停止。
- en: Now, let us see the algorithm in action with the help of small example.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个小例子来看看算法的实际应用。
- en: 'For the algorithm to work, we need a linearly separable data set. Let us assume
    the data is generated by the following function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使算法工作，我们需要一个线性可分的数据集。让我们假设数据是由以下函数生成的：
- en: '![Perceptron](img/00051.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/00051.jpeg)'
- en: 'Based on the preceding equation, the correct separator will be given as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的方程，正确的分隔符将给出如下：
- en: '![Perceptron](img/00052.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/00052.jpeg)'
- en: 'Generating an input vector `x` using uniformly distributed data in R is done
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用R中的均匀分布数据生成输入向量`x`的方法如下：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have the data, we need a function to classify it into one of the
    two categories.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，我们需要一个函数来将其分类为两个类别之一。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The helper function, `calculate_distance`, calculates the distance of each point
    from the separator, while `linear_classifier` classifies each point either as
    belonging to class `-1` or class `+1`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助函数`calculate_distance`计算每个点到分隔符的距离，而`linear_classifier`类将每个点分类为属于类`-1`或类`+1`。
- en: The perceptron algorithm then uses the preceding classifier function to find
    the correct separator using the training data set.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，感知器算法使用先前的分类器函数，通过训练数据集找到正确的分隔符。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It's now time to train the perceptron!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候训练感知器了！
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The plot will look as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图表将如下所示：
- en: '![Perceptron](img/00053.jpeg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/00053.jpeg)'
- en: The perceptron working its way to find the correct classifier. The correct classifier
    is shown in green
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器正在寻找正确的分类器。正确的分类器用绿色显示
- en: The preceding plots show the perceptron's training state. Each incorrect classifier
    is shown with a red line. As shown, the perceptron ends after finding the correct
    classifier marked in green.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了感知器的训练状态。每个错误的分类器都用红线表示。如图所示，感知器在找到标记为绿色的正确分类器后结束。
- en: 'A zoomed-in view of the final separator can be seen as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，可以查看最终分隔符的放大视图：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The plot looks as shown in the following figure:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图表如下所示：
- en: '![Perceptron](img/00054.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/00054.jpeg)'
- en: The correct classifier function as found by perceptron
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器找到的正确分类器函数
- en: Families of algorithms
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法系列
- en: There are tons of algorithms in the machine learning universe and more are devised
    each year. There is tremendous research happening in this space and hence the
    ever increasing list of algorithms. It is also a fact that the more these algorithms
    are being used, the more improvements in them are being discovered. Machine learning
    is one space where industry and academia are running hand in hand.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域中存在大量的算法，每年都有新的算法被设计出来。在这个领域正在进行着大量的研究，因此算法列表不断增长。事实上，这些算法被使用得越多，对其改进的发现也越多。机器学习是一个工业和学术界紧密合作的空间。
- en: But, as Spider-Man was told that *with great power comes great responsibility*,
    the reader should also understand the responsibility at hand. With so many algorithms
    available, it is necessary to understand what they are and where they fit. It
    can feel overwhelming and confusing at first but that is when categorizing them
    into families helps.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，正如蜘蛛侠被告知的那样，“能力越大，责任越大”，读者也应该理解手头的责任。在如此多的算法可用的情况下，了解它们是什么以及它们适合哪里是必要的。一开始可能会感到压倒性和困惑，但这时将它们分类到系列中会很有帮助。
- en: Machine learning algorithms can be categorized in many ways. The most common
    way is to group them into supervised learning algorithms and unsupervised learning
    algorithms.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以根据多种方式进行分类。最常见的方法是将它们分为监督学习算法和无监督学习算法。
- en: Supervised learning algorithms
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习算法
- en: Supervised learning refers to algorithms which are trained on a predefined data
    set called the training data set. The training data set is usually a two element
    tuple consisting of an input element and a desired output element or signal. In
    general, the input element is a vector. The supervised learning algorithm uses
    the training data set to produce the desired function. The function so produced
    (or rather inferred) is then utilized to correctly map new data, better termed
    as the test data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是指那些在预定义的数据集上训练的算法，这个数据集被称为训练数据集。通常，训练数据集是一个包含输入元素和期望输出元素或信号的二元组。一般来说，输入元素是一个向量。监督学习算法使用训练数据集来产生期望的函数。然后，产生的函数（或更确切地说，推断出的函数）被用来正确地映射新数据，更准确地说是测试数据。
- en: An algorithm which has learnt well will be able to correctly determine the outputs
    for unseen data in a reasonable way. This brings in the concepts of generalization
    and overfitting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个学习良好的算法能够以合理的方式正确确定未见数据的结果。这引入了泛化和过拟合的概念。
- en: Briefly, generalization refers to the concept wherein an algorithm generalizes
    the desired function based upon the (limited) training data to handle unseen data
    in a correct manner. Overfitting is exactly the opposite concept of generalization,
    wherein an algorithm infers a function such that it maps exactly to the training
    data set (including the noise). This may result in huge errors when the function
    learnt by the algorithm is checked against new/unseen data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，泛化是指算法根据（有限的）训练数据泛化所需函数，以正确处理未见数据的概念。过拟合是泛化的相反概念，其中算法推断一个函数，使其正好映射到训练数据集（包括噪声）。当算法学到的函数与新的/未见的数据进行比较时，这可能导致巨大的误差。
- en: Both generalization and overfitting revolve around the random errors or noise
    in the input data. While generalization tries to minimize the effect of noise,
    overfitting does the opposite by fitting in noise as well.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化和过拟合都围绕着输入数据中的随机误差或噪声。虽然泛化试图最小化噪声的影响，但过拟合则相反，通过拟合噪声来实现。
- en: 'Problems to be solved using supervised methods can be divided into the following
    steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用监督方法要解决的问题可以分为以下步骤：
- en: '**Prepare training data**: Data preparation is the most important step for
    all machine learning algorithms. Since supervised learning utilizes labeled input
    data sets (data sets which consist of corresponding outputs for given inputs),
    this step becomes even more important. This data is usually labeled by human experts
    or from measurements.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备训练数据**：数据准备是所有机器学习算法最重要的步骤。由于监督学习使用标记的输入数据集（包含给定输入的对应输出的数据集），这一步骤变得更加重要。这些数据通常由人类专家或测量标记。'
- en: '**Prepare the model**: The model is the representation of the input data set
    and the learnt pattern. The model representation is affected by factors such as
    input features and the learning algorithm itself. The accuracy of the inferred
    function also depends on how this representation is formed.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备模型**：模型是输入数据集和学到的模式的表示。模型表示受输入特征和学习算法本身等因素的影响。推断函数的准确性也取决于这种表示是如何形成的。'
- en: '**Choose an algorithm**: Based on the problem being solved and the input information,
    an algorithm is then chosen to learn and solve the problem.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择算法**：根据要解决的问题和输入信息，然后选择一个算法来学习和解决问题。'
- en: '**Examine and fine tune**: This is an iterative step where the algorithm is
    run on the input data set and the parameters are fine tuned to achieve the desired
    level of output. The algorithm is then tested on the test data set to evaluate
    its performance and measure the error.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查和微调**：这是一个迭代步骤，其中算法在输入数据集上运行，并对参数进行微调以达到期望的输出水平。然后，算法在测试数据集上测试以评估其性能和测量误差。'
- en: 'Under supervised learning, two major sub categories are:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，有两个主要的子类别：
- en: '**Regression based machine learning**: Learning algorithms which help us answer
    quantitative questions such as how many? or how much? The outputs are generally
    continuous values. More formally, these algorithms predict the output values for
    unseen/new data based on the training data and the model formed. The output values
    are continuous in this case. Linear regression, multivariate regression, regression
    trees, and so on are a few supervised regression algorithms.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于回归的机器学习**：帮助回答诸如有多少？或有多少？等定量问题的学习算法。输出通常是连续值。更正式地说，这些算法根据训练数据和形成的模型预测未见/新数据的输出值。在这种情况下，输出值是连续的。线性回归、多元回归、回归树等是一些监督回归算法。'
- en: '**Classification based machine learning**: Learning algorithms which help us
    answer objective questions or yes-or-no predictions. For example, questions such
    as is this component faulty? or can this tumor cause cancer? More formally, these
    algorithms predict the class labels for unseen or new data based upon the training
    data and model formed. **Support Vector Machines** (**SVM**), decision trees,
    random forests, and so on are a few commonly used supervised classification algorithms.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于分类的机器学习**：帮助回答客观问题或是与否预测的学习算法。例如，像这个组件是否损坏？或这个肿瘤是否会导致癌症？等问题的预测。更正式地说，这些算法根据训练数据和形成的模型预测未见或新数据的类别标签。**支持向量机**（**SVM**）、决策树、随机森林等是一些常用的监督分类算法。'
- en: Let us look at some supervised learning algorithms in detail.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看一些监督学习算法。
- en: Linear regression
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: Regression, as mentioned previously, helps us answer quantitative questions.
    Regression has its roots in the statistics domain. Researchers use a linear relationship
    to predict the output value `Y` for a given input value `X`. This linear relationship
    is called a linear regression or regression line.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，回归帮助我们回答定量问题。回归的根源在统计学领域。研究人员使用线性关系来预测给定输入值`X`的输出值`Y`。这种线性关系称为线性回归或回归线。
- en: 'Mathematically, linear regression is represented as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，线性回归表示为：
- en: '![Linear regression](img/00055.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00055.jpeg)'
- en: Where, `b[0]` is the intercept or the point where the line crosses the `y` axis.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`b[0]`是截距，即直线与`y`轴相交的点。
- en: '`b[1]` is the slope of the line, that is, the change in `y` over change in
    `x`.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`b[1]`是直线的斜率，即`y`随`x`变化的改变量。'
- en: The preceding equation is pretty similar to how a straight line is represented
    and hence the name linear regression.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程与表示直线的方式非常相似，因此得名线性回归。
- en: Now, how do we decide which line we fit for our input so that it predicts well
    for unknown data? Well, for this we need an error measure. There can be various
    error measures; the most commonly used is the **least squares method**.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何决定为我们的输入数据拟合哪条线，以便它能很好地预测未知数据？嗯，为此我们需要一个误差度量。可以有各种误差度量；最常用的是**最小二乘法**。
- en: 'Before we define the least squares method, we first need to understand the
    term residual. Residual is simply the deviation of Y from a fitted value. Mathematically:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义最小二乘法之前，我们首先需要理解残差这个术语。残差简单地是Y与拟合值的偏差。数学上：
- en: '![Linear regression](img/00056.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00056.jpeg)'
- en: Where, `ŷ[i]` is the deviated value of `y`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`ŷ[i]`是`y`的偏差值。
- en: The least squares method states that the most optimal fit of a model to data
    occurs when the sum of the squares of residuals is minimum.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘法表明，当残差的平方和最小时，模型与数据的最佳拟合发生。
- en: 'Mathematically:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上：
- en: '![Linear regression](img/00057.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00057.jpeg)'
- en: We use calculus to minimize the sum of squares for residuals and find the corresponding
    coefficients.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用微积分来最小化残差平方和，并找到相应的系数。
- en: Now that we understand linear regression, let us take a real world example to
    see it in action.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了线性回归，让我们用一个现实世界的例子来看看它是如何应用的。
- en: Suppose we have data related to the height and weight of school children. The
    data scientist in you suddenly starts thinking about whether there is any relation
    between the weights and heights of these children. Formally, could the weight
    of a child be predicted based upon his/her given height?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些与学校儿童身高和体重相关的数据。你内心的数据科学家突然开始思考这些儿童的体重和身高之间是否存在任何关系。正式来说，一个孩子的体重能否根据其给定的身高进行预测？
- en: To fit in linear regression, the first step is to understand the data and see
    whether there is a correlation between the two variables (`weight` and `height`).
    Since in this case we are dealing with just two dimensions, visualizing the data
    using a scatter plot will help us understand it quickly. This will also enable
    us to determine if the variables have some linear relationship or not.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应线性回归，第一步是理解数据并查看两个变量（`体重`和`身高`）之间是否存在相关性。由于在这种情况下我们只处理两个维度，使用散点图可视化数据将帮助我们快速理解它。这还将使我们能够确定变量之间是否存在某种线性关系。
- en: Let us prepare our data first and visualize it on a scatter plot along with
    the correlation coefficient.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先准备我们的数据，并在散点图上可视化它，同时显示相关系数。
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Output:**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The scatter plot looks like this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图看起来是这样的：
- en: '![Linear regression](img/00058.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00058.jpeg)'
- en: Figure displaying data points across weight and height dimensions
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 显示体重和身高维度数据点的图
- en: The preceding scatter plot proves that our intuition about weight and height
    having a linear relationship was correct. This can further be confirmed using
    the correlation function, which gives us a value of `0.88`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 上述散点图证明了我们对体重和身高之间具有线性关系的直觉是正确的。这可以通过相关函数进一步确认，它给出了`0.88`的值。
- en: Time to prepare the model for our data set! We use the inbuilt utility `lm`
    or the linear model utility to find the coefficients `b[0]` and `b[1]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候为我们的数据集准备模型了！我们使用内置的`lm`或线性模型实用工具来找到系数`b[0]`和`b[1]`。
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output looks like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来是这样的：
- en: '![Linear regression](img/00059.jpeg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00059.jpeg)'
- en: You can experiment a bit more to find out more details calculated by the `lm`
    utility using the following commands. We encourage you to go ahead and try these
    out.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下命令进一步实验，以找出`lm`实用工具计算出的更多细节。我们鼓励你继续尝试这些命令。
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As the final piece, let us visualize the regression line on our scatter plot
    itself.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一部分，让我们在我们的散点图上可视化回归线。
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The scatter plot looks like the following figure:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图看起来如下所示：
- en: '![Linear regression](img/00060.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00060.jpeg)'
- en: Scatter plot with regression calculated regression line
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 带有计算回归线的散点图
- en: Thus, we saw how the relationship between two variables can be identified and
    predictions can be made using a few lines of code. But we are not done yet. There
    are a couple of caveats which the reader must understand before deciding whether
    to use linear regression or not.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到了如何通过几行代码识别两个变量之间的关系并做出预测。但我们还没有完成。在决定是否使用线性回归之前，读者必须理解一些注意事项。
- en: 'Linear regression can be used to predict the output values for given inputs,
    if and only if:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果且仅当给定输入的输出值可以预测时，线性回归才能使用：
- en: The scatter plot forms a linear pattern
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 散点图形成线性模式
- en: The correlation between them is moderate to strong (beyond `0.5` or `-0.5`)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们之间的相关性是中等到强的（超过`0.5`或`-0.5`）
- en: Cases where only one of the previous two conditions are met can lead to incorrect
    predictions or altogether invalid models. For example, if we only check the correlation
    and find it to be strong and skip the step where we look at the scatter plot,
    then that may lead to invalid predictions as you might have tried to fit a straight
    line when the data itself is following a curved shape (note that curved data sets
    can also have high correlation values and hence the mistake).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只满足上述两个条件中的一个，可能会导致错误的预测或完全无效的模型。例如，如果我们只检查相关性并发现它很强，而跳过了查看散点图的步骤，那么这可能会导致无效的预测，因为你可能试图拟合一条直线，而数据本身是遵循曲线形状的（注意，曲线数据集也可以有很高的相关性值，因此这个错误）。
- en: It is important to remember that **correlation does not imply causation**. In
    simple words, correlation between two variables does not necessarily imply that
    one causes the other. There might be a case that cause and effect are indirectly
    related due to a third variable termed as the cofounding variable. The most common
    example used to describe this issue is the relationship between shoe size and
    reading ability. From the survey data (if one existed!) it could be inferred that
    larger shoe sizes relate to higher reading ability, but this clearly does not
    mean that large feet cause good reading skills to be developed. It might be rather
    interesting to note that young children have small feet and have not yet been
    taught to read. In such a case, the two variables are more accurately correlated
    with age.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住**相关性不等于因果关系**。简单来说，两个变量之间的相关性并不一定意味着一个导致另一个。可能存在一种情况，由于一个被称为共同变量的第三变量，因果关系是间接相关的。用来描述这个问题的最常见例子是鞋码与阅读能力之间的关系。从调查数据（如果有的话！）可以推断出，较大的鞋码与较高的阅读能力相关，但这显然并不意味着大脚会导致良好的阅读技能。可能更有趣的是注意到，年幼的孩子脚小，还没有被教过阅读。在这种情况下，这两个变量更准确地与年龄相关。
- en: You should now be coming to similar conclusion about the weight to height example
    we used earlier. Well yes, the earlier example also suffers from similar fallacy,
    yet it served as an easy to use scenario. Feel free to look around and model cases
    which do not suffer from this.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该对之前我们使用的体重与身高示例得出类似的结论。嗯，之前的例子也存在着类似的谬误，但它作为一个易于使用的场景。请随意四处看看，并建模那些不受此影响的案例。
- en: Linear regression finds application in the field of finance where it is used
    for things such as quantification of risks on investments. It is also used widely
    in the field of economics for trendline analysis and so on.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归在金融领域得到应用，用于投资风险的量化等。它也在经济学领域被广泛用于趋势线分析等。
- en: Apart from linear regression, **logistic regression**, **stepwise regression**,
    **Multivariate Adaptive** **Regression Splines** (**MARS**), and others are a
    few more supervised regression learning algorithms.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了线性回归之外，**逻辑回归**、**逐步回归**、**多元自适应回归样条**（**MARS**）和其他一些监督回归学习算法。
- en: K-Nearest Neighbors (KNN)
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-最近邻（KNN）
- en: K-Nearest Neighbors or KNN algorithms are amongst the simplest algorithms from
    an implementation and understanding point of view. They are yet another type of
    supervised learning algorithm which helps us classify data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现和理解的角度来看，K-Nearest Neighbors 或 KNN 算法是最简单的算法之一。它们是另一种监督学习算法，帮助我们分类数据。
- en: KNN can be easily described using the quote *like and like strike together—Plato*,
    that is, similar things are likely have similar properties. KNN utilizes this
    very concept to label a data point based upon its similarity with respect to its
    neighbors.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 可以用引用 *“同类相吸”——柏拉图* 来简单描述，即相似的事物很可能具有相似的性质。KNN 利用这一概念根据其与邻居的相似性来标记数据点。
- en: Formally, KNN can be described as the process to classify unlabeled (or unseen)
    data points by assigning them the class of the most similar labeled data points
    (or training samples).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，KNN 可以描述为通过将未标记（或未见）的数据点分配给最相似标记数据点（或训练样本）的类别来对它们进行分类的过程。
- en: KNN is a supervised learning algorithm. Hence, it begins with a training data
    set of examples which are classified into different classes. The algorithm then
    picks each data point in the test data set and, based upon a chosen similarity
    measure, identifies its *k* nearest neighbors (where k is specified in advance).
    The data point is then assigned the class of the majority of the k nearest neighbors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 是一种监督学习算法。因此，它从一个被分类为不同类别的训练数据集开始。然后，算法选择测试数据集中的每个数据点，并根据选择的相似度度量，确定其 *k*
    个最近邻（其中 k 是预先指定的）。然后，数据点被分配给 k 个最近邻中的大多数的类别。
- en: 'The trick up the KNN''s sleeve is the similarity measure. There are various
    similarity measures available to us. The decision to choose one is based on the
    complexity of the problem, the type of data, and so on. Euclidean distance is
    one such measure which is widely used. Euclidean distance is the shortest direct
    route between two points. Mathematically it is given as:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 的秘诀在于相似度度量。我们有各种相似度度量可供选择。选择哪种度量取决于问题的复杂性、数据的类型等因素。欧几里得距离就是这样一种广泛使用的度量，它是两点之间最短的直接路线。数学上，它表示为：
- en: '![K-Nearest Neighbors (KNN)](img/00061.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![K-Nearest Neighbors (KNN)](img/00061.jpeg)'
- en: Manhattan distance, Cosine distance, and Minkowski distance are some other types
    of distance measures which can be used for finding the nearest neighbors.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离、余弦距离和闵可夫斯基距离是其他一些可以用于寻找最近邻的距离度量。
- en: The next parameter for KNN algorithm is the `k` in the K-Nearest Neighbors.
    The value of k determines how well the KNN model generalizes to the test data.
    The balance between overfitting and underfitting the training data depends upon
    the value of `k`. With slight deliberation, it is easy to understand that a large
    `k` will minimize the impact of variance caused by noisy data, but at the same
    time, it will also undermine small but important patterns in the data. This problem
    is called the **bias-variance tradeoff**.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: KNN 算法的下一个参数是 K-Nearest Neighbors 中的 `k`。`k` 的值决定了 KNN 模型对测试数据的泛化程度。训练数据过拟合和欠拟合之间的平衡取决于
    `k` 的值。经过稍微思考，很容易理解较大的 `k` 将最小化由噪声数据引起的方差的影响，但同时也将削弱数据中的小但重要的模式。这个问题被称为 **偏差-方差权衡**。
- en: The optimal value of k, even though hard to determine, lies between the extremes
    of `k=1` to `k=total number of training samples`. A common practice is to set
    the value of `k` equal to the square root of training instances, usually between
    3 and 10\. Though a common practice, the value of `k` is dependent on the complexity
    of the concept to be learned and the number of training examples.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: k 的最佳值，尽管难以确定，但位于 `k=1` 到 `k=训练样本总数` 的极端之间。一种常见的做法是将 `k` 的值设置为训练实例的平方根，通常在 3
    到 10 之间。尽管这是一种常见的做法，但 `k` 的值取决于要学习概念复杂性和训练样本数量。
- en: 'The next step in pursuit of the KNN algorithm is preparation of data. Features
    used to prepare the input vectors should be on similar scales. The rationale for
    this step is that the distance formula is dependent on how the features are measured.
    For example, if certain features have large range of values as compared to others,
    the distance measurements will then be dominated by such measures. The method
    of scaling features to a similar scale is called **normalization**. Very much
    like the distance measure, there are various normalization methods available.
    One such method is min-max normalization, given mathematically as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 追求KNN算法的下一步是准备数据。用于准备输入向量的特征应该处于相似的尺度。这一步骤的合理性在于距离公式依赖于特征的测量方式。例如，如果某些特征与其他特征相比具有较大的值范围，那么距离测量将主要由这些测量值决定。将特征缩放到相似尺度的方法称为**归一化**。与距离测量一样，有各种归一化方法可用。其中一种方法是min-max归一化，数学上表示为：
- en: '![K-Nearest Neighbors (KNN)](img/00062.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![K-Nearest Neighbors (KNN)](img/00062.jpeg)'
- en: 'Before we begin with our example to understand KNN, let us outline the steps
    to be performed to execute KNN:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始示例以了解KNN之前，让我们概述执行KNN要执行的步骤：
- en: '**Collect data and explore data**: We need to collect data relevant to the
    concept to be learnt. We also need to explore the data to understand various features,
    know the range of their values, and determine the class labels.'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集数据并探索数据**：我们需要收集与要学习概念相关的数据。我们还需要探索数据，以了解各种特征，了解其值范围，并确定类标签。'
- en: '**Normalize data**: As discussed previously, KNN''s dependence on distance
    measure makes it very important that we normalize the data to remove any inconsistency
    or bias in calculations.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**归一化数据**：如前所述，KNN对距离测量的依赖性使得我们非常重要的是要对数据进行归一化，以消除计算中的任何不一致性或偏差。'
- en: '**Create training and test data sets**: Since it is important to learn a concept
    and prepare a model which generalizes to acceptable levels for unseen data, we
    need to prepare training and test data sets. The test data set, even though labeled,
    is used to determine the accuracy and the ability of the model to generalize the
    concept learnt. A usual practice is to divide the input sample into two-third
    and one-third portions for training and test data sets respectively. It is equally
    important that the two data sets are a good mix of all the class labels and data
    points, that is both the data sets should be representative subsets of the full
    data.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建训练和测试数据集**：由于学习一个概念并准备一个可以推广到未见数据可接受水平的模型非常重要，我们需要准备训练和测试数据集。尽管测试数据集已标记，但它用于确定模型的准确性和泛化所学习概念的能力。通常的做法是将输入样本分为三分之二和三分之一，分别用于训练和测试数据集。同样重要的是，这两个数据集应该是所有类标签和数据点的良好混合，也就是说，这两个数据集应该是完整数据的有代表性的子集。'
- en: '**Train the model**: Now that we have all the things in place, we can use the
    training data set, test data set, the labels, and the value of `k` to train our
    model and label the data points in the test data set.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：现在我们已经准备好了所有东西，我们可以使用训练数据集、测试数据集、标签和`k`的值来训练我们的模型，并为测试数据集中的数据点标记标签。'
- en: '**Evaluate the model**: The final step is to evaluate the learnt pattern. In
    this step, we determine how well the algorithm has predicted the class labels
    of the test data set as compared to their known labels. Usually a confusion matrix
    is prepared for the same.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估模型**：最后一步是评估学习到的模式。在这个步骤中，我们确定算法相对于已知标签预测测试数据集类标签的效果如何。通常为此准备一个混淆矩阵。'
- en: Now, let us see KNN in action. The problem at hand is to classify different
    species of flowers based upon certain features. For this particular example, we
    will be using the Iris data set. This data set comes built in with the default
    installation of R.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看KNN的实际应用。当前的问题是根据某些特征对不同的花卉种类进行分类。对于这个特定的例子，我们将使用Iris数据集。这个数据集是R默认安装的一部分。
- en: Collecting and exploring data
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 收集和探索数据
- en: Step one is to collect and explore the data. Let us first gather the data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是收集和探索数据。让我们首先收集数据。
- en: 'To check if your system has the required dataset, type in just the name:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查您的系统是否有所需的数据库，只需输入名称：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you do not have the data set available, no worries! You can download it
    as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有可用的数据集，不用担心！您可以按照以下方式下载：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that we have the data, it is time to explore and understand it. For exploring
    the data set and its attributes, we use the following commands:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，是时候探索和理解它了。为了探索数据集及其属性，我们使用以下命令：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Output:**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Collecting and exploring data](img/00063.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![收集和探索数据](img/00063.jpeg)'
- en: '[PRE13]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Output:**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Collecting and exploring data](img/00064.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![收集和探索数据](img/00064.jpeg)'
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Output:**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Collecting and exploring data](img/00065.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![收集和探索数据](img/00065.jpeg)'
- en: The summary command helps us understand the data in a better way. It clearly
    shows different attributes along with `min`, `max`, `median`, and other such statistics.
    These help us in the coming steps where we might have to scale or normalize the
    data or features.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary`命令帮助我们更好地理解数据。它清楚地显示了不同的属性以及`min`、`max`、`median`和其他此类统计数据。这些有助于我们在接下来的步骤中，可能需要缩放或归一化数据或特征。'
- en: During step one is where we usually label our input data. Since our current
    data set is already labeled, we can skip this step for this example problem. Let
    us visually see how the species are spread. We take help of the famous scatter
    plot again, but this time we use a package called `ggvis`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们通常会对输入数据进行标记。由于我们当前的数据集已经标记，因此对于这个例子，我们可以跳过这一步。让我们通过视觉方式查看物种的分布情况。我们再次求助著名的散点图，但这次我们使用一个名为`ggvis`的包。
- en: 'You can install `ggvis` as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以安装`ggvis`如下：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For visualizing petal widths and lengths for all 3 species, we use the following
    code snippet:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化所有3种物种的花瓣宽度和长度，我们使用以下代码片段：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '[PRE17]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Collecting and exploring data](img/00066.jpeg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![收集和探索数据](img/00066.jpeg)'
- en: The preceding plot clearly shows that there is a high correlation between petal
    widths and lengths for **Iris-setosa** flowers, while it is a little less for
    the other two species.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表清楚地表明，对于**Iris-setosa**花，花瓣宽度和长度之间存在高度相关性，而对于其他两种物种则稍微低一些。
- en: Note
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Try visualizing sepal width versus sepal length as well and see if you can spot
    any correlation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试可视化萼片宽度和萼片长度，看看你是否能发现任何相关性。
- en: Normalizing data
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据归一化
- en: 'The next step is to normalize the data so that all the features are on the
    same scale. As seen from the data exploration step, the values of all the attributes
    are more or less in a comparable range. But, for the sake of this example, let
    us write a min-max normalization function:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据进行归一化，以便所有特征都在相同的尺度上。如数据探索步骤所示，所有属性的值或多或少都在一个可比较的范围内。但是，为了这个例子，让我们编写一个最小-最大归一化函数：
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Remember, normalization does not alter the data, it simply scales it. Therefore,
    even though our data does not require normalization, doing so will not cause any
    harm.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，归一化不会改变数据，它只是缩放它。因此，即使我们的数据不需要归一化，这样做也不会造成任何伤害。
- en: Note
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**注意**'
- en: '**Note**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: In the following steps, we will be using un-normalized data for clarity of output.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用未归一化的数据以使输出更清晰。
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following is a summary of the normalized data frame:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对归一化DataFrame的总结：
- en: '![Normalizing data](img/00067.jpeg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![数据归一化](img/00067.jpeg)'
- en: Creating training and test data sets
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建训练集和测试集
- en: Now that we have our data normalized, we can divide it into training and test
    data sets. We will follow the usual two-third one-third rule of splitting the
    data into two. As mentioned earlier, both data sets should be representative of
    the whole data and hence we need to pick proper samples. We will utilize R's `sample()`
    function to prepare our samples.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据归一化，我们可以将其分为训练集和测试集。我们将遵循通常的三分之二对三分之一的数据分割规则。如前所述，这两个数据集应该代表整个数据，因此我们需要选择合适的样本。我们将利用R的`sample()`函数来准备我们的样本。
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Output:**'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Creating training and test data sets](img/00068.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![创建训练集和测试集](img/00068.jpeg)'
- en: '[PRE21]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Learning from data/training the model
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从数据中学习/训练模型
- en: 'Once we have the data ready in our training and test data sets, we can proceed
    to the next step and learn from the data using KNN. The KNN implementation in
    R is present in the class library. The KNN function takes the following inputs:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在训练集和测试集中准备好了数据，我们就可以进行下一步，使用KNN从数据中学习。R中的KNN实现位于类库中。KNN函数需要以下输入：
- en: '`train`: The data frame containing the training data.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train`：包含训练数据的DataFrame。'
- en: '`test`: The data frame containing the test data.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test`：包含测试数据的DataFrame。'
- en: '`class`: A vector containing the class labels. Also called the factor vector.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class`：包含类别标签的向量。也称为因子向量。'
- en: '`k`: The value of k-nearest neighbors.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：k-最近邻算法中k的值。'
- en: 'For the current case, let us assume the value of `k` to be `3`. Odd numbers
    are usually good at breaking ties. KNN is executed as:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前情况，让我们假设`k`的值为`3`。奇数通常在打破平局时表现良好。KNN的执行如下：
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Output:**'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Learning from data/training the model](img/00069.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![从数据中学习/训练模型](img/00069.jpeg)'
- en: A quick scan of the output shows everything correct except one versicolor label
    amongst virginica's. Though this one was easy to spot, there are better ways to
    evaluate the model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 快速浏览输出显示，除了virginica中的一个versicolor标签之外，所有内容都是正确的。尽管这个标签很容易找到，但还有更好的方法来评估模型。
- en: Evaluating the model
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'This brings us to the last step where we evaluate the model. We do this by
    preparing a confusion matrix or a cross table which helps us understand how the
    predicted labels are with respect to the known labels of the test data. R provides
    us with yet another utility function called `CrossTable()` which is present in
    the `gmodels` library. Let us see the output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这将我们带到了最后一步，即评估模型。我们通过准备一个混淆矩阵或交叉表来完成这项工作，这有助于我们了解预测标签与测试数据的已知标签之间的关系。R提供了一个名为`CrossTable()`的实用函数，该函数位于`gmodels`库中。让我们看看输出：
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Output:**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '![Evaluating the model](img/00070.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![评估模型](img/00070.jpeg)'
- en: From the preceding output we can conclude that the model has labeled one instance
    of virginica as versicolor while all the other test data points have been correctly
    labeled. This also helps us infer that the choice of `k=3` was indeed good enough.
    We urge the reader to try the same example with different values of `k` and see
    the change in results.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以得出结论，该模型将virginica的一个实例标记为versicolor，而所有其他测试数据点都被正确标记。这也帮助我们推断出选择`k=3`确实是足够的。我们敦促读者尝试用不同的`k`值运行相同的示例，并观察结果的变化。
- en: KNN is a simple yet powerful algorithm which makes no assumptions about the
    underlying data distribution and hence can be used in cases where relationships
    between features and classes are complex or difficult to understand.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是一个简单而强大的算法，它不对底层数据分布做出假设，因此可以在特征和类别之间的关系复杂或难以理解的情况下使用。
- en: On the downside, KNN is a resource intensive algorithm as it requires a large
    amount of memory to process the data. Dependency on distance measures and missing
    data requires additional processing which is another overhead with this algorithm.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一方面来看，KNN是一个资源密集型的算法，因为它需要大量的内存来处理数据。对距离度量和缺失数据的依赖需要额外的处理，这也是该算法的另一个开销。
- en: Despite its limitations, KNN is used in a number of real life applications such
    as for text mining, predicting heart attacks, predicting cancer and so on. KNN
    also finds application in the field of finance and agriculture as well.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有其局限性，KNN在许多实际应用中都有使用，例如文本挖掘、预测心脏病发作、预测癌症等等。KNN还应用于金融和农业领域。
- en: Decision trees, random forests, and support vector machines are some of the
    most popular and widely used supervised classification algorithms.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树、随机森林和支持向量机是一些最流行和广泛使用的监督分类算法。
- en: Unsupervised learning algorithms
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习算法
- en: Unsupervised learning refers to algorithms which learn a concept(s) on their
    own. Now that we are familiar with the concept of supervised learning, let us
    utilize our knowledge to understand unsupervised learning.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是指算法自己学习概念（们）。既然我们已经熟悉了监督学习的概念，让我们利用我们的知识来理解无监督学习。
- en: Unlike supervised learning algorithms, which require a labeled input training
    data set, unsupervised learning algorithms are tasked with finding relationships
    and patterns in the data without any labeled training data set. These algorithms
    process the input data to mine for rules, detect patterns, summarize and group
    the data points which helps in deriving meaningful insights, and describing the
    data to the users. In the case of unsupervised learning algorithms, there is no
    concept of training and test data sets. Rather, as mentioned before, input data
    is analyzed and used to derive patterns and relationships.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 与需要标记输入训练数据集的监督学习算法不同，无监督学习算法的任务是在没有任何标记训练数据集的情况下，在数据中寻找关系和模式。这些算法处理输入数据以挖掘规则、检测模式、总结和分组数据点，这有助于得出有意义的见解，并向用户描述数据。在无监督学习算法的情况下，不存在训练和测试数据集的概念。相反，如前所述，输入数据被分析并用于推导模式和关系。
- en: 'Similar to supervised learning, unsupervised learning algorithms can also be
    divided into two main categories:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习类似，无监督学习算法也可以分为两大类：
- en: '**Association rule based machine learning**: These algorithms mine the input
    data to identify patterns and rules. The rules explain interesting relationships
    between the variables in the data set to depict frequent itemsets and patterns
    which occur in the data. These rules in turn help discover useful insights for
    any business or organization from their huge data repositories. Popular algorithms
    include Apriori and FP-Growth.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于关联规则的机器学习**：这些算法挖掘输入数据以识别模式和规则。这些规则解释数据集中变量之间的有趣关系，以描述频繁项集和数据中出现的模式。这些规则反过来又帮助从任何企业或组织的庞大数据存储库中发现有用的见解。流行的算法包括Apriori和FP-Growth。'
- en: '**Clustering based machine learning**: Similar to supervised learning based
    classification algorithms, the main objective of these algorithms is to cluster
    or group the input data points into different classes or categories using just
    features derived from the input data alone and no other external information.
    Unlike classification, the output labels are not known beforehand in clustering.
    Some popular clustering algorithms include k-means, k-medoids, and hierarchical
    clustering.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于聚类的机器学习**：与基于监督学习的分类算法类似，这些算法的主要目标是仅使用从输入数据中提取的特征，将输入数据点聚类或分组到不同的类别或类别中，而不需要其他外部信息。与分类不同，聚类中事先并不知道输出标签。一些流行的聚类算法包括k-means、k-medoids和层次聚类。'
- en: Let us look at some unsupervised learning algorithms.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些无监督学习算法。
- en: Apriori algorithm
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apriori算法
- en: This algorithm which took the world by storm was proposed by Agarwal and Srikant
    in 1993\. The algorithm is designed to handle transactional data, where each transaction
    is a set of items, or itemset. The algorithm in short identifies the item sets
    which are subsets of at least C transactions in the data set.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这个震惊世界的算法由Agarwal和Srikant于1993年提出。该算法旨在处理事务数据，其中每个事务是一组物品或项集。简而言之，该算法识别出数据集中至少有C个事务的子集的项集。
- en: 'Formally, let `┬` be a set of items and `D` be a set of transactions, where
    each transaction `T` is a subset of `┬`. Mathematically:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，设 `┬` 为一个物品集合，`D` 为一个事务集合，其中每个事务 `T` 是 `┬` 的一个子集。数学上：
- en: '![Apriori algorithm](img/00071.jpeg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![Apriori算法](img/00071.jpeg)'
- en: 'Then an association rule is an implication of the form `X → Y`, where a transaction
    `T` contains `X` as a subset of `┬` and:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一个关联规则是形式为 `X → Y` 的蕴涵，其中事务 `T` 包含 `┬` 的一个子集 `X`，并且：
- en: '![Apriori algorithm](img/00072.jpeg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![Apriori算法](img/00072.jpeg)'
- en: The implication `X → Y` holds true in the transaction set `D` with a confidence
    factor `c` if `c%` of the transactions in `D` that contain `X` also contain `Y`.
    The association rule `X → Y` is said to have a support factor of `s` if `s%` of
    transactions in `D` contain `X U Y`. Hence, given a set of transactions `D`, the
    task of identifying association rules implies generating all such rules that have
    confidence and support greater than a user defined thresholds called `minsup`
    (for minimum support threshold) and `minconf` (for minimum confidence threshold).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在事务集合 `D` 中，包含 `X` 的 `c%` 事务也包含 `Y`，则蕴涵 `X → Y` 在事务集合 `D` 中成立，具有置信度因子 `c`。如果
    `s%` 的事务在 `D` 中包含 `X U Y`，则关联规则 `X → Y` 被说成具有支持度因子 `s`。因此，给定一个事务集合 `D`，识别关联规则的任务意味着生成所有具有大于用户定义的阈值（称为
    `minsup`，即最小支持度阈值）和 `minconf`（即最小置信度阈值）的置信度和支持度的规则。
- en: Broadly, the algorithm works in two steps. The first one being identification
    of the itemsets whose occurrence exceeds a predefined threshold. Such itemsets
    are called **Frequent Itemsets**. The second step is to generate association rules
    from the identified frequent itemsets which satisfy the constraints of minimum
    confidence and support.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上，该算法分为两个步骤。第一步是识别出现次数超过预定义阈值的项集。这样的项集被称为**频繁项集**。第二步是从识别出的频繁项集中生成满足最小置信度和支持度约束的关联规则。
- en: 'The same two steps can be explained better using the following pseudo-code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下伪代码更好地解释这两个相同的步骤：
- en: '![Apriori algorithm](img/00073.jpeg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![Apriori算法](img/00073.jpeg)'
- en: Now let us see the algorithm in action. The data set in consideration is the
    UCI machine learning repository's `Adult` data set. The data set contains census
    data with attributes such as gender, age, marital status, native country, and
    occupation, along with economic attributes such as work class, income, and so
    on. We will use this data set to identify if there are association rules between
    census information and the income of the person.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看算法的实际应用。所考虑的数据集是UCI机器学习仓库的`Adult`数据集。该数据集包含人口普查数据，具有性别、年龄、婚姻状况、国籍和职业等属性，以及工作类别、收入等经济属性。我们将使用这个数据集来识别人口普查信息与个人收入之间是否存在关联规则。
- en: The Apriori algorithm is present in the `arules` library and the data set in
    consideration is named `Adult.` It is also available with default R installation.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法存在于`arules`库中，所考虑的数据集命名为`Adult`。它也包含在默认的R安装中。
- en: '[PRE24]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Time to explore our data set and see a few sample records:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候探索我们的数据集并查看一些样本记录了：
- en: '[PRE25]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We get to know that the data set contains some `48k` transactions with 115 columns.
    We also get information regarding the distribution of itemsets based on their
    sizes. The `inspect` function gives us a peek into sample transactions and the
    values each of the columns hold.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到数据集包含一些 `48k` 笔交易，有115列。我们还获得了关于项目集大小分布的信息。`inspect`函数让我们窥视样本交易以及每列所持有的值。
- en: 'Now, let us build some relationships:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们建立一些关系：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The Apriori algorithm uses the `Adult` data set as input to identify rules
    and patterns in the transactional data. On viewing the summary, we can see that
    the algorithm successfully identified `84` rules meeting the support and confidence
    constraints of `50%` and `80%` respectively. Now that we have identified the rules,
    let us see what they are:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori算法使用`Adult`数据集作为输入，以识别交易数据中的规则和模式。通过查看摘要，我们可以看到算法成功识别了满足支持度和置信度约束的`84`条规则，分别为`50%`和`80%`。现在我们已经识别了规则，让我们看看它们是什么：
- en: '![Apriori algorithm](img/00074.jpeg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![Apriori算法](img/00074.jpeg)'
- en: The rules are of the form `X→ Y` where `X` is the `lhs` or left-hand side and
    `Y` is the `rhs` or right-hand side. The preceding image displays corresponding
    confidence and support values as well. From the output we can infer that if people
    are working full time then their chances of facing capital loss is almost none
    (confidence factor 95.8%). Another rule helps us infer that people who work for
    a private employer also have close to no chance of facing a capital loss. Such
    rules can be used in preparing policies or schemes for social welfare, economic
    reforms, and so on.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 规则的形式为`X→ Y`，其中`X`是`lhs`或左侧，`Y`是`rhs`或右侧。前面的图像还显示了相应的置信度和支持度值。从输出中我们可以推断出，如果人们全职工作，那么他们面临资本损失的机会几乎为零（置信度因子95.8%）。另一条规则帮助我们推断，为私营雇主工作的人面临资本损失的机会也几乎为零。这样的规则可以用于制定社会福利、经济改革等政策或方案。
- en: Apart from Apriori, there are other association rule mining algorithms such
    as FP Growth, ECLAT, and many others which have been used for various applications
    over the years.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Apriori之外，还有其他关联规则挖掘算法，如FP Growth、ECLAT等，这些算法多年来被用于各种应用。
- en: K-Means
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-Means
- en: In the world of unsupervised clustering algorithms, the most simple and widely
    used algorithm is K-Means. As we have seen recently, unsupervised learning algorithms
    process the input data without any prior labels or training to come up with patterns
    and relationships. Clustering algorithms in particular help us cluster or partition
    the data points.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督聚类算法的世界中，最简单且最广泛使用的算法是K-Means。正如我们最近所看到的，无监督学习算法在没有任何先前的标签或训练的情况下处理输入数据，以找出模式和关系。特别是聚类算法帮助我们聚类或划分数据点。
- en: By definition, clustering refers to the task of grouping objects into groups
    such that elements of a group are more similar to each other than those in other
    groups. K-Means does the same in an unsupervised manner.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，聚类是指将对象分组到一组中，使得该组内的元素彼此之间比其他组中的元素更相似。K-Means以无监督的方式执行同样的操作。
- en: Mathematically, given a set of `n` observations `{x1,x2,…,xn}`, where each observation
    is a *d* dimensional vector, the algorithm tries to partition these *n* observations
    into `k (≤ n)` sets by minimizing an objective function.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，给定一个包含 `n` 个观察值的集合 `{x1,x2,…,xn}`，其中每个观察值是一个 *d* 维向量，算法试图通过最小化一个目标函数将这些
    *n* 个观察值划分为 `k (≤ n)` 个集合。
- en: As with the other algorithms, there can be different objective functions. For
    the sake of simplicity, we will use the most widely used function called the **with-in
    cluster sum of squares** or **WCSS function**.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他算法一样，可以有不同目标函数。为了简单起见，我们将使用最广泛使用的函数，称为**簇内平方和**或**WCSS函数**。
- en: '![K-Means](img/00075.jpeg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means](img/00075.jpeg)'
- en: Here `μ[i]` is the mean of points in the partition `S[i]`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`μ[i]`是分区`S[i]`中点的均值。
- en: The algorithm follows a simple two step iterative process, where the first step
    is called the assignment step, followed by the update step.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法遵循一个简单的两步迭代过程，其中第一步称为分配步骤，随后是更新步骤。
- en: 'Initialize by setting the means for `k` partitions: `m1,m2…mk`'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过设置`k`个分区的均值来初始化：`m1,m2…mk`
- en: 'Until the mean does not change or the change is lower than a certain threshold:'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直到均值不再变化或变化低于某个阈值：
- en: '**Assignment step**: Assign each observation to a partition for which the with-in
    cluster sum of squares value is minimum, that is, assign the observation to a
    partition whose mean is closest to the observation.'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配步骤**：将每个观测值分配给一个使簇内平方和最小的分区，即分配给一个均值最接近观测值的分区。'
- en: '**Update step**: For `i` in `1 to k`, update each mean `mi` based on all the
    observations in that partition.'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新步骤**：对于`i`在`1到k`之间，根据该分区中的所有观测值更新每个均值`mi`。'
- en: The algorithm can use different initialization methods. The most common ones
    are Forgy and Random Partition methods. I encourage you to read more on these.
    Also, apart from the input data set, the algorithm requires the value of `k`,
    that is the number of clusters to be formed. The optimal value may depend on various
    factors and is generally decided based on the use case.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法可以使用不同的初始化方法。最常见的是Forgy和随机分区方法。我鼓励你阅读更多关于这些内容。此外，除了输入数据集，算法还需要`k`的值，即要形成的聚类数量。最佳值可能取决于各种因素，通常根据用例来决定。
- en: Let us see the algorithm in action.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看算法的实际应用。
- en: We will again use the Iris flower data set which we already used for the KNN
    algorithm. For KNN we already had the species labeled and then tried to learn
    and classify the data points in the test data set into correct classes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用Iris花卉数据集，我们之前已经用它来为KNN算法。对于KNN，我们已经有物种标签，然后尝试在测试数据集中学习并分类数据点到正确的类别。
- en: With K-Means, we also aim to achieve the same partitioning of the data but without
    any labeled training data set (or supervision).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K-Means，我们同样旨在实现相同的数据分区，但没有任何标记的训练数据集（或监督）。
- en: '[PRE27]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now that we have the output from `k-means`, let us see how well it has partitioned
    the various species. Remember, `k-means` does not have partition labels and simply
    groups the data points.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了`k-means`的输出，让我们看看它如何将各种物种进行分区。记住，`k-means`没有分区标签，只是将数据点分组。
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Output:**'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '![K-Means](img/00076.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means](img/00076.jpeg)'
- en: 'The output shows that the species setosa matches cluster label 2, versicolor
    matches label 3, and so on. Visually, it is easy to make out how the data points
    are clustered:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示，setosa物种与聚类标签2匹配，versicolor与标签3匹配，等等。从视觉上看，很容易看出数据点是如何分组的：
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![K-Means](img/00077.jpeg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![K-Means](img/00077.jpeg)'
- en: K-Means finds wide usage in areas like computer graphics for color quantization;
    it is combined with other algorithms and used for natural language processing,
    computer vision, and so on.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: K-Means在计算机图形学中的颜色量化等领域得到广泛应用；它与其他算法结合使用，用于自然语言处理、计算机视觉等。
- en: There are different variations of `k-means` (R itself provides three different
    variations). Apart from k-means, other unsupervised clustering algorithms are
    k-medoids, hierarchical clustering, and others.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`k-means`有不同变体（R本身提供了三种不同的变体）。除了k-means，其他无监督聚类算法还包括k-medoids、层次聚类等。'
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Through this chapter, we formally defined the concept of machine learning. We
    talked about how a machine learning algorithm actually learns a concept. We touched
    upon various other concepts such as generalization, overfitting, training, testing,
    frequent itemsets, and so on. We also learnt about the families of machine learning
    algorithms. We went through different machine learning algorithms to understand
    the magic under the hood, along with their areas of application.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们正式定义了机器学习的概念。我们讨论了机器学习算法实际上是如何学习一个概念的。我们还触及了各种其他概念，如泛化、过拟合、训练、测试、频繁项集等。我们还了解了机器学习算法的家族。我们探讨了不同的机器学习算法，以了解其背后的魔法以及它们的应用领域。
- en: With this knowledge we are ready to solve some real world problems and save
    the world.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些知识，我们准备好解决一些现实世界的问题并拯救世界。
- en: The coming few chapters build on the concepts in this chapter to solve specific
    problems and use cases. Get ready for some action!
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几章将基于本章的概念来解决具体问题和用例。准备好迎接一些行动吧！
