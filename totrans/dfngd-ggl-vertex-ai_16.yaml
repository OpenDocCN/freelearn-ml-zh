- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vision-Based Defect Detection System – Machines Can See Now!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Computer Vision** (**CV**) is a field of artificial intelligence concerned
    with giving machines the ability to analyze and extract meaningful information
    from digital images, videos, and other visual input, as well as take actions or
    make recommendations based on the extracted information. Decades of research in
    the field of CV have led to the development of powerful **Machine Learning** (**ML**)-based
    vision algorithms that are capable of classifying images into some pre-defined
    categories, detecting objects from images, understanding written content from
    digital images, and detecting actions being performed in videos. Such vision algorithms
    have given businesses and organizations the ability to analyze large amounts of
    digital content (images and videos) and also automate processes to make instant
    decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: CV-based algorithms have changed the way we interact with smart devices in our
    day-to-day life – for example, we can now unlock smartphones by just showing our
    face, and photo editing apps today can make us look younger or older. Another
    important use case of applying CV-based ML algorithms is defect detection. ML
    algorithms can be leveraged to analyze visual input and detect defects in product
    images, which can be quite useful for manufacturing industries.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will develop a real-world defect detection solution using
    deep learning on Google Cloud. We will also see how to deploy our vision-based
    defect detection model as a Vertex AI endpoint so that it can be utilized for
    online prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Vision-based defect detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a vision model to a Vertex AI endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting online predictions from a vision model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code samples used in this chapter can be found at the following GitHub
    address: [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter16).'
  prefs: []
  type: TYPE_NORMAL
- en: Vision-based defect detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CV is capable nowadays of detecting visual defects on object surfaces or inconsistencies
    in their designs (such as dents or scratches on a car body), by just analyzing
    their digital photographs or videos. The manufacturing industry can leverage CV
    algorithms to automatically detect and remove low-quality and defected products
    from being packed and reaching customers. There are many possible ways to detect
    defects within digital content using CV-based algorithms. One simple idea is to
    solve defect detection as a classification problem, where a vision model can be
    trained to classify images such as *good* or *defected*. A more complex defect
    detection system will also locate the exact area of an image with a defect. The
    problem of identifying and locating visual defects can be solved using object-detection
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will build and train a simple defect detection system step
    by step. In this example, we will use ML classification as a mechanism to detect
    visually defected products. Let’s explore the example.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this experiment, we have downloaded an open source dataset from Kaggle.
    The dataset has over a thousand colored images of glass bangles. These images
    contain bangles of different sizes and colors and can be classified into three
    major categories, based on their manufacturing quality and damages – good, defected,
    and broken. Defected bangles may have a manufacturing defect such as invariable
    width or improper circular shape, while broken bangles would have some piece of
    the circle missing. In this experiment, we will utilize some of these images to
    train an ML classification model and test it on a few of the unseen samples. Images
    are already separated into the aforementioned categories, so there’s no need for
    manual data annotation. The dataset used in this experiment can be downloaded
    from the Kaggle link provided in the Jupyter Notebook corresponding to this experiment.
    The GitHub location of the code samples is presented at the beginning of this
    chapter in the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: We have already downloaded and extracted the dataset into the same directory
    as our Jupyter Notebook. Now, we can start looking at some of the image samples.
    Let’s get into the coding part now.
  prefs: []
  type: TYPE_NORMAL
- en: Importing useful libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first step is to import useful Python packages for our experiment. Here,
    `cv2` refers to the OpenCV library, which has lots of prebuilt functionalities
    for dealing with images and other CV tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s look at the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and verifying data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s load all the image file paths into three separate lists, one for
    each category – good, defected, and broken. Keeping three separate lists will
    make it easier to keep track of image labels. Let’s also print the exact number
    of images within each category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have 520 good-quality images, 244 defected images, and 316 broken bangle
    images in total. Next, let’s verify a few samples from each category by plotting
    them using `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Checking few samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will randomly choose a few image paths from each of the previously
    discussed lists and plot them with their category name as their title.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot a few good bangle images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will plot a few random defective bangle pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will also plot some broken bangle images in a similar way so that
    we can see all three categories visually and learn more about the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding scripts is shown in *Figure 16**.1* where each row
    represents a few random samples from each of the aforementioned categories. The
    category name is present in the title of images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – Bangle image samples from each category – good, defected, and
    broken](img/B17792_16_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.1 – Bangle image samples from each category – good, defected, and
    broken
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding figure, good bangles are the ones that look like a
    perfect circle with an even width, defected bangles may have some uneven width
    or surface, while broken bangles are easily distinguishable, as they are missing
    some part of their circular shape. So, differentiating between good and defected
    bangles can be a bit challenging, but differentiating both of these categories
    from the broken bangles should be easier for the algorithm. Let’s see how it goes
    in the next few sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will prepare data for our **TensorFlow** (**TF**)-based deep
    learning model. A TF-based model requires all the inputs to have the same shape,
    so the first step would be to make all the input images the same shape. We will
    also downgrade the quality of images a bit so that they are more memory-efficient
    while reading them and performing calculations over them during training. We have
    to keep one thing in mind – we can’t degrade the data quality significantly such
    that it becomes hard for the model to find any visual clues to detect defected
    surfaces. For this experiment, we will resize each image to a 200x200 resolution.
    Secondly, as we are only concerned about finding defects, the color of the image
    is not important to us. So, we will convert all the colored images into grayscale
    images, as it reduces the images’ channels from 3 to 1; thus, the image size becomes
    a third of its original size. Finally, we will convert them into NumPy arrays,
    as TF models require the input tensors to be NumPy arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the pre-processing steps that we will follow for each image
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading the colored image using the OpenCV library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resizing an image to a fixed size (200 x 200)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting image to grayscale (black and white)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the list of images into NumPy arrays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding one channel dimensions, as required by convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s first follow these steps for good bangle images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will pre-process the defected bangle images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will follow the same pre-processing steps for the broken bangle
    images. We will also print the final shapes of NumPy arrays for each category
    of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Each image array now will have the shape of 200x200x1, and the first dimension
    will represent the total number of images. The following is the output of the
    previous script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s split the data into training and test partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data into train and test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As our data is now ready and compatible with TF-model format, we just need to
    perform one last important step of splitting the data into two partitions – train
    and test. The *train* partition will be shown to the model for learning purposes
    during training of the model, and the *test* partition will be kept separate and
    will not contribute in the model parameters’ update. Once the model is trained,
    we will check how well it performs on the unseen test partition of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As our test data should also have a significant number of samples from each
    category, we will divide each of the image categories into train and test partitions
    and, finally, merge all the train and test partitions together. We will utilize
    the first 75% of the images from each category array for training and the rest
    of the 25% images for testing purposes, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The previously defined code also prints the shape of the train and test partitions
    just for data verification purposes. The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Our training and test data is almost ready now; we just need corresponding labels
    for model training. In the next step, we will create label arrays for both partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Final preparation of training and testing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have three pairs of train and test partitions (one for each category),
    let’s combine them into a single pair of train and test partitions for model training
    and testing purpose. After concatenating these NumPy arrays, we will also perform
    reshuffling so that images from each of the categories are well-mixed. This is
    important, as during training we will only send small batches to the model, so
    for smooth training of the model, each batch should have samples from all the
    classes. As test data is kept separate, there is no need to shuffle it.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we also need to create corresponding label arrays as well. As
    ML algorithms only support numeric data, we need to encode our output categories
    into some numeric values. We can represent our three categories with three numbers
    – 0, 1, and 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following label mapping rule to encode our categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Good – 0**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defected – 1**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Broken –** **2**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet concatenates all the training partitions into a
    single train partition and also creates a label array, using the aforementioned
    mapping rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will concatenate all the test partitions into a single test partition.
    We will also create a label array for our test partition, which will help us to
    check the accuracy metrics of our trained model. Here, we also print the final
    shapes of the train and test partitions, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this script shows that we have 810 training images and 270 test
    images in total, as shown in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed before, it is very important to shuffle our training partition,
    ensuring that images from each category are well-mixed and each batch will have
    a good variety. The important thing to keep in mind while shuffling is that we
    also need to shuffle the label array accordingly to avoid any data label mismatches.
    For this purpose, we have defined a Python function that shuffles two given arrays
    in unison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Our training and testing dataset is now all set. We can now move to the model
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: TF model architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will define a model architecture for our TF-based deep learning
    classification model. As we are dealing with images in this experiment, we will
    utilize **Convolutional Neural Network** (**CNN**) layers to learn and extract
    important features from training images. CNNs have proved to be quite useful in
    the field of CV. In general, a few CNN layers are stacked on top of each other
    to extract low-level (minor details) and high-level (big shape-related) feature
    information. We will also create a CNN-based feature extraction architecture in
    a similar way, combining it with a few fully connected layers. The final fully
    connected layer should have three neurons to generate output for each category,
    and a *sigmoid* activation layer to have that output in the form of a probability
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional block
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now define a reusable convolutional block that we can use repeatedly
    to create our final model architecture. In the convolutional block, we will have
    each convolutional layer followed by the layers of **Batch Normalization** (**BN**),
    **ReLU** activation, a max-pooling layer, and a dropout layer. Here, the layers
    of BN and dropout are for regularization purposes to ensure the smooth learning
    of our TF model. The following Python snippet defines our convolutional block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s define our complete model architecture by making use of this convolutional
    block.
  prefs: []
  type: TYPE_NORMAL
- en: TF model definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can now define our final TF model architecture, which can utilize a few
    iterations of the convolutional block defined in the previous step. We will first
    define an input layer to tell the model about the size of the input images that
    it will be expecting during training. As discussed earlier, each image in our
    dataset has a size of 200x200x1\. The following Python code defines the convolution-based
    feature extraction part of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use a `Flatten()` layer to bring all the features into a single
    dimension and apply fully connected layers to refine these features. Finally,
    we use another fully connected layer with three neurons, followed by a `softmax`
    activation, to generate probabilistic output for three classes. We then define
    our model object with input and output layers and print out a summary of the model
    for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet prints the model summary, which looks something similar to what
    is shown in *Figure 16**.2* (for a complete summary, check out the Jupyter Notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2: The model Summary for the TF-based defect detection architecture](img/B17792_16_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: The model Summary for the TF-based defect detection architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Our TF model graph is now ready, so we can now compile and fit our model.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this step, we can define the appropriate loss function, optimization algorithm,
    and metrics. As we have a multi-label classification problem, we will utilize
    *categorical cross-entropy* as a loss function. We will utilize the *Adam* optimizer
    with its default values and `''accuracy''` as a metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are all set to start training our model on the previously curated dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we are all set to launch the training of our model, as our data and model
    object are both set. We plan to train our model for 50 epochs with a batch size
    of 64\. After each epoch, we will keep checking the model’s loss and accuracy
    on training and testing partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The training logs with loss and accuracy values will look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Once the training is complete, we can start verifying the results of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the training progress
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will utilize the `history` variable defined in the previous
    step to plot the progress of the training loss, test loss, train accuracy, and
    test accuracy with progressing epochs. These graphs can help us understand whether
    our model training is going in the right direction or not. Also, we can check
    the ideal number of epochs required to get reasonable accuracy on test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first plot the training and validation loss of the model with an epoch
    number on the *X* axis and a loss value on the *Y* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The output of this snippet is shown in *Figure 16**.3*. We can see in the figure
    that training and validation loss decrease as the training progresses, which tells
    us that our model training is going in the right direction and our model is learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3 – Training and validation loss](img/B17792_16_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 – Training and validation loss
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can also plot the accuracy of the model on training and test partitions
    with the progress of training, as shown in following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot can be seen in *Figure 16**.4*. We can see that training
    accuracy keeps increasing and reaches close to 100% as the model starts overfitting,
    while the validation accuracy increases to around 70% and keeps fluctuating around
    that. It means that our current setup is capable of achieving around 70% accuracy
    on the test set. This accuracy value can be improved further by either increasing
    the capacity of our network (by adding a few more layers), or by improving the
    way we extract features from the images. For our experiment, this accuracy is
    satisfactory, and we will move forward with it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4 – Training and validation accuracy with the progress of training
    epochs](img/B17792_16_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 – Training and validation accuracy with the progress of training
    epochs
  prefs: []
  type: TYPE_NORMAL
- en: As our model training is now complete, we can start making predictions on unseen
    data. Let’s first check the results on our test set.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here comes the interesting part where we check the TF-model results on our unseen
    test set. Before checking the numbers, let’s first plot a few test images along
    with their true labels and model outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the results on a few random test images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s first visually verify the results of the model by choosing a few random
    images from our test set, making predictions on them, and plotting them with model
    predictions along with actual labels. We will put the label and prediction information
    in the image titles, as shown in the following Python snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output of this snippet is shown in *Figure 16**.4*. We can see that the
    model is slightly confused between class 0 and class 1 but easily identifies class
    2, due to big clues on the shape. As a reminder, class 0 here represents good
    bangles, class 1 represents defected bangles, and class 2 represents broken bangles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.5 – The model results for the classification of good, defected,
    and broken bangles](img/B17792_16_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.5 – The model results for the classification of good, defected, and
    broken bangles
  prefs: []
  type: TYPE_NORMAL
- en: As per *Figure 16**.5*, our model is doing a good job of identifying class 2
    but is slightly confused between class 0 and class 1, due to very tiny visual
    clues. Next, let’s check the metrics on the entire test set to get a sense of
    the quality of our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Classification report
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To generate the classification report on our entire test set, we first need
    to generate model outputs for the entire test set. We will choose the class with
    maximum probability as the model prediction (note that choosing the output class
    like that may not be the best option when we have highly imbalanced datasets).
    The following is the Python code that generates the model output on the entire
    test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now print the classification report for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the classification report:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The classification report indicates that our model has a F1 score of around
    0.65 for class 0 and class 1, and 0.71 for class 2\. As suspected, the model is
    doing a better job at identifying broken bangle images. Thus, the recall of the
    model for the `Broken` class is very good, around 92%. Overall, our model is doing
    a decent job but has potential for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving the accuracy of this model can be a good exercise for you. The following
    are some hints that may help to increase the overall accuracy of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Work with better resolution (something better than 200x200)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deeper model (more and bigger CNN layers to get better features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation (make the model more robust)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A better network (better feature extraction with an attention mechanism or any
    other feature extraction strategy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, let’s print the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: A confusion matrix can help to determine what kind of mistakes our model makes.
    In other words, when it classifies class 0 incorrectly, which class does our model
    confuse it with?
  prefs: []
  type: TYPE_NORMAL
- en: With this, our exercise of training a custom TF-based model for the task of
    defect detection is complete. We now have an average-performing trained TF model.
    Now, let’s see how can we deploy this model on Google Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a vision model to a Vertex AI endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we completed our experiment of training a TF-based
    vision model to identify detects from product images. We now have a trained model
    that can identify defected or broken bangle images. To make this model usable
    in downstream applications, we need to deploy it to an endpoint so that we can
    query that endpoint, getting outputs for new input images on demand. There are
    certain things that are important to consider while deploying a model, such as
    expected traffic, expected latency, and expected cost. Based on these factors,
    we can choose the best infrastructure to deploy our models. If there are strict
    low-latency requirements, we can deploy our model to machines with accelerators
    (such as **Graphical Processing Units** (**GPUs**) or **Tensor Processing Units**
    (**TPUs**)). Conversely, we don’t have the necessity of online or on-demand predictions,
    so we don’t need to deploy our model to an endpoint. Offline batch-prediction
    requests can be handled without even deploying the model to an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Saving model to Google Cloud Storage (GCS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our case, we are interested in deploying our model to a Vertex AI endpoint,
    just to see how it works. The very first step is to save our trained TF model
    into GCS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Once the model is saved, the next step would be to upload this model artifact
    to the Vertex AI Model Registry. Once our model is uploaded to the Model Registry,
    it can be deployed easily, either by using the Google Cloud console UI or Python
    scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the TF model to the Vertex Model Registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will utilize the Vertex AI SDK to upload our TF model to the
    Model Registry. Alternatively, we can also use the console UI to do the same.
    For this purpose, we will be required to provide a model display name, a region
    to upload the model to, the URI of the saved model artifact, and a serving container
    image. Note that the serving container images must have dependencies installed
    for appropriate versions of frameworks, such as TF. Google Cloud provides a list
    of prebuilt serving containers that can be readily used to serve the models. In
    our case, we will also use a prebuilt container that supports TF 2.11 for serving.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up some configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can go ahead and upload the model artifact to the Model Registry, as
    shown in the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We have now successfully uploaded our TF model to the Registry. We can now locate
    our model, using either the model display name or the model resource ID, as per
    the previous output. Next, we need an endpoint to deploy our model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Vertex AI endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will create a Vertex AI endpoint that will be used to serve
    our model prediction requests. Again, the endpoints can also be created using
    the Google Cloud console UI, but here, we will do it programmatically with the
    Vertex AI SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the sample function that can be leveraged to create an endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use this function to create an endpoint for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This snippet gives us the following output, and it confirms that the endpoint
    has been created (note that this endpoint is currently empty without any model):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the following command to list all the endpoints within a region
    to verify whether it has been successfully created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This command gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Our endpoint is now set, so we can go ahead and deploy a model now.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model to the Vertex AI endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now have our TF model in the Model Registry, and we have also created a
    Vertex AI endpoint. We are now all set to deploy our model to this endpoint. This
    action can also be performed using the Google Cloud console UI, but here, we will
    do it programmatically using the Vertex AI SDK. First, let’s get the details of
    our model from the Registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s deploy our model to the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as the execution of this snippet is complete without any errors, our
    model should be ready to accept requests for online predictions. This deployed
    model information should be visible in the Google Cloud console UI as well. We
    can also verify the deployed model by using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This should print the resources related to the deployed model. The output will
    look something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We have now verified that our TF model is successfully deployed. Now, we are
    all set to call this endpoint for predictions. In the next section, we will see
    how an endpoint with a deployed model can serve online predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting online predictions from a vision model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we deployed our custom TF-based model to a Vertex AI
    Endpoint so that we could embed it into any downstream application, querying it
    for on-demand or online predictions. In this section, we will see how we can call
    this endpoint for online predictions programmatically using Python. However, the
    prediction requests can also be made by using a `curl` command and sending a JSON
    file with input data.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few things to consider while making prediction requests; the most
    important part is pre-processing the input data accordingly. In the first section,
    when we trained our model, we did some pre-processing on our image dataset to
    make it compatible with the model. Similarly, while requesting the predictions,
    we should follow the exact same data preparation steps. Otherwise, either the
    model request will fail, due to an incompatible input format, or it will give
    bad results, due to training-serving skew. We already have pre-processed test
    images, so we can pass them directly within the prediction requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we will pass the input image to the request JSON, we will need to encode
    it into a JSON serializable format. So, we will send our input image to the model
    encode with the `base64` format. Check out the following snippet for an example
    payload creation of a single test image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s set some configurations to call the endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now go ahead and make a prediction request to our Vertex AI endpoint.
    Note that our parameters dictionary is empty, which means that we will get the
    raw model predictions as a result. In a more customized setting, we can also pass
    some parameters, such as *thresholds*, to perform minor post-processing on the
    model predictions accordingly. Check out the following Python code to request
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, we can pass a number of images in the `instances` variable and get
    an on-demand or online prediction result from an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we created an end-to-end vision-based solution to detect visual
    defects from images. We saw how CNN-based deep learning architectures can be used
    to extract useful features from images and then use those features for tasks such
    as classification. After training and testing our model, we went ahead and deployed
    it to a Vertex AI endpoint, allowing it to serve online or on-demand prediction
    requests for any number of downstream applications.
  prefs: []
  type: TYPE_NORMAL
- en: After completing this chapter, you should be confident about how to approach
    vision-based problems and how to utilize ML to solve them. You should now be able
    to train your own vision-based classification models to solve real-world business
    problems. After completing the second section on deploying a custom model to a
    Vertex AI endpoint and the third section on getting online prediction from a Vertex
    endpoint, you should now be able to make your custom vision models usable for
    any downstream business application, by deploying them to Google Vertex AI. We
    hope this chapter was a good learning experience, with a hands-on real-world example.
    The next chapter will also present a hands-on example of a real-world, NLP-related
    use case.
  prefs: []
  type: TYPE_NORMAL
