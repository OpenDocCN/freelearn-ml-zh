<html><head></head><body>
		<div id="_idContainer147">
			<h1 id="_idParaDest-91"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.1.1">Chapter 6: Unsupervised Machine Learning</span></h1>
			<p><span class="koboSpan" id="kobo.2.1">Oftentimes, many data science tutorials that you will encounter in courses and training revolve around the field of </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">Supervised Machine Learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">SML</span></strong><span class="koboSpan" id="kobo.6.1">) in which data and its corresponding labels are used to develop predictive models to automate tasks. </span><span class="koboSpan" id="kobo.6.2">However, in real-world data, the availability of pre-labeled or categorized data is seldom the case, and most datasets you will encounter will be in their raw and unlabeled form. </span><span class="koboSpan" id="kobo.6.3">For cases such as these, or whose primary objectives are more exploratory or not necessarily of automatable fashion, the field of unsupervised ML will be of great value.</span></p>
			<p><span class="koboSpan" id="kobo.7.1">Over the course of this chapter, we will explore many methods relating to the areas of clustering and </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">Dimensionality Reduction</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">DR</span></strong><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">The main topics we will explore are listed here:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.12.1">Introduction to </span><strong class="bold"><span class="koboSpan" id="kobo.13.1">Unsupervised Learning</span></strong><span class="koboSpan" id="kobo.14.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.15.1">UL</span></strong><span class="koboSpan" id="kobo.16.1">)</span></li>
				<li><span class="koboSpan" id="kobo.17.1">Understanding clustering algorithms</span></li>
				<li><span class="koboSpan" id="kobo.18.1">Tutorial – breast cancer prediction via clustering</span></li>
				<li><span class="koboSpan" id="kobo.19.1">Understanding DR</span></li>
				<li><span class="koboSpan" id="kobo.20.1">Tutorial – exploring DR models</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.21.1">With these topics in mind, let's now go ahead and get started!</span></p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.22.1">Introduction to UL</span></h1>
			<p><span class="koboSpan" id="kobo.23.1">We will </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.24.1">define UL as a subset of ML in which models are trained without the existence of categories or labels. </span><span class="koboSpan" id="kobo.24.2">Unlike its supervised counterpart, UL relies on the development of models to capture patterns in the form of features to extract insights from the data. </span><span class="koboSpan" id="kobo.24.3">Let's now take a closer look at the two main categories of UL.</span></p>
			<p><span class="koboSpan" id="kobo.25.1">There exist many different methods and techniques that fall within the scope of UL. </span><span class="koboSpan" id="kobo.25.2">We can </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.26.1">group these methods into two main categories: those </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.27.1">with </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">discrete</span></strong><span class="koboSpan" id="kobo.29.1"> data (</span><strong class="bold"><span class="koboSpan" id="kobo.30.1">clustering</span></strong><span class="koboSpan" id="kobo.31.1">) and those with </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">continuous</span></strong><span class="koboSpan" id="kobo.33.1"> data (</span><strong class="bold"><span class="koboSpan" id="kobo.34.1">DR</span></strong><span class="koboSpan" id="kobo.35.1">). </span><span class="koboSpan" id="kobo.35.2">We can see a graphical representation of this here: </span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<span class="koboSpan" id="kobo.36.1"><img src="image/B17761_06_001.jpg" alt="Figure 6.1 – The two types of UL "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.37.1">Figure 6.1 – The two types of UL</span></p>
			<p><span class="koboSpan" id="kobo.38.1">In each of these techniques, data is either grouped or transformed in order to determine </span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.39.1">labels or extract insights and representations without knowing the labels or categories of the data ahead of time. </span><span class="koboSpan" id="kobo.39.2">Take, for example, the breast cancer dataset we worked with in </span><a href="B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082"><em class="italic"><span class="koboSpan" id="kobo.40.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.41.1">, </span><em class="italic"><span class="koboSpan" id="kobo.42.1">Understanding Machine Learning</span></em><span class="koboSpan" id="kobo.43.1">, in which we developed a classification model. </span><span class="koboSpan" id="kobo.43.2">We trained the model by explicitly telling it which observations within the data were malignant and which were benign, thus allowing it to learn the differences </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.44.1">through the features. </span><span class="koboSpan" id="kobo.44.2">Similar to our supervised model, we can train an unsupervised </span><strong class="bold"><span class="koboSpan" id="kobo.45.1">clustering</span></strong><span class="koboSpan" id="kobo.46.1"> model to make similar predictions by clustering our data into groups (malignant and benign) without knowing the labels or classes ahead of time. </span><span class="koboSpan" id="kobo.46.2">There are many different types of clustering models we can use, and we will explore a few of these in the following section, and others further along in this chapter.</span></p>
			<p><span class="koboSpan" id="kobo.47.1">In addition to clustering our data, we can also explore and transform our data through a method known as </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">DR</span></strong><span class="koboSpan" id="kobo.49.1">, which we will define as the transformation of high-dimensional </span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.50.1">data into a lower-dimensional space in which the meaningful properties of the features are retained. </span><span class="koboSpan" id="kobo.50.2">Data transformations can either be used to reduce the number of features down to a few or to engineer new and useful features </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.51.1">for a given dataset. </span><span class="koboSpan" id="kobo.51.2">One of the most popular methods </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.52.1">that fall within this category is a process known as </span><strong class="bold"><span class="koboSpan" id="kobo.53.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.54.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.55.1">PCA</span></strong><span class="koboSpan" id="kobo.56.1">)—we will explore this specific model in detail further along in this chapter.</span></p>
			<p><span class="koboSpan" id="kobo.57.1">Within the scope of both of these categories falls a niche field that is not quite yet a third category </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.58.1">given its broad application—this is known as </span><strong class="bold"><span class="koboSpan" id="kobo.59.1">anomaly detection</span></strong><span class="koboSpan" id="kobo.60.1">. </span><span class="koboSpan" id="kobo.60.2">Anomaly detection within the scope of UL, as the name suggests, is a method for the detection of anomalies within an unlabeled dataset. </span><span class="koboSpan" id="kobo.60.3">Note that, unlike clustering methods in which there is generally a balance within the different labels of a dataset (for example, 50:50), anomalies tend to be rare in the sense that the number of observations is usually anything but balanced. </span><span class="koboSpan" id="kobo.60.4">The most popular methods today when it comes to anomaly detection </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.61.1">from an unsupervised perspective tend to </span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.62.1">not only include </span><strong class="bold"><span class="koboSpan" id="kobo.63.1">clustering</span></strong><span class="koboSpan" id="kobo.64.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.65.1">DR</span></strong><span class="koboSpan" id="kobo.66.1">, but also </span><strong class="bold"><span class="koboSpan" id="kobo.67.1">neural networks</span></strong><span class="koboSpan" id="kobo.68.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.69.1">isolation forests</span></strong><span class="koboSpan" id="kobo.70.1">. </span></p>
			<p><span class="koboSpan" id="kobo.71.1">Now that we've gained a sense of some of the high-level concepts relating to UL and know our objectives, let's now go ahead and get started with some details and examples for each.</span></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.72.1">Understanding clustering algorithms</span></h1>
			<p><span class="koboSpan" id="kobo.73.1">One of the </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.74.1">most common methods that fall within the </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.75.1">category of UL is </span><strong class="bold"><span class="koboSpan" id="kobo.76.1">clustering analysis</span></strong><span class="koboSpan" id="kobo.77.1">. </span><span class="koboSpan" id="kobo.77.2">The main idea behind clustering analysis is the grouping of data into two or more categories of a </span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.78.1">similar nature to form groups or </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">clusters</span></strong><span class="koboSpan" id="kobo.80.1">. </span><span class="koboSpan" id="kobo.80.2">Within this section, we will explore these different clustering models, and subsequently apply our knowledge in a real-world scenario concerning the development of predictive models for the detection of breast cancer. </span><span class="koboSpan" id="kobo.80.3">Let's go ahead and explore some of the most common clustering algorithms.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.81.1">Exploring the different clustering algorithms</span></h2>
			<p><span class="koboSpan" id="kobo.82.1">There exists </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.83.1">not one, but a broad spectrum of clustering algorithms, each with its own approach to how to best cluster data depending on the dataset at hand. </span><span class="koboSpan" id="kobo.83.2">We can divide these clustering algorithms into two general categories: </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">hierarchical</span></strong><span class="koboSpan" id="kobo.85.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.86.1">partitional</span></strong><span class="koboSpan" id="kobo.87.1"> clustering. </span><span class="koboSpan" id="kobo.87.2">We can see a graphical representation of this here: </span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<span class="koboSpan" id="kobo.88.1"><img src="image/B17761_06_002.jpg" alt="Figure 6.2 – The two types of clustering algorithms "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.89.1">Figure 6.2 – The two types of clustering algorithms</span></p>
			<p><span class="koboSpan" id="kobo.90.1">With these different areas of clustering in mind, let's now go ahead and explore these in more detail, beginning with hierarchical clustering.</span></p>
			<h3><span class="koboSpan" id="kobo.91.1">Hierarchical clustering</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.92.1">Hierarchical clustering</span></strong><span class="koboSpan" id="kobo.93.1">, as the </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.94.1">name suggests, is a method </span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.95.1">that attempts to cluster data based on a given hierarchy using two types of approaches: </span><strong class="bold"><span class="koboSpan" id="kobo.96.1">agglomerative</span></strong><span class="koboSpan" id="kobo.97.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">divisive</span></strong><span class="koboSpan" id="kobo.99.1">. </span><span class="koboSpan" id="kobo.99.2">Agglomerative </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.100.1">clustering is </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.101.1">known as a </span><em class="italic"><span class="koboSpan" id="kobo.102.1">bottom-up</span></em><span class="koboSpan" id="kobo.103.1"> approach in which each observation in a dataset is assigned its own cluster and is subsequently merged with other clusters to form a hierarchy. </span><span class="koboSpan" id="kobo.103.2">Alternatively, </span><strong class="bold"><span class="koboSpan" id="kobo.104.1">divisive clustering</span></strong><span class="koboSpan" id="kobo.105.1"> is a </span><em class="italic"><span class="koboSpan" id="kobo.106.1">top-down</span></em><span class="koboSpan" id="kobo.107.1"> approach in which all observations for a given dataset </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.108.1">begin in a single cluster and are then split up. </span><span class="koboSpan" id="kobo.108.2">We can </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.109.1">see a graphical representation of this here: </span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<span class="koboSpan" id="kobo.110.1"><img src="image/B17761_06_003.jpg" alt="Figure 6.3 – The difference between agglomerative and divisive clustering  "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.111.1">Figure 6.3 – The difference between agglomerative and divisive clustering </span></p>
			<p><span class="koboSpan" id="kobo.112.1">With the concept of hierarchical clustering in mind, we can imagine a number of useful </span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.113.1">applications this can help us with </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.114.1">when it comes to phylogenetic trees and other areas of biology. </span><span class="koboSpan" id="kobo.114.2">On the other hand, there also exist other methods of clustering in which hierarchy is not accounted for, such as when using </span><strong class="bold"><span class="koboSpan" id="kobo.115.1">Euclidean</span></strong><span class="koboSpan" id="kobo.116.1"> distance.</span></p>
			<h3><span class="koboSpan" id="kobo.117.1">Euclidean distance</span></h3>
			<p><span class="koboSpan" id="kobo.118.1">In addition </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.119.1">to hierarchical clustering, we also have a set of </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.120.1">models that fall under the idea </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.121.1">of </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">partition-based clustering</span></strong><span class="koboSpan" id="kobo.123.1">. </span><span class="koboSpan" id="kobo.123.2">The main idea </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.124.1">here is separating </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.125.1">or partitioning your dataset to form clusters using a given method. </span><span class="koboSpan" id="kobo.125.2">Two of the most common types of partition-based clustering are </span><strong class="bold"><span class="koboSpan" id="kobo.126.1">distance-based clustering</span></strong><span class="koboSpan" id="kobo.127.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.128.1">probability-based clustering</span></strong><span class="koboSpan" id="kobo.129.1">. </span><span class="koboSpan" id="kobo.129.2">When it comes to distance-based clustering, the main </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.130.1">idea here is determining whether a given data </span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.131.1">point belongs to a cluster based solely on distance such as </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">Euclidean distance</span></strong><span class="koboSpan" id="kobo.133.1">. </span><span class="koboSpan" id="kobo.133.2">An example of this is the </span><strong class="bold"><span class="koboSpan" id="kobo.134.1">K-Means</span></strong><span class="koboSpan" id="kobo.135.1"> clustering algorithm—one of the most common clustering algorithms, given its simplicity.</span></p>
			<p><span class="koboSpan" id="kobo.136.1">Note that </span><strong class="bold"><span class="koboSpan" id="kobo.137.1">Euclidean</span></strong><span class="koboSpan" id="kobo.138.1"> distance, sometimes referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.139.1">Pythagorean</span></strong><span class="koboSpan" id="kobo.140.1"> distance, from a </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.141.1">mathematical perspective, is defined as the distance </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.142.1">between two points on a Cartesian coordinate system. </span><span class="koboSpan" id="kobo.142.2">For example, for two points, </span><em class="italic"><span class="koboSpan" id="kobo.143.1">p</span></em><span class="koboSpan" id="kobo.144.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.145.1">p1</span></em><span class="koboSpan" id="kobo.146.1">, </span><em class="italic"><span class="koboSpan" id="kobo.147.1">p2</span></em><span class="koboSpan" id="kobo.148.1">) and </span><em class="italic"><span class="koboSpan" id="kobo.149.1">q</span></em><span class="koboSpan" id="kobo.150.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.151.1">q1</span></em><span class="koboSpan" id="kobo.152.1">, </span><em class="italic"><span class="koboSpan" id="kobo.153.1">q2</span></em><span class="koboSpan" id="kobo.154.1">), the Euclidean distance can be calculated as follows:</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<span class="koboSpan" id="kobo.155.1"><img src="image/Formula_B17761_06_001.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.156.1">Within the </span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.157.1">context of two dimensions, this model is fairly simple and easy to calculate. </span><span class="koboSpan" id="kobo.157.2">However, the complexity of this model can increase when given more dimensions, simply represented as follows:</span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<span class="koboSpan" id="kobo.158.1"><img src="image/Formula_B17761_06_002.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.159.1">Now that we have gained a better sense of the concept of Euclidean distance, let's now take a look at an actual application known as K-Means.</span></p>
			<h3><span class="koboSpan" id="kobo.160.1">K-Means clustering</span></h3>
			<p><span class="koboSpan" id="kobo.161.1">With the </span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.162.1">concept of Euclidean distance </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.163.1">in mind, let's now take a close look at how this can be applied within the context of K-Means. </span><span class="koboSpan" id="kobo.163.2">The K-Means algorithm attempts to cluster data by separating samples into </span><em class="italic"><span class="koboSpan" id="kobo.164.1">k</span></em><span class="koboSpan" id="kobo.165.1"> groups consisting of equal variance and minimizing a </span><strong class="bold"><span class="koboSpan" id="kobo.166.1">criterion</span></strong><span class="koboSpan" id="kobo.167.1"> (inertia). </span><span class="koboSpan" id="kobo.167.2">The algorithm's objective is to select </span><em class="italic"><span class="koboSpan" id="kobo.168.1">k</span></em> <strong class="bold"><span class="koboSpan" id="kobo.169.1">centroids</span></strong><span class="koboSpan" id="kobo.170.1"> that minimize the inertia.</span></p>
			<p><span class="koboSpan" id="kobo.171.1">The K-Means model is quite simple in the sense that it operates in three simple steps, represented as stars in the following diagram: </span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<span class="koboSpan" id="kobo.172.1"><img src="image/B17761_06_004.jpg" alt="Figure 6.4 – K-Means clustering steps "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.173.1">Figure 6.4 – K-Means clustering steps</span></p>
			<p><span class="koboSpan" id="kobo.174.1">First, a specified number of </span><em class="italic"><span class="koboSpan" id="kobo.175.1">k</span></em> <strong class="bold"><span class="koboSpan" id="kobo.176.1">centroids</span></strong><span class="koboSpan" id="kobo.177.1"> are randomly initialized. </span><span class="koboSpan" id="kobo.177.2">Second, each of the observations, represented by the circles, is then clustered based on distance. </span><span class="koboSpan" id="kobo.177.3">The mean </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.178.1">of all observations in a given cluster is then calculated, and </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.179.1">the centroid is moved to that mean. </span><span class="koboSpan" id="kobo.179.2">The process repeats over and over until convergence is reached based on a predetermined threshold. </span></p>
			<p><strong class="bold"><span class="koboSpan" id="kobo.180.1">K-Means</span></strong><span class="koboSpan" id="kobo.181.1"> is one of the most commonly used clustering algorithms out there, given its simplicity and relatively acceptable computation. </span><span class="koboSpan" id="kobo.181.2">It works well with high-dimensional data and is relatively easy to implement. </span><span class="koboSpan" id="kobo.181.3">However, it does have its limitations in the sense that it does make the assumption that the clusters are of a spherical nature, which often leads to the misgrouping of data with clusters of non-spherical shapes. </span><span class="koboSpan" id="kobo.181.4">Take, for example, another dataset in which the clusters are not of a spherical nature but are more ovular. </span><span class="koboSpan" id="kobo.181.5">The application of the </span><strong class="bold"><span class="koboSpan" id="kobo.182.1">K-Means</span></strong><span class="koboSpan" id="kobo.183.1"> model, which operates on the notion of </span><strong class="bold"><span class="koboSpan" id="kobo.184.1">distance</span></strong><span class="koboSpan" id="kobo.185.1">, would not yield the most accurate results, as shown in the following screenshot: </span></p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<span class="koboSpan" id="kobo.186.1"><img src="image/B17761_06_005.jpg" alt="Figure 6.5 – K-Means clustering with non-spherical clusters "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.187.1">Figure 6.5 – K-Means clustering with non-spherical clusters</span></p>
			<p><span class="koboSpan" id="kobo.188.1">When </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.189.1">operating with non-spherical </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.190.1">clusters, a good alternative to a </span><strong class="bold"><span class="koboSpan" id="kobo.191.1">distance</span></strong><span class="koboSpan" id="kobo.192.1">-based model would be a statistical-based approach such as a </span><strong class="bold"><span class="koboSpan" id="kobo.193.1">Gaussian Mixture Model</span></strong><span class="koboSpan" id="kobo.194.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.195.1">GMM</span></strong><span class="koboSpan" id="kobo.196.1">).</span></p>
			<h3><span class="koboSpan" id="kobo.197.1">GMMs</span></h3>
			<p><span class="koboSpan" id="kobo.198.1">GMMs, within </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.199.1">the context of clustering, are </span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.200.1">algorithms that consist of </span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.201.1">a particular number of </span><strong class="bold"><span class="koboSpan" id="kobo.202.1">Gaussian distributions</span></strong><span class="koboSpan" id="kobo.203.1">. </span><span class="koboSpan" id="kobo.203.2">Each of these distributions represents a particular cluster. </span><span class="koboSpan" id="kobo.203.3">So far within the confines of this book, we have not yet discussed Gaussian distributions—a concept you will often hear about and come across throughout your career as a data scientist. </span><span class="koboSpan" id="kobo.203.4">Let's go ahead and define this. </span></p>
			<p><span class="koboSpan" id="kobo.204.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.205.1">Gaussian distribution</span></strong><span class="koboSpan" id="kobo.206.1"> can be thought of as a statistical equation representing data points that are symmetrically distributed around their mean value. </span><span class="koboSpan" id="kobo.206.2">You will often </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.207.1">hear this distribution referred to as a bell curve. </span><span class="koboSpan" id="kobo.207.2">We can represent the </span><strong class="bold"><span class="koboSpan" id="kobo.208.1">probability density function</span></strong><span class="koboSpan" id="kobo.209.1"> of a Gaussian distribution as follows:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<span class="koboSpan" id="kobo.210.1"><img src="image/Formula_B17761_06_003.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.211.1">Here, </span><span class="koboSpan" id="kobo.212.1"><img src="image/Formula_B17761_06_004.png" alt=""/></span><span class="koboSpan" id="kobo.213.1"> represents the mean and </span><span class="koboSpan" id="kobo.214.1"><img src="image/Formula_B17761_06_005.png" alt=""/></span><span class="koboSpan" id="kobo.215.1"> represents the variance. </span><span class="koboSpan" id="kobo.215.2">Note that this function represents a single variable. </span><span class="koboSpan" id="kobo.215.3">Upon the addition of other variables, we would begin to venture into the space of multivariate Gaussian models, in which </span><em class="italic"><span class="koboSpan" id="kobo.216.1">x</span></em><span class="koboSpan" id="kobo.217.1"> and </span><span class="koboSpan" id="kobo.218.1"><img src="image/Formula_B17761_06_006.png" alt=""/></span><span class="koboSpan" id="kobo.219.1"> represent vectors of length </span><span class="koboSpan" id="kobo.220.1"><img src="image/Formula_B17761_06_007.png" alt=""/></span><span class="koboSpan" id="kobo.221.1">. </span><span class="koboSpan" id="kobo.221.2">In a dataset consisting of </span><em class="italic"><span class="koboSpan" id="kobo.222.1">k</span></em><span class="koboSpan" id="kobo.223.1"> clusters, we would need a mixture of </span><em class="italic"><span class="koboSpan" id="kobo.224.1">k</span></em><span class="koboSpan" id="kobo.225.1"> Gaussian distributions, in which each distribution has a mean and variance. </span><span class="koboSpan" id="kobo.225.2">These two values are determined through a technique known as </span><strong class="bold"><span class="koboSpan" id="kobo.226.1">Expectation-Maximization</span></strong><span class="koboSpan" id="kobo.227.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.228.1">EM</span></strong><span class="koboSpan" id="kobo.229.1">).</span></p>
			<p><span class="koboSpan" id="kobo.230.1">We will define </span><strong class="bold"><span class="koboSpan" id="kobo.231.1">EM</span></strong><span class="koboSpan" id="kobo.232.1"> as an algorithm that determines the proper parameters for a given </span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.233.1">model when some data is considered missing or incomplete. </span><span class="koboSpan" id="kobo.233.2">These missing or incomplete items are known as </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">latent variables</span></strong><span class="koboSpan" id="kobo.235.1">, and within </span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.236.1">the confines of UL, we can consider the actual clusters to be unknown. </span><span class="koboSpan" id="kobo.236.2">Note that if the clusters were known, we </span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.237.1">would be able to determine the mean </span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.238.1">and variance; however, we need to know the mean and variance to determine the cluster (think of the classic chicken-or-egg situation). </span><span class="koboSpan" id="kobo.238.2">We can use EM within the scope of the data to determine the proper values of these two variables to best fit the model parameters. </span><span class="koboSpan" id="kobo.238.3">With all this in mind, we are now in a position to discuss GMMs more intelligently. </span></p>
			<p><span class="koboSpan" id="kobo.239.1">We previously defined a GMM as a model consisting of multiple Gaussian distributions. </span><span class="koboSpan" id="kobo.239.2">We will now elaborate on this definition by including the fact that it is a probabilistic model consisting of multiple Gaussian distributions and utilizes a </span><strong class="bold"><span class="koboSpan" id="kobo.240.1">soft clustering</span></strong><span class="koboSpan" id="kobo.241.1"> approach by determining the membership of a data point to </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.242.1">a given cluster based on probability rather than a distance. </span><span class="koboSpan" id="kobo.242.2">Notice that this is in contrast to K-Means, which utilizes a </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">hard clustering</span></strong><span class="koboSpan" id="kobo.244.1"> approach. </span><span class="koboSpan" id="kobo.244.2">Using the previous example dataset shown in </span><em class="italic"><span class="koboSpan" id="kobo.245.1">Figure 6.5</span></em><span class="koboSpan" id="kobo.246.1"> in the </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.247.1">previous section, the application of a GMM would likely lead to improved results, as depicted here:</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<span class="koboSpan" id="kobo.248.1"><img src="image/B17761_06_006.jpg" alt="Figure 6.6 – K-Means clustering versus GMMs "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.249.1">Figure 6.6 – K-Means clustering versus GMMs</span></p>
			<p><span class="koboSpan" id="kobo.250.1">Within </span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.251.1">this section, we discussed a few of the </span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.252.1">most common clustering algorithms commonly used in many applications within the field of biotechnology. </span><span class="koboSpan" id="kobo.252.2">We see clustering being applied in areas such as bio-molecular data, scientific literature, manufacturing, and even oncology, as we will experience in the following tutorial.</span></p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.253.1">Tutorial – breast cancer prediction via clustering</span></h2>
			<p><span class="koboSpan" id="kobo.254.1">Over </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.255.1">the course of this tutorial, we will explore the application of commonly used clustering algorithms for the analysis and prediction of cancer using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">Wisconsin Breast Cancer</span></strong><span class="koboSpan" id="kobo.257.1"> dataset we applied in </span><a href="B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082"><em class="italic"><span class="koboSpan" id="kobo.258.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.259.1">, </span><em class="italic"><span class="koboSpan" id="kobo.260.1">Understanding Machine Learning</span></em><span class="koboSpan" id="kobo.261.1">. </span><span class="koboSpan" id="kobo.261.2">When we last visited this dataset, we approached the development of a model from the perspective of a supervised classifier in which we knew the labels of our observations ahead of time. </span><span class="koboSpan" id="kobo.261.3">However, in most real-world scenarios, knowledge of the labels ahead of time is rare. </span><strong class="bold"><span class="koboSpan" id="kobo.262.1">Clustering analysis</span></strong><span class="koboSpan" id="kobo.263.1">, as we will soon see, can be highly </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.264.1">valuable in these situations, and can even be used to label data to use within the context of a classifier later on. </span><span class="koboSpan" id="kobo.264.2">Over the course of this tutorial, we will develop our models using the data but pretend that we do not know the labels ahead of time. </span><span class="koboSpan" id="kobo.264.3">We will only use known labels to compare the results of our models. </span><span class="koboSpan" id="kobo.264.4">With this in mind, let's go ahead and get started!</span></p>
			<p><span class="koboSpan" id="kobo.265.1">We will </span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.266.1">begin by importing our dataset as we have previously done and check the shape, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.267.1">df = pd.read_csv("../../datasets/dataset_wisc_sd.csv")</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.268.1">print(df.shape)</span></p>
			<p><span class="koboSpan" id="kobo.269.1">We notice that there are 569 rows of data in this dataset. </span><span class="koboSpan" id="kobo.269.2">In our previous application, we had cleaned up the data to address missing and corrupt values. </span><span class="koboSpan" id="kobo.269.3">Let's go ahead and clean those up, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.270.1">df = df.replace(r'\\n','', regex=True) </span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.271.1">df = df.dropna()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.272.1">print(df.shape)</span></p>
			<p><span class="koboSpan" id="kobo.273.1">With the current shape of the data consisting of 569 rows with 32 columns, this now matches our previous dataset, and we are now ready to proceed.</span></p>
			<p><span class="koboSpan" id="kobo.274.1">Although we will not be using these labels to develop any models, let's take a quick look at them, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.275.1">import seaborn as sns</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.276.1">sns.countplot(df['diagnosis']);</span></p>
			<p><span class="koboSpan" id="kobo.277.1">We can see in the following screenshot that there are two classes—</span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">M</span></strong><span class="koboSpan" id="kobo.279.1"> for malignant and </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">B</span></strong><span class="koboSpan" id="kobo.281.1"> for benign. </span><span class="koboSpan" id="kobo.281.2">The two classes are not perfectly balanced but will do for the purposes of our clustering model:</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<span class="koboSpan" id="kobo.282.1"><img src="image/B17761_06_007.jpg" alt="Figure 6.7 – The distribution of the two classes "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.283.1">Figure 6.7 – The distribution of the two classes</span></p>
			<p><span class="koboSpan" id="kobo.284.1">To make </span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.285.1">our comparison to these labels easier during the following steps of our clustering analysis, let's go ahead and encode these labels as numerical values in which we will convert </span><strong class="source-inline"><span class="koboSpan" id="kobo.286.1">M</span></strong><span class="koboSpan" id="kobo.287.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">1</span></strong><span class="koboSpan" id="kobo.289.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">B</span></strong><span class="koboSpan" id="kobo.291.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">0</span></strong><span class="koboSpan" id="kobo.293.1">, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.294.1">df['diagnosis'] = df['diagnosis'].map({'M':1,'B':0}) </span></p>
			<p><span class="koboSpan" id="kobo.295.1">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.296.1">df.head()</span></strong><span class="koboSpan" id="kobo.297.1"> function to see the first few rows of our dataset and confirm that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.298.1">diagnosis</span></strong><span class="koboSpan" id="kobo.299.1"> column did in fact get encoded properly. </span><span class="koboSpan" id="kobo.299.2">Next, we will prepare a quick pairplot of a few select features, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.300.1">select_feats = ["diagnosis", "radius_mean", "texture_mean", "smoothness_mean"]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.301.1">sns.pairplot(df[select_feats], hue = 'diagnosis', markers=["s", "o"])</span></p>
			<p><span class="koboSpan" id="kobo.302.1">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">markers</span></strong><span class="koboSpan" id="kobo.304.1"> argument to specify two distinct shapes to plot the two classes, yielding the following pairplot showing scatter plots of our features:</span></p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<span class="koboSpan" id="kobo.305.1"><img src="image/B17761_06_008.png.jpg" alt="Figure 6.8 – Pairplot of select features "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.306.1">Figure 6.8 – Pairplot of select features</span></p>
			<p><span class="koboSpan" id="kobo.307.1">Our first </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.308.1">objective is to look over the many features and get a sense of which two features show the least amount of overlap or the best degree of separation. </span><span class="koboSpan" id="kobo.308.2">We can see that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">smoothness_mean</span></strong><span class="koboSpan" id="kobo.310.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.311.1">texture_mean</span></strong><span class="koboSpan" id="kobo.312.1"> columns have a high degree of overlap; however, </span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">radius_mean</span></strong><span class="koboSpan" id="kobo.314.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">texture_mean</span></strong><span class="koboSpan" id="kobo.316.1"> seem less so. </span><span class="koboSpan" id="kobo.316.2">We can take a closer look at these by plotting a scatter plot using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">seaborn</span></strong><span class="koboSpan" id="kobo.318.1"> library, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.319.1">sns.scatterplot(x="radius_mean", y="texture_mean", hue="diagnosis", style='diagnosis', data=df, markers=["s", "o"])</span></p>
			<p><span class="koboSpan" id="kobo.320.1">Notice that once again, we can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">style</span></strong><span class="koboSpan" id="kobo.322.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">markers</span></strong><span class="koboSpan" id="kobo.324.1"> arguments to shape the data points, thus yielding the following diagram as output:</span></p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<span class="koboSpan" id="kobo.325.1"><img src="image/B17761_06_009.png.jpg" alt="Figure 6.9 – Scatter plot of the two features that showed good separation "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.326.1">Figure 6.9 – Scatter plot of the two features that showed good separation</span></p>
			<p><span class="koboSpan" id="kobo.327.1">Next, we will normalize our data. </span><span class="koboSpan" id="kobo.327.2">In statistics, normalization or standardization can </span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.328.1">have a wide variety of meanings and are sometimes used interchangeably. </span><span class="koboSpan" id="kobo.328.2">We will define normalization to mean the rescaling of values into a range of [</span><em class="italic"><span class="koboSpan" id="kobo.329.1">0</span></em><span class="koboSpan" id="kobo.330.1">,</span><em class="italic"><span class="koboSpan" id="kobo.331.1">1</span></em><span class="koboSpan" id="kobo.332.1">]. </span><span class="koboSpan" id="kobo.332.2">On the other hand, we will define standardization to mean the rescaling of data to have a mean value of 0, and a standard deviation value of 1. </span><span class="koboSpan" id="kobo.332.3">For the purposes of our current objectives, we will want to standardize our data as we have previously done using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.333.1">StandardScaler</span></strong><span class="koboSpan" id="kobo.334.1"> class. </span><span class="koboSpan" id="kobo.334.2">Recall that this class standardizes features within the dataset by removing the mean and scaling to variance, which can be represented as follows:</span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<span class="koboSpan" id="kobo.335.1"><img src="image/Formula_B17761_06_008.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.336.1">Here, </span><span class="koboSpan" id="kobo.337.1"><img src="image/Formula_B17761_06_009.png" alt=""/></span><span class="koboSpan" id="kobo.338.1"> is the standard score of a sample, </span><span class="koboSpan" id="kobo.339.1"><img src="image/Formula_B17761_06_010.png" alt=""/></span><span class="koboSpan" id="kobo.340.1"> is the mean, and </span><span class="koboSpan" id="kobo.341.1"><img src="image/Formula_B17761_06_011.png" alt=""/></span><span class="koboSpan" id="kobo.342.1"> is the standard deviation. </span><span class="koboSpan" id="kobo.342.2">We can apply this in Python with the following code:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.343.1">from sklearn.preprocessing import StandardScaler</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.344.1">scaler = StandardScaler()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.345.1">X = df.drop(columns = ["id", "diagnosis"])</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.346.1">y = df.diagnosis.values</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.347.1">X_scaled = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)</span></p>
			<p><span class="koboSpan" id="kobo.348.1">With </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.349.1">our dataset scaled, we are now ready to start applying a few models. </span><span class="koboSpan" id="kobo.349.2">We will begin with the agglomerative clustering model from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">sklearn</span></strong><span class="koboSpan" id="kobo.351.1"> library. </span></p>
			<h3><span class="koboSpan" id="kobo.352.1">Agglomerative clustering</span></h3>
			<p><span class="koboSpan" id="kobo.353.1">Recall that </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">agglomerative</span></strong><span class="koboSpan" id="kobo.355.1"> clustering is a method in which clusters are formed by recursively </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.356.1">merging clusters together. </span><span class="koboSpan" id="kobo.356.2">Let's go ahead and implement the agglomerative clustering algorithm with our dataset, as follows:</span></p>
			<ol>
				<li><span class="koboSpan" id="kobo.357.1">First, we will import the specific class of interest from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.358.1">sklearn</span></strong><span class="koboSpan" id="kobo.359.1"> library, and then create an instance of our model by specifying the number of classes we want and setting the linkage as </span><strong class="source-inline"><span class="koboSpan" id="kobo.360.1">ward</span></strong><span class="koboSpan" id="kobo.361.1">—one of the most common agglomerative clustering methods used. </span><span class="koboSpan" id="kobo.361.2">The code is illustrated in the following snippet:</span><p class="source-code"><span class="koboSpan" id="kobo.362.1">from sklearn.cluster import AgglomerativeClustering</span></p><p class="source-code"><span class="koboSpan" id="kobo.363.1">agc = AgglomerativeClustering(n_clusters=2, linkage="ward")</span></p></li>
				<li><span class="koboSpan" id="kobo.364.1">Next, we will fit our model to our dataset, and predict the clusters to which they belong. </span><span class="koboSpan" id="kobo.364.2">Notice in the following code snippet that we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">fit_predict()</span></strong><span class="koboSpan" id="kobo.366.1"> function, using the first two features, </span><strong class="source-inline"><span class="koboSpan" id="kobo.367.1">radius_mean</span></strong><span class="koboSpan" id="kobo.368.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.369.1">texture_mean</span></strong><span class="koboSpan" id="kobo.370.1">, and not the whole dataset: </span><p class="source-code"><span class="koboSpan" id="kobo.371.1">agc_featAll_pred = agc.fit_predict(X_scaled.iloc[:, :2])</span></p></li>
				<li><span class="koboSpan" id="kobo.372.1">We can </span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.373.1">then use </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">matplotlib</span></strong><span class="koboSpan" id="kobo.375.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">seaborn</span></strong><span class="koboSpan" id="kobo.377.1"> to generate a diagram showing the actual (</span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">true</span></strong><span class="koboSpan" id="kobo.379.1">) results on the left and predicted agglomerative clustering results on the right, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.380.1">import matplotlib.pyplot as plt</span></p><p class="source-code"><span class="koboSpan" id="kobo.381.1">import seaborn as sns</span></p><p class="source-code"><span class="koboSpan" id="kobo.382.1">plt.figure(figsize=(20, 5))</span></p><p class="source-code"><span class="koboSpan" id="kobo.383.1">plt.subplot(121)</span></p><p class="source-code"><span class="koboSpan" id="kobo.384.1">plt.title("Actual Results")</span></p><p class="source-code"><span class="koboSpan" id="kobo.385.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue=y, style=y, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.386.1">ax.legend(loc="upper right")</span></p><p class="source-code"><span class="koboSpan" id="kobo.387.1">plt.subplot(122)</span></p><p class="source-code"><span class="koboSpan" id="kobo.388.1">plt.title("Agglomerative Clustering")</span></p><p class="source-code"><span class="koboSpan" id="kobo.389.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue=agc_featAll_pred, style=agc_featAll_pred, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.390.1">ax.legend(loc="upper right")</span></p><p><span class="koboSpan" id="kobo.391.1">Notice in the preceding code snippet the use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">subplot()</span></strong><span class="koboSpan" id="kobo.393.1"> functionality </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.394.1">in which the value </span><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">122</span></strong><span class="koboSpan" id="kobo.396.1"> was used to represent </span><strong class="source-inline"><span class="koboSpan" id="kobo.397.1">1</span></strong><span class="koboSpan" id="kobo.398.1"> as the total number of rows, </span><strong class="source-inline"><span class="koboSpan" id="kobo.399.1">2</span></strong><span class="koboSpan" id="kobo.400.1"> as the total number of columns, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">2</span></strong><span class="koboSpan" id="kobo.402.1"> as the specific index location of the plot. </span><span class="koboSpan" id="kobo.402.2">You can view the output here:</span></p><div id="_idContainer123" class="IMG---Figure"><span class="koboSpan" id="kobo.403.1"><img src="image/B17761_06_010.png.jpg" alt="Figure 6.10 – Results of the agglomerative clustering model relative to the actual results "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.404.1">Figure 6.10 – Results of the agglomerative clustering model relative to the actual results</span></p></li>
				<li><span class="koboSpan" id="kobo.405.1">From an initial estimation, we see that the model did a fairly reasonable job in distinguishing between the two clusters, having known very little about the actual true outcome. </span><span class="koboSpan" id="kobo.405.2">We can get a quick measure of its performance using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.406.1">accuracy_score</span></strong><span class="koboSpan" id="kobo.407.1"> method from </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">sklearn</span></strong><span class="koboSpan" id="kobo.409.1">. </span><span class="koboSpan" id="kobo.409.2">Although getting a sense of the recall and f-1 scores is also important, we will stick to accuracy for simplicity for now. </span><span class="koboSpan" id="kobo.409.3">The code is illustrated in the following snippet:</span><p class="source-code"><span class="koboSpan" id="kobo.410.1">from sklearn.metrics import accuracy_score</span></p><p class="source-code"><span class="koboSpan" id="kobo.411.1">print(accuracy_score(y, agc_featAll_pred))</span></p><p class="source-code"><span class="koboSpan" id="kobo.412.1">0.832740</span></p></li>
			</ol>
			<p><span class="koboSpan" id="kobo.413.1">In summary, the agglomerative clustering model using only the first two features of the dataset yielded an accuracy of roughly 83%—not a bad first attempt! </span><span class="koboSpan" id="kobo.413.2">If you are following along using the provided code, I would encourage you to try adding yet </span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.414.1">another feature and fitting the model with three or four or five features instead of just two and see whether you are able to improve the performance. </span><span class="koboSpan" id="kobo.414.2">Better yet, explore the other features provided in this dataset, and see whether you can find others that offer better separation and beat our 83% metric. </span><span class="koboSpan" id="kobo.414.3">Let's now investigate the performance of K-Means instead.</span></p>
			<h3><span class="koboSpan" id="kobo.415.1">K-Means clustering</span></h3>
			<p><span class="koboSpan" id="kobo.416.1">Let's now </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.417.1">investigate the application of </span><strong class="bold"><span class="koboSpan" id="kobo.418.1">K-Means</span></strong><span class="koboSpan" id="kobo.419.1"> clustering using the dataset. </span><span class="koboSpan" id="kobo.419.2">Recall that the K-Means algorithm attempts to cluster data by partitioning the data into </span><em class="italic"><span class="koboSpan" id="kobo.420.1">k</span></em><span class="koboSpan" id="kobo.421.1"> clusters based on the location of their centroids. </span><span class="koboSpan" id="kobo.421.2">We can apply the K-Means algorithm using the following steps:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.422.1">We will begin by importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">KMeans</span></strong><span class="koboSpan" id="kobo.424.1"> class from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">sklearn</span></strong><span class="koboSpan" id="kobo.426.1"> library, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.427.1">from sklearn.cluster import KMeans</span></p></li>
				<li><span class="koboSpan" id="kobo.428.1">Next, we can initialize an instance of the K-Means model and specify the number of clusters being </span><strong class="source-inline"><span class="koboSpan" id="kobo.429.1">2</span></strong><span class="koboSpan" id="kobo.430.1">, the number of iterations being </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">10</span></strong><span class="koboSpan" id="kobo.432.1">, and the initialization method being </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">k-means++</span></strong><span class="koboSpan" id="kobo.434.1">. </span><span class="koboSpan" id="kobo.434.2">This initialization setting simply selects the initial cluster centers using an algorithm, with the aim of speeding up convergence. </span><span class="koboSpan" id="kobo.434.3">We can adjust the parameters in a process known as tuning in order to maximize the performance of the model. </span><span class="koboSpan" id="kobo.434.4">The code is illustrated in the following snippet:</span><p class="source-code"><span class="koboSpan" id="kobo.435.1">kmc = KuMeans(n_clusters=2, n_init=10, init="k-means++")</span></p></li>
				<li><span class="koboSpan" id="kobo.436.1">We can then use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.437.1">fit_predict()</span></strong><span class="koboSpan" id="kobo.438.1"> method to fit our data and predict the clusters for each of the observations. </span><span class="koboSpan" id="kobo.438.2">Notice in the following code snippet that the model is only fitting and predicting the outcomes based on the first two features alone:</span><p class="source-code"><span class="koboSpan" id="kobo.439.1">kmc_feat2_pred = kmc.fit_predict(X_scaled.iloc[:, :2])</span></p></li>
				<li><span class="koboSpan" id="kobo.440.1">Finally, we </span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.441.1">can go ahead and plot the results of our predictions in comparison to the true values of the known classes using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.442.1">seaborn</span></strong><span class="koboSpan" id="kobo.443.1"> library, as follows: </span><p class="source-code"><span class="koboSpan" id="kobo.444.1">plt.figure(figsize=(20, 5))</span></p><p class="source-code"><span class="koboSpan" id="kobo.445.1">plt.subplot(131)</span></p><p class="source-code"><span class="koboSpan" id="kobo.446.1">plt.title("Actual Results")</span></p><p class="source-code"><span class="koboSpan" id="kobo.447.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue=y, style=y, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.448.1">ax.legend(loc="upper right")</span></p><p class="source-code"><span class="koboSpan" id="kobo.449.1">plt.subplot(132)</span></p><p class="source-code"><span class="koboSpan" id="kobo.450.1">plt.title("KMeans Results (Features=2)")</span></p><p class="source-code"><span class="koboSpan" id="kobo.451.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue= kmc_feat2_pred , style= kmc_feat2_pred, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.452.1">ax.legend(loc="upper right")</span></p><p><span class="koboSpan" id="kobo.453.1">Upon executing this code, we get a scatter plot showing our results, as follows:</span></p><div id="_idContainer124" class="IMG---Figure"><span class="koboSpan" id="kobo.454.1"><img src="image/B17761_06_011.png.jpg" alt="Figure 6.11 – Results of the K-Means clustering model relative to the actual results "/></span></div><p class="figure-caption"><span class="koboSpan" id="kobo.455.1">Figure 6.11 – Results of the K-Means clustering model relative to the actual results</span></p><p><span class="koboSpan" id="kobo.456.1">While </span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.457.1">examining the two plots, we notice that the model did a remarkable job at separating the bulk of the data between the two clusters. </span><span class="koboSpan" id="kobo.457.2">Notice that </span><strong class="bold"><span class="koboSpan" id="kobo.458.1">K-Means</span></strong><span class="koboSpan" id="kobo.459.1">, as opposed to agglomerative clustering, separated the boundary between the two clusters quite sharply defined. </span><span class="koboSpan" id="kobo.459.2">K-Means is known as a </span><em class="italic"><span class="koboSpan" id="kobo.460.1">hard</span></em><span class="koboSpan" id="kobo.461.1"> clustering model in the sense that the centroids and their distance from data points dictate the membership of a data point to a given cluster. </span><span class="koboSpan" id="kobo.461.2">Notice that this strongly defined boundary was the case for only the first two features and yielded an accuracy of 86%. </span><span class="koboSpan" id="kobo.461.3">Let's go ahead and try this with a few more features.</span></p><p><span class="koboSpan" id="kobo.462.1">Although written in a non-Python fashion for illustrative purposes, we can fit out model using </span><strong class="source-inline"><span class="koboSpan" id="kobo.463.1">2</span></strong><span class="koboSpan" id="kobo.464.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">3</span></strong><span class="koboSpan" id="kobo.466.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">4</span></strong><span class="koboSpan" id="kobo.468.1">, and all the features, as follows:</span></p><p class="source-code"><span class="koboSpan" id="kobo.469.1">kmc_feat2_pred = kmc.fit_predict(X_scaled.iloc[:, :2])</span></p><p class="source-code"><span class="koboSpan" id="kobo.470.1">kmc_feat3_pred = kmc.fit_predict(X_scaled.iloc[:, :3])</span></p><p class="source-code"><span class="koboSpan" id="kobo.471.1">kmc_feat4_pred = kmc.fit_predict(X_scaled.iloc[:, :4])</span></p><p class="source-code"><span class="koboSpan" id="kobo.472.1">kmc_featall_pred = kmc.fit_predict(X_scaled.iloc[:, :])</span></p></li>
				<li><span class="koboSpan" id="kobo.473.1">Next, using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">subplot()</span></strong><span class="koboSpan" id="kobo.475.1"> methodology, we can generate four plots to illustrate </span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.476.1">the changes in which each individual subplot represents one of the plots depicted. </span><span class="koboSpan" id="kobo.476.2">Here's the code we'll need:</span><p class="source-code"><span class="koboSpan" id="kobo.477.1">plt.figure(figsize=(20, 5))</span></p><p class="source-code"><span class="koboSpan" id="kobo.478.1">plt.subplot(141)</span></p><p class="source-code"><span class="koboSpan" id="kobo.479.1">plt.title("KMeans Results (Features=2)")</span></p><p class="source-code"><span class="koboSpan" id="kobo.480.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue=kmc_feat2_pred, style=kmc_feat2_pred, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.481.1">ax.legend(loc="upper right")</span></p><p class="source-code"><span class="koboSpan" id="kobo.482.1"># Apply the same for the other plots</span></p><p><span class="koboSpan" id="kobo.483.1">With the code executed, we yield the following diagram showing the results:</span></p></li>
			</ol>
			<p class="figure-caption">  </p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<span class="koboSpan" id="kobo.484.1"><img src="image/B17761_06_012.jpg" alt="Figure 6.12 – Results of the K-Means clustering model with increasing features "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.485.1">Figure 6.12 – Results of the K-Means clustering model with increasing features</span></p>
			<p><span class="koboSpan" id="kobo.486.1">We can </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.487.1">calculate the accuracy using only two features to be ~86%, whereas three features yielded 89%. </span><span class="koboSpan" id="kobo.487.2">We will notice, however, that the numbers not only begin to plateau with more features included but also decrease when all features were included, yielding a lower accuracy of 82%. </span><span class="koboSpan" id="kobo.487.3">Note that as we begin to add more features to the model, we are </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.488.1">adding more dimensions. </span><span class="koboSpan" id="kobo.488.2">For example, with </span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.489.1">three features, we are now using a </span><strong class="bold"><span class="koboSpan" id="kobo.490.1">three-dimensional</span></strong><span class="koboSpan" id="kobo.491.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.492.1">3D</span></strong><span class="koboSpan" id="kobo.493.1">) model, as shown by the blended border between the two datasets. </span><span class="koboSpan" id="kobo.493.2">In some cases, the more features we have, the bigger the strain it will have on a given model. </span><span class="koboSpan" id="kobo.493.3">This borders a </span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.494.1">concept known as the </span><strong class="bold"><span class="koboSpan" id="kobo.495.1">Curse of Dimensionality</span></strong><span class="koboSpan" id="kobo.496.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.497.1">COD</span></strong><span class="koboSpan" id="kobo.498.1">) in the sense that the volume of the space begins to increase at an incredible rate given more dimensions, which can impact the performance of the model. </span><span class="koboSpan" id="kobo.498.2">We will touch on some of the ways we can remedy this in the future, particularly in the following tutorial, as we begin to discuss </span><strong class="bold"><span class="koboSpan" id="kobo.499.1">DR</span></strong><span class="koboSpan" id="kobo.500.1">.</span></p>
			<p><span class="koboSpan" id="kobo.501.1">In summary, we </span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.502.1">were able to apply the K-Means model on our dataset and were able to yield a considerable accuracy of 89% using the first three features. </span><span class="koboSpan" id="kobo.502.2">Let's now go ahead and explore the application of a statistical method such as GMM.</span></p>
			<h3><span class="koboSpan" id="kobo.503.1">GMMs</span></h3>
			<p><span class="koboSpan" id="kobo.504.1">Let's now </span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.505.1">explore the application of GMM</span><strong class="bold"><span class="koboSpan" id="kobo.506.1">s</span></strong><span class="koboSpan" id="kobo.507.1"> on our dataset. </span><span class="koboSpan" id="kobo.507.2">Recall that these models represent a mixture of probability distributions, and the membership of an observation to a cluster is calculated based on that probability and not on Euclidean distance. </span><span class="koboSpan" id="kobo.507.3">With that in mind, let's go ahead and get started, as follows:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.508.1">We can begin by importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">GaussianMixture</span></strong><span class="koboSpan" id="kobo.510.1"> class from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.511.1">sklearn</span></strong><span class="koboSpan" id="kobo.512.1"> library, like this:</span><p class="source-code"><span class="koboSpan" id="kobo.513.1">from sklearn.mixture import GaussianMixture</span></p></li>
				<li><span class="koboSpan" id="kobo.514.1">Next, we will create an instance of the model and specify the number of components as </span><strong class="source-inline"><span class="koboSpan" id="kobo.515.1">2</span></strong><span class="koboSpan" id="kobo.516.1">, and set the covariance type as </span><strong class="source-inline"><span class="koboSpan" id="kobo.517.1">full</span></strong><span class="koboSpan" id="kobo.518.1"> such that each component has its own covariance matrix, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.519.1">gmm = GaussianMixture(n_components=2, covariance_type="full")</span></p></li>
				<li><span class="koboSpan" id="kobo.520.1">We will then fit the data model with our data, once again using only the first two features, and predict the clusters for each of the observations, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.521.1">gmm_featAll_pred = 1-gmm.fit_predict(X_scaled.iloc[:, :2])</span></p></li>
				<li><span class="koboSpan" id="kobo.522.1">Finally, we </span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.523.1">can go ahead and plot the results using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">seaborn</span></strong><span class="koboSpan" id="kobo.525.1"> library, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.526.1">plt.figure(figsize=(20, 5))</span></p><p class="source-code"><span class="koboSpan" id="kobo.527.1">plt.subplot(131)</span></p><p class="source-code"><span class="koboSpan" id="kobo.528.1">plt.title("Actual Results")</span></p><p class="source-code"><span class="koboSpan" id="kobo.529.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue=y, style=y, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.530.1">ax.legend(loc="upper right")</span></p><p class="source-code"><span class="koboSpan" id="kobo.531.1">plt.subplot(132)</span></p><p class="source-code"><span class="koboSpan" id="kobo.532.1">plt.title("Gaussian Mixture Results (Features=All)")</span></p><p class="source-code"><span class="koboSpan" id="kobo.533.1">ax = sns.scatterplot(x="radius_mean", y="texture_mean", hue=gmm_featAll_pred, style=gmm_featAll_pred, data=X_scaled, markers=["s", "o"])</span></p><p class="source-code"><span class="koboSpan" id="kobo.534.1">ax.legend(loc="upper right")</span></p><p><span class="koboSpan" id="kobo.535.1">Upon executing our code, we yield the following output, showing the actual results of the dataset relative to our predicted ones:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<span class="koboSpan" id="kobo.536.1"><img src="image/B17761_06_013.png.jpg" alt="Figure 6.13 – Results of the GMM relative to the actual results "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.537.1">Figure 6.13 – Results of the GMM relative to the actual results</span></p>
			<p><span class="koboSpan" id="kobo.538.1">Once again, we can see that the boundary between the two classes is very defined </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.539.1">within the Gaussian model, in which there is little to no blending, as the actual results show, thus yielding an accuracy of ~85%. </span><span class="koboSpan" id="kobo.539.2">Notice, however, that relative to the K-Means model, the GMM predicted a dense circular distribution in blue, with some members of the orange class wrapping around it in a very non-circular fashion. </span></p>
			<p><span class="koboSpan" id="kobo.540.1">Similar to the previous model, we can once again add some more features to this model in an attempt to further improve the performance. </span><span class="koboSpan" id="kobo.540.2">However, we see in the following screenshot that despite the addition of more features from left to right, the model does not improve, and the predictive capabilities begin to suffer:</span></p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<span class="koboSpan" id="kobo.541.1"><img src="image/B17761_06_014.png.jpg" alt="Figure 6.14 – Results of the GMM with increasing features "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.542.1">Figure 6.14 – Results of the GMM with increasing features</span></p>
			<p><span class="koboSpan" id="kobo.543.1">In summary, over </span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.544.1">the course of this tutorial, we investigated the use of clustering analysis to develop various predictive models for a dataset while assuming the absence of labels. </span><span class="koboSpan" id="kobo.544.2">Throughout the tutorial, we investigated the use of three of the most common clustering models: </span><strong class="bold"><span class="koboSpan" id="kobo.545.1">agglomerative</span></strong><span class="koboSpan" id="kobo.546.1"> clustering, </span><strong class="bold"><span class="koboSpan" id="kobo.547.1">K-Means</span></strong><span class="koboSpan" id="kobo.548.1"> clustering, and GMMs. </span><span class="koboSpan" id="kobo.548.2">We investigated the specific properties of all three models and their applicability to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">Wisconsin Breast Cancer</span></strong><span class="koboSpan" id="kobo.550.1"> dataset. </span><span class="koboSpan" id="kobo.550.2">We determined that the K-Means model using three features showed optimal performance relative to other models, some </span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.551.1">of which utilized the dataset as a whole. </span><span class="koboSpan" id="kobo.551.2">We can speculate that all of the features contribute some level of significance when it comes to predictive power; however, the inclusion of all features within the models showed degraded performance. </span><span class="koboSpan" id="kobo.551.3">We will investigate some ways to mitigate this in the following section, pertaining to </span><strong class="bold"><span class="koboSpan" id="kobo.552.1">DR</span></strong><span class="koboSpan" id="kobo.553.1">.</span></p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.554.1">Understanding DR</span></h1>
			<p><span class="koboSpan" id="kobo.555.1">The second category of </span><strong class="bold"><span class="koboSpan" id="kobo.556.1">UL</span></strong><span class="koboSpan" id="kobo.557.1"> that we will discuss is known as </span><strong class="bold"><span class="koboSpan" id="kobo.558.1">DR</span></strong><span class="koboSpan" id="kobo.559.1">. </span><span class="koboSpan" id="kobo.559.2">As the full name states, these </span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.560.1">are simply methods used to reduce the number of dimensions in a given dataset. </span><span class="koboSpan" id="kobo.560.2">Take, for example, a highly featured dataset with 100 or so columns—DR algorithms can be used to help reduce the number of columns down to perhaps 5 while preserving the value that each of those original 100 columns contains. </span><span class="koboSpan" id="kobo.560.3">You can think of DR as the process of condensing a dataset in a horizontal fashion. </span><span class="koboSpan" id="kobo.560.4">The resulting columns can generally be divided into two types: new features, in the sense that a new </span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.561.1">column with new numerical values was generated in a process known as </span><strong class="bold"><span class="koboSpan" id="kobo.562.1">Feature Engineering</span></strong><span class="koboSpan" id="kobo.563.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.564.1">FE</span></strong><span class="koboSpan" id="kobo.565.1">), or old features, in the sense that only the most useful columns were preserved in a process known as </span><strong class="bold"><span class="koboSpan" id="kobo.566.1">feature selection</span></strong><span class="koboSpan" id="kobo.567.1">. </span><span class="koboSpan" id="kobo.567.2">Over </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.568.1">the course of the following section and within the confines of UL, we will be focusing more on the aspect of FE as we create new features representing reduced versions of many others. </span><span class="koboSpan" id="kobo.568.2">We can see a graphical illustration of this concept here: </span></p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<span class="koboSpan" id="kobo.569.1"><img src="image/B17761_06_015.jpg" alt="Figure 6.15 – Graphical representation of DR "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.570.1">Figure 6.15 – Graphical representation of DR</span></p>
			<p><span class="koboSpan" id="kobo.571.1">There are many different methods that we can use to implement DR, each with its own process and underlying theory; however, before we begin implementing these, there </span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.572.1">is a very important concept we need to address. </span><span class="koboSpan" id="kobo.572.2">You are now probably wondering why DR matters. </span><span class="koboSpan" id="kobo.572.3">Why would any data scientist eliminate features after another data scientist or data engineer went through all of the trouble to put together a comprehensive and rich dataset to begin with? </span><span class="koboSpan" id="kobo.572.4">There are three answers to this question, as outlined here: </span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.573.1">We are not necessarily eliminating any data from our given dataset but are exploring our data from a different window, which may provide some new insights that we would not have seen using the original dataset. </span></li>
				<li><span class="koboSpan" id="kobo.574.1">Developing models with many features is a computationally expensive process, therefore the ability to train our model using fewer features will always be faster, less computationally intensive, and more favorable.</span></li>
				<li><span class="koboSpan" id="kobo.575.1">The use </span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.576.1">of DR can help reduce noise within the dataset to further improve clustering models and data visualizations. </span><p><span class="koboSpan" id="kobo.577.1">With these answers in mind, let's now go ahead and talk about a concept that you will hear in many meetings, discussions, and interviews—the COD.</span></p></li>
			</ul>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.578.1">Avoiding the COD</span></h2>
			<p><span class="koboSpan" id="kobo.579.1">The COD is regarded </span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.580.1">as a general phenomenon that arises when handling highly dimensional datasets—a term that was originally coined by Richard E. </span><span class="koboSpan" id="kobo.580.2">Bellman. </span><span class="koboSpan" id="kobo.580.3">In essence, the COD refers to issues </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.581.1">that arise with highly dimensional datasets that do not occur in lower-dimensional datasets of similar size. </span><span class="koboSpan" id="kobo.581.2">As the number of features in a given dataset increases, the total number of samples will also increase proportionally. </span><span class="koboSpan" id="kobo.581.3">Take, for example, some dataset consisting of one dimension. </span><span class="koboSpan" id="kobo.581.4">Within this dataset, let's assume that we would need to examine a total of 10 regions. </span><span class="koboSpan" id="kobo.581.5">If we added a second dimension, we would now need to examine a total of 100 regions. </span><span class="koboSpan" id="kobo.581.6">Finally, if we added a third dimension, we would now need to examine a total of 1,000 regions. </span><span class="koboSpan" id="kobo.581.7">Think back for a moment to some of the datasets we have been working with so far that extend well beyond 1,000 rows and have at least 10 columns—the complexity of datasets such as these can grow quite rapidly. </span><span class="koboSpan" id="kobo.581.8">The main takeaway point here is that feature growth has a large impact on the development of a model. </span><span class="koboSpan" id="kobo.581.9">We can see a graphical illustration of this here: </span></p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<span class="koboSpan" id="kobo.582.1"><img src="image/B17761_06_016.jpg" alt="Figure 6.16 – Graphical representation of the COD "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.583.1">Figure 6.16 – Graphical representation of the COD</span></p>
			<p><span class="koboSpan" id="kobo.584.1">As the </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.585.1">number of </span><strong class="bold"><span class="koboSpan" id="kobo.586.1">features</span></strong><span class="koboSpan" id="kobo.587.1"> begins to increase, so does the overall </span><strong class="bold"><span class="koboSpan" id="kobo.588.1">complexity</span></strong><span class="koboSpan" id="kobo.589.1"> of an ML model, which can have a number of negative impacts such as overfitting, thus </span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.590.1">resulting in poor </span><strong class="bold"><span class="koboSpan" id="kobo.591.1">performance</span></strong><span class="koboSpan" id="kobo.592.1">. </span><span class="koboSpan" id="kobo.592.2">One of the main motivations to reduce the dimensionality of a dataset is to ensure that overfitting is avoided, thus resulting in a more robust model. </span><span class="koboSpan" id="kobo.592.3">We can see a graphical illustration of this here: </span></p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<span class="koboSpan" id="kobo.593.1"><img src="image/B17761_06_017.jpg" alt="Figure 6.17 – The effect of higher dimensions on model performance "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.594.1">Figure 6.17 – The effect of higher dimensions on model performance</span></p>
			<p><span class="koboSpan" id="kobo.595.1">The necessity to reduce datasets from being highly dimensional to a low-dimensional form is especially true in the life science and biotechnology sectors. </span><span class="koboSpan" id="kobo.595.2">Throughout </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.596.1">the many processes that scientists and engineers face within this field, there are generally hundreds of features relating to any given process. </span><span class="koboSpan" id="kobo.596.2">Whether we are looking for datasets relating to protein structures, monoclonal antibody titer, small </span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.597.1">molecule docking site selection, </span><strong class="bold"><span class="koboSpan" id="kobo.598.1">Bispecific T-cell Engager</span></strong><span class="koboSpan" id="kobo.599.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.600.1">BiTE</span></strong><span class="koboSpan" id="kobo.601.1">) drug design, or even datasets relating to </span><strong class="bold"><span class="koboSpan" id="kobo.602.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.603.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.604.1">NLP</span></strong><span class="koboSpan" id="kobo.605.1">), the reduction of features will always be useful </span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.606.1">and in many cases necessary for the development of a good ML model.</span></p>
			<p><span class="koboSpan" id="kobo.607.1">Now that </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.608.1">we have gained a better understanding of DR as it relates to the concept of the COD and the many benefits that can arise from these methods, let's now go ahead and look at a few of the most common models we should know about in this field.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.609.1">Tutorial – exploring DR models</span></h2>
			<p><span class="koboSpan" id="kobo.610.1">There are many different ways we can classify the numerous dimensionality algorithms </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.611.1">out there, based on type, function, or outcome, and so on. </span><span class="koboSpan" id="kobo.611.2">However, for the purposes of getting a strong overview of DR in just a few pages within this chapter, we will classify our models as being either of a linear or non-linear fashion. </span><span class="koboSpan" id="kobo.611.3">Linear and non-linear models are two different types of data transformations. </span><span class="koboSpan" id="kobo.611.4">We can think of data transformations as methods in which data is altered or reshaped in one way or another. </span><span class="koboSpan" id="kobo.611.5">We can loosely define linear methods as transformations in which the output of a model is proportional to its input. </span><span class="koboSpan" id="kobo.611.6">Take, for example, </span><em class="italic"><span class="koboSpan" id="kobo.612.1">p</span></em><span class="koboSpan" id="kobo.613.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.614.1">q</span></em><span class="koboSpan" id="kobo.615.1"> being two mathematical vectors. </span></p>
			<p><span class="koboSpan" id="kobo.616.1">We can consider a transformation to be linear when the following apply:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.617.1">The transformation of </span><em class="italic"><span class="koboSpan" id="kobo.618.1">p</span></em><span class="koboSpan" id="kobo.619.1"> is multiplied by a scalar and its result is the same as multiplying </span><em class="italic"><span class="koboSpan" id="kobo.620.1">p</span></em><span class="koboSpan" id="kobo.621.1"> by the scalar and then applying the transformation.</span></li>
				<li><span class="koboSpan" id="kobo.622.1">The transformation of </span><em class="italic"><span class="koboSpan" id="kobo.623.1">p</span></em><span class="koboSpan" id="kobo.624.1"> + </span><em class="italic"><span class="koboSpan" id="kobo.625.1">q</span></em><span class="koboSpan" id="kobo.626.1"> is the same as the transformation of </span><em class="italic"><span class="koboSpan" id="kobo.627.1">p</span></em><span class="koboSpan" id="kobo.628.1"> + the transformation of </span><em class="italic"><span class="koboSpan" id="kobo.629.1">q</span></em><span class="koboSpan" id="kobo.630.1">.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.631.1">If a model does not satisfy these two properties, it is considered a non-linear model. </span><span class="koboSpan" id="kobo.631.2">Many different models fall within the scope of these two classes; however, for the purposes of this chapter, we will take a look at four main models that have gained quite a bit of popularity within the data science community over recent years. </span><span class="koboSpan" id="kobo.631.3">We can see a graphical illustration of this here: </span></p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<span class="koboSpan" id="kobo.632.1"><img src="image/B17761_06_018.jpg" alt="Figure 6.18 – Two examples of models for each of the fields of DR "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.633.1">Figure 6.18 – Two examples of models for each of the fields of DR</span></p>
			<p><span class="koboSpan" id="kobo.634.1">Within the </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.635.1">scope of linear methods, we will take a close look at </span><strong class="bold"><span class="koboSpan" id="kobo.636.1">PCA</span></strong><span class="koboSpan" id="kobo.637.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.638.1">Singular Value Decomposition</span></strong><span class="koboSpan" id="kobo.639.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.640.1">SVD</span></strong><span class="koboSpan" id="kobo.641.1">). </span><span class="koboSpan" id="kobo.641.2">In addition, within the scope of non-linear methods, we will take a close look at </span><strong class="bold"><span class="koboSpan" id="kobo.642.1">t-distributed Stochastic Neighbor Embedding</span></strong><span class="koboSpan" id="kobo.643.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.644.1">t-SNE</span></strong><span class="koboSpan" id="kobo.645.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.646.1">Uniform Manifold Approximation and Projection</span></strong><span class="koboSpan" id="kobo.647.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.648.1">UMAP</span></strong><span class="koboSpan" id="kobo.649.1">). </span><span class="koboSpan" id="kobo.649.2">With these four models in mind and how they fit into the grand scheme of DR, let's go ahead and get started.</span></p>
			<h3><span class="koboSpan" id="kobo.650.1">PCA</span></h3>
			<p><span class="koboSpan" id="kobo.651.1">One of </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.652.1">the most common and widely </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.653.1">discussed forms of UL is </span><strong class="bold"><span class="koboSpan" id="kobo.654.1">PCA</span></strong><span class="koboSpan" id="kobo.655.1">. </span><span class="koboSpan" id="kobo.655.2">PCA is a linear form of DR, allowing users to transform a large dataset of correlated features into a smaller number of uncorrelated features known as principal components. </span><span class="koboSpan" id="kobo.655.3">These </span><strong class="bold"><span class="koboSpan" id="kobo.656.1">principal components</span></strong><span class="koboSpan" id="kobo.657.1">, although numerically fewer than their original features, can still retain as much of the variation or </span><em class="italic"><span class="koboSpan" id="kobo.658.1">richness</span></em><span class="koboSpan" id="kobo.659.1"> as the original dataset. </span><span class="koboSpan" id="kobo.659.2">We can see a graphical illustration of this here: </span></p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<span class="koboSpan" id="kobo.660.1"><img src="image/B17761_06_019.jpg" alt="Figure 6.19 – Graphical representation of PCA and its principal components "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.661.1">Figure 6.19 – Graphical representation of PCA and its principal components</span></p>
			<p><span class="koboSpan" id="kobo.662.1">There are a few things that need to happen in order to effectively implement PCA on any given dataset. </span><span class="koboSpan" id="kobo.662.2">Let's take a high-level overview of what these steps are and how they can impact the final outcome. </span><span class="koboSpan" id="kobo.662.3">We must first normalize or standardize our data to ensure that the mean is 0 and the standard deviation is 1. </span><span class="koboSpan" id="kobo.662.4">Next, we calculate what is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.663.1">covariance matrix</span></strong><span class="koboSpan" id="kobo.664.1">, which is a square matrix containing the covariance between each of the pairs of elements. </span><span class="koboSpan" id="kobo.664.2">In a </span><strong class="bold"><span class="koboSpan" id="kobo.665.1">two-dimensional</span></strong><span class="koboSpan" id="kobo.666.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.667.1">2D</span></strong><span class="koboSpan" id="kobo.668.1">) dataset, we can represent a covariance matrix as such:</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<span class="koboSpan" id="kobo.669.1"><img src="image/Formula_B17761_06_012.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.670.1">Next, we can calculate the </span><strong class="bold"><span class="koboSpan" id="kobo.671.1">eigenvalues</span></strong><span class="koboSpan" id="kobo.672.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.673.1">eigenvectors</span></strong><span class="koboSpan" id="kobo.674.1"> for the covariance matrix, as follows:</span></p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<span class="koboSpan" id="kobo.675.1"><img src="image/Formula_B17761_06_013.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.676.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.677.1"><img src="image/Formula_Symbol.png" alt=""/></span></em><span class="koboSpan" id="kobo.678.1"> is an eigenvalue for a given matrix </span><em class="italic"><span class="koboSpan" id="kobo.679.1">A</span></em><span class="koboSpan" id="kobo.680.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.681.1">I</span></em><span class="koboSpan" id="kobo.682.1"> is the identity matrix. </span><span class="koboSpan" id="kobo.682.2">Using the eigenvector, we can determine the eigenvalue </span><em class="italic"><span class="koboSpan" id="kobo.683.1">v</span></em><span class="koboSpan" id="kobo.684.1"> using the following equation:</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<span class="koboSpan" id="kobo.685.1"><img src="image/Formula_B17761_06_014.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.686.1">Next, the eigenvalues are ordered from the largest to the smallest, which represent the components in order of significance. </span><span class="koboSpan" id="kobo.686.2">A dataset with </span><em class="italic"><span class="koboSpan" id="kobo.687.1">n</span></em><span class="koboSpan" id="kobo.688.1"> variables or features will have </span><em class="italic"><span class="koboSpan" id="kobo.689.1">n</span></em><span class="koboSpan" id="kobo.690.1"> eigenvalues and eigenvectors. </span><span class="koboSpan" id="kobo.690.2">We can then limit the number of eigenvalues or vectors to a predetermined number, thus reducing the dimensions of our dataset. </span><span class="koboSpan" id="kobo.690.3">We can then form what we call a feature vector, using the eigenvectors of interest.</span></p>
			<p><span class="koboSpan" id="kobo.691.1">Finally, we can form the </span><strong class="bold"><span class="koboSpan" id="kobo.692.1">principal components</span></strong><span class="koboSpan" id="kobo.693.1"> using the </span><strong class="bold"><span class="koboSpan" id="kobo.694.1">transpose</span></strong><span class="koboSpan" id="kobo.695.1"> of the feature vector, as well as the transpose of the scaled data of the original dataset, and multiplying the two together, as follows:</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<span class="koboSpan" id="kobo.696.1"><img src="image/Formula_B17761_06_015.jpg" alt=""/></span>
				</div>
			</div>
			<p><span class="koboSpan" id="kobo.697.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.698.1">PrincipalComponents</span></em><span class="koboSpan" id="kobo.699.1"> is returned as a matrix. </span><span class="koboSpan" id="kobo.699.2">Easy, right?</span></p>
			<p><span class="koboSpan" id="kobo.700.1">Let's now go ahead and implement PCA using Python, as follows:</span></p>
			<ol>
				<li value="1"><span class="koboSpan" id="kobo.701.1">First, we import PCA from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.702.1">sklearn</span></strong><span class="koboSpan" id="kobo.703.1"> library and instantiate a new PCA model. </span><span class="koboSpan" id="kobo.703.2">We can set the number of components as </span><strong class="source-inline"><span class="koboSpan" id="kobo.704.1">2</span></strong><span class="koboSpan" id="kobo.705.1">, representing the fact that we only want two components returned to us, and use </span><strong class="source-inline"><span class="koboSpan" id="kobo.706.1">full</span></strong><span class="koboSpan" id="kobo.707.1"> for </span><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">svd_solver</span></strong><span class="koboSpan" id="kobo.709.1">. </span><span class="koboSpan" id="kobo.709.2">We can then fit the data on our scaled dataset, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.710.1">from sklearn.decomposition import PCA</span></p><div id="_idContainer138"/><div id="_idContainer139"/><div id="_idContainer140"/><p class="source-code"><span class="koboSpan" id="kobo.711.1">pca_2d = PCA(n_components=2, svd_solver='full')</span></p><p class="source-code"><span class="koboSpan" id="kobo.712.1">pca_2d.fit(X_scaled)</span></p></li>
				<li><span class="koboSpan" id="kobo.713.1">Next, we can transform our data and assign our output matrix to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.714.1">data_pca_2d</span></strong><span class="koboSpan" id="kobo.715.1"> variable, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.716.1">data_pca_2d = pca_2d.fit_transform(X_scaled)</span></p></li>
				<li><span class="koboSpan" id="kobo.717.1">Finally, we can go ahead and plot the results using </span><strong class="source-inline"><span class="koboSpan" id="kobo.718.1">seaborn</span></strong><span class="koboSpan" id="kobo.719.1">, as follows:</span><p class="source-code"><span class="koboSpan" id="kobo.720.1">plt.xlabel("Principal Component 1")</span></p><p class="source-code"><span class="koboSpan" id="kobo.721.1">plt.ylabel("Principal Component 2")</span></p><p class="source-code"><span class="koboSpan" id="kobo.722.1">sns.scatterplot(x=data_pca_2d[:,0], y=data_pca_2d[:,1], hue=y, style=y, markers=["s", "o"])</span></p><p><span class="koboSpan" id="kobo.723.1">Upon executing this code, this will yield a scatter plot showing our principal components with our points colored using </span><strong class="source-inline"><span class="koboSpan" id="kobo.724.1">y</span></strong><span class="koboSpan" id="kobo.725.1">, as shown here:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<span class="koboSpan" id="kobo.726.1"><img src="image/B17761_06_020.png.jpg" alt="Figure 6.20 – Scatter plot of the PCA results "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.727.1">Figure 6.20 – Scatter plot of the PCA results</span></p>
			<p><span class="koboSpan" id="kobo.728.1">PCA is a fast and efficient method best used as a precursor to the development of </span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.729.1">ML models when the number of dimensions has become too complex. </span><span class="koboSpan" id="kobo.729.2">Think back for a </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.730.1">moment to the dataset we used in our clustering analysis relating to breast cancer predictions. </span><span class="koboSpan" id="kobo.730.2">Instead of running our models on the raw or scaled data, we could implement a DR algorithm such as PCA to reduce our dimensions down only two principal components before applying the subsequent clustering model. </span><span class="koboSpan" id="kobo.730.3">Remember that PCA is only one of many linear models. </span><span class="koboSpan" id="kobo.730.4">Let's now go ahead and explore another popular linear model known as SVD.</span></p>
			<h3><span class="koboSpan" id="kobo.731.1">SVD</span></h3>
			<p><strong class="bold"><span class="koboSpan" id="kobo.732.1">SVD</span></strong><span class="koboSpan" id="kobo.733.1"> is a </span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.734.1">popular </span><strong class="bold"><span class="koboSpan" id="kobo.735.1">matrix decomposition</span></strong><span class="koboSpan" id="kobo.736.1"> method commonly used to reduce a dataset to a simpler form. </span><span class="koboSpan" id="kobo.736.2">In this section, we will </span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.737.1">focus specifically on the application of truncated SVD. </span><span class="koboSpan" id="kobo.737.2">This model is quite similar to that of PCA; however, the </span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.738.1">main difference is that the estimator does not center prior to its computation. </span><span class="koboSpan" id="kobo.738.2">Essentially, this difference allows the model to be used with sparse matrices quite efficiently. </span></p>
			<p><span class="koboSpan" id="kobo.739.1">Let's now introduce </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.740.1">and take a look at a new dataset that we can use to apply SVD: </span><em class="italic"><span class="koboSpan" id="kobo.741.1">single-cell RNA</span></em><span class="koboSpan" id="kobo.742.1"> (where </span><strong class="bold"><span class="koboSpan" id="kobo.743.1">RNA</span></strong><span class="koboSpan" id="kobo.744.1"> stands for </span><strong class="bold"><span class="koboSpan" id="kobo.745.1">ribonucleic acid</span></strong><span class="koboSpan" id="kobo.746.1">). </span><span class="koboSpan" id="kobo.746.2">The dataset can be found at </span><a href="http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt"><span class="koboSpan" id="kobo.747.1">http://blood.stemcells.cam.ac.uk/data/nestorowa_corrected_log2_transformed_counts.txt</span></a><span class="koboSpan" id="kobo.748.1">. </span><span class="koboSpan" id="kobo.748.2">This dataset pertains to the topic of single-cell sequencing—a process that examines sequences of individual cells to better understand their properties and functions. </span><span class="koboSpan" id="kobo.748.3">Datasets such as these tend to have many columns of data, making them prime candidates for DR models. </span><span class="koboSpan" id="kobo.748.4">Let's go ahead and import this dataset, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.749.1">dfx = pd.read_csv("../../datasets/single_cell_rna/nestorowa_corrected_log2_transformed_counts.txt", sep=' ',  )</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.750.1">dfx.shape</span></p>
			<p><span class="koboSpan" id="kobo.751.1">Taking a look at the shape, we can see that there are 3,991 rows of data and 1,645 columns. </span><span class="koboSpan" id="kobo.751.2">Relative to the many other datasets we have used, this number is quite large. </span><span class="koboSpan" id="kobo.751.3">Within the field of biotechnology, DR is very commonly used to help reduce such datasets into more manageable entities. </span><span class="koboSpan" id="kobo.751.4">Notice that the index contains some information about the type of cell we are looking at. </span><span class="koboSpan" id="kobo.751.5">To make our visuals more interesting, let's capture this annotation data by executing the following code:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.752.1">dfy = pd.DataFrame()</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.753.1">dfy['annotation'] = dfx.index.str[:4]</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.754.1">dfy['annotation'].value_counts()</span></p>
			<p><span class="koboSpan" id="kobo.755.1">With the data all set, let's go ahead and implement truncated SVD on this dataset. </span><span class="koboSpan" id="kobo.755.2">We can once again begin by instantiating a truncated SVD model and setting the components to </span><strong class="source-inline"><span class="koboSpan" id="kobo.756.1">2</span></strong><span class="koboSpan" id="kobo.757.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.758.1">7</span></strong><span class="koboSpan" id="kobo.759.1"> iterations, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.760.1">from sklearn.decomposition import TruncatedSVD</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.761.1">svd_2d = TruncatedSVD(n_components=2, n_iter=7)</span></p>
			<p><span class="koboSpan" id="kobo.762.1">Next, we can go ahead and use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">fit_transform()</span></strong><span class="koboSpan" id="kobo.764.1"> method to both fit our data and transform the DataFrame to a two-column dataset, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.765.1">data_svd_2d = svd_2d.fit_transform(dfx)</span></p>
			<p><span class="koboSpan" id="kobo.766.1">Finally, we </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.767.1">can finish things up by plotting </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.768.1">our dataset using a scatter plot, and color by annotation. </span><span class="koboSpan" id="kobo.768.2">The code is illustrated in the following snippet:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.769.1">sns.scatterplot(x=data_svd_2d[:,0], y=data_svd_2d[:,1], hue=dfy.annotation, style=dfy.annotation, markers = ["o", "s", "v"])</span></p>
			<p><span class="koboSpan" id="kobo.770.1">We can see the results of executing this code in the following screenshot: </span></p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<span class="koboSpan" id="kobo.771.1"><img src="image/B17761_06_021.png.jpg" alt="Figure 6.21 – Scatter plot of the results of the SVD model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.772.1">Figure 6.21 – Scatter plot of the results of the SVD model</span></p>
			<p><span class="koboSpan" id="kobo.773.1">In the preceding screenshot, we can see the almost 1,400 columns worth of data being reduced to a simple 2D representation—quite fascinating, isn't it? </span><span class="koboSpan" id="kobo.773.2">One of the biggest advantages of being able to reduce data in this fashion is that it assists with model development. </span><span class="koboSpan" id="kobo.773.3">Let's assume, for the sake of example, that we wish to </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.774.1">implement any of our previous </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.775.1">clustering algorithms on this extensive dataset. </span><span class="koboSpan" id="kobo.775.2">It would take considerably longer to train any given model on a dataset of nearly 1,400 columns compared to a dataset with 2 columns. </span><span class="koboSpan" id="kobo.775.3">In fact, if we implemented a GMM on this dataset, the total training time would be </span><strong class="bold"><span class="koboSpan" id="kobo.776.1">12.4 s ± 158 ms</span></strong><span class="koboSpan" id="kobo.777.1"> using the original dataset, relative to </span><strong class="bold"><span class="koboSpan" id="kobo.778.1">4.06 ms ± 26.6 ms</span></strong><span class="koboSpan" id="kobo.779.1"> using the reduced dataset. </span><span class="koboSpan" id="kobo.779.2">Although linear models can be very useful when it comes to DR, non-linear models can also be similarly impressive. </span><span class="koboSpan" id="kobo.779.3">Next, let's take a look at a popular model known as t-SNE.</span></p>
			<h3><span class="koboSpan" id="kobo.780.1">t-SNE</span></h3>
			<p><span class="koboSpan" id="kobo.781.1">On </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.782.1">the side of non-linear DR, one of the most popular models commonly seen in action is </span><strong class="bold"><span class="koboSpan" id="kobo.783.1">t-SNE</span></strong><span class="koboSpan" id="kobo.784.1">. </span><span class="koboSpan" id="kobo.784.2">One of the unique features of the t-SNE model relative to the other dimensionality models we have talked about is the fact that it uses probability distribution to represent similarities between neighbors. </span><span class="koboSpan" id="kobo.784.3">Simply stated, t-SNE is a statistical method allowing for the DR and visualization of high-dimensional data in which similar points are close together and dissimilar ones are further apart.</span></p>
			<p><span class="koboSpan" id="kobo.785.1">t-SNE </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.786.1">is a type of </span><strong class="bold"><span class="koboSpan" id="kobo.787.1">manifold</span></strong><span class="koboSpan" id="kobo.788.1"> model, which </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.789.1">from a mathematical perspective is a topological </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.790.1">space resembling </span><strong class="bold"><span class="koboSpan" id="kobo.791.1">Euclidean</span></strong><span class="koboSpan" id="kobo.792.1"> space. </span><span class="koboSpan" id="kobo.792.2">The concept of a manifold is complex, extensive, and well beyond the scope of this book. </span><span class="koboSpan" id="kobo.792.3">For the purposes of simplicity, we will state that manifolds describe a large number of geometric surfaces such as a sphere, torus, or cross surface. </span><span class="koboSpan" id="kobo.792.4">Within the confines of the t-SNE model, the main objective is to use geometric shapes to give users a feel or intuition of how the high-dimensional data is arranged or organized. </span></p>
			<p><span class="koboSpan" id="kobo.793.1">Let's now take a close look at the application of t-SNE using Python. </span><span class="koboSpan" id="kobo.793.2">Once again, we can apply this model on our single-cell RNA dataset and get a sense of what the high-dimensional organization of this data looks like from a geometric perspective. </span><span class="koboSpan" id="kobo.793.3">Many parameters within t-SNE can be changed and tuned to fit given purposes; however, there is one in particular worth mentioning briefly—perplexity. </span><strong class="bold"><span class="koboSpan" id="kobo.794.1">Perplexity</span></strong><span class="koboSpan" id="kobo.795.1"> is a </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.796.1">parameter related to the number of nearest </span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.797.1">neighbors used as input when it comes to manifold learning. </span><span class="koboSpan" id="kobo.797.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.798.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.799.1"> library recommends considering values between 5 and 50. </span><span class="koboSpan" id="kobo.799.2">Let's go ahead and </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.800.1">take a look at a few examples.</span></p>
			<p><span class="koboSpan" id="kobo.801.1">Implementing </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.802.1">this model is quite simple, thanks to the high-level </span><strong class="bold"><span class="koboSpan" id="kobo.803.1">Application Programming Interface</span></strong><span class="koboSpan" id="kobo.804.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.805.1">API</span></strong><span class="koboSpan" id="kobo.806.1">) provided by </span><strong class="source-inline"><span class="koboSpan" id="kobo.807.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.808.1">. </span><span class="koboSpan" id="kobo.808.2">We can begin by importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.809.1">TSNE</span></strong><span class="koboSpan" id="kobo.810.1"> class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.812.1"> and setting the number of components to </span><strong class="source-inline"><span class="koboSpan" id="kobo.813.1">2</span></strong><span class="koboSpan" id="kobo.814.1"> and the perplexity to </span><strong class="source-inline"><span class="koboSpan" id="kobo.815.1">10</span></strong><span class="koboSpan" id="kobo.816.1">. </span><span class="koboSpan" id="kobo.816.2">We can then chain the </span><strong class="source-inline"><span class="koboSpan" id="kobo.817.1">fit_transform()</span></strong><span class="koboSpan" id="kobo.818.1"> method using our dataset, as illustrated in the following code snippet:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.819.1">from sklearn.manifold import TSNE</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.820.1">data_tsne_2d_p10 = TSNE(n_components=2, perplexity=10.0).fit_transform(dfx)</span></p>
			<p><span class="koboSpan" id="kobo.821.1">We can then go ahead and plot our data to visualize the results using </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">seaborn</span></strong><span class="koboSpan" id="kobo.823.1">, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.824.1">sns.scatterplot(x=data_tsne_2d_p10[:,0], y=data_tsne_2d_p10[:,1], hue=dfy.annotation, style=dfy.annotation, markers = ["o", "s", "v"])</span></p>
			<p><span class="koboSpan" id="kobo.825.1">We can see the output of this in the following screenshot:</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<span class="koboSpan" id="kobo.826.1"><img src="image/B17761_06_022.png.jpg" alt="Figure 6.22 – Scatter plot of the results of the t-SNE model "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.827.1">Figure 6.22 – Scatter plot of the results of the t-SNE model</span></p>
			<p><span class="koboSpan" id="kobo.828.1">Quite the result! </span><span class="koboSpan" id="kobo.828.2">We can see in the preceding screenshot that the model, without any knowledge of the labels, made a 2D projection of the relationship between the data points using the huge dataset it was given. </span><span class="koboSpan" id="kobo.828.3">The geometric shape </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.829.1">produced gives us a sense of the </span><em class="italic"><span class="koboSpan" id="kobo.830.1">look</span></em><span class="koboSpan" id="kobo.831.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.832.1">feel</span></em><span class="koboSpan" id="kobo.833.1"> of the data. </span><span class="koboSpan" id="kobo.833.2">We can see based on this </span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.834.1">depiction that a few points seem to be considered outliers as they are depicted much further away, like islands relative to the main continent. </span><span class="koboSpan" id="kobo.834.2">Recall that we used a perplexity value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.835.1">10</span></strong><span class="koboSpan" id="kobo.836.1"> for this particular diagram. </span><span class="koboSpan" id="kobo.836.2">Let's go ahead and explore this parameter using a few different values, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.837.1">data_tsne_2d_p1 = TSNE(n_components=2, perplexity=1.0).fit_transform(dfx)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.838.1">data_tsne_2d_p10 = TSNE(n_components=2, perplexity=10.0).fit_transform(dfx)</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.839.1">data_tsne_2d_p30 = TSNE(n_components=2, perplexity=30.0).fit_transform(dfx)</span></p>
			<p><span class="koboSpan" id="kobo.840.1">Using these calculated values, we can visualize them next to each other using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.841.1">seaborn</span></strong><span class="koboSpan" id="kobo.842.1"> library, as follows:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<span class="koboSpan" id="kobo.843.1"><img src="image/B17761_06_023.png.jpg" alt="Figure 6.23 – Scatter plots of the t-SNE model with increasing perplexities "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.844.1">Figure 6.23 – Scatter plots of the t-SNE model with increasing perplexities</span></p>
			<p><span class="koboSpan" id="kobo.845.1">When it comes to high-dimensional data, t-SNE is one of the most commonly used models to not only reduce your dimensions but also explore your data by getting </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.846.1">a unique </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.847.1">feel for its features and their relationships. </span><span class="koboSpan" id="kobo.847.2">Although t-SNE can be useful and effective, it does have a few negative aspects. </span><span class="koboSpan" id="kobo.847.3">First, it does not scale well for large sample sizes such as those you would see in some cases of RNA sequencing data. </span><span class="koboSpan" id="kobo.847.4">Second, it also does not preserve global data structures in the sense that similarities across different clusters are not well maintained. </span><span class="koboSpan" id="kobo.847.5">Another popular model that attempts to address some of these concerns and utilizes a similar approach to t-SNE is known as UMAP. </span><span class="koboSpan" id="kobo.847.6">Let's explore this model in the following section.</span></p>
			<h3><span class="koboSpan" id="kobo.848.1">UMAP</span></h3>
			<p><span class="koboSpan" id="kobo.849.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.850.1">UMAP</span></strong><span class="koboSpan" id="kobo.851.1"> model </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.852.1">is a popular algorithm used for the reduction of dimensions and visualizing </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.853.1">of data based on manifold learning techniques, similar to that of t-SNE. </span><span class="koboSpan" id="kobo.853.2">There are three main assumptions that the algorithm is founded on, as described on their main website (</span><a href="https://umap-learn.readthedocs.io/en/latest"><span class="koboSpan" id="kobo.854.1">https://umap-learn.readthedocs.io/en/latest</span></a><span class="koboSpan" id="kobo.855.1">) and outlined here:</span></p>
			<ul>
				<li><span class="koboSpan" id="kobo.856.1">The dataset is uniformly distributed on a Riemannian manifold.</span></li>
				<li><span class="koboSpan" id="kobo.857.1">The Riemannian metric is locally constant.</span></li>
				<li><span class="koboSpan" id="kobo.858.1">The manifold is locally connected.</span></li>
			</ul>
			<p><span class="koboSpan" id="kobo.859.1">Although UMAP and t-SNE are quite similar, there are a few key differences between </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.860.1">them. </span><span class="koboSpan" id="kobo.860.2">One </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.861.1">of the most important differences relates to the idea of similarity preservation. </span><span class="koboSpan" id="kobo.861.2">The UMAP model claims to preserve both local and global data in the sense that both local and global—or inter-cluster and intra-cluster—information is maintained. </span><span class="koboSpan" id="kobo.861.3">We can see a graphical representation of this concept here: </span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<span class="koboSpan" id="kobo.862.1"><img src="image/B17761_06_024.jpg" alt="Figure 6.24 – Graphical representation of local and global similarities "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.863.1">Figure 6.24 – Graphical representation of local and global similarities</span></p>
			<p><span class="koboSpan" id="kobo.864.1">Let's now go ahead and apply UMAP on our single-cell RNA dataset. </span><span class="koboSpan" id="kobo.864.2">We can begin by importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.865.1">umap</span></strong><span class="koboSpan" id="kobo.866.1"> library and instantiating a new instance of the UMAP model in which we specify the number of components as </span><strong class="source-inline"><span class="koboSpan" id="kobo.867.1">2</span></strong><span class="koboSpan" id="kobo.868.1"> and the number of neighbors as </span><strong class="source-inline"><span class="koboSpan" id="kobo.869.1">5</span></strong><span class="koboSpan" id="kobo.870.1">. </span><span class="koboSpan" id="kobo.870.2">This second parameter represents the size of a local </span><strong class="bold"><span class="koboSpan" id="kobo.871.1">neighborhood</span></strong><span class="koboSpan" id="kobo.872.1"> used for manifold approximation. </span><span class="koboSpan" id="kobo.872.2">The code is illustrated in the following snippet:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.873.1">import umap</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.874.1">data_umap_2d_n5 = umap.UMAP(n_components=2, n_neighbors=5).fit_transform(dfx)</span></p>
			<p><span class="koboSpan" id="kobo.875.1">We </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.876.1">can then go ahead and plot the data using </span><strong class="source-inline"><span class="koboSpan" id="kobo.877.1">seaborn</span></strong><span class="koboSpan" id="kobo.878.1">, as follows:</span></p>
			<p class="source-code"><span class="koboSpan" id="kobo.879.1">sns.scatterplot(x=data_umap_2d_n5[:,0], y=data_umap_2d_n5[:,1], hue=dfy.annotation, style=dfy.annotation, markers = ["o", "s", "v"])</span></p>
			<p><span class="koboSpan" id="kobo.880.1">Upon </span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.881.1">executing the code, we yield the following output:</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<span class="koboSpan" id="kobo.882.1"><img src="image/B17761_06_025.png.jpg" alt="Figure 6.25 – Scatter plot of the UMAP results "/></span>
				</div>
			</div>
			<p class="figure-caption"><span class="koboSpan" id="kobo.883.1">Figure 6.25 – Scatter plot of the UMAP results</span></p>
			<p><span class="koboSpan" id="kobo.884.1">Once again, quite the visual! </span><span class="koboSpan" id="kobo.884.2">We can see in this depiction relative to t-SNE that some clusters have moved around. </span><span class="koboSpan" id="kobo.884.3">If you recall in t-SNE, the majority of the data was pulled together with no regard as to how similar clusters were to one another. </span><span class="koboSpan" id="kobo.884.4">Using UMAP, this information is preserved, and we are able to get a better sense of how these clusters relate to one another. </span><span class="koboSpan" id="kobo.884.5">Notice the spread of the </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.885.1">data relative </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.886.1">to its depiction in t-SNE. </span><span class="koboSpan" id="kobo.886.2">Similar to t-SNE, we can see that some </span><em class="italic"><span class="koboSpan" id="kobo.887.1">groups</span></em><span class="koboSpan" id="kobo.888.1"> of points are clustered together in different neighborhoods. </span></p>
			<p><span class="koboSpan" id="kobo.889.1">In summary, UMAP is a powerful model similar to t-SNE in which both local and global information is preserved when it comes to neighborhoods or clusters. </span><span class="koboSpan" id="kobo.889.2">Most commonly used for visualizations, this model is an excellent way to gain a sense of the </span><em class="italic"><span class="koboSpan" id="kobo.890.1">look</span></em><span class="koboSpan" id="kobo.891.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.892.1">feel</span></em><span class="koboSpan" id="kobo.893.1"> of any high-dimensional dataset in just a few lines of code.</span></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.894.1">Summary</span></h1>
			<p><span class="koboSpan" id="kobo.895.1">Over the course of this chapter, we gained a strong and high-level understanding of the field of UL, its uses, and its applications. </span><span class="koboSpan" id="kobo.895.2">We then explored a few of the most popular ML methods as they relate to clustering and DR. </span><span class="koboSpan" id="kobo.895.3">Within the field of clustering, we looked over some of the most commonly used models such as hierarchical clustering, K-Means clustering, and GMMs. </span><span class="koboSpan" id="kobo.895.4">We learned about the differences between Euclidean distances and probabilities and how they relate to model predictions. </span><span class="koboSpan" id="kobo.895.5">In addition, we also applied these models to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.896.1">Wisconsin Breast Cancer</span></strong><span class="koboSpan" id="kobo.897.1"> dataset and managed to achieve relatively high accuracy in a few of them. </span><span class="koboSpan" id="kobo.897.2">Within the field of DR, we gained a strong understanding of the significance of the field as it relates to the COD. </span><span class="koboSpan" id="kobo.897.3">We then implemented a number of models such as PCA, SVD, t-SNE, and UMAP using the </span><em class="italic"><span class="koboSpan" id="kobo.898.1">single-cell RNA</span></em><span class="koboSpan" id="kobo.899.1"> dataset in which we managed to reduce more than 1,400 columns down to 2. </span><span class="koboSpan" id="kobo.899.2">We then visualized our results using </span><strong class="source-inline"><span class="koboSpan" id="kobo.900.1">seaborn</span></strong><span class="koboSpan" id="kobo.901.1"> and examined the differences between the models. </span><span class="koboSpan" id="kobo.901.2">Over the course of this chapter, we managed to develop our models without the use of labels, which we used only for comparison after the development process. </span></p>
			<p><span class="koboSpan" id="kobo.902.1">Over the course of the next chapter, we will explore the field of SML, in which we use data in addition to its labels to develop powerful predictive models.</span></p>
		</div>
	</body></html>