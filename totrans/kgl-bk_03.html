<html><head></head><body>
  <div id="_idContainer047" class="Basic-Text-Frame">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-38" class="chapterTitle">Organizing Data with Datasets</h1>
    <p class="normal">In his story <em class="italic">The Adventure of the Copper Beeches</em>, Arthur Conan Doyle has Sherlock Holmes shout “<em class="italic">Data! Data! Data! I cannot make bricks without clay</em>.” This mindset, which served the most famous detective in literature so well, should be adopted by every data scientist. For that reason, we begin the more technical part of this book with a chapter dedicated to data: specifically, in the Kaggle context, leveraging the power of the Kaggle Datasets functionality for our purposes.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Setting up a dataset</li>
      <li class="bulletList">Gathering the data</li>
      <li class="bulletList">Working with datasets</li>
      <li class="bulletList">Using Kaggle Datasets in Google Colab</li>
      <li class="bulletList">Legal caveats</li>
    </ul>
    <h1 id="_idParaDest-39" class="heading-1">Setting up a dataset</h1>
    <p class="normal">In principle, any data you can use you <a id="_idIndexMarker143"/>can upload to Kaggle (subject to limitations; see the <em class="italic">Legal caveats</em> section later on). The specific limits at the time of writing are <strong class="keyWord">100 GB per private dataset</strong> and a <strong class="keyWord">100 GB total</strong> quota. Keep in mind that the size limit per single dataset is calculated uncompressed; uploading compressed versions speeds up the transfer but does not help against the limits. You can check the most recent documentation for the <a id="_idIndexMarker144"/>datasets at this link: <a href="https://www.kaggle.com/docs/datasets"><span class="url">https://www.kaggle.com/docs/datasets</span></a>.</p>
    <p class="normal">Kaggle promotes itself as a “home of data science” and the impressive collection of datasets available from the site certainly lends some credence to that claim. Not only can you find data on topics<a id="_idIndexMarker145"/> ranging from oil prices to anime recommendations, but it is also impressive how quickly data ends up there. When the emails of <em class="italic">Anthony Fauci</em> were released under the <em class="italic">Freedom of Information Act</em> in May 2021 (<a href="https://www.washingtonpost.com/politics/interactive/2021/tony-fauci-emails/"><span class="url">https://www.washingtonpost.com/politics/interactive/2021/tony-fauci-emails/</span></a>), they were uploaded as a Kaggle dataset a mere 48 hours later.</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.1: Trending and popular datasets on Kaggle</p>
    <p class="normal">Before uploading the data for your project into a dataset, make sure to check the existing content. For several popular applications (image classification, NLP, financial time series), there is a chance it has already been stored there.</p>
    <p class="normal">For the sake of this introduction, let us assume the kind of data you will be using in your project is not already there, so you need to create a new dataset. When you head to the menu with three lines on the left-hand side and click on <strong class="screenText">Data</strong>, you will be redirected to the <strong class="screenText">Datasets</strong> page:</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_02.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 2.2: The Datasets page</p>
    <p class="normal">When you click on <strong class="screenText">+ New Dataset</strong>, you<a id="_idIndexMarker146"/> will be prompted for the basics: uploading the actual data and giving it a title:</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_03.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 2.3: Entering dataset details</p>
    <p class="normal">The icons on the left-hand side correspond to the different sources you can utilize for your dataset. We describe them in the order they are shown on the page:</p>
    <ul>
      <li class="bulletList">Upload a file from a local drive (shown in the figure)</li>
      <li class="bulletList">Create from a remote URL</li>
      <li class="bulletList">Import a GitHub repository </li>
      <li class="bulletList">Use output files from an existing Notebook</li>
      <li class="bulletList">Import a Google Cloud Storage file</li>
    </ul>
    <p class="normal"><strong class="keyWord">An important point about the GitHub option</strong>: This feature is <a id="_idIndexMarker147"/>particularly handy when it comes to experimental libraries. While frequently offering hitherto unavailable functionality, they are usually not<a id="_idIndexMarker148"/> included in the Kaggle environment, so if you want to use such a library in your code, you can import it as a dataset, as demonstrated below:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Go to <strong class="screenText">Datasets</strong> and click <strong class="screenText">+ New Dataset</strong>.</li>
      <li class="numberedList">Select the GitHub icon.</li>
      <li class="numberedList">Insert the link to the repository, as well as the title for the dataset.</li>
      <li class="numberedList">Click on <strong class="screenText">Create</strong> at the bottom right:</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B17574_02_04.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 2.4: Dataset from GitHub repository</p>
    <p class="normal">Next to the <strong class="screenText">Create</strong> button, there is another one marked <strong class="screenText">Private</strong>. By default, any dataset you create is private: only you, its creator, can view and edit it. It is probably a good idea to leave this setting at default at the dataset creation stage and only at a later stage make it public (available to either a select list of contributors, or everyone).</p>
    <p class="normal">Keep in mind that Kaggle is a popular <a id="_idIndexMarker149"/>platform and many people upload their datasets – including private ones – so try to think of a non-generic title. This will increase the chance of your dataset actually being noticed.</p>
    <p class="normal">Once you have completed all the steps and clicked <strong class="screenText">Create</strong>, voilà! Your first dataset is ready. You can then head to the <strong class="screenText">Data</strong> tab:</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.5: The Data tab</p>
    <p class="normal">The screenshot above demonstrates the different information you can provide about your dataset; the more you do provide, the higher the <strong class="keyWord">usability index</strong>. This<a id="_idIndexMarker150"/> index is a synthetic measure summarizing how well your dataset is described. Datasets with higher usability indexes appear higher up in the search results. For each dataset, the usability index is based on several factors, including the level of documentation, the availability of related public content like Notebooks as references, file types, and coverage of key metadata.</p>
    <p class="normal">In principle, you do not have to fill out all the fields shown in the image above; your newly created dataset is perfectly usable without them (and if it is a private one, you probably do not care; after all, you know what is in it). However, community etiquette would suggest filling out the information for the datasets you make public: the more you specify, the more usable the data will be to others.</p>
    <h1 id="_idParaDest-40" class="heading-1">Gathering the data</h1>
    <p class="normal">Apart from legal aspects, there is no<a id="_idIndexMarker151"/> real limit on the kind of content you can store in the datasets: tabular data, images, text; if it fits within the size requirements, you can store it. This includes data harvested from other sources; tweets by hashtag or topic are among the popular datasets at the time of writing:</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_06.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 2.6: Tweets are among the most popular datasets</p>
    <p class="normal">Discussion of the different frameworks for harvesting data from social media (Twitter, Reddit, and so on) is outside the scope of this book.</p>
    <div class="interviewBox">
      <div class="intervieweePhoto">
        <img src="../Images/Andrew_Maranhao.png" alt=""/>
      </div>
      <p class="intervieweeName">Andrew Maranhão</p>
      <p class="normal"><a href="https://www.kaggle.com/andrewmvd"><span class="url">https://www.kaggle.com/andrewmvd</span></a></p>
      <p class="normal">We spoke to Andrew Maranhão (aka Larxel), Datasets Grandmaster (number 1 in Datasets at time of writing) and Senior<a id="_idIndexMarker152"/> Data Scientist at the Hospital Albert Einstein in São Paulo, about his rise to Datasets success, his tips for creating datasets, and his general experiences on Kaggle.</p>
      <p class="interviewHeader">What’s your favourite kind of competition and why? In terms of techniques and solving approaches, what is your specialty on Kaggle?</p>
      <p class="normal"><em class="italic">Medical imaging is usually my favourite. It speaks to my purpose and job. Among medical competitions, NLP is language-bound, tabular data varies widely among hospitals, but imaging is mostly the same, so any advancement in this context can bring about benefits for many countries across the world, and I love this impact potential. I also have a liking for NLP and tabular data, but I suppose this is pretty standard.</em></p>
      <p class="interviewHeader">Tell us about a particularly challenging competition you entered, and what insights you used to tackle the task.</p>
      <p class="normal"><em class="italic">In a tuberculosis detection in x-ray images competition, we had around 1,000 images, which is a pretty small number for capturing all the manifestations of the disease. I came up with two ideas to offset this:</em></p>
      <ol class="numberedList" style="list-style-type: decimal;">
        <li class="numberedList" value="1"><em class="italic"> Pre-train on external data of pneumonia detection (~20k images), as pneumonia can be mistaken for tuberculosis.</em></li>
        <li class="numberedList"><em class="italic"> Pre-train on multilabel classification of lung abnormalities (~600k images) and use grad-CAM with a simple SSD to generate bounding box annotations of classification labels.</em></li>
      </ol>
      <p class="normal"><em class="italic">In the end, a simple blend of these two achieved 22% more compared to the result that the second-place team had. It happened at a medical convention, with about 100 teams participating.</em></p>

      <p class="interviewHeader">You have become a Dataset Grandmaster and achieved the number 1 rank in Datasets. How do you choose topics and find, gather, and publish data for your datasets on Kaggle?</p>
      <p class="normal"><em class="italic">This is a big question; I’ll try to break it down piece by piece.</em></p>
      <ol class="numberedList" style="list-style-type: decimal;">
        <li class="numberedList" value="1"><strong class="bold-italic" style="font-style: italic;">Set yourself a purpose</strong></li>
      </ol>
      <p class="normal"><em class="italic">The first thing that I have in mind when choosing a topic is the reason I am doing this in the first place.</em></p>
      <p class="normal"><em class="italic">When there is a deeper reason underneath, great datasets just come off as a result, not as a goal in itself. Fei Fei Li, the head of the lab that created ImageNet, revealed in a TED talk that she </em><em class="italic"><a id="_idIndexMarker153"/></em><em class="italic">wanted to create a world where machines would be able to reason and appreciate the world with their vision in the same way her children did.</em></p>
      <p class="normal"><em class="italic">Having a purpose in mind will make it more likely that you’ll engage and improve over time, and will also differentiate you and your datasets. You can certainly live off tabular data on everyday topics, though I find that unlikely to leave a lasting impact.</em></p>
      <ol class="numberedList" style="list-style-type: decimal;">
        <li class="numberedList" value="2"><strong class="bold-italic" style="font-style: italic;">A great dataset is the embodiment of a great question</strong></li>
      </ol>
      <p class="normal"><em class="italic">If we look at the greatest datasets in current literature, such as ImageNet and others, we can see some common themes:</em></p>
      <ul>
        <li class="bulletList"><em class="italic">It is a daring, relevant question with great potential for all of us (scientific or real-world application)</em></li>
        <li class="bulletList"><em class="italic">The data was well collected, controlled for quality, and well documented</em></li>
        <li class="bulletList"><em class="italic">There is an adequate amount of data and diversity for our current hardware</em></li>
        <li class="bulletList"><em class="italic">It has an active community that continuously improves the data and/or builds upon that question</em></li>
      </ul>
      <p class="normal"><em class="italic">As I mentioned before, I feel that asking questions is a primary role of a data scientist and is likely to become even more prominent as automated machine and deep learning solutions advance. This is where datasets can certainly exercise something unique to your skillset.</em></p>
      <ol class="numberedList" style="list-style-type: decimal;">
        <li class="numberedList" value="1"><strong class="bold-italic" style="font-style: italic;">Create your process for success, rather than only pursuing success for the sake of success</strong></li>
      </ol>
      <p class="normal"><em class="italic">Quality far overshadows quantity; you only need 15 datasets to become a Grandmaster and the flagship datasets of AI are few and well made.</em></p>

      <p class="normal"><em class="italic">I have thrown away as many datasets as I have published. It takes time, and it is not a one and done type of thing as many people treat it – datasets have a maintenance and continuous improvement side to them.</em></p>
      <p class="normal"><em class="italic">One thing that is very often overlooked is supporting the community that gathers around your data. Notebooks and datasets are mutual efforts, so supporting those who take the time to analyze your data goes a long way for your dataset too. Analyzing their bottlenecks and choices can give directions as to what pre-processing steps could be done and provided, and also the clarity of your documentation.</em></p>
      <p class="normal"><em class="italic">All in all, the process that I recommend starts with setting your purpose, breaking it down into objectives and topics, formulating questions to fulfil these topics, surveying possible sources of data, selecting and gathering, pre-processing, documenting, publishing, maintaining and supporting, and finally, improvement actions.</em></p>
      <p class="normal"><em class="italic">For instance, let’s say that you would like to increase social welfare; you break it down into an objective, say, racial equity. From there, you analyze topics related to the objective and find the Black Lives Matter movement. From here, you formulate the question: how can I make sense of the millions of voices talking about it?</em></p>
      <p class="normal"><em class="italic">This narrows down your data type to NLP, which you can gather data for from news articles, YouTube comments, and tweets (which you choose, as it seems more representative of your question and feasible). You pre-process the data, removing identifiers, and document the collection process and dataset purpose.</em></p>
      <p class="normal"><em class="italic">With that done, you publish it, and a few Kagglers attempt topic modeling but struggle to do so because some tweets contain many foreign languages that create encoding problems. You </em><em class="italic"><a id="_idIndexMarker154"/></em><em class="italic">support them by giving them advice and highlighting their work, and decide to go back and narrow the tweets down to English, to fix this for good.</em></p>
      <p class="normal"><em class="italic">Their analysis reveals the demands, motivations, and fears relating to the movement. With their efforts, it was possible to break down millions of tweets into a set of recommendations that may improve racial equity in society. </em></p>

      <p class="numberedList">4.<strong class="bold-italic" style="font-style: italic;"> Doing a good job is all that is in your control</strong></p>
      <p class="normal"><em class="italic">Ultimately, it is other people that turn you into a Grandmaster, and votes don’t always translate into effort or impact. In one of my datasets, about Cyberpunk 2077, I worked on it for about 40 hours total and, to this day, it is still one of my least upvoted datasets.</em></p>
      <p class="normal"><em class="italic">But it doesn’t matter. I put in the effort, I tried, and I learned what I could — that’s what is in my control, and next week I’ll do it again no matter what. Do your best and keep going.</em></p>
      <p class="interviewHeader">Are there any particular tools or libraries that you would recommend using for data analysis/machine learning?</p>
      <p class="normal"><em class="italic">Strangely enough, I both </em><em class="italic"><a id="_idIndexMarker155"/></em><em class="italic">recommend and unrecommend libraries. LightGBM is a great tabular ML library with a fantastic ratio of performance to compute time, CatBoost can sometimes outperform it, but it comes at the cost of increased compute time, during which you could be having and testing new ideas. Optuna is great for hyperparameter tuning, Streamlit for frontends, Gradio for MVPs, Fast API for microservices, Plotly and Plotly Express for charts, PyTorch and its derivatives for deep learning.</em></p>
      <p class="normal"><em class="italic">While libraries are great, I also suggest that at some point in your career you take the time to implement it yourself. I first heard this advice from Andrew Ng and then from many others of equal calibre. Doing this creates very in-depth knowledge that sheds new light on what your model does and how it responds to tuning, data, noise, and more.</em></p>
      <p class="interviewHeader">In your experience, what do inexperienced Kagglers often overlook? What do you know now that you wish you’d known when you first started?</p>
      <p class="normal"><em class="italic">Over the years, the things I wished I realized sooner the most were:</em></p>
      <ol class="numberedList" style="list-style-type: decimal;">
        <li class="numberedList" value="1"><em class="italic"> Absorbing all the knowledge at the end of a competition</em></li>
        <li class="numberedList"><em class="italic"> Replication of winning solutions in finished competitions</em></li>
      </ol>
      <p class="normal"><em class="italic">In the pressure of a competition drawing to a close, you can see the leaderboard shaking more than ever before. This makes it less likely that you will take risks and take the time to see things in all their detail. When a competition is over, you don’t have that rush and can take as long as you need; you can also replicate the rationale of the winners who made their solutions known.</em></p>
      <p class="normal"><em class="italic">If you have the discipline, this will do wonders for your data science skills, so the bottom line is: stop when you are done, not when the competition ends. I have also heard this advice from an Andrew Ng keynote, where he recommended replicating papers as one of his best ways to develop yourself as an AI practitioner.</em></p>

      <p class="normal"><em class="italic">Also, at the end of a competition , you are likely to be exhausted and just want to call it a day. No problem there; just keep in mind that the discussion forum after the competition is done is one of the most knowledge-rich places on Planet Earth, primarily because </em><em class="italic"><a id="_idIndexMarker156"/></em><em class="italic">many rationales and code for winning solutions are made public there. Take the time to read and study what the winners did; don’t give into the desire to move on to something else, as you might miss a great learning opportunity.</em></p>
      <p class="interviewHeader">Has Kaggle helped you in your career? If so, how?</p>
      <p class="normal"><em class="italic">Kaggle helped my career by providing a wealth of knowledge, experience and also building my portfolio. My first job as a data scientist was largely due to Kaggle and DrivenData competitions. All throughout my career, I studied competition solutions and participated in a few more. Further engagement on Datasets and Notebooks also proved very fruitful in learning new techniques and asking better questions.</em></p>
      <p class="normal"><em class="italic">In my opinion, asking great questions is the primary challenge faced by a data scientist. Answering them is surely great as well, although I believe we are not far from a future where automated solutions will be more and more prevalent in modeling. There will always be room for modeling, but I suppose a lot of work will be streamlined in that regard. Asking great questions, however, is far harder to automate – if the question is not good, even the best solution could be meaningless.</em></p>
      <p class="interviewHeader">Have you ever used something you have done in Kaggle competitions in order to build your portfolio to show to potential employers?</p>
      <p class="normal"><em class="italic">Absolutely. I landed my first job as a data scientist in 2017 using Kaggle as proof of knowledge. To this day, it is still a fantastic CV component, as educational backgrounds and degrees are less representative of data science knowledge and experience than a portfolio is.</em></p>
      <p class="normal"><em class="italic">A portfolio with projects with competitions shows not just added experience but also a willingness to going above and beyond for development, which is arguably more important for long-term success.</em></p>
      <p class="interviewHeader">Do you use other competition platforms? How do they compare to Kaggle?</p>
      <p class="normal"><em class="italic">I also use DrivenData and AICrowd. The great thing about them is that they allow organizations that don’t have the same access to financial resources, such as start-ups and research institutions, to create competitions.</em></p>
      <p class="normal"><em class="italic">Great competitions come from a combination of great questions and great data, and this can happen regardless of company size. Kaggle has a bigger and more active community, and the</em><em class="italic"><a id="_idIndexMarker157"/></em><em class="italic"> hardware they provide, coupled with the data and Notebook capabilities, make it the best option; yet both DrivenData and AICrowd introduce just as interesting challenges and allow for more diversity.</em></p>

      <p class="interviewHeader">What’s the most important thing someone should keep in mind or do when they’re entering a competition?</p>
      <p class="normal"><em class="italic">Assuming your primary goal is development, my recommendation is that you pick a competition on a topic that interests you and a task that you haven’t done before. Critical sense and competence require depth and diversity. Focusing and giving your best will guarantee depth, and diversity is achieved by doing things you have not done before or have not done in the same way.</em></p>
    </div>
    <h1 id="_idParaDest-41" class="heading-1">Working with datasets</h1>
    <p class="normal">Once you have<a id="_idIndexMarker158"/> created a dataset, you probably want to use it in your analysis. In this section, we discuss different methods of going about this.</p>
    <p class="normal">Very likely, the most important one is starting a Notebook where you use your dataset as a primary source. You can do this by going to the dataset page and then clicking on <strong class="screenText">New Notebook</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.7: Creating a Notebook from the dataset page</p>
    <p class="normal">Once you have done this, you will be redirected to your <strong class="keyWord">Notebook</strong> page:</p>
    <figure class="mediaobject"><img src="../Images/B17574_02_08.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 2.8: Starting a Notebook using your dataset</p>
    <p class="normal">Here are a few <a id="_idIndexMarker159"/>pointers around this:</p>
    <ul>
      <li class="bulletList">The alphanumeric title is generated automatically; you can edit it by clicking on it.</li>
      <li class="bulletList">On the right-hand side under <strong class="screenText">Data</strong>, you see the list of data sources attached to your Notebook; the dataset I selected can be accessed under <code class="inlineCode">../input/</code> or from <code class="inlineCode">/kaggle/input/</code>.</li>
      <li class="bulletList">The opening block (with the imported packages, descriptive comments, and printing the list of available files) is added automatically to a new Python Notebook.</li>
    </ul>
    <p class="normal">With this basic setup, you can start to write a Notebook for your analysis and utilize your dataset as a data source. We will discuss Notebooks at greater length in <em class="chapterRef">Chapter 4</em>,<em class="italic"> Leveraging Discussion Forums</em>.</p>
    <h1 id="_idParaDest-42" class="heading-1">Using Kaggle Datasets in Google Colab</h1>
    <p class="normal">Kaggle Notebooks are free to <a id="_idIndexMarker160"/>use, but not without limits (more on that in <em class="chapterRef">Chapter 4</em>), and the first one you are likely to hit is the time limit. A <a id="_idIndexMarker161"/>popular alternative is to move to<a id="_idIndexMarker162"/> Google Colab, a free Jupyter Notebook environment that runs entirely in the cloud: <a href="https://colab.research.google.com"><span class="url">https://colab.research.google.com</span></a>.</p>
    <p class="normal">Even once we’ve moved the computations there, we might still want to have access to the Kaggle datasets, so importing them into Colab is a rather handy feature. The remainder of this section discusses the steps necessary to use Kaggle Datasets through Colab.</p>
    <p class="normal">The first thing we do, assuming we are already registered on Kaggle, is head to the account page to<a id="_idIndexMarker163"/> generate the <strong class="keyWord">API token</strong> (an access token containing<a id="_idIndexMarker164"/> security credentials for a login session, user identification, privileges, and so on):</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Go to your account, which can be found at <code class="inlineCode">https://www.kaggle.com/USERNAME/account</code>, and click on <strong class="screenText">Create New API Token</strong>:</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B17574_02_09.png" alt="Obraz zawierający tekst  Opis wygenerowany automatycznie"/></figure>
    <p class="packt_figref">Figure 2.9: Creating a new API token</p>
    <p class="normal">A file named kaggle.json containing your username and token will be created.</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="2">The next step is to create a folder named <code class="inlineCode">Kaggle</code> in your Google Drive and upload the <code class="inlineCode">.json</code> file there:</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B17574_02_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.10: Uploading the .json file into Google Drive</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="3">Once done, you need to<a id="_idIndexMarker165"/> create a new Colab notebook and <a id="_idIndexMarker166"/>mount your drive by running the following code in the notebook:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> google.colab <span class="hljs-keyword">import</span> drive
drive.mount(<span class="hljs-string">'/content/gdrive'</span>)
</code></pre>
      </li>
      <li class="numberedList">Get the authorization code from the URL prompt and provide it in the empty box that appears, and then execute the following code to provide the path to the <code class="inlineCode">.json</code> config:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os
<span class="hljs-comment"># content/gdrive/My Drive/Kaggle is the path where kaggle.json is </span>
<span class="hljs-comment"># present in the Google Drive</span>
os.environ[<span class="hljs-string">'KAGGLE_CONFIG_DIR'</span>] = <span class="hljs-string">"/content/gdrive/My Drive/Kaggle"</span>
<span class="hljs-comment"># change the working directory</span>
%cd /content/gdrive/My Drive/Kaggle
<span class="hljs-comment"># check the present working directory using the pwd command</span>
</code></pre>
      </li>
      <li class="numberedList">We can download the dataset now. Begin by going to the dataset’s page on Kaggle, clicking on the three dots next to <strong class="screenText">New Notebook</strong>, and selecting <strong class="screenText">Copy API command</strong>:</li>
    </ol>
    <figure class="mediaobject"><img src="../Images/B17574_02_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.11: Copying the API command</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="6">Run the API command to download the Dataset (readers interested in details of the commands<a id="_idIndexMarker167"/> used can consult the official documentation: <a href="https://www.kaggle.com/docs/api"><span class="url">https://www.kaggle.com/docs/api</span></a>) :
        <pre class="programlisting code"><code class="hljs-code">!kaggle datasets download -d ajaypalsinghlo/world-happiness-report-<span class="hljs-number">2021</span>
</code></pre>
      </li>
      <li class="numberedList">The dataset will be downloaded to the <code class="inlineCode">Kaggle</code> folder as a <code class="inlineCode">.zip</code> archive – unpack it and you are good to go.</li>
    </ol>
    <p class="normal">As you can see from the list above, using<a id="_idIndexMarker168"/> a Kaggle dataset in Colab is a straightforward process – all you need is an API token, and making the switch gives you the possibility of using more GPU hours than what is granted by Kaggle.</p>
    <h1 id="_idParaDest-43" class="heading-1">Legal caveats</h1>
    <p class="normal">Just because you can put some<a id="_idIndexMarker169"/> data on Kaggle does not necessarily mean that you should. An excellent example would be the <em class="italic">People of Tinder</em> dataset. In 2017, a developer used the Tinder API to scrape the website for semi-private profiles and uploaded the data on Kaggle. After the issue became known, Kaggle ended up taking the dataset down. You can read the full story here: <a href="https://www.forbes.com/sites/janetwburns/2017/05/02/tinder-profiles-have-been-looted-again-this-time-for-teaching-ai-to-genderize-faces/?sh=1afb86b25454"><span class="url">https://www.forbes.com/sites/janetwburns/2017/05/02/tinder-profiles-have-been-looted-again-this-time-for-teaching-ai-to-genderize-faces/?sh=1afb86b25454</span></a>.</p>
    <p class="normal">In general, before you upload <a id="_idIndexMarker170"/>anything to Kaggle, ask yourself two questions:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1"><strong class="keyWord">Is it allowed from a copyright standpoint?</strong> Remember to always check the licenses. When in doubt, you can always consult <a href="https://opendefinition.org/guide/data/"><span class="url">https://opendefinition.org/guide/data/</span></a> or contact Kaggle.</li>
      <li class="numberedList"><strong class="keyWord">Are there privacy risks associated with this dataset?</strong> Just because posting certain types of information is not, strictly speaking, illegal, doing so might be harmful to another person’s privacy. </li>
    </ol>
    <p class="normal">The limitations speak to common sense, so they are not too likely to hamper your efforts on Kaggle.</p>
    <h1 id="_idParaDest-44" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we introduced Kaggle Datasets, the standardized manner of storing and using data in the platform. We discussed dataset creation, ways of working outside of Kaggle, and the most important functionality: using a dataset in your Notebook. This provides a good segue to our next chapter, where we focus our attention on Kaggle Notebooks.</p>
    <h1 id="_idParaDest-45" class="heading-1">Join our book’s Discord space</h1>
    <p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask me Anything</em> session with the authors: </p>
    <p class="normal"><a href="https://packt.link/KaggleDiscord"><span class="url">https://packt.link/KaggleDiscord</span></a></p>
    <p class="normal"><img src="../Images/QR_Code40480600921811704671.png" alt=""/></p>
  </div>
</body></html>