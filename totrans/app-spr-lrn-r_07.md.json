["```py\n    library(mlbench)\n    library(randomForest)\n    library(dplyr)\n    ```", "```py\n    data(PimaIndiansDiabetes)\n    df<-PimaIndiansDiabetes\n    ```", "```py\n    str(df)\n    ```", "```py\n    'data.frame':768 obs. of  9 variables:\n     $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...\n     $ glucose : num  148 85 183 89 137 116 78 115 197 125...\n     $ pressure: num  72 66 64 66 40 74 50 0 70 96 ...\n     $ triceps : num  35 29 0 23 35 0 32 0 45 0 ...\n     $ insulin : num  0 0 0 94 168 0 88 0 543 0 ...\n     $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...\n     $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...\n     $ age     : num  50 31 32 21 33 30 26 29 53 54 ...\n     $ diabetes: Factor w/ 2 levels \"neg\",\"pos\": 2 1 2 1 2 1 2 1 2 2 ...\n    ```", "```py\n    library(caret)\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    train_index<- sample(seq_len(nrow(df)),floor(0.7 * nrow(df)))\n    train <- df[train_index,]\n    test <- df[-train_index,]\n    ```", "```py\n    print(\"Training Dataset shape:\")\n    print(dim(train))\n    print(\"Test Dataset shape:\")\n    print(dim(test))\n    ```", "```py\n    model <-randomForest(diabetes~.,data=train, mtry =3)\n    ```", "```py\n    print(model)\n    ```", "```py\n    y_predicted<- predict(model, newdata = test)\n    ```", "```py\n    results<-confusionMatrix(y_predicted, test$diabetes, positive= 'pos')\n    print(\"Confusion Matrix  (Test Data)- \")\n    print(results$table)\n    ```", "```py\n    results$overall[1]\n    ```", "```py\n    library(caret)\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    train_control = trainControl(method = \"cv\", number=5, savePredictions = TRUE,verboseIter = TRUE)\n    ```", "```py\n    parameter_values = expand.grid(mtry=3)\n    ```", "```py\n    model_rf_kfold<- train(diabetes~., data=df, trControl=train_control, \n                    method=\"rf\",  metric= \"Accuracy\", \n    tuneGrid = parameter_values)\n    ```", "```py\n    model_rf_kfold$results[2]\n    ```", "```py\n    print(\"Shape of Prediction Dataset\")\n    print(dim(model_rf_kfold$pred))\n    print(\"Prediction detailed results - \")\n    head(model_rf_kfold$pred) #print first 6 rows\n    tail(model_rf_kfold$pred) #print last 6 rows\n    print(\"Accuracy across each Fold-\")\n    model_rf_kfold$resample\n    print(paste(\"Average Accuracy :\",mean(model_rf_kfold$resample$Accuracy)))\n    print(paste(\"Std. Dev Accuracy :\",sd(model_rf_kfold$resample$Accuracy)))\n    ```", "```py\n    + Fold1: mtry=3 \n    - Fold1: mtry=3 \n    + Fold2: mtry=3 \n    - Fold2: mtry=3 \n    + Fold3: mtry=3 \n    - Fold3: mtry=3 \n    + Fold4: mtry=3 \n    - Fold4: mtry=3 \n    + Fold5: mtry=3 \n    - Fold5: mtry=3 \n    ...\n    Accuracy: 0.7590782\n    \"Shape of Prediction Dataset\"\n    768   5\n    \"Prediction detailed results - \"\n    ...\n    \"Average Accuracy : 0.759078176725236\"\n    \"Std. Dev Accuracy : 0.0225461480724459\"\n    ```", "```py\n    set.seed(2019)\n    train_control = trainControl(method = \"LOOCV\", savePredictions = TRUE)\n    ```", "```py\n    parameter_values = expand.grid(mtry=3)\n    ```", "```py\n    model_rf_LOOCV<- train(diabetes~., data=df, trControl=train_control, \n                        method=\"rf\",  metric= \"Accuracy\", \n    tuneGrid = parameter_values)\n    ```", "```py\n    print(model_rf_LOOCV$results[2])\n    ```", "```py\n    print(\"Shape of Prediction Dataset\")\n    print(dim(model_rf_LOOCV$pred))\n    print(\"Prediction detailed results - \")\n    head(model_rf_LOOCV$pred) #print first 6 rows\n    tail(model_rf_LOOCV$pred) #print last 6 rows\n    ```", "```py\n    Accuracy\n    1 0.7721354\n    [1] \"Shape of Prediction Dataset\"\n    [1] 768   4\n    [1] \"Prediction detailed results - \"\n    \"Shape of Prediction Dataset\"\n     768   4\n    \"Prediction detailed results - \"\n    ...\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    train_control = trainControl(method = \"cv\",  number=5, savePredictions = TRUE)\n    ```", "```py\n    parameter_grid = expand.grid(mtry=c(1,2,3,4,5,6))\n    ```", "```py\n    model_rf_gridSearch<- train(diabetes~., data=df, trControl=train_control, \n                   method=\"rf\",  metric= \"Accuracy\", \n    tuneGrid = parameter_grid)\n    ```", "```py\n    print(\"Accuracy across hyperparameter Combinations:\")\n    print(model_rf_gridSearch$results[,1:2])\n    ```", "```py\n    [1] \"Accuracy across hyperparameter Combinations:\"\n    mtry  Accuracy\n    1    1 0.7564893\n    2    2 0.7604108\n    3    3 0.7642730\n    4    4 0.7668704\n    5    5 0.7629658\n    6    6 0.7590697\n    ```", "```py\n    print(\"Shape of Prediction Dataset\")\n    print(dim(model_rf_gridSearch$pred))\n    [1] \"Shape of Prediction Dataset\"\n    [1] 4608    5\n    print(\"Prediction detailed results - \")\n    print(head(model_rf_gridSearch$pred)) #print the first 6 rows\n    print(tail(model_rf_gridSearch$pred)) #print the last 6 rows\n    [1] \"Prediction detailed results - \"\n    predobsrowIndexmtry Resample\n    1  neg pos       10    1    Fold1\n    2  neg pos       24    1    Fold1\n    3  neg neg       34    1    Fold1\n    4  neg pos       39    1    Fold1\n    5  neg neg       43    1    Fold1\n    6  neg neg       48    1    Fold1\n    predobsrowIndexmtry Resample\n    4603  neg neg      752    6    Fold5\n    4604  neg neg      753    6    Fold5\n    4605  pos pos      755    6    Fold5\n    4606  neg neg      759    6    Fold5\n    4607  neg neg      761    6    Fold5\n    4608  pos pos      762    6    Fold5\n    print(\"Best value for Hyperparameter 'mtry':\")\n    print(model_rf_gridSearch$bestTune)\n    [1] \"Best value for Hyperparameter 'mtry':\"\n    mtry\n    4    4\n    print(\"Final (Best) Model \")\n    print(model_rf_gridSearch$finalModel)\n    [1] \"Final (Best) Model \"\n    Call:\n    randomForest(x = x, y = y, mtry = param$mtry) \n                   Type of random forest: classification\n                         Number of trees: 500\n    No. of variables tried at each split: 4\n    OOB estimate of  error rate: 23.7%\n    Confusion matrix:\n        neg pos class.error\n    neg 423  77    0.154000\n    pos 105 163    0.391791\n    ```", "```py\n    library(repr)\n    options(repr.plot.width=8, repr.plot.height=5)\n    plot(model_rf_gridSearch)\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    library(dplyr)\n    ```", "```py\n    train_control = trainControl(method = \"cv\",  number=5, savePredictions = TRUE)\n    ```", "```py\n    parameter_grid = expand.grid(nrounds = c(30,50,60,100),\n                                 eta=c(0.01,0.1,0.2,0.3),\n    max_depth = c(2,3,4,5),\n                                 gamma = c(1),\n    colsample_bytree = c(0.7),\n    min_child_weight = c(1)  ,\n                                 subsample = c(0.6)\n                                )\n    ```", "```py\n    model_xgb_gridSearch<- train(diabetes~., data=df, trControl=train_control, \n                   method=\"xgbTree\",  metric= \"Accuracy\",\n    tuneGrid = parameter_grid)\n    ```", "```py\n    print(\"Shape of Prediction Dataset\")\n    print(dim(model_xgb_gridSearch$pred))\n    \"Shape of Prediction Dataset\"\n      49152    11\n    print(\"Prediction detailed results - \")\n    head(model_xgb_gridSearch$pred) #print the first 6 rows\n    tail(model_xgb_gridSearch$pred) #print the last 6 rows\n    [1] \"Prediction detailed results - \"\n    predobsrowIndex  eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds Resample\n    1  pos pos        3 0.01         2     1              0.7                1       0.6     100    Fold1\n    2  neg neg        6 0.01         2     1              0.7                1       0.6     100    Fold1\n    3  neg pos       20 0.01         2     1              0.7                1       0.6     100    Fold1\n    4  pos pos       23 0.01         2     1              0.7                1       0.6     100    Fold1\n    5  pos pos       25 0.01         2     1              0.7                1       0.6     100    Fold1\n    6  pos pos       27 0.01         2     1              0.7                1       0.6     100    Fold1\n    predobsrowIndex eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds Resample\n    49147  neg pos      732 0.3         5     1              0.7                1       0.6      60    Fold5\n    49148  neg pos      740 0.3         5     1              0.7                1       0.6      60    Fold5\n    49149  neg neg      743 0.3         5     1              0.7                1       0.6      60    Fold5\n    49150  pos pos      749 0.3         5     1              0.7                1       0.6      60    Fold5\n    49151  neg pos      751 0.3         5     1              0.7                1       0.6      60    Fold5\n    49152  neg neg      763 0.3         5     1              0.7                1       0.6      60    Fold5\n    print(\"Best values for all selected Hyperparameters:\")\n    model_xgb_gridSearch$bestTune\n    [1] \"Best values for all selected Hyperparameters:\"\n    nroundsmax_depth eta gamma colsample_bytreemin_child_weight subsample\n    27      60         4 0.1     1              0.7                1       0.6\n    ```", "```py\n    print(\"Average results across different combination of Hyperparameter Values\")\n    model_xgb_gridSearch$results %>% arrange(desc(Accuracy)) %>% head(5)\n    ```", "```py\n    [1] \"Average results across different combination of Hyperparameter Values\"\n       eta max_depth gamma colsample_bytreemin_child_weight subsample nrounds  Accuracy     Kappa AccuracySD\n    1 0.10         4     1              0.7                1       0.6      60 0.7695612 0.4790457 0.02507631\n    2 0.01         3     1              0.7                1       0.6      30 0.7695442 0.4509049 0.02166056\n    3 0.01         2     1              0.7                1       0.6     100 0.7695187 0.4521142 0.03373126\n    4 0.30         2     1              0.7                1       0.6      30 0.7682540 0.4782334 0.01943638\n    5 0.01         5     1              0.7                1       0.6      30 0.7682455 0.4592689 0.02836553\n    KappaSD\n    1 0.05067601\n    2 0.05587205\n    3 0.08038248\n    4 0.04249313\n    5 0.06049950\n    ```", "```py\n    plot(model_xgb_gridSearch)\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    train_control = trainControl(method = \"cv\",  number=5, savePredictions = TRUE)\n    ```", "```py\n    model_rf_randomSearch<- train(diabetes~., data=df, trControl=train_control, \n                            method=\"rf\",  metric= \"Accuracy\",tuneLength = 15)\n    ```", "```py\n    print(\"Shape of Prediction Dataset\")\n    print(dim(model_rf_randomSearch$pred))\n    [1] \"Shape of Prediction Dataset\"\n    [1] 5376    5\n    print(\"Prediction detailed results - \")\n    head(model_rf_randomSearch$pred) #print the first 6 rows\n    tail(model_rf_randomSearch$pred) #print the last 6 rows\n    [1] \"Prediction detailed results - \"\n    predobsrowIndexmtry Resample\n    1  pos pos        1    2    Fold1\n    2  neg neg        4    2    Fold1\n    3  pos pos        9    2    Fold1\n    4  neg pos       10    2    Fold1\n    5  neg neg       13    2    Fold1\n    6  pos pos       17    2    Fold1\n    predobsrowIndexmtry Resample\n    5371  neg neg      737    8    Fold5\n    5372  neg neg      742    8    Fold5\n    5373  neg neg      743    8    Fold5\n    5374  neg pos      758    8    Fold5\n    5375  neg neg      759    8    Fold5\n    5376  neg neg      765    8    Fold5\n    print(\"Best values for all selected Hyperparameters:\")\n    model_rf_randomSearch$bestTune\n    [1] \"Best values for all selected Hyperparameters:\"\n    mtry\n    7    8\n    ```", "```py\n    model_rf_randomSearch$results %>% arrange(desc(Accuracy)) %>% head(5)\n    ```", "```py\n    mtry  Accuracy     Kappa AccuracySDKappaSD\n    1    8 0.7838299 0.5190606 0.02262610 0.03707616\n    2    7 0.7773194 0.5047353 0.02263485 0.03760842\n    3    3 0.7760037 0.4945296 0.02629540 0.05803215\n    4    6 0.7734063 0.4964970 0.02451711 0.04409090\n    5    5 0.7720907 0.4895592 0.02618707 0.04796626\n    ```", "```py\n    plot(model_rf_randomSearch)\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    train_control = trainControl(method = \"cv\",  number=5, savePredictions = TRUE)\n    ```", "```py\n    model_xgb_randomSearch<- train(diabetes~., data=df, trControl=train_control, \n                                   method=\"xgbTree\", metric= \"Accuracy\",tuneLength = 15)\n    ```", "```py\n    print(\"Shape of Prediction Dataset\")\n    print(dim(model_xgb_randomSearch$pred))\n    print(\"Prediction detailed results - \")\n    head(model_xgb_randomSearch$pred) #print the first 6 rows\n    tail(model_xgb_randomSearch$pred) #print the last 6 rows\n    print(\"Best values for all selected Hyperparameters:\")\n    model_xgb_randomSearch$bestTune\n    ```", "```py\n    model_xgb_randomSearch$results %>% arrange(desc(Accuracy)) %>% head(5)\n    ```", "```py\n    \"Shape of Prediction Dataset\"\n    10368000       11\n    \"Prediction detailed results - \"\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    library(MlBayesOpt)\n    ```", "```py\n    model_rf_bayesain<- rf_opt(train_data = train,\n    train_label = diabetes,\n    test_data = test,\n    test_label = diabetes,\n    mtry_range = c(1L, ncol(df)-1),\n    num_tree = 50,\n    init_points = 10,\n    n_iter = 10,                       \n    acq = \"poi\", eps = 0, \n    optkernel = list(type = \"exponential\", power =2))\n    ```", "```py\n    set.seed(2019)\n    ```", "```py\n    model_xgb_bayesian<- xgb_opt(train, diabetes, test, diabetes,objectfun ='binary:logistic',evalmetric='logloss',eta_range = c(0.1, 1L), max_depth_range = c(2L, 8L),nrounds_range = c(70, 160L), bytree_range = c(0.4, 1L), init_points = 4, n_iter = 10, acq = \"poi\", eps = 0, optkernel = list(type = \"exponential\", power =2))\n    ```"]