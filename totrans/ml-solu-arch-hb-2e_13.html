<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer194">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">13</span></h1>
<h1 class="chapterTitle" id="_idParaDest-351"><span class="koboSpan" id="kobo.2.1">Bias, Explainability, Privacy, and Adversarial Attacks</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">In the previous chapter, we explored the topic of AI risk management framework and discussed its importance in mitigating the risks associated with AI systems. </span><span class="koboSpan" id="kobo.3.2">We covered the core concepts of what it is, the importance of identifying and assessing risks, and recommendations for managing those risks. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we will take a more in-depth look at several specific risk topics and technical techniques for mitigations. </span><span class="koboSpan" id="kobo.3.4">We will explore the essential areas of </span><strong class="keyWord"><span class="koboSpan" id="kobo.4.1">bias</span></strong><span class="koboSpan" id="kobo.5.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.6.1">explainability</span></strong><span class="koboSpan" id="kobo.7.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.8.1">privacy</span></strong><span class="koboSpan" id="kobo.9.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.10.1">adversarial attacks</span></strong><span class="koboSpan" id="kobo.11.1">, and how they relate to AI systems. </span><span class="koboSpan" id="kobo.11.2">These are some of the most pertinent areas in responsible AI practices, and it is important for ML practitioners to develop a foundational understanding of these topics and the technical solutions. </span><span class="koboSpan" id="kobo.11.3">Specifically, we will examine how bias can lead to unfair and discriminatory outcomes, and how explainability can enhance the transparency and accountability of AI systems. </span><span class="koboSpan" id="kobo.11.4">We will also discuss the criticality of privacy in AI systems, as well as the potential risks of adversarial attacks and how to mitigate them.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.12.1">To sum up, the following topics will be covered in this chapter:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.13.1">What is bias?</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.14.1">What is explainability?</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.15.1">Understanding security and privacy-preserving ML</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.16.1">Understanding adversarial attacks and how to defend against them</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.17.1">Hands-on lab – detecting bias, explaining models, training privacy-preserving mode, and simulating adversarial attack</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-352"><span class="koboSpan" id="kobo.18.1">Understanding bias</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.19.1">Detecting and mitigating bias is a crucial focus area for AI risk management. </span><span class="koboSpan" id="kobo.19.2">The presence of bias in </span><a id="_idIndexMarker1295"/><span class="koboSpan" id="kobo.20.1">ML models can expose an organization to potential legal risks but also lead to negative publicity, causing reputational damage and public relations issues. </span><span class="koboSpan" id="kobo.20.2">Specific laws and regulations, such as the </span><em class="italic"><span class="koboSpan" id="kobo.21.1">Equal Credit Opportunity Act</span></em><span class="koboSpan" id="kobo.22.1">, also prohibit discrimination in business transactions, like credit transactions, based on race, skin color, religion, sex, nationality origin, marital status, and age. </span><span class="koboSpan" id="kobo.22.2">Some other examples of laws against discrimination include the </span><em class="italic"><span class="koboSpan" id="kobo.23.1">Civil Rights Act of 1964</span></em><span class="koboSpan" id="kobo.24.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.25.1">Age Discrimination in Employment Act of 1967</span></em><span class="koboSpan" id="kobo.26.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.27.1">ML bias can result from the underlying prejudice in data. </span><span class="koboSpan" id="kobo.27.2">Since ML models are trained using data, if the data has a bias, then the trained model will also exhibit bias behaviors. </span><span class="koboSpan" id="kobo.27.3">For example, if you build an ML model to predict the loan default rate as part of the loan application review process, and you use race as one of the features in the training data, then the ML algorithm can potentially pick up race-related patterns and favor certain ethnic groups over others. </span><span class="koboSpan" id="kobo.27.4">Bias can be introduced in different stages of the ML lifecycle. </span><span class="koboSpan" id="kobo.27.5">For example, there could be data selection bias as certain groups might have stronger representation in the data collection stage. </span><span class="koboSpan" id="kobo.27.6">There could be labeling bias where a human makes an intentional or unintentional mistake in assigning labels to a dataset. </span><span class="koboSpan" id="kobo.27.7">Data sources with disinformation can also be a source of bias that results in biased AI solutions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.28.1">The ability to explain the decisions made by models helps an organization to satisfy compliance and audit requirements from the governance bodies. </span><span class="koboSpan" id="kobo.28.2">Furthermore, model explainability helps an organization understand the cause-and-effect relationships between the inputs and the ML prediction to make better business decisions. </span><span class="koboSpan" id="kobo.28.3">For example, if you can understand the reasons (such as rewards programs) behind strong customer interest in a financial product, you can adjust your business strategy, such as doubling down on rewards programs, to increase revenues. </span><span class="koboSpan" id="kobo.28.4">Being able to explain model decisions also helps establish trust with domain experts in the ML models. </span><span class="koboSpan" id="kobo.28.5">If domain experts agree with how the predictions are made by the models, they would be more likely to adopt the models for decision makings. </span><span class="koboSpan" id="kobo.28.6">There are various techniques for bias detection and model explainability, and we will take a closer look at some of the techniques next.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.29.1">To detect and mitigate bias, some guiding principles need to be established on what is considered as fair. </span><span class="koboSpan" id="kobo.29.2">For example, a bank’s loan approval process should treat similar people similarly and the process may be considered fair when applicants with similar qualifications are assessed similarly. </span><span class="koboSpan" id="kobo.29.3">The bank also needs to ensure different demographic subgroups are treated equally for loan approval and measure metrics such as the rate for loan rejection to be approximately similar across different demographic subgroups.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.30.1">Depending on the definition of fairness, bias can be measured using different metrics. </span><span class="koboSpan" id="kobo.30.2">Some of the metrics </span><a id="_idIndexMarker1296"/><span class="koboSpan" id="kobo.31.1">might even contradict each other. </span><span class="koboSpan" id="kobo.31.2">Therefore, you need to choose the metrics that best support the definition of fairness with social and legal considerations and inputs from different demographic groups. </span><span class="koboSpan" id="kobo.31.3">In this section, we list some of the bias </span><a id="_idIndexMarker1297"/><span class="koboSpan" id="kobo.32.1">metrics for consideration:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.33.1">Class imbalance</span></strong><span class="koboSpan" id="kobo.34.1">: This metric measures the imbalanced representations of different demographic groups, especially disadvantaged groups, in a dataset.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.35.1">Difference in positive proportion in observed labels</span></strong><span class="koboSpan" id="kobo.36.1">: This metric measures the differences in positive labels across different demographic groups.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.37.1">Kullback–Leibler (KL) divergence</span></strong><span class="koboSpan" id="kobo.38.1">: This metric compares the probability distribution in features and labels for the different groups, such as advantaged and disadvantaged groups.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.39.1">Conditional demographic disparity in labels</span></strong><span class="koboSpan" id="kobo.40.1">: This metric measures whether a group has a bigger proportion of rejected outcomes than the proportion of accepted outcomes in the same group.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.41.1">Recall difference</span></strong><span class="koboSpan" id="kobo.42.1">: This metric measures whether an ML model is finding more true positives for one group (advantaged group) than other groups (disadvantaged groups).</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.43.1">There are several techniques that can potentially mitigate bias after it has been detected, although these techniques have inherent challenges and limitations. </span><span class="koboSpan" id="kobo.43.2">The following are some </span><a id="_idIndexMarker1298"/><span class="koboSpan" id="kobo.44.1">examples of approaches that may be employed:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.45.1">Removal of features</span></strong><span class="koboSpan" id="kobo.46.1">: This approach can help mitigate bias by removing features that can contribute to the bias such as gender and age. </span><span class="koboSpan" id="kobo.46.2">However, there are also limitations and challenges with this approach, including proxy problems, meaning that removing sensitive features like gender or age may not entirely remove bias if other features in the data are correlated with the sensitive attributes. </span><span class="koboSpan" id="kobo.46.3">Furthermore, some relevant information may be lost by removing features, which could negatively impact the model’s performance or usefulness.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.47.1">Rebalance of training data</span></strong><span class="koboSpan" id="kobo.48.1">: This approach helps correct bias in different numbers of representations for the different groups in the training data. </span><span class="koboSpan" id="kobo.48.2">However, rebalancing the training data may not be feasible or effective if the initial dataset is highly imbalanced or if the underrepresented groups have intrinsically different distributions. </span><span class="koboSpan" id="kobo.48.3">In addition, artificially rebalancing the data may </span><a id="_idIndexMarker1299"/><span class="koboSpan" id="kobo.49.1">introduce other biases or distortions, and it may not address underlying societal biases that are reflected in the data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.50.1">Adjust labels in the training data</span></strong><span class="koboSpan" id="kobo.51.1">: This approach brings the proportions of labels close together for the different subgroups. </span><span class="koboSpan" id="kobo.51.2">However, this approach assumes that the labels themselves are not biased, which may not always be the case, especially if the labels were assigned by humans who may have their own biases. </span><span class="koboSpan" id="kobo.51.3">Also, adjusting labels may be difficult or impossible in some domains, especially if the ground truth is unknown or if the labels are not subjective.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.52.1">There are other general challenges with bias mitigation, including the lack of ground truth; as in many real-world scenarios, it is difficult to determine the true unbiased ground truth, making it challenging to accurately measure and mitigate bias. </span><span class="koboSpan" id="kobo.52.2">Additionally, these approaches often focus on mitigating bias with respect to a single sensitive attribute, such as gender or race, but may not address intersectional biases that arise from </span><a id="_idIndexMarker1300"/><span class="koboSpan" id="kobo.53.1">the combination of multiple sensitive attributes. </span><span class="koboSpan" id="kobo.53.2">Furthermore, in some cases, mitigating bias may come at the cost of reduced model performance or accuracy, necessitating a balance between these competing objectives of fairness and performance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.54.1">It’s important to note that bias mitigation is an active area of research, and more advanced techniques are being developed to address some of the limitations and challenges. </span><span class="koboSpan" id="kobo.54.2">These include adversarial debiasing, a technique that uses an adversary model to predict sensitive attributes (e.g., gender, race) from the primary model’s internal representations or outputs. </span><span class="koboSpan" id="kobo.54.3">Another technique is causal modeling, which aims to ensure that an individual’s prediction or outcome should not change significantly if their sensitive attribute(s) were different, all else being equal. </span><span class="koboSpan" id="kobo.54.4">Additionally, a combination of approaches and careful monitoring and evaluation may be necessary to effectively mitigate bias in real-world applications.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.55.1">There are a number of open-source libraries for fairness and bias management, such as:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.56.1">Fairness (</span><a href="https://github.com/algofairness/fairness-comparison"><span class="url"><span class="koboSpan" id="kobo.57.1">https://github.com/algofairness/fairness-comparison</span></span></a><span class="koboSpan" id="kobo.58.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.59.1">Aequitas (</span><a href="https://github.com/dssg/aequitas"><span class="url"><span class="koboSpan" id="kobo.60.1">https://github.com/dssg/aequitas</span></span></a><span class="koboSpan" id="kobo.61.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.62.1">Themis (</span><a href="https://github.com/LASER-UMASS/Themis"><span class="url"><span class="koboSpan" id="kobo.63.1">https://github.com/LASER-UMASS/Themis</span></span></a><span class="koboSpan" id="kobo.64.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.65.1">Responsibly (</span><a href="https://github.com/ResponsiblyAI/responsibly"><span class="url"><span class="koboSpan" id="kobo.66.1">https://github.com/ResponsiblyAI/responsibly</span></span></a><span class="koboSpan" id="kobo.67.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.68.1">IBM AI Fairness 360 (</span><a href="https://aif360.res.ibm.com/"><span class="url"><span class="koboSpan" id="kobo.69.1">https://aif360.res.ibm.com/</span></span></a><span class="koboSpan" id="kobo.70.1">)</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.71.1">There is also a component in SageMaker for bias detection, which we will cover in greater detail in a later section.</span></p>
<h1 class="heading-1" id="_idParaDest-353"><span class="koboSpan" id="kobo.72.1">Understanding ML explainability</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.73.1">There are two main </span><a id="_idIndexMarker1301"/><span class="koboSpan" id="kobo.74.1">concepts when it comes to explaining the behaviors of an ML model:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.75.1">Global explainability</span></strong><span class="koboSpan" id="kobo.76.1">: This is the overall behavior of a model across all data points used </span><a id="_idIndexMarker1302"/><span class="koboSpan" id="kobo.77.1">for model training and/or prediction. </span><span class="koboSpan" id="kobo.77.2">This helps to understand collectively how different input features affect the outcome of model predictions. </span><span class="koboSpan" id="kobo.77.3">For example, after training an ML model for credit scoring, it is determined that income is the most important feature in predicting high credit scores across data points for all loan applicants.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.78.1">Local explainability</span></strong><span class="koboSpan" id="kobo.79.1">: This is the behavior of a model for a single data point (instance), and </span><a id="_idIndexMarker1303"/><span class="koboSpan" id="kobo.80.1">which features had the most influence on the prediction for a single data point. </span><span class="koboSpan" id="kobo.80.2">For example, when you try to explain which features influenced the decision the most for a single loan applicant, it might turn out that education was the most important feature, even though income was the most important feature at the global level.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.81.1">Some ML algorithms such as linear regression and decision trees are considered explainable algorithms with a built-in ability to explain the model. </span><span class="koboSpan" id="kobo.81.2">For example, the coefficients of linear regression models directly represent the relative importance of different input features, and the split points in a decision tree represent the rules used for decision making.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.82.1">For black-box models such as neural networks, it is very hard to explain how the decisions are made in part due to non-linearity and model complexity. </span><span class="koboSpan" id="kobo.82.2">One technique for solving this is to use a white-box surrogate model to help explain the decisions of a black-box model. </span><span class="koboSpan" id="kobo.82.3">For example, you can train a linear regression model in parallel with a black-box neural network model using the same input data. </span><span class="koboSpan" id="kobo.82.4">While the linear regression model might not have the same performance as the black-box model, it can be used to explain at a high level how the decision was made. </span><span class="koboSpan" id="kobo.82.5">However, there are known limitations to the white-box surrogate model. </span><span class="koboSpan" id="kobo.82.6">Linear regression models, as mentioned in the example, may not be able to capture the complex non-linear relationships learned by neural networks, leading to an inaccurate representation of the decision-making process. </span><span class="koboSpan" id="kobo.82.7">Furthermore, while simple surrogate models, like linear regression, may provide a global </span><a id="_idIndexMarker1304"/><span class="koboSpan" id="kobo.83.1">approximation of the black-box model’s behavior, they may fail to capture local patterns or decision boundaries.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.84.1">There are various open-source packages, such as </span><strong class="keyWord"><span class="koboSpan" id="kobo.85.1">LIME</span></strong><span class="koboSpan" id="kobo.86.1"> (which stands for </span><strong class="keyWord"><span class="koboSpan" id="kobo.87.1">local interpretable model-agnostic explanations</span></strong><span class="koboSpan" id="kobo.88.1">), and </span><strong class="keyWord"><span class="koboSpan" id="kobo.89.1">SHAP</span></strong><span class="koboSpan" id="kobo.90.1"> (which stands for </span><strong class="keyWord"><span class="koboSpan" id="kobo.91.1">SHapley Additive exPlanations</span></strong><span class="koboSpan" id="kobo.92.1">), for model explainability. </span><span class="koboSpan" id="kobo.92.2">Both LIME and SHAP adopt the surrogate model approach.</span></p>
<h2 class="heading-2" id="_idParaDest-354"><span class="koboSpan" id="kobo.93.1">LIME</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.94.1">LIME </span><a id="_idIndexMarker1305"/><span class="koboSpan" id="kobo.95.1">supports local (instance) explainability, as the name suggests. </span><span class="koboSpan" id="kobo.95.2">The main idea behind LIME is to perturb the </span><a id="_idIndexMarker1306"/><span class="koboSpan" id="kobo.96.1">original data points (tweak the data points), feed them into the black-box model, and see the corresponding outputs. </span><span class="koboSpan" id="kobo.96.2">The perturbed data points are small changes to the original data point and are weighted based on their proximities to the original data. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.97.1">It then fits a surrogate model, such as linear regression, using the perturbed data points and responses. </span><span class="koboSpan" id="kobo.97.2">Finally, the trained linear model is used to explain how the decision was made for the original data point.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.98.1">LIME can be </span><a id="_idIndexMarker1307"/><span class="koboSpan" id="kobo.99.1">installed as a regular Python package and can be used to explain text classifiers, image </span><a id="_idIndexMarker1308"/><span class="koboSpan" id="kobo.100.1">classifiers, tabular classifiers, and regression models. </span><span class="koboSpan" id="kobo.100.2">The following are the explainers </span><a id="_idIndexMarker1309"/><span class="koboSpan" id="kobo.101.1">available in LIME:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.102.1">Tabular data explainer</span></strong><span class="koboSpan" id="kobo.103.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.104.1">lime_tabular.LimeTabularExplainer()</span></code></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.105.1">Image data explainer</span></strong><span class="koboSpan" id="kobo.106.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.107.1">lime_image.LimeImageExplainer()</span></code></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.108.1">Text data explainer</span></strong><span class="koboSpan" id="kobo.109.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.110.1">lime_text.LimeTextExplainer()</span></code></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.111.1">LIME has certain limitations. </span><span class="koboSpan" id="kobo.111.2">Its explanations rely on perturbed samples generated around the instance of interest, and the quality of these explanations can be influenced by the sampling process. </span><span class="koboSpan" id="kobo.111.3">Different sampling techniques or perturbation functions may yield different explanations. </span><span class="koboSpan" id="kobo.111.4">While LIME can highlight the importance of individual features for a specific prediction, it may not offer a clear interpretation of how these features are combined or interact within the black-box model. </span><span class="koboSpan" id="kobo.111.5">The computational cost of generating LIME explanations can be high, particularly for high-dimensional data or complex models, as it necessitates creating and evaluating numerous perturbed samples for each instance of interest. </span><span class="koboSpan" id="kobo.111.6">LIME generates local explanations by approximating the behavior of </span><a id="_idIndexMarker1310"/><span class="koboSpan" id="kobo.112.1">the black-box model around the instance of interest using an interpretable model (e.g., linear regression). </span><span class="koboSpan" id="kobo.112.2">However, this local approximation may not accurately reflect the true behavior </span><a id="_idIndexMarker1311"/><span class="koboSpan" id="kobo.113.1">of the complex model, especially in regions with high non-linearities or discontinuities. </span><span class="koboSpan" id="kobo.113.2">Additionally, the linear surrogate might be inaccurate for local data points that defy approximation by a linear model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.114.1">Despite these limitations, LIME remains a popular and useful technique for generating local explanations, especially when combined with other interpretability methods or when used in conjunction with domain expertise.</span></p>
<h2 class="heading-2" id="_idParaDest-355"><span class="koboSpan" id="kobo.115.1">SHAP</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.116.1">SHAP is a </span><a id="_idIndexMarker1312"/><span class="koboSpan" id="kobo.117.1">more popular package, and it addresses some of the shortcomings of LIME. </span><span class="koboSpan" id="kobo.117.2">It computes the contribution </span><a id="_idIndexMarker1313"/><span class="koboSpan" id="kobo.118.1">of each feature to the prediction using the coalition game theory concept, where each feature value of each data instance is a player in the coalition.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.119.1">The basic idea behind the coalition game theory is to form different permutations of coalitions of players when playing a game, then observe the game results from the different permutations, and finally calculate the contribution of each player. </span><span class="koboSpan" id="kobo.119.2">For example, if there are 3 features (</span><em class="italic"><span class="koboSpan" id="kobo.120.1">A, B,</span></em><span class="koboSpan" id="kobo.121.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.122.1">C</span></em><span class="koboSpan" id="kobo.123.1">) in the training dataset, then there will be 8 distinct coalitions (</span><em class="italic"><span class="koboSpan" id="kobo.124.1">2</span></em><sup class="italic"><span class="koboSpan" id="kobo.125.1">^</span></sup><em class="italic"><span class="koboSpan" id="kobo.126.1">3</span></em><span class="koboSpan" id="kobo.127.1">). </span><span class="koboSpan" id="kobo.127.2">We train one model for each distinct coalition for a total of 8 models. </span><span class="koboSpan" id="kobo.127.3">We use all 8 models to generate predictions on the dataset, figure out the marginal contribution of each feature, and assign a Shapley value to each feature to indicate the feature importance. </span><span class="koboSpan" id="kobo.127.4">For example, if the model that uses a coalition with only features </span><em class="italic"><span class="koboSpan" id="kobo.128.1">A</span></em><span class="koboSpan" id="kobo.129.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.130.1">B</span></em><span class="koboSpan" id="kobo.131.1"> generates an output of </span><em class="italic"><span class="koboSpan" id="kobo.132.1">50</span></em><span class="koboSpan" id="kobo.133.1">, and the model that uses features </span><em class="italic"><span class="koboSpan" id="kobo.134.1">A, B,</span></em><span class="koboSpan" id="kobo.135.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.136.1">C</span></em><span class="koboSpan" id="kobo.137.1"> generates an output of </span><em class="italic"><span class="koboSpan" id="kobo.138.1">60</span></em><span class="koboSpan" id="kobo.139.1">, then feature </span><em class="italic"><span class="koboSpan" id="kobo.140.1">C</span></em><span class="koboSpan" id="kobo.141.1"> has a marginal contribution of </span><em class="italic"><span class="koboSpan" id="kobo.142.1">10</span></em><span class="koboSpan" id="kobo.143.1">. </span><span class="koboSpan" id="kobo.143.2">This is just a generalization of the concept; the actual calculation and assignments are more involved.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.144.1">SHAP can also be installed like a regular Python package. </span><span class="koboSpan" id="kobo.144.2">It can be used to explain tree ensemble </span><a id="_idIndexMarker1314"/><span class="koboSpan" id="kobo.145.1">models, natural </span><a id="_idIndexMarker1315"/><span class="koboSpan" id="kobo.146.1">language models (such as transformers), and deep learning models. </span><span class="koboSpan" id="kobo.146.2">It has the following main explainers:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.147.1">TreeExplainer</span></strong><span class="koboSpan" id="kobo.148.1">: An </span><a id="_idIndexMarker1316"/><span class="koboSpan" id="kobo.149.1">implementation for computing SHAP values for trees and ensemble of trees algorithms</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.150.1">DeepExplainer</span></strong><span class="koboSpan" id="kobo.151.1">: An </span><a id="_idIndexMarker1317"/><span class="koboSpan" id="kobo.152.1">implementation for computing SHAP values for deep learning models</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.153.1">GradientExplainer</span></strong><span class="koboSpan" id="kobo.154.1">: An </span><a id="_idIndexMarker1318"/><span class="koboSpan" id="kobo.155.1">implementation of expected gradients to approximate SHAP values for deep learning models</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.156.1">LinearExplainer</span></strong><span class="koboSpan" id="kobo.157.1">: For </span><a id="_idIndexMarker1319"/><span class="koboSpan" id="kobo.158.1">an explanation of linear models with independent features</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.159.1">KernelExplainer</span></strong><span class="koboSpan" id="kobo.160.1">: A </span><a id="_idIndexMarker1320"/><span class="koboSpan" id="kobo.161.1">model-agnostic method to estimate SHAP values for any model because it makes no assumptions about the model type</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.162.1">SHAP is widely considered the state-of-the-art model explainability algorithm, and it has been implemented in commercial offerings such as SageMaker. </span><span class="koboSpan" id="kobo.162.2">It can be used for both computing global feature importance as well as local explainability for a single instance. </span><span class="koboSpan" id="kobo.162.3">However, SHAP </span><a id="_idIndexMarker1321"/><span class="koboSpan" id="kobo.163.1">does come with certain limitations. </span><span class="koboSpan" id="kobo.163.2">Computing SHAP values, particularly for intricate models and high-dimensional data, can be computationally expensive and time consuming. </span><span class="koboSpan" id="kobo.163.3">This can pose challenges </span><a id="_idIndexMarker1322"/><span class="koboSpan" id="kobo.164.1">when applying SHAP to real-time or large-scale applications. </span><span class="koboSpan" id="kobo.164.2">SHAP values are computed based on the assumption that features are independent of each other. </span><span class="koboSpan" id="kobo.164.3">Nevertheless, in many real-world datasets, features may exhibit high correlations or complex interactions, violating this assumption and resulting in inaccurate or misleading explanations. </span><span class="koboSpan" id="kobo.164.4">Despite offering a numerical measure of feature importance, interpreting and conveying the significance of these values to non-technical stakeholders can be challenging, especially in complex domains.</span></p>
<h1 class="heading-1" id="_idParaDest-356"><span class="koboSpan" id="kobo.165.1">Understanding security and privacy-preserving ML</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.166.1">ML models often rely on vast amounts of data, including potentially sensitive information </span><a id="_idIndexMarker1323"/><span class="koboSpan" id="kobo.167.1">about individuals, such as personal details, financial records, medical histories, or browsing behavior. </span><span class="koboSpan" id="kobo.167.2">The improper handling or exposure of this data can lead to serious privacy breaches, putting individuals at risk of discrimination, identity theft, or other harmful consequences. </span><span class="koboSpan" id="kobo.167.3">To ensure compliance with data privacy regulations or even internal data privacy controls, ML systems need to provide foundational infrastructure security features such as data encryption, network isolation, compute isolation, and private connectivity. </span><span class="koboSpan" id="kobo.167.4">With a SageMaker-based ML platform, you can enable the following key security controls:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.168.1">Private networking</span></strong><span class="koboSpan" id="kobo.169.1">: As SageMaker is a fully managed service, it runs in an AWS-owned </span><a id="_idIndexMarker1324"/><span class="koboSpan" id="kobo.170.1">account. </span><span class="koboSpan" id="kobo.170.2">By default, resources in your own AWS account communicate with SageMaker APIs via the public internet. </span><span class="koboSpan" id="kobo.170.3">To enable private connectivity to SageMaker components from your own AWS environment, you can attach them </span><a id="_idIndexMarker1325"/><span class="koboSpan" id="kobo.171.1">to a subnet in your own </span><strong class="keyWord"><span class="koboSpan" id="kobo.172.1">virtual private cloud</span></strong><span class="koboSpan" id="kobo.173.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.174.1">VPC</span></strong><span class="koboSpan" id="kobo.175.1">).</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.176.1">Storage encryption</span></strong><span class="koboSpan" id="kobo.177.1">: Data-at-rest encryption can be enabled by providing an encryption </span><a id="_idIndexMarker1326"/><span class="koboSpan" id="kobo.178.1">key when you create a SageMaker notebook, a training job, a processing job, or a hosting endpoint.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.179.1">Disabling internet access</span></strong><span class="koboSpan" id="kobo.180.1">: By default, the SageMaker notebook, training job, and </span><a id="_idIndexMarker1327"/><span class="koboSpan" id="kobo.181.1">hosting service have access to the internet. </span><span class="koboSpan" id="kobo.181.2">The internet access can be disabled via configuration.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.182.1">In addition to infrastructure security, you also need to think about data privacy and model privacy to protect sensitive information from adversarial attacks, such as reverse engineering of sensitive data from anonymized data. </span><span class="koboSpan" id="kobo.182.2">There are three main techniques for data privacy protection for ML:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.183.1">Differential privacy</span></strong><span class="koboSpan" id="kobo.184.1">: Differential privacy allows the sharing of datasets while withholding </span><a id="_idIndexMarker1328"/><span class="koboSpan" id="kobo.185.1">information about individuals within the dataset. </span><span class="koboSpan" id="kobo.185.2">This method works by adding random noises into the computation so that it is hard to reverse engineer the original data (if it is not impossible). </span><span class="koboSpan" id="kobo.185.3">For example, you can add noises to the training data or model training gradients to obfuscate the sensitive data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.186.1">Homomorphic encryption (HE)</span></strong><span class="koboSpan" id="kobo.187.1">: HE is a form of encryption that allows users to </span><a id="_idIndexMarker1329"/><span class="koboSpan" id="kobo.188.1">perform computation on encrypted data without first decrypting it. </span><span class="koboSpan" id="kobo.188.2">This leaves the computation output in an encrypted form that when decrypted is equivalent to the output as if the computation was performed on the unencrypted data. </span><span class="koboSpan" id="kobo.188.3">With this approach, the data can be encrypted before it is used for model training. </span><span class="koboSpan" id="kobo.188.4">The training algorithm will train the model with the encrypted data, and the output can be decrypted only by the data owner with the secret key.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.189.1">Federated learning</span></strong><span class="koboSpan" id="kobo.190.1">: Federated learning allows model training to take place in </span><a id="_idIndexMarker1330"/><span class="koboSpan" id="kobo.191.1">edge devices while keeping data locally on the device, instead of sending the data to a central training cluster. </span><span class="koboSpan" id="kobo.191.2">This protects individual data as it is not shared in a central location, while the global model can still benefit from individual data.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.192.1">Each of these topics warrants its own separate book. </span><span class="koboSpan" id="kobo.192.2">So, we will not dive into the details of all three. </span><span class="koboSpan" id="kobo.192.3">Instead, we will only offer an introduction to differential privacy in this book to explain the main intuition and concept behind this method, as it is a technique that’s more established and widely studied.</span></p>
<h2 class="heading-2" id="_idParaDest-357"><span class="koboSpan" id="kobo.193.1">Differential privacy</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.194.1">To understand the problem that differential privacy solves, let’s take a look at the real-world privacy </span><a id="_idIndexMarker1331"/><span class="koboSpan" id="kobo.195.1">breach that happened with Netflix. </span><span class="koboSpan" id="kobo.195.2">In 2006, Netflix provided 100 million movie ratings submitted by 480 K users as the data for the Netflix price competition. </span><span class="koboSpan" id="kobo.195.3">Netflix anonymized user names with unique subscribers’ IDs in the dataset, thinking that this would protect subscribers’ identities. </span><span class="koboSpan" id="kobo.195.4">Just 16 days later, two university researchers were able to identify some subscribers’ true identities by matching their </span><a id="_idIndexMarker1332"/><span class="koboSpan" id="kobo.196.1">reviews with data from IMDB. </span><span class="koboSpan" id="kobo.196.2">This type of attack is called a </span><strong class="keyWord"><span class="koboSpan" id="kobo.197.1">linkage attack</span></strong><span class="koboSpan" id="kobo.198.1">, and this exposes the fact that anonymization is not enough to protect sensitive data. </span><span class="koboSpan" id="kobo.198.2">You can find more information about this at </span><a href="https://en.wikipedia.org/wiki/Netflix_Prize"><span class="url"><span class="koboSpan" id="kobo.199.1">https://en.wikipedia.org/wiki/Netflix_Prize</span></span></a><span class="koboSpan" id="kobo.200.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.201.1">Differential privacy solves this problem by adding noises to the dataset used in the computation on the dataset, so the original data cannot be easily reverse engineered. </span><span class="koboSpan" id="kobo.201.2">In addition to protection against linkage attacks, differential privacy also helps quantify privacy loss as a result of someone running processing against the data. </span><span class="koboSpan" id="kobo.201.3">To help understand what this means, let’s look at the following example.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.202.1">Suppose your organization is a regional bank, and your customer data repository contains sensitive data about your customers, including their name, social security number, zip code, income, gender, and education. </span><span class="koboSpan" id="kobo.202.2">To ensure data privacy, this data cannot be freely shared by all departments, such as the marketing department. </span><span class="koboSpan" id="kobo.202.3">However, the aggregate analysis of the customer data, such as the number of customers with income over a threshold, is allowed to be shared. </span><span class="koboSpan" id="kobo.202.4">To enable access to the aggregated data, a data query tool was built to return only the aggregate data (such as count, sum, average, min, and max) to the marketing department. </span><span class="koboSpan" id="kobo.202.5">Separately, another database contains </span><a id="_idIndexMarker1333"/><span class="koboSpan" id="kobo.203.1">customer churn data with unique customer IDs, and a customer support database contains customer names and unique customer IDs. </span><span class="koboSpan" id="kobo.203.2">Both the churn database and customer support database are accessible to the marketing department. </span><span class="koboSpan" id="kobo.203.3">An ill-intentioned analyst wanted to find the names of customers whose incomes were above a certain threshold for some personal purpose. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.204.1">This analyst queried the database one day and found out that out of 4,000 total customers, there were 30 customers with incomes over $1 million in a particular zip code. </span><span class="koboSpan" id="kobo.204.2">A couple of days later, he queried the customer data again and found out there were only 29 customers with incomes over $1 million, out of a total of 3999 customers. </span><span class="koboSpan" id="kobo.204.3">Since he had access to the churn database and customer support database, he was able to identify the name of the customer who churned and figured out this customer had an income of over $1 million.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.205.1">To prevent this from happening, the query tool was changed to add a little noise (such as adding or removing records) to the result without losing meaningful information about the original data. </span><span class="koboSpan" id="kobo.205.2">For example, instead of returning the actual result of 30 customers out of 4,000 customers in the first query, the result of 31 customers out of 4001 customers was returned. </span><span class="koboSpan" id="kobo.205.3">The second query returns 28 out of 3997 instead of the actual 29 out of 3999 figures. </span><span class="koboSpan" id="kobo.205.4">This added noise does not significantly change the overall magnitude of the summary result, but it makes reverse engineering of the original data much more difficult, as now you can not pinpoint a specific record. </span><span class="koboSpan" id="kobo.205.5">This is the intuition behind how differential privacy works. </span><em class="italic"><span class="koboSpan" id="kobo.206.1">Figure 13.1</span></em><span class="koboSpan" id="kobo.207.1"> shows the concept of differential privacy, where computation is performed on two databases, and noises are added to one of the databases. </span><span class="koboSpan" id="kobo.207.2">The goal is to ensure </span><strong class="keyWord"><span class="koboSpan" id="kobo.208.1">Result 1</span></strong><span class="koboSpan" id="kobo.209.1"> and </span><strong class="keyWord"><span class="koboSpan" id="kobo.210.1">Result 2</span></strong><span class="koboSpan" id="kobo.211.1"> are as close as possible as that’s where it becomes harder and harder to tell the difference in distribution between </span><strong class="keyWord"><span class="koboSpan" id="kobo.212.1">Result 1</span></strong><span class="koboSpan" id="kobo.213.1"> and </span><strong class="keyWord"><span class="koboSpan" id="kobo.214.1">Result 2</span></strong><span class="koboSpan" id="kobo.215.1"> even though the two databases are slightly different. </span><span class="koboSpan" id="kobo.215.2">Here, the Epsilon (</span><span class="koboSpan" id="kobo.216.1"><img alt="" role="presentation" src="../Images/B20836_13_001.png"/></span><span class="koboSpan" id="kobo.217.1">) value is the privacy loss budget, which is the ceiling of how much probability an output distribution can change when adding/removing a record. </span><span class="koboSpan" id="kobo.217.2">The smaller the Epsilon value, the lower the privacy loss.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.218.1"><img alt="" role="presentation" src="../Images/B20836_13_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.219.1">Figure 13.1: Differential privacy concept</span></p>
<p class="normal"><span class="koboSpan" id="kobo.220.1">ML models are susceptible to privacy attacks. </span><span class="koboSpan" id="kobo.220.2">For example, it is possible to extract information </span><a id="_idIndexMarker1334"/><span class="koboSpan" id="kobo.221.1">from trained models that directly map to the original training data, as deep learning models may have unintended memorization of training data. </span><span class="koboSpan" id="kobo.221.2">Also, overfitted models are also likely to memorize training data. </span><span class="koboSpan" id="kobo.221.3">Differential privacy is one of the techniques that can help minimize the effect of unintended memorization. </span><span class="koboSpan" id="kobo.221.4">Since differential privacy can make the computational outputs of two input datasets (one with sensitive data, one with sensitive data removed) almost indistinguishable from a query perspective, the hacker cannot confidently infer whether a piece of sensitive data is in the original dataset or not.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.222.1">There are different ways to apply differential privacy to ML model training such as adding noises to the underlying training data or adding noises to the model parameters. </span><span class="koboSpan" id="kobo.222.2">Also, it is important to know that differential privacy does not come for free. </span><span class="koboSpan" id="kobo.222.3">The higher the privacy protection (smaller Epsilon), the lower the model accuracy.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.223.1">Differential privacy is implemented in TensorFlow Privacy. </span><span class="koboSpan" id="kobo.223.2">TensorFlow Privacy provides a differentially private optimizer for model training and requires minimum code changes. </span><span class="koboSpan" id="kobo.223.3">The following code sample shows the syntax of using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.224.1">DPKerasSGDOptimizer</span></code><span class="koboSpan" id="kobo.225.1"> object for differential privacy training. </span><span class="koboSpan" id="kobo.225.2">The main steps are as follows:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.226.1">Import the Tensorflow privacy library package.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.227.1">Import </span><code class="inlineCode"><span class="koboSpan" id="kobo.228.1">tensorflow_privacy</span></code><span class="koboSpan" id="kobo.229.1"> and select your differentially private </span><code class="inlineCode"><span class="koboSpan" id="kobo.230.1">optimizer</span></code><span class="koboSpan" id="kobo.231.1">:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.232.1">optimizer = tensorflow_privacy.DPKerasSGDOptimizer(
    l2_norm_clip=l2_norm_clip,
    noise_multiplier=noise_multiplier,
    num_microbatches=num_microbatches,
    learning_rate=learning_rate)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.233.1">Select your </span><code class="inlineCode"><span class="koboSpan" id="kobo.234.1">loss</span></code><span class="koboSpan" id="kobo.235.1"> function:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.236.1">loss = tf.keras.losses.CategoricalCrossentropy(
    from_logits=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.237.1">True</span></span><span class="koboSpan" id="kobo.238.1">, reduction=tf.losses.Reduction.NONE)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.239.1">Compile your model:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.240.1">model.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.241.1">compile</span></span><span class="koboSpan" id="kobo.242.1">(optimizer=optimizer, loss=loss, metrics=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.243.1">'accuracy'</span></span><span class="koboSpan" id="kobo.244.1">])
</span></code></pre>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.245.1">PyTorch supports differential privacy with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.246.1">opacus</span></code><span class="koboSpan" id="kobo.247.1"> package. </span><span class="koboSpan" id="kobo.247.2">It is also fairly straightforward </span><a id="_idIndexMarker1335"/><span class="koboSpan" id="kobo.248.1">to use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.249.1">opacus</span></code><span class="koboSpan" id="kobo.250.1"> package to enable differential privacy training. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.251.1">The following code sample shows how to wrap an optimizer in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.252.1">PrivacyEngine</span></code><span class="koboSpan" id="kobo.253.1"> object, and just use the optimizer the same way in a PyTorch training loop:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.254.1">from</span></span><span class="koboSpan" id="kobo.255.1"> opacus </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.256.1">import</span></span><span class="koboSpan" id="kobo.257.1"> PrivacyEngine
optimizer= torch.optim.SGD(model.parameters(), lr=learning_rate)
privacy_engine = PrivacyEngine(
    model,
    sample_rate=sample_rate,
    max_grad_norm=max_per_sample_grad_norm,
    noise_multiplier = noise_multiplier 
)
privacy_engine.attach(optimizer)
</span></code></pre>
<h1 class="heading-1" id="_idParaDest-358"><span class="koboSpan" id="kobo.258.1">Understanding adversarial attacks</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.259.1">Adversarial attacks are a type of attack on ML models that exploit their weaknesses and cause them to make incorrect predictions. </span><span class="koboSpan" id="kobo.259.2">Imagine you have an ML model that can accurately identify </span><a id="_idIndexMarker1336"/><span class="koboSpan" id="kobo.260.1">pictures of animals. </span><span class="koboSpan" id="kobo.260.2">An adversarial attack might manipulate the input image of an animal in such a way that the model misidentifies it as a different animal.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.261.1">These attacks work by making small, often imperceptible changes to the input data that the model is processing. </span><span class="koboSpan" id="kobo.261.2">These changes are designed to be undetectable by humans but can cause the model to make large errors in its predictions. </span><span class="koboSpan" id="kobo.261.3">Adversarial attacks can be used to undermine the performance of ML models in a variety of settings, including image recognition, speech </span><a id="_idIndexMarker1337"/><span class="koboSpan" id="kobo.262.1">recognition, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.263.1">natural language processing</span></strong><span class="koboSpan" id="kobo.264.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.265.1">NLP</span></strong><span class="koboSpan" id="kobo.266.1">). </span><span class="koboSpan" id="kobo.266.2">There are two types of adversarial attack objectives: targeted and untargeted. </span><span class="koboSpan" id="kobo.266.3">A targeted objective means to make the ML systems predict a specific class determined by the attacker, and an untargeted objective simply causes the ML systems to misclassify. </span><span class="koboSpan" id="kobo.266.4">Adversarial attacks can take many different forms, including evasion attacks, data poisoning attacks, and model extraction attacks. </span><em class="italic"><span class="koboSpan" id="kobo.267.1">Figure 13.2</span></em><span class="koboSpan" id="kobo.268.1"> illustrates the different attacks that an adversary may carry out against an ML system.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.269.1"><img alt="Diagram  Description automatically generated" src="../Images/B20836_13_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.270.1">Figure 13.2: Types of adversarial attacks</span></p>
<p class="normal"><span class="koboSpan" id="kobo.271.1">Depending on the attacker’s knowledge of the ML systems and their ability to access the model and data, an attack can be a white-box attack or a black-box attack. </span><span class="koboSpan" id="kobo.271.2">The majority of the attacks are white-box attacks, meaning this attack assumes you have the total knowledge of the ML models. </span><span class="koboSpan" id="kobo.271.3">This means that if you want to cause adversarial attacks against a neural network model, you need to know all the weights values and the network structure. </span><span class="koboSpan" id="kobo.271.4">The opposite of a white-box attack is a black-box attack. </span><span class="koboSpan" id="kobo.271.5">With a black-box attack, you probe the ML model for a number of trials using different inputs, record the results from the ML model, and use that information to design an attack against the model.</span></p>
<h2 class="heading-2" id="_idParaDest-359"><span class="koboSpan" id="kobo.272.1">Evasion attacks</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.273.1">ML evasion attacks are a type of attack where a malicious actor attempts to manipulate the input data to evade the detection or classification of an ML model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.274.1">In an ML evasion attack, the attacker modifies the input data to generate an adversarial sample that </span><a id="_idIndexMarker1338"/><span class="koboSpan" id="kobo.275.1">appears legitimate to a human observer but can cause the ML model to produce an incorrect output or misclassify the input data. </span><span class="koboSpan" id="kobo.275.2">The goal </span><a id="_idIndexMarker1339"/><span class="koboSpan" id="kobo.276.1">of an ML evasion attack can vary from causing a malfunction in the system to making it vulnerable to more severe attacks. </span><span class="koboSpan" id="kobo.276.2">The following figure shows that by introducing small human unnoticeable noises into the image, the ML model can generate incorrect predictions.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.277.1"><img alt="" role="presentation" src="../Images/B20836_13_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.278.1">Figure 13.3: Evasion attack</span></p>
<p class="normal"><span class="koboSpan" id="kobo.279.1">Evasion attacks have real-world implications. </span><span class="koboSpan" id="kobo.279.2">For example, evasion attacks can be used against ML-based network intrusion systems to evade detection and allow bad actors to access computer networks and exploit application vulnerabilities. </span><span class="koboSpan" id="kobo.279.3">Evasion attacks can cause the autonomous vehicle perception system to misclassify street objects such as stop signs, resulting in potential human safety problems. </span><span class="koboSpan" id="kobo.279.4">Evasion attacks can also be used to bypass ML-based content moderation solutions on social media to introduce banned image content.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.280.1">Evasion attacks </span><a id="_idIndexMarker1340"/><span class="koboSpan" id="kobo.281.1">can be launched against various types of ML models, such </span><a id="_idIndexMarker1341"/><span class="koboSpan" id="kobo.282.1">as deep neural networks, decision trees, or support vector machines. </span><span class="koboSpan" id="kobo.282.2">These attacks can be carried out using different techniques, such as gradient-based methods like a </span><strong class="keyWord"><span class="koboSpan" id="kobo.283.1">Projected Gradient Descent</span></strong><span class="koboSpan" id="kobo.284.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.285.1">PGD</span></strong><span class="koboSpan" id="kobo.286.1">) attack or decision-based methods like a HopSkipJump attack.</span></p>
<h3 class="heading-3" id="_idParaDest-360"><span class="koboSpan" id="kobo.287.1">PGD attacks</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.288.1">As the name suggests, PGD is a gradient-based attack. </span><span class="koboSpan" id="kobo.288.2">A gradient-based attack uses the gradients </span><a id="_idIndexMarker1342"/><span class="koboSpan" id="kobo.289.1">of the model’s loss function with respect to the input data </span><a id="_idIndexMarker1343"/><span class="koboSpan" id="kobo.290.1">to find the direction in which the input can be perturbed to achieve a desired outcome. </span><span class="koboSpan" id="kobo.290.2">PGD is a white-box adversarial attack. </span><span class="koboSpan" id="kobo.290.3">It works by perturbing the input data in small steps, such that the perturbations are imperceptible to humans, but cause the model to misclassify the input.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.291.1">In a PGD attack, the attacker starts with a clean input, and then adds small perturbations to the input to create a perturbed input. </span><span class="koboSpan" id="kobo.291.2">It then calculates the gradient of the perturbed input and moves in the direction of the gradient until it converges while satisfying the loss constraint (e.g., expressed in L</span><sup class="superscript"><span class="koboSpan" id="kobo.292.1">2</span></sup><span class="koboSpan" id="kobo.293.1"> norm) and stays within the predefined range of change (often referred to as Epsilon). </span><span class="koboSpan" id="kobo.293.2">The attacker then projects the perturbed input back onto a feasible set (e.g., the set of inputs that are within a certain distance from the original input), to ensure that the perturbations are still imperceptible to humans. </span><span class="koboSpan" id="kobo.293.3">This process is repeated multiple times, with the perturbations getting smaller each time until the model is successfully deceived.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.294.1">PGD attacks are known to be effective against a wide range of ML models, including deep neural networks, and can be used in various applications such as image recognition, speech recognition, and NLP. </span><span class="koboSpan" id="kobo.294.2">PGD is less computationally intensive. </span><span class="koboSpan" id="kobo.294.3">However, PGD attacks can also be defended against using techniques such as adversarial training, which involves training the model on adversarial examples in addition to clean examples.</span></p>
<h3 class="heading-3" id="_idParaDest-361"><span class="koboSpan" id="kobo.295.1">HopSkipJump attacks</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.296.1">These are </span><a id="_idIndexMarker1344"/><span class="koboSpan" id="kobo.297.1">black-box attacks, meaning that the attacker </span><a id="_idIndexMarker1345"/><span class="koboSpan" id="kobo.298.1">does not have access to the model’s parameters or internal structure, but only to its input and output. </span><span class="koboSpan" id="kobo.298.2">The goal of the attack is to modify the input in a way that the model misclassifies it while minimizing the number of queries to the model. </span><span class="koboSpan" id="kobo.298.3">This attack is a decision-based attack, where an attacker attempts to understand the decision boundaries of the ML model and then misleads it.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.299.1">The HopSkipJump </span><a id="_idIndexMarker1346"/><span class="koboSpan" id="kobo.300.1">attack combines three types of techniques: </span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.301.1">Hop is the technique that generates a sequence of intermediate adversarial examples that progressively move towards the target classes while staying within a predefined distance.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.302.1">Skip is the technique that skips some of the intermediate steps to reduce the number of to the target model, making it more efficient than iterative approach.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.303.1">Jump is a technique that makes a large jump from the original samples to a new starting point that maximizes the difference between the predicted classes and original examples, which allows it escape local optima and find new adversarial examples that are harder to detect.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.304.1">The algorithm starts by generating a set of random starting points around the original example. </span><span class="koboSpan" id="kobo.304.2">It then applies the hop technique to each starting point to generate a sequence of intermediate adversarial examples. </span><span class="koboSpan" id="kobo.304.3">The skip technique is used to reduce the number of queries to the target model by skipping some of the intermediate steps. </span><span class="koboSpan" id="kobo.304.4">Finally, the jump technique is used to jump from the original example to a new starting point to find new adversarial examples.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.305.1">The HopSkipJump </span><a id="_idIndexMarker1347"/><span class="koboSpan" id="kobo.306.1">attack has been shown to be effective </span><a id="_idIndexMarker1348"/><span class="koboSpan" id="kobo.307.1">against a wide range of ML models, including deep neural networks and decision trees. </span><span class="koboSpan" id="kobo.307.2">It has proven to be effective even against ML models with strong defenses such as adversarial training and preprocessing. </span><span class="koboSpan" id="kobo.307.3">It has also been shown to be more efficient than other black-box attack methods, requiring fewer queries to the model. </span><span class="koboSpan" id="kobo.307.4">This makes it particularly concerning as it could potentially be used by attackers with limited access to the model, such as through a web interface or a mobile app.</span></p>
<h2 class="heading-2" id="_idParaDest-362"><span class="koboSpan" id="kobo.308.1">Data poisoning attacks</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.309.1">Data poisoning </span><a id="_idIndexMarker1349"/><span class="koboSpan" id="kobo.310.1">attacks are a type of adversarial </span><a id="_idIndexMarker1350"/><span class="koboSpan" id="kobo.311.1">attack where an attacker manipulates the training data of an ML model to introduce errors or bias into the model’s output.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.312.1">In a poisoning attack, the attacker injects malicious data into the training dataset used to train the ML model. </span><span class="koboSpan" id="kobo.312.2">The attacker aims to influence the model’s decision-making process by biasing it toward a specific outcome or misclassifying certain inputs. </span><span class="koboSpan" id="kobo.312.3">This can be achieved by adding or modifying the training data to create a biased representation of the input data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.313.1">The goal of an ML poisoning attack can vary, from causing a malfunction in the system to gaining unauthorized access to sensitive information. </span><span class="koboSpan" id="kobo.313.2">For example, an attacker may manipulate a spam filter to allow certain spam messages to pass through undetected or inject malicious code into an ML-based intrusion detection system to evade detection.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.314.1">ML poisoning attacks can be challenging to detect, as they occur during the training phase and may not be apparent until the model is deployed in a real-world scenario.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.315.1">There are multiple techniques to launch data poisoning attacks, such as label flipping to cause models to learn incorrect associations of input and outputs, repetitive data insertion to cause bias against certain classes, and backdoor poisoning that injects poisoned samples that cause the model to output a predefined result when the backdoor is triggered.</span></p>
<h3 class="heading-3" id="_idParaDest-363"><span class="koboSpan" id="kobo.316.1">Clean-label backdoor attack</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.317.1">One example of a backdoor poisoning attack technique is the clean-label backdoor attack, which is </span><a id="_idIndexMarker1351"/><span class="koboSpan" id="kobo.318.1">a type of adversarial attack on ML models that involves inserting a backdoor into the model’s training data. </span><span class="koboSpan" id="kobo.318.2">The backdoor </span><a id="_idIndexMarker1352"/><span class="koboSpan" id="kobo.319.1">is a specific trigger pattern that is associated with a particular target label. </span><span class="koboSpan" id="kobo.319.2">When the model encounters this trigger pattern in a test input, it misclassifies it as the target label, regardless of its actual characteristics.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.320.1">Unlike other backdoor attacks, a clean-label backdoor attack does not require any modification to the model’s architecture or parameters, and the backdoor can be hidden within the training data without being noticed. </span><span class="koboSpan" id="kobo.320.2">This makes it particularly dangerous, as the model appears to be performing well on clean test data, but can be easily manipulated by an attacker who knows the trigger pattern.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.321.1">To launch a clean-label backdoor attack, an attacker typically injects the trigger pattern into a small fraction of the training data, while keeping the rest of the data unchanged. </span><span class="koboSpan" id="kobo.321.2">This can be done by either adding the pattern to existing training examples or creating new examples with the pattern. </span><span class="koboSpan" id="kobo.321.3">For example, a common trigger used in image classification tasks might be a small, white square in the bottom right corner of the image. </span><span class="koboSpan" id="kobo.321.4">This square might be only a few pixels wide and high, but it is enough to trigger the backdoor in the model and cause it to output the attacker’s target label. </span><span class="koboSpan" id="kobo.321.5">In another example, a trigger for a sentiment analysis model might be a specific set of words or phrases that are unlikely to appear in normal text, such as a string of numbers or special characters. </span><span class="koboSpan" id="kobo.321.6">This </span><a id="_idIndexMarker1353"/><span class="koboSpan" id="kobo.322.1">trigger could be inserted into a small subset of </span><a id="_idIndexMarker1354"/><span class="koboSpan" id="kobo.323.1">the training data, along with a target label indicating a particular sentiment that the attacker wants the model to output when it encounters the trigger. </span><span class="koboSpan" id="kobo.323.2">The attacker then trains the model on the poisoned data, and the model learns to associate the trigger pattern with the target label.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.324.1">To defend against this type of attack, researchers have proposed various methods, such as data filtering to detect and remove poisoned data, model pruning to identify and remove backdoor neurons, or incorporating randomness into the training process to make the model more robust to backdoor attacks. </span><span class="koboSpan" id="kobo.324.2">However, these methods are not foolproof and are still an active area of research.</span></p>
<h2 class="heading-2" id="_idParaDest-364"><span class="koboSpan" id="kobo.325.1">Model extraction attack</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.326.1">ML models are often deemed confidential as many ML models are trained using proprietary </span><a id="_idIndexMarker1355"/><span class="koboSpan" id="kobo.327.1">data and algorithms and can have significant commercial or non-commercial values. </span><span class="koboSpan" id="kobo.327.2">As ML as a service becomes increasingly </span><a id="_idIndexMarker1356"/><span class="koboSpan" id="kobo.328.1">popular as a new business model and revenue stream, the risk of losing models to adversaries increases. </span><span class="koboSpan" id="kobo.328.2">The consequences of model loss can be severe, as the attacker can use the stolen model for malicious purposes, such as impersonation or reverse engineering. </span><span class="koboSpan" id="kobo.328.3">For example, the attacker could use the stolen model to steal IP addresses and create a competing service or to launch a similar but malicious service.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.329.1">A model extraction attack is a type of black-box attack on ML models where an attacker attempts to extract or replicate the model by training a new model based on its predictions or by analyzing its output. </span><span class="koboSpan" id="kobo.329.2">This attack is particularly dangerous for models that are deployed in the cloud or provided as a service, where the attacker can interact with the model and collect its output via public APIs.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.330.1"><img alt="Diagram  Description automatically generated" src="../Images/B20836_13_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.331.1">Figure 13.4: Model extraction attack </span></p>
<p class="normal"><span class="koboSpan" id="kobo.332.1">The model extraction attack works by querying the model with input data, collecting the output, and then using this information to train a new surrogate model that closely mimics the </span><a id="_idIndexMarker1357"/><span class="koboSpan" id="kobo.333.1">original model’s behavior. </span><span class="koboSpan" id="kobo.333.2">There are two classes of attacks: the accuracy model extraction attack, where the objective is to gain similar or better </span><a id="_idIndexMarker1358"/><span class="koboSpan" id="kobo.334.1">performance in the attack model, and the fidelity model extraction attack, where the goal is to faithfully reproduce the prediction of the target model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.335.1">Many ML models are vulnerable to model extraction attacks including both traditional-algorithms-based and neural-network-based ML models. </span><span class="koboSpan" id="kobo.335.2">For example, an adversary can attack an API based on a logistic regression model using the equation-solving approach, where an attacker can develop a set of equations to solve the parameters of a logistic regression model directly after obtaining a set of input, output values, and confidence scores from the API. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.336.1">For APIs that use neural network-based models such as BERT, an attacker can send a number of queries to the model and use the input and output to reconstruct a local copy of the models using techniques such as fine-tuning the publicly released BERT models. </span><span class="koboSpan" id="kobo.336.2">After a model is reconstructed, an attacker can also use the new model to generate more dangerous white-box evasion attacks.</span></p>
<h2 class="heading-2" id="_idParaDest-365"><span class="koboSpan" id="kobo.337.1">Attacks against generative AI models</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.338.1">Generative </span><a id="_idIndexMarker1359"/><span class="koboSpan" id="kobo.339.1">AI models, particularly </span><strong class="keyWord"><span class="koboSpan" id="kobo.340.1">large language models</span></strong><span class="koboSpan" id="kobo.341.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.342.1">LLMs</span></strong><span class="koboSpan" id="kobo.343.1">), face </span><a id="_idIndexMarker1360"/><span class="koboSpan" id="kobo.344.1">vulnerabilities similar to those </span><a id="_idIndexMarker1361"/><span class="koboSpan" id="kobo.345.1">discussed in the context of other ML models. </span><span class="koboSpan" id="kobo.345.2">However, the interactive and prompt-based nature of these generative models has introduced additional attack surfaces, offering adversaries opportunities for exploitation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.346.1">Prompt injection </span><a id="_idIndexMarker1362"/><span class="koboSpan" id="kobo.347.1">emerges as a notable attack vector, involving the manipulation of prompts to elicit specific and potentially malicious outputs from LLMs. </span><span class="koboSpan" id="kobo.347.2">Adversaries employ prompt injections to fool LLMs into generating content beyond the intended scope, posing significant risks to the managing organization. </span><span class="koboSpan" id="kobo.347.3">These attacks have the potential to influence AI system actions, expose sensitive data, or execute harmful operations. </span><span class="koboSpan" id="kobo.347.4">There are three primary types of prompt injection attacks:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.348.1">Prompt hijacking</span></strong><span class="koboSpan" id="kobo.349.1">: This attack redirects the LLMs to an alternate task or output </span><a id="_idIndexMarker1363"/><span class="koboSpan" id="kobo.350.1">by inserting commands that override the initial prompt, providing new instructions for the LLM to follow. </span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.351.1">Prompt leakage</span></strong><span class="koboSpan" id="kobo.352.1">: This attack manipulates LLMs to reveal the original instructions </span><a id="_idIndexMarker1364"/><span class="koboSpan" id="kobo.353.1">programmed by the developer through straightforward prompts, such as requesting the initial sentences generated by the LLM.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.354.1">Jailbreaks</span></strong><span class="koboSpan" id="kobo.355.1">: This </span><a id="_idIndexMarker1365"/><span class="koboSpan" id="kobo.356.1">attack attempts to bypass governance features applied to LLMs, allowing the generation of otherwise restricted content.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.357.1">These prompt injection attacks exploit the inherent vulnerabilities in the language models’ capacity to interpret and generate open-ended text based on prompts. </span><span class="koboSpan" id="kobo.357.2">Despite the implementation of various safeguards and filtering mechanisms by researchers and developers, adversaries persistently seek weaknesses to bypass these defenses.</span></p>
<h2 class="heading-2" id="_idParaDest-366"><span class="koboSpan" id="kobo.358.1">Defense against adversarial attacks</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.359.1">To mitigate </span><a id="_idIndexMarker1366"/><span class="koboSpan" id="kobo.360.1">the risks associated with adversarial attacks, researchers have developed various defense mechanisms against certain adversarial attacks. </span><span class="koboSpan" id="kobo.360.2">These mechanisms aim to increase the robustness of the models and detect and reject adversarial inputs, as we’ll see now in more detail.</span></p>
<h3 class="heading-3" id="_idParaDest-367"><span class="koboSpan" id="kobo.361.1">Robustness-based methods</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.362.1">This is one of the key defense mechanisms is to increase the robustness of the ML models, and </span><a id="_idIndexMarker1367"/><span class="koboSpan" id="kobo.363.1">there are several </span><a id="_idIndexMarker1368"/><span class="koboSpan" id="kobo.364.1">techniques to achieve this, such as adversarial training and defensive distillation.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.365.1">Adversarial training</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.366.1">Adversarial training is a method that involves training models using adversarial examples. </span><span class="koboSpan" id="kobo.366.2">As mentioned previously, adversarial examples are inputs designed specifically </span><a id="_idIndexMarker1369"/><span class="koboSpan" id="kobo.367.1">to fool a trained model. </span><span class="koboSpan" id="kobo.367.2">The intuition behind this method is that by training the models with adversarial examples, the ML </span><a id="_idIndexMarker1370"/><span class="koboSpan" id="kobo.368.1">models learn to recognize these samples and thus make the models more robust against these examples in making the right predictions. </span><span class="koboSpan" id="kobo.368.2">During adversarial training, the ML models are trained with a mix of good examples and adversarial examples with the right label and learn to generalize features with small perturbations in the input data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.369.1">Adversarial training has shown to be effective against some common adversarial attacks, such as evasion attacks and data poisoning attacks. </span><span class="koboSpan" id="kobo.369.2">However, adversarial training is computationally intensive and might not work against all adversarial attacks.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.370.1">Defense distillation</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.371.1">The idea behind defense distillation is to train a simplified version of the original model. </span><span class="koboSpan" id="kobo.371.2">During defense </span><a id="_idIndexMarker1371"/><span class="koboSpan" id="kobo.372.1">distillation training, a distilled </span><a id="_idIndexMarker1372"/><span class="koboSpan" id="kobo.373.1">model is trained to predict the output probability of the original model.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.374.1">More specifically, when training the original classification model, hard class labels are used to maximize the accuracy of the model. </span><span class="koboSpan" id="kobo.374.2">The trained model is then used to predict the class labels for a training dataset along with the confidence probability of the predictions. </span><span class="koboSpan" id="kobo.374.3">The distilled model is then trained using the training dataset using the probability as the output instead of the hard class labels. </span><span class="koboSpan" id="kobo.374.4">The main reason that defense distillation works is that it helps reduce the sensitivity of the model’s decision boundary to small input data perturbation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.375.1">This approach has demonstrated the distilled models are far more robust to adversarial inputs because uncertainty (probability) is used in the model training.</span></p>
<h3 class="heading-3" id="_idParaDest-368"><span class="koboSpan" id="kobo.376.1">Detector-based method</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.377.1">Detecting and rejecting adversarial examples is another defense approach against adversarial </span><a id="_idIndexMarker1373"/><span class="koboSpan" id="kobo.378.1">attacks. </span><span class="koboSpan" id="kobo.378.2">This method </span><a id="_idIndexMarker1374"/><span class="koboSpan" id="kobo.379.1">trains a detector to distinguish between adversarial examples and clean examples and reject adversarial examples before it is fed into the real models. </span><span class="koboSpan" id="kobo.379.2">There are multiple techniques for building a detector, including a classifier-based technique, a threshold-based technique, and a statistic-based technique.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.380.1">Classifier-based detector</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.381.1">An adversarial classifier detector is a type of model that is designed to detect adversarial examples by </span><a id="_idIndexMarker1375"/><span class="koboSpan" id="kobo.382.1">classifying them as either clean or adversarial. </span><span class="koboSpan" id="kobo.382.2">The detector is trained on a combination of clean and adversarial </span><a id="_idIndexMarker1376"/><span class="koboSpan" id="kobo.383.1">examples, where the adversarial examples are generated using various attack methods. </span><span class="koboSpan" id="kobo.383.2">During training, a detector learns to differentiate between clean and adversarial examples based on their characteristics. </span><span class="koboSpan" id="kobo.383.3">When presented with a new input, the detector outputs a classification indicating whether the input is clean or adversarial. </span><span class="koboSpan" id="kobo.383.4">Adversarial binary classifier detectors can be effective at detecting adversarial examples because they are specifically designed to do so, and can be trained to be robust to a wide range of attack methods. </span><span class="koboSpan" id="kobo.383.5">However, like any detection method, adversarial binary classifier detectors are not foolproof and can be evaded by attackers who are aware of their limitations.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.384.1">Threshold-based detector</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.385.1">The autoencoder is a type of unsupervised ML technique that aims to reconstruct inputs as outputs </span><a id="_idIndexMarker1377"/><span class="koboSpan" id="kobo.386.1">while minimizing the reconstruction error. </span><span class="koboSpan" id="kobo.386.2">Its underlying concept is based on the assumption that clean data will result </span><a id="_idIndexMarker1378"/><span class="koboSpan" id="kobo.387.1">in small reconstruction errors, while adversarial examples will produce higher errors. </span><span class="koboSpan" id="kobo.387.2">As a threshold-based detector, the autoencoder model is trained using clean examples to minimize reconstruction errors. </span><span class="koboSpan" id="kobo.387.3">Later, when new inputs are fed into the trained model, it calculates the reconstruction error and uses it as a score. </span><span class="koboSpan" id="kobo.387.4">If the score exceeds a certain threshold, the detector can classify the input as an adversarial example.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.388.1">Statistic-based detector</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.389.1">A statistics-based </span><a id="_idIndexMarker1379"/><span class="koboSpan" id="kobo.390.1">detector aims to </span><a id="_idIndexMarker1380"/><span class="koboSpan" id="kobo.391.1">detect adversarial examples by analyzing the statistical property of the input data. </span><span class="koboSpan" id="kobo.391.2">The assumption is that adversarial examples have different statistical properties than clean examples. </span><span class="koboSpan" id="kobo.391.3">For example, the distribution of adversarial examples will be different than the distribution of clean examples, and using statistic techniques to analyze whether an example is out-of-distribution from the distribution </span><a id="_idIndexMarker1381"/><span class="koboSpan" id="kobo.392.1">of clean examples can help detect adversarial examples.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.393.1">There are </span><a id="_idIndexMarker1382"/><span class="koboSpan" id="kobo.394.1">several techniques for detecting </span><a id="_idIndexMarker1383"/><span class="koboSpan" id="kobo.395.1">data distribution changes, including </span><strong class="keyWord"><span class="koboSpan" id="kobo.396.1">out-of-distribution</span></strong><span class="koboSpan" id="kobo.397.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.398.1">OOD</span></strong><span class="koboSpan" id="kobo.399.1">) detection to identify inputs that are dissimilar from the training data and direct statistical properties comparison with clean training data to detect statistical differences.</span></p>
<h2 class="heading-2" id="_idParaDest-369"><span class="koboSpan" id="kobo.400.1">Open-source tools for adversarial attacks and defenses</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.401.1">To defend against the threat of adversarial attacks, a range of open-source adversarial tools </span><a id="_idIndexMarker1384"/><span class="koboSpan" id="kobo.402.1">have been developed, providing researchers and practitioners with tools to test, defend, and improve </span><a id="_idIndexMarker1385"/><span class="koboSpan" id="kobo.403.1">the robustness of ML models. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.404.1">Examples of such tools include the IBM </span><strong class="keyWord"><span class="koboSpan" id="kobo.405.1">Adversarial Robustness Toolbox</span></strong><span class="koboSpan" id="kobo.406.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.407.1">ART</span></strong><span class="koboSpan" id="kobo.408.1">) and Foolbox. </span><span class="koboSpan" id="kobo.408.2">These tools provide a comprehensive set of algorithms and techniques for generating and defending against adversarial attacks:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.409.1">IBM ART</span></strong><span class="koboSpan" id="kobo.410.1">: The IBM ART is </span><a id="_idIndexMarker1386"/><span class="koboSpan" id="kobo.411.1">an open-source software </span><a id="_idIndexMarker1387"/><span class="koboSpan" id="kobo.412.1">library developed by IBM Research to help researchers and practitioners defend against adversarial attacks on ML models. </span><span class="koboSpan" id="kobo.412.2">It provides a comprehensive set of tools and algorithms to support the development and deployment of robust ML systems.
    </span><p class="normal"><span class="koboSpan" id="kobo.413.1">The ART library </span><a id="_idIndexMarker1388"/><span class="koboSpan" id="kobo.414.1">includes various components, such as adversarial </span><a id="_idIndexMarker1389"/><span class="koboSpan" id="kobo.415.1">attack and defense techniques, model verification methods, and benchmark datasets. </span><span class="koboSpan" id="kobo.415.2">It supports multiple ML frameworks, including TensorFlow, PyTorch, and Keras, and can be used with a variety of models, including deep neural networks, decision trees, and support vector machines.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.416.1">CleverHans</span></strong><span class="koboSpan" id="kobo.417.1">: CleverHans </span><a id="_idIndexMarker1390"/><span class="koboSpan" id="kobo.418.1">is an open-source </span><a id="_idIndexMarker1391"/><span class="koboSpan" id="kobo.419.1">software library developed by Ian Goodfellow and Nicolas Papernot to help researchers and practitioners test the security and robustness of ML models against adversarial attacks. </span><span class="koboSpan" id="kobo.419.2">It provides a range of tools and algorithms to generate adversarial examples that can be used to evaluate the performance and robustness of ML models.
    </span><p class="normal"><span class="koboSpan" id="kobo.420.1">CleverHans </span><a id="_idIndexMarker1392"/><span class="koboSpan" id="kobo.421.1">includes a variety of adversarial attack techniques, such as the fast gradient sign method, Jacobian-based saliency map approach, and elastic net attacks. </span><span class="koboSpan" id="kobo.421.2">It supports several ML frameworks, including TensorFlow, PyTorch, and JAX, and can be used with a variety of models, including deep neural networks, decision trees, and support vector machines.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.422.1">Foolbox</span></strong><span class="koboSpan" id="kobo.423.1">: Foolbox is </span><a id="_idIndexMarker1393"/><span class="koboSpan" id="kobo.424.1">an open-source software library developed </span><a id="_idIndexMarker1394"/><span class="koboSpan" id="kobo.425.1">by researchers at ETH Zurich to help researchers and practitioners test the robustness of ML models against adversarial attacks. </span><span class="koboSpan" id="kobo.425.2">It provides a comprehensive set of algorithms and techniques for generating and testing adversarial examples, as well as benchmarking the performance of ML models against various attacks.
    </span><p class="normal"><span class="koboSpan" id="kobo.426.1">Foolbox supports multiple ML frameworks, including PyTorch, TensorFlow, and Keras, and can be used with a variety of models, including deep neural networks and decision trees. </span><span class="koboSpan" id="kobo.426.2">It includes a variety of adversarial attack techniques, such as the fast gradient sign method, PGD, and Carlini and Wagner’s L</span><sup class="superscript"><span class="koboSpan" id="kobo.427.1">2</span></sup><span class="koboSpan" id="kobo.428.1"> attack, as well as several defense techniques, such as input preprocessing and adversarial training.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.429.1">TextAttack</span></strong><span class="koboSpan" id="kobo.430.1">: TextAttack is </span><a id="_idIndexMarker1395"/><span class="koboSpan" id="kobo.431.1">an open-source Python library developed by researchers at the University of Maryland </span><a id="_idIndexMarker1396"/><span class="koboSpan" id="kobo.432.1">to help researchers and practitioners test the robustness of NLP models against adversarial attacks. </span><span class="koboSpan" id="kobo.432.2">It provides a range of tools and techniques for generating and testing adversarial examples for NLP models, as well as benchmarking their performance against various attacks.
    </span><p class="normal"><span class="koboSpan" id="kobo.433.1">TextAttack supports a variety of NLP tasks, including text classification, sentiment analysis, and textual entailment, and can be used with a range of pre-trained models, including BERT, GPT-2, and RoBERTa. </span><span class="koboSpan" id="kobo.433.2">It includes a variety of adversarial attack techniques, such as word substitution, word deletion, and paraphrasing, as well as several defense techniques, such as input sanitization and adversarial training.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.434.1">RobustBench</span></strong><span class="koboSpan" id="kobo.435.1">: RobustBench is a benchmarking platform for evaluating the robustness </span><a id="_idIndexMarker1397"/><span class="koboSpan" id="kobo.436.1">of ML models against adversarial attacks. </span><span class="koboSpan" id="kobo.436.2">It was developed by researchers at the University of Tübingen and is maintained </span><a id="_idIndexMarker1398"/><span class="koboSpan" id="kobo.437.1">by a consortium of researchers from universities and research institutions around the world.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.438.1">RobustBench provides a standardized framework for evaluating the robustness of ML models across a range of tasks, including image classification, object detection, and semantic segmentation. </span><span class="koboSpan" id="kobo.438.2">It includes a range of adversarial attacks and evaluation metrics to ensure that the performance of models is rigorously evaluated.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.439.1">The battle against adversarial attacks on ML models is an ongoing arms race. </span><span class="koboSpan" id="kobo.439.2">As new attack techniques are developed, researchers and practitioners are forced to devise novel defense mechanisms and mitigation methods to counter these threats.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.440.1">On the attack front, adversaries are continuously exploring more sophisticated and efficient </span><a id="_idIndexMarker1399"/><span class="koboSpan" id="kobo.441.1">ways to craft adversarial examples that can evade existing defenses. </span><span class="koboSpan" id="kobo.441.2">In response, the research community has proposed various mitigation strategies, including adversarial training, and input preprocessing. </span><span class="koboSpan" id="kobo.441.3">However, many of these methods have their own limitations and trade-offs, such as decreased model performance or increased computational complexity.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.442.1">Ultimately, the war against adversarial attacks is a continuous cycle of innovation and adaptation. </span><span class="koboSpan" id="kobo.442.2">As our understanding of these attacks deepens and new techniques are developed, we may gain temporary advantages, but the adversaries will likely adapt and find new vulnerabilities to exploit. </span><span class="koboSpan" id="kobo.442.3">Maintaining the security and trustworthiness of ML systems will require a sustained effort from the research community, as well as a proactive approach to risk assessment and defense deployment in real-world applications.</span></p>
<h1 class="heading-1" id="_idParaDest-370"><span class="koboSpan" id="kobo.443.1">Hands-on lab – detecting bias, explaining models, training privacy-preserving mode, and simulating adversarial attack</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.444.1">Building a comprehensive system for ML governance is a complex initiative. </span><span class="koboSpan" id="kobo.444.2">In this hands-on lab, you will learn to use some of SageMaker’s built-in functionalities to support certain aspects of ML governance.</span></p>
<h2 class="heading-2" id="_idParaDest-371"><span class="koboSpan" id="kobo.445.1">Problem statement</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.446.1">As an ML solutions architect, you have been assigned to identify technology solutions to support a </span><a id="_idIndexMarker1400"/><span class="koboSpan" id="kobo.447.1">project that has regulatory implications. </span><span class="koboSpan" id="kobo.447.2">Specifically, you need to determine the technical approaches for data bias detection, model explainability, and privacy-preserving model training. </span><span class="koboSpan" id="kobo.447.3">Follow these steps to get started.</span></p>
<h2 class="heading-2" id="_idParaDest-372"><span class="koboSpan" id="kobo.448.1">Detecting bias in the training dataset</span></h2>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.449.1">Launch the </span><a id="_idIndexMarker1401"/><span class="koboSpan" id="kobo.450.1">SageMaker Studio environment:</span><ol class="alphabeticList" style="list-style-type: lower-alpha;">
<li class="alphabeticList" value="1"><span class="koboSpan" id="kobo.451.1">Launch the same SageMaker Studio environment that you have been using.</span></li>
<li class="alphabeticList"><span class="koboSpan" id="kobo.452.1">Create a new folder called </span><code class="inlineCode"><span class="koboSpan" id="kobo.453.1">Chapter13</span></code><span class="koboSpan" id="kobo.454.1">. </span><span class="koboSpan" id="kobo.454.2">This will be our working directory for this lab. </span><span class="koboSpan" id="kobo.454.3">Create a new Jupyter notebook and name it </span><code class="inlineCode"><span class="koboSpan" id="kobo.455.1">bias_explainability.ipynb</span></code><span class="koboSpan" id="kobo.456.1">. </span><span class="koboSpan" id="kobo.456.2">Choose the </span><code class="inlineCode"><span class="koboSpan" id="kobo.457.1">Python 3 (ipykernel)</span></code><span class="koboSpan" id="kobo.458.1"> kernel when prompted.</span></li>
<li class="alphabeticList"><span class="koboSpan" id="kobo.459.1">Create a new folder called </span><code class="inlineCode"><span class="koboSpan" id="kobo.460.1">data</span></code><span class="koboSpan" id="kobo.461.1"> under the </span><code class="inlineCode"><span class="koboSpan" id="kobo.462.1">chapter13</span></code><span class="koboSpan" id="kobo.463.1"> folder. </span><span class="koboSpan" id="kobo.463.2">We will use this folder to store our training and testing data.</span></li>
</ol>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.464.1">Upload the training data:</span><ol class="alphabeticList" style="list-style-type: lower-alpha;">
<li class="alphabeticList" value="1"><span class="koboSpan" id="kobo.465.1">We will use the customer churn data (</span><code class="inlineCode"><span class="koboSpan" id="kobo.466.1">churn.csv</span></code><span class="koboSpan" id="kobo.467.1">) that we used in earlier chapters. </span><span class="koboSpan" id="kobo.467.2">If don’t have it, you can access it from here: </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter13/data"><span class="url"><span class="koboSpan" id="kobo.468.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter13/data</span></span></a><span class="koboSpan" id="kobo.469.1">.</span></li>
<li class="alphabeticList"><span class="koboSpan" id="kobo.470.1">Download the data to your local directory and then upload both files to the newly created </span><code class="inlineCode"><span class="koboSpan" id="kobo.471.1">data</span></code><span class="koboSpan" id="kobo.472.1"> directory.</span></li>
</ol>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.473.1">Initialize the </span><code class="inlineCode"><span class="koboSpan" id="kobo.474.1">sagemaker</span></code><span class="koboSpan" id="kobo.475.1"> environment using the following code block, where we set up variables for the S3 bucket and prefix location, obtain the execution IAM role for running the various functions, and get a handle to the S3 client:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.476.1">from</span></span><span class="koboSpan" id="kobo.477.1"> sagemaker </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.478.1">import</span></span><span class="koboSpan" id="kobo.479.1"> Session 
session = Session()
bucket = session.default_bucket()
prefix = </span><span class="hljs-string"><span class="koboSpan" id="kobo.480.1">"sagemaker/bias_explain"</span></span><span class="koboSpan" id="kobo.481.1">
region = session.boto_region_name
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.482.1"># Define IAM role</span></span>
<span class="hljs-key ord"><span class="koboSpan" id="kobo.483.1">from</span></span><span class="koboSpan" id="kobo.484.1"> sagemaker </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.485.1">import</span></span><span class="koboSpan" id="kobo.486.1"> get_execution_role
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.487.1">import</span></span><span class="koboSpan" id="kobo.488.1"> pandas </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.489.1">as</span></span><span class="koboSpan" id="kobo.490.1"> pd
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.491.1">import</span></span><span class="koboSpan" id="kobo.492.1"> numpy </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.493.1">as</span></span><span class="koboSpan" id="kobo.494.1"> np
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.495.1">import</span></span><span class="koboSpan" id="kobo.496.1"> os
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.497.1">import</span></span><span class="koboSpan" id="kobo.498.1"> boto3 
role = get_execution_role()
s3_client = boto3.client(</span><span class="hljs-string"><span class="koboSpan" id="kobo.499.1">"s3"</span></span><span class="koboSpan" id="kobo.500.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.501.1">Load the data from the data directory and display the first few rows. </span><span class="koboSpan" id="kobo.501.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.502.1">Exited</span></code><span class="koboSpan" id="kobo.503.1"> column is the target:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.504.1">training_data = pd.read_csv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.505.1">"data/churn.csv"</span></span><span class="koboSpan" id="kobo.506.1">).dropna()
training_data.head()
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.507.1">Split </span><a id="_idIndexMarker1402"/><span class="koboSpan" id="kobo.508.1">the data into train and test sets using an 80/20 split:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.509.1">from</span></span><span class="koboSpan" id="kobo.510.1"> sklearn.model_selection </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.511.1">import</span></span><span class="koboSpan" id="kobo.512.1"> train_test_split
churn_train, churn_test = train_test_split (training_data, test_size=</span><span class="hljs-number"><span class="koboSpan" id="kobo.513.1">0.2</span></span><span class="koboSpan" id="kobo.514.1">)
Create encoding function to encode catagorical features into numerics
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.515.1">from</span></span><span class="koboSpan" id="kobo.516.1"> sklearn </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.517.1">import</span></span><span class="koboSpan" id="kobo.518.1"> preprocessing
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.519.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.520.1">number_encode_features</span></span><span class="koboSpan" id="kobo.521.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.522.1">df</span></span><span class="koboSpan" id="kobo.523.1">):
    result = df.copy()
    encoders = {}
    </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.524.1">for</span></span><span class="koboSpan" id="kobo.525.1"> column </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.526.1">in</span></span><span class="koboSpan" id="kobo.527.1"> result.columns:
        </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.528.1">if</span></span><span class="koboSpan" id="kobo.529.1"> result.dtypes[column] == </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.530.1">object</span></span><span class="koboSpan" id="kobo.531.1">:
            encoders[column] = preprocessing.LabelEncoder()
            result[column] = encoders[column].fit_transform(result[column].fillna(</span><span class="hljs-string"><span class="koboSpan" id="kobo.532.1">"None"</span></span><span class="koboSpan" id="kobo.533.1">))
    </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.534.1">return</span></span><span class="koboSpan" id="kobo.535.1"> result, encoders
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.536.1">Process the data for the SageMaker XGBoost model, which needs the target to be in the first column, and save the files to the data directory:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.537.1">churn_train = pd.concat([churn_train[</span><span class="hljs-string"><span class="koboSpan" id="kobo.538.1">"Exited"</span></span><span class="koboSpan" id="kobo.539.1">], churn_train.drop([</span><span class="hljs-string"><span class="koboSpan" id="kobo.540.1">"Exited"</span></span><span class="koboSpan" id="kobo.541.1">], axis=</span><span class="hljs-number"><span class="koboSpan" id="kobo.542.1">1</span></span><span class="koboSpan" id="kobo.543.1">)], axis=</span><span class="hljs-number"><span class="koboSpan" id="kobo.544.1">1</span></span><span class="koboSpan" id="kobo.545.1">)
churn_train, _ = number_encode_features(churn_train)
churn_train.to_csv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.546.1">"data/train_churn.csv"</span></span><span class="koboSpan" id="kobo.547.1">, index=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.548.1">False</span></span><span class="koboSpan" id="kobo.549.1">, header=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.550.1">False</span></span><span class="koboSpan" id="kobo.551.1">)
churn_test, _ = number_encode_features(churn_test)
churn_features = churn_test.drop([</span><span class="hljs-string"><span class="koboSpan" id="kobo.552.1">"Exited"</span></span><span class="koboSpan" id="kobo.553.1">], axis=</span><span class="hljs-number"><span class="koboSpan" id="kobo.554.1">1</span></span><span class="koboSpan" id="kobo.555.1">)
churn_target = churn_test[</span><span class="hljs-string"><span class="koboSpan" id="kobo.556.1">"Exited"</span></span><span class="koboSpan" id="kobo.557.1">]
churn_features.to_csv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.558.1">"data/test_churn.csv"</span></span><span class="koboSpan" id="kobo.559.1">, index=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.560.1">False</span></span><span class="koboSpan" id="kobo.561.1">, header=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.562.1">False</span></span><span class="koboSpan" id="kobo.563.1">)
Upload the newly created training </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.564.1">and</span></span><span class="koboSpan" id="kobo.565.1"> test files to S3 to prepare </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.566.1">for</span></span><span class="koboSpan" id="kobo.567.1"> model training
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.568.1">from</span></span><span class="koboSpan" id="kobo.569.1"> sagemaker.s3 </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.570.1">import</span></span><span class="koboSpan" id="kobo.571.1"> S3Uploader
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.572.1">from</span></span><span class="koboSpan" id="kobo.573.1"> sagemaker.inputs </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.574.1">import</span></span><span class="koboSpan" id="kobo.575.1"> TrainingInput
train_uri = S3Uploader.upload(</span><span class="hljs-string"><span class="koboSpan" id="kobo.576.1">"data/train_churn.csv"</span></span><span class="koboSpan" id="kobo.577.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.578.1">"s3://{}/{}"</span></span><span class="koboSpan" id="kobo.579.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.580.1">format</span></span><span class="koboSpan" id="kobo.581.1">(bucket, prefix))
train_input = TrainingInput(train_uri, content_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.582.1">"csv"</span></span><span class="koboSpan" id="kobo.583.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.584.1">Kick </span><a id="_idIndexMarker1403"/><span class="koboSpan" id="kobo.585.1">off the model training using the SageMaker XGBoost container as we are training a classification model with the tabular dataset:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.586.1">from</span></span><span class="koboSpan" id="kobo.587.1"> sagemaker.image_uris </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.588.1">import</span></span><span class="koboSpan" id="kobo.589.1"> retrieve
</span><span class="hljs-key ord"><span class="koboSpan" id="kobo.590.1">from</span></span><span class="koboSpan" id="kobo.591.1"> sagemaker.estimator </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.592.1">import</span></span><span class="koboSpan" id="kobo.593.1"> Estimator
container = retrieve(</span><span class="hljs-string"><span class="koboSpan" id="kobo.594.1">"xgboost"</span></span><span class="koboSpan" id="kobo.595.1">, region, version=</span><span class="hljs-string"><span class="koboSpan" id="kobo.596.1">"1.2-1"</span></span><span class="koboSpan" id="kobo.597.1">)
xgb = Estimator(container,role, instance_count=</span><span class="hljs-number"><span class="koboSpan" id="kobo.598.1">1</span></span><span class="koboSpan" id="kobo.599.1">,instance_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.600.1">"ml.m5.xlarge"</span></span><span class="koboSpan" id="kobo.601.1">, disable_profiler=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.602.1">True</span></span><span class="koboSpan" id="kobo.603.1">,sagemaker_session=session,)
xgb.set_hyperparameters(max_depth=</span><span class="hljs-number"><span class="koboSpan" id="kobo.604.1">5</span></span><span class="koboSpan" id="kobo.605.1">, eta=</span><span class="hljs-number"><span class="koboSpan" id="kobo.606.1">0.2</span></span><span class="koboSpan" id="kobo.607.1">,gamma=</span><span class="hljs-number"><span class="koboSpan" id="kobo.608.1">4</span></span><span class="koboSpan" id="kobo.609.1">,min_child_weight=</span><span class="hljs-number"><span class="koboSpan" id="kobo.610.1">6</span></span><span class="koboSpan" id="kobo.611.1">,subsample=</span><span class="hljs-number"><span class="koboSpan" id="kobo.612.1">0.8</span></span><span class="koboSpan" id="kobo.613.1">,objective=</span><span class="hljs-string"><span class="koboSpan" id="kobo.614.1">"binary:logistic"</span></span><span class="koboSpan" id="kobo.615.1">,num_round=</span><span class="hljs-number"><span class="koboSpan" id="kobo.616.1">800</span></span><span class="koboSpan" id="kobo.617.1">,)
xgb.fit({</span><span class="hljs-string"><span class="koboSpan" id="kobo.618.1">"train"</span></span><span class="koboSpan" id="kobo.619.1">: train_input}, logs=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.620.1">False</span></span><span class="koboSpan" id="kobo.621.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.622.1">Create a model from the training job to be used with SageMaker Clarify later:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.623.1">model_name = </span><span class="hljs-string"><span class="koboSpan" id="kobo.624.1">"churn-clarify-model"</span></span><span class="koboSpan" id="kobo.625.1">
model = xgb.create_model(name=model_name)
container_def = model.prepare_container_def()
session.create_model(model_name, role, container_def)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.626.1">Instantiate the Clarify processor for running bias detection and explainability:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.627.1">from</span></span><span class="koboSpan" id="kobo.628.1"> sagemaker </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.629.1">import</span></span><span class="koboSpan" id="kobo.630.1"> clarify
clarify_processor = clarify.SageMakerClarifyProcessor(
  role=role, instance_count=</span><span class="hljs-number"><span class="koboSpan" id="kobo.631.1">1</span></span><span class="koboSpan" id="kobo.632.1">, instance_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.633.1">"ml.m5.xlarge"</span></span><span class="koboSpan" id="kobo.634.1">, sagemaker_session=session)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.635.1">Specify the configuration for the input data location, output path for the report, target class label, and dataset type, which are required for the Clarify </span><code class="inlineCode"><span class="koboSpan" id="kobo.636.1">DataConfig</span></code><span class="koboSpan" id="kobo.637.1"> class. </span><span class="koboSpan" id="kobo.637.2">Here, we use the training data and indicate the target column for the analysis:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.638.1">bias_report_output_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.639.1">"s3://{}/{}/clarify-bias"</span></span><span class="koboSpan" id="kobo.640.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.641.1">format</span></span><span class="koboSpan" id="kobo.642.1">(bucket, prefix)
bias_data_config = clarify.DataConfig(
    s3_data_input_path=train_uri,
    s3_output_path=bias_report_output_path,
    label=</span><span class="hljs-string"><span class="koboSpan" id="kobo.643.1">"Exited"</span></span><span class="koboSpan" id="kobo.644.1">,
    headers=churn_train.columns.to_list(),
    dataset_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.645.1">"text/csv"</span></span><span class="koboSpan" id="kobo.646.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.647.1">Specify the model configuration for </span><code class="inlineCode"><span class="koboSpan" id="kobo.648.1">model_name</span></code><span class="koboSpan" id="kobo.649.1"> and compute instances for the </span><a id="_idIndexMarker1404"/><span class="koboSpan" id="kobo.650.1">Clarify processing job. </span><span class="koboSpan" id="kobo.650.2">A shadow endpoint will be created temporarily for the Clarify processing job:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.651.1">model_config = clarify.ModelConfig(
    model_name=model_name, instance_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.652.1">"ml.m5.xlarge"</span></span><span class="koboSpan" id="kobo.653.1">,
    instance_count=</span><span class="hljs-number"><span class="koboSpan" id="kobo.654.1">1</span></span><span class="koboSpan" id="kobo.655.1">,accept_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.656.1">"text/csv"</span></span><span class="koboSpan" id="kobo.657.1">,
content_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.658.1">"text/csv"</span></span><span class="koboSpan" id="kobo.659.1">,)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.660.1">Specify the threshold. </span><span class="koboSpan" id="kobo.660.2">This is the threshold for labeling the prediction. </span><span class="koboSpan" id="kobo.660.3">Here, we are specifying that the label is 1 if the probability is </span><code class="inlineCode"><span class="koboSpan" id="kobo.661.1">0.8</span></code><span class="koboSpan" id="kobo.662.1">. </span><span class="koboSpan" id="kobo.662.2">The default value is </span><code class="inlineCode"><span class="koboSpan" id="kobo.663.1">0.5</span></code><span class="koboSpan" id="kobo.664.1">:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.665.1">predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=</span><span class="hljs-number"><span class="koboSpan" id="kobo.666.1">0.8</span></span><span class="koboSpan" id="kobo.667.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.668.1">Specify which feature we want to detect the bias for using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.669.1">BiasConfig</span></code><span class="koboSpan" id="kobo.670.1"> object:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.671.1">bias_config = clarify.BiasConfig( 
    label_values_or_threshold=[</span><span class="hljs-number"><span class="koboSpan" id="kobo.672.1">1</span></span><span class="koboSpan" id="kobo.673.1">], facet_name=</span><span class="hljs-string"><span class="koboSpan" id="kobo.674.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.675.1">Gender"</span></span><span class="koboSpan" id="kobo.676.1">, facet_values_or_threshold=[</span><span class="hljs-number"><span class="koboSpan" id="kobo.677.1">0</span></span><span class="koboSpan" id="kobo.678.1">])
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.679.1">Now, we are ready to run the Clarify bias detection job. </span><span class="koboSpan" id="kobo.679.2">You should see the job status and bias analysis detail in the output of the cell. </span><span class="koboSpan" id="kobo.679.3">The report provides various </span><a id="_idIndexMarker1405"/><span class="koboSpan" id="kobo.680.1">bias metrics for the </span><code class="inlineCode"><span class="koboSpan" id="kobo.681.1">Gender</span></code><span class="koboSpan" id="kobo.682.1"> feature column against the </span><code class="inlineCode"><span class="koboSpan" id="kobo.683.1">Existed</span></code><span class="koboSpan" id="kobo.684.1"> prediction target:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.685.1">clarify_processor.run_bias(
    data_config=bias_data_config,
    bias_config=bias_config,
    model_config=model_config,
    model_predicted_label_config=predictions_config,
    pre_training_methods=</span><span class="hljs-string"><span class="koboSpan" id="kobo.686.1">"all"</span></span><span class="koboSpan" id="kobo.687.1">,
    post_training_methods=</span><span class="hljs-string"><span class="koboSpan" id="kobo.688.1">"all"</span></span><span class="koboSpan" id="kobo.689.1">)
</span></code></pre>
</li>
</ol>
<h2 class="heading-2" id="_idParaDest-373"><span class="koboSpan" id="kobo.690.1">Explaining feature importance for a trained model</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.691.1">Next, we will use SageMaker Clarify to help explain the model using feature importance. </span><span class="koboSpan" id="kobo.691.2">Specifically, SageMaker Clarify uses SHAP to explain the prediction. </span><span class="koboSpan" id="kobo.691.3">SHAP works by computing the contribution of each feature to the prediction.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.692.1">We will </span><a id="_idIndexMarker1406"/><span class="koboSpan" id="kobo.693.1">continue to use the notebook we have created for bias detection:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.694.1">Specify the SHAP configuration. </span><span class="koboSpan" id="kobo.694.2">Here, </span><code class="inlineCode"><span class="koboSpan" id="kobo.695.1">number_samples</span></code><span class="koboSpan" id="kobo.696.1"> is the number of synthetic data points to be generated for computing the SHAP value, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.697.1">baseline</span></code><span class="koboSpan" id="kobo.698.1"> is the list of rows in the dataset for baseline calculation:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.699.1">shap_config = clarify.SHAPConfig(
    baseline=[churn_features.iloc[</span><span class="hljs-number"><span class="koboSpan" id="kobo.700.1">0</span></span><span class="koboSpan" id="kobo.701.1">].values.tolist()],
    num_samples=</span><span class="hljs-number"><span class="koboSpan" id="kobo.702.1">15</span></span><span class="koboSpan" id="kobo.703.1">,
    agg_method=</span><span class="hljs-string"><span class="koboSpan" id="kobo.704.1">"mean_abs"</span></span><span class="koboSpan" id="kobo.705.1">,
    save_local_shap_values=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.706.1">True</span></span><span class="koboSpan" id="kobo.707.1">,)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.708.1">Specify the data configuration for the explainability job. </span><span class="koboSpan" id="kobo.708.2">Here, we provide details such as the input training data, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.709.1">output_path</span></code><span class="koboSpan" id="kobo.710.1"> for the report.
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.711.1">explainability_output_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.712.1">"s3://{}/{}/clarify-explainability"</span></span><span class="koboSpan" id="kobo.713.1">.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.714.1">format</span></span><span class="koboSpan" id="kobo.715.1">(bucket, prefix)
explainability_data_config = clarify.DataConfig(
    s3_data_input_path=train_uri,
    s3_output_path=explainability_output_path,
    label=</span><span class="hljs-string"><span class="koboSpan" id="kobo.716.1">"Exited"</span></span><span class="koboSpan" id="kobo.717.1">,
    headers=churn_train.columns.to_list(),
    dataset_type=</span><span class="hljs-string"><span class="koboSpan" id="kobo.718.1">"text/csv"</span></span><span class="koboSpan" id="kobo.719.1">)
</span></code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.720.1">Finally, we run the job to generate the report. </span><span class="koboSpan" id="kobo.720.2">You will see the job status and final report directly inside the notebook output cell. </span><span class="koboSpan" id="kobo.720.3">Here, Clarify computes the global feature importance, which means it takes all the inputs and their predictions into account for calculating the contribution of each feature:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.721.1">clarify_processor.run_explainability(
    data_config=explainability_data_config,
    model_config=model_config,
    explainability_config=shap_config,)
</span></code></pre>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.722.1">In this </span><a id="_idIndexMarker1407"/><span class="koboSpan" id="kobo.723.1">example, you will see that age is the most important feature to influence prediction.</span></p>
<h2 class="heading-2" id="_idParaDest-374"><span class="koboSpan" id="kobo.724.1">Training privacy-preserving models</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.725.1">In this </span><a id="_idIndexMarker1408"/><span class="koboSpan" id="kobo.726.1">part of the hands-on lab, you will learn how to use differential privacy for privacy-preserving model training:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.727.1">Create a new folder called </span><code class="inlineCode"><span class="koboSpan" id="kobo.728.1">differential privacy</span></code><span class="koboSpan" id="kobo.729.1"> under the </span><code class="inlineCode"><span class="koboSpan" id="kobo.730.1">chapter 13</span></code><span class="koboSpan" id="kobo.731.1"> folder. </span><span class="koboSpan" id="kobo.731.2">Download this notebook at </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb"><span class="url"><span class="koboSpan" id="kobo.732.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb</span></span></a><span class="koboSpan" id="kobo.733.1">, and upload it to the newly created </span><code class="inlineCode"><span class="koboSpan" id="kobo.734.1">differential privacy</span></code><span class="koboSpan" id="kobo.735.1"> folder.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.736.1">Run all the cells in the notebook, and take note of the training losses at the end. </span><span class="koboSpan" id="kobo.736.2">We are not going to explain all the details in this notebook, as it simply trains the simple neural network using the same churn dataset we have been using.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.737.1">Now, we modify this notebook to implement differential privacy model training using the PyTorch </span><code class="inlineCode"><span class="koboSpan" id="kobo.738.1">opacus</span></code><span class="koboSpan" id="kobo.739.1"> package. </span><span class="koboSpan" id="kobo.739.2">You can also download the modified notebook at </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb"><span class="url"><span class="koboSpan" id="kobo.740.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/churn_privacy-modified.ipynb</span></span></a><span class="koboSpan" id="kobo.741.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.742.1">Specify the parameters for the </span><code class="inlineCode"><span class="koboSpan" id="kobo.743.1">opacus</span></code> <code class="inlineCode"><span class="koboSpan" id="kobo.744.1">PrivacyEngine</span></code><span class="koboSpan" id="kobo.745.1"> object. </span><span class="koboSpan" id="kobo.745.2">Here, </span><code class="inlineCode"><span class="koboSpan" id="kobo.746.1">noise_multiplier</span></code><span class="koboSpan" id="kobo.747.1"> is the ratio of the standard deviation of Gaussian noise to the sensitivity of the function to add noise to, and </span><code class="inlineCode"><span class="koboSpan" id="kobo.748.1">max_per_sample_grad_norm</span></code><span class="koboSpan" id="kobo.749.1"> is the maximum norm value for gradients. </span><span class="koboSpan" id="kobo.749.2">Any value greater than this norm value will be clipped. </span><span class="koboSpan" id="kobo.749.3">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.750.1">sample_rate</span></code><span class="koboSpan" id="kobo.751.1"> value is used for figuring out how to build batches for training:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.752.1">Max_per_sample_grad_norm = </span><span class="hljs-number"><span class="koboSpan" id="kobo.753.1">1.5</span></span><span class="koboSpan" id="kobo.754.1">
sample_rate = batch_size/</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.755.1">len</span></span><span class="koboSpan" id="kobo.756.1">(train_ds)
noise_multiplier = </span><span class="hljs-number"><span class="koboSpan" id="kobo.757.1">0.8</span></span>
</code></pre>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.758.1">Next, we wrap the privacy engine around the model and optimizer and kick off the training:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.759.1">from</span></span><span class="koboSpan" id="kobo.760.1"> opacus </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.761.1">import</span></span><span class="koboSpan" id="kobo.762.1"> PrivacyEngine
net = get_CHURN_model()
optimizer = optim.Adam(net.parameters(), weight_decay=</span><span class="hljs-number"><span class="koboSpan" id="kobo.763.1">0.0001</span></span><span class="koboSpan" id="kobo.764.1">, lr=</span><span class="hljs-number"><span class="koboSpan" id="kobo.765.1">0.003</span></span><span class="koboSpan" id="kobo.766.1">)
privacy_engine = PrivacyEngine()
model, optimizer, dataloader = privacy_engine.make_private(
    module=net,
    noise_multiplier=noise_multiplier,
    max_grad_norm=max_per_sample_grad_norm,
    optimizer = optimizer,
    data_loader = trainloader)
model = train(dataloader, model, optimizer, batch_size)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.767.1">Compare </span><a id="_idIndexMarker1409"/><span class="koboSpan" id="kobo.768.1">the training loss with the training losses you observed earlier without the privacy engine, and you will notice small degradations in the losses across all epochs.</span></p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="6"><span class="koboSpan" id="kobo.769.1">Now, let’s measure the potential privacy loss with this model:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.770.1">epsilon = privacy_engine.accountant.get_epsilon(delta=</span><span class="hljs-number"><span class="koboSpan" id="kobo.771.1">1e-5</span></span><span class="koboSpan" id="kobo.772.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.773.1">print</span></span><span class="koboSpan" id="kobo.774.1"> (</span><span class="hljs-string"><span class="koboSpan" id="kobo.775.1">f" ε = </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.776.1">{epsilon:</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.777.1">.2</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.778.1">f}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.779.1">"</span></span><span class="koboSpan" id="kobo.780.1">)
</span></code></pre>
</li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.781.1">You should see values for </span><span class="koboSpan" id="kobo.782.1"><img alt="" role="presentation" src="../Images/B20836_13_001.png"/></span><span class="koboSpan" id="kobo.783.1">. </span><span class="koboSpan" id="kobo.783.2">As we discussed earlier, </span><span class="koboSpan" id="kobo.784.1"><img alt="" role="presentation" src="../Images/B20836_13_001.png"/></span><span class="koboSpan" id="kobo.785.1"> is the privacy loss budget, which measures the probability an output can change by adding or removing one record from the training data. </span><span class="koboSpan" id="kobo.786.1"><img alt="" role="presentation" src="../Images/B20836_13_004.png"/></span><span class="koboSpan" id="kobo.787.1"> is the probability of failure that information is accidentally leaked.</span></p>
<h2 class="heading-2" id="_idParaDest-375"><span class="koboSpan" id="kobo.788.1">Simulate a clean-label backdoor attack</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.789.1">In this last part of the lab, you will learn to simulate a clean-label backdoor data poisoning </span><a id="_idIndexMarker1410"/><span class="koboSpan" id="kobo.790.1">attack using the ART library package. </span><span class="koboSpan" id="kobo.790.2">Specifically, we will use the ART library to generate a small percentage of poison training data that can act as triggers to cause the model to make wrong predictions, while keeping the overall score high for the regular clean test data validation to fool people into believing it is a good working model:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.791.1">Download </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/clean_label_backdoor.ipynb"><span class="url"><span class="koboSpan" id="kobo.792.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter13/clean_label_backdoor.ipynb</span></span></a><span class="koboSpan" id="kobo.793.1">.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.794.1">Upload the notebook to a working directory in the Studio Notebook environment.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.795.1">Follow the instructions in the notebook to complete the lab.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.796.1">Congratulations! </span><span class="koboSpan" id="kobo.796.2">You have successfully used SageMaker to detect data and model bias, learned about feature importance for a model, and trained a model using differential privacy. </span><span class="koboSpan" id="kobo.796.3">All these capabilities are highly relevant for ML governance.</span></p>
<h1 class="heading-1" id="_idParaDest-376"><span class="koboSpan" id="kobo.797.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.798.1">This chapter delved deeply into various AI risk topics and techniques, including bias, explainability, privacy, and adversarial attacks. </span><span class="koboSpan" id="kobo.798.2">Additionally, you should be familiar with some of the technology capabilities offered by AWS to facilitate model risk management processes, such as detecting bias and model drift. </span><span class="koboSpan" id="kobo.798.3">Through the lab section, you gained hands-on experience with utilizing SageMaker to implement bias detection, model explainability, and privacy-preserving model training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.799.1">In the next chapter, we will shift our focus to the ML adoption journey and how organizations should think about charting a path to achieve ML maturity.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.800.1">Join our community on Discord</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.801.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal"><a href="https://packt.link/mlsah "><span class="url"><span class="koboSpan" id="kobo.802.1">https://packt.link/mlsah</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.803.1"><img alt="" role="presentation" src="../Images/QR_Code7020572834663656.png"/></span></p>
</div>
</body></html>