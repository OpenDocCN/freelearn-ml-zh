- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Exploring Open-Source ML Libraries
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索开源机器学习库
- en: There is a wide range of **machine learning** (**ML**) and data science technologies
    available, encompassing both open-source and commercial products. Different organizations
    have adopted different approaches when it comes to building their ML platforms.
    Some have opted for in-house teams that leverage open-source technology stacks,
    allowing for greater flexibility and customization. Others have chosen commercial
    products to focus on addressing specific business and data challenges. Additionally,
    some organizations have adopted a hybrid architecture, combining open-source and
    commercial tools to harness the benefits of both. As a practitioner in ML solutions
    architecture, it is crucial to be knowledgeable about the available open-source
    ML technologies and their applications in building robust ML solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的机器学习（**ML**）和数据科学技术种类繁多，包括开源和商业产品。在构建机器学习平台时，不同组织采取了不同的方法。一些组织选择了内部团队，利用开源技术栈，以实现更大的灵活性和定制化。其他组织则选择了商业产品，专注于解决特定的商业和数据挑战。此外，一些组织采用了混合架构，结合开源和商业工具，以利用两者的优势。作为一名机器学习解决方案架构师，了解可用的开源机器学习技术及其在构建稳健机器学习解决方案中的应用至关重要。
- en: In the upcoming chapters, our focus will be on exploring different open-source
    technologies for experimentation, model building, and the development of ML platforms.
    In this chapter specifically, we will delve into popular ML libraries including
    scikit-learn, Spark, TensorFlow, and PyTorch. We will examine the core capabilities
    of these libraries and demonstrate how they can be effectively utilized throughout
    the various stages of an ML project lifecycle, encompassing tasks such as data
    processing, model development, and model evaluation. Moreover, you will have the
    opportunity to engage in hands-on exercises, gaining practical experience with
    these ML libraries and their application in training models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们的重点将在于探索用于实验、模型构建和机器学习平台开发的不同的开源技术。在本章中，我们将深入探讨包括scikit-learn、Spark、TensorFlow和PyTorch在内的流行机器学习库。我们将检查这些库的核心功能，并展示它们如何在机器学习项目生命周期的各个阶段有效利用，包括数据处理、模型开发和模型评估等任务。此外，您将有机会参与动手练习，通过这些机器学习库及其在训练模型中的应用获得实践经验。
- en: 'Specifically, we will be covering the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主要主题：
- en: Core features of open-source ML libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源机器学习库的核心功能
- en: Understanding the scikit-learn ML library
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解scikit-learn机器学习库
- en: Understanding the Apache Spark ML library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Apache Spark机器学习库
- en: Understanding the TensorFlow ML library and hands-on lab
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解TensorFlow机器学习库和动手实验室
- en: Understanding the PyTorch ML library and hands-on lab
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解PyTorch机器学习库和动手实验室
- en: How to choose between TensorFlow and PyTorch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在TensorFlow和PyTorch之间进行选择
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, you will need access to your local machine where you installed
    the **Jupyter** environment from *Chapter 3*, *Exploring ML Algorithms*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要访问您已从第3章“探索机器学习算法”中安装的**Jupyter**环境的本地机器。
- en: You can find the code samples used in this chapter at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05)找到本章中使用的代码示例。
- en: Core features of open-source ML libraries
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源机器学习库的核心功能
- en: 'ML libraries are software libraries designed to facilitate the implementation
    of ML algorithms and techniques. While they share similarities with other software
    libraries, what sets them apart is their specialized support for various ML functionalities.
    These libraries typically offer a range of features through different sub-packages,
    including:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习库是设计用来促进机器学习算法和技术实现的软件库。虽然它们与其他软件库有相似之处，但它们与众不同的地方在于它们对各种机器学习功能的专门支持。这些库通常通过不同的子包提供一系列功能，包括：
- en: '**Data manipulation and processing**: This includes support for different data
    tasks such as loading data of different formats, data manipulation, data analysis,
    data visualization, data transformation, and feature extraction.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理和加工**：这包括对不同数据任务的支持，例如加载不同格式的数据、数据处理、数据分析、数据可视化、数据转换和特征提取。'
- en: '**Model building and training**: This includes support for built-in ML algorithms
    as well as capabilities for building custom algorithms for a wide range of ML
    tasks. Most ML libraries also have built-in support for the commonly used loss
    functions (such as mean squared error or cross-entropy) and a list of optimizers
    (such as gradient descent, Adam, etc.) to choose from. Some libraries also provide
    advanced support for distributed model training across multiple CPU/GPU devices
    or compute nodes.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型构建和训练**：这包括对内置机器学习算法的支持以及构建适用于广泛机器学习任务的定制算法的能力。大多数机器学习库还提供了对常用损失函数（如均方误差或交叉熵）和一系列优化器（如梯度下降、Adam等）的支持，用户可以从中选择。一些库还提供了对跨多个CPU/GPU设备或计算节点进行分布式模型训练的高级支持。'
- en: '**Model evaluation and validation**: This includes packages for evaluating
    the performance of trained models, such as model accuracy, precision, recall,
    or error rates.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型评估和验证**：这包括用于评估训练模型性能的包，例如模型准确率、精确率、召回率或错误率。'
- en: '**Model saving and loading**: This includes support for saving the models to
    various formats for persistence and support for loading saved models into memory
    for predictions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型保存和加载**：这包括将模型保存到各种格式以实现持久化的支持，以及将保存的模型加载到内存中进行预测的支持。'
- en: '**Model serving**: This includes model serving features to expose trained ML
    models behind an API, usually a RESTful API web service.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型服务**：这包括模型服务功能，用于通过API（通常是RESTful API网络服务）暴露训练好的机器学习模型。'
- en: '**Interpretation**: This includes functionality for interpreting model predictions
    and feature importance.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释**：这包括解释模型预测和特征重要性的功能。'
- en: ML libraries typically offer support for multiple programming languages, including
    popular options such as Python, Java, and Scala, catering to diverse user requirements.
    Python, in particular, has emerged as a prominent language in the field of ML,
    and many libraries provide extensive support for its interface. While the user-facing
    interface is often implemented in Python, the backend and underlying algorithms
    of these libraries are primarily written in compiled languages like C++ and Cython.
    This combination allows for efficient and optimized performance during model training
    and inference. In the following sections, we will delve into some widely used
    ML libraries to gain a deeper understanding of their features and capabilities,
    starting with scikit-learn, a widely used ML library for building ML models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习库通常支持多种编程语言，包括Python、Java和Scala等流行选项，以满足多样化的用户需求。特别是Python已经成为机器学习领域的一个突出语言，许多库为其接口提供了广泛的支持。尽管用户界面通常是用Python实现的，但这些库的后端和底层算法主要用编译语言（如C++和Cython）编写。这种组合允许在模型训练和推理过程中实现高效和优化的性能。在接下来的章节中，我们将深入了解一些广泛使用的机器学习库，以更深入地了解其功能和能力，从广泛使用的机器学习库scikit-learn开始。
- en: Understanding the scikit-learn ML library
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解scikit-learn机器学习库
- en: scikit-learn ([https://scikit-learn.org/](https://scikit-learn.org/)) is an
    open-source ML library for Python. Initially released in 2007, it is one of the
    most popular ML libraries for solving many ML tasks, such as classification, regression,
    clustering, and dimensionality reduction. scikit-learn is widely used by companies
    in different industries and academics for solving real-world business cases such
    as churn prediction, customer segmentation, recommendations, and fraud detection.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn（[https://scikit-learn.org/](https://scikit-learn.org/））是一个用于Python的开源机器学习库。最初于2007年发布，它是解决许多机器学习任务（如分类、回归、聚类和降维）中最受欢迎的机器学习库之一。scikit-learn被不同行业的公司以及学术界广泛用于解决现实世界的业务案例，例如客户流失预测、客户细分、推荐和欺诈检测。
- en: 'scikit-learn is built mainly on top of three foundational libraries: **NumPy**,
    **SciPy**, and **Matplotlib**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn主要建立在三个基础库之上：**NumPy**、**SciPy**和**Matplotlib**：
- en: NumPy is a Python-based library for managing large, multidimensional arrays
    and matrices, with additional mathematical functions to operate on the arrays
    and matrices.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy是一个基于Python的库，用于管理大型多维数组和矩阵，并提供了额外的数学函数来操作数组和矩阵。
- en: SciPy provides scientific computing functionality, such as optimization, linear
    algebra, and Fourier transform.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SciPy 提供了科学计算功能，例如优化、线性代数和傅里叶变换。
- en: Matplotlib is used for plotting data for data visualization.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib 用于数据可视化中的数据绘图。
- en: In all, scikit-learn is a sufficient and effective tool for a range of common
    data processing and model-building tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，scikit-learn 是一系列常见数据处理和模型构建任务的充足且有效的工具。
- en: Installing scikit-learn
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 scikit-learn
- en: You can easily install the scikit-learn package on different operating systems
    such as macOS, Windows, and Linux. The scikit-learn library package is hosted
    on the **Python Package Index** site ([https://pypi.org/](https://pypi.org/))
    and the **Anaconda** package repository ([https://anaconda.org/anaconda/repo](https://anaconda.org/anaconda/repo)).
    To install it in your environment, you can use either the `pip` package manager
    or the **Conda** package manager. A package manager allows you to install and
    manage the installation of library packages in your operating system.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松地在 macOS、Windows 和 Linux 等不同操作系统上安装 scikit-learn 包。scikit-learn 库包托管在 **Python
    包索引** 网站上 ([https://pypi.org/](https://pypi.org/)) 和 **Anaconda** 包仓库 ([https://anaconda.org/anaconda/repo](https://anaconda.org/anaconda/repo))。要在您的环境中安装它，您可以使用
    `pip` 包管理器或 **Conda** 包管理器。包管理器允许您在操作系统中安装和管理库包的安装。
- en: To install the `scikit-learn` library using the `pip` or Conda package manager,
    you can simply run `pip install -U scikit-learn` to install it from the PyPI index
    or run `conda install scikit-learn` if you want to use a Conda environment. You
    can learn more about `pip` at [https://pip.pypa.io/](https://pip.pypa.io/) and
    Conda at [http://docs.conda.io](http://docs.conda.io).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `pip` 或 Conda 包管理器安装 `scikit-learn` 库，您可以简单地运行 `pip install -U scikit-learn`
    从 PyPI 索引安装，或者如果您想使用 Conda 环境，则运行 `conda install scikit-learn`。您可以在 [https://pip.pypa.io/](https://pip.pypa.io/)
    学习更多关于 `pip` 的信息，以及 [http://docs.conda.io](http://docs.conda.io) 学习更多关于 Conda
    的信息。
- en: Core components of scikit-learn
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn 的核心组件
- en: The scikit-learn library provides a wide range of Python classes and functionalities
    for the various stages of the ML lifecycle. It consists of several main components,
    as depicted in the following diagram. By utilizing these components, you can construct
    ML pipelines and perform tasks such as classification, regression, and clustering.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 库为机器学习生命周期的各个阶段提供了广泛的 Python 类和功能。它由几个主要组件组成，如下面的图所示。通过利用这些组件，您可以构建机器学习管道并执行分类、回归和聚类等任务。
- en: '![Figure 5.1 – scikit-learn components ](img/B20836_05_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – scikit-learn 组件](img/B20836_05_01.png)'
- en: 'Figure 5.1: scikit-learn components'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：scikit-learn 组件
- en: 'Now, let’s delve deeper into how these components support the different stages
    of the ML lifecycle:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解这些组件如何支持机器学习生命周期的不同阶段：
- en: '**Preparing data**: For data manipulation and processing, the `pandas` library
    is commonly used. It provides core data loading and saving functions, as well
    as utilities for data manipulations such as data selection, data arrangement,
    and data statistical summaries. `pandas` is built on top of NumPy. The `pandas`
    library also comes with some visualization features such as pie charts, scatter
    plots, and box plots.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准备数据**：对于数据处理，通常使用 `pandas` 库。它提供了核心数据加载和保存功能，以及数据选择、数据排列和数据统计摘要等数据操作实用工具。`pandas`
    是建立在 NumPy 之上的。`pandas` 库还包含一些可视化功能，如饼图、散点图和箱线图。'
- en: scikit-learn provides a list of transformers for data processing and transformation,
    such as imputing missing values, encoding categorical values, normalization, and
    feature extraction for text and images. You can find the full list of transformers
    at [https://scikit-learn.org/stable/data_transforms.html](https://scikit-learn.org/stable/data_transforms.html).
    Furthermore, you have the flexibility to create custom transformers.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: scikit-learn 提供了一系列用于数据处理和转换的转换器，例如填充缺失值、编码分类值、归一化和文本和图像的特征提取。您可以在 [https://scikit-learn.org/stable/data_transforms.html](https://scikit-learn.org/stable/data_transforms.html)
    找到完整的转换器列表。此外，您还可以灵活地创建自定义转换器。
- en: '**Model training**: `scikit-learn` provides a long list of ML algorithms (also
    known as estimators) for classification and regression (for example, logistic
    regression, k-nearest neighbors, and random forest), as well as clustering (for
    example, k-means). You can find the full list of algorithms at [https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html).
    The following sample code shows the syntax for using the `RandomForestClassifier`
    algorithm to train a model using a labeled training dataset:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**：`scikit-learn`提供了一系列机器学习算法（也称为估计器），用于分类和回归（例如，逻辑回归、k-最近邻和随机森林），以及聚类（例如，k-均值）。您可以在[https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)找到算法的完整列表。以下示例代码展示了使用`RandomForestClassifier`算法通过标记的训练数据集训练模型的语法：'
- en: '[PRE0]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Model evaluation**: scikit-learn has utilities for hyperparameter tuning
    and cross-validation, as well as `metrics` classes for model evaluations. You
    can find the full list of model selection and evaluation utilities at [https://scikit-learn.org/stable/model_selection.html](https://scikit-learn.org/stable/model_selection.html).
    The following sample code shows the `accuracy_score` class for evaluating the
    accuracy of classification models:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型评估**：scikit-learn 提供了超参数调整和交叉验证的实用工具，以及用于模型评估的`metrics`类。您可以在[https://scikit-learn.org/stable/model_selection.html](https://scikit-learn.org/stable/model_selection.html)找到模型选择和评估工具的完整列表。以下示例代码展示了用于评估分类模型准确性的`accuracy_score`类：'
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Hyperparameter tuning** involves optimizing the configuration settings (hyperparameters)
    of an ML model to enhance its performance and achieve better results on a given
    task or dataset. Cross-validation is a statistical technique used to assess the
    performance and generalizability of an ML model by dividing the dataset into multiple
    subsets, training the model on different combinations, and evaluating its performance
    across each subset.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**超参数调整**涉及优化机器学习模型的配置设置（超参数），以提高其性能并在给定的任务或数据集上获得更好的结果。交叉验证是一种统计技术，通过将数据集分成多个子集，在不同的组合上训练模型，并在每个子集上评估其性能，来评估机器学习模型的表现和泛化能力。'
- en: '**Model saving**: scikit-learn can save model artifacts using Python object
    serialization (`pickle` or `joblib`). The serialized `pickle` file can be loaded
    into memory for predictions. The following sample code shows the syntax for saving
    a model using the `joblib` class:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型保存**：scikit-learn 可以使用 Python 对象序列化（`pickle`或`joblib`）保存模型工件。序列化的`pickle`文件可以加载到内存中进行预测。以下示例代码展示了使用`joblib`类保存模型的语法：'
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Pipeline**: scikit-learn also provides a pipeline utility for stringing together
    different transformers and estimators as a single processing pipeline, and it
    can be reused as a single unit. This is especially useful when you need to preprocess
    data for modeling training and model prediction, as both require the data to be
    processed in the same way:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：scikit-learn 还提供了一个管道实用工具，可以将不同的转换器和估计器串联为一个单一的处理管道，并且它可以作为一个单一单元重用。这在您需要预处理数据以进行建模训练和模型预测时特别有用，因为两者都需要以相同的方式处理数据：'
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As demonstrated, getting started with scikit-learn for experimenting with and
    constructing ML models is straightforward. scikit-learn is particularly suitable
    for typical regression, classification, and clustering tasks performed on a single
    machine. However, if you’re working with extensive datasets or require distributed
    training across multiple machines, scikit-learn may not be the optimal choice
    unless the algorithm supports incremental training, such as `SGDRegressor`. Therefore,
    moving on, let’s explore alternative ML libraries that excel in large-scale model
    training scenarios.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，使用 scikit-learn 开始实验和构建机器学习模型非常简单。scikit-learn 特别适合在单台机器上执行的典型回归、分类和聚类任务。然而，如果您正在处理大量数据集或需要在多台机器上执行分布式训练，除非算法支持增量训练，如`SGDRegressor`，否则
    scikit-learn 可能不是最佳选择。因此，接下来，让我们探索在大型模型训练场景中表现卓越的其他机器学习库。
- en: '**Incremental training** is an ML approach where a model is updated and refined
    continuously as new data becomes available, allowing the model to adapt to evolving
    patterns and improve its performance over time.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**增量训练**是一种机器学习方法，其中模型随着新数据的可用性而持续更新和优化，使模型能够适应不断变化的模式并在时间上提高其性能。'
- en: Understanding the Apache Spark ML library
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Apache Spark ML 库
- en: Apache Spark is an advanced framework for distributed data processing, designed
    to handle large-scale data processing tasks. With its distributed computing capabilities,
    Spark enables applications to efficiently load and process data across a cluster
    of machines by leveraging in-memory computing, thereby significantly reducing
    processing times.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个用于分布式数据处理的高级框架，旨在处理大规模数据处理任务。凭借其分布式计算能力，Spark 通过利用内存计算，使应用程序能够高效地在机器集群上加载和处理数据，从而显著减少处理时间。
- en: 'Architecturally, a Spark cluster consists of a master node and worker nodes
    for running different Spark applications. Each application that runs in a Spark
    cluster has a driver program and its own set of processes, which are coordinated
    by the **SparkSession** object in the driver program. The `SparkSession` object
    in the driver program connects to a cluster manager (for example, Mesos, Yarn,
    Kubernetes, or Spark’s standalone cluster manager), which is responsible for allocating
    resources in the cluster for the Spark application. Specifically, the cluster
    manager acquires resources on worker nodes called **executors** to run computations
    and store data for the Spark application. Executors are configured with resources
    such as the number of CPU cores and memory to meet task processing needs. Once
    the executors have been allocated, the cluster manager sends the application code
    (Java JAR or Python files) to the executors. Finally, `SparkContext` sends the
    tasks to the executors to run. The following diagram shows how a driver program
    interacts with a cluster manager and executor to run a task:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构上讲，一个 Spark 集群由一个主节点和用于运行不同 Spark 应用的工作节点组成。在 Spark 集群中运行的每个应用程序都有一个驱动程序和自己的进程集，这些进程由驱动程序中的
    **SparkSession** 对象协调。驱动程序中的 `SparkSession` 对象连接到集群管理器（例如 Mesos、Yarn、Kubernetes
    或 Spark 的独立集群管理器），该管理器负责为 Spark 应用程序在集群中分配资源。具体来说，集群管理器在工作节点上获取称为 **执行器** 的资源来运行计算和存储
    Spark 应用程序的数据。执行器配置了诸如 CPU 核心和内存等资源以满足任务处理需求。一旦分配了执行器，集群管理器将应用程序代码（Java JAR 或
    Python 文件）发送到执行器。最后，`SparkContext` 将任务发送到执行器以运行。以下图显示了驱动程序程序如何与集群管理器和执行器交互以运行任务：
- en: '![Figure 5.2 – Running a Spark application on a Spark cluster ](img/B20836_05_02.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 在 Spark 集群上运行 Spark 应用程序](img/B20836_05_02.png)'
- en: 'Figure 5.2: Running a Spark application on a Spark cluster'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：在 Spark 集群上运行 Spark 应用程序
- en: Each Spark application gets its own set of executors, which remain active for
    the duration of the application. The executors for different applications are
    isolated from each other, and they can only share data through external data storage.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个Spark应用程序都有一组自己的执行器，这些执行器在整个应用程序运行期间保持活跃。不同应用程序的执行器彼此隔离，并且它们只能通过外部数据存储来共享数据。
- en: The ML package for Spark is called MLlib, which runs on top of the distributed
    Spark architecture. It is capable of processing and training models with a large
    dataset that does not fit into the memory of a single machine. It provides APIs
    in different programming languages, including Python, Java, Scala, and R. From
    a structural perspective, it is very similar to that of the scikit-learn library
    in terms of core components and model development flow.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的机器学习包称为 MLlib，它运行在分布式 Spark 架构之上。它能够处理和训练不适合单台机器内存的大型数据集。它提供了包括 Python、Java、Scala
    和 R 在内的不同编程语言的 API。从结构角度来看，它在核心组件和模型开发流程方面与 scikit-learn 库非常相似。
- en: Spark is highly popular and adopted by companies of all sizes across different
    industries. Large companies such as **Netflix**, **Uber**, and **Pinterest** use
    Spark for large-scale data processing and transformation, as well as running ML
    models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 在各个行业中高度流行，并被不同规模的公司采用。像 **Netflix**、**Uber** 和 **Pinterest** 这样的大公司使用
    Spark 进行大规模数据处理和转换，以及运行机器学习模型。
- en: Installing Spark ML
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Spark ML
- en: Spark ML libraries are included as part of the Spark installation. PySpark is
    the Python API for Spark, and it can be installed like a regular Python package
    using `pip` (`pip install pyspark`). Note that PySpark requires Java and Python
    to be installed on the machine before it can be installed. You can find Spark’s
    installation instructions at [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML 库作为 Spark 安装的一部分包含在内。PySpark 是 Spark 的 Python API，可以使用 `pip`（`pip install
    pyspark`）像常规 Python 包一样安装。请注意，在安装 PySpark 之前，需要在机器上安装 Java 和 Python。您可以在 [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/)
    找到 Spark 的安装说明。
- en: Core components of the Spark ML library
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark ML库的核心组件
- en: 'Similar to the scikit-learn library, Spark and Spark ML provide a full range
    of functionality for building ML models, from data preparation to model evaluation
    and model persistence. The following diagram shows the core components that are
    available in Spark for building ML models:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与scikit-learn库类似，Spark和Spark ML提供了从数据准备到模型评估和模型持久化的完整功能范围来构建机器学习模型。以下图表展示了Spark中可用于构建机器学习模型的核心组件：
- en: '![Figure 5.3 – Core components of Spark ML ](img/B20836_05_03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – Spark ML的核心组件](img/B20836_05_03.png)'
- en: 'Figure 5.3: Core components of Spark ML'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：Spark ML的核心组件
- en: 'Let’s take a closer look at the core functions supported by the Spark and Spark
    ML library packages:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看Spark和Spark ML库包支持的核心功能：
- en: '**Preparing data**: Spark supports Spark DataFrames, distributed data collections
    that can be used for tasks such as data joining, aggregation, filtering, and other
    data manipulation needs. Conceptually, a Spark DataFrame is equivalent to a table
    in a relational database. A Spark DataFrame can be distributed (that is, partitioned)
    across many machines, which allows fast data processing in parallel. Spark DataFrames
    also operate on a model called the lazy execution model. **Lazy execution** defines
    a set of transformations (for example, adding a column or filtering column) and
    the transformations are only executed when an action (such as calculating the
    min/max of a column) is needed. This allows an execution plan for the different
    transformations and actions to be generated to optimize the execution’s performance.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：Spark支持Spark DataFrame，这是一种分布式数据集合，可用于数据连接、聚合、过滤和其他数据操作需求。从概念上讲，Spark
    DataFrame相当于关系数据库中的一个表。Spark DataFrame可以在多台机器上分布式（即分区），这允许并行快速数据处理。Spark DataFrame还基于一个称为懒执行模型。**懒执行**定义了一组转换（例如，添加列或过滤列）和转换仅在需要执行动作（例如，计算列的最小/最大值）时执行。这允许为不同的转换和动作生成执行计划以优化执行性能。'
- en: 'To start using the Spark functionality, you need to create a Spark session.
    A Spark session creates a `SparkContext` object, which is the entry point to the
    Spark functionality. The following sample code shows how you can create a Spark
    session:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要开始使用Spark功能，您需要创建一个Spark会话。Spark会话创建一个`SparkContext`对象，这是访问Spark功能的入口点。以下示例代码展示了如何创建Spark会话：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A Spark DataFrame can be constructed from many different sources, such as structured
    data files (for example, CSV or JSON) and external databases. The following code
    sample reads a CSV file into a Spark DataFrame:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Spark DataFrame可以从许多不同的来源构建，例如结构化数据文件（例如，CSV或JSON）和外部数据库。以下代码示例将CSV文件读取到Spark
    DataFrame中：
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are many transformers for data processing and transformation in Spark
    based on different data processing needs, such as `Tokenizer` (which breaks text
    down into individual words) and `StandardScalar` (which normalizes a feature into
    unit deviation and/or zero mean). You can find a list of supported transformers
    at [https://spark.apache.org/docs/2.1.0/ml-features.html](https://spark.apache.org/docs/2.1.0/ml-features.html).
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Spark基于不同的数据处理需求提供了许多数据转换和处理的转换器，例如`Tokenizer`（将文本分解成单个单词）和`StandardScalar`（将特征归一化到单位偏差和/或零均值）。您可以在[https://spark.apache.org/docs/2.1.0/ml-features.html](https://spark.apache.org/docs/2.1.0/ml-features.html)找到支持的转换器列表。
- en: 'To use a transformer, first, you must initiate it with function parameters,
    such as `inputCol` and `outputCol`, then call the `fit()` function on the DataFrame
    that contains the data, and finally, call the `transform()` function to transfer
    the features in the DataFrame:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要使用转换器，首先必须使用函数参数（如`inputCol`和`outputCol`）初始化它，然后在包含数据的DataFrame上调用`fit()`函数，最后调用`transform()`函数来转换DataFrame中的特征：
- en: '[PRE6]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Model training**: Spark ML supports a wide range of ML algorithms for classification,
    regression, clustering, recommendation, and topic modeling. You can find a list
    of Spark ML algorithms at [https://spark.apache.org/docs/1.4.1/mllib-guide.html](https://spark.apache.org/docs/1.4.1/mllib-guide.html).
    The following code sample shows how you can train a logistic regression model:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**：Spark ML支持广泛的机器学习算法，包括分类、回归、聚类、推荐和主题建模。您可以在[https://spark.apache.org/docs/1.4.1/mllib-guide.html](https://spark.apache.org/docs/1.4.1/mllib-guide.html)找到Spark
    ML算法的列表。以下代码示例展示了如何训练一个逻辑回归模型：'
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Model evaluation**: For model selection and evaluation, Spark ML provides
    utilities for cross-validation, hyperparameter tuning, and model evaluation metrics.
    You can find the list of evaluators at [https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html).
    The following code block illustrates using the `BinaryClassificationEvaluator`
    to evaluate a model with the `areaUnderPR` metric:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型评估**：对于模型选择和评估，Spark ML提供了交叉验证、超参数调整和模型评估指标的工具。您可以在[https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html)找到评估器的列表。以下代码块展示了如何使用`BinaryClassificationEvaluator`和`areaUnderPR`指标来评估模型：'
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Pipeline**: Spark ML also has support for the pipeline concept, similar to
    that of scikit-learn. With the pipeline concept, you can sequence a series of
    transformation and model training steps as a unified repeatable step:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：Spark ML也支持管道概念，类似于scikit-learn。使用管道概念，你可以将一系列转换和模型训练步骤作为一个统一的可重复步骤进行排序：'
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Model saving**: The Spark ML pipeline can be serialized into a serialization
    format called an Mleap bundle, which is an external library from Spark. A serialized
    Mleap bundle can be deserialized back into Spark for batch scoring or an Mleap
    runtime to run real-time APIs. You can find more details about Mleap at [https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/).
    The following code shows the syntax for serializing a Spark model into the Mleap
    format:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型保存**：Spark ML管道可以被序列化为一种称为Mleap bundle的序列化格式，这是Spark的一个外部库。序列化的Mleap bundle可以被反序列化回Spark进行批量评分或Mleap运行时来运行实时API。您可以在[https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/)找到有关Mleap的更多详细信息。以下代码展示了将Spark模型序列化为Mleap格式的语法：'
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Spark is a versatile framework that enables large-scale data processing and
    ML. While it excels in traditional ML tasks, it also offers limited support for
    neural network training, including the multilayer perceptron algorithm. However,
    for more comprehensive deep learning capabilities, we will explore dedicated ML
    libraries including TensorFlow and PyTorch in the upcoming sections.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个多功能的框架，它使大规模数据处理和机器学习成为可能。虽然它在传统的机器学习任务中表现出色，但它也提供了对神经网络训练的有限支持，包括多层感知器算法。然而，为了更全面的深度学习功能，我们将在接下来的章节中探讨专门的机器学习库，包括TensorFlow和PyTorch。
- en: Understanding the TensorFlow deep learning library
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TensorFlow深度学习库
- en: Initially released in 2015, TensorFlow is a popular open-source ML library,
    primarily backed up by Google, which is mainly designed for deep learning. TensorFlow
    has been used by companies of all sizes for training and building state-of-the-art
    deep learning models for a range of use cases, including computer vision, speech
    recognition, question-answering, text summarization, forecasting, and robotics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow最初于2015年发布，是一个流行的开源机器学习库，主要由谷歌支持，主要用于深度学习。TensorFlow已被各种规模的公司用于训练和构建用于各种用例的最先进的深度学习模型，包括计算机视觉、语音识别、问答、文本摘要、预测和机器人技术。
- en: 'TensorFlow works based on the concept of the computational graph, where data
    flows through nodes that represent mathematical operations. The core idea is to
    construct a graph of operations and tensors, with tensors being *n*-dimensional
    arrays that carry data. An example of a tensor could be a scalar value (for example,
    `1.0`), a one-dimensional vector (for example, `[1.0, 2.0, 3.0]`), a two-dimensional
    matrix (for example, `[[1.0, 2.0, 3.0]`, `[4.0, 5.0, 6.0]]`), or even higher dimensional
    matrices. Operations are performed on these tensors, allowing for mathematical
    computations like addition or matrix multiplication. The following diagram shows
    a sample computational graph for performing a sequence of mathematical operations
    on tensors:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow基于计算图的概念工作，其中数据通过表示数学运算的节点流动。核心思想是构建一个操作和张量的图，其中张量是*n*-维数组，携带数据。张量的一个例子可以是标量值（例如，`1.0`），一维向量（例如，`[1.0,
    2.0, 3.0]`），二维矩阵（例如，`[[1.0, 2.0, 3.0]`, `[4.0, 5.0, 6.0]]`），甚至是更高维度的矩阵。在这些张量上执行操作，允许进行数学计算，如加法或矩阵乘法。以下图显示了执行一系列数学操作的张量示例计算图：
- en: '![Figure 5.4 – Data flow diagram ](img/B20836_05_04.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 数据流图](img/B20836_05_04.png)'
- en: 'Figure 5.4: Data flow diagram'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：数据流图
- en: 'In the preceding computational diagram, the rectangular nodes are mathematical
    operations, while the circles represent tensors. This particular diagram shows
    a computational graph for performing an artificial neuron tensor operation, which
    is to perform a matrix multiplication of *W* and *X*, followed by the addition
    of *b*, and, lastly, apply a *ReLU* action function. The equivalent mathematical
    formula is as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的计算图中，矩形节点是数学运算，而圆圈代表张量。这个特定的图表展示了执行人工神经元张量运算的计算图，即执行 *W* 和 *X* 的矩阵乘法，然后加上
    *b*，最后应用 *ReLU* 动作函数。等效的数学公式如下：
- en: '![](img/B20836_05_001.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20836_05_001.png)'
- en: TensorFlow allows users to define and manipulate the computation graph using
    its high-level API or by directly working with its lower-level components. This
    flexibility allows researchers and developers to create complex models and algorithms.
    Furthermore, TensorFlow supports distributed computing, allowing the graph to
    be executed across multiple devices or machines, which is crucial for handling
    large-scale ML tasks. This distributed architecture enables TensorFlow to leverage
    the power of clusters or GPUs to accelerate the training and inference of deep
    learning models.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 允许用户通过其高级 API 或直接与底层组件交互来定义和操作计算图。这种灵活性使得研究人员和开发者能够创建复杂的模型和算法。此外，TensorFlow
    支持分布式计算，允许图在多个设备或机器上执行，这对于处理大规模机器学习任务至关重要。这种分布式架构使得 TensorFlow 能够利用集群或 GPU 的力量来加速深度学习模型的训练和推理。
- en: Installing TensorFlow
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 TensorFlow
- en: TensorFlow can be installed using the `pip install --upgrade tensorflow` command
    in a Python-based environment. After installation, TensorFlow can be used just
    like any other Python library package.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用基于 Python 的环境中的 `pip install --upgrade tensorflow` 命令来安装 TensorFlow。安装后，TensorFlow
    可以像任何其他 Python 库包一样使用。
- en: Core components of TensorFlow
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 的核心组件
- en: 'The TensorFlow library provides a rich set of features for different ML steps,
    from data preparation to model serving. The following diagram illustrates the
    core building blocks of the TensorFlow library:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 库为不同的机器学习步骤提供了丰富的功能，从数据准备到模型部署。以下图表展示了 TensorFlow 库的核心构建模块：
- en: '![Figure 5.5 – TensorFlow components ](img/B20836_05_05.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – TensorFlow 组件](img/B20836_05_05.png)'
- en: 'Figure 5.5: TensorFlow components'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：TensorFlow 组件
- en: 'Training an ML model using TensorFlow 2.x involves the following main steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.x 训练机器学习模型涉及以下主要步骤：
- en: '**Preparing the dataset**: TensorFlow 2.x provides a `tf.data` library for
    efficiently loading data from sources (such as files), transforming data (such
    as changing the values of the dataset), and setting up the dataset for training
    (such as configuring batch size or data prefetching). These data classes provide
    efficient ways to pass data to the training algorithms for optimized model training.
    The TensorFlow **Keras** API also provides a list of built-in classes (MNIST,
    CIFAR, IMDB, MNIST Fashion, and Reuters Newswire) for building simple deep learning
    models. You can also feed a NumPy array or Python generator (a function that behaves
    like an iterator) to a model in TensorFlow for model training, but `tf.data` is
    the recommended approach.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备数据集**：TensorFlow 2.x 提供了一个 `tf.data` 库，用于高效地从源（如文件）加载数据，转换数据（如更改数据集的值），以及设置用于训练的数据集（如配置批量大小或数据预取）。这些数据类提供了将数据传递给训练算法以优化模型训练的高效方式。TensorFlow
    的 **Keras** API 还提供了一系列内置类（MNIST、CIFAR、IMDB、MNIST Fashion 和 Reuters Newswire），用于构建简单的深度学习模型。您还可以将
    NumPy 数组或 Python 生成器（一个类似迭代器的函数）馈送到 TensorFlow 中的模型进行模型训练，但 `tf.data` 是推荐的方法。'
- en: '**Defining the neural network**: TensorFlow 2.x provides multiple ways to use
    or build a neural network for model training. You can use the premade estimators
    (the `tf.estimator` class) such as `DNNRegressor` and `DNNClassifier` to train
    models. Or, you can create custom neural networks using the `tf.keras` class,
    which provides a list of primitives such as `tf.keras.layers` for constructing
    neural network layers and `tf.keras.activation` such as ReLU, **Sigmoid**, and
    **Softmax** for building neural networks. Softmax is usually used as the last
    output of a neural network for a multiclass problem. It takes a vector of real
    numbers (positive and negative) as input and normalizes the vector as a probability
    distribution to represent the probabilities of different class labels, such as
    the different types of hand-written digits. For binary classification problems,
    Sigmoid is normally used and it returns a value between 0 and 1.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义神经网络**：TensorFlow 2.x提供了多种使用或构建神经网络进行模型训练的方法。您可以使用预制的估计器（如`tf.estimator`类的`DNNRegressor`和`DNNClassifier`）来训练模型。或者，您可以使用`tf.keras`类创建自定义神经网络，该类提供了一系列原语，如`tf.keras.layers`用于构建神经网络层，以及`tf.keras.activation`如ReLU、**Sigmoid**和**Softmax**用于构建神经网络。Softmax通常用作多类问题的神经网络最后一个输出，它接受一个实数向量（正数和负数）作为输入，并将向量归一化为概率分布，以表示不同类别标签的概率，例如不同类型的手写数字。对于二元分类问题，通常使用Sigmoid，它返回介于0和1之间的值。'
- en: '**Defining the loss function**: TensorFlow 2.x provides a list of built-in
    loss functions such as **mean squared error** (**MSE**) and **mean absolute error**
    (**MAE**) for regression tasks and cross-entropy loss for classification tasks.
    You can find more details about MSE and MAE at [https://en.wikipedia.org/wiki/Mean_squared_error](https://en.wikipedia.org/wiki/Mean_squared_error)
    and [https://en.wikipedia.org/wiki/Mean_absolute_error](https://en.wikipedia.org/wiki/Mean_absolute_error).
    You can find a list of supported loss functions in the `tf.keras.losses` class.
    For more details about the different losses, refer to [https://keras.io/api/losses/](https://keras.io/api/losses/).
    There is also the flexibility to define custom loss functions if the built-in
    loss functions do not meet the needs.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义损失函数**：TensorFlow 2.x提供了内置的损失函数列表，例如用于回归任务的**均方误差**（MSE）和**平均绝对误差**（MAE），以及用于分类任务的交叉熵损失。您可以在[https://en.wikipedia.org/wiki/Mean_squared_error](https://en.wikipedia.org/wiki/Mean_squared_error)和[https://en.wikipedia.org/wiki/Mean_absolute_error](https://en.wikipedia.org/wiki/Mean_absolute_error)找到关于MSE和MAE的更多详细信息。您可以在`tf.keras.losses`类中找到支持的损失函数列表。有关不同损失函数的更多详细信息，请参阅[https://keras.io/api/losses/](https://keras.io/api/losses/)。如果内置的损失函数不能满足需求，还可以定义自定义损失函数。'
- en: '**Selecting the optimizer**: TensorFlow 2.x provides a list of built-in optimizers
    for model training, such as the **Adam** optimizer and the **stochastic gradient
    descent** (**SGD**) optimizer for parameters optimization, with its `tf.keras.optimizers`
    class. You can find more details about the different supported optimizers at [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/).
    Adam and SGD are two of the most commonly used optimizers.'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择优化器**：TensorFlow 2.x的`tf.keras.optimizers`类提供了一系列内置的优化器，例如用于参数优化的**Adam**优化器和**随机梯度下降**（SGD）优化器。您可以在[https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)找到关于不同支持的优化器的更多详细信息。Adam和SGD是最常用的优化器之一。'
- en: '**Selecting the evaluation metrics**: TensorFlow 2.x has a list of built-in
    model evaluation metrics (for example, accuracy and cross-entropy) for model training
    evaluations with its `tf.keras.metrics` class. You can also define custom metrics
    for model evaluation during training.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择评估指标**：TensorFlow 2.x的`tf.keras.metrics`类提供了一系列内置的模型评估指标（例如准确率和交叉熵），用于模型训练评估。您还可以在训练期间为模型评估定义自定义指标。'
- en: '**Compiling the network into a model**: This step compiles the defined network,
    along with the defined loss function, optimizer, and evaluation metrics, into
    a computational graph that’s ready for model training.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将网络编译成模型**：此步骤将定义的网络、定义的损失函数、优化器和评估指标编译成一个准备进行模型训练的计算图。'
- en: '**Fitting the model**: This step kicks off the model training process by feeding
    the data to the computational graph through batches and multiple epochs to optimize
    the model parameters.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型拟合**：此步骤通过将数据通过批次和多个epoch传递到计算图中来启动模型训练过程，以优化模型参数。'
- en: '**Evaluating the trained model**: Once the model has been trained, you can
    evaluate the model using the `evaluate()` function against the test data.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估训练好的模型**：一旦模型训练完成，您可以使用`evaluate()`函数对测试数据进行模型评估。'
- en: '**Saving the model**: The model can be saved in the TensorFlow **SavedModel**
    serialization format or **Hierarchical Data Format** (**HDF5**) format.'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保存模型**：模型可以保存为TensorFlow的**SavedModel**序列化格式或**层次数据格式**（**HDF5**）格式。'
- en: '**Model serving**: TensorFlow comes with a model serving framework called TensorFlow
    Serving, which we will cover in greater detail in *Chapter 7*, *Open-Source ML
    Platform*.'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型服务**：TensorFlow附带了一个名为TensorFlow Serving的模型服务框架，我们将在*第7章*，*开源机器学习平台*中更详细地介绍。'
- en: The TensorFlow library is designed for large-scale production-grade data processing
    and model training. As such, it provides capabilities for large-scale distributed
    data processing and model training on a cluster of servers against a large dataset.
    We will cover large-scale distributed data processing and model training in greater
    detail in *Chapter 10*, *Advanced ML Engineering*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow库旨在用于大规模生产级数据处理和模型训练。因此，它提供了在服务器集群上对大型数据集进行大规模分布式数据处理和模型训练的能力。我们将在*第10章*，*高级机器学习工程*中更详细地介绍大规模分布式数据处理和模型训练。
- en: To support the complete process of building and deploying ML pipelines, TensorFlow
    provides **TensorFlow Extended** (**TFX**). TFX integrates multiple components
    and libraries from the TensorFlow ecosystem, creating a cohesive platform for
    tasks such as data ingestion, data validation, preprocessing, model training,
    model evaluation, and model deployment. Its architecture is designed to be modular
    and scalable, enabling users to tailor and expand the pipeline to meet their specific
    requirements. You can get more details about TFX at [https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持构建和部署机器学习管道的完整流程，TensorFlow提供了**TensorFlow Extended**（**TFX**）。TFX集成了TensorFlow生态系统中的多个组件和库，创建了一个用于数据摄取、数据验证、预处理、模型训练、模型评估和模型部署等任务的统一平台。其架构设计为模块化和可扩展，使用户能够根据其特定需求定制和扩展管道。您可以在[https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx)上获取更多关于TFX的详细信息。
- en: TensorFlow offers an expanded ecosystem of libraries and extensions for solving
    a wide range of advanced ML problems, including federated learning (training a
    model using decentralized data), model optimization (optimizing a model for deployment
    and execution), and probabilistic reasoning (reasoning under uncertainty using
    probability theory). It also provides support for mobile and edge devices with
    this TensorFlow Lite component, and support for browsers through the TensorFlow.js
    library.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow提供了一系列库和扩展，用于解决各种高级机器学习问题，包括联邦学习（使用去中心化数据进行模型训练）、模型优化（优化模型以部署和执行）和概率推理（使用概率论在不确定性下进行推理）。它还通过TensorFlow
    Lite组件为移动和边缘设备提供支持，并通过TensorFlow.js库为浏览器提供支持。
- en: Hands-on exercise – training a TensorFlow model
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动手练习 - 训练TensorFlow模型
- en: With deep learning dominating the recent ML advancement, it is important to
    have some hands-on experience with deep learning frameworks. In this exercise,
    you will learn how to install the TensorFlow library in your local Jupyter environment
    and build and train a simple neural network model. Launch a Jupyter notebook that
    you have previously installed on your machine. If you don’t remember how to do
    this, revisit the *Hands-on lab* section of *Chapter 3*, *Exploring ML Algorithms*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习在最近机器学习进步中的主导地位，拥有一些深度学习框架的实践经验非常重要。在这个练习中，您将学习如何在您的本地Jupyter环境中安装TensorFlow库，并构建和训练一个简单的神经网络模型。启动您之前安装在机器上的Jupyter笔记本。如果您不记得如何做，请回顾*第3章*，*探索机器学习算法*中的*动手实验室*部分。
- en: 'Once the Jupyter notebook is running, create a new folder by selecting the
    **New** dropdown and then **Folder**. Rename the folder `TensorFlowLab`. Open
    the `TensorFlowLab` folder, create a new notebook inside this folder, and rename
    the notebook `Tensorflow-lab1.ipynb`. Now, let’s get started:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Jupyter笔记本运行起来，通过选择**新建**下拉菜单然后**文件夹**来创建一个新的文件夹。将文件夹重命名为`TensorFlowLab`。打开`TensorFlowLab`文件夹，在此文件夹内创建一个新的笔记本，并将其重命名为`Tensorflow-lab1.ipynb`。现在，让我们开始吧：
- en: 'Inside the first cell, run the following code to install TensorFlow. As mentioned
    in *Chapter 3*, `pip` is the Python package installation utility:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个单元格中，运行以下代码来安装TensorFlow。如*第3章*所述，`pip`是Python包安装工具：
- en: '[PRE11]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we must import the library and load the sample training data. We will
    use the built-in `fashion_mnist` dataset that comes with the `keras` library to
    do so. Next, we must load the data into a `tf.data.Dataset` class and then call
    its `batch()` function to set up a batch size. Run the following code block in
    a new cell to load the data and configure the dataset:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须导入库并加载样本训练数据。我们将使用`keras`库内置的`fashion_mnist`数据集来完成此操作。接下来，我们必须将数据加载到`tf.data.Dataset`类中，然后调用其`batch()`函数来设置批大小。在新的单元格中运行以下代码块以加载数据并配置数据集：
- en: '[PRE12]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s see what the data looks like. Run the following code block in a new cell
    to view the sample data. **Matplotlib** is a Python visualization library, used
    to display an image:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看数据是什么样的。在新的单元格中运行以下代码块以查看样本数据。**Matplotlib**是一个Python可视化库，用于显示图像：
- en: '[PRE13]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we build a simple **Multilayer Perception** (**MLP**) network with two
    hidden layers (one with `100` nodes and one with `50` nodes) and an output layer
    with `10` nodes (each node represents a class label). Then, we must compile the
    network using the **Adam** optimizer, use the cross-entropy loss as the optimization
    objective, and use accuracy as the measuring metric.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建一个简单的**多层感知器**（**MLP**）网络，包含两个隐藏层（一个包含`100`个节点，另一个包含`50`个节点）和一个包含`10`个节点的输出层（每个节点代表一个类别标签）。然后，我们必须使用**Adam**优化器编译网络，使用交叉熵损失作为优化目标，并使用准确度作为衡量指标。
- en: The Adam optimizer is a variation of **gradient descent** (**GD**), and it improves
    upon GD mainly in the area of the adaptive learning rate for updating the parameters
    to improve model convergence, whereas GD uses a constant learning rate for parameter
    updating. Cross-entropy measures the performance of a classification model, where
    the output is the probability distribution for the different classes adding up
    to 1\. The cross-entropy error increases when the predicted distribution diverges
    from the actual class label.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Adam优化器是**梯度下降**（**GD**）的一种变体，它主要在自适应学习率更新参数以改善模型收敛方面改进GD，而GD使用恒定的学习率进行参数更新。交叉熵衡量分类模型的性能，其中输出是不同类别的概率分布，总和为1。当预测分布与实际类别标签偏离时，交叉熵误差会增加。
- en: 'To kick off the training process, we must call the `fit()` function, which
    is a required step in this case. We will run the training for `10` epochs. One
    epoch is one pass of the entire training dataset. Note that we are running 10
    epochs here for illustration purposes only. The actual number will be based on
    the specific training job and the desired model performance:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要启动训练过程，我们必须调用`fit()`函数，这是此情况下的一个必要步骤。我们将运行10个epoch的训练。一个epoch是整个训练数据集的一次遍历。请注意，这里运行10个epoch只是为了说明目的。实际的数量将基于具体的训练作业和期望的模型性能：
- en: '[PRE14]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When the model is training, you should see a loss metric and accuracy metric
    being reported for each epoch to help understand the progress of the training
    job.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当模型正在训练时，你应该会看到每个epoch都会报告损失指标和准确度指标，以帮助理解训练作业的进度。
- en: 'Now that the model has been trained, we need to validate its performance using
    the test dataset. In the following code, we are creating a `test_ds` for the test
    data:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已经训练完成，我们需要使用测试数据集来验证其性能。在下面的代码中，我们正在为测试数据创建一个`test_ds`：
- en: '[PRE15]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can also use the standalone `keras.metrics` to evaluate the model. Here,
    we are getting the prediction results and using `tf.keras.metrics.Accuracy` to
    calculate the accuracy of predictions against the true values in `test[1]`:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以使用独立的`keras.metrics`来评估模型。在这里，我们正在获取预测结果，并使用`tf.keras.metrics.Accuracy`来计算与`test[1]`中真实值相比的预测准确度：
- en: '[PRE16]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You may notice the accuracy metrics in the previous step and this step are slightly
    different. That’s because the dataset samples used for evaluation are not exactly
    the same.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能会注意到前一步和这一步的准确度指标略有不同。这是因为用于评估的数据集样本并不完全相同。
- en: 'To save the model, run the following code in a new cell. It will save the model
    in the `SavedModel` serialization format:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要保存模型，请在新的单元格中运行以下代码。它将以`SavedModel`序列化格式保存模型：
- en: '[PRE17]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Open the `model` directory. You should see that several files have been generated,
    such as `saved_model.pb`, and several files under the `variables` subdirectory.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`model`目录。你应该会看到生成了几个文件，例如`saved_model.pb`，以及`variables`子目录下的几个文件。
- en: Great job! You have successfully installed the TensorFlow package in your local
    Jupyter environment and completed the training of a deep learning model. Through
    this process, you now possess the basic knowledge about TensorFlow and its capabilities
    for training deep learning models. Let’s shift our focus to PyTorch, another widely
    used and highly regarded deep learning library that excels in both experimental
    and production-grade ML model training.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！您已成功在本地Jupyter环境中安装了TensorFlow包并完成了深度学习模型的训练。通过这个过程，您现在已经掌握了TensorFlow及其在训练深度学习模型方面的能力。让我们将注意力转向PyTorch，这是另一个广泛使用且备受推崇的深度学习库，它在实验和生产级机器学习模型训练方面都表现出色。
- en: Understanding the PyTorch deep learning library
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解PyTorch深度学习库
- en: PyTorch is an open-source ML library that was designed for deep learning using
    GPUs and CPUs. Initially released in 2016, it is a highly popular ML framework
    with a large following and many adoptions. Many technology companies, including
    tech giants such as **Facebook**, **Microsoft**, and **Airbnb**, all use PyTorch
    heavily for a wide range of deep learning use cases, such as computer vision and
    **natural language processing** (**NLP**).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个开源的机器学习库，旨在使用GPU和CPU进行深度学习。最初于2016年发布，它是一个高度流行的机器学习框架，拥有庞大的追随者和众多采用者。许多科技公司，包括像**Facebook**、**Microsoft**和**Airbnb**这样的科技巨头，都在广泛的深度学习用例中大量使用PyTorch，例如计算机视觉和**自然语言处理**（**NLP**）。
- en: PyTorch strikes a good balance of performance (using a C++ backend) with ease
    of use with default support for dynamic computational graphs and interoperability
    with the rest of the Python ecosystem. For example, with PyTorch, you can easily
    convert between NumPy arrays and PyTorch tensors. To allow for easy backward propagation,
    PyTorch has built-in support for automatically computing gradients, a vital requirement
    for gradient-based model optimization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在性能（使用C++后端）和易用性之间取得了良好的平衡，默认支持动态计算图和与Python生态系统的互操作性。例如，使用PyTorch，您可以轻松地在NumPy数组和PyTorch张量之间进行转换。为了便于反向传播，PyTorch内置了对自动计算梯度的支持，这对于基于梯度的模型优化是必不可少的。
- en: The PyTorch library consists of several key modules, including tensors, **autograd**,
    **optimizer**, and **neural network**. Tensors are used to store and operate multidimensional
    arrays of numbers. You can perform various operations on tensors such as matrix
    multiplication, transpose, returning the max number, and dimensionality manipulation.
    PyTorch supports automatic gradient calculation with its Autograd module. When
    performing a forward pass, the Autograd module simultaneously builds up a function
    that computes the gradient. The Optimizer module provides various algorithms such
    as SGD and Adam for updating model parameters. The Neural Network module provides
    modules that represent different layers of a neural network such as the linear
    layer, embedding layer, and dropout layer. It also provides a list of loss functions
    that are commonly used for training deep learning models.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch库由几个关键模块组成，包括张量、**autograd**、**optimizer**和**neural network**。张量用于存储和操作多维数组。您可以对张量执行各种操作，如矩阵乘法、转置、返回最大值和维度操作。PyTorch通过其Autograd模块支持自动梯度计算。在执行正向传播时，Autograd模块同时构建一个计算梯度的函数。Optimizer模块提供了SGD和Adam等算法，用于更新模型参数。神经网络模块提供了表示神经网络不同层的模块，如线性层、嵌入层和dropout层。它还提供了一组常用的损失函数列表，这些函数常用于训练深度学习模型。
- en: Installing PyTorch
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装PyTorch
- en: PyTorch can run on different operating systems, including Linux, Mac, and Windows.
    You can follow the instructions at [https://pytorch.org/](https://pytorch.org/)
    to install it in your environment. For example, you can use the `pip install torch`
    command to install it in a Python-based environment.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch可以在不同的操作系统上运行，包括Linux、Mac和Windows。您可以通过访问[https://pytorch.org/](https://pytorch.org/)上的说明来在您的环境中安装它。例如，您可以使用`pip
    install torch`命令在基于Python的环境中安装它。
- en: Core components of PyTorch
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch的核心组件
- en: 'Similar to TensorFlow, PyTorch also supports the end-to-end ML workflow, from
    data preparation to model serving. The following diagram shows what different
    PyTorch modules are used to train and serve a PyTorch model:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与TensorFlow类似，PyTorch也支持端到端的机器学习工作流程，从数据准备到模型部署。以下图表显示了不同的PyTorch模块用于训练和部署PyTorch模型：
- en: '![Figure 5.6 – PyTorch modules for model training and serving ](img/B20836_05_06.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 用于模型训练和部署的PyTorch模块](img/B20836_05_06.png)'
- en: 'Figure 5.6: PyTorch modules for model training and serving'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：PyTorch用于模型训练和服务的模块
- en: 'The steps involved in training a deep learning model are very similar to that
    of TensorFlow model training. We’ll look at the PyTorch-specific details in the
    following steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型涉及的步骤与TensorFlow模型训练的步骤非常相似。我们将在以下步骤中查看PyTorch特有的细节：
- en: '**Preparing the dataset**: PyTorch provides two primitives for dataset and
    data loading management: `torch.utils.data.Dataset` and `torch.utils.data.Dataloader`.
    `Dataset` stores data samples and their corresponding labels, while `Dataloader`
    wraps around the dataset and provides easy and efficient access to the data for
    model training. `Dataloader` provides functions such as `shuffle`, `batch_size`,
    and `prefetch_factor` to control how the data is loaded and fed to the training
    algorithm. Additionally, as the data in the dataset might need to be transformed
    before training is performed, `Dataset` allows you to use a user-defined function
    to transform the data.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备数据集**：PyTorch提供了两个用于数据集和数据加载管理的原语：`torch.utils.data.Dataset`和`torch.utils.data.Dataloader`。`Dataset`存储数据样本及其对应的标签，而`Dataloader`包装数据集，为模型训练提供方便和高效的数据访问。`Dataloader`提供了如`shuffle`、`batch_size`和`prefetch_factor`等函数来控制数据的加载和提供给训练算法的方式。此外，由于数据集中的数据在训练之前可能需要转换，`Dataset`允许您使用用户定义的函数来转换数据。'
- en: '**Defining the neural network**: PyTorch provides a high-level abstraction
    for building neural networks with its `torch.nn` class, which provides built-in
    support for different neural network layers such as linear layers and convolutional
    layers, as well as activation layers such as Sigmoid and ReLU. It also has container
    classes such as `nn.Sequential` for packaging different layers into a complete
    network. Existing neural networks can also be loaded into PyTorch for training.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义神经网络**：PyTorch通过其`torch.nn`类提供了构建神经网络的抽象层，该类为不同的神经网络层（如线性层和卷积层）以及激活层（如Sigmoid和ReLU）提供了内置支持。它还包含容器类，如`nn.Sequential`，用于将不同的层打包成一个完整的网络。现有的神经网络也可以加载到PyTorch中进行训练。'
- en: '**Defining the loss function**: PyTorch provides several built-in loss functions
    in its `torch.nn` class, such as `nn.MSELoss` and `nn.CrossEntropyLoss`.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义损失函数**：PyTorch在其`torch.nn`类中提供了多个内置的损失函数，例如`nn.MSELoss`和`nn.CrossEntropyLoss`。'
- en: '**Selecting the optimizer**: PyTorch provides several optimizers with its `nn.optim`
    classes. Examples of optimizers include `optim.SGD`, `optim.Adam`, and `optim.RMSProp`.
    All the optimizers have a `step()` function that updates model parameters with
    each forward pass. There’s also a backward pass that calculates the gradients.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择优化器**：PyTorch通过其`nn.optim`类提供了多个优化器。优化器的例子包括`optim.SGD`、`optim.Adam`和`optim.RMSProp`。所有优化器都有一个`step()`函数，用于在每个前向传递中更新模型参数。还有一个反向传递，用于计算梯度。'
- en: '**Selecting the evaluation metrics**: The PyTorch `ignite.metrics` class provides
    several evaluation metrics such as precision, recall, and `RootMeanSquaredError`
    for evaluating model performances. You can learn more about precision and recall
    at [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall).
    You can also use the scikit-learn metrics libraries to help evaluate models.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择评估指标**：PyTorch的`ignite.metrics`类提供了多个评估指标，例如精确度、召回率和`RootMeanSquaredError`，用于评估模型性能。您可以在[https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)上了解更多关于精确度和召回率的信息。您还可以使用scikit-learn指标库来帮助评估模型。'
- en: '**Training the model**: Training a model in PyTorch involves three main steps
    in each training loop: forward pass the training data, backward pass the training
    data to calculate the gradient, and perform the optimizer step to update the gradient.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：在PyTorch中训练模型涉及每个训练循环中的三个主要步骤：前向传递训练数据，反向传递训练数据以计算梯度，并执行优化器步骤以更新梯度。'
- en: '**Saving/loading the model**: The `torch.save()` function saves a model in
    a serialized `pickle` format. The `torch.load()` function loads a serialized model
    into memory for inference. A common convention is to save the files with the `.pth`
    or `.pt` extension. You can also save multiple models into a single file.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**保存/加载模型**：`torch.save()`函数以序列化的`pickle`格式保存模型。`torch.load()`函数将序列化的模型加载到内存中进行推理。常见的约定是使用`.pth`或`.pt`扩展名保存文件。您还可以将多个模型保存到单个文件中。'
- en: '**Model serving**: PyTorch comes with a model serving library called TorchServe,
    which we will cover in more detail in *Chapter 7*, *Open-Source ML Platforms*.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型服务**：PyTorch附带一个名为TorchServe的模型服务库，我们将在第7章“开源机器学习平台”中更详细地介绍。'
- en: The PyTorch library supports large-scale distributed data processing and model
    training, which we will cover in more detail in *Chapter 10*, *Advanced ML Engineering*.
    Like TensorFlow, PyTorch also offers an ecosystem of library packages for a wide
    range of ML problems, including ML privacy, adversarial robustness, video understanding,
    and drug discovery.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch库支持大规模分布式数据处理和模型训练，我们将在第10章“高级机器学习工程”中更详细地介绍。像TensorFlow一样，PyTorch也提供了一套广泛的库包，用于解决各种机器学习问题，包括机器学习隐私、对抗鲁棒性、视频理解和药物发现。
- en: Now that you have learned about the fundamentals of PyTorch, let’s get hands-on
    through a simple exercise.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了PyTorch的基础知识，让我们通过一个简单的练习来动手实践。
- en: Hands-on exercise – building and training a PyTorch model
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动手练习 - 构建和训练PyTorch模型
- en: 'In this hands-on exercise, you will learn how to install the PyTorch library
    on your local machine and train a simple deep learning model using PyTorch. Launch
    a Jupyter notebook that you have previously installed on your machine. If you
    don’t remember how to do this, visit the *Hands-on lab* section of *Chapter 3*,
    *Exploring ML Algorithms*. Now, let’s get started:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手练习中，你将学习如何在你的本地机器上安装PyTorch库，并使用PyTorch训练一个简单的深度学习模型。启动你之前安装在本机的Jupyter笔记本。如果你不记得如何做，请访问第3章“探索机器学习算法”中的*动手实验室*部分。现在，让我们开始吧：
- en: 'Create a new folder called `pytorch-lab` in your Jupyter Notebook environment
    and create a new notebook file called `pytorch-lab1.ipynb`. Run the following
    command in a cell to install PyTorch and the `torchvision` package. `torchvision`
    contains a set of computer vision models and datasets. We will use the pre-built
    MNIST dataset in the `torchvision` package for this exercise:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的Jupyter笔记本环境中创建一个名为`pytorch-lab`的新文件夹，并创建一个名为`pytorch-lab1.ipynb`的新笔记本文件。在一个单元中运行以下命令来安装PyTorch和`torchvision`包。`torchvision`包含一系列计算机视觉模型和数据集。我们将使用`torchvision`包中的预构建MNIST数据集来完成这个练习：
- en: '[PRE18]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The following sample code shows the previously mentioned main components. Be
    sure to run each code block in a separate Jupyter notebook cell for optimal readability.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下示例代码显示了之前提到的主组件。请确保在每个单独的Jupyter笔记本单元中运行每个代码块，以获得最佳的可读性。
- en: 'First, we must import the necessary library packages and load the MNIST dataset
    from the `torchvision` dataset class:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们必须导入必要的库包，并从`torchvision`数据集类中加载MNIST数据集：
- en: '[PRE19]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we must construct an MLP neural network for classification. This MLP
    network has two hidden layers with ReLU activation for the first and second layers.
    The MLP model takes an input size of `784`, which is the flattened dimension of
    a 28x28 image. The first hidden layer has `128` nodes (neurons), while the second
    layer has `64` nodes (neurons). The final layer has `10` nodes because we have
    10 class labels:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须构建一个用于分类的多层感知器（MLP）神经网络。这个MLP网络有两个隐藏层，第一层和第二层使用ReLU激活函数。MLP模型接受`784`大小的输入，这是28x28图像的展平维度。第一隐藏层有`128`个节点（神经元），而第二层有`64`个节点（神经元）。最终层有`10`个节点，因为我们有10个类别标签：
- en: '[PRE20]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here’s a sample of the image data:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是图像数据的示例：
- en: '[PRE21]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we must define a **cross-entropy loss function** for the training process
    since we want to measure the error in the probability distribution for all the
    labels. Internally, PyTorch’s `CrossEntropyLoss` automatically applies a `softmax`
    to the network output to calculate the probability distributions for the different
    classes. For the optimizer, we have chosen the Adam optimizer with a learning
    rate of `0.003`. The `view()` function flattens the two-dimensional input array
    (28x28) into a one-dimensional vector since our neural network takes a one-dimensional
    vector input:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须为训练过程定义一个**交叉熵损失函数**，因为我们想测量所有标签的概率分布中的误差。在内部，PyTorch的`CrossEntropyLoss`自动将`softmax`应用于网络输出，以计算不同类别的概率分布。对于优化器，我们选择了学习率为`0.003`的Adam优化器。`view()`函数将二维输入数组（28x28）展平成一维向量，因为我们的神经网络接受一维向量输入：
- en: '[PRE22]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Learning rate is a hyperparameter that determines the size of the steps taken
    during the optimization process.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率是决定优化过程中步长大小的一个超参数。
- en: 'Now, let’s start the training process. We are going to run `15` epochs. Unlike
    the TensorFlow Keras API, where you just call a `fit()` function to start the
    training, PyTorch requires you to build a training loop and specifically run the
    forward pass (`model (images)`), run the backward pass to learn (`loss.backward()`),
    update the model weights (`optimizer.step()`), and then calculate the total loss
    and the average loss. For each training step, `trainloader` returns one batch
    (a batch size of `64`) of training data samples. Each training sample is flattened
    into a 784-long vector. The optimizer is reset with zeros for each training step:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练过程。我们将运行 `15` 个纪元。与 TensorFlow Keras API 不同，在那里你只需调用一个 `fit()` 函数来开始训练，PyTorch
    需要你构建一个训练循环，并特别运行正向传递（`model (images)`），运行反向传递以学习（`loss.backward()`），更新模型权重（`optimizer.step()`），然后计算总损失和平均损失。对于每个训练步骤，`trainloader`
    返回一个批次（批次大小为 `64`）的训练数据样本。每个训练样本被展平成一个 784 个元素的向量。优化器在每个训练步骤中用零重置：
- en: '[PRE23]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: When the training code runs, it should print out the average loss for each epoch.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当训练代码运行时，它应该打印出每个纪元的平均损失。
- en: 'To test the accuracy using the validation data, we must run the validation
    dataset through the trained model and use scikit-learn`.metrics.accuracy_score()`
    to calculate the model’s accuracy:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用验证数据测试准确度，我们必须将验证数据集通过训练模型运行，并使用 scikit-learn 的 `.metrics.accuracy_score()`
    函数来计算模型的准确度：
- en: '[PRE24]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, we must save the model to a file:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须将模型保存到文件中：
- en: '[PRE25]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Congratulations! You have successfully installed PyTorch in your local Jupyter
    environment and trained a deep learning PyTorch model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功在本地 Jupyter 环境中安装 PyTorch 并训练了一个深度学习 PyTorch 模型。
- en: How to choose between TensorFlow and PyTorch
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 TensorFlow 和 PyTorch 之间进行选择
- en: 'TensorFlow and PyTorch are the two most popular frameworks in the domain of
    deep learning. So, a pertinent question arises: How does one make an informed
    choice between the two? To help answer this question, let’s do a quick comparative
    analysis of these frameworks:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 PyTorch 是深度学习领域中最受欢迎的两个框架。因此，一个相关的问题随之而来：一个人如何在这两个框架之间做出明智的选择？为了帮助回答这个问题，让我们对这些框架进行快速的比较分析：
- en: '**Ease of use**: PyTorch is generally considered more user-friendly and Pythonic.
    The control flow feels closer to native Python and the dynamic computation graphs
    of PyTorch make it easier to debug and iterate compared to TensorFlow’s static
    graphs. However, eager execution support in TensorFlow 2.0 helps close this gap.
    PyTorch is also considered more object-oriented than TensorFlow.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：PyTorch 通常被认为更易于使用且更符合 Python 风格。其控制流程感觉更接近原生 Python，PyTorch 的动态计算图比
    TensorFlow 的静态图更容易调试和迭代。然而，TensorFlow 2.0 中的即时执行支持有助于缩小这一差距。PyTorch 也被认为比 TensorFlow
    更面向对象。'
- en: '**Community popularity**: Both frameworks enjoy robust community support and
    are highly popular. TensorFlow initially had a lead; however, PyTorch has caught
    up in popularity in recent years, according to the Google Trends report. PyTorch
    is more widely adopted within the research community and dominates the implementation
    of research papers.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社区受欢迎程度**：这两个框架都享有坚实的社区支持，并且非常受欢迎。TensorFlow 最初领先；然而，根据 Google Trends 报告，PyTorch
    在近年来在受欢迎程度上已经迎头赶上。PyTorch 在研究社区中更广泛地被采用，并在研究论文的实施中占据主导地位。'
- en: '**Model availability**: TensorFlow has the TensorFlow Model Garden, which hosts
    a collection of models utilizing TensorFlow APIs, covering various ML tasks such
    as computer vision, NLP, and recommendation. It also features TensorFlow Hub,
    offering a collection of pre-trained models ready for deployment or fine-tuning
    across a wide range of ML tasks. Similarly, PyTorch has PyTorch Hub, a library
    integrated into PyTorch that provides easy access to a broad array of pre-trained
    models for computer vision, NLP, and more.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可用性**：TensorFlow 有 TensorFlow Model Garden，它托管了一个使用 TensorFlow API 的模型集合，涵盖了各种机器学习任务，如计算机视觉、NLP
    和推荐。它还提供了 TensorFlow Hub，它提供了一系列预训练模型，可用于部署或微调各种机器学习任务。同样，PyTorch 有 PyTorch Hub，这是一个集成到
    PyTorch 中的库，提供了对广泛预训练模型的便捷访问，包括计算机视觉、NLP 等。'
- en: '**Deployment**: Both frameworks are suitable for the production deployment
    of ML models. TensorFlow is considered to have a more comprehensive model deployment
    stack with TensorFlow Serving, TensorFlow Lite for mobile and edge devices, and
    TensorFlow.js for browser deployment. TensorFlow Extended is an end-to-end model
    deployment platform encompassing model validation, monitoring, and explanation.
    PyTorch offers TorchServe, a model-serving framework for PyTorch models, and PyTorch
    Mobile for deploying models on iOS and Android devices. PyTorch relies more on
    third-party solutions for end-to-end integration in the deployment process.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署**：这两个框架都适合机器学习模型的实际部署。TensorFlow 被认为拥有更全面的模型部署栈，包括 TensorFlow Serving、TensorFlow
    Lite 用于移动和边缘设备，以及 TensorFlow.js 用于浏览器部署。TensorFlow Extended 是一个端到端模型部署平台，包括模型验证、监控和解释。PyTorch
    提供了用于 PyTorch 模型的模型服务框架 TorchServe，以及 PyTorch Mobile 用于在 iOS 和 Android 设备上部署模型。PyTorch
    在部署过程中更依赖于第三方解决方案来实现端到端集成。'
- en: To conclude, both frameworks provide comparable capabilities throughout the
    entire ML lifecycle, accommodating similar use cases. If your organization has
    already committed to either TensorFlow or PyTorch, it is advisable to proceed
    with that decision. However, for those embarking on the initial stages, PyTorch
    might offer a more accessible starting point owing to its ease of use.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这两个框架在整个机器学习生命周期中提供了可比的能力，适应了类似的使用案例。如果你的组织已经承诺使用 TensorFlow 或 PyTorch，建议继续执行该决定。然而，对于那些刚开始的人来说，PyTorch
    可能由于其易用性而提供一个更易于入门的起点。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have explored several popular open-source ML library packages,
    including scikit-learn, Spark ML, TensorFlow, and PyTorch. By now, you should
    have a good understanding of the fundamental components of these libraries and
    how they can be leveraged to train ML models. Additionally, we have delved into
    the TensorFlow and PyTorch frameworks to construct artificial neural networks,
    train deep learning models, and save these models to files. These model files
    can then be utilized in model-serving environments to make predictions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了几个流行的开源机器学习库包，包括 scikit-learn、Spark ML、TensorFlow 和 PyTorch。到目前为止，你应该对这些库的基本组件以及如何利用它们来训练机器学习模型有了很好的理解。此外，我们还深入探讨了
    TensorFlow 和 PyTorch 框架，用于构建人工神经网络、训练深度学习模型并将这些模型保存到文件中。这些模型文件随后可以在模型服务环境中用于预测。
- en: In the next chapter, we will delve into Kubernetes and its role as a foundational
    infrastructure for constructing open-source ML solutions.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨 Kubernetes 以及其在构建开源机器学习解决方案的基础设施中的作用。
- en: Join our community on Discord
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mlsah](https://packt.link/mlsah)'
- en: '![](img/QR_Code70205728346636561.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code70205728346636561.png)'
