- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Open-Source ML Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a wide range of **machine learning** (**ML**) and data science technologies
    available, encompassing both open-source and commercial products. Different organizations
    have adopted different approaches when it comes to building their ML platforms.
    Some have opted for in-house teams that leverage open-source technology stacks,
    allowing for greater flexibility and customization. Others have chosen commercial
    products to focus on addressing specific business and data challenges. Additionally,
    some organizations have adopted a hybrid architecture, combining open-source and
    commercial tools to harness the benefits of both. As a practitioner in ML solutions
    architecture, it is crucial to be knowledgeable about the available open-source
    ML technologies and their applications in building robust ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, our focus will be on exploring different open-source
    technologies for experimentation, model building, and the development of ML platforms.
    In this chapter specifically, we will delve into popular ML libraries including
    scikit-learn, Spark, TensorFlow, and PyTorch. We will examine the core capabilities
    of these libraries and demonstrate how they can be effectively utilized throughout
    the various stages of an ML project lifecycle, encompassing tasks such as data
    processing, model development, and model evaluation. Moreover, you will have the
    opportunity to engage in hands-on exercises, gaining practical experience with
    these ML libraries and their application in training models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will be covering the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Core features of open-source ML libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the scikit-learn ML library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Apache Spark ML library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the TensorFlow ML library and hands-on lab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the PyTorch ML library and hands-on lab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to choose between TensorFlow and PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will need access to your local machine where you installed
    the **Jupyter** environment from *Chapter 3*, *Exploring ML Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code samples used in this chapter at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: Core features of open-source ML libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML libraries are software libraries designed to facilitate the implementation
    of ML algorithms and techniques. While they share similarities with other software
    libraries, what sets them apart is their specialized support for various ML functionalities.
    These libraries typically offer a range of features through different sub-packages,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data manipulation and processing**: This includes support for different data
    tasks such as loading data of different formats, data manipulation, data analysis,
    data visualization, data transformation, and feature extraction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model building and training**: This includes support for built-in ML algorithms
    as well as capabilities for building custom algorithms for a wide range of ML
    tasks. Most ML libraries also have built-in support for the commonly used loss
    functions (such as mean squared error or cross-entropy) and a list of optimizers
    (such as gradient descent, Adam, etc.) to choose from. Some libraries also provide
    advanced support for distributed model training across multiple CPU/GPU devices
    or compute nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation and validation**: This includes packages for evaluating
    the performance of trained models, such as model accuracy, precision, recall,
    or error rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model saving and loading**: This includes support for saving the models to
    various formats for persistence and support for loading saved models into memory
    for predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model serving**: This includes model serving features to expose trained ML
    models behind an API, usually a RESTful API web service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretation**: This includes functionality for interpreting model predictions
    and feature importance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML libraries typically offer support for multiple programming languages, including
    popular options such as Python, Java, and Scala, catering to diverse user requirements.
    Python, in particular, has emerged as a prominent language in the field of ML,
    and many libraries provide extensive support for its interface. While the user-facing
    interface is often implemented in Python, the backend and underlying algorithms
    of these libraries are primarily written in compiled languages like C++ and Cython.
    This combination allows for efficient and optimized performance during model training
    and inference. In the following sections, we will delve into some widely used
    ML libraries to gain a deeper understanding of their features and capabilities,
    starting with scikit-learn, a widely used ML library for building ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the scikit-learn ML library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: scikit-learn ([https://scikit-learn.org/](https://scikit-learn.org/)) is an
    open-source ML library for Python. Initially released in 2007, it is one of the
    most popular ML libraries for solving many ML tasks, such as classification, regression,
    clustering, and dimensionality reduction. scikit-learn is widely used by companies
    in different industries and academics for solving real-world business cases such
    as churn prediction, customer segmentation, recommendations, and fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'scikit-learn is built mainly on top of three foundational libraries: **NumPy**,
    **SciPy**, and **Matplotlib**:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy is a Python-based library for managing large, multidimensional arrays
    and matrices, with additional mathematical functions to operate on the arrays
    and matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SciPy provides scientific computing functionality, such as optimization, linear
    algebra, and Fourier transform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib is used for plotting data for data visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all, scikit-learn is a sufficient and effective tool for a range of common
    data processing and model-building tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Installing scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can easily install the scikit-learn package on different operating systems
    such as macOS, Windows, and Linux. The scikit-learn library package is hosted
    on the **Python Package Index** site ([https://pypi.org/](https://pypi.org/))
    and the **Anaconda** package repository ([https://anaconda.org/anaconda/repo](https://anaconda.org/anaconda/repo)).
    To install it in your environment, you can use either the `pip` package manager
    or the **Conda** package manager. A package manager allows you to install and
    manage the installation of library packages in your operating system.
  prefs: []
  type: TYPE_NORMAL
- en: To install the `scikit-learn` library using the `pip` or Conda package manager,
    you can simply run `pip install -U scikit-learn` to install it from the PyPI index
    or run `conda install scikit-learn` if you want to use a Conda environment. You
    can learn more about `pip` at [https://pip.pypa.io/](https://pip.pypa.io/) and
    Conda at [http://docs.conda.io](http://docs.conda.io).
  prefs: []
  type: TYPE_NORMAL
- en: Core components of scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scikit-learn library provides a wide range of Python classes and functionalities
    for the various stages of the ML lifecycle. It consists of several main components,
    as depicted in the following diagram. By utilizing these components, you can construct
    ML pipelines and perform tasks such as classification, regression, and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – scikit-learn components ](img/B20836_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: scikit-learn components'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s delve deeper into how these components support the different stages
    of the ML lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing data**: For data manipulation and processing, the `pandas` library
    is commonly used. It provides core data loading and saving functions, as well
    as utilities for data manipulations such as data selection, data arrangement,
    and data statistical summaries. `pandas` is built on top of NumPy. The `pandas`
    library also comes with some visualization features such as pie charts, scatter
    plots, and box plots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn provides a list of transformers for data processing and transformation,
    such as imputing missing values, encoding categorical values, normalization, and
    feature extraction for text and images. You can find the full list of transformers
    at [https://scikit-learn.org/stable/data_transforms.html](https://scikit-learn.org/stable/data_transforms.html).
    Furthermore, you have the flexibility to create custom transformers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model training**: `scikit-learn` provides a long list of ML algorithms (also
    known as estimators) for classification and regression (for example, logistic
    regression, k-nearest neighbors, and random forest), as well as clustering (for
    example, k-means). You can find the full list of algorithms at [https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html).
    The following sample code shows the syntax for using the `RandomForestClassifier`
    algorithm to train a model using a labeled training dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Model evaluation**: scikit-learn has utilities for hyperparameter tuning
    and cross-validation, as well as `metrics` classes for model evaluations. You
    can find the full list of model selection and evaluation utilities at [https://scikit-learn.org/stable/model_selection.html](https://scikit-learn.org/stable/model_selection.html).
    The following sample code shows the `accuracy_score` class for evaluating the
    accuracy of classification models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Hyperparameter tuning** involves optimizing the configuration settings (hyperparameters)
    of an ML model to enhance its performance and achieve better results on a given
    task or dataset. Cross-validation is a statistical technique used to assess the
    performance and generalizability of an ML model by dividing the dataset into multiple
    subsets, training the model on different combinations, and evaluating its performance
    across each subset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model saving**: scikit-learn can save model artifacts using Python object
    serialization (`pickle` or `joblib`). The serialized `pickle` file can be loaded
    into memory for predictions. The following sample code shows the syntax for saving
    a model using the `joblib` class:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Pipeline**: scikit-learn also provides a pipeline utility for stringing together
    different transformers and estimators as a single processing pipeline, and it
    can be reused as a single unit. This is especially useful when you need to preprocess
    data for modeling training and model prediction, as both require the data to be
    processed in the same way:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As demonstrated, getting started with scikit-learn for experimenting with and
    constructing ML models is straightforward. scikit-learn is particularly suitable
    for typical regression, classification, and clustering tasks performed on a single
    machine. However, if you’re working with extensive datasets or require distributed
    training across multiple machines, scikit-learn may not be the optimal choice
    unless the algorithm supports incremental training, such as `SGDRegressor`. Therefore,
    moving on, let’s explore alternative ML libraries that excel in large-scale model
    training scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '**Incremental training** is an ML approach where a model is updated and refined
    continuously as new data becomes available, allowing the model to adapt to evolving
    patterns and improve its performance over time.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Apache Spark ML library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an advanced framework for distributed data processing, designed
    to handle large-scale data processing tasks. With its distributed computing capabilities,
    Spark enables applications to efficiently load and process data across a cluster
    of machines by leveraging in-memory computing, thereby significantly reducing
    processing times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecturally, a Spark cluster consists of a master node and worker nodes
    for running different Spark applications. Each application that runs in a Spark
    cluster has a driver program and its own set of processes, which are coordinated
    by the **SparkSession** object in the driver program. The `SparkSession` object
    in the driver program connects to a cluster manager (for example, Mesos, Yarn,
    Kubernetes, or Spark’s standalone cluster manager), which is responsible for allocating
    resources in the cluster for the Spark application. Specifically, the cluster
    manager acquires resources on worker nodes called **executors** to run computations
    and store data for the Spark application. Executors are configured with resources
    such as the number of CPU cores and memory to meet task processing needs. Once
    the executors have been allocated, the cluster manager sends the application code
    (Java JAR or Python files) to the executors. Finally, `SparkContext` sends the
    tasks to the executors to run. The following diagram shows how a driver program
    interacts with a cluster manager and executor to run a task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Running a Spark application on a Spark cluster ](img/B20836_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Running a Spark application on a Spark cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Each Spark application gets its own set of executors, which remain active for
    the duration of the application. The executors for different applications are
    isolated from each other, and they can only share data through external data storage.
  prefs: []
  type: TYPE_NORMAL
- en: The ML package for Spark is called MLlib, which runs on top of the distributed
    Spark architecture. It is capable of processing and training models with a large
    dataset that does not fit into the memory of a single machine. It provides APIs
    in different programming languages, including Python, Java, Scala, and R. From
    a structural perspective, it is very similar to that of the scikit-learn library
    in terms of core components and model development flow.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is highly popular and adopted by companies of all sizes across different
    industries. Large companies such as **Netflix**, **Uber**, and **Pinterest** use
    Spark for large-scale data processing and transformation, as well as running ML
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Spark ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark ML libraries are included as part of the Spark installation. PySpark is
    the Python API for Spark, and it can be installed like a regular Python package
    using `pip` (`pip install pyspark`). Note that PySpark requires Java and Python
    to be installed on the machine before it can be installed. You can find Spark’s
    installation instructions at [https://spark.apache.org/docs/latest/](https://spark.apache.org/docs/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: Core components of the Spark ML library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the scikit-learn library, Spark and Spark ML provide a full range
    of functionality for building ML models, from data preparation to model evaluation
    and model persistence. The following diagram shows the core components that are
    available in Spark for building ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Core components of Spark ML ](img/B20836_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Core components of Spark ML'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the core functions supported by the Spark and Spark
    ML library packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing data**: Spark supports Spark DataFrames, distributed data collections
    that can be used for tasks such as data joining, aggregation, filtering, and other
    data manipulation needs. Conceptually, a Spark DataFrame is equivalent to a table
    in a relational database. A Spark DataFrame can be distributed (that is, partitioned)
    across many machines, which allows fast data processing in parallel. Spark DataFrames
    also operate on a model called the lazy execution model. **Lazy execution** defines
    a set of transformations (for example, adding a column or filtering column) and
    the transformations are only executed when an action (such as calculating the
    min/max of a column) is needed. This allows an execution plan for the different
    transformations and actions to be generated to optimize the execution’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To start using the Spark functionality, you need to create a Spark session.
    A Spark session creates a `SparkContext` object, which is the entry point to the
    Spark functionality. The following sample code shows how you can create a Spark
    session:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A Spark DataFrame can be constructed from many different sources, such as structured
    data files (for example, CSV or JSON) and external databases. The following code
    sample reads a CSV file into a Spark DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are many transformers for data processing and transformation in Spark
    based on different data processing needs, such as `Tokenizer` (which breaks text
    down into individual words) and `StandardScalar` (which normalizes a feature into
    unit deviation and/or zero mean). You can find a list of supported transformers
    at [https://spark.apache.org/docs/2.1.0/ml-features.html](https://spark.apache.org/docs/2.1.0/ml-features.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To use a transformer, first, you must initiate it with function parameters,
    such as `inputCol` and `outputCol`, then call the `fit()` function on the DataFrame
    that contains the data, and finally, call the `transform()` function to transfer
    the features in the DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Model training**: Spark ML supports a wide range of ML algorithms for classification,
    regression, clustering, recommendation, and topic modeling. You can find a list
    of Spark ML algorithms at [https://spark.apache.org/docs/1.4.1/mllib-guide.html](https://spark.apache.org/docs/1.4.1/mllib-guide.html).
    The following code sample shows how you can train a logistic regression model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Model evaluation**: For model selection and evaluation, Spark ML provides
    utilities for cross-validation, hyperparameter tuning, and model evaluation metrics.
    You can find the list of evaluators at [https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.MulticlassClassificationEvaluator.html).
    The following code block illustrates using the `BinaryClassificationEvaluator`
    to evaluate a model with the `areaUnderPR` metric:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Pipeline**: Spark ML also has support for the pipeline concept, similar to
    that of scikit-learn. With the pipeline concept, you can sequence a series of
    transformation and model training steps as a unified repeatable step:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Model saving**: The Spark ML pipeline can be serialized into a serialization
    format called an Mleap bundle, which is an external library from Spark. A serialized
    Mleap bundle can be deserialized back into Spark for batch scoring or an Mleap
    runtime to run real-time APIs. You can find more details about Mleap at [https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/).
    The following code shows the syntax for serializing a Spark model into the Mleap
    format:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Spark is a versatile framework that enables large-scale data processing and
    ML. While it excels in traditional ML tasks, it also offers limited support for
    neural network training, including the multilayer perceptron algorithm. However,
    for more comprehensive deep learning capabilities, we will explore dedicated ML
    libraries including TensorFlow and PyTorch in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the TensorFlow deep learning library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Initially released in 2015, TensorFlow is a popular open-source ML library,
    primarily backed up by Google, which is mainly designed for deep learning. TensorFlow
    has been used by companies of all sizes for training and building state-of-the-art
    deep learning models for a range of use cases, including computer vision, speech
    recognition, question-answering, text summarization, forecasting, and robotics.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow works based on the concept of the computational graph, where data
    flows through nodes that represent mathematical operations. The core idea is to
    construct a graph of operations and tensors, with tensors being *n*-dimensional
    arrays that carry data. An example of a tensor could be a scalar value (for example,
    `1.0`), a one-dimensional vector (for example, `[1.0, 2.0, 3.0]`), a two-dimensional
    matrix (for example, `[[1.0, 2.0, 3.0]`, `[4.0, 5.0, 6.0]]`), or even higher dimensional
    matrices. Operations are performed on these tensors, allowing for mathematical
    computations like addition or matrix multiplication. The following diagram shows
    a sample computational graph for performing a sequence of mathematical operations
    on tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Data flow diagram ](img/B20836_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Data flow diagram'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding computational diagram, the rectangular nodes are mathematical
    operations, while the circles represent tensors. This particular diagram shows
    a computational graph for performing an artificial neuron tensor operation, which
    is to perform a matrix multiplication of *W* and *X*, followed by the addition
    of *b*, and, lastly, apply a *ReLU* action function. The equivalent mathematical
    formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20836_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorFlow allows users to define and manipulate the computation graph using
    its high-level API or by directly working with its lower-level components. This
    flexibility allows researchers and developers to create complex models and algorithms.
    Furthermore, TensorFlow supports distributed computing, allowing the graph to
    be executed across multiple devices or machines, which is crucial for handling
    large-scale ML tasks. This distributed architecture enables TensorFlow to leverage
    the power of clusters or GPUs to accelerate the training and inference of deep
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow can be installed using the `pip install --upgrade tensorflow` command
    in a Python-based environment. After installation, TensorFlow can be used just
    like any other Python library package.
  prefs: []
  type: TYPE_NORMAL
- en: Core components of TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The TensorFlow library provides a rich set of features for different ML steps,
    from data preparation to model serving. The following diagram illustrates the
    core building blocks of the TensorFlow library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – TensorFlow components ](img/B20836_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: TensorFlow components'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training an ML model using TensorFlow 2.x involves the following main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing the dataset**: TensorFlow 2.x provides a `tf.data` library for
    efficiently loading data from sources (such as files), transforming data (such
    as changing the values of the dataset), and setting up the dataset for training
    (such as configuring batch size or data prefetching). These data classes provide
    efficient ways to pass data to the training algorithms for optimized model training.
    The TensorFlow **Keras** API also provides a list of built-in classes (MNIST,
    CIFAR, IMDB, MNIST Fashion, and Reuters Newswire) for building simple deep learning
    models. You can also feed a NumPy array or Python generator (a function that behaves
    like an iterator) to a model in TensorFlow for model training, but `tf.data` is
    the recommended approach.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defining the neural network**: TensorFlow 2.x provides multiple ways to use
    or build a neural network for model training. You can use the premade estimators
    (the `tf.estimator` class) such as `DNNRegressor` and `DNNClassifier` to train
    models. Or, you can create custom neural networks using the `tf.keras` class,
    which provides a list of primitives such as `tf.keras.layers` for constructing
    neural network layers and `tf.keras.activation` such as ReLU, **Sigmoid**, and
    **Softmax** for building neural networks. Softmax is usually used as the last
    output of a neural network for a multiclass problem. It takes a vector of real
    numbers (positive and negative) as input and normalizes the vector as a probability
    distribution to represent the probabilities of different class labels, such as
    the different types of hand-written digits. For binary classification problems,
    Sigmoid is normally used and it returns a value between 0 and 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defining the loss function**: TensorFlow 2.x provides a list of built-in
    loss functions such as **mean squared error** (**MSE**) and **mean absolute error**
    (**MAE**) for regression tasks and cross-entropy loss for classification tasks.
    You can find more details about MSE and MAE at [https://en.wikipedia.org/wiki/Mean_squared_error](https://en.wikipedia.org/wiki/Mean_squared_error)
    and [https://en.wikipedia.org/wiki/Mean_absolute_error](https://en.wikipedia.org/wiki/Mean_absolute_error).
    You can find a list of supported loss functions in the `tf.keras.losses` class.
    For more details about the different losses, refer to [https://keras.io/api/losses/](https://keras.io/api/losses/).
    There is also the flexibility to define custom loss functions if the built-in
    loss functions do not meet the needs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selecting the optimizer**: TensorFlow 2.x provides a list of built-in optimizers
    for model training, such as the **Adam** optimizer and the **stochastic gradient
    descent** (**SGD**) optimizer for parameters optimization, with its `tf.keras.optimizers`
    class. You can find more details about the different supported optimizers at [https://keras.io/api/optimizers/](https://keras.io/api/optimizers/).
    Adam and SGD are two of the most commonly used optimizers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selecting the evaluation metrics**: TensorFlow 2.x has a list of built-in
    model evaluation metrics (for example, accuracy and cross-entropy) for model training
    evaluations with its `tf.keras.metrics` class. You can also define custom metrics
    for model evaluation during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compiling the network into a model**: This step compiles the defined network,
    along with the defined loss function, optimizer, and evaluation metrics, into
    a computational graph that’s ready for model training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fitting the model**: This step kicks off the model training process by feeding
    the data to the computational graph through batches and multiple epochs to optimize
    the model parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluating the trained model**: Once the model has been trained, you can
    evaluate the model using the `evaluate()` function against the test data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Saving the model**: The model can be saved in the TensorFlow **SavedModel**
    serialization format or **Hierarchical Data Format** (**HDF5**) format.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model serving**: TensorFlow comes with a model serving framework called TensorFlow
    Serving, which we will cover in greater detail in *Chapter 7*, *Open-Source ML
    Platform*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The TensorFlow library is designed for large-scale production-grade data processing
    and model training. As such, it provides capabilities for large-scale distributed
    data processing and model training on a cluster of servers against a large dataset.
    We will cover large-scale distributed data processing and model training in greater
    detail in *Chapter 10*, *Advanced ML Engineering*.
  prefs: []
  type: TYPE_NORMAL
- en: To support the complete process of building and deploying ML pipelines, TensorFlow
    provides **TensorFlow Extended** (**TFX**). TFX integrates multiple components
    and libraries from the TensorFlow ecosystem, creating a cohesive platform for
    tasks such as data ingestion, data validation, preprocessing, model training,
    model evaluation, and model deployment. Its architecture is designed to be modular
    and scalable, enabling users to tailor and expand the pipeline to meet their specific
    requirements. You can get more details about TFX at [https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow offers an expanded ecosystem of libraries and extensions for solving
    a wide range of advanced ML problems, including federated learning (training a
    model using decentralized data), model optimization (optimizing a model for deployment
    and execution), and probabilistic reasoning (reasoning under uncertainty using
    probability theory). It also provides support for mobile and edge devices with
    this TensorFlow Lite component, and support for browsers through the TensorFlow.js
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – training a TensorFlow model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With deep learning dominating the recent ML advancement, it is important to
    have some hands-on experience with deep learning frameworks. In this exercise,
    you will learn how to install the TensorFlow library in your local Jupyter environment
    and build and train a simple neural network model. Launch a Jupyter notebook that
    you have previously installed on your machine. If you don’t remember how to do
    this, revisit the *Hands-on lab* section of *Chapter 3*, *Exploring ML Algorithms*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the Jupyter notebook is running, create a new folder by selecting the
    **New** dropdown and then **Folder**. Rename the folder `TensorFlowLab`. Open
    the `TensorFlowLab` folder, create a new notebook inside this folder, and rename
    the notebook `Tensorflow-lab1.ipynb`. Now, let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside the first cell, run the following code to install TensorFlow. As mentioned
    in *Chapter 3*, `pip` is the Python package installation utility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must import the library and load the sample training data. We will
    use the built-in `fashion_mnist` dataset that comes with the `keras` library to
    do so. Next, we must load the data into a `tf.data.Dataset` class and then call
    its `batch()` function to set up a batch size. Run the following code block in
    a new cell to load the data and configure the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see what the data looks like. Run the following code block in a new cell
    to view the sample data. **Matplotlib** is a Python visualization library, used
    to display an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we build a simple **Multilayer Perception** (**MLP**) network with two
    hidden layers (one with `100` nodes and one with `50` nodes) and an output layer
    with `10` nodes (each node represents a class label). Then, we must compile the
    network using the **Adam** optimizer, use the cross-entropy loss as the optimization
    objective, and use accuracy as the measuring metric.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Adam optimizer is a variation of **gradient descent** (**GD**), and it improves
    upon GD mainly in the area of the adaptive learning rate for updating the parameters
    to improve model convergence, whereas GD uses a constant learning rate for parameter
    updating. Cross-entropy measures the performance of a classification model, where
    the output is the probability distribution for the different classes adding up
    to 1\. The cross-entropy error increases when the predicted distribution diverges
    from the actual class label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To kick off the training process, we must call the `fit()` function, which
    is a required step in this case. We will run the training for `10` epochs. One
    epoch is one pass of the entire training dataset. Note that we are running 10
    epochs here for illustration purposes only. The actual number will be based on
    the specific training job and the desired model performance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the model is training, you should see a loss metric and accuracy metric
    being reported for each epoch to help understand the progress of the training
    job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the model has been trained, we need to validate its performance using
    the test dataset. In the following code, we are creating a `test_ds` for the test
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also use the standalone `keras.metrics` to evaluate the model. Here,
    we are getting the prediction results and using `tf.keras.metrics.Accuracy` to
    calculate the accuracy of predictions against the true values in `test[1]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You may notice the accuracy metrics in the previous step and this step are slightly
    different. That’s because the dataset samples used for evaluation are not exactly
    the same.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To save the model, run the following code in a new cell. It will save the model
    in the `SavedModel` serialization format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Open the `model` directory. You should see that several files have been generated,
    such as `saved_model.pb`, and several files under the `variables` subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Great job! You have successfully installed the TensorFlow package in your local
    Jupyter environment and completed the training of a deep learning model. Through
    this process, you now possess the basic knowledge about TensorFlow and its capabilities
    for training deep learning models. Let’s shift our focus to PyTorch, another widely
    used and highly regarded deep learning library that excels in both experimental
    and production-grade ML model training.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the PyTorch deep learning library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is an open-source ML library that was designed for deep learning using
    GPUs and CPUs. Initially released in 2016, it is a highly popular ML framework
    with a large following and many adoptions. Many technology companies, including
    tech giants such as **Facebook**, **Microsoft**, and **Airbnb**, all use PyTorch
    heavily for a wide range of deep learning use cases, such as computer vision and
    **natural language processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch strikes a good balance of performance (using a C++ backend) with ease
    of use with default support for dynamic computational graphs and interoperability
    with the rest of the Python ecosystem. For example, with PyTorch, you can easily
    convert between NumPy arrays and PyTorch tensors. To allow for easy backward propagation,
    PyTorch has built-in support for automatically computing gradients, a vital requirement
    for gradient-based model optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch library consists of several key modules, including tensors, **autograd**,
    **optimizer**, and **neural network**. Tensors are used to store and operate multidimensional
    arrays of numbers. You can perform various operations on tensors such as matrix
    multiplication, transpose, returning the max number, and dimensionality manipulation.
    PyTorch supports automatic gradient calculation with its Autograd module. When
    performing a forward pass, the Autograd module simultaneously builds up a function
    that computes the gradient. The Optimizer module provides various algorithms such
    as SGD and Adam for updating model parameters. The Neural Network module provides
    modules that represent different layers of a neural network such as the linear
    layer, embedding layer, and dropout layer. It also provides a list of loss functions
    that are commonly used for training deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch can run on different operating systems, including Linux, Mac, and Windows.
    You can follow the instructions at [https://pytorch.org/](https://pytorch.org/)
    to install it in your environment. For example, you can use the `pip install torch`
    command to install it in a Python-based environment.
  prefs: []
  type: TYPE_NORMAL
- en: Core components of PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to TensorFlow, PyTorch also supports the end-to-end ML workflow, from
    data preparation to model serving. The following diagram shows what different
    PyTorch modules are used to train and serve a PyTorch model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – PyTorch modules for model training and serving ](img/B20836_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: PyTorch modules for model training and serving'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in training a deep learning model are very similar to that
    of TensorFlow model training. We’ll look at the PyTorch-specific details in the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing the dataset**: PyTorch provides two primitives for dataset and
    data loading management: `torch.utils.data.Dataset` and `torch.utils.data.Dataloader`.
    `Dataset` stores data samples and their corresponding labels, while `Dataloader`
    wraps around the dataset and provides easy and efficient access to the data for
    model training. `Dataloader` provides functions such as `shuffle`, `batch_size`,
    and `prefetch_factor` to control how the data is loaded and fed to the training
    algorithm. Additionally, as the data in the dataset might need to be transformed
    before training is performed, `Dataset` allows you to use a user-defined function
    to transform the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defining the neural network**: PyTorch provides a high-level abstraction
    for building neural networks with its `torch.nn` class, which provides built-in
    support for different neural network layers such as linear layers and convolutional
    layers, as well as activation layers such as Sigmoid and ReLU. It also has container
    classes such as `nn.Sequential` for packaging different layers into a complete
    network. Existing neural networks can also be loaded into PyTorch for training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Defining the loss function**: PyTorch provides several built-in loss functions
    in its `torch.nn` class, such as `nn.MSELoss` and `nn.CrossEntropyLoss`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selecting the optimizer**: PyTorch provides several optimizers with its `nn.optim`
    classes. Examples of optimizers include `optim.SGD`, `optim.Adam`, and `optim.RMSProp`.
    All the optimizers have a `step()` function that updates model parameters with
    each forward pass. There’s also a backward pass that calculates the gradients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selecting the evaluation metrics**: The PyTorch `ignite.metrics` class provides
    several evaluation metrics such as precision, recall, and `RootMeanSquaredError`
    for evaluating model performances. You can learn more about precision and recall
    at [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall).
    You can also use the scikit-learn metrics libraries to help evaluate models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training the model**: Training a model in PyTorch involves three main steps
    in each training loop: forward pass the training data, backward pass the training
    data to calculate the gradient, and perform the optimizer step to update the gradient.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Saving/loading the model**: The `torch.save()` function saves a model in
    a serialized `pickle` format. The `torch.load()` function loads a serialized model
    into memory for inference. A common convention is to save the files with the `.pth`
    or `.pt` extension. You can also save multiple models into a single file.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model serving**: PyTorch comes with a model serving library called TorchServe,
    which we will cover in more detail in *Chapter 7*, *Open-Source ML Platforms*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The PyTorch library supports large-scale distributed data processing and model
    training, which we will cover in more detail in *Chapter 10*, *Advanced ML Engineering*.
    Like TensorFlow, PyTorch also offers an ecosystem of library packages for a wide
    range of ML problems, including ML privacy, adversarial robustness, video understanding,
    and drug discovery.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned about the fundamentals of PyTorch, let’s get hands-on
    through a simple exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on exercise – building and training a PyTorch model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this hands-on exercise, you will learn how to install the PyTorch library
    on your local machine and train a simple deep learning model using PyTorch. Launch
    a Jupyter notebook that you have previously installed on your machine. If you
    don’t remember how to do this, visit the *Hands-on lab* section of *Chapter 3*,
    *Exploring ML Algorithms*. Now, let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new folder called `pytorch-lab` in your Jupyter Notebook environment
    and create a new notebook file called `pytorch-lab1.ipynb`. Run the following
    command in a cell to install PyTorch and the `torchvision` package. `torchvision`
    contains a set of computer vision models and datasets. We will use the pre-built
    MNIST dataset in the `torchvision` package for this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The following sample code shows the previously mentioned main components. Be
    sure to run each code block in a separate Jupyter notebook cell for optimal readability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we must import the necessary library packages and load the MNIST dataset
    from the `torchvision` dataset class:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must construct an MLP neural network for classification. This MLP
    network has two hidden layers with ReLU activation for the first and second layers.
    The MLP model takes an input size of `784`, which is the flattened dimension of
    a 28x28 image. The first hidden layer has `128` nodes (neurons), while the second
    layer has `64` nodes (neurons). The final layer has `10` nodes because we have
    10 class labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s a sample of the image data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must define a **cross-entropy loss function** for the training process
    since we want to measure the error in the probability distribution for all the
    labels. Internally, PyTorch’s `CrossEntropyLoss` automatically applies a `softmax`
    to the network output to calculate the probability distributions for the different
    classes. For the optimizer, we have chosen the Adam optimizer with a learning
    rate of `0.003`. The `view()` function flattens the two-dimensional input array
    (28x28) into a one-dimensional vector since our neural network takes a one-dimensional
    vector input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Learning rate is a hyperparameter that determines the size of the steps taken
    during the optimization process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s start the training process. We are going to run `15` epochs. Unlike
    the TensorFlow Keras API, where you just call a `fit()` function to start the
    training, PyTorch requires you to build a training loop and specifically run the
    forward pass (`model (images)`), run the backward pass to learn (`loss.backward()`),
    update the model weights (`optimizer.step()`), and then calculate the total loss
    and the average loss. For each training step, `trainloader` returns one batch
    (a batch size of `64`) of training data samples. Each training sample is flattened
    into a 784-long vector. The optimizer is reset with zeros for each training step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When the training code runs, it should print out the average loss for each epoch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To test the accuracy using the validation data, we must run the validation
    dataset through the trained model and use scikit-learn`.metrics.accuracy_score()`
    to calculate the model’s accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we must save the model to a file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully installed PyTorch in your local Jupyter
    environment and trained a deep learning PyTorch model.
  prefs: []
  type: TYPE_NORMAL
- en: How to choose between TensorFlow and PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow and PyTorch are the two most popular frameworks in the domain of
    deep learning. So, a pertinent question arises: How does one make an informed
    choice between the two? To help answer this question, let’s do a quick comparative
    analysis of these frameworks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ease of use**: PyTorch is generally considered more user-friendly and Pythonic.
    The control flow feels closer to native Python and the dynamic computation graphs
    of PyTorch make it easier to debug and iterate compared to TensorFlow’s static
    graphs. However, eager execution support in TensorFlow 2.0 helps close this gap.
    PyTorch is also considered more object-oriented than TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community popularity**: Both frameworks enjoy robust community support and
    are highly popular. TensorFlow initially had a lead; however, PyTorch has caught
    up in popularity in recent years, according to the Google Trends report. PyTorch
    is more widely adopted within the research community and dominates the implementation
    of research papers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model availability**: TensorFlow has the TensorFlow Model Garden, which hosts
    a collection of models utilizing TensorFlow APIs, covering various ML tasks such
    as computer vision, NLP, and recommendation. It also features TensorFlow Hub,
    offering a collection of pre-trained models ready for deployment or fine-tuning
    across a wide range of ML tasks. Similarly, PyTorch has PyTorch Hub, a library
    integrated into PyTorch that provides easy access to a broad array of pre-trained
    models for computer vision, NLP, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment**: Both frameworks are suitable for the production deployment
    of ML models. TensorFlow is considered to have a more comprehensive model deployment
    stack with TensorFlow Serving, TensorFlow Lite for mobile and edge devices, and
    TensorFlow.js for browser deployment. TensorFlow Extended is an end-to-end model
    deployment platform encompassing model validation, monitoring, and explanation.
    PyTorch offers TorchServe, a model-serving framework for PyTorch models, and PyTorch
    Mobile for deploying models on iOS and Android devices. PyTorch relies more on
    third-party solutions for end-to-end integration in the deployment process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To conclude, both frameworks provide comparable capabilities throughout the
    entire ML lifecycle, accommodating similar use cases. If your organization has
    already committed to either TensorFlow or PyTorch, it is advisable to proceed
    with that decision. However, for those embarking on the initial stages, PyTorch
    might offer a more accessible starting point owing to its ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored several popular open-source ML library packages,
    including scikit-learn, Spark ML, TensorFlow, and PyTorch. By now, you should
    have a good understanding of the fundamental components of these libraries and
    how they can be leveraged to train ML models. Additionally, we have delved into
    the TensorFlow and PyTorch frameworks to construct artificial neural networks,
    train deep learning models, and save these models to files. These model files
    can then be utilized in model-serving environments to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into Kubernetes and its role as a foundational
    infrastructure for constructing open-source ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mlsah](https://packt.link/mlsah )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code70205728346636561.png)'
  prefs: []
  type: TYPE_IMG
