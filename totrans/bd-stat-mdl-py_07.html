<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-114"><a id="_idTextAnchor118"/>7</h1>
<h1 id="_idParaDest-115"><a id="_idTextAnchor119"/>Multiple Linear Regression</h1>
<p>In the last chapter, we discussed <strong class="bold">simple linear regression</strong> (<strong class="bold">SLR</strong>) using<a id="_idIndexMarker529"/> one variable to explain a target variable. In this chapter, we will discuss <strong class="bold">multiple linear regression</strong> (<strong class="bold">MLR</strong>), which<a id="_idIndexMarker530"/> is a model that leverages multiple explanatory variables to model a response variable. Two of the major conundrums facing multivariate modeling are multicollinearity and the bias-variance trade-off. Following an overview of MLR, we will provide an induction into the methodologies used for evaluating and minimizing multicollinearity. We will then discuss methods for leveraging the bias-variance trade-off to our benefit as analysts. Finally, we will discuss handling multicollinearity using <strong class="bold">Principal Component Regression</strong> (<strong class="bold">PCR</strong>) to <a id="_idIndexMarker531"/>minimize overfitting without removing features but rather transforming them instead.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Multiple linear regression</li>
<li>Feature selection</li>
<li>Shrinkage methods</li>
<li>Dimension reduction</li>
</ul>
<h1 id="_idParaDest-116"><a id="_idTextAnchor120"/>Multiple linear regression</h1>
<p>In the <a id="_idIndexMarker532"/>previous chapter, we discussed SLR. With SLR, we were able to predict the value of a variable (commonly called the response variable, denoted as <em class="italic">y</em>) using another variable (commonly called the explanatory variable, denoted as <em class="italic">x</em>). The SLR model is expressed by the following equation where β 0 is the intercept term and β 1 is the slope of the linear model.</p>
<p>y = β 0 + β 1 x + ϵ</p>
<p>While this is a useful model, in many problems, multiple explanatory variables could be used to predict the response variable. For example, if we wanted to predict home prices, we might want to consider many variables, which may include lot size, the number of bedrooms, the number of bathrooms, and overall size. In this situation, we can expand the previous model to include these additional variables. This is called MLR. The MLR model can be expressed with the following equation.</p>
<p>y = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p + ϵ</p>
<p>Like <a id="_idIndexMarker533"/>the previous equation, β 0 represents an intercept. In this equation, in addition to the intercept value, we have a β parameter for each explanatory variable. Each explanatory variable is denoted as <em class="italic">x</em> with a numerical subscript. We can include as many explanatory variables as desired in the model, which is why the equation shows a final subscript of p.</p>
<p class="callout-heading">Impacts of data sizes on MLR</p>
<p class="callout">In general, we can include as many explanatory variables as desired in the model. However, there are some realistic limits. One of these potential limits is the number of samples relative to the number of explanatory variables. MLR tends to work best when the number of samples (N) is much less than the number of explanatory variables (P). As P approaches N, the model becomes difficult to estimate. If P is large compared to N, it would be wise to consider dimension reduction, which will be discussed later in this chapter.</p>
<p>Let’s look at an example. The Python package <code>scikit-learn</code> contains several datasets for practice with modeling. We will utilize the <code>diabetes</code> dataset, which includes vital measurements and quantitative measures of the disease progression after one year. More information about this dataset can be found at this link: <a href="https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset</a>.We will attempt to model the relationship between the vital statistics and the disease progression.</p>
<p>The explanatory variables in this dataset are <code>age</code>, <code>bmi</code>, <code>bp</code>, <code>s1</code>, <code>s2</code>, <code>s3</code>, <code>s4</code>, <code>s5</code>, and <code>s6</code>. The response variable is a measurement of disease progression. This is how we would express this model mathematically:</p>
<p>progression = β 0 + β 1 x age + β 2 x bmi + β 3 x bp + β 4 x s1 + β 5 x s2</p>
<p>+ β 6 x s3 + β 7 x s4 + β 8 x s5 + β 9 x s6 + ϵ</p>
<p>As discussed previously, we have a β for each variable in the model.</p>
<p class="callout-heading">Transformations on the diabetes dataset</p>
<p class="callout">The explanatory variables in this dataset have been transformed from the original measurements. Referring to the dataset user’s guide, we can see that each variable has been mean-centered and scaled. In addition, the <code>sex</code> variable has been converted from a categorical variable into a numerical variable.</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor121"/>Adding categorical variables</h2>
<p>Up until this point, we<a id="_idIndexMarker534"/> have <a id="_idIndexMarker535"/>only considered continuous variables in our model, that is, variables that can have any value on the number line. Thinking back to <a href="B18945_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a><em class="italic">, Distributions of Data</em>, the variables we have considered before now have been ratio data. However, the linear regression model is not limited to using continuous variables. We can include categorical variables in the model. Recall from <a href="B18945_02.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a><em class="italic">, Distributions of Data</em>, categorical variables are associated with groups of items. In statistical learning, the groups are generally<a id="_idIndexMarker536"/> called <strong class="bold">levels</strong> of the variable. For example, if we had five students, three with MS degrees, one with a PhD degree, and one with a BS degree, we would say that the student categorical variable has three levels: BS, MS, and PhD. Let’s look at how we include categorical variables in the model.</p>
<p>We will include the categorical variable in the model by including additional β terms. Specifically, we will add the number of levels (L) minus one additional β term. For the student example with three levels, we would add two additional β terms. Using one less than the number of levels will allow us to choose one level as a reference (or baseline) and compare the other levels to that reference level. We include the β terms in the model just like the other β terms. Let’s say in this example, we are modeling income level, and in addition to the education levels, we have the experience of the individuals. Then, we would construct our model like this:</p>
<p>income = β 0 + β 1 x experience + β 2 x ms + β 3 x phd</p>
<p>With the addition of the beta terms, we also must make a data transformation. We will have to create <strong class="bold">dummy variables</strong> for<a id="_idIndexMarker537"/> each of the <em class="italic">non-reference</em> levels in the categorical variable. This process is called the dummification of the categorical variable. In dummification, we create a column for each non-reference level and insert ones where the level occurs in the original variable and zeros where it does not appear in the original variable. This process is demonstrated in <em class="italic">Figure 7</em><em class="italic">.1</em>.</p>
<div><div><img alt="Figure 7.1 – Dummification of a categorical variable" height="144" src="img/B18945_07_001.jpg" width="396"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Dummification of a categorical variable</p>
<p>The <a id="_idIndexMarker538"/>original variable contains<a id="_idIndexMarker539"/> the BS, MS, and PhD levels. BS is chosen as the reference level and two dummy variables are created for the levels MS and PhD. This process of creating additional columns for categorical variables is called <strong class="bold">encoding</strong>. There <a id="_idIndexMarker540"/>are many types of variable encodings and encoding is a widely used technique in statistical learning and machine learning.</p>
<p class="callout-heading">Categorical levels are shifts by a constant</p>
<p class="callout">Let’s look a little deeper at the impact of categorical variables on the linear regression model. Earlier, we discussed how categorical variables are encoded with dummy variables that take on values of zero and one. Essentially, that means that we are switching the associated β terms, depending on the level, which will shift the response by a constant. For example, in the income model, when we want to calculate the income of an individual with a BS degree, the model resolves to the following:</p>
<p class="callout">income = β 0 + β 1 x experience</p>
<p class="callout">And when we want to calculate the income of an individual with an MS degree, the model resolves to the following:</p>
<p class="callout">income = β 0 + β 1 x experience + β 2</p>
<p class="callout">This means that levels of categories only shift the output of the model from the reference level by a constant, the beta associated with the level.</p>
<p>Returning to<a id="_idIndexMarker541"/> our previous example, our dataset includes a categorical variable: the <code>sex</code> of the patient. The dataset <a id="_idIndexMarker542"/>includes two levels for <code>sex</code>. We will choose one level to serve as a reference and we will create a dummy variable for the other level. With this knowledge, let’s fit this data to a linear regression model and discuss the assumptions of multiple linear regression.</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor122"/>Evaluating model fit</h2>
<p>Whenever <a id="_idIndexMarker543"/>we fit a parametric model, we should verify that the model assumptions are met. As discussed in the previous chapter, if the <a id="_idIndexMarker544"/>model assumptions are not met, the model may provide misleading results. In the previous chapter, we discussed the four assumptions for simple linear regression:</p>
<ul>
<li>A linear relationship between the response variable and explanatory variables</li>
<li>Normality of the residuals</li>
<li>Homoscedasticity of the residuals</li>
<li>Independent samples</li>
</ul>
<p>These assumptions also apply here, but in this new modeling context, we have an additional assumption: no or<a id="_idIndexMarker545"/> little <strong class="bold">multicollinearity</strong>. Multicollinearity occurs when two or more of the explanatory variables in an MLR model are highly correlated. The presence of multicollinearity impacts the statistical significance of the independent variables. Ideally, all the explanatory variables will be uncorrelated (or linearly independent). However, we can accept a small amount of multicollinearity without much impact on the model.</p>
<p>Let’s evaluate the model on each of these assumptions.</p>
<h3>Linear relationships</h3>
<p>To<a id="_idIndexMarker546"/> check for linear relationships between the response variable and the explanatory variable, we can look at scatter plots of each variable against the response variable. Based on the plots in <em class="italic">Figure 7</em><em class="italic">.2</em>, several of the variables, including <code>bmi</code>, <code>bp</code>, <code>s3</code>, <code>s4</code>, and <code>s5</code>, possibly exhibit a linear relationship with the <a id="_idIndexMarker547"/>response variable.</p>
<div><div><img alt="Figure 7.2 – Scatter plots of the response variable against the explanatory variables" height="1315" src="img/B18945_07_002.jpg" width="717"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Scatter plots of the response variable against the explanatory variables</p>
<p>While it would be ideal if any of the variables showed a strong linear relationship with the response variable, in an MLR model, we have the benefit of the combination of variables. We <a id="_idIndexMarker548"/>can add variables that appear to be more useful and remove variables that do not appear to be useful. This is called <strong class="bold">feature selection</strong>, which <a id="_idIndexMarker549"/>we will cover in the next section.</p>
<h3>Normality of the residuals</h3>
<p>Recall <a id="_idIndexMarker550"/>from the previous chapter that we expect the residuals from a well-fitted model to appear randomly distributed. We could look at this with a histogram or a QQ plot. The residuals from our model are shown in <em class="italic">Figure 7</em><em class="italic">.3</em>.</p>
<div><div><img alt="Figure 7.3 – Residual value" height="438" src="img/B18945_07_003.jpg" width="621"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Residual value</p>
<p>Based on the histogram in <em class="italic">Figure 7</em><em class="italic">.3</em>, we cannot reject the possibility that the residuals are normally distributed. In general, when evaluating this assumption, we are checking for <em class="italic">egregious</em> violations. It turns out that the linear regression is relatively robust against this assumption. However, that does not mean this assumption can be ignored.</p>
<h3>Homoscedasticity of the residuals</h3>
<p>In the<a id="_idIndexMarker551"/> previous chapter, we also discussed homoscedasticity. For a well-fitted model, we expect that the residuals should exhibit homoscedasticity. The plot in <em class="italic">Figure 7</em><em class="italic">.4</em> shows the scatter plot of the residuals against the values predicted by the model.</p>
<div><div><img alt="Figure 7.4 – Scatter plot of model residuals for the predicted versus actual values" height="442" src="img/B18945_07_004.jpg" width="682"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Scatter plot of model residuals for the predicted versus actual values</p>
<p>There <a id="_idIndexMarker552"/>does not appear to be a clear pattern of changing variance or a significant outlier that would violate the assumption of homoscedasticity. If there had been a pattern, that would have been a sign that one or more of the variables may need to be transformed, or a sign of a non-linear relationship between the response and the explanatory variables.</p>
<h3>Independent samples</h3>
<p>In the <a id="_idIndexMarker553"/>last chapter, we discussed independent sampling and its impact on this type of model. However, we cannot make a certain determination on whether the samples are independent without knowing the sampling methodology. Since we do not know the sampling strategy for this dataset, we will assume this assumption is met and proceed with the model. In a real modeling setting, this assumption should never be taken for granted.</p>
<h3>Multicollinearity</h3>
<p>The new <a id="_idIndexMarker554"/>assumption for MLR is that there is little or no multicollinearity in the explanatory variables. Multicollinearity is a situation that occurs when two or more variables are strongly linearly correlated. We commonly use<a id="_idIndexMarker555"/> the <strong class="bold">variance inflation factor</strong> (<strong class="bold">VIF</strong>) to detect multicollinearity. The VIF is a measurement of how much the coefficient of an explanatory variable is influenced by other explanatory variables. A lower VIF is better where the minimum value is 1, meaning there is no correlation. We generally consider a VIF of 5 or more to be too high. When a high VIF is detected in a set of explanatory variables, we repeatedly remove the variable with the highest VIF until the VIF values for each variable are below 5. Let’s look at an example with our <a id="_idIndexMarker556"/>current data. The process of removing variables with high VIFs is shown in <em class="italic">Figure 7</em><em class="italic">.5</em>.</p>
<div><div><img alt="Figure 7.5 – Removing high VIF variables from a dataset" height="440" src="img/B18945_07_005.jpg" width="912"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Removing high VIF variables from a dataset</p>
<p><em class="italic">Figure 7</em><em class="italic">.5</em> shows the process of removing variables with a high VIF from the diabetes dataset. The leftmost table in the figure shows the original dataset where the highest VIF is 59.2, which corresponds to variable S1. Then, we remove this variable from the dataset and recalculate the VIF. Now we see that the highest VIF is 7.8, corresponding to variable <code>s4</code>. We remove this variable and recalculate the VIF. Now, all VIFs are below 5, indicating that there is a low correlation between the remaining variables. With these variables removed, we need to fit the model again.</p>
<p>With the model fit and the assumptions of the model verified, let’s look at the fit results and discuss how to interpret the results.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor123"/>Interpreting the results</h2>
<p>Fitting<a id="_idIndexMarker557"/> the model, we get the following <a id="_idIndexMarker558"/>results from <code>statsmodels</code>. The output is divided into three sections:</p>
<ul>
<li>The top section contains high-level statistics about the model</li>
<li>The middle section contains details about the model coefficients</li>
<li>The bottom section contains diagnostic tests about the data and the residuals</li>
</ul>
<p>Let’s walk<a id="_idIndexMarker559"/> through each section of this model.</p>
<p class="IMG---Figure"><img alt="Figure 7.6 – Results from statsmodels OLS regression" height="552" src="img/B18945_07_006.png" width="773"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Results from statsmodels OLS regression</p>
<h3>High-level statistics and metrics (top section)</h3>
<p>In<a id="_idIndexMarker560"/> the top section of the fit results, we have model-level information. The left side of the top section contains information about the model such as the <strong class="bold">degrees of freedom</strong> (<strong class="bold">df</strong>) and the number of observations. The right side of the top section contains model metrics. Model metrics are useful for comparing models. We will discuss more about model metrics in the section on feature selection.</p>
<h3>Model coefficient details</h3>
<p>The middle section contains details about the model coefficients (the β terms in the equations listed previously). For the purposes of this section, we will focus on two columns in the middle section: <code>coef</code> and <code>P&gt;|t|</code>. The <code>coef</code> column is the model coefficient estimated for the model equation (the estimate of β or termed  ˆ β ). The column labeled <code>P&gt;|t|</code> is the p-value for a significance test for the coefficient. We are interested in both columns for interpreting the model. Let’s start with the p-value column.</p>
<p>The null hypothesis for this test is that <strong class="bold">the value of the </strong>β<strong class="bold"> parameter is equal to zero</strong>. Recall the following from <a href="B18945_03.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, </em><em class="italic">Hypothesis Testing</em>:</p>
<ul>
<li>A p-value below the significance threshold means we reject the null hypothesis</li>
<li>A p-value above the significance threshold means that we fail to reject the null hypothesis</li>
</ul>
<p>In this <a id="_idIndexMarker561"/>example, we would reject the null hypothesis for the <code>bmi</code> variable, but we fail to reject the null hypothesis for the age variable. Once we have determined the significant variables, we can move on to interpreting the meaning of the significant variables. We will not be able to provide an interpretation of the other variables because we cannot reject that their coefficient values might be zero. If the coefficient value is zero, then the variable makes no contribution to the model. Let’s look at how to interpret the coefficients.</p>
<p>Often, when we construct a model, we want to understand how the parts of the model affect the output. In an MLR model, this comes down to understanding the coefficients of the model. We have two types of variables, continuous and categorical, and the associated coefficients have different meanings.</p>
<h4>Interpreting continuous variable coefficients</h4>
<p>For the <a id="_idIndexMarker562"/>continuous variables, such as <code>BMI</code>, as the value of the variable increases, so does its significance to the output of the model. A unit increase in the value of the variable is associated with an increase in the mean of the dependent variable by the size of the coefficient with all other variables held constant. Let’s take the <code>bmi</code> variable as an example. Since the coefficient of the <code>bmi</code> variable is approximately 526, we would say, “A unit increase in <code>bmi</code> would be associated with a 526 increase in the mean of the diabetes measurement with all other variables held constant.” Of course, the coefficients can also take on negative values.</p>
<h4>Interpreting categorical variable coefficients</h4>
<p>For the <a id="_idIndexMarker563"/>categorical variables, such as <code>sex</code>, recall that, unlike the continuous variables, the values of the categorical are dummy-encoded, and therefore can only take on two values: zero and one. Also, recall that one level was chosen to be the reference level. In this case, <code>sex</code> level 0 is the reference level, and we can compare this reference to <code>sex</code> level 1. When we use the reference level, the coefficient will not affect the output of the model. Thus, the categorical-level change is associated with a change in the mean of the dependent variable by the size of the coefficient with all other variables held constant. Since the coefficient of the <code>sex</code> variable is approximately -22, we would say, “The level of <code>sex</code> is associated with a decrease of 22 in the mean of the diabetes measurement compared to the reference level with all other variables held constant.”</p>
<h3>Diagnostic tests</h3>
<p>The <a id="_idIndexMarker564"/>bottom section of the fit results contains diagnostic statistics for the data and residuals. Glancing over the list, several should be from <a href="B18945_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a><em class="italic">, Simple Linear Regression</em>. The Durbin-Watson test is a test for serial correlation (data sampled sequentially over time). A result around 2 is not indicative of serial correlation. The skew and kurtosis are measurements of the shape of the distribution of the residuals. These results indicate almost no skew, but possibly some kurtosis. These are likely small deviations from the normal distribution and are not cause for concern as we saw in the plots earlier.</p>
<p>In this section, we looked at our first model that can use multiple explanatory variables to predict a response variable. However, we noticed that several of the variables included in the model were not statistically significant. For this model, we only selected features by removing any features that had high VIF scores, but there are other methods to consider when choosing features. In the next section, we will discuss comparing models and feature selection.</p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>Feature selection</h1>
<p>The are<a id="_idIndexMarker565"/> many factors that influence the success or failure of a model, such as sampling, data quality, feature creation, and model selection, several of which we have not covered. One of those critical factors is <strong class="bold">feature selection</strong>. Feature selection is simply the process of choosing or systematically determining the best features for a model from an existing set of features. We have done some simple feature selection<a id="_idIndexMarker566"/> already. In the previous section, we removed features that had high VIFs. In this section, we will look at some methods for feature selection. The methods presented in this section fall into two categories: statistical methods for feature selection and performance-based methods for feature selection. Let’s start with statistical methods.</p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor125"/>Statistical methods for feature selection</h2>
<p>Statistical methods <a id="_idIndexMarker567"/>for feature selection rely on the<a id="_idIndexMarker568"/> primary tool that we have used throughout the previous chapters: statistical significance. The methods presented in this sub-section will be based on the statistical properties of the features themselves. We will cover two statistical methods for feature selection: correlation and statistical significance.</p>
<h3>Correlation</h3>
<p>The <a id="_idIndexMarker569"/>first statistical method we will discuss is <strong class="bold">correlation</strong>. We have discussed correlation in this chapter and in previous chapters; recall that correlation is a description of the relationship between two variables. Variables can be positively correlated, uncorrelated, or negatively correlated. In terms of feature selection, we want to <em class="italic">remove features that are uncorrelated with the response variable</em>. A feature that is uncorrelated with the response variable does not have a relationship with the response variable. Thus, an uncorrelated feature would not be a good predictor of the response variable.</p>
<p>Recall from <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, Parametric Tests</em>, that we can use Pearson’s correlation coefficient to measure the linear correlation between two variables. In fact, we can calculate the correlation coefficient between all features and the target variable. After performing those calculations, we can construct a correlation ranking as shown in <em class="italic">Figure 7</em><em class="italic">.7</em>.</p>
<div><div><img alt="Figure 7.7 – Feature correlation ranking" height="467" src="img/B18945_07_007.jpg" width="650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Feature correlation ranking</p>
<p>When <a id="_idIndexMarker570"/>evaluating features based on correlation, we are most interested in features with a high <em class="italic">absolute</em> correlation. For example, the correlation ranking in <em class="italic">Figure 7</em><em class="italic">.7</em> shows the following:</p>
<ul>
<li><code>bmi</code> and <code>s5</code> exhibit a strong correlation with the response variable</li>
<li><code>bp</code>, <code>s4</code>, <code>s6</code>, and <code>s3</code> exhibit moderate correlation with the response variable</li>
<li><code>s1</code>, <code>age</code>, and <code>s2</code> exhibit a weak correlation with the response variable</li>
</ul>
<p>While <code>sex</code> may appear to show no correlation with the response variable, Pearson’s correlation coefficient cannot be used with categorical features. From this correlation ranking, we can see that, at least, <code>bmi</code>, <code>s5</code>, and <code>bp</code>, are likely among the best features in this dataset for predicting the response. In fact, these features were considered statistically significant in our model. Now let’s discuss selection using statistical significance.</p>
<h3>Statistical significance</h3>
<p>Assessing <a id="_idIndexMarker571"/>features using correlation is generally a good first step for feature selection. We can easily eliminate features that are uncorrelated with the response variable. However, depending on the problem, we could still be left with many features that are correlated with the response. We can further select features using the statistical significance of the feature in the context of a model. However, in recent years, these methods for feature selection have somewhat fallen out of favor in the community. For this reason, we will not focus on these methods, but will only describe them for understanding.</p>
<p>Recall when we fit the MLR model, the results included a test for statistical significance for each feature in the model. We can use that test to select features. There are three well-known algorithms for selecting features based on statistical significance: <strong class="bold">forward selection</strong>, <strong class="bold">backward selection</strong>, and <strong class="bold">stepwise regression</strong>. In forward selection, we start without <a id="_idIndexMarker572"/>any variables in the model and then iteratively add one variable at a time using the p-value to choose the best feature to add at each iteration. We stop once the p-values of any features in the model size are above a predefined threshold, such as 0.05. Backward selection takes the opposite approach. We start with all features in the model, then iteratively remove features one variable at a time using the p-value to determine the least important feature. The final algorithm is stepwise regression (also called bidirectional elimination). Stepwise regression is performed using both forward and backward tests. Start with no features in the model, then in each iteration, perform one forward selection step followed by one backward selection pass.</p>
<p>These selection methods were widely used in the past. However, in recent years, performance-based methods have become more widely used. Let’s discuss performance-based methods now.</p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor126"/>Performance-based methods for feature selection</h2>
<p>The <a id="_idIndexMarker573"/>primary issue with the statistical feature<a id="_idIndexMarker574"/> selection methods mentioned previously is that they tend<a id="_idIndexMarker575"/> to create <strong class="bold">overfit</strong> models. An overfit model is a model that fits the given data exactly and fails to generalize to new data. Performance-based methods overcome overfitting using a method called <strong class="bold">cross-validation</strong>. In <a id="_idIndexMarker576"/>cross-validation, we have two <a id="_idIndexMarker577"/>datasets: a <strong class="bold">training dataset</strong> used to fit the model and a <strong class="bold">test dataset</strong> for<a id="_idIndexMarker578"/> evaluating the model. We can build models from multiple sets of features, fit all those potential models on the training set, and finally rank them based on the performance of the models on the testing set with a given metric.</p>
<h3>Comparing models</h3>
<p>Before we <a id="_idIndexMarker579"/>get into feature selection methods, let’s first discuss how to compare models. We use metrics to compare models. On a basic level, we can use metrics to help us determine whether one set of features is better than another set of features based on model performance. Many metrics can be used for comparing models. We will discuss two metrics, <strong class="bold">mean square error</strong> (<strong class="bold">MSE</strong>) and <strong class="bold">mean absolute percentage error</strong> (<strong class="bold">MAPE</strong>), which are, by far, two of the most commonly used metrics for regression models.</p>
<h4>MSE</h4>
<p>The MSE is <a id="_idIndexMarker580"/>given by the following formula, where <em class="italic">N</em> is the number of samples, y is the response variable, and  ˆ y  is the predicted value of the response variable.</p>
<p>MSE =  1 _ N  ∑ i=1 N (y i −  ˆ y  i) 2</p>
<p>In other words, take the differences between the response values and the predicted response values, square the differences, and finally take the mean of the squared differences. A small extension of this metric commonly used is the <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>), which<a id="_idIndexMarker581"/> is simply the square root of the MSE. The RMSE is used when it is desirable for the metric to have the same units as the response variable.</p>
<h4>MAPE</h4>
<p>The MAPE is <a id="_idIndexMarker582"/>given by the following formula, where <em class="italic">N</em> is the number of samples, y is the response variable, and  ˆ y  is the predicted value of the response variable:</p>
<p>MAPE = 100% _ N  ∑ i=1 N  |y i −  ˆ y  i _ y i |</p>
<p>This formula is<a id="_idIndexMarker583"/> like the formula for the MSE, but instead of taking the mean of the squared error, we take the mean of the percent error. This makes the MAPE easier to interpret than the MSE, which is a distinct advantage over the MSE.</p>
<p>Now that we have discussed model validation and metrics, let’s put these concepts together to perform feature selection using model performance as an indicator.</p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor127"/>Recursive feature elimination</h2>
<p><strong class="bold">Recursive feature elimination</strong> (<strong class="bold">RFE</strong>) is a method for selecting an optimal number of features<a id="_idIndexMarker584"/> in a model using a metric. Much<a id="_idIndexMarker585"/> like the backward selection method mentioned previously, the RFE algorithm starts with all features in the model, then removes features with the least influence on the model. At each step, cross-validation is performed. When RFE is completed, we will be able to see the cross-validation performance of the model over the various sets of features.</p>
<p>In this example, we use the linear regression implementation and RFE implementation from <code>scikit-learn</code> (<code>sklearn</code>), which is a primary package used for machine learning in the Python ecosystem. In the following code example, we set up RFE to use the MAPE as the scoring metric (<code>make_scorer(mape ,greater_is_better=False)</code>), remove one feature at each step (<code>step=1</code>), and indicate with <code>cv=2</code> that it should score the model using two cross-validation sets:</p>
<pre class="source-code">
linear_model = LinearRegression()
linear_model.fit(X, y)
rfecv = RFECV(
    estimator=linear_model,
    step=1,
    cv=2,
    scoring=make_scorer(mape ,greater_is_better=False),
    min_features_to_select=1
)
rfecv.fit(X,y)</pre>
<p>Once we fit the RFE object, we can look at the results to see how to model scored over the various sets of features. The performance of the model using the MAPE is shown in <em class="italic">Figure 7</em><em class="italic">.8</em>.</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 7.8 – Linear regression performance over RFE steps, scoring with the MAPE" height="712" src="img/B18945_07_008.jpg" width="1098"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Linear regression performance over RFE steps, scoring with the MAPE</p>
<p>It’s clear<a id="_idIndexMarker586"/> that<a id="_idIndexMarker587"/> including all features produces the best-performing model but including all 10 features only provides a small increase over the performance of just five features. While the best-performing model contains all 10 features, we would still need to consider model assumptions and verify that the model is well fit.</p>
<p>In this section, we have looked at several methods for feature selection, including statistical methods and performance-based methods. We also discussed metrics and how to compare models, including the reason for splitting the data into training and validation sets. In the next section, we will look at linear regression shrinkage methods. These types of models use a method called regularization, which in some ways acts like model-based feature selection.</p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor128"/>Shrinkage methods</h1>
<p>The <strong class="bold">bias-variance trade-off</strong> is a<a id="_idIndexMarker588"/> decision point all statistics and<a id="_idIndexMarker589"/> machine learning practitioners must balance when performing modeling. Too much of either renders results useless. To catch these when they become issues, we look at test results and the residuals. For example, assuming a useful set of features and the appropriate model have been selected, a model that performs well on validation, but poorly on a test set could be indicative of too much variance and conversely, a model that fails to perform well at all could have too much bias. In either case, both models fail to generalize well. However, while bias in a model can be identified in poor model performance from the start, high variance can be notoriously deceptive as it has the potential to perform very well during training and even during validation, depending on the data. High-variance models frequently use values of coefficients that are unnecessarily high when very similar results can be obtained from coefficients that are not. Further, in using coefficients that are not unnecessarily high, the model is more likely to be in bias-variance equilibrium, which provides it a better chance of generalizing well on future data. Additionally, more reliable insights into the influence of current factors on a given target are provided when model coefficients are not exaggerated, which aids in more useful descriptive analysis. This brings us to the concept of shrinkage.</p>
<p><strong class="bold">Shrinkage</strong> is a method that reduces model variance by shrinking model parameter coefficients toward zero. The coefficients are derived by applying least squares regression to all variables considered for a model. The amount of shrinkage is based on the parameters’ contribution to least squares estimates; parameters that contribute to a high level of squared error will have their coefficients pushed toward zero or zeroed out altogether. In this case, shrinkage can be used for variable elimination. Variables that do not contribute to high squared error across the model fit will have a minimal reduction in coefficient value, thus being useful for model fitting, assuming the practical purpose of their inclusion is vetted. The reason shrinkage – also<a id="_idIndexMarker590"/> called <strong class="bold">regularization</strong> – is important is because it helps models include useful variables while preventing them from introducing too much variance and thus overfitting. Preventing excess variance is particularly useful for ensuring that models generalize well over time. Let’s look at some of the <a id="_idIndexMarker591"/>most common shrinkage<a id="_idIndexMarker592"/> techniques, <strong class="bold">ridge regression</strong> and <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong> (<strong class="bold">LASSO</strong>) <strong class="bold">Regression</strong>.</p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor129"/>Ridge regression</h2>
<p>Recall<a id="_idIndexMarker593"/> the <a id="_idIndexMarker594"/>formulation for the <strong class="bold">residual sum of squared</strong> <strong class="bold">errors</strong> (<strong class="bold">RSS</strong>) for multiple linear regression using least squares regression is as follows:</p>
<p>RSS = ∑ i=1 n  (y i −  ˆ β  0 − ∑ j=1 p  ˆ β  j x ij) 2</p>
<p>where <em class="italic">n</em> is the number of samples and <em class="italic">p</em> is the number of parameters. Ridge regression adds a scalar, <em class="italic">λ</em>, called <a id="_idIndexMarker595"/>a <strong class="bold">tuning parameter</strong> – which must be greater than or equal to 0 – that gets multiplied by the model parameter coefficient estimates,  ˆ β  j 2, to create<a id="_idIndexMarker596"/> a <strong class="bold">shrinkage penalty</strong>. This gets added back to the RSS equation such that the new least squares regression’s fitting procedure is now defined as follows:</p>
<p>RSS + 𝝀∑ j=1 p  ˆ β  j 2</p>
<p>When λ=0, the respective ridge<a id="_idIndexMarker597"/> regression <strong class="bold">penalty term</strong> is 0. However, as λ ⟶ ∞, the model coefficients shrink toward zero. The new fitting procedure can be rewritten as follows:</p>
<p>‖y − Xβ‖ 2 2 + 𝝀 ‖β‖ 2 2</p>
<p>where RSS = ‖y − Xβ‖ 2 2, ‖β‖ 2 2 is the squared L 2 <strong class="bold">norm</strong> (Euclidean norm) of the regression coefficients array, and <em class="italic">X</em> is the design matrix. Further simplification brings this to the following:</p>
<p>‖y − Xβ‖ 2 2 + 𝝀 β T β</p>
<p>which can be rewritten in the closed form by using the tuning parameter’s scalar multiple of the identity matrix to <a id="_idIndexMarker598"/>derive the <strong class="bold">ridge regression coefficient estimates</strong>,  ˆ β  R, as follows:</p>
<p> ˆ β  R = (X T X + 𝝀I) −1 X T y</p>
<p>Simply stated, the ridge regression coefficient estimates are the set of coefficients that minimizes the least squares regression output as follows:</p>
<p>  ˆ β  R = argmin{∑ i=1 n   (y i − ˆ β 0 − ∑ j=1 p  ˆ β  j x ij) 2 + 𝝀∑ j=1 p  ˆ β  j 2}</p>
<p class="callout-heading">Standardizing coefficients prior to applying ridge regression</p>
<p class="callout">Because ridge regression seeks to minimize the error for the entire dataset and the tuning parameter for doing so is applied within the L 2 norm normalization process of taking the root of summed squared terms, it is important to apply a standard scaler to each variable in the model so that all variables are on the same scale, prior to applying ridge regression. If this is not performed, ridge regression will almost certainly fail to be useful for helping the model generalize across datasets.</p>
<p>In<a id="_idIndexMarker599"/> summary, ridge regression reduces variance in a model by using the L 2 <strong class="bold">penalty</strong>, which penalizes the sum of squared coefficients. However, it is important to note the L 2 <strong class="bold">norm</strong>, and consequently, ridge regression, <strong class="bold">will never produce a zero-valued coefficient</strong>. Therefore, ridge regression is an excellent tool for reducing variance when the analyst seeks to use all terms in the model. However, ridge regression cannot be used for variable elimination. For variable elimination, we can use the LASSO regression shrinkage method, which uses the L 1 <strong class="bold">penalty</strong> to regularize coefficients with the L 1 <strong class="bold">norm</strong>, which uses the absolute value to shrink values to and including zero.</p>
<p> Let’s walk through <a id="_idIndexMarker600"/>an example of a ridge regression implementation in Python.</p>
<p>First, let’s load the Boston home prices dataset from <code>scikit-learn</code>. In the final line, we add the constant for the intercept. Note that this process can be repeated with comparable results, albeit with different input variables, using the California housing data set by running <code>from sklearn.datasets import fetch_california_housing</code> in place of <code>from sklearn.datasets </code><code>import load_boston</code>:</p>
<pre class="source-code">
from sklearn.metrics import mean_squared_error as MSE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston
import statsmodels.api as sm
import pandas as pd
boston_housing = load_boston()
df_boston = pd.DataFrame(boston_housing.data, columns = boston_housing.feature_names)
df_boston['PRICE'] = boston_housing.target
df_boston = sm.add_constant(df_boston, prepend=False</pre>
<p>The first three records are given here:</p>
<div><div><img alt="Figure 7.9 – First three records of the Boston housing data" height="204" src="img/B18945_07_009.jpg" width="1660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – First three records of the Boston housing data</p>
<p>We <a id="_idIndexMarker601"/>set <code>PRICE</code> as our target. Recall, ridge regression is most<a id="_idIndexMarker602"/> useful when there are either more parameters than samples or there is excessive variance. Let’s assume both of these assumptions are met:</p>
<pre class="source-code">
X = df_boston.drop('PRICE', axis=1)
y = df_boston['PRICE']</pre>
<p>As noted previously, <code>scikit-learn</code>’s <code>StandardScaler</code> function:</p>
<pre class="source-code">
sc = StandardScaler()
X_scaled = sc.fit_transform(X)</pre>
<p>Next, let’s take a 75/25 train/test split of the data for the model. We use <code>shuffle=True</code> to randomly shuffle the data so we test with a random sample, which is more likely to be representative of the population:</p>
<pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.25, shuffle=True)</pre>
<p>Now we can <a id="_idIndexMarker603"/>compare <code>fit_regularized</code>, we set the required argument for <code>method</code> to <code>'elastic_net'</code>. We will discuss Elastic Net shortly, but for now, note the <code>L1_wt</code> argument applies ridge regression <strong class="bold">when set to 0</strong>. Alpha is the tuning parameter, λ, in the ridge regression penalty term. A small alpha allows for large coefficients and a large alpha pushes the<a id="_idIndexMarker604"/> coefficients toward zero. Here, we fit the training data to derive the mean squared error:</p>
<pre class="source-code">
ols_model = sm.OLS(y_train, X_train)
compiled_model = ols_model.fit()
compiled_model_ridge = ols_model.fit_regularized(method = 'elastic_net', L1_wt=0, alpha=0.1,refit=True)
print('OLS Error: ', MSE(y_train,
    compiled_model.predict(X_train)) )
print('Ridge Regression Error: ', MSE(y_train,
    compiled_model_ridge.predict(X_train)))</pre>
<p>We can see ridge regression has a slightly higher amount of error</p>
<p><code>OLS Error:  </code><code>530.7235449265926</code></p>
<p><code>Ridge Regression Error:  </code><code>533.2278083730833</code></p>
<p>Next, we fit the test data to see how the model generalizes on unseen data. We measure again with the mean squared error:</p>
<pre class="source-code">
print('OLS Error: ', MSE(y_test, compiled_model.predict(X_test)) )
print('Ridge Regression Error: ', MSE(y_test, compiled_model_ridge.predict(X_test)))</pre>
<p>We can see that <a id="_idIndexMarker605"/>the number of errors increased for both on the <a id="_idIndexMarker606"/>test data. However, the OLS regression produced a slightly higher error in proportion to the ridge regression approach:</p>
<p><code>OLS Error:  </code><code>580.8138216493896</code></p>
<p><code>Ridge Regression Error:  </code><code>575.5186673728349</code></p>
<p>Here, we can observe the OLS regression coefficients and the regularized coefficients in <em class="italic">Figure 7</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 7.10 – OLS regression coefficients before and after ridge regression regularization" height="210" src="img/B18945_07_010.jpg" width="1643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – OLS regression coefficients before and after ridge regression regularization</p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor130"/>LASSO regression</h2>
<p>Earlier<a id="_idIndexMarker607"/> in this section, we mentioned how variable<a id="_idIndexMarker608"/> coefficients with unnecessarily high values contribute to model variance, and in so doing, take away a model’s ability to generalize as well as it could were the coefficients reasonable. We demonstrated applying ridge regression to enact this. A very popular alternative to ridge regression, however, is LASSO regression. LASSO regression follows a similar procedure of adding a penalty term to the model’s residual sum of squared error and seeks to minimize the resulting value (error). However, LASSO uses the L 1 norm rather than L 2. Consequently, it is possible to obtain absolute zero coefficient values as the tuning parameter reaches a sufficient size, thus functioning as a feature selection and shrinkage tool.</p>
<p>The LASSO equation seeks to minimize overall model error by shrinking each variable’s coefficient using the following method:</p>
<p>  ˆ β  L = argmin{∑ i=1 n   (y i − ˆ β 0 − ∑ j=1 p  ˆ β  j x ij) 2 + 𝝀∑ j=1 p | ˆ β  j| }</p>
<p>The only difference between LASSO and ridge regression is the penalty term | ˆ β  j|.</p>
<p class="callout-heading">Choosing λ</p>
<p class="callout">The value of<a id="_idIndexMarker609"/> the tuning parameter, λ, is best selected using cross-validation. While the tuning parameter can go to infinity, in theory, it is typical to start with values less than 1, such as at 0.1 increasing by increments of tenths up to 1. After, typically integer values are used.</p>
<p>Using the <a id="_idIndexMarker610"/>same data we used for ridge regression, we apply LASSO regression. Here, we set <code>L1_wt=1</code>, indicating the L 1 norm will be applied:</p>
<pre class="source-code">
ols_model = sm.OLS(y_train, X_train)
compiled_model = ols_model.fit()
compiled_model_lasso = ols_model.fit_regularized(method='elastic_net', L1_wt=1, alpha=0.1,refit=True)</pre>
<p>We will follow the same steps for LASSO as with ridge regression in that we first check the errors on the training data, then again on the test data to see how the models generalize:</p>
<pre class="source-code">
print('OLS Error: ', MSE(y_train, compiled_model.predict(X_train)) )
print('LASSO Regression Error: ', MSE(y_train, compiled_model_lasso.predict(X_train)))</pre>
<p>The output shows OLS regression slightly outperforming LASSO. This could be due to higher variance. However, practically speaking, the results are the same:</p>
<p><code>OLS </code><code>Error: 530.7235449265926</code></p>
<p><code>LASSO Regression </code><code>Error: 531.2440812254207</code></p>
<p>Now, we need to check the model’s performance on the holdout data:</p>
<pre class="source-code">
print('OLS Error: ', MSE(y_test, compiled_model.predict(X_test)) )
print('LASSO Regression Error: ', MSE(y_test, compiled_model_lasso.predict(X_test)))</pre>
<p>We can see the<a id="_idIndexMarker611"/> two models again have essentially the same error. However, with fewer features, the LASSO model may be easier to trust to generalize on future data. That may depend on the researcher’s level of subject knowledge, however:</p>
<p><code>OLS </code><code>Error: 546.1338399374557</code></p>
<p><code>LASSO Regression </code><code>Error: 546.716239805892</code></p>
<p>As with<a id="_idIndexMarker612"/> ridge regression, some of the coefficients have been minimized in the L 1 norm regularization process. However, we also see, using the same alpha value, four variables have been minimized to zero, thus eliminating them from the model altogether. Three of the features were comparatively shrunk almost to zero by ridge regression, but not <code>ZN</code>, which we see in <em class="italic">Figure 7</em><em class="italic">.11</em> has been reduced to <code>0</code>. The model error has been slightly improved, which may not appear significant, but when considering the elimination of four variables, we can consider the model to have more generalization with less dependence on exogenous variables.</p>
<div><div><img alt="Figure 7.11 – OLS regression coefficients before and after LASSO regression regularization" height="213" src="img/B18945_07_011.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – OLS regression coefficients before and after LASSO regression regularization</p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor131"/>Elastic Net</h2>
<p><strong class="bold">Elastic Net</strong> is <a id="_idIndexMarker613"/>another common shrinkage method that <a id="_idIndexMarker614"/>can be applied to manage a bias-variance trade-off. This method applies the tuning parameter to a combination of ridge and LASSO regression where the proportion of influence from either is determined by the hyperparameter α. The equation Elastic Net minimizes is as follows:</p>
<p>( ˆ β  0,  ˆ β ) = argmin⎧ ⎪ ⎨ ⎪ ⎩ 1 _ 2n  ∑ i=1 n (y i −  ˆ β  0 − ∑ j=1 p  ˆ β  j x ij) 2 + λ( 1 − α _ 2  ∑ j=1 p  ˆ β  j 2 + α∑ j=1 p | ˆ β  j|)⎫ ⎪ ⎬ ⎪ ⎭</p>
<p>Naturally, depending on the values of α, Elastic Net can also generate absolute zero-valued coefficient parameter estimates where it cancels out the ridge regression penalty term. When all input variables are needed – for example, if they have already been pruned following procedures such as those outlined in the <em class="italic">Feature selection</em> section of this chapter – Elastic Net is most likely to outperform both ridge and LASSO regression, especially when there are highly correlated features in the dataset that must be included to capture necessary variance. In the next section, we will discuss dimension reduction. Specifically, we p  rovide an in-depth overview of PCR, which uses <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) to<a id="_idIndexMarker615"/> extract useful information from systems that contain correlated features required to evaluate the target.</p>
<p>First, let’s walk through a regression example using Elastic Net using the same data we used for ridge and LASSO regression. Following the previous Elastic Net minimizing equation, we set <code>Lt_wt=0.5</code>, meaning an equal, 50/50 balance between ridge and LASSO regression. Differently, however, we applied <code>alpha=8</code> instead of <code>0.1</code> as we used in ridge and LASSO regression to gain an improvement over the OLS regression coefficients. Recall that as the tuning parameter approaches infinity, coefficients approach 0. Therefore, we can conclude based on the Elastic Net coefficients that 8 is a very high tuning parameter:</p>
<pre class="source-code">
ols_model = sm.OLS(y_train, X_train)
compiled_model = ols_model.fit()
compiled_model_elastic = ols_model.fit_regularized(method='elastic_net', L1_wt=0.5, alpha=8,refit=True)</pre>
<p>Let’s test the model on the training data:</p>
<pre class="source-code">
print('OLS Error: ', MSE(y_train, compiled_model.predict(X_train)) )
print('Elastic Net Regression Error: ', MSE(y_train, compiled_model_elastic.predict(X_train)))</pre>
<p>Here, we see <a id="_idIndexMarker616"/>Elastic Net has added errors into the model compared to OLS regression:</p>
<p><code>OLS </code><code>Error: 530.7235449265926</code></p>
<p><code>Elastic Net Regression </code><code>Error: 542.678919923863</code></p>
<p>Now let’s check the model errors for the holdout data:</p>
<pre class="source-code">
print('OLS Error: ', MSE(y_test, compiled_model.predict(X_test)) )
print('Elastic Net Regression Error: ', MSE(y_test, compiled_model_elastic.predict(X_test)))</pre>
<p>Observing <a id="_idIndexMarker617"/>the results in <em class="italic">Figure 7</em><em class="italic">.12</em>, we can see how Elastic Net has traded variance – which increased the error on training – for bias, which has enabled the model to generalize better on holdout. We can see better results with Elastic Net than OLS regression on holdout. However, the improved error suggests the added bias provided a better chance of lower error, but is not something to be expected:</p>
<p><code>OLS </code><code>Error: 546.1338399374557</code></p>
<p><code>Elastic Net Regression </code><code>Error: 514.8301731640446</code></p>
<p>Here we can see the coefficients before and after Elastic Net’s implementation:</p>
<div><div><img alt="Figure 7.12 – OLS regression coefficients before and after Elastic Net regularization" height="233" src="img/B18945_07_012.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – OLS regression coefficients before and after Elastic Net regularization</p>
<p>We can see with a<a id="_idIndexMarker618"/> sufficiently large tuning parameter value (<code>alpha=8</code>), most of <a id="_idIndexMarker619"/>the coefficients with the balanced Elastic Net have been forced to absolute zero. The only coefficients remaining had comparatively large values with OLS regression. Notably, for the variables that remain with coefficients (<code>RM</code> and <code>LSTAT</code>), Elastic Net increased both coefficients where ridge regression and LASSO either reduced them slightly.</p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor132"/>Dimension reduction</h1>
<p>In this section, we <a id="_idIndexMarker620"/>will use a specific technique – <strong class="bold">PCR</strong> – to study MLR. This technique is useful when we need to deal with a multicollinearity data issue. Multicollinearity occurs when an independent variable is highly correlated with another independent variable, or an independent variable can be predicted from another independent variable in a regression model. A high correlation can affect the result poorly when fitting a model.</p>
<p>The<a id="_idIndexMarker621"/> PCR technique is based on PCA as used in unsupervised machine learning for data compression and exploratory analysis. The idea behind it is to use the dimension reduction technique, PCA, on these original variables to create new uncorrelated variables. The information obtained on these new variables helps us to understand the relationship and then apply the MLR algorithm to these new variables. The PCA technique can also be used in a classification problem, which we will discuss in the next chapter.</p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor133"/>PCA – a hands-on introduction</h2>
<p>PCA is a<a id="_idIndexMarker622"/> dimension reduction technique based on linear <a id="_idIndexMarker623"/>algebra by linearly transforming data using linear combinations of original variables into a new coordinate system. These new linear combinations are <a id="_idIndexMarker624"/>called <strong class="bold">principal components</strong> (<strong class="bold">PCs</strong>). The difference between the PCA technique and the feature selection technique or shrinkage methods mentioned in previous sections is that the original independent variables are maintained but the new PC variables are transformed into a new coordinate space. In other words, PCA uses the original data to arrive at a new representation or a new structure. The number of PC variables is the same as the number of original independent variables, but these new PC variables are uncorrelated with each other. The PC variables are created and ordered from the most to the least amount of variability. The sum of variances is the same between the original independent variables and the newly transformed PC variables.</p>
<p>Before <a id="_idIndexMarker625"/>conducting PCA, we perform pre-processing for the dataset by subtracting the mean from each data point and normalizing the standard deviation of each independent variable. In a high-level structure, the goal of the PCA technique is to find vectors v 1, v 2, … , v k such that any data point, x, in the dataset can be approximately represented as a linear combination:</p>
<p>x = ∑ i=1 k a i v i</p>
<p>for some constants a i with i =  ‾ 1, k . Assume that we have these k vectors; then, each data point can be written as a vector in R k corresponding the projections:</p>
<p>x = &lt;x, v 1&gt; ⋅ v 1 + &lt;x, v 2&gt; ⋅ v 2 + …  +  &lt;x, v k&gt; ⋅ v k</p>
<p>In other<a id="_idIndexMarker626"/> words, if we have d original independent variables, we will construct a d × k−dimensional transformation matrix that can map any data point onto a new k-dimensional variable subspace with k smaller than d. It means that we have performed a dimension reduction by a linear transformation of the original independent variables. There are several applications of PCA, but here, we will cite a paper, <em class="italic">Gene mirror geography within Europe</em>, published in <em class="italic">Nature (2008)</em> (<a href="https://pubmed.ncbi.nlm.nih.gov/18758442/">https://pubmed.ncbi.nlm.nih.gov/18758442/</a>). Authors considered a sample of 3,000 European individuals genotyped at over half a million variable DNA sites in the human genome; then, each individual was represented using more than half a million genetic markers. This means that it produced a matrix with dimensions larger than 3,000 x 500,000. They performed a PCA on the dataset to find the most meaningful vectors, v 1 and v 2 (the first and second components), where each person only corresponds to two numbers. The authors plotted each person based on two numbers in a two-dimensional plane and then colored each point according to the country they came from.</p>
<div><div><img alt="Figure 7.13 – PCA analysis for gene mirror geography within Europe" height="575" src="img/B18945_07_013.jpg" width="746"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – PCA analysis for gene mirror geography within Europe</p>
<div><div><img alt="Figure 7.14 – PCA analysis for gene mirror geography within Europe" height="391" src="img/B18945_07_014.jpg" width="757"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – PCA analysis for gene mirror geography within Europe</p>
<p>Surprisingly, the <a id="_idIndexMarker627"/>PCA technique performed well in the plots by <a id="_idIndexMarker628"/>showing genetic similarities that are similar to the European map.</p>
<p>In the next part, we will discuss how to use PCA to conduct a PCR analysis in practice.</p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>PCR – a hands-on salary prediction study</h2>
<p>To <a id="_idIndexMarker629"/>conduct a PCR analysis, we first perform a PCA to obtain the PCs and then decide to keep the first k PCs that contain the most explainable amount of variability. Here, k is the dimensionality of the new PC variable space. Finally, we fit MLR on <a id="_idIndexMarker630"/>these new variables.</p>
<p>We will consider a hands-on salary prediction task from the open source Kaggle data – <a href="https://www.kaggle.com/datasets/floser/hitters">https://www.kaggle.com/datasets/floser/hitters</a> – to illustrate the PCR method. If following along, please download the dataset from the Kaggle URL:</p>
<ol>
<li><strong class="bold">Setting up and loading </strong><strong class="bold">the data</strong></li>
</ol>
<p>Import the necessary libraries to be used in this study and loading the Hitters data. For simplicity, we will drop all missing values in the dataset. There are 19 independent variables (16 numerical and 3 categorical) with the target ‘<code>Salary</code>’. The categorical independent variables ‘<code>League</code>’, ‘<code>Division</code>’, and ‘<code>NewLeague</code>’ are converted into dummy variables. We preprocess and standardize the<a id="_idIndexMarker631"/> features, and create a train and a test set <a id="_idIndexMarker632"/>before conducting the PCA step:</p>
<pre class="source-code">
# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
#location of dataset
url = "/content/Hitters.csv"
#read in data
data = pd.read_csv(url).dropna() # to simply the analysis, we drop all missing values
# create dummies variables
dummies_variables = pd.get_dummies(data[['League', 'Division', 'NewLeague']])
# create features and target
target = data['Salary']
feature_to_drop = data.drop(['Salary', 'League', 'Division', 'NewLeague'],axis=1).astype('float64')
X = pd.concat([feature_to_drop, dummies_variables[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)
#scaled data - preprocessing
X_scaled = scale(X)
# train test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.2, random_state=42)</pre>
<ol>
<li value="2"><strong class="bold">Generating </strong><strong class="bold">all PCs</strong></li>
</ol>
<p>As the <a id="_idIndexMarker633"/>next <a id="_idIndexMarker634"/>step, we generate all the PCs for the training set. This performance produces 19 new PC variables because there are 19 original independent variables:</p>
<pre class="source-code">
# First generate all the principal components
pca = PCA()
X_pc_train = pca.fit_transform(X_train)
X_pc_train.shape</pre>
<ol>
<li value="3"><strong class="bold">Determining the best number of PCs to </strong><strong class="bold">be used</strong></li>
</ol>
<p>The next step is to perform a 10-fold cross-validation MLR and choose the best number of PCs to use by using the RMSE:</p>
<pre class="source-code">
# Define cross-validation folds
cv = KFold(n_splits=10, shuffle=True, random_state=42)
model = LinearRegression()
rmse_score = []
# Calculate MSE score - based on 19 PCs
for i in range(1, X_pc_train.shape[1]+1):
    rmse = -cross_val_score(model, X_pc_train[:,:i], y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
    rmse_score.append(rmse)
# Plot results
plt.plot(rmse_score, '-o')
plt.xlabel('Number of principal components in regression')
plt.ylabel('RMSE')
plt.title('Salary')
plt.xlim(xmin=-1)
plt.xticks(np.arange(X_pc_train.shape[1]), np.arange(1, X_pc_train.shape[1]+1))
plt.show()</pre>
<p>Here’s<a id="_idIndexMarker635"/> the <a id="_idIndexMarker636"/>plot produced:</p>
<div><div><img alt="Figure 7.15 – Number of PCs" height="278" src="img/B18945_07_015.jpg" width="389"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Number of PCs</p>
<p>From this, we <a id="_idIndexMarker637"/>see that the best number of PCs is 6, corresponding <a id="_idIndexMarker638"/>with the lowest cross-validation RMSE.</p>
<ol>
<li value="4"><strong class="bold">Retraining the model and </strong><strong class="bold">performing prediction</strong></li>
</ol>
<p>We will use this number to train a regression model on the training data and make predictions on the test data:</p>
<pre class="source-code">
# Train regression model on training data
model = LinearRegression()
model.fit(X_pc_train[:,:6], y_train)
pcr_score_train = -cross_val_score(model, X_pc_train[:,:6], y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
# Prediction with test data
X_pc_test = pca.fit_transform(X_test)[:,:6]
pred = model.predict(X_pc_test)
pcr_score_test = mean_squared_error(y_test, pred, squared=False)</pre>
<p>Remark that <a id="_idIndexMarker639"/>PCR analysis is more difficult to interpret the results of <a id="_idIndexMarker640"/>than feature selection or shrinkage methods, and this analysis performance is better if the few first PCs capture the most explainable amount of variability.</p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor135"/>Summary</h1>
<p>In this chapter, we discussed the concept of MLR and topics aiding in its implementation. These topics included feature selection methods, shrinkage methods, and PCR. Using these tools, we were able to demonstrate approaches to reduce the risk of modeling excess variance. In doing so, we were able to also induce model bias so that models can have a better chance of generalizing on unseen data with minimal complications as frequently faced when overfitting.</p>
<p>In the next chapter, we will begin a discussion on classification with the introduction of logistic regression, which fits a sigmoid to a linear regression model to derive probabilities of binary class membership.</p>
</div>
</div>

<div><div><h1 id="_idParaDest-132" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor136"/>Part 3:Classification Models</h1>
<p>In this part, we discuss the types of problems that can be solved with classification, coefficients of correlation and determination, multivariate modeling, model selection and variable adjustment with regularization.</p>
<p>It includes the following chapters:</p>
<ul>
<li><a href="B18945_08.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Discrete Models</em></li>
<li><a href="B18945_09.xhtml#_idTextAnchor148"><em class="italic">Chapter 9</em></a>, <em class="italic">Discriminant Analysis</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</div></body></html>