<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer110">
<h1 class="chapter-number" id="_idParaDest-114"><a id="_idTextAnchor118"/>7</h1>
<h1 id="_idParaDest-115"><a id="_idTextAnchor119"/>Multiple Linear Regression</h1>
<p>In the last chapter, we discussed <strong class="bold">simple linear regression</strong> (<strong class="bold">SLR</strong>) using<a id="_idIndexMarker529"/> one variable to explain a target variable. In this chapter, we will discuss <strong class="bold">multiple linear regression</strong> (<strong class="bold">MLR</strong>), which<a id="_idIndexMarker530"/> is a model that leverages multiple explanatory variables to model a response variable. Two of the major conundrums facing multivariate modeling are multicollinearity and the bias-variance trade-off. Following an overview of MLR, we will provide an induction into the methodologies used for evaluating and minimizing multicollinearity. We will then discuss methods for leveraging the bias-variance trade-off to our benefit as analysts. Finally, we will discuss handling multicollinearity using <strong class="bold">Principal Component Regression</strong> (<strong class="bold">PCR</strong>) to <a id="_idIndexMarker531"/>minimize overfitting without removing features but rather transforming <span class="No-Break">them instead.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>Multiple <span class="No-Break">linear regression</span></li>
<li><span class="No-Break">Feature selection</span></li>
<li><span class="No-Break">Shrinkage methods</span></li>
<li><span class="No-Break">Dimension reduction</span></li>
</ul>
<h1 id="_idParaDest-116"><a id="_idTextAnchor120"/>Multiple linear regression</h1>
<p>In the <a id="_idIndexMarker532"/>previous chapter, we discussed SLR. With SLR, we were able to predict the value of a variable (commonly called the response variable, denoted as <em class="italic">y</em>) using another variable (commonly called the explanatory variable, denoted as <em class="italic">x</em>). The SLR model is expressed by the following equation where <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> is the intercept term and <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> is the slope of the <span class="No-Break">linear model.</span></p>
<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ϵ</span></p>
<p>While this is a useful model, in many problems, multiple explanatory variables could be used to predict the response variable. For example, if we wanted to predict home prices, we might want to consider many variables, which may include lot size, the number of bedrooms, the number of bathrooms, and overall size. In this situation, we can expand the previous model to include these additional variables. This is called MLR. The MLR model can be expressed with the <span class="No-Break">following equation.</span></p>
<p><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ϵ</span></p>
<p>Like <a id="_idIndexMarker533"/>the previous equation, <span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> represents an intercept. In this equation, in addition to the intercept value, we have a <span class="_-----MathTools-_Math_Variable">β</span> parameter for each explanatory variable. Each explanatory variable is denoted as <em class="italic">x</em> with a numerical subscript. We can include as many explanatory variables as desired in the model, which is why the equation shows a final subscript <span class="No-Break">of </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break">.</span></p>
<p class="callout-heading">Impacts of data sizes on MLR</p>
<p class="callout">In general, we can include as many explanatory variables as desired in the model. However, there are some realistic limits. One of these potential limits is the number of samples relative to the number of explanatory variables. MLR tends to work best when the number of samples (N) is much less than the number of explanatory variables (P). As P approaches N, the model becomes difficult to estimate. If P is large compared to N, it would be wise to consider dimension reduction, which will be discussed later in <span class="No-Break">this chapter.</span></p>
<p>Let’s look at an example. The Python package <strong class="source-inline">scikit-learn</strong> contains several datasets for practice with modeling. We will utilize the <strong class="source-inline">diabetes</strong> dataset, which includes vital measurements and quantitative measures of the disease progression after one year. More information about this dataset can be found at this link: <a href="https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset">https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset</a>.We will attempt to model the relationship between the vital statistics and the <span class="No-Break">disease progression.</span></p>
<p>The explanatory variables in this dataset are <strong class="source-inline">age</strong>, <strong class="source-inline">bmi</strong>, <strong class="source-inline">bp</strong>, <strong class="source-inline">s1</strong>, <strong class="source-inline">s2</strong>, <strong class="source-inline">s3</strong>, <strong class="source-inline">s4</strong>, <strong class="source-inline">s5</strong>, and <strong class="source-inline">s6</strong>. The response variable is a measurement of disease progression. This is how we would express this <span class="No-Break">model mathematically:</span></p>
<p><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">4</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">s</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">7</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Number">4</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">8</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">9</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">ϵ</span></p>
<p>As discussed previously, we have a <span class="_-----MathTools-_Math_Variable">β</span> for each variable in <span class="No-Break">the model.</span></p>
<p class="callout-heading">Transformations on the diabetes dataset</p>
<p class="callout">The explanatory variables in this dataset have been transformed from the original measurements. Referring to the dataset user’s guide, we can see that each variable has been mean-centered and scaled. In addition, the <strong class="source-inline">sex</strong> variable has been converted from a categorical variable into a <span class="No-Break">numerical variable.</span></p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor121"/>Adding categorical variables</h2>
<p>Up until this point, we<a id="_idIndexMarker534"/> have <a id="_idIndexMarker535"/>only considered continuous variables in our model, that is, variables that can have any value on the number line. Thinking back to <a href="B18945_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Distributions of Data</em>, the variables we have considered before now have been ratio data. However, the linear regression model is not limited to using continuous variables. We can include categorical variables in the model. Recall from <a href="B18945_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">, Distributions of Data</em>, categorical variables are associated with groups of items. In statistical learning, the groups are generally<a id="_idIndexMarker536"/> called <strong class="bold">levels</strong> of the variable. For example, if we had five students, three with MS degrees, one with a PhD degree, and one with a BS degree, we would say that the student categorical variable has three levels: BS, MS, and PhD. Let’s look at how we include categorical variables in <span class="No-Break">the model.</span></p>
<p>We will include the categorical variable in the model by including additional <span class="_-----MathTools-_Math_Variable">β</span> terms. Specifically, we will add the number of levels (<span class="_-----MathTools-_Math_Variable">L</span>) minus one additional <span class="_-----MathTools-_Math_Variable">β</span> term. For the student example with three levels, we would add two additional <span class="_-----MathTools-_Math_Variable">β</span> terms. Using one less than the number of levels will allow us to choose one level as a reference (or baseline) and compare the other levels to that reference level. We include the <span class="_-----MathTools-_Math_Variable">β</span> terms in the model just like the other <span class="_-----MathTools-_Math_Variable">β</span> terms. Let’s say in this example, we are modeling income level, and in addition to the education levels, we have the experience of the individuals. Then, we would construct our model <span class="No-Break">like this:</span></p>
<p><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">h</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">d</span></span></p>
<p>With the addition of the beta terms, we also must make a data transformation. We will have to create <strong class="bold">dummy variables</strong> for<a id="_idIndexMarker537"/> each of the <em class="italic">non-reference</em> levels in the categorical variable. This process is called the dummification of the categorical variable. In dummification, we create a column for each non-reference level and insert ones where the level occurs in the original variable and zeros where it does not appear in the original variable. This process is demonstrated in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 7.1 – Dummification of a categorical variable" height="144" src="image/B18945_07_001.jpg" width="396"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Dummification of a categorical variable</p>
<p>The <a id="_idIndexMarker538"/>original variable contains<a id="_idIndexMarker539"/> the BS, MS, and PhD levels. BS is chosen as the reference level and two dummy variables are created for the levels MS and PhD. This process of creating additional columns for categorical variables is called <strong class="bold">encoding</strong>. There <a id="_idIndexMarker540"/>are many types of variable encodings and encoding is a widely used technique in statistical learning and <span class="No-Break">machine learning.</span></p>
<p class="callout-heading">Categorical levels are shifts by a constant</p>
<p class="callout">Let’s look a little deeper at the impact of categorical variables on the linear regression model. Earlier, we discussed how categorical variables are encoded with dummy variables that take on values of zero and one. Essentially, that means that we are switching the associated <span class="_-----MathTools-_Math_Variable">β</span> terms, depending on the level, which will shift the response by a constant. For example, in the income model, when we want to calculate the income of an individual with a BS degree, the model resolves to <span class="No-Break">the following:</span></p>
<p class="callout"><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">r</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">c</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span></p>
<p class="callout">And when we want to calculate the income of an individual with an MS degree, the model resolves to <span class="No-Break">the following:</span></p>
<p class="callout"><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">β</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p class="callout">This means that levels of categories only shift the output of the model from the reference level by a constant, the beta associated with <span class="No-Break">the level.</span></p>
<p>Returning to<a id="_idIndexMarker541"/> our previous example, our dataset includes a categorical variable: the <strong class="source-inline">sex</strong> of the patient. The dataset <a id="_idIndexMarker542"/>includes two levels for <strong class="source-inline">sex</strong>. We will choose one level to serve as a reference and we will create a dummy variable for the other level. With this knowledge, let’s fit this data to a linear regression model and discuss the assumptions of multiple <span class="No-Break">linear regression.</span></p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor122"/>Evaluating model fit</h2>
<p>Whenever <a id="_idIndexMarker543"/>we fit a parametric model, we should verify that the model assumptions are met. As discussed in the previous chapter, if the <a id="_idIndexMarker544"/>model assumptions are not met, the model may provide misleading results. In the previous chapter, we discussed the four assumptions for simple <span class="No-Break">linear regression:</span></p>
<ul>
<li>A linear relationship between the response variable and <span class="No-Break">explanatory variables</span></li>
<li>Normality of <span class="No-Break">the residuals</span></li>
<li>Homoscedasticity of <span class="No-Break">the residuals</span></li>
<li><span class="No-Break">Independent samples</span></li>
</ul>
<p>These assumptions also apply here, but in this new modeling context, we have an additional assumption: no or<a id="_idIndexMarker545"/> little <strong class="bold">multicollinearity</strong>. Multicollinearity occurs when two or more of the explanatory variables in an MLR model are highly correlated. The presence of multicollinearity impacts the statistical significance of the independent variables. Ideally, all the explanatory variables will be uncorrelated (or linearly independent). However, we can accept a small amount of multicollinearity without much impact on <span class="No-Break">the model.</span></p>
<p>Let’s evaluate the model on each of <span class="No-Break">these assumptions.</span></p>
<h3>Linear relationships</h3>
<p>To<a id="_idIndexMarker546"/> check for linear relationships between the response variable and the explanatory variable, we can look at scatter plots of each variable against the response variable. Based on the plots in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, several of the variables, including <strong class="source-inline">bmi</strong>, <strong class="source-inline">bp</strong>, <strong class="source-inline">s3</strong>, <strong class="source-inline">s4</strong>, and <strong class="source-inline">s5</strong>, possibly exhibit a linear relationship with the <a id="_idIndexMarker547"/><span class="No-Break">response variable.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 7.2 – Scatter plots of the response variable against the explanatory variables" height="1315" src="image/B18945_07_002.jpg" width="717"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Scatter plots of the response variable against the explanatory variables</p>
<p>While it would be ideal if any of the variables showed a strong linear relationship with the response variable, in an MLR model, we have the benefit of the combination of variables. We <a id="_idIndexMarker548"/>can add variables that appear to be more useful and remove variables that do not appear to be useful. This is called <strong class="bold">feature selection</strong>, which <a id="_idIndexMarker549"/>we will cover in the <span class="No-Break">next section.</span></p>
<h3>Normality of the residuals</h3>
<p>Recall <a id="_idIndexMarker550"/>from the previous chapter that we expect the residuals from a well-fitted model to appear randomly distributed. We could look at this with a histogram or a QQ plot. The residuals from our model are shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 7.3 – Residual value" height="438" src="image/B18945_07_003.jpg" width="621"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Residual value</p>
<p>Based on the histogram in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, we cannot reject the possibility that the residuals are normally distributed. In general, when evaluating this assumption, we are checking for <em class="italic">egregious</em> violations. It turns out that the linear regression is relatively robust against this assumption. However, that does not mean this assumption can <span class="No-Break">be ignored.</span></p>
<h3>Homoscedasticity of the residuals</h3>
<p>In the<a id="_idIndexMarker551"/> previous chapter, we also discussed homoscedasticity. For a well-fitted model, we expect that the residuals should exhibit homoscedasticity. The plot in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em> shows the scatter plot of the residuals against the values predicted by <span class="No-Break">the model.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 7.4 – Scatter plot of model residuals for the predicted versus actual values" height="442" src="image/B18945_07_004.jpg" width="682"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Scatter plot of model residuals for the predicted versus actual values</p>
<p>There <a id="_idIndexMarker552"/>does not appear to be a clear pattern of changing variance or a significant outlier that would violate the assumption of homoscedasticity. If there had been a pattern, that would have been a sign that one or more of the variables may need to be transformed, or a sign of a non-linear relationship between the response and the <span class="No-Break">explanatory variables.</span></p>
<h3>Independent samples</h3>
<p>In the <a id="_idIndexMarker553"/>last chapter, we discussed independent sampling and its impact on this type of model. However, we cannot make a certain determination on whether the samples are independent without knowing the sampling methodology. Since we do not know the sampling strategy for this dataset, we will assume this assumption is met and proceed with the model. In a real modeling setting, this assumption should never be taken <span class="No-Break">for granted.</span></p>
<h3>Multicollinearity</h3>
<p>The new <a id="_idIndexMarker554"/>assumption for MLR is that there is little or no multicollinearity in the explanatory variables. Multicollinearity is a situation that occurs when two or more variables are strongly linearly correlated. We commonly use<a id="_idIndexMarker555"/> the <strong class="bold">variance inflation factor</strong> (<strong class="bold">VIF</strong>) to detect multicollinearity. The VIF is a measurement of how much the coefficient of an explanatory variable is influenced by other explanatory variables. A lower VIF is better where the minimum value is 1, meaning there is no correlation. We generally consider a VIF of 5 or more to be too high. When a high VIF is detected in a set of explanatory variables, we repeatedly remove the variable with the highest VIF until the VIF values for each variable are below 5. Let’s look at an example with our <a id="_idIndexMarker556"/>current data. The process of removing variables with high VIFs is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 7.5 – Removing high VIF variables from a dataset" height="440" src="image/B18945_07_005.jpg" width="912"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Removing high VIF variables from a dataset</p>
<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em> shows the process of removing variables with a high VIF from the diabetes dataset. The leftmost table in the figure shows the original dataset where the highest VIF is 59.2, which corresponds to variable S1. Then, we remove this variable from the dataset and recalculate the VIF. Now we see that the highest VIF is 7.8, corresponding to variable <strong class="source-inline">s4</strong>. We remove this variable and recalculate the VIF. Now, all VIFs are below 5, indicating that there is a low correlation between the remaining variables. With these variables removed, we need to fit the <span class="No-Break">model again.</span></p>
<p>With the model fit and the assumptions of the model verified, let’s look at the fit results and discuss how to interpret <span class="No-Break">the results.</span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor123"/>Interpreting the results</h2>
<p>Fitting<a id="_idIndexMarker557"/> the model, we get the following <a id="_idIndexMarker558"/>results from <strong class="source-inline">statsmodels</strong>. The output is divided into <span class="No-Break">three sections:</span></p>
<ul>
<li>The top section contains high-level statistics about <span class="No-Break">the model</span></li>
<li>The middle section contains details about the <span class="No-Break">model coefficients</span></li>
<li>The bottom section contains diagnostic tests about the data and <span class="No-Break">the residuals</span></li>
</ul>
<p>Let’s walk<a id="_idIndexMarker559"/> through each section of <span class="No-Break">this model.</span></p>
<p class="IMG---Figure"><img alt="Figure 7.6 – Results from statsmodels OLS regression" height="552" src="image/B18945_07_006.png" width="773"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Results from statsmodels OLS regression</p>
<h3>High-level statistics and metrics (top section)</h3>
<p>In<a id="_idIndexMarker560"/> the top section of the fit results, we have model-level information. The left side of the top section contains information about the model such as the <strong class="bold">degrees of freedom</strong> (<strong class="bold">df</strong>) and the number of observations. The right side of the top section contains model metrics. Model metrics are useful for comparing models. We will discuss more about model metrics in the section on <span class="No-Break">feature selection.</span></p>
<h3>Model coefficient details</h3>
<p>The middle section contains details about the model coefficients (the <span class="_-----MathTools-_Math_Variable">β</span> terms in the equations listed previously). For the purposes of this section, we will focus on two columns in the middle section: <strong class="source-inline">coef</strong> and <strong class="source-inline">P&gt;|t|</strong>. The <strong class="source-inline">coef</strong> column is the model coefficient estimated for the model equation (the estimate of <span class="_-----MathTools-_Math_Variable">β</span> or termed <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span>). The column labeled <strong class="source-inline">P&gt;|t|</strong> is the p-value for a significance test for the coefficient. We are interested in both columns for interpreting the model. Let’s start with the <span class="No-Break">p-value column.</span></p>
<p>The null hypothesis for this test is that <strong class="bold">the value of the </strong><span class="_-----MathTools-_Math_Variable">β</span><strong class="bold"> parameter is equal to zero</strong>. Recall the following from <a href="B18945_03.xhtml#_idTextAnchor055"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, </em><span class="No-Break"><em class="italic">Hypothesis Testing</em></span><span class="No-Break">:</span></p>
<ul>
<li>A p-value below the significance threshold means we reject the <span class="No-Break">null hypothesis</span></li>
<li>A p-value above the significance threshold means that we fail to reject the <span class="No-Break">null hypothesis</span></li>
</ul>
<p>In this <a id="_idIndexMarker561"/>example, we would reject the null hypothesis for the <strong class="source-inline">bmi</strong> variable, but we fail to reject the null hypothesis for the age variable. Once we have determined the significant variables, we can move on to interpreting the meaning of the significant variables. We will not be able to provide an interpretation of the other variables because we cannot reject that their coefficient values might be zero. If the coefficient value is zero, then the variable makes no contribution to the model. Let’s look at how to interpret <span class="No-Break">the coefficients.</span></p>
<p>Often, when we construct a model, we want to understand how the parts of the model affect the output. In an MLR model, this comes down to understanding the coefficients of the model. We have two types of variables, continuous and categorical, and the associated coefficients have <span class="No-Break">different meanings.</span></p>
<h4>Interpreting continuous variable coefficients</h4>
<p>For the <a id="_idIndexMarker562"/>continuous variables, such as <strong class="source-inline">BMI</strong>, as the value of the variable increases, so does its significance to the output of the model. A unit increase in the value of the variable is associated with an increase in the mean of the dependent variable by the size of the coefficient with all other variables held constant. Let’s take the <strong class="source-inline">bmi</strong> variable as an example. Since the coefficient of the <strong class="source-inline">bmi</strong> variable is approximately 526, we would say, “A unit increase in <strong class="source-inline">bmi</strong> would be associated with a 526 increase in the mean of the diabetes measurement with all other variables held constant.” Of course, the coefficients can also take on <span class="No-Break">negative values.</span></p>
<h4>Interpreting categorical variable coefficients</h4>
<p>For the <a id="_idIndexMarker563"/>categorical variables, such as <strong class="source-inline">sex</strong>, recall that, unlike the continuous variables, the values of the categorical are dummy-encoded, and therefore can only take on two values: zero and one. Also, recall that one level was chosen to be the reference level. In this case, <strong class="source-inline">sex</strong> level 0 is the reference level, and we can compare this reference to <strong class="source-inline">sex</strong> level 1. When we use the reference level, the coefficient will not affect the output of the model. Thus, the categorical-level change is associated with a change in the mean of the dependent variable by the size of the coefficient with all other variables held constant. Since the coefficient of the <strong class="source-inline">sex</strong> variable is approximately -22, we would say, “The level of <strong class="source-inline">sex</strong> is associated with a decrease of 22 in the mean of the diabetes measurement compared to the reference level with all other variables <span class="No-Break">held constant.”</span></p>
<h3>Diagnostic tests</h3>
<p>The <a id="_idIndexMarker564"/>bottom section of the fit results contains diagnostic statistics for the data and residuals. Glancing over the list, several should be from <a href="B18945_06.xhtml#_idTextAnchor104"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Simple Linear Regression</em>. The Durbin-Watson test is a test for serial correlation (data sampled sequentially over time). A result around 2 is not indicative of serial correlation. The skew and kurtosis are measurements of the shape of the distribution of the residuals. These results indicate almost no skew, but possibly some kurtosis. These are likely small deviations from the normal distribution and are not cause for concern as we saw in the <span class="No-Break">plots earlier.</span></p>
<p>In this section, we looked at our first model that can use multiple explanatory variables to predict a response variable. However, we noticed that several of the variables included in the model were not statistically significant. For this model, we only selected features by removing any features that had high VIF scores, but there are other methods to consider when choosing features. In the next section, we will discuss comparing models and <span class="No-Break">feature selection.</span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>Feature selection</h1>
<p>The are<a id="_idIndexMarker565"/> many factors that influence the success or failure of a model, such as sampling, data quality, feature creation, and model selection, several of which we have not covered. One of those critical factors is <strong class="bold">feature selection</strong>. Feature selection is simply the process of choosing or systematically determining the best features for a model from an existing set of features. We have done some simple feature selection<a id="_idIndexMarker566"/> already. In the previous section, we removed features that had high VIFs. In this section, we will look at some methods for feature selection. The methods presented in this section fall into two categories: statistical methods for feature selection and performance-based methods for feature selection. Let’s start with <span class="No-Break">statistical methods.</span></p>
<h2 id="_idParaDest-121"><a id="_idTextAnchor125"/>Statistical methods for feature selection</h2>
<p>Statistical methods <a id="_idIndexMarker567"/>for feature selection rely on the<a id="_idIndexMarker568"/> primary tool that we have used throughout the previous chapters: statistical significance. The methods presented in this sub-section will be based on the statistical properties of the features themselves. We will cover two statistical methods for feature selection: correlation and <span class="No-Break">statistical significance.</span></p>
<h3>Correlation</h3>
<p>The <a id="_idIndexMarker569"/>first statistical method we will discuss is <strong class="bold">correlation</strong>. We have discussed correlation in this chapter and in previous chapters; recall that correlation is a description of the relationship between two variables. Variables can be positively correlated, uncorrelated, or negatively correlated. In terms of feature selection, we want to <em class="italic">remove features that are uncorrelated with the response variable</em>. A feature that is uncorrelated with the response variable does not have a relationship with the response variable. Thus, an uncorrelated feature would not be a good predictor of the <span class="No-Break">response variable.</span></p>
<p>Recall from <a href="B18945_04.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><em class="italic">, Parametric Tests</em>, that we can use Pearson’s correlation coefficient to measure the linear correlation between two variables. In fact, we can calculate the correlation coefficient between all features and the target variable. After performing those calculations, we can construct a correlation ranking as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="Figure 7.7 – Feature correlation ranking" height="467" src="image/B18945_07_007.jpg" width="650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Feature correlation ranking</p>
<p>When <a id="_idIndexMarker570"/>evaluating features based on correlation, we are most interested in features with a high <em class="italic">absolute</em> correlation. For example, the correlation ranking in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.7</em> shows <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="source-inline">bmi</strong> and <strong class="source-inline">s5</strong> exhibit a strong correlation with the <span class="No-Break">response variable</span></li>
<li><strong class="source-inline">bp</strong>, <strong class="source-inline">s4</strong>, <strong class="source-inline">s6</strong>, and <strong class="source-inline">s3</strong> exhibit moderate correlation with the <span class="No-Break">response variable</span></li>
<li><strong class="source-inline">s1</strong>, <strong class="source-inline">age</strong>, and <strong class="source-inline">s2</strong> exhibit a weak correlation with the <span class="No-Break">response variable</span></li>
</ul>
<p>While <strong class="source-inline">sex</strong> may appear to show no correlation with the response variable, Pearson’s correlation coefficient cannot be used with categorical features. From this correlation ranking, we can see that, at least, <strong class="source-inline">bmi</strong>, <strong class="source-inline">s5</strong>, and <strong class="source-inline">bp</strong>, are likely among the best features in this dataset for predicting the response. In fact, these features were considered statistically significant in our model. Now let’s discuss selection using <span class="No-Break">statistical significance.</span></p>
<h3>Statistical significance</h3>
<p>Assessing <a id="_idIndexMarker571"/>features using correlation is generally a good first step for feature selection. We can easily eliminate features that are uncorrelated with the response variable. However, depending on the problem, we could still be left with many features that are correlated with the response. We can further select features using the statistical significance of the feature in the context of a model. However, in recent years, these methods for feature selection have somewhat fallen out of favor in the community. For this reason, we will not focus on these methods, but will only describe them <span class="No-Break">for understanding.</span></p>
<p>Recall when we fit the MLR model, the results included a test for statistical significance for each feature in the model. We can use that test to select features. There are three well-known algorithms for selecting features based on statistical significance: <strong class="bold">forward selection</strong>, <strong class="bold">backward selection</strong>, and <strong class="bold">stepwise regression</strong>. In forward selection, we start without <a id="_idIndexMarker572"/>any variables in the model and then iteratively add one variable at a time using the p-value to choose the best feature to add at each iteration. We stop once the p-values of any features in the model size are above a predefined threshold, such as 0.05. Backward selection takes the opposite approach. We start with all features in the model, then iteratively remove features one variable at a time using the p-value to determine the least important feature. The final algorithm is stepwise regression (also called bidirectional elimination). Stepwise regression is performed using both forward and backward tests. Start with no features in the model, then in each iteration, perform one forward selection step followed by one backward <span class="No-Break">selection pass.</span></p>
<p>These selection methods were widely used in the past. However, in recent years, performance-based methods have become more widely used. Let’s discuss performance-based <span class="No-Break">methods now.</span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor126"/>Performance-based methods for feature selection</h2>
<p>The <a id="_idIndexMarker573"/>primary issue with the statistical feature<a id="_idIndexMarker574"/> selection methods mentioned previously is that they tend<a id="_idIndexMarker575"/> to create <strong class="bold">overfit</strong> models. An overfit model is a model that fits the given data exactly and fails to generalize to new data. Performance-based methods overcome overfitting using a method called <strong class="bold">cross-validation</strong>. In <a id="_idIndexMarker576"/>cross-validation, we have two <a id="_idIndexMarker577"/>datasets: a <strong class="bold">training dataset</strong> used to fit the model and a <strong class="bold">test dataset</strong> for<a id="_idIndexMarker578"/> evaluating the model. We can build models from multiple sets of features, fit all those potential models on the training set, and finally rank them based on the performance of the models on the testing set with a <span class="No-Break">given metric.</span></p>
<h3>Comparing models</h3>
<p>Before we <a id="_idIndexMarker579"/>get into feature selection methods, let’s first discuss how to compare models. We use metrics to compare models. On a basic level, we can use metrics to help us determine whether one set of features is better than another set of features based on model performance. Many metrics can be used for comparing models. We will discuss two metrics, <strong class="bold">mean square error</strong> (<strong class="bold">MSE</strong>) and <strong class="bold">mean absolute percentage error</strong> (<strong class="bold">MAPE</strong>), which are, by far, two of the most commonly used metrics for <span class="No-Break">regression models.</span></p>
<h4>MSE</h4>
<p>The MSE is <a id="_idIndexMarker580"/>given by the following formula, where <em class="italic">N</em> is the number of samples, <span class="_-----MathTools-_Math_Variable">y</span> is the response variable, and <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span> is the predicted value of the <span class="No-Break">response variable.</span></p>
<p><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>In other words, take the differences between the response values and the predicted response values, square the differences, and finally take the mean of the squared differences. A small extension of this metric commonly used is the <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>), which<a id="_idIndexMarker581"/> is simply the square root of the MSE. The RMSE is used when it is desirable for the metric to have the same units as the <span class="No-Break">response variable.</span></p>
<h4>MAPE</h4>
<p>The MAPE is <a id="_idIndexMarker582"/>given by the following formula, where <em class="italic">N</em> is the number of samples, <span class="_-----MathTools-_Math_Variable">y</span> is the response variable, and <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span> is the predicted value of the <span class="No-Break">response variable:</span></p>
<p><span class="_-----MathTools-_Math_Variable">M</span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">100</span><span class="_-----MathTools-_Math_Text">%</span><span class="_-----MathTools-_Math_Text"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span></p>
<p>This formula is<a id="_idIndexMarker583"/> like the formula for the MSE, but instead of taking the mean of the squared error, we take the mean of the percent error. This makes the MAPE easier to interpret than the MSE, which is a distinct advantage over <span class="No-Break">the MSE.</span></p>
<p>Now that we have discussed model validation and metrics, let’s put these concepts together to perform feature selection using model performance as <span class="No-Break">an indicator.</span></p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor127"/>Recursive feature elimination</h2>
<p><strong class="bold">Recursive feature elimination</strong> (<strong class="bold">RFE</strong>) is a method for selecting an optimal number of features<a id="_idIndexMarker584"/> in a model using a metric. Much<a id="_idIndexMarker585"/> like the backward selection method mentioned previously, the RFE algorithm starts with all features in the model, then removes features with the least influence on the model. At each step, cross-validation is performed. When RFE is completed, we will be able to see the cross-validation performance of the model over the various sets <span class="No-Break">of features.</span></p>
<p>In this example, we use the linear regression implementation and RFE implementation from <strong class="source-inline">scikit-learn</strong> (<strong class="source-inline">sklearn</strong>), which is a primary package used for machine learning in the Python ecosystem. In the following code example, we set up RFE to use the MAPE as the scoring metric (<strong class="source-inline">make_scorer(mape ,greater_is_better=False)</strong>), remove one feature at each step (<strong class="source-inline">step=1</strong>), and indicate with <strong class="source-inline">cv=2</strong> that it should score the model using two <span class="No-Break">cross-validation sets:</span></p>
<pre class="source-code">
linear_model = LinearRegression()
linear_model.fit(X, y)
rfecv = RFECV(
    estimator=linear_model,
    step=1,
    cv=2,
    scoring=make_scorer(mape ,greater_is_better=False),
    min_features_to_select=1
)
rfecv.fit(X,y)</pre>
<p>Once we fit the RFE object, we can look at the results to see how to model scored over the various sets of features. The performance of the model using the MAPE is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">.</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<img alt="Figure 7.8 – Linear regression performance over RFE steps, scoring with the MAPE" height="712" src="image/B18945_07_008.jpg" width="1098"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Linear regression performance over RFE steps, scoring with the MAPE</p>
<p>It’s clear<a id="_idIndexMarker586"/> that<a id="_idIndexMarker587"/> including all features produces the best-performing model but including all 10 features only provides a small increase over the performance of just five features. While the best-performing model contains all 10 features, we would still need to consider model assumptions and verify that the model is <span class="No-Break">well fit.</span></p>
<p>In this section, we have looked at several methods for feature selection, including statistical methods and performance-based methods. We also discussed metrics and how to compare models, including the reason for splitting the data into training and validation sets. In the next section, we will look at linear regression shrinkage methods. These types of models use a method called regularization, which in some ways acts like model-based <span class="No-Break">feature selection.</span></p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor128"/>Shrinkage methods</h1>
<p>The <strong class="bold">bias-variance trade-off</strong> is a<a id="_idIndexMarker588"/> decision point all statistics and<a id="_idIndexMarker589"/> machine learning practitioners must balance when performing modeling. Too much of either renders results useless. To catch these when they become issues, we look at test results and the residuals. For example, assuming a useful set of features and the appropriate model have been selected, a model that performs well on validation, but poorly on a test set could be indicative of too much variance and conversely, a model that fails to perform well at all could have too much bias. In either case, both models fail to generalize well. However, while bias in a model can be identified in poor model performance from the start, high variance can be notoriously deceptive as it has the potential to perform very well during training and even during validation, depending on the data. High-variance models frequently use values of coefficients that are unnecessarily high when very similar results can be obtained from coefficients that are not. Further, in using coefficients that are not unnecessarily high, the model is more likely to be in bias-variance equilibrium, which provides it a better chance of generalizing well on future data. Additionally, more reliable insights into the influence of current factors on a given target are provided when model coefficients are not exaggerated, which aids in more useful descriptive analysis. This brings us to the concept <span class="No-Break">of shrinkage.</span></p>
<p><strong class="bold">Shrinkage</strong> is a method that reduces model variance by shrinking model parameter coefficients toward zero. The coefficients are derived by applying least squares regression to all variables considered for a model. The amount of shrinkage is based on the parameters’ contribution to least squares estimates; parameters that contribute to a high level of squared error will have their coefficients pushed toward zero or zeroed out altogether. In this case, shrinkage can be used for variable elimination. Variables that do not contribute to high squared error across the model fit will have a minimal reduction in coefficient value, thus being useful for model fitting, assuming the practical purpose of their inclusion is vetted. The reason shrinkage – also<a id="_idIndexMarker590"/> called <strong class="bold">regularization</strong> – is important is because it helps models include useful variables while preventing them from introducing too much variance and thus overfitting. Preventing excess variance is particularly useful for ensuring that models generalize well over time. Let’s look at some of the <a id="_idIndexMarker591"/>most common shrinkage<a id="_idIndexMarker592"/> techniques, <strong class="bold">ridge regression</strong> and <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong> (<span class="No-Break"><strong class="bold">LASSO</strong></span><span class="No-Break">) </span><span class="No-Break"><strong class="bold">Regression</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor129"/>Ridge regression</h2>
<p>Recall<a id="_idIndexMarker593"/> the <a id="_idIndexMarker594"/>formulation for the <strong class="bold">residual sum of squared</strong> <strong class="bold">errors</strong> (<strong class="bold">RSS</strong>) for multiple linear regression using least squares regression is <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">n</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">p</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">i</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>where <em class="italic">n</em> is the number of samples and <em class="italic">p</em> is the number of parameters. Ridge regression adds a scalar, <em class="italic">λ</em>, called <a id="_idIndexMarker595"/>a <strong class="bold">tuning parameter</strong> – which must be greater than or equal to 0 – that gets multiplied by the model parameter coefficient estimates, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, to create<a id="_idIndexMarker596"/> a <strong class="bold">shrinkage penalty</strong>. This gets added back to the RSS equation such that the new least squares regression’s fitting procedure is now defined <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic">R</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">S</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝝀</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">p</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">ˆ</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>When λ=0, the respective ridge<a id="_idIndexMarker597"/> regression <strong class="bold">penalty term</strong> is 0. However, as <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⟶</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">∞</span>, the model coefficients shrink toward zero. The new fitting procedure can be rewritten <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Base">‖</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">X</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">β</span><span class="_-----MathTools-_Math_Symbol_Extended">‖</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝝀</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">‖</span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Symbol_Extended">‖</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span></p>
<p>where RSS = <span class="_-----MathTools-_Math_Base">‖</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">‖</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">‖</span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable">‖</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> is the squared <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> <strong class="bold">norm</strong> (Euclidean norm) of the regression coefficients array, and <em class="italic">X</em> is the design matrix. Further simplification brings this to <span class="No-Break">the following:</span></p>
<p><span class="_-----MathTools-_Math_Base">‖</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">X</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">β</span><span class="_-----MathTools-_Math_Symbol_Extended">‖</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝝀</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">T</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Symbol_Extended">β</span></span></p>
<p>which can be rewritten in the closed form by using the tuning parameter’s scalar multiple of the identity matrix to <a id="_idIndexMarker598"/>derive the <strong class="bold">ridge regression coefficient estimates</strong>, <span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span>, <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">ˆ</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">T</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝝀</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">I</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">T</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">y</span></span></p>
<p>Simply stated, the ridge regression coefficient estimates are the set of coefficients that minimizes the least squares regression output <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">ˆ</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">a</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">r</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">g</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">m</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">n</span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">n</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">p</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝝀</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">p</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">ˆ</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>
<p class="callout-heading">Standardizing coefficients prior to applying ridge regression</p>
<p class="callout">Because ridge regression seeks to minimize the error for the entire dataset and the tuning parameter for doing so is applied within the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> norm normalization process of taking the root of summed squared terms, it is important to apply a standard scaler to each variable in the model so that all variables are on the same scale, prior to applying ridge regression. If this is not performed, ridge regression will almost certainly fail to be useful for helping the model generalize <span class="No-Break">across datasets.</span></p>
<p>In<a id="_idIndexMarker599"/> summary, ridge regression reduces variance in a model by using the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> <strong class="bold">penalty</strong>, which penalizes the sum of squared coefficients. However, it is important to note the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> <strong class="bold">norm</strong>, and consequently, ridge regression, <strong class="bold">will never produce a zero-valued coefficient</strong>. Therefore, ridge regression is an excellent tool for reducing variance when the analyst seeks to use all terms in the model. However, ridge regression cannot be used for variable elimination. For variable elimination, we can use the LASSO regression shrinkage method, which uses the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> <strong class="bold">penalty</strong> to regularize coefficients with the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> <strong class="bold">norm</strong>, which uses the absolute value to shrink values to and <span class="No-Break">including zero.</span></p>
<p> Let’s walk through <a id="_idIndexMarker600"/>an example of a ridge regression implementation <span class="No-Break">in Python.</span></p>
<p>First, let’s load the Boston home prices dataset from <strong class="source-inline">scikit-learn</strong>. In the final line, we add the constant for the intercept. Note that this process can be repeated with comparable results, albeit with different input variables, using the California housing data set by running <strong class="source-inline">from sklearn.datasets import fetch_california_housing</strong> in place of <strong class="source-inline">from sklearn.datasets </strong><span class="No-Break"><strong class="source-inline">import load_boston</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
from sklearn.metrics import mean_squared_error as MSE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston
import statsmodels.api as sm
import pandas as pd
boston_housing = load_boston()
df_boston = pd.DataFrame(boston_housing.data, columns = boston_housing.feature_names)
df_boston['PRICE'] = boston_housing.target
df_boston = sm.add_constant(df_boston, prepend=False</pre>
<p>The first three records are <span class="No-Break">given here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="Figure 7.9 – First three records of the Boston housing data" height="204" src="image/B18945_07_009.jpg" width="1660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – First three records of the Boston housing data</p>
<p>We <a id="_idIndexMarker601"/>set <strong class="source-inline">PRICE</strong> as our target. Recall, ridge regression is most<a id="_idIndexMarker602"/> useful when there are either more parameters than samples or there is excessive variance. Let’s assume both of these assumptions <span class="No-Break">are met:</span></p>
<pre class="source-code">
X = df_boston.drop('PRICE', axis=1)
y = df_boston['PRICE']</pre>
<p>As noted previously, <strong class="bold">data must be standardized</strong> for ingestion into the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> norm, so let’s apply that with <strong class="source-inline">scikit-learn</strong>’s <span class="No-Break"><strong class="source-inline">StandardScaler</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
sc = StandardScaler()
X_scaled = sc.fit_transform(X)</pre>
<p>Next, let’s take a 75/25 train/test split of the data for the model. We use <strong class="source-inline">shuffle=True</strong> to randomly shuffle the data so we test with a random sample, which is more likely to be representative of <span class="No-Break">the population:</span></p>
<pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.25, shuffle=True)</pre>
<p>Now we can <a id="_idIndexMarker603"/>compare <strong class="bold">ordinary least squares</strong> (<strong class="bold">OLS</strong>) regression to OLS regression using coefficients regularized through ridge regression. In the imported function here, <strong class="source-inline">fit_regularized</strong>, we set the required argument for <strong class="source-inline">method</strong> to <strong class="source-inline">'elastic_net'</strong>. We will discuss Elastic Net shortly, but for now, note the <strong class="source-inline">L1_wt</strong> argument applies ridge regression <strong class="bold">when set to 0</strong>. Alpha is the tuning parameter, λ, in the ridge regression penalty term. A small alpha allows for large coefficients and a large alpha pushes the<a id="_idIndexMarker604"/> coefficients toward zero. Here, we fit the training data to derive the mean <span class="No-Break">squared error:</span></p>
<pre class="source-code">
ols_model = sm.OLS(y_train, X_train)
compiled_model = ols_model.fit()
compiled_model_ridge = ols_model.fit_regularized(method = 'elastic_net', L1_wt=0, alpha=0.1,refit=True)
print('OLS Error: ', MSE(y_train,
    compiled_model.predict(X_train)) )
print('Ridge Regression Error: ', MSE(y_train,
    compiled_model_ridge.predict(X_train)))</pre>
<p>We can see ridge regression has a slightly higher amount <span class="No-Break">of error</span></p>
<p><strong class="source-inline">OLS Error:  </strong><span class="No-Break"><strong class="source-inline">530.7235449265926</strong></span></p>
<p><strong class="source-inline">Ridge Regression Error:  </strong><span class="No-Break"><strong class="source-inline">533.2278083730833</strong></span></p>
<p>Next, we fit the test data to see how the model generalizes on unseen data. We measure again with the mean <span class="No-Break">squared error:</span></p>
<pre class="source-code">
print('OLS Error: ', MSE(y_test, compiled_model.predict(X_test)) )
print('Ridge Regression Error: ', MSE(y_test, compiled_model_ridge.predict(X_test)))</pre>
<p>We can see that <a id="_idIndexMarker605"/>the number of errors increased for both on the <a id="_idIndexMarker606"/>test data. However, the OLS regression produced a slightly higher error in proportion to the ridge <span class="No-Break">regression approach:</span></p>
<p><strong class="source-inline">OLS Error:  </strong><span class="No-Break"><strong class="source-inline">580.8138216493896</strong></span></p>
<p><strong class="source-inline">Ridge Regression Error:  </strong><span class="No-Break"><strong class="source-inline">575.5186673728349</strong></span></p>
<p>Here, we can observe the OLS regression coefficients and the regularized coefficients in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="Figure 7.10 – OLS regression coefficients before and after ridge regression regularization" height="210" src="image/B18945_07_010.jpg" width="1643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – OLS regression coefficients before and after ridge regression regularization</p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor130"/>LASSO regression</h2>
<p>Earlier<a id="_idIndexMarker607"/> in this section, we mentioned how variable<a id="_idIndexMarker608"/> coefficients with unnecessarily high values contribute to model variance, and in so doing, take away a model’s ability to generalize as well as it could were the coefficients reasonable. We demonstrated applying ridge regression to enact this. A very popular alternative to ridge regression, however, is LASSO regression. LASSO regression follows a similar procedure of adding a penalty term to the model’s residual sum of squared error and seeks to minimize the resulting value (error). However, LASSO uses the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> norm rather than <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>. Consequently, it is possible to obtain absolute zero coefficient values as the tuning parameter reaches a sufficient size, thus functioning as a feature selection and <span class="No-Break">shrinkage tool.</span></p>
<p>The LASSO equation seeks to minimize overall model error by shrinking each variable’s coefficient using the <span class="No-Break">following method:</span></p>
<p><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">ˆ</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">a</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">r</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">g</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">m</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">n</span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">n</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">p</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝝀</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">p</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>
<p>The only difference between LASSO and ridge regression is the penalty term <span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">j</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">|</span></span><span class="No-Break">.</span></p>
<p class="callout-heading">Choosing λ</p>
<p class="callout">The value of<a id="_idIndexMarker609"/> the tuning parameter, λ, is best selected using cross-validation. While the tuning parameter can go to infinity, in theory, it is typical to start with values less than 1, such as at 0.1 increasing by increments of tenths up to 1. After, typically integer values <span class="No-Break">are used.</span></p>
<p>Using the <a id="_idIndexMarker610"/>same data we used for ridge regression, we apply LASSO regression. Here, we set <strong class="source-inline">L1_wt=1</strong>, indicating the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> norm will <span class="No-Break">be applied:</span></p>
<pre class="source-code">
ols_model = sm.OLS(y_train, X_train)
compiled_model = ols_model.fit()
compiled_model_lasso = ols_model.fit_regularized(method='elastic_net', L1_wt=1, alpha=0.1,refit=True)</pre>
<p>We will follow the same steps for LASSO as with ridge regression in that we first check the errors on the training data, then again on the test data to see how the <span class="No-Break">models generalize:</span></p>
<pre class="source-code">
print('OLS Error: ', MSE(y_train, compiled_model.predict(X_train)) )
print('LASSO Regression Error: ', MSE(y_train, compiled_model_lasso.predict(X_train)))</pre>
<p>The output shows OLS regression slightly outperforming LASSO. This could be due to higher variance. However, practically speaking, the results are <span class="No-Break">the same:</span></p>
<p><strong class="source-inline">OLS </strong><span class="No-Break"><strong class="source-inline">Error: 530.7235449265926</strong></span></p>
<p><strong class="source-inline">LASSO Regression </strong><span class="No-Break"><strong class="source-inline">Error: 531.2440812254207</strong></span></p>
<p>Now, we need to check the model’s performance on the <span class="No-Break">holdout data:</span></p>
<pre class="source-code">
print('OLS Error: ', MSE(y_test, compiled_model.predict(X_test)) )
print('LASSO Regression Error: ', MSE(y_test, compiled_model_lasso.predict(X_test)))</pre>
<p>We can see the<a id="_idIndexMarker611"/> two models again have essentially the same error. However, with fewer features, the LASSO model may be easier to trust to generalize on future data. That may depend on the researcher’s level of subject <span class="No-Break">knowledge, however:</span></p>
<p><strong class="source-inline">OLS </strong><span class="No-Break"><strong class="source-inline">Error: 546.1338399374557</strong></span></p>
<p><strong class="source-inline">LASSO Regression </strong><span class="No-Break"><strong class="source-inline">Error: 546.716239805892</strong></span></p>
<p>As with<a id="_idIndexMarker612"/> ridge regression, some of the coefficients have been minimized in the <span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> norm regularization process. However, we also see, using the same alpha value, four variables have been minimized to zero, thus eliminating them from the model altogether. Three of the features were comparatively shrunk almost to zero by ridge regression, but not <strong class="source-inline">ZN</strong>, which we see in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.11</em> has been reduced to <strong class="source-inline">0</strong>. The model error has been slightly improved, which may not appear significant, but when considering the elimination of four variables, we can consider the model to have more generalization with less dependence on <span class="No-Break">exogenous variables.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="Figure 7.11 – OLS regression coefficients before and after LASSO regression regularization" height="213" src="image/B18945_07_011.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – OLS regression coefficients before and after LASSO regression regularization</p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor131"/>Elastic Net</h2>
<p><strong class="bold">Elastic Net</strong> is <a id="_idIndexMarker613"/>another common shrinkage method that <a id="_idIndexMarker614"/>can be applied to manage a bias-variance trade-off. This method applies the tuning parameter to a combination of ridge and LASSO regression where the proportion of influence from either is determined by the hyperparameter α. The equation Elastic Net minimizes is <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">⎧</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎪</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎨</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎪</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎩</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">∑</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Variable">=</span><span class="_-----MathTools-_Math_Variable">1</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">ˆ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">ˆ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">β</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">⎫</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎪</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎬</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎪</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">⎭</span></p>
<p>Naturally, depending on the values of α, Elastic Net can also generate absolute zero-valued coefficient parameter estimates where it cancels out the ridge regression penalty term. When all input variables are needed – for example, if they have already been pruned following procedures such as those outlined in the <em class="italic">Feature selection</em> section of this chapter – Elastic Net is most likely to outperform both ridge and LASSO regression, especially when there are highly correlated features in the dataset that must be included to capture necessary variance. In the next section, we will discuss dimension reduction. Specifically, we p  rovide an in-depth overview of PCR, which uses <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) to<a id="_idIndexMarker615"/> extract useful information from systems that contain correlated features required to evaluate <span class="No-Break">the target.</span></p>
<p>First, let’s walk through a regression example using Elastic Net using the same data we used for ridge and LASSO regression. Following the previous Elastic Net minimizing equation, we set <strong class="source-inline">Lt_wt=0.5</strong>, meaning an equal, 50/50 balance between ridge and LASSO regression. Differently, however, we applied <strong class="source-inline">alpha=8</strong> instead of <strong class="source-inline">0.1</strong> as we used in ridge and LASSO regression to gain an improvement over the OLS regression coefficients. Recall that as the tuning parameter approaches infinity, coefficients approach 0. Therefore, we can conclude based on the Elastic Net coefficients that 8 is a very high <span class="No-Break">tuning parameter:</span></p>
<pre class="source-code">
ols_model = sm.OLS(y_train, X_train)
compiled_model = ols_model.fit()
compiled_model_elastic = ols_model.fit_regularized(method='elastic_net', L1_wt=0.5, alpha=8,refit=True)</pre>
<p>Let’s test the model on the <span class="No-Break">training data:</span></p>
<pre class="source-code">
print('OLS Error: ', MSE(y_train, compiled_model.predict(X_train)) )
print('Elastic Net Regression Error: ', MSE(y_train, compiled_model_elastic.predict(X_train)))</pre>
<p>Here, we see <a id="_idIndexMarker616"/>Elastic Net has added errors into the model compared to <span class="No-Break">OLS regression:</span></p>
<p><strong class="source-inline">OLS </strong><span class="No-Break"><strong class="source-inline">Error: 530.7235449265926</strong></span></p>
<p><strong class="source-inline">Elastic Net Regression </strong><span class="No-Break"><strong class="source-inline">Error: 542.678919923863</strong></span></p>
<p>Now let’s check the model errors for the <span class="No-Break">holdout data:</span></p>
<pre class="source-code">
print('OLS Error: ', MSE(y_test, compiled_model.predict(X_test)) )
print('Elastic Net Regression Error: ', MSE(y_test, compiled_model_elastic.predict(X_test)))</pre>
<p>Observing <a id="_idIndexMarker617"/>the results in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.12</em>, we can see how Elastic Net has traded variance – which increased the error on training – for bias, which has enabled the model to generalize better on holdout. We can see better results with Elastic Net than OLS regression on holdout. However, the improved error suggests the added bias provided a better chance of lower error, but is not something to <span class="No-Break">be expected:</span></p>
<p><strong class="source-inline">OLS </strong><span class="No-Break"><strong class="source-inline">Error: 546.1338399374557</strong></span></p>
<p><strong class="source-inline">Elastic Net Regression </strong><span class="No-Break"><strong class="source-inline">Error: 514.8301731640446</strong></span></p>
<p>Here we can see the coefficients before and after Elastic <span class="No-Break">Net’s implementation:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<img alt="Figure 7.12 – OLS regression coefficients before and after Elastic Net regularization" height="233" src="image/B18945_07_012.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – OLS regression coefficients before and after Elastic Net regularization</p>
<p>We can see with a<a id="_idIndexMarker618"/> sufficiently large tuning parameter value (<strong class="source-inline">alpha=8</strong>), most of <a id="_idIndexMarker619"/>the coefficients with the balanced Elastic Net have been forced to absolute zero. The only coefficients remaining had comparatively large values with OLS regression. Notably, for the variables that remain with coefficients (<strong class="source-inline">RM</strong> and <strong class="source-inline">LSTAT</strong>), Elastic Net increased both coefficients where ridge regression and LASSO either reduced <span class="No-Break">them slightly.</span></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor132"/>Dimension reduction</h1>
<p>In this section, we <a id="_idIndexMarker620"/>will use a specific technique – <strong class="bold">PCR</strong> – to study MLR. This technique is useful when we need to deal with a multicollinearity data issue. Multicollinearity occurs when an independent variable is highly correlated with another independent variable, or an independent variable can be predicted from another independent variable in a regression model. A high correlation can affect the result poorly when fitting <span class="No-Break">a model.</span></p>
<p>The<a id="_idIndexMarker621"/> PCR technique is based on PCA as used in unsupervised machine learning for data compression and exploratory analysis. The idea behind it is to use the dimension reduction technique, PCA, on these original variables to create new uncorrelated variables. The information obtained on these new variables helps us to understand the relationship and then apply the MLR algorithm to these new variables. The PCA technique can also be used in a classification problem, which we will discuss in the <span class="No-Break">next chapter.</span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor133"/>PCA – a hands-on introduction</h2>
<p>PCA is a<a id="_idIndexMarker622"/> dimension reduction technique based on linear <a id="_idIndexMarker623"/>algebra by linearly transforming data using linear combinations of original variables into a new coordinate system. These new linear combinations are <a id="_idIndexMarker624"/>called <strong class="bold">principal components</strong> (<strong class="bold">PCs</strong>). The difference between the PCA technique and the feature selection technique or shrinkage methods mentioned in previous sections is that the original independent variables are maintained but the new PC variables are transformed into a new coordinate space. In other words, PCA uses the original data to arrive at a new representation or a new structure. The number of PC variables is the same as the number of original independent variables, but these new PC variables are uncorrelated with each other. The PC variables are created and ordered from the most to the least amount of variability. The sum of variances is the same between the original independent variables and the newly transformed <span class="No-Break">PC variables.</span></p>
<p>Before <a id="_idIndexMarker625"/>conducting PCA, we perform pre-processing for the dataset by subtracting the mean from each data point and normalizing the standard deviation of each independent variable. In a high-level structure, the goal of the PCA technique is to find vectors <span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span> such that any data point, <span class="_-----MathTools-_Math_Variable">x</span>, in the dataset can be approximately represented as a <span class="No-Break">linear combination:</span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">k</span><span class="_-----MathTools-_Math_Variable_v-bold-italic"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">i</span></span></p>
<p>for some constants <span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span> with <span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">‾</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Operator">.</span> Assume that we have these <span class="_-----MathTools-_Math_Variable">k</span> vectors; then, each data point can be written as a vector in <span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span> corresponding <span class="No-Break">the projections:</span></p>
<p><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">&lt;</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">&lt;</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">&lt;</span><span class="_-----MathTools-_Math_Variable_v-bold-italic">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">k</span><span class="_-----MathTools-_Math_Base">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">v</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-bold-italic">k</span></span></p>
<p>In other<a id="_idIndexMarker626"/> words, if we have <span class="_-----MathTools-_Math_Variable">d</span> original independent variables, we will construct a <span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Operator">−</span>dimensional transformation matrix that can map any data point onto a new <span class="_-----MathTools-_Math_Variable">k</span>-dimensional variable subspace with <span class="_-----MathTools-_Math_Variable">k</span> smaller than <span class="_-----MathTools-_Math_Variable">d</span>. It means that we have performed a dimension reduction by a linear transformation of the original independent variables. There are several applications of PCA, but here, we will cite a paper, <em class="italic">Gene mirror geography within Europe</em>, published in <em class="italic">Nature (2008)</em> (<a href="https://pubmed.ncbi.nlm.nih.gov/18758442/">https://pubmed.ncbi.nlm.nih.gov/18758442/</a>). Authors considered a sample of 3,000 European individuals genotyped at over half a million variable DNA sites in the human genome; then, each individual was represented using more than half a million genetic markers. This means that it produced a matrix with dimensions larger than 3,000 x 500,000. They performed a PCA on the dataset to find the most meaningful vectors, <span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> and <span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> (the first and second components), where each person only corresponds to two numbers. The authors plotted each person based on two numbers in a two-dimensional plane and then colored each point according to the country they <span class="No-Break">came from.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<img alt="Figure 7.13 – PCA analysis for gene mirror geography within Europe" height="575" src="image/B18945_07_013.jpg" width="746"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – PCA analysis for gene mirror geography within Europe</p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<img alt="Figure 7.14 – PCA analysis for gene mirror geography within Europe" height="391" src="image/B18945_07_014.jpg" width="757"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – PCA analysis for gene mirror geography within Europe</p>
<p>Surprisingly, the <a id="_idIndexMarker627"/>PCA technique performed well in the plots by <a id="_idIndexMarker628"/>showing genetic similarities that are similar to the <span class="No-Break">European map.</span></p>
<p>In the next part, we will discuss how to use PCA to conduct a PCR analysis <span class="No-Break">in practice.</span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>PCR – a hands-on salary prediction study</h2>
<p>To <a id="_idIndexMarker629"/>conduct a PCR analysis, we first perform a PCA to obtain the PCs and then decide to keep the first <span class="_-----MathTools-_Math_Variable">k</span> PCs that contain the most explainable amount of variability. Here, <span class="_-----MathTools-_Math_Variable">k</span> is the dimensionality of the new PC variable space. Finally, we fit MLR on <a id="_idIndexMarker630"/>these <span class="No-Break">new variables.</span></p>
<p>We will consider a hands-on salary prediction task from the open source Kaggle data – <a href="https://www.kaggle.com/datasets/floser/hitters">https://www.kaggle.com/datasets/floser/hitters</a> – to illustrate the PCR method. If following along, please download the dataset from the <span class="No-Break">Kaggle URL:</span></p>
<ol>
<li><strong class="bold">Setting up and loading </strong><span class="No-Break"><strong class="bold">the data</strong></span></li>
</ol>
<p>Import the necessary libraries to be used in this study and loading the Hitters data. For simplicity, we will drop all missing values in the dataset. There are 19 independent variables (16 numerical and 3 categorical) with the target ‘<strong class="source-inline">Salary</strong>’. The categorical independent variables ‘<strong class="source-inline">League</strong>’, ‘<strong class="source-inline">Division</strong>’, and ‘<strong class="source-inline">NewLeague</strong>’ are converted into dummy variables. We preprocess and standardize the<a id="_idIndexMarker631"/> features, and create a train and a test set <a id="_idIndexMarker632"/>before conducting the <span class="No-Break">PCA step:</span></p>
<pre class="source-code">
# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.decomposition import PCA
#location of dataset
url = "/content/Hitters.csv"
#read in data
data = pd.read_csv(url).dropna() # to simply the analysis, we drop all missing values
# create dummies variables
dummies_variables = pd.get_dummies(data[['League', 'Division', 'NewLeague']])
# create features and target
target = data['Salary']
feature_to_drop = data.drop(['Salary', 'League', 'Division', 'NewLeague'],axis=1).astype('float64')
X = pd.concat([feature_to_drop, dummies_variables[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)
#scaled data - preprocessing
X_scaled = scale(X)
# train test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, target, test_size=0.2, random_state=42)</pre>
<ol>
<li value="2"><strong class="bold">Generating </strong><span class="No-Break"><strong class="bold">all PCs</strong></span></li>
</ol>
<p>As the <a id="_idIndexMarker633"/>next <a id="_idIndexMarker634"/>step, we generate all the PCs for the training set. This performance produces 19 new PC variables because there are 19 original <span class="No-Break">independent variables:</span></p>
<pre class="source-code">
# First generate all the principal components
pca = PCA()
X_pc_train = pca.fit_transform(X_train)
X_pc_train.shape</pre>
<ol>
<li value="3"><strong class="bold">Determining the best number of PCs to </strong><span class="No-Break"><strong class="bold">be used</strong></span></li>
</ol>
<p>The next step is to perform a 10-fold cross-validation MLR and choose the best number of PCs to use by using <span class="No-Break">the RMSE:</span></p>
<pre class="source-code">
# Define cross-validation folds
cv = KFold(n_splits=10, shuffle=True, random_state=42)
model = LinearRegression()
rmse_score = []
# Calculate MSE score - based on 19 PCs
for i in range(1, X_pc_train.shape[1]+1):
    rmse = -cross_val_score(model, X_pc_train[:,:i], y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
    rmse_score.append(rmse)
# Plot results
plt.plot(rmse_score, '-o')
plt.xlabel('Number of principal components in regression')
plt.ylabel('RMSE')
plt.title('Salary')
plt.xlim(xmin=-1)
plt.xticks(np.arange(X_pc_train.shape[1]), np.arange(1, X_pc_train.shape[1]+1))
plt.show()</pre>
<p>Here’s<a id="_idIndexMarker635"/> the <a id="_idIndexMarker636"/><span class="No-Break">plot produced:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<img alt="Figure 7.15 – Number of PCs" height="278" src="image/B18945_07_015.jpg" width="389"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Number of PCs</p>
<p>From this, we <a id="_idIndexMarker637"/>see that the best number of PCs is 6, corresponding <a id="_idIndexMarker638"/>with the lowest <span class="No-Break">cross-validation RMSE.</span></p>
<ol>
<li value="4"><strong class="bold">Retraining the model and </strong><span class="No-Break"><strong class="bold">performing prediction</strong></span></li>
</ol>
<p>We will use this number to train a regression model on the training data and make predictions on the <span class="No-Break">test data:</span></p>
<pre class="source-code">
# Train regression model on training data
model = LinearRegression()
model.fit(X_pc_train[:,:6], y_train)
pcr_score_train = -cross_val_score(model, X_pc_train[:,:6], y_train, cv=cv, scoring='neg_root_mean_squared_error').mean()
# Prediction with test data
X_pc_test = pca.fit_transform(X_test)[:,:6]
pred = model.predict(X_pc_test)
pcr_score_test = mean_squared_error(y_test, pred, squared=False)</pre>
<p>Remark that <a id="_idIndexMarker639"/>PCR analysis is more difficult to interpret the results of <a id="_idIndexMarker640"/>than feature selection or shrinkage methods, and this analysis performance is better if the few first PCs capture the most explainable amount <span class="No-Break">of variability.</span></p>
<h1 id="_idParaDest-131"><a id="_idTextAnchor135"/>Summary</h1>
<p>In this chapter, we discussed the concept of MLR and topics aiding in its implementation. These topics included feature selection methods, shrinkage methods, and PCR. Using these tools, we were able to demonstrate approaches to reduce the risk of modeling excess variance. In doing so, we were able to also induce model bias so that models can have a better chance of generalizing on unseen data with minimal complications as frequently faced <span class="No-Break">when overfitting.</span></p>
<p>In the next chapter, we will begin a discussion on classification with the introduction of logistic regression, which fits a sigmoid to a linear regression model to derive probabilities of binary <span class="No-Break">class membership.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer111">
<h1 id="_idParaDest-132" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor136"/>Part 3:Classification Models</h1>
<p>In this part, we discuss the types of problems that can be solved with classification, coefficients of correlation and determination, multivariate modeling, model selection and variable adjustment <span class="No-Break">with regularization.</span></p>
<p>It includes the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18945_08.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a>, <em class="italic">Discrete Models</em></li>
<li><a href="B18945_09.xhtml#_idTextAnchor148"><em class="italic">Chapter 9</em></a>, <em class="italic">Discriminant Analysis</em></li>
</ul>
</div>
<div>
<div id="_idContainer112">
</div>
</div>
<div>
<div id="_idContainer113">
</div>
</div>
<div>
<div id="_idContainer114">
</div>
</div>
<div>
<div id="_idContainer115">
</div>
</div>
<div>
<div id="_idContainer116">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer117">
</div>
</div>
<div>
<div id="_idContainer118">
</div>
</div>
<div>
<div id="_idContainer119">
</div>
</div>
</div></body></html>