- en: Introduction to Deep Learning with Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala for
    Regression Analysis*, to [Chapter 6](4935de5b-9527-4ff2-82ed-927dea04c77a.xhtml),
    *Scala for Recommender System*, we have learned about linear and classic **machine
    learning** (**ML**) algorithms through real-life examples. In this chapter, we
    will explain some basic concepts of **deep learning** (**DL**). We will start
    with DL, which is one of the emerging branches of ML. We will briefly discuss
    some of the most well-known and widely used neural network architectures and DL
    frameworks and libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will use the **Long Short-Term Memory** (**LSTM**) architecture
    for cancer type classification from a very high-dimensional dataset curated from
    **The Cancer Genome Atlas** (**TCGA**). The following topics will be covered in
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: DL versus ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL and neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep neural network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DL frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapters can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2vwrxzb](http://bit.ly/2vwrxzb)'
  prefs: []
  type: TYPE_NORMAL
- en: DL versus ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Simple ML methods that were used in small-scale data analysis are not effective
    anymore because the effectiveness of ML methods diminishes with large and high-dimensional
    datasets. Here comes DL—a branch of ML based on a set of algorithms that attempt
    to model high-level abstractions in data. Ian Goodfellow *et al.* (Deep Learning,
    MIT Press, 2016) defined DL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Deep learning is a particular kind of machine learning that achieves great
    power and flexibility by learning to represent the world as a nested hierarchy
    of concepts, with each concept defined in relation to simpler concepts, and more
    abstract representations computed in terms of less abstract ones."'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the ML model, a DL model also takes in an input, *X*, and learns
    high-level abstractions or patterns from it to predict an output of *Y*. For example,
    based on the stock prices of the past week, a DL model can predict the stock price
    for the next day. When performing training on such historical stock data, a DL
    model tries to minimize the difference between the prediction and the actual values.
    This way, a DL model tries to generalize to inputs that it hasn't seen before
    and makes predictions on test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might be wondering, if an ML model can do the same tasks, why do we
    need DL for this? Well, DL models tend to perform well with large amounts of data,
    whereas old ML models stop improving after a certain point. The core concept of
    DL is inspired by the structure and function of the brain, which are called **artificial
    neural networks** (**ANNs**). Being at the core of DL, ANNs help you learn the
    associations between sets of inputs and outputs in order to make more robust and
    accurate predictions. However, DL is not only limited to ANNs; there have been
    many theoretical advances, software stacks, and hardware improvements that bring
    DL to the masses. Let''s look at an example; suppose we want to develop a predictive
    analytics model, such as an animal recognizer, where our system has to resolve
    two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: To classify whether an image represents a cat or a dog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To cluster images of dogs and cats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we solve the first problem using a typical ML method, we must define the
    facial features (ears, eyes, whiskers, and so on) and write a method to identify
    which features (typically nonlinear) are more important when classifying a particular
    animal.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, at the same time, we cannot address the second problem because classical
    ML algorithms for clustering images (such as k-means) cannot handle nonlinear
    features. Take a look at the following diagram, which shows a workflow that we
    would follow whether we wanted to classify if the given image is of a cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a07fad8a-a998-4ae3-9b5f-662fa5a9dbe0.png)'
  prefs: []
  type: TYPE_IMG
- en: DL algorithms will take these two problems one step further, and the most important
    features will be extracted automatically after determining which features are
    the most important for classification or clustering. In contrast, when using a
    classical ML algorithm, we would have to provide the features manually.
  prefs: []
  type: TYPE_NORMAL
- en: A DL algorithm would take more sophisticated steps instead. For example, first,
    it would identify the edges that are the most relevant when clustering cats or
    dogs. It would then try to find various combinations of shapes and edges hierarchically.
    This step is called **extract, transform, and load** (**ETL**). Then after several
    iterations, hierarchical identification of complex concepts and features would
    be carried out. Then, based on the identified features, the DL algorithm would
    decide which of these features are most significant for classifying the animal.
    This step is known as feature extraction. Finally, it would take out the label
    column and perform unsupervised training using **autoencoders** (**AEs**) to extract
    the latent features to be redistributed to k-means for clustering. Then, the **clustering
    assignment hardening loss** (**CAH loss**) and reconstruction loss are jointly
    optimized toward optimal clustering assignment.
  prefs: []
  type: TYPE_NORMAL
- en: However, in practice, a DL algorithm is fed with a raw image representations,
    which doesn't see an image as we see it because it only knows the position of
    each pixel and its color. The image is divided into various layers of analysis.
    At a lower level, the software analyzes, for example, a grid of a few pixels with
    the task of detecting a type of color or various nuances. If it finds something,
    it informs the next level, which at this point checks whether or not that given
    color belongs to a larger form, such as a line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process continues to the upper levels until the algorithm understand what
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ed01b07-4a89-4d9c-ada7-91ca8d11c792.png)'
  prefs: []
  type: TYPE_IMG
- en: Although *dog versus cat* is an example of a very simple classifier, software
    that's capable of doing these types of things is now widespread and is found in
    systems for recognizing faces, or in those for searching an image on Google, for
    example. This kind of software is based on DL algorithms. On the contrary, by
    using a linear ML algorithm, we cannot build such applications since these algorithms
    are incapable of handling nonlinear image features.
  prefs: []
  type: TYPE_NORMAL
- en: Also, using ML approaches, we typically only handle a few hyperparameters. However,
    when neural networks are brought to the mix, things become too complex. In each
    layer, there are millions or even billions of hyperparameters to tune—so many
    that the cost function becomes non-convex. Another reason for this is that the
    activation functions that are used in hidden layers are nonlinear, so the cost
    is non-convex.
  prefs: []
  type: TYPE_NORMAL
- en: DL and ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs, which are inspired by how a human brain works, form the core of deep learning
    and its true realization. Today's revolution around deep learning would have not
    been possible without ANNs. Thus, to understand DL, we need to understand how
    neural networks work.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs and the human brain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs represent one aspect of the human nervous system and how the nervous system
    consists of a number of neurons that communicate with each other using axons.
    The receptors receive the stimuli either internally or from the external world.
    Then, they pass this information into the biological neurons for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of dendrites, in addition to another long extension called
    the axon. Toward its extremities, there are minuscule structures called synaptic
    terminals, which are used to connect one neuron to the dendrites of other neurons.
    Biological neurons receive short electrical impulses called signals from other
    neurons, and in response, they trigger their own signals.
  prefs: []
  type: TYPE_NORMAL
- en: We can thus summarize that the neuron comprises a cell body (also known as the
    soma), one or more dendrites for receiving signals from other neurons, and an
    axon for carrying out the signals that are generated by the neurons. A neuron
    is in an active state when it is sending signals to other neurons. However, when
    it is receiving signals from other neurons, it is in an inactive state. In an
    idle state, a neuron accumulates all the signals that are received before reaching
    a certain activation threshold. This whole thing motivated researchers to test
    out ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most significant progress in ANNs and DL can be described with the following
    timeline. We have already seen how the artificial neurons and perceptrons provided
    the base in 1943 and 1958, respectively. Then, the XOR was formulated as a linearly
    non-separable problem in 1969 by Minsky *et al.*, but later, in 1974, Werbos *et
    al*. demonstrated the backpropagation algorithm for training the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: However, the most significant advancement happened in the 1980s when John Hopfield
    *et al.* proposed the Hopfield network in 1982\. Then, one of the godfathers of
    the neural network and DL, Hinton and his team proposed the Boltzmann machine
    in 1985\. However, probably one of the most significant advances happened in 1986
    when Hinton *et al.* successfully trained the MLP and Jordan *et al.* proposed
    RNNs. In the same year, Smolensky *et al.* also proposed the improved version
    of Boltzmann machine called the **Restricted Boltzmann Machine** (**RBM**).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in the 90s era, the most significant year was 1997, when Lecun *et
    al.* proposed LeNet in 1990, and Jordan *et al.* proposed the Recurrent Neural
    Network in 1997\. In the same year, Schuster *et al.* proposed the improved version
    of LSTM and the improved version of the original RNN, called the bidirectional
    RNN. The following timeline provides a brief glimpse into the history of different
    neural network architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c03a0a3-f5ec-46e1-8a0a-7930fcfcb77c.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite significant advances in computing, from 1997 to 2005, we didn't experience
    much advancement until Hinton struck again in 2006 when he and his team proposed
    the **Deep Belief Network** (**DBN**) by stacking multiple RBMs. Then, in 2012,
    Hinton invented dropout, which significantly improves regularization and overfitting
    in deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: After that, Ian Goodfellow *et al.* introduced GANs, which was a significant
    milestone in image recognition. In 2017, Hinton proposed CapsNet to overcome the
    limitation of regular CNNs, which was one of the most significant milestones so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: How does an ANN learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based on the concept of biological neurons, the term and idea of ANNs arose.
    Similar to biological neurons, the artificial neuron consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: One or more incoming connections that aggregate signals from neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more output connections for carrying the signal to the other neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An activation function, which determines the numerical value of the output signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besides the state of a neuron, synaptic weight is considered, which influences
    the connection within the network. Each weight has a numerical value indicated
    by *W[ij]*, which is the synaptic weight connecting neuron *i* to neuron *j*.
    Now, for each neuron *i*, an input vector can be defined by *x[i] = (x[1], x[2],…x[n])* and
    a weight vector can be defined by *w[i] = (w[i1], w[i2],…w[in])*. Now, depending
    on the position of a neuron, the weights and the output function determine the
    behavior of an individual neuron. Then, during forward propagation, each unit
    in the hidden layer gets the following signal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92d1e4fa-5382-4d0e-badf-3dd0b12929e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Nevertheless, among the weights, there is also a special type of weight called
    a bias unit, *b*. Technically, bias units aren''t connected to any previous layer,
    so they don''t have true activity. But still, the bias *b* value allows the neural
    network to shift the activation function to the left or right. By taking the bias
    unit into consideration, the modified network output is formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b946acf-c821-45ca-bada-d40d862b86ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation signifies that each hidden unit gets the sum of inputs,
    multiplied by the corresponding weight—this is known as the **Summing junction**.
    Then, the resultant output in the **Summing junction** is passed through the activation
    function, which squashes the output, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10255eed-9719-413b-abc4-ce71fb87c327.png)'
  prefs: []
  type: TYPE_IMG
- en: Working principal of an artificial neuron model
  prefs: []
  type: TYPE_NORMAL
- en: 'A practical neural network architecture, however, is composed of input, hidden,
    and output layers that are composed of *nodes* that make up a network structure,
    but still follow the working principal of an artificial neuron model, as shown
    in the preceding diagram. The input layer only accepts numeric data, such as features
    in real numbers, images with pixel values, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab4cea48-30f0-49c0-a88d-e9f40e6f7fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: A neural network with one input layer, three hidden layers, and an output layer
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the hidden layers perform most of the computation to learn the patterns
    and the network evaluates how accurate its prediction is compared to the actual
    output using a special mathematical function called the loss function. It could
    be a complex one or a very simple mean squared error, which can be defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4eeafcd-746d-48a5-8a91-7e3c3016ba17.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/2fe01c92-f29a-441d-b7b0-094ae8aaedc5.png)
    signifies the prediction made by the network, while *Y* represents the actual
    or expected output. Finally, when the error is no longer being reduced, the neural
    network converges and makes prediction through the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning process for a neural network is configured as an iterative process
    of the optimization of the weights. The weights are updated in each epoch. Once
    the training starts, the aim is to generate predictions by minimizing the loss
    function. The performance of the network is then evaluated on the test set. We
    already know about the simple concept of an artificial neuron. However, generating
    only some artificial signals is not enough to learn a complex task. As such, a
    commonly used supervised learning algorithm is the backpropagation algorithm,
    which is very commonly used to train a complex ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, training such a neural network is an optimization problem too, in
    which we try to minimize the error by adjusting network weights and biases iteratively,
    by using backpropagation through **gradient descent** (**GD**). This approach
    forces the network to backtrack through all its layers to update the weights and
    biases across nodes in the opposite direction of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: However, this process using GD does not guarantee that the global minimum is
    reached. The presence of hidden units and the nonlinearity of the output function
    means that the behavior of the error is very complex and has many local minimas.
    This backpropagation step is typically performed thousands or millions of times,
    using many training batches, until the model parameters converge to values that
    minimize the cost function. The training process ends when the error on the validation
    set begins to increase, because this could mark the beginning of a phase overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The downside of using GD is that it takes too long to converge, which makes
    it impossible to meet the demand of handling large-scale training data. Therefore,
    a faster GD, called **Stochastic Gradient Descent** (**SDG**) was proposed, which
    is also a widely used optimizer in DNN training. In SGD, we use only one training
    sample per iteration from the training set to update the network parameters, which
    is a stochastic approximation of the true cost gradient.
  prefs: []
  type: TYPE_NORMAL
- en: There are other advanced optimizers nowadays such as Adam, RMSProp, ADAGrad,
    Momentum, and so on. Each of them is either an direct or indirect optimized version
    of SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Weight and bias initialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, here''s a tricky question: how do we initialize the weights? Well, if
    we initialize all the weights to the same value (for example, 0 or 1), each hidden
    neuron will get exactly the same signal. Let''s try to break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: If all weights are initialized to 1, then each unit gets a signal equal to the
    sum of the inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If all weights are 0, which is even worse, then every neuron in a hidden layer
    will get zero signal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For network weight initialization, Xavier initialization is used widely. It
    is similar to random initialization but often turns out to work much better, since
    it can identify the rate of initialization depending on the total number of input
    and output neurons by default.
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering whether you can get rid of random initialization while
    training a regular DNN. Well, recently, some researchers have been talking about
    random orthogonal matrix initialization's that perform better than just any random
    initialization for training DNNs. When it comes to initializing the biases, we
    can initialize them to be zero.
  prefs: []
  type: TYPE_NORMAL
- en: But setting the biases to a small constant value, such as 0.01 for all biases, ensures
    that all **rectified linear units** (**ReLUs**) can propagate some gradient. However,
    it neither performs well nor shows consistent improvement. Therefore, sticking
    with zero is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To allow a neural network to learn complex decision boundaries, we apply a
    non-linear activation function to some of its layers. Commonly used functions
    include Tanh, ReLU, softmax, and variants of these. More technically, each neuron
    receives a signal of the weighted sum of the synaptic weights and the activation
    values of the neurons that are connected as input. One of the most widely used
    functions for this purpose is the so-called sigmoid logistic function, which is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/485cc8f4-cfc3-42a1-8d14-b6f5d143320d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The domain of this function includes all real numbers, and the co-domain is
    (0, 1). This means that any value obtained as an output from a neuron (as per
    the calculation of its activation state) will always be between zero and one.
    The **Sigmoid** function, as represented in the following diagram, provides an
    interpretation of the saturation rate of a neuron, from not being active (equal
    to **0**) to complete saturation, which occurs at a predetermined maximum value
    (equal to **1**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71b9595a-1e70-44ca-bb40-9c67cbb70d24.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid versus Tanh activation function
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, a hyperbolic tangent, or **Tanh**, is another form of activation
    function. **Tanh** flattens a real-valued number between **-1** and **1**. The
    preceding graph shows the difference between **Tanh** and **Sigmoid** activation
    functions. In particular, mathematically, *tanh* activation function can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a68918c2-518e-4731-8602-c27cdd1d5beb.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, in the last level of an **feedforward neural network** (**FFNN**),
    the softmax function is applied as the decision boundary. This is a common case,
    especially when solving a classification problem. The softmax function used for
    the probability distribution over the possible classes in a multiclass classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: For a regression problem, we do not need to use any activation function since
    the network generates continuous values—that is, probabilities. However, I've
    seen people using the IDENTITY activation function for regression problems nowadays.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, choosing proper activation functions and network weight initializations
    are two problems that make a network perform at its best and help to obtain good
    training. Now that we know the brief history of neural networks, let's deep-dive
    into different architectures in the next section, which will give us an idea on
    their usage.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can categorize DL architectures into four groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep neural networks** (**DNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emergent architectures** (**EAs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, DNNs, CNNs, and RNNs have many improved variants. Although most of
    the variants are proposed or developed for solving domain-specific research problems,
    the basic working principles still follow the original DNN, CNN, and RNN architectures.
    The following subsections will give you a brief introduction to these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DNNs are neural networks that have a complex and deeper architecture with a
    large number of neurons in each layer, and many connections between them. Although
    DNN refers to a very deep network, for simplicity, we consider MLP, **stacked
    autoencoder** (**SAE**), and **deep belief networks** (**DBNs**) as DNN architectures.
    These architectures mostly work as an FFNN, meaning information propagates from
    input to output layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple perceptrons are stacked together as MLPs, where layers are connected
    as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since
    it has three layers: an input layer, a hidden layer, and an output layer. This
    way, the signal propagates one way, from the input layer to the hidden layers
    to the output layer, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a34e4760-33b6-4a15-9cb4-cb5cd82e407d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Autoencoders and RBMs are the basic building blocks for SAEs and DBNs, respectively.
    Unlike MLP, which is an FFNN that''s trained in a supervised way, both SAEs and
    DBNs are trained in two phases: unsupervised pre-training and supervised fine-tuning.
    In unsupervised pre-training, layers are stacked in order and trained in a layer-wise
    manner with used, unlabeled data. In supervised fine-tuning, an output classifier
    layer is stacked and the complete neural network is optimized by retraining with
    labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One problem with MLP is that it often overfits the data, so it doesn''t generalize
    well. To overcome this issue, DBN was proposed by Hinton *et al.* It uses a greedy,
    layer-by-layer, pre-training algorithm. DBNs are composed of a visible layer and
    multiple hidden unit layers. The building blocks of a DBN are RBMs, as shown in
    the following diagram, where several RBMs are stacked one after another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3e89b54-3a46-4dd1-b198-34325301e352.png)'
  prefs: []
  type: TYPE_IMG
- en: The top two layers have undirected, symmetric connections in between, but the
    lower layers have directed connections from the preceding layer. Despite numerous
    successes, DBNs are now being replaced with AEs.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AEs are also special types of neural networks that learn automatically from
    the input data. AEs consists of two components: the encoder and the decoder. The
    encoder compresses the input into a latent-space representation. Then, the decoder
    part tries to reconstruct the original input data from this representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: Encodes or compresses the input into a latent-space representation
    using a function known as *h=f(x)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: Decodes or reconstructs the input from the latent space representation
    using a function known as *r=g(h)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, an AE can be described by a function of *g(f(x)) = o*, where we want *0*
    as close as the original input of *x*. The following diagram shows how an AE typically
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a6851b9-4411-461d-b66e-7e92955c723c.png)'
  prefs: []
  type: TYPE_IMG
- en: AEs are very useful at data denoising and dimensionality reduction for data
    visualization. AEs can learn data projections, called representations, more effectively
    than PCA.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs have achieved much and have a wide adoption in computer vision (for example,
    image recognition). In CNN networks, the connections schemes are significantly
    different compared to an MLP or DBN. A few of the convolutional layers are connected
    in a cascade style. Each layer is backed up by a ReLU layer, a pooling layer,
    and additional convolutional layers (+ReLU), and another pooling layer, which
    is followed by a fully connected layer and a softmax layer. The following diagram
    is a schematic of the architecture of a CNN that's used for facial recognition,
    which takes facial images as input and predicts emotions such as anger, disgust,
    fear, happy, sad and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/27a311c6-d8b2-467b-aacd-a482d6de6746.png)'
  prefs: []
  type: TYPE_IMG
- en: A schematic architecture of a CNN used for facial recognition
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, DNNs have no prior knowledge of how the pixels are organized because
    they do not know that nearby pixels are close. CNNs embed this prior knowledge
    using lower layers by using feature maps in small areas of the image, while the
    higher layers combine lower-level features into larger features.
  prefs: []
  type: TYPE_NORMAL
- en: This works well with most of the natural images, giving CNNs a decisive head
    start over DNNs. The output from each convolutional layer is a set of objects,
    called feature maps, that are generated by a single kernel filter. Then, the feature
    maps can be used to define a new input to the next layer. Each neuron in a CNN
    network produces an output, followed by an activation threshold, which is proportional
    to the input and not bound.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In RNNs, connections between units form a directed cycle. The RNN architecture
    was originally conceived by Hochreiter and Schmidhuber in 1997\. RNN architectures
    have standard MLPs, plus added loops so that they can exploit the powerful nonlinear
    mapping capabilities of the MLP. They also have some form of memory. The following
    diagram shows a very basic RNN that has an input layer, two recurrent layers,
    and an output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a79f8197-278c-46ab-9f04-735a6f4720a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this basic RNN suffers from gradient vanishing and the exploding problem,
    and cannot model long-term dependencies. These architectures include LSTM, **gated
    recurrent units** (**GRUs**), bidirectional-LSTM, and other variants. Consequently,
    LSTM and GRU can overcome the drawbacks of regular RNNs: the gradient vanishing/exploding
    problem and long-short term dependency.'
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ian Goodfellow *et al.* introduced GANs in a paper named *Generative Adversarial
    Nets* (see more at [https:/​/​arxiv.​org/abs/​1406.​2661v1](https://arxiv.org/pdf/1406.2661v1.pdf)).
    The following diagram briefly shows the working principles of a GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/704ace27-a78b-4d83-9a16-a160d9c4230c.png)'
  prefs: []
  type: TYPE_IMG
- en: Working principles of GAN
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs are deep neural network architectures that consist of two networks, a
    generator, and a discriminator, that are pitted against each other (hence the
    name, *adversarial*):'
  prefs: []
  type: TYPE_NORMAL
- en: The **Generator** tries to generate data samples out of a specific probability
    distribution and is very similar to the actual object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Discriminator** will judge whether its input is coming from the original
    training set or from the generator part
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many DL practitioners think that GANs were one of the most important advancements
    because GANs can be used to mimic any distribution of data, and based on the data
    distribution, GANs can be taught to create robot artist images, super-resolution
    images, text-to-image synthesis, music, speech, and more.
  prefs: []
  type: TYPE_NORMAL
- en: For example, because of the concept of adversarial training, Facebook's AI research
    director, Yann LeCun, called GANs the most interesting idea in the last 10 years
    of ML.
  prefs: []
  type: TYPE_NORMAL
- en: Capsule networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In CNNs, each layer understands an image at a much more granular level through
    a slow receptive field or max pooling operations. If the images have rotation,
    tilt, or very different shapes or orientation, CNNs fail to extract such spatial
    information and show very poor performance at image processing tasks. Even the
    pooling operations in CNNs cannot much help against such positional invariance.
    This issue in CNNs has led us to the recent advancement of CapsNet through the
    paper titled *Dynamic Routing Between Capsules* (see more at [https:/​/​arxiv.​org/​abs/​1710.​09829](https://arxiv.org/pdf/1710.09829.pdf))
    by Geoffrey Hinton *et al*:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A capsule is a group of neurons whose activity vector represents the instantiation
    parameters of a specific type of entity, such as an object or an object part."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike a regular DNN, where we keep on adding layers, in CapsNets, the idea
    is to add more layers inside a single layer. This way, a CapsNet is a nested set
    of neural layers. In CapsNet, the vector inputs and outputs of a capsule are computed
    using the routing algorithm, which iteratively transfers information and process
    **self-consistent field** (**SCF**) procedure, used in physics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba38228e-2977-4355-8110-f87b8e1970b2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a schematic diagram of a simple three-layer CapsNet.
    The length of the activity vector of each capsule in the **DigiCaps** layer indicates
    the presence of an instance of each class, which is used to calculate the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about the working principles of neural networks and
    the different neural network architectures, implementing something hands-on would
    be great. However, before that, let's take a look at some popular DL libraries
    and frameworks, which come with the implementation of these network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: DL frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several popular DL frameworks. Each of them comes with some pros and
    cons. Some of them are desktop-based and some of them are cloud-based platforms
    where you can deploy/run your DL applications. However, most of the libraries
    that are released under an open license help when people are using graphics processors,
    which can ultimately help in speeding up the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such frameworks and libraries include TensorFlow, PyTorch, Keras, Deeplearning4j,
    H2O, and the **Microsoft Cognitive Toolkit** (**CNTK**). Even a few years back,
    other implementations including Theano, Caffee, and Neon were used widely. However,
    these are now obsolete. Since we will focus on learning in Scala, JVM-based DL
    libraries such as Deeplearning4j can be a reasonable choice. **Deeplearning4j**
    (**DL4J**) is one of the first commercial-grade, open source, distributed DL libraries
    that was built for Java and Scala. This also provides integrated support for Hadoop
    and Spark. DL4J is built for use in business environments on distributed GPUs
    and CPUs. DL4J aims to be cutting-edge and Plug and Play, with more convention
    than configuration, which allows for fast prototyping for non-researchers. The
    following diagram shows last year''s Google Trends, illustrating how popular TensorFlow
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f27e1f6d-cc5a-4c23-bde9-1f0576077c29.png)'
  prefs: []
  type: TYPE_IMG
- en: Trends of different DL frameworks—TensorFlow and Keras are dominating the most;
    however, Theano is losing its popularity; on the other hand, Deeplearning4j is
    emerging for JVM
  prefs: []
  type: TYPE_NORMAL
- en: Its numerous libraries can be integrated with DL4J and will make your JVM experience
    easier, regardless of whether you are developing your ML application in Java or
    Scala. Similar to NumPy for JVM, ND4J comes up with basic operations of linear
    algebra (matrix creation, addition, and multiplication). However, ND4S is a scientific
    computing library for linear algebra and matrix manipulation. It also provides
    n-dimensional arrays for JVM-based languages.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the preceding libraries, there are some recent initiatives for DL
    on the cloud. The idea is to bring DL capability to big data with millions of
    billions of data points and high dimensional data. For example, **Amazon Web Services**
    (**AWS**), Microsoft Azure, Google Cloud Platform, and **NVIDIA GPU Cloud** (**NGC**)
    all offer machine and DL services that are native to their public clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In October 2017, AWS released **Deep Learning AMIs** (**DLAMIs**) for **Amazon
    Elastic Compute Cloud** (**Amazon EC2**) P3 instances. These AMIs come pre-installed
    with DL frameworks, such as TensorFlow, Gluon, and Apache MXNet, which are optimized
    for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances. The DL service
    currently offers three types of AMIs: Conda AMI, Base AMI, and AMI with Source
    Code.'
  prefs: []
  type: TYPE_NORMAL
- en: The CNTK is Azure's open source, DL service. Similar to AWS' offering, it focuses
    on tools that can help developers build and deploy DL applications. The toolkit
    is installed in Python 2.7, in the root environment. Azure also provides a model
    gallery that includes resources, such as code samples, to help enterprises get
    started with the service.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated
    containers (see [https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)).
    The NGC features containerized deep learning frameworks such as TensorFlow, PyTorch,
    MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the
    latest NVIDIA GPUs on participating cloud-service providers. Nevertheless, there
    are also third-party services available through their respective marketplaces.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know the working principles of neural network architectures and
    have seen a brief overview on available DL frameworks for implementing DL solutions,
    let's move on to the next section for some hands-on learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large-scale cancer genomics data often comes in multi-platform and heterogeneous
    forms. These datasets impose great challenges in terms of the bioinformatics approach
    and computational algorithms. Numerous researchers have proposed to utilize this
    data to overcome several challenges, using classical ML algorithms as either the
    primary subject or a supporting element for cancer diagnosis and prognosis.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Genomics data covers all data related to DNA on living things. Although in this
    thesis we will also use other types of data, as such as transcriptomic data (RNA
    and miRNA), for convenience purposes, all data will be termed as genomics data.
    Research on human genetics made a huge breakthrough in recent years due to the
    success of the **Human Genome Project** (**HGP**) (1984-2000) on sequencing the
    full sequence of human DNA. Now, let's see what a real-life dataset looks like
    that can be used for our purposes. We will be using the *gene expression cancer
    RNA-Seq* dataset, which can be downloaded from the UCI ML repository (see [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq) for
    more information).
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is a random subset of another dataset that was reported in the
    following paper: Weinstein, John N., *et al*. *The cancer genome atlas pan-cancer
    analysis project.* Nature Genetics 45.10 (2013): 1113-1120\. The name of the project
    is The Pan-Cancer analysis project. It assembled data from thousands of patients
    with primary tumors occurring in different sites of the body. It covered 12 tumor
    types, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Glioblastoma multiforme** (**GBM**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lymphoblastic acute myeloid leukemia** (**AML**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head and neck squamous carcinoma** (**HNSC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lung adenocarcinoma** (**LUAD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L****ung squamous carcinoma** (**LUSC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Breast carcinoma** (**BRCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kidney renal clear cell carcinoma** (**KIRC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ovarian carcinoma** (**OV**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bladder carcinoma** (**BLCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Colon adenocarcinoma** (**COAD**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uterine cervical and endometrial carcinoma** (**UCEC**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rectal adenocarcinoma** (**READ**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This collection of data is a part of the RNA-Seq (HiSeq) PANCAN dataset. It
    is a random extraction of gene expressions of patients that have different types
    of tumors: BRCA, KIRC, COAD, LUAD, and PRAD.'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is a random collection of cancer patients from 801 patients, each
    having 20,531 attributes. Samples (`instances`) are stored row-wise. Variables
    (`attributes`) of each sample are RNA-Seq gene expression levels measured by the
    Illumina HiSeq platform. A dummy name (`gene_XX`) is provided for each attribute.
    The attributes are ordered consistently with the original submission. For example,
    `gene_1` on `sample_0` is significantly and differentially expressed with a value
    of `2.01720929003`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you download the dataset, you will see that there are two CSV files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data.csv`: Contains the gene expression data of each sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels.csv`: The labels associated with each sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at the processed dataset. Note that we will only look at
    a few select features considering the high dimensionality in the following screenshot,
    where the first column represents sample IDs (that is, anonymous patient IDs).
    The rest of the columns represent how a certain gene expression occurs in the
    tumor samples of the patients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a15ea25-0ace-4c47-9600-0e95ac5624dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, look at the labels in the following table. Here, the `id` column contains
    the sample IDs and the `Class` column represents the cancer labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0001e482-8aeb-42d1-9307-0c8eba945c9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, you can imagine why I have chosen this dataset. Although we will not have
    many samples, the dataset is still highly dimensional. In addition, this type
    of high dimensional dataset is very suitable for applying a DL algorithm. Therefore,
    if the features and labels are given, can we classify these samples based on features
    and the ground truth? Why not? We will try to solve this problem with the DL4J
    library. First, we have to configure our programming environment so that we can
    write our code.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the programming environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how to configure DL4J, ND4s, Spark, and ND4J
    before getting started with the coding. The following are the prerequisites that
    you must take into account when working with DL4J:'
  prefs: []
  type: TYPE_NORMAL
- en: Java 1.8+ (64-bit only)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Maven for an automated build and dependency manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ IDEA or Eclipse IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Git for version control and CI/CD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following libraries can be integrated with DJ4J to enhance your JVM experience
    while developing your ML applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DL4J**: The core neural network framework, which comes with many DL architectures
    and underlying functionalities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ND4J**: Can be considered as the NumPy of JVM. It comes with some basic operations
    of linear algebra. Examples are matrix creation, addition, and multiplication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataVec**: This library enables ETL operations while performing feature engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JavaCPP**: This library acts as the bridge between Java and Native C++.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arbiter**: This library provides basic evaluation functionalities for the
    DL algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL4J**: Deep reinforcement learning for the JVM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ND4S**: This is a scientific computing library, and it also supports n-dimensional
    arrays for JVM-based languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are using Maven on your preferred IDE, let''s define the project properties
    to mention these versions in the `pom.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use all the dependencies required for DL4J, ND4S, and ND4J, as shown
    in the `pom.xml` file. By the way, DL4J comes with Spark 2.1.0\. Additionally,
    if a native system BLAS is not configured on your machine, ND4J''s performance
    will be reduced. You will experience the following warning once you execute any
    simple code written in Scala:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'However, installing and configuring a BLAS, such as OpenBLAS or IntelMKL, is
    not that difficult; you can invest some time and do it. Refer to the following
    URL for further details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open)'
  prefs: []
  type: TYPE_NORMAL
- en: Well done! Our programming environment is ready for simple DL application development.
    Now, it's time to get our hands dirty with some sample code.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we do not have any unlabeled data, I would like to select some samples
    randomly for testing. One more thing to note is that features and labels come
    in two separate files. Therefore, we can perform the necessary preprocessing and
    then join them together so that our preprocessed data will have features and labels
    together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the rest of the data will be used for training. Finally, we''ll save
    the training and testing sets in a separate CSV file to be used later on. Follow
    these steps to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the samples and see the statistics. Here, we use the `read()`
    method of Spark, but specify the necessary options and format too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will see some related statistics, such as the number of features and
    the number of samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, there are `801` samples from `801` distinct patients and the dataset
    is too high in dimensions, since it has `20532` features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, since the `id` column represents only the patient''s anonymous
    ID, so we can simply drop it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the labels using the `read()` method of Spark and also specify
    the necessary options and format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We have already seen what the label DataFrame looks like. We will skip the
    `id`. However, the `Class` column is categorical. As we mentioned previously,
    DL4J does not support categorical labels that need to be predicted. Therefore,
    we have to convert it into a numeric format (an integer, to be more specific);
    for that, I would use `StringIndexer()` from Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `StringIndexer()`, we apply the index operation to the `Class`
    column, and rename it `label`. Additionally, we `skip` null entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we perform the indexing operation by calling the `fit()` and `transform()`
    operations, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s take a look at the indexed DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line of code should convert the `label` column in numeric format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/583202d7-f812-471d-a3af-03284a68f1b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fantastic! Now, all the columns (including features and labels) are numeric. Thus,
    we can join both features and labels into a single DataFrame. For that, we can
    use the `join()` method from Spark, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can generate both the training and test sets by randomly splitting `combinedDF`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see the `count` of samples in each set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'There should be 561 samples in the training set and 240 samples in the test
    set. Finally, we save them in separate CSV files to be used later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the training and test sets, we can train the network with the
    training set and evaluate the model with the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark will generate CSV files under the `output` folder, under the project root.
    However, you might see a very different name. I suggest that you to rename them
    `TCGA_train.csv` and `TCGA_test.csv` for the training and test sets, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the high dimensionality, I would rather try a better network such
    as LSTM, which is an improved variant of RNN. At this point, some contextual information
    about LSTM would be helpful to grasp this idea, and will be provided after the
    following section.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we prepared the training and test sets. However, we
    need to put some extra effort into making them consumable by DL4J. To be more
    specific, DL4J expects the training data in numeric format and the last column
    to be the `label` column. The remaining data should be features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now try to prepare our training and test sets like that. First, we
    will find the files where we saved the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will define the required parameters, such as the number of features,
    number of classes, and batch size. Here, I am using `128` as the `batchSize`,
    but you can adjust it accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset is used for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the data we want to classify:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding two lines of code, `readCSVDataset()` is basically
    a wrapper that reads the data in CSV format, and then the `RecordReaderDataSetIterator()`
    method converts the record reader into a dataset iterator.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM network construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a neural network with DL4J starts with `MultiLayerConfiguration`,
    which organizes network layers and their hyperparameters. Then, the created layers
    are added using the `NeuralNetConfiguration.Builder()` interface. As shown in
    the following diagram, the LSTM network consists of five layers: an input layer,
    which is followed by three LSTM layers. The last layer is an RNN layer, which
    is also the output layer in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6eb2b39a-02bf-43e6-bf32-2b55178d0b74.png)'
  prefs: []
  type: TYPE_IMG
- en: An LSTM network for cancer type prediction, which takes 20,531 features and
    fixed bias (that is, 1) and generates multi-class outputs
  prefs: []
  type: TYPE_NORMAL
- en: 'To create LSTM layers, DL4J provides the implementation of an LSTM class. However,
    before we start creating layers for the network, let''s define some hyperparameters,
    such as the number of input/hidden/output nodes (neurons):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create the network by specifying layers. The first, second, and third
    layers are LSTM layers. The last layer is an RNN layer. For all the hidden LSTM
    layers, we specify the number of input and output units, and we use ReLU as the
    activation function. However, since it''s a multiclass classification problem,
    we use `SOFTMAX` as the `activation` function for the output layer, with `MCXNET`
    as the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, the softmax `activation` function gives a probability
    distribution over classes, and `MCXENT` is the cross-entropy loss function in
    a multiclass classification setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, with DL4J, we add the layers we created earlier using the `NeuralNetConfiguration.Builder()`
    interface. First, we add all the LSTM layers, which are followed by the final
    RNN output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we used SGD as the optimizer, which tries to optimize
    the `MCXNET` loss function. Then, we initialize the network weight using `XAVIER`,
    and `Adam` acts as the network updater with SGD. Finally, we initialize a multilayer
    network using the preceding multilayer configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Additionally, we can inspect the number of hyperparameters across layers and
    in the whole network. Typically, this type of network has a lot of hyperparameters.
    Let''s print the number of parameters in the network (and for each layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As I stated previously, our network has 910 million parameters, which is huge.
    This also poses a great challenge while tuning hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Network training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will create a `MultiLayerNetwork` using the preceding `MultiLayerConfiguration`.
    Then, we will initialize the network and start the training on the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we also specify that we do not need to do any pre-training (which is
    typically needed in DBN or stacked autoencoders).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the training has been completed, the next task is to evaluate the model,
    which we'll do on the test set here. For the evaluation, we will be using the `Evaluation()` method.
    This method creates an evaluation object with five possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s iterate the evaluation over every test sample and get the network''s
    prediction from the trained model. Finally, the `eval()` method checks the prediction
    against the true class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Wow! Unbelievable! Our LSTM network has accurately classified the samples.
    Finally, let''s see how the classifier predicts across each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The predictive accuracy for cancer type prediction using LSTM is suspiciously
    higher, isn't it? Did our model underfit? Did our model overfit?
  prefs: []
  type: TYPE_NORMAL
- en: Observing the training using Deeplearning4j UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our accuracy is suspiciously higher, we can observe how the training went.
    Yes, there are ways to find out if it went through overfitting, since we can observe
    the training, validation, and test losses on the DL4J UI. However, I won't discuss
    the details here. Take a look at [https://deeplearning4j.org/docs/latest/deeplearning4j-nn-visualization](https://deeplearning4j.org/docs/latest/deeplearning4j-nn-visualization) for
    more information on how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to classify cancer patients on the basis of tumor
    types from a very high-dimensional gene expression dataset curated from TCGA.
    Our LSTM architecture managed to achieve 99% accuracy, which is outstanding. Nevertheless, we
    discussed many aspects of DL4J, which will be helpful in upcoming chapters. Finally,
    we saw answers to some frequent questions related to this project, LSTM networks,
    and DL4J hyperparameters/network tuning.
  prefs: []
  type: TYPE_NORMAL
- en: This is, more or less, the end of our little journey in developing ML projects
    using Scala and different open source frameworks. Throughout these chapters, I
    have tried to provide you with several examples of how to use these wonderful
    technologies efficiently for developing ML projects. While writing this book,
    I had to keep many constraints in my mind; for example, the page count, API availability,
    and of course, my expertise.
  prefs: []
  type: TYPE_NORMAL
- en: However, overall, I tried to make the book simple by avoiding unnecessary details
    on the theory, as you can read about that in many books, blogs, and websites.
    I will also keep the code of this book updated on the GitHub repository at [https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide).
    Feel free to open a new issue or any pull request to improve the code and stay
    tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I'll upload the solution to each chapter as Zeppelin notebooks
    so that you can run the code interactively. By the way, Zeppelin is a web-based
    notebook that enables data-driven, interactive data analytics, and collaborative
    documents with SQL and Scala. Once you have configured Zeppelin on your preferred
    platform, you can download the notebook from the GitHub repository, import them
    into Zeppelin, and get going. For more detail, you can take a look at [https://zeppelin.apache.org/](https://zeppelin.apache.org/).
  prefs: []
  type: TYPE_NORMAL
