- en: Introduction to Deep Learning with Scala
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala进行回归分析简介
- en: Throughout [Chapter 2](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml), *Scala for
    Regression Analysis*, to [Chapter 6](4935de5b-9527-4ff2-82ed-927dea04c77a.xhtml),
    *Scala for Recommender System*, we have learned about linear and classic **machine
    learning** (**ML**) algorithms through real-life examples. In this chapter, we
    will explain some basic concepts of **deep learning** (**DL**). We will start
    with DL, which is one of the emerging branches of ML. We will briefly discuss
    some of the most well-known and widely used neural network architectures and DL
    frameworks and libraries.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第2章](f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml) “使用Scala进行回归分析” 到 [第6章](4935de5b-9527-4ff2-82ed-927dea04c77a.xhtml)
    “使用Scala进行推荐系统” 中，我们通过实际案例学习了线性经典 **机器学习**（**ML**）算法。在本章中，我们将解释一些 **深度学习**（**DL**）的基本概念。我们将从深度学习开始，这是机器学习的一个新兴分支。我们将简要讨论一些最著名和最广泛使用的神经网络架构和深度学习框架和库。
- en: 'Finally, we will use the **Long Short-Term Memory** (**LSTM**) architecture
    for cancer type classification from a very high-dimensional dataset curated from
    **The Cancer Genome Atlas** (**TCGA**). The following topics will be covered in
    this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用来自 **The Cancer Genome Atlas**（**TCGA**）的非常高维数据集的 **长短期记忆**（**LSTM**）架构进行癌症类型分类。本章将涵盖以下主题：
- en: DL versus ML
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习与机器学习
- en: DL and neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习与神经网络
- en: Deep neural network architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络架构
- en: DL frameworks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习框架
- en: Getting started with learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始学习
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的机器上已安装并配置了Scala 2.11.x和Java 1.8.x。
- en: 'The code files of this chapters can be found on GitHub:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07)'
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用：
- en: '[http://bit.ly/2vwrxzb](http://bit.ly/2vwrxzb)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2vwrxzb](http://bit.ly/2vwrxzb)'
- en: DL versus ML
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与机器学习
- en: 'Simple ML methods that were used in small-scale data analysis are not effective
    anymore because the effectiveness of ML methods diminishes with large and high-dimensional
    datasets. Here comes DL—a branch of ML based on a set of algorithms that attempt
    to model high-level abstractions in data. Ian Goodfellow *et al.* (Deep Learning,
    MIT Press, 2016) defined DL as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在小规模数据分析中使用的一些简单机器学习方法不再有效，因为随着大型和高维数据集的增加，机器学习方法的有效性会降低。于是出现了深度学习——一种基于一组试图在数据中模拟高级抽象的算法的机器学习分支。Ian
    Goodfellow 等人（《深度学习》，麻省理工学院出版社，2016年）将深度学习定义为如下：
- en: '"Deep learning is a particular kind of machine learning that achieves great
    power and flexibility by learning to represent the world as a nested hierarchy
    of concepts, with each concept defined in relation to simpler concepts, and more
    abstract representations computed in terms of less abstract ones."'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “深度学习是一种特殊的机器学习方法，通过学习将世界表示为嵌套的概念层次结构，每个概念都是相对于更简单的概念定义的，并且更抽象的表示是通过更不抽象的表示来计算的，从而实现了强大的功能和灵活性。”
- en: Similar to the ML model, a DL model also takes in an input, *X*, and learns
    high-level abstractions or patterns from it to predict an output of *Y*. For example,
    based on the stock prices of the past week, a DL model can predict the stock price
    for the next day. When performing training on such historical stock data, a DL
    model tries to minimize the difference between the prediction and the actual values.
    This way, a DL model tries to generalize to inputs that it hasn't seen before
    and makes predictions on test data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习模型类似，深度学习模型也接受一个输入 *X*，并从中学习高级抽象或模式以预测输出 *Y*。例如，基于过去一周的股票价格，深度学习模型可以预测下一天的股票价格。在训练此类历史股票数据时，深度学习模型试图最小化预测值与实际值之间的差异。这样，深度学习模型试图推广到它之前未见过的新输入，并在测试数据上做出预测。
- en: 'Now, you might be wondering, if an ML model can do the same tasks, why do we
    need DL for this? Well, DL models tend to perform well with large amounts of data,
    whereas old ML models stop improving after a certain point. The core concept of
    DL is inspired by the structure and function of the brain, which are called **artificial
    neural networks** (**ANNs**). Being at the core of DL, ANNs help you learn the
    associations between sets of inputs and outputs in order to make more robust and
    accurate predictions. However, DL is not only limited to ANNs; there have been
    many theoretical advances, software stacks, and hardware improvements that bring
    DL to the masses. Let''s look at an example; suppose we want to develop a predictive
    analytics model, such as an animal recognizer, where our system has to resolve
    two problems:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道，如果ML模型可以完成同样的任务，为什么我们还需要DL？嗯，DL模型在大数据量下往往表现良好，而旧的ML模型在某个点之后就会停止改进。DL的核心概念灵感来源于大脑的结构和功能，被称为**人工神经网络**（**ANNs**）。作为DL的核心，ANNs帮助您学习输入和输出集合之间的关联，以便做出更稳健和准确的预测。然而，DL不仅限于ANNs；已经有许多理论进步、软件堆栈和硬件改进，使DL普及。让我们看一个例子；假设我们想要开发一个预测分析模型，例如动物识别器，我们的系统必须解决两个问题：
- en: To classify whether an image represents a cat or a dog
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要分类图像是否代表猫或狗
- en: To cluster images of dogs and cats
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要对猫和狗的图像进行聚类
- en: If we solve the first problem using a typical ML method, we must define the
    facial features (ears, eyes, whiskers, and so on) and write a method to identify
    which features (typically nonlinear) are more important when classifying a particular
    animal.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用典型的机器学习（ML）方法来解决第一个问题，我们必须定义面部特征（耳朵、眼睛、胡须等）并编写一个方法来识别在分类特定动物时哪些特征（通常是非线性）更重要。
- en: 'However, at the same time, we cannot address the second problem because classical
    ML algorithms for clustering images (such as k-means) cannot handle nonlinear
    features. Take a look at the following diagram, which shows a workflow that we
    would follow whether we wanted to classify if the given image is of a cat:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与此同时，我们无法解决第二个问题，因为用于聚类图像的经典ML算法（如k-means）无法处理非线性特征。看看以下流程图，它显示了如果我们想要分类给定的图像是否为猫时我们将遵循的流程：
- en: '![](img/a07fad8a-a998-4ae3-9b5f-662fa5a9dbe0.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a07fad8a-a998-4ae3-9b5f-662fa5a9dbe0.png)'
- en: DL algorithms will take these two problems one step further, and the most important
    features will be extracted automatically after determining which features are
    the most important for classification or clustering. In contrast, when using a
    classical ML algorithm, we would have to provide the features manually.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: DL算法将这两个问题进一步推进，在确定哪些特征对分类或聚类最为重要后，最重要的特征将被自动提取。相比之下，当使用经典ML算法时，我们必须手动提供特征。
- en: A DL algorithm would take more sophisticated steps instead. For example, first,
    it would identify the edges that are the most relevant when clustering cats or
    dogs. It would then try to find various combinations of shapes and edges hierarchically.
    This step is called **extract, transform, and load** (**ETL**). Then after several
    iterations, hierarchical identification of complex concepts and features would
    be carried out. Then, based on the identified features, the DL algorithm would
    decide which of these features are most significant for classifying the animal.
    This step is known as feature extraction. Finally, it would take out the label
    column and perform unsupervised training using **autoencoders** (**AEs**) to extract
    the latent features to be redistributed to k-means for clustering. Then, the **clustering
    assignment hardening loss** (**CAH loss**) and reconstruction loss are jointly
    optimized toward optimal clustering assignment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）算法会采取更复杂的步骤。例如，首先，它会识别在聚类猫或狗时最相关的边缘。然后，它会尝试以分层的方式找到各种形状和边缘的组合。这一步被称为**提取、转换和加载**（**ETL**）。然后，经过几次迭代后，将进行复杂概念和特征的分层识别。然后，基于识别出的特征，DL算法将决定哪些特征对分类动物最为重要。这一步被称为特征提取。最后，它会提取标签列并使用**自动编码器**（**AEs**）进行无监督训练，以提取要重新分配给k-means进行聚类的潜在特征。然后，**聚类分配硬化损失**（**CAH损失**）和重建损失将共同优化以实现最佳的聚类分配。
- en: However, in practice, a DL algorithm is fed with a raw image representations,
    which doesn't see an image as we see it because it only knows the position of
    each pixel and its color. The image is divided into various layers of analysis.
    At a lower level, the software analyzes, for example, a grid of a few pixels with
    the task of detecting a type of color or various nuances. If it finds something,
    it informs the next level, which at this point checks whether or not that given
    color belongs to a larger form, such as a line.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，深度学习算法使用的是原始图像表示，它并不像我们看待图像那样看待图像，因为它只知道每个像素的位置及其颜色。图像被划分为各种分析层。在较低层次，软件分析，例如，几个像素的网格，任务是检测某种颜色或各种细微差别。如果它发现某些东西，它会通知下一层，此时该层检查给定的颜色是否属于更大的形状，例如一条线。
- en: 'The process continues to the upper levels until the algorithm understand what
    is shown in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程一直持续到算法理解以下图中所示的内容：
- en: '![](img/1ed01b07-4a89-4d9c-ada7-91ca8d11c792.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ed01b07-4a89-4d9c-ada7-91ca8d11c792.png)'
- en: Although *dog versus cat* is an example of a very simple classifier, software
    that's capable of doing these types of things is now widespread and is found in
    systems for recognizing faces, or in those for searching an image on Google, for
    example. This kind of software is based on DL algorithms. On the contrary, by
    using a linear ML algorithm, we cannot build such applications since these algorithms
    are incapable of handling nonlinear image features.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然**狗与猫**是一个非常简单的分类器的例子，但现在能够执行这些类型任务的软件已经非常普遍，例如在识别面部或搜索谷歌图片的系统中发现。这类软件基于深度学习算法。相反，使用线性机器学习算法，我们无法构建这样的应用程序，因为这些算法无法处理非线性图像特征。
- en: Also, using ML approaches, we typically only handle a few hyperparameters. However,
    when neural networks are brought to the mix, things become too complex. In each
    layer, there are millions or even billions of hyperparameters to tune—so many
    that the cost function becomes non-convex. Another reason for this is that the
    activation functions that are used in hidden layers are nonlinear, so the cost
    is non-convex.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用机器学习方法，我们通常只处理几个超参数。然而，当引入神经网络时，事情变得过于复杂。在每个层中，都有数百万甚至数十亿个超参数需要调整——如此之多，以至于代价函数变得非凸。另一个原因是，在隐藏层中使用的激活函数是非线性的，因此代价是非凸的。
- en: DL and ANNs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与ANNs（人工神经网络）
- en: ANNs, which are inspired by how a human brain works, form the core of deep learning
    and its true realization. Today's revolution around deep learning would have not
    been possible without ANNs. Thus, to understand DL, we need to understand how
    neural networks work.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类大脑工作方式启发的ANNs（人工神经网络）是深度学习的核心和真正实现。今天围绕深度学习的革命如果没有ANNs（人工神经网络）是不可能发生的。因此，为了理解深度学习，我们需要了解神经网络是如何工作的。
- en: ANNs and the human brain
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANNs（人工神经网络）与人类大脑
- en: ANNs represent one aspect of the human nervous system and how the nervous system
    consists of a number of neurons that communicate with each other using axons.
    The receptors receive the stimuli either internally or from the external world.
    Then, they pass this information into the biological neurons for further processing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs（人工神经网络）代表了人类神经系统的一个方面，以及神经系统由许多通过轴突相互通信的神经元组成。感受器接收来自内部或外部世界的刺激。然后，它们将此信息传递给生物神经元以进行进一步处理。
- en: There are a number of dendrites, in addition to another long extension called
    the axon. Toward its extremities, there are minuscule structures called synaptic
    terminals, which are used to connect one neuron to the dendrites of other neurons.
    Biological neurons receive short electrical impulses called signals from other
    neurons, and in response, they trigger their own signals.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了另一个被称为轴突的长延伸之外，还有许多树突。在其末端，有微小的结构称为突触末端，用于将一个神经元连接到其他神经元的树突。生物神经元从其他神经元接收称为信号的短暂电脉冲，作为回应，它们触发自己的信号。
- en: We can thus summarize that the neuron comprises a cell body (also known as the
    soma), one or more dendrites for receiving signals from other neurons, and an
    axon for carrying out the signals that are generated by the neurons. A neuron
    is in an active state when it is sending signals to other neurons. However, when
    it is receiving signals from other neurons, it is in an inactive state. In an
    idle state, a neuron accumulates all the signals that are received before reaching
    a certain activation threshold. This whole thing motivated researchers to test
    out ANNs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以总结说，神经元由细胞体（也称为胞体）、一个或多个用于接收来自其他神经元信号的树突，以及一个用于执行神经元产生的信号的轴突组成。当神经元向其他神经元发送信号时，它处于活跃状态。然而，当它从其他神经元接收信号时，它处于非活跃状态。在空闲状态下，神经元积累所有接收到的信号，直到达到一定的激活阈值。这一切激励研究人员测试人工神经网络（ANNs）。
- en: A brief history of neural networks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络简史
- en: The most significant progress in ANNs and DL can be described with the following
    timeline. We have already seen how the artificial neurons and perceptrons provided
    the base in 1943 and 1958, respectively. Then, the XOR was formulated as a linearly
    non-separable problem in 1969 by Minsky *et al.*, but later, in 1974, Werbos *et
    al*. demonstrated the backpropagation algorithm for training the perceptron.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络和深度学习最显著的进步可以用以下时间线来描述。我们已经看到，人工神经元和感知器分别在1943年和1958年为基础。然后，1969年，明斯基（Minsky）等人将XOR表述为线性不可分问题，但后来在1974年，韦伯斯（Werbos）等人证明了用于训练感知器的反向传播算法。
- en: However, the most significant advancement happened in the 1980s when John Hopfield
    *et al.* proposed the Hopfield network in 1982\. Then, one of the godfathers of
    the neural network and DL, Hinton and his team proposed the Boltzmann machine
    in 1985\. However, probably one of the most significant advances happened in 1986
    when Hinton *et al.* successfully trained the MLP and Jordan *et al.* proposed
    RNNs. In the same year, Smolensky *et al.* also proposed the improved version
    of Boltzmann machine called the **Restricted Boltzmann Machine** (**RBM**).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最显著的进步发生在20世纪80年代，当时约翰·霍普菲尔德（John Hopfield）等人于1982年提出了霍普菲尔德网络。然后，神经网络和深度学习的奠基人之一辛顿及其团队于1985年提出了玻尔兹曼机。然而，可能最显著的进步发生在1986年，当时辛顿等人成功训练了多层感知器（MLP），乔丹等人提出了RNNs。同年，斯莫伦斯基（Smolensky）等人还提出了改进的玻尔兹曼机，称为**受限玻尔兹曼机**（**RBM**）。
- en: 'However, in the 90s era, the most significant year was 1997, when Lecun *et
    al.* proposed LeNet in 1990, and Jordan *et al.* proposed the Recurrent Neural
    Network in 1997\. In the same year, Schuster *et al.* proposed the improved version
    of LSTM and the improved version of the original RNN, called the bidirectional
    RNN. The following timeline provides a brief glimpse into the history of different
    neural network architectures:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在20世纪90年代，最显著的一年是1997年，当时勒克伦（Lecun）等人于1990年提出了LeNet，乔丹（Jordan）等人于1997年提出了循环神经网络（RNN）。同年，舒斯特（Schuster）等人提出了改进的LSTM和原始RNN的改进版本，称为双向RNN。以下时间线简要概述了不同神经网络架构的历史：
- en: '![](img/7c03a0a3-f5ec-46e1-8a0a-7930fcfcb77c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c03a0a3-f5ec-46e1-8a0a-7930fcfcb77c.png)'
- en: Despite significant advances in computing, from 1997 to 2005, we didn't experience
    much advancement until Hinton struck again in 2006 when he and his team proposed
    the **Deep Belief Network** (**DBN**) by stacking multiple RBMs. Then, in 2012,
    Hinton invented dropout, which significantly improves regularization and overfitting
    in deep neural networks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算取得了显著进步，但从1997年到2005年，我们并没有经历太多的进步，直到辛顿在2006年再次取得突破，当时他和他的团队通过堆叠多个RBM提出了**深度信念网络**（**DBN**）。然后，在2012年，辛顿发明了dropout，这显著提高了深度神经网络的正则化和过拟合。
- en: After that, Ian Goodfellow *et al.* introduced GANs, which was a significant
    milestone in image recognition. In 2017, Hinton proposed CapsNet to overcome the
    limitation of regular CNNs, which was one of the most significant milestones so
    far.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，伊恩·古德费洛（Ian Goodfellow）等人引入了生成对抗网络（GANs），这在图像识别领域是一个重要的里程碑。2017年，辛顿（Hinton）提出了CapsNet以克服常规卷积神经网络（CNNs）的局限性，这至今为止是最重要的里程碑之一。
- en: How does an ANN learn?
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络是如何学习的？
- en: 'Based on the concept of biological neurons, the term and idea of ANNs arose.
    Similar to biological neurons, the artificial neuron consists of the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生物神经元的理念，人工神经网络（ANNs）的术语和概念应运而生。与生物神经元相似，人工神经元由以下部分组成：
- en: One or more incoming connections that aggregate signals from neurons
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个汇聚来自神经元信号的输入连接
- en: One or more output connections for carrying the signal to the other neurons
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个输出连接，用于将信号传递到其他神经元
- en: An activation function, which determines the numerical value of the output signal
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数，它决定了输出信号的数值
- en: 'Besides the state of a neuron, synaptic weight is considered, which influences
    the connection within the network. Each weight has a numerical value indicated
    by *W[ij]*, which is the synaptic weight connecting neuron *i* to neuron *j*.
    Now, for each neuron *i*, an input vector can be defined by *x[i] = (x[1], x[2],…x[n])* and
    a weight vector can be defined by *w[i] = (w[i1], w[i2],…w[in])*. Now, depending
    on the position of a neuron, the weights and the output function determine the
    behavior of an individual neuron. Then, during forward propagation, each unit
    in the hidden layer gets the following signal:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了神经元的当前状态外，还考虑了突触权重，这影响了网络内的连接。每个权重都有一个由 *W[ij]* 表示的数值，它是连接神经元 *i* 和神经元 *j*
    的突触权重。现在，对于每个神经元 *i*，可以定义一个输入向量 *x[i] = (x[1], x[2],…x[n])* 和一个权重向量 *w[i] = (w[i1],
    w[i2],…w[in])*。现在，根据神经元的定位，权重和输出函数决定了单个神经元的行为。然后，在正向传播过程中，隐藏层中的每个单元都会接收到以下信号：
- en: '![](img/92d1e4fa-5382-4d0e-badf-3dd0b12929e5.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92d1e4fa-5382-4d0e-badf-3dd0b12929e5.png)'
- en: 'Nevertheless, among the weights, there is also a special type of weight called
    a bias unit, *b*. Technically, bias units aren''t connected to any previous layer,
    so they don''t have true activity. But still, the bias *b* value allows the neural
    network to shift the activation function to the left or right. By taking the bias
    unit into consideration, the modified network output is formulated as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在权重中，还有一种特殊的权重类型，称为偏置单元，*b*。技术上讲，偏置单元不连接到任何前一层，因此它们没有真正的活动。但仍然，偏置 *b* 的值允许神经网络将激活函数向左或向右移动。考虑偏置单元后，修改后的网络输出如下所示：
- en: '![](img/1b946acf-c821-45ca-bada-d40d862b86ba.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b946acf-c821-45ca-bada-d40d862b86ba.png)'
- en: 'The preceding equation signifies that each hidden unit gets the sum of inputs,
    multiplied by the corresponding weight—this is known as the **Summing junction**.
    Then, the resultant output in the **Summing junction** is passed through the activation
    function, which squashes the output, as depicted in the following diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程表示每个隐藏单元都得到输入的总和，乘以相应的权重——这被称为 **求和节点**。然后，**求和节点**中的结果输出通过激活函数，如图所示进行压缩：
- en: '![](img/10255eed-9719-413b-abc4-ce71fb87c327.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10255eed-9719-413b-abc4-ce71fb87c327.png)'
- en: Working principal of an artificial neuron model
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经元模型的工作原理
- en: 'A practical neural network architecture, however, is composed of input, hidden,
    and output layers that are composed of *nodes* that make up a network structure,
    but still follow the working principal of an artificial neuron model, as shown
    in the preceding diagram. The input layer only accepts numeric data, such as features
    in real numbers, images with pixel values, and so on:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个实际的神经网络架构是由输入、隐藏和输出层组成的，这些层由 *nodes* 构成网络结构，但仍遵循前面图表中所示的人工神经元模型的工作原理。输入层只接受数值数据，例如实数特征、具有像素值的图像等：
- en: '![](img/ab4cea48-30f0-49c0-a88d-e9f40e6f7fd8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab4cea48-30f0-49c0-a88d-e9f40e6f7fd8.png)'
- en: A neural network with one input layer, three hidden layers, and an output layer
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有一个输入层、三个隐藏层和一个输出层的神经网络
- en: 'Here, the hidden layers perform most of the computation to learn the patterns
    and the network evaluates how accurate its prediction is compared to the actual
    output using a special mathematical function called the loss function. It could
    be a complex one or a very simple mean squared error, which can be defined as
    follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，隐藏层执行大部分计算以学习模式，网络通过使用称为损失函数的特殊数学函数来评估其预测与实际输出的准确性。它可能很复杂，也可能非常简单，可以定义为以下：
- en: '![](img/e4eeafcd-746d-48a5-8a91-7e3c3016ba17.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4eeafcd-746d-48a5-8a91-7e3c3016ba17.png)'
- en: In the preceding equation, ![](img/2fe01c92-f29a-441d-b7b0-094ae8aaedc5.png)
    signifies the prediction made by the network, while *Y* represents the actual
    or expected output. Finally, when the error is no longer being reduced, the neural
    network converges and makes prediction through the output layer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，![](img/2fe01c92-f29a-441d-b7b0-094ae8aaedc5.png) 表示网络做出的预测，而 *Y* 代表实际或预期的输出。最后，当错误不再减少时，神经网络收敛并通过输出层进行预测。
- en: Training a neural network
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: The learning process for a neural network is configured as an iterative process
    of the optimization of the weights. The weights are updated in each epoch. Once
    the training starts, the aim is to generate predictions by minimizing the loss
    function. The performance of the network is then evaluated on the test set. We
    already know about the simple concept of an artificial neuron. However, generating
    only some artificial signals is not enough to learn a complex task. As such, a
    commonly used supervised learning algorithm is the backpropagation algorithm,
    which is very commonly used to train a complex ANN.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的训练过程被配置为一个迭代优化权重的过程。权重在每个时代更新。一旦开始训练，目标是通过最小化损失函数来生成预测。然后，网络的性能在测试集上评估。我们已经了解了人工神经元的基本概念。然而，仅生成一些人工信号是不够学习复杂任务的。因此，常用的监督学习算法是反向传播算法，它被广泛用于训练复杂的ANN。
- en: Ultimately, training such a neural network is an optimization problem too, in
    which we try to minimize the error by adjusting network weights and biases iteratively,
    by using backpropagation through **gradient descent** (**GD**). This approach
    forces the network to backtrack through all its layers to update the weights and
    biases across nodes in the opposite direction of the loss function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，训练这样的神经网络也是一个优化问题，我们通过迭代调整网络权重和偏差，使用通过**梯度下降**（**GD**）的反向传播来最小化误差。这种方法迫使网络反向遍历所有层，以更新节点间的权重和偏差，方向与损失函数相反。
- en: However, this process using GD does not guarantee that the global minimum is
    reached. The presence of hidden units and the nonlinearity of the output function
    means that the behavior of the error is very complex and has many local minimas.
    This backpropagation step is typically performed thousands or millions of times,
    using many training batches, until the model parameters converge to values that
    minimize the cost function. The training process ends when the error on the validation
    set begins to increase, because this could mark the beginning of a phase overfitting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用梯度下降法（GD）的过程并不能保证达到全局最小值。隐藏单元的存在和输出函数的非线性意味着误差的行为非常复杂，并且有许多局部最小值。这个反向传播步骤通常要执行成千上万次，使用许多训练批次，直到模型参数收敛到最小化成本函数的值。当验证集上的误差开始增加时，训练过程结束，因为这可能标志着过拟合阶段的开始。
- en: The downside of using GD is that it takes too long to converge, which makes
    it impossible to meet the demand of handling large-scale training data. Therefore,
    a faster GD, called **Stochastic Gradient Descent** (**SDG**) was proposed, which
    is also a widely used optimizer in DNN training. In SGD, we use only one training
    sample per iteration from the training set to update the network parameters, which
    is a stochastic approximation of the true cost gradient.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GD的缺点是它收敛得太慢，这使得它无法满足处理大规模训练数据的需求。因此，提出了一个更快的GD，称为**随机梯度下降**（**SDG**），它也是DNN训练中广泛使用的优化器。在SGD中，我们使用训练集中的单个训练样本在每个迭代中更新网络参数，这是对真实成本梯度的随机近似。
- en: There are other advanced optimizers nowadays such as Adam, RMSProp, ADAGrad,
    Momentum, and so on. Each of them is either an direct or indirect optimized version
    of SGD.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在还有其他一些高级优化器，如Adam、RMSProp、ADAGrad、Momentum等。它们中的每一个都是SGD的直接或间接优化版本。
- en: Weight and bias initialization
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重和偏差初始化
- en: 'Now, here''s a tricky question: how do we initialize the weights? Well, if
    we initialize all the weights to the same value (for example, 0 or 1), each hidden
    neuron will get exactly the same signal. Let''s try to break it down:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有一个棘手的问题：我们如何初始化权重？好吧，如果我们把所有权重初始化为相同的值（例如，0或1），每个隐藏神经元将接收到完全相同的信号。让我们来分析一下：
- en: If all weights are initialized to 1, then each unit gets a signal equal to the
    sum of the inputs
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有权重都初始化为1，那么每个单元接收到的信号等于输入的总和
- en: If all weights are 0, which is even worse, then every neuron in a hidden layer
    will get zero signal
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有权重都是0，这甚至更糟糕，那么隐藏层中的每个神经元都将接收到零信号
- en: For network weight initialization, Xavier initialization is used widely. It
    is similar to random initialization but often turns out to work much better, since
    it can identify the rate of initialization depending on the total number of input
    and output neurons by default.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络权重初始化，广泛使用Xavier初始化。它与随机初始化类似，但通常效果更好，因为它可以默认根据输入和输出神经元的总数来确定初始化速率。
- en: You may be wondering whether you can get rid of random initialization while
    training a regular DNN. Well, recently, some researchers have been talking about
    random orthogonal matrix initialization's that perform better than just any random
    initialization for training DNNs. When it comes to initializing the biases, we
    can initialize them to be zero.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道在训练常规深度神经网络（DNN）时是否可以去掉随机初始化。好吧，最近，一些研究人员一直在讨论随机正交矩阵初始化，这种初始化对于训练DNN来说比任何随机初始化都好。当涉及到初始化偏差时，我们可以将它们初始化为零。
- en: But setting the biases to a small constant value, such as 0.01 for all biases, ensures
    that all **rectified linear units** (**ReLUs**) can propagate some gradient. However,
    it neither performs well nor shows consistent improvement. Therefore, sticking
    with zero is recommended.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但是将偏差设置为一个小常数值，例如所有偏差的0.01，确保所有**修正线性单元**（**ReLUs**）都能传播一些梯度。然而，它既没有表现出良好的性能，也没有显示出一致的改进。因此，建议坚持使用零。
- en: Activation functions
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'To allow a neural network to learn complex decision boundaries, we apply a
    non-linear activation function to some of its layers. Commonly used functions
    include Tanh, ReLU, softmax, and variants of these. More technically, each neuron
    receives a signal of the weighted sum of the synaptic weights and the activation
    values of the neurons that are connected as input. One of the most widely used
    functions for this purpose is the so-called sigmoid logistic function, which is
    defined as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使神经网络能够学习复杂的决策边界，我们对其某些层应用非线性激活函数。常用的函数包括Tanh、ReLU、softmax及其变体。从技术上讲，每个神经元接收一个信号，该信号是突触权重和连接为输入的神经元的激活值的加权和。为此目的最广泛使用的函数之一是所谓的sigmoid逻辑函数，其定义如下：
- en: '![](img/485cc8f4-cfc3-42a1-8d14-b6f5d143320d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/485cc8f4-cfc3-42a1-8d14-b6f5d143320d.png)'
- en: 'The domain of this function includes all real numbers, and the co-domain is
    (0, 1). This means that any value obtained as an output from a neuron (as per
    the calculation of its activation state) will always be between zero and one.
    The **Sigmoid** function, as represented in the following diagram, provides an
    interpretation of the saturation rate of a neuron, from not being active (equal
    to **0**) to complete saturation, which occurs at a predetermined maximum value
    (equal to **1**):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的定义域包括所有实数，而陪域是（0, 1）。这意味着从神经元（根据其激活状态的计算）获得的任何输出值都将始终介于零和一之间。以下图中表示的**Sigmoid**函数提供了对神经元饱和率的解释，从非激活状态（等于**0**）到完全饱和，这发生在预定的最大值（等于**1**）：
- en: '![](img/71b9595a-1e70-44ca-bb40-9c67cbb70d24.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/71b9595a-1e70-44ca-bb40-9c67cbb70d24.png)'
- en: Sigmoid versus Tanh activation function
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid与Tanh激活函数
- en: 'On the other hand, a hyperbolic tangent, or **Tanh**, is another form of activation
    function. **Tanh** flattens a real-valued number between **-1** and **1**. The
    preceding graph shows the difference between **Tanh** and **Sigmoid** activation
    functions. In particular, mathematically, *tanh* activation function can be expressed
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，双曲正切，或**Tanh**，是另一种激活函数形式。**Tanh**将一个介于**-1**和**1**之间的实数值拉平。前面的图表显示了**Tanh**和**Sigmoid**激活函数之间的差异。特别是，从数学上讲，*tanh*激活函数可以表示如下：
- en: '![](img/a68918c2-518e-4731-8602-c27cdd1d5beb.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a68918c2-518e-4731-8602-c27cdd1d5beb.png)'
- en: In general, in the last level of an **feedforward neural network** (**FFNN**),
    the softmax function is applied as the decision boundary. This is a common case,
    especially when solving a classification problem. The softmax function used for
    the probability distribution over the possible classes in a multiclass classification
    problem.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在**前馈神经网络**（**FFNN**）的最后一层，应用softmax函数作为决策边界。这是一个常见的情况，尤其是在解决分类问题时。在多类分类问题中，softmax函数用于对可能的类别进行概率分布。
- en: For a regression problem, we do not need to use any activation function since
    the network generates continuous values—that is, probabilities. However, I've
    seen people using the IDENTITY activation function for regression problems nowadays.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，我们不需要使用任何激活函数，因为网络生成的是连续值——即概率。然而，我注意到现在有些人使用IDENTITY激活函数来解决回归问题。
- en: To conclude, choosing proper activation functions and network weight initializations
    are two problems that make a network perform at its best and help to obtain good
    training. Now that we know the brief history of neural networks, let's deep-dive
    into different architectures in the next section, which will give us an idea on
    their usage.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，选择合适的激活函数和网络权重初始化是使网络发挥最佳性能并有助于获得良好训练的两个问题。既然我们已经了解了神经网络简短的历史，那么让我们在下一节深入探讨不同的架构，这将给我们一个关于它们用法的想法。
- en: Neural network architectures
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: 'We can categorize DL architectures into four groups:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将深度学习架构分为四组：
- en: '**Deep neural networks** (**DNNs**)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度神经网络**（**DNNs**）'
- en: '**Convolutional neural networks** (**CNNs**)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）'
- en: '**Recurrent neural networks** (**RNNs**)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）'
- en: '**Emergent architectures** (**EAs**)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**涌现架构**（**EAs**）'
- en: However, DNNs, CNNs, and RNNs have many improved variants. Although most of
    the variants are proposed or developed for solving domain-specific research problems,
    the basic working principles still follow the original DNN, CNN, and RNN architectures.
    The following subsections will give you a brief introduction to these architectures.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DNNs、CNNs和RNNs有许多改进的变体。尽管大多数变体都是为了解决特定领域的研究问题而提出或开发的，但它们的基本工作原理仍然遵循原始的DNN、CNN和RNN架构。以下小节将简要介绍这些架构。
- en: DNNs
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNNs
- en: DNNs are neural networks that have a complex and deeper architecture with a
    large number of neurons in each layer, and many connections between them. Although
    DNN refers to a very deep network, for simplicity, we consider MLP, **stacked
    autoencoder** (**SAE**), and **deep belief networks** (**DBNs**) as DNN architectures.
    These architectures mostly work as an FFNN, meaning information propagates from
    input to output layers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: DNNs是具有复杂和更深架构的神经网络，每一层都有大量神经元，并且它们之间有许多连接。尽管DNN指的是一个非常深的网络，但为了简单起见，我们将MLP、**堆叠自编码器**（**SAE**）和**深度信念网络**（**DBNs**）视为DNN架构。这些架构大多作为FFNN工作，意味着信息从输入层传播到输出层。
- en: 'Multiple perceptrons are stacked together as MLPs, where layers are connected
    as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since
    it has three layers: an input layer, a hidden layer, and an output layer. This
    way, the signal propagates one way, from the input layer to the hidden layers
    to the output layer, as shown in the following diagram:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多个感知器堆叠在一起形成MLP，其中层以有向图的形式连接。本质上，MLP是最简单的FFNN之一，因为它有三层：输入层、隐藏层和输出层。这样，信号以单向传播，从输入层到隐藏层再到输出层，如下面的图所示：
- en: '![](img/a34e4760-33b6-4a15-9cb4-cb5cd82e407d.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a34e4760-33b6-4a15-9cb4-cb5cd82e407d.png)'
- en: 'Autoencoders and RBMs are the basic building blocks for SAEs and DBNs, respectively.
    Unlike MLP, which is an FFNN that''s trained in a supervised way, both SAEs and
    DBNs are trained in two phases: unsupervised pre-training and supervised fine-tuning.
    In unsupervised pre-training, layers are stacked in order and trained in a layer-wise
    manner with used, unlabeled data. In supervised fine-tuning, an output classifier
    layer is stacked and the complete neural network is optimized by retraining with
    labeled data.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器和RBM是SAE和DBN的基本构建块。与以监督方式训练的FFNN MLP不同，SAE和DBN都是在两个阶段进行训练的：无监督预训练和监督微调。在无监督预训练中，层按顺序堆叠并以分层方式使用未标记的数据进行训练。在监督微调中，堆叠一个输出分类器层，并通过使用标记数据进行重新训练来优化整个神经网络。
- en: 'One problem with MLP is that it often overfits the data, so it doesn''t generalize
    well. To overcome this issue, DBN was proposed by Hinton *et al.* It uses a greedy,
    layer-by-layer, pre-training algorithm. DBNs are composed of a visible layer and
    multiple hidden unit layers. The building blocks of a DBN are RBMs, as shown in
    the following diagram, where several RBMs are stacked one after another:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的一个问题是它经常过拟合数据，因此泛化能力不好。为了克服这个问题，Hinton等人提出了DBN。它使用一种贪婪的、层级的、预训练算法。DBN由一个可见层和多个隐藏单元层组成。DBN的构建块是RBM，如下面的图所示，其中几个RBM一个接一个地堆叠：
- en: '![](img/d3e89b54-3a46-4dd1-b198-34325301e352.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3e89b54-3a46-4dd1-b198-34325301e352.png)'
- en: The top two layers have undirected, symmetric connections in between, but the
    lower layers have directed connections from the preceding layer. Despite numerous
    successes, DBNs are now being replaced with AEs.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最上面的两层之间有未定向、对称的连接，但底层有从前一层的有向连接。尽管DBNs取得了许多成功，但现在它们正被AEs所取代。
- en: Autoencoders
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'AEs are also special types of neural networks that learn automatically from
    the input data. AEs consists of two components: the encoder and the decoder. The
    encoder compresses the input into a latent-space representation. Then, the decoder
    part tries to reconstruct the original input data from this representation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AEs也是从输入数据自动学习的特殊类型的神经网络。AE由两个组件组成：编码器和解码器。编码器将输入压缩成潜在空间表示。然后，解码器部分试图从这个表示中重建原始输入数据：
- en: '**Encoder**: Encodes or compresses the input into a latent-space representation
    using a function known as *h=f(x)*'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：使用称为 *h=f(x)* 的函数将输入编码或压缩成潜在空间表示。'
- en: '**Decoder**: Decodes or reconstructs the input from the latent space representation
    using a function known as *r=g(h)*'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：使用称为 *r=g(h)* 的函数从潜在空间表示解码或重建输入。'
- en: 'So, an AE can be described by a function of *g(f(x)) = o*, where we want *0*
    as close as the original input of *x*. The following diagram shows how an AE typically
    works:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个AE可以通过一个函数来描述 *g(f(x)) = o*，其中我们希望 *0* 尽可能接近原始输入 *x*。以下图显示了AE通常的工作方式：
- en: '![](img/6a6851b9-4411-461d-b66e-7e92955c723c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6a6851b9-4411-461d-b66e-7e92955c723c.png)'
- en: AEs are very useful at data denoising and dimensionality reduction for data
    visualization. AEs can learn data projections, called representations, more effectively
    than PCA.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: AEs在数据去噪和降维以用于数据可视化方面非常有用。AEs比PCA更有效地学习数据投影，称为表示。
- en: CNNs
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNNs
- en: CNNs have achieved much and have a wide adoption in computer vision (for example,
    image recognition). In CNN networks, the connections schemes are significantly
    different compared to an MLP or DBN. A few of the convolutional layers are connected
    in a cascade style. Each layer is backed up by a ReLU layer, a pooling layer,
    and additional convolutional layers (+ReLU), and another pooling layer, which
    is followed by a fully connected layer and a softmax layer. The following diagram
    is a schematic of the architecture of a CNN that's used for facial recognition,
    which takes facial images as input and predicts emotions such as anger, disgust,
    fear, happy, sad and so on.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs取得了很大的成就，并在计算机视觉（例如，图像识别）中得到广泛应用。在CNN网络中，连接方案与MLP或DBN相比有显著不同。一些卷积层以级联方式连接。每一层都由一个ReLU层、一个池化层和额外的卷积层（+ReLU）以及另一个池化层支持，然后是一个全连接层和一个softmax层。以下图是用于面部识别的CNN架构示意图，它以面部图像为输入，预测情绪，如愤怒、厌恶、恐惧、快乐、悲伤等。
- en: '![](img/27a311c6-d8b2-467b-aacd-a482d6de6746.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/27a311c6-d8b2-467b-aacd-a482d6de6746.png)'
- en: A schematic architecture of a CNN used for facial recognition
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用于面部识别的CNN的示意图架构
- en: Importantly, DNNs have no prior knowledge of how the pixels are organized because
    they do not know that nearby pixels are close. CNNs embed this prior knowledge
    using lower layers by using feature maps in small areas of the image, while the
    higher layers combine lower-level features into larger features.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，DNNs对像素的排列没有先验知识，因为它们不知道附近的像素是接近的。CNNs通过在图像的小区域使用特征图来利用这种先验知识，而高层将低级特征组合成更高级的特征。
- en: This works well with most of the natural images, giving CNNs a decisive head
    start over DNNs. The output from each convolutional layer is a set of objects,
    called feature maps, that are generated by a single kernel filter. Then, the feature
    maps can be used to define a new input to the next layer. Each neuron in a CNN
    network produces an output, followed by an activation threshold, which is proportional
    to the input and not bound.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这与大多数自然图像都很好，使CNNs在DNNs中取得了决定性的领先优势。每个卷积层的输出是一组对象，称为特征图，由单个核滤波器生成。然后，特征图可以用来定义下一层的新输入。CNN网络中的每个神经元都产生一个输出，随后是一个激活阈值，该阈值与输入成正比，没有界限。
- en: RNNs
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNNs
- en: 'In RNNs, connections between units form a directed cycle. The RNN architecture
    was originally conceived by Hochreiter and Schmidhuber in 1997\. RNN architectures
    have standard MLPs, plus added loops so that they can exploit the powerful nonlinear
    mapping capabilities of the MLP. They also have some form of memory. The following
    diagram shows a very basic RNN that has an input layer, two recurrent layers,
    and an output layer:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNNs中，单元之间的连接形成一个有向循环。RNN架构最初由Hochreiter和Schmidhuber在1997年构思。RNN架构具有标准的MLP，并增加了循环，以便它们可以利用MLP强大的非线性映射能力。它们也具有某种形式的记忆。以下图显示了一个非常基本的RNN，它有一个输入层、两个循环层和一个输出层：
- en: '![](img/a79f8197-278c-46ab-9f04-735a6f4720a0.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a79f8197-278c-46ab-9f04-735a6f4720a0.png)'
- en: 'However, this basic RNN suffers from gradient vanishing and the exploding problem,
    and cannot model long-term dependencies. These architectures include LSTM, **gated
    recurrent units** (**GRUs**), bidirectional-LSTM, and other variants. Consequently,
    LSTM and GRU can overcome the drawbacks of regular RNNs: the gradient vanishing/exploding
    problem and long-short term dependency.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个基本的RNN受梯度消失和爆炸问题的影响，无法建模长期依赖。这些架构包括LSTM、**门控循环单元**（**GRUs**）、双向-LSTM和其他变体。因此，LSTM和GRU可以克服常规RNN的缺点：梯度消失/爆炸问题和长期短期依赖。
- en: Generative adversarial networks (GANs)
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: 'Ian Goodfellow *et al.* introduced GANs in a paper named *Generative Adversarial
    Nets* (see more at [https:/​/​arxiv.​org/abs/​1406.​2661v1](https://arxiv.org/pdf/1406.2661v1.pdf)).
    The following diagram briefly shows the working principles of a GAN:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Ian Goodfellow等人在一篇名为《生成对抗网络》（见更多内容[https:/​/​arxiv.​org/​abs/​1406.​2661v1](https://arxiv.org/pdf/1406.2661v1.pdf)）的论文中介绍了GANs。以下图表简要展示了GAN的工作原理：
- en: '![](img/704ace27-a78b-4d83-9a16-a160d9c4230c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/704ace27-a78b-4d83-9a16-a160d9c4230c.png)'
- en: Working principles of GAN
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的工作原理
- en: 'GANs are deep neural network architectures that consist of two networks, a
    generator, and a discriminator, that are pitted against each other (hence the
    name, *adversarial*):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是由两个网络组成的深度神经网络架构，一个生成器和一个判别器，它们相互对抗（因此得名，*对抗*）：
- en: The **Generator** tries to generate data samples out of a specific probability
    distribution and is very similar to the actual object
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**试图从一个特定的概率分布中生成数据样本，并且与实际对象非常相似'
- en: The **Discriminator** will judge whether its input is coming from the original
    training set or from the generator part
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器**将判断其输入是否来自原始训练集或生成器部分'
- en: Many DL practitioners think that GANs were one of the most important advancements
    because GANs can be used to mimic any distribution of data, and based on the data
    distribution, GANs can be taught to create robot artist images, super-resolution
    images, text-to-image synthesis, music, speech, and more.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习实践者认为，GANs是其中最重要的进步之一，因为GANs可以用来模拟任何数据分布，并且基于数据分布，GANs可以学会创建机器人艺术家图像、超分辨率图像、文本到图像合成、音乐、语音等。
- en: For example, because of the concept of adversarial training, Facebook's AI research
    director, Yann LeCun, called GANs the most interesting idea in the last 10 years
    of ML.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于对抗训练的概念，Facebook的人工智能研究总监Yann LeCun将GAN称为过去10年机器学习中最有趣的想法。
- en: Capsule networks
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络
- en: 'In CNNs, each layer understands an image at a much more granular level through
    a slow receptive field or max pooling operations. If the images have rotation,
    tilt, or very different shapes or orientation, CNNs fail to extract such spatial
    information and show very poor performance at image processing tasks. Even the
    pooling operations in CNNs cannot much help against such positional invariance.
    This issue in CNNs has led us to the recent advancement of CapsNet through the
    paper titled *Dynamic Routing Between Capsules* (see more at [https:/​/​arxiv.​org/​abs/​1710.​09829](https://arxiv.org/pdf/1710.09829.pdf))
    by Geoffrey Hinton *et al*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，每一层通过缓慢的接受场或最大池化操作以更细粒度的水平理解图像。如果图像有旋转、倾斜或非常不同的形状或方向，CNN无法提取此类空间信息，在图像处理任务中表现出非常差的性能。即使CNN中的池化操作也无法在很大程度上帮助对抗这种位置不变性。CNN中的这个问题促使我们通过Geoffrey
    Hinton等人撰写的题为《胶囊之间的动态路由》（见更多内容[https:/​/​arxiv.​org/​abs/​1710.​09829](https://arxiv.org/pdf/1710.09829.pdf)）的论文，最近在CapsNet方面取得了进展：
- en: '"A capsule is a group of neurons whose activity vector represents the instantiation
    parameters of a specific type of entity, such as an object or an object part."'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: “胶囊是一组神经元，其活动向量表示特定类型实体（如对象或对象部分）的实例化参数。”
- en: 'Unlike a regular DNN, where we keep on adding layers, in CapsNets, the idea
    is to add more layers inside a single layer. This way, a CapsNet is a nested set
    of neural layers. In CapsNet, the vector inputs and outputs of a capsule are computed
    using the routing algorithm, which iteratively transfers information and process
    **self-consistent field** (**SCF**) procedure, used in physics:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们不断添加层的常规DNN不同，在CapsNets中，想法是在单个层内添加更多层。这样，CapsNet是一个嵌套的神经网络层集。在CapsNet中，胶囊的向量输入和输出通过路由算法计算，该算法迭代地传输信息和处理**自洽场**（**SCF**）过程，这在物理学中应用：
- en: '![](img/ba38228e-2977-4355-8110-f87b8e1970b2.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba38228e-2977-4355-8110-f87b8e1970b2.png)'
- en: The preceding diagram shows a schematic diagram of a simple three-layer CapsNet.
    The length of the activity vector of each capsule in the **DigiCaps** layer indicates
    the presence of an instance of each class, which is used to calculate the loss.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示了简单三层CapsNet的示意图。**DigiCaps**层中每个胶囊的活动向量长度表示每个类实例的存在，这被用来计算损失。
- en: Now that we have learned about the working principles of neural networks and
    the different neural network architectures, implementing something hands-on would
    be great. However, before that, let's take a look at some popular DL libraries
    and frameworks, which come with the implementation of these network architectures.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了神经网络的工作原理和不同的神经网络架构，动手实现一些内容将会很棒。然而，在那之前，让我们看看一些流行的深度学习库和框架，它们提供了这些网络架构的实现。
- en: DL frameworks
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习框架
- en: There are several popular DL frameworks. Each of them comes with some pros and
    cons. Some of them are desktop-based and some of them are cloud-based platforms
    where you can deploy/run your DL applications. However, most of the libraries
    that are released under an open license help when people are using graphics processors,
    which can ultimately help in speeding up the learning process.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个流行的深度学习框架。每个框架都有其优缺点。其中一些是基于桌面的，而另一些是基于云的平台，您可以在这些平台上部署/运行您的深度学习应用。然而，大多数开源许可证下发布的库在人们使用图形处理器时都有帮助，这最终有助于加快学习过程。
- en: 'Such frameworks and libraries include TensorFlow, PyTorch, Keras, Deeplearning4j,
    H2O, and the **Microsoft Cognitive Toolkit** (**CNTK**). Even a few years back,
    other implementations including Theano, Caffee, and Neon were used widely. However,
    these are now obsolete. Since we will focus on learning in Scala, JVM-based DL
    libraries such as Deeplearning4j can be a reasonable choice. **Deeplearning4j**
    (**DL4J**) is one of the first commercial-grade, open source, distributed DL libraries
    that was built for Java and Scala. This also provides integrated support for Hadoop
    and Spark. DL4J is built for use in business environments on distributed GPUs
    and CPUs. DL4J aims to be cutting-edge and Plug and Play, with more convention
    than configuration, which allows for fast prototyping for non-researchers. The
    following diagram shows last year''s Google Trends, illustrating how popular TensorFlow
    is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架和库包括TensorFlow、PyTorch、Keras、Deeplearning4j、H2O以及**微软认知工具包**（**CNTK**）。甚至就在几年前，其他实现如Theano、Caffee和Neon也被广泛使用。然而，这些现在都已过时。由于我们将专注于Scala的学习，基于JVM的深度学习库如Deeplearning4j可以是一个合理的选择。**Deeplearning4j**（**DL4J**）是第一个为Java和Scala构建的商业级、开源、分布式深度学习库。这也提供了对Hadoop和Spark的集成支持。DL4J是为在分布式GPU和CPU上用于商业环境而构建的。DL4J旨在成为前沿和即插即用，具有比配置更多的惯例，这允许非研究人员快速原型设计。以下图表显示了去年的Google趋势，说明了TensorFlow有多受欢迎：
- en: '![](img/f27e1f6d-cc5a-4c23-bde9-1f0576077c29.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f27e1f6d-cc5a-4c23-bde9-1f0576077c29.png)'
- en: Trends of different DL frameworks—TensorFlow and Keras are dominating the most;
    however, Theano is losing its popularity; on the other hand, Deeplearning4j is
    emerging for JVM
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不同深度学习框架的趋势——TensorFlow和Keras占据主导地位；然而，Theano正在失去其受欢迎程度；另一方面，Deeplearning4j在JVM上崭露头角
- en: Its numerous libraries can be integrated with DL4J and will make your JVM experience
    easier, regardless of whether you are developing your ML application in Java or
    Scala. Similar to NumPy for JVM, ND4J comes up with basic operations of linear
    algebra (matrix creation, addition, and multiplication). However, ND4S is a scientific
    computing library for linear algebra and matrix manipulation. It also provides
    n-dimensional arrays for JVM-based languages.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它的众多库可以与DL4J集成，无论您是在Java还是Scala中开发机器学习应用，都将使您的JVM体验更加容易。类似于JVM的NumPy，ND4J提供了线性代数的基本操作（矩阵创建、加法和乘法）。然而，ND4S是一个用于线性代数和矩阵操作的科学研究库。它还为基于JVM的语言提供了多维数组。
- en: Apart from the preceding libraries, there are some recent initiatives for DL
    on the cloud. The idea is to bring DL capability to big data with millions of
    billions of data points and high dimensional data. For example, **Amazon Web Services**
    (**AWS**), Microsoft Azure, Google Cloud Platform, and **NVIDIA GPU Cloud** (**NGC**)
    all offer machine and DL services that are native to their public clouds.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述库之外，还有一些最近在云上进行的深度学习倡议。想法是将深度学习能力带给拥有数以亿计数据点和高维数据的大数据。例如，**亚马逊网络服务**（**AWS**）、微软Azure、谷歌云平台以及**NVIDIA
    GPU云**（**NGC**）都提供了其公共云本地的机器和深度学习服务。
- en: 'In October 2017, AWS released **Deep Learning AMIs** (**DLAMIs**) for **Amazon
    Elastic Compute Cloud** (**Amazon EC2**) P3 instances. These AMIs come pre-installed
    with DL frameworks, such as TensorFlow, Gluon, and Apache MXNet, which are optimized
    for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances. The DL service
    currently offers three types of AMIs: Conda AMI, Base AMI, and AMI with Source
    Code.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年10月，AWS为**Amazon Elastic Compute Cloud**（**Amazon EC2**）P3实例发布了**深度学习AMI**（**DLAMIs**）。这些AMI预先安装了深度学习框架，如TensorFlow、Gluon和Apache
    MXNet，这些框架针对Amazon EC2 P3实例中的NVIDIA Volta V100 GPU进行了优化。该深度学习服务目前提供三种类型的AMI：Conda
    AMI、Base AMI和带源代码的AMI。
- en: The CNTK is Azure's open source, DL service. Similar to AWS' offering, it focuses
    on tools that can help developers build and deploy DL applications. The toolkit
    is installed in Python 2.7, in the root environment. Azure also provides a model
    gallery that includes resources, such as code samples, to help enterprises get
    started with the service.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK是Azure的开源深度学习服务。类似于AWS的提供，它专注于可以帮助开发者构建和部署深度学习应用程序的工具。工具包安装在Python 2.7的根环境中。Azure还提供了一个模型库，其中包括代码示例等资源，以帮助企业开始使用该服务。
- en: On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated
    containers (see [https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)).
    The NGC features containerized deep learning frameworks such as TensorFlow, PyTorch,
    MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the
    latest NVIDIA GPUs on participating cloud-service providers. Nevertheless, there
    are also third-party services available through their respective marketplaces.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，NGC通过GPU加速容器（见[https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/](https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/)）为AI科学家和研究人员提供支持。NGC具有容器化的深度学习框架，如TensorFlow、PyTorch、MXNet等，这些框架由NVIDIA经过调整、测试和认证，可在参与云服务提供商的最新NVIDIA
    GPU上运行。尽管如此，通过它们各自的市场，也有第三方服务可用。
- en: Now that you know the working principles of neural network architectures and
    have seen a brief overview on available DL frameworks for implementing DL solutions,
    let's move on to the next section for some hands-on learning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了神经网络架构的工作原理，并且对可用于实现深度学习解决方案的DL框架有了简要的了解，让我们继续到下一部分进行一些动手学习。
- en: Getting started with learning
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习
- en: Large-scale cancer genomics data often comes in multi-platform and heterogeneous
    forms. These datasets impose great challenges in terms of the bioinformatics approach
    and computational algorithms. Numerous researchers have proposed to utilize this
    data to overcome several challenges, using classical ML algorithms as either the
    primary subject or a supporting element for cancer diagnosis and prognosis.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模癌症基因组数据通常以多平台和异构形式出现。这些数据集在生物信息学方法和计算算法方面提出了巨大的挑战。许多研究人员提出了利用这些数据来克服几个挑战的方法，使用经典机器学习算法作为主要主题或癌症诊断和预后支持的元素。
- en: Description of the dataset
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: Genomics data covers all data related to DNA on living things. Although in this
    thesis we will also use other types of data, as such as transcriptomic data (RNA
    and miRNA), for convenience purposes, all data will be termed as genomics data.
    Research on human genetics made a huge breakthrough in recent years due to the
    success of the **Human Genome Project** (**HGP**) (1984-2000) on sequencing the
    full sequence of human DNA. Now, let's see what a real-life dataset looks like
    that can be used for our purposes. We will be using the *gene expression cancer
    RNA-Seq* dataset, which can be downloaded from the UCI ML repository (see [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq) for
    more information).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 基因组数据涵盖了与生物体DNA相关的所有数据。尽管在本论文中我们也会使用其他类型的数据，例如转录组数据（RNA和miRNA），为了方便起见，所有数据都将被称为基因组数据。由于**人类基因组计划**（**HGP**）（1984-2000）在测序人类DNA全序列方面的成功，近年来人类遗传学研究取得了巨大的突破。现在，让我们看看一个可以用于我们目的的真实数据集是什么样的。我们将使用*基因表达癌症RNA-Seq*数据集，该数据集可以从UCI
    ML存储库下载（更多信息请见[https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)）。
- en: 'This dataset is a random subset of another dataset that was reported in the
    following paper: Weinstein, John N., *et al*. *The cancer genome atlas pan-cancer
    analysis project.* Nature Genetics 45.10 (2013): 1113-1120\. The name of the project
    is The Pan-Cancer analysis project. It assembled data from thousands of patients
    with primary tumors occurring in different sites of the body. It covered 12 tumor
    types, including the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '**Glioblastoma multiforme** (**GBM**)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lymphoblastic acute myeloid leukemia** (**AML**)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Head and neck squamous carcinoma** (**HNSC**)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lung adenocarcinoma** (**LUAD**)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L****ung squamous carcinoma** (**LUSC**)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Breast carcinoma** (**BRCA**)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kidney renal clear cell carcinoma** (**KIRC**)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ovarian carcinoma** (**OV**)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bladder carcinoma** (**BLCA**)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Colon adenocarcinoma** (**COAD**)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uterine cervical and endometrial carcinoma** (**UCEC**)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rectal adenocarcinoma** (**READ**)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This collection of data is a part of the RNA-Seq (HiSeq) PANCAN dataset. It
    is a random extraction of gene expressions of patients that have different types
    of tumors: BRCA, KIRC, COAD, LUAD, and PRAD.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is a random collection of cancer patients from 801 patients, each
    having 20,531 attributes. Samples (`instances`) are stored row-wise. Variables
    (`attributes`) of each sample are RNA-Seq gene expression levels measured by the
    Illumina HiSeq platform. A dummy name (`gene_XX`) is provided for each attribute.
    The attributes are ordered consistently with the original submission. For example,
    `gene_1` on `sample_0` is significantly and differentially expressed with a value
    of `2.01720929003`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'When you download the dataset, you will see that there are two CSV files:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '`data.csv`: Contains the gene expression data of each sample'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels.csv`: The labels associated with each sample'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a look at the processed dataset. Note that we will only look at
    a few select features considering the high dimensionality in the following screenshot,
    where the first column represents sample IDs (that is, anonymous patient IDs).
    The rest of the columns represent how a certain gene expression occurs in the
    tumor samples of the patients:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a15ea25-0ace-4c47-9600-0e95ac5624dd.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: 'Now, look at the labels in the following table. Here, the `id` column contains
    the sample IDs and the `Class` column represents the cancer labels:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0001e482-8aeb-42d1-9307-0c8eba945c9d.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Now, you can imagine why I have chosen this dataset. Although we will not have
    many samples, the dataset is still highly dimensional. In addition, this type
    of high dimensional dataset is very suitable for applying a DL algorithm. Therefore,
    if the features and labels are given, can we classify these samples based on features
    and the ground truth? Why not? We will try to solve this problem with the DL4J
    library. First, we have to configure our programming environment so that we can
    write our code.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the programming environment
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how to configure DL4J, ND4s, Spark, and ND4J
    before getting started with the coding. The following are the prerequisites that
    you must take into account when working with DL4J:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在开始编码之前如何配置DL4J、ND4s、Spark和ND4J。以下是在使用DL4J时你必须考虑的先决条件：
- en: Java 1.8+ (64-bit only)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 1.8+ (仅64位)
- en: Apache Maven for an automated build and dependency manager
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Maven用于自动构建和依赖关系管理器
- en: IntelliJ IDEA or Eclipse IDE
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IntelliJ IDEA或Eclipse IDE
- en: Git for version control and CI/CD
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Git用于版本控制和CI/CD
- en: 'The following libraries can be integrated with DJ4J to enhance your JVM experience
    while developing your ML applications:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下库可以与DJ4J集成，以增强你在开发机器学习应用程序时的JVM体验：
- en: '**DL4J**: The core neural network framework, which comes with many DL architectures
    and underlying functionalities.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DL4J**：核心神经网络框架，包含许多深度学习架构和底层功能。'
- en: '**ND4J**: Can be considered as the NumPy of JVM. It comes with some basic operations
    of linear algebra. Examples are matrix creation, addition, and multiplication.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4J**：可以被认为是JVM的NumPy。它包含一些线性代数的基本操作。例如矩阵创建、加法和乘法。'
- en: '**DataVec**: This library enables ETL operations while performing feature engineering.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataVec**：这个库在执行特征工程的同时允许ETL操作。'
- en: '**JavaCPP**: This library acts as the bridge between Java and Native C++.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaCPP**：这个库充当Java和原生C++之间的桥梁。'
- en: '**Arbiter**: This library provides basic evaluation functionalities for the
    DL algorithms.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arbiter**：这个库为深度学习算法提供基本的评估功能。'
- en: '**RL4J**: Deep reinforcement learning for the JVM.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL4J**：JVM的深度强化学习。'
- en: '**ND4S**: This is a scientific computing library, and it also supports n-dimensional
    arrays for JVM-based languages.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4S**：这是一个科学计算库，它也支持基于JVM的语言的n维数组。'
- en: 'If you are using Maven on your preferred IDE, let''s define the project properties
    to mention these versions in the `pom.xml` file:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在你喜欢的IDE上使用Maven，让我们定义项目属性，在`pom.xml`文件中提及这些版本：
- en: '[PRE0]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, use all the dependencies required for DL4J, ND4S, and ND4J, as shown
    in the `pom.xml` file. By the way, DL4J comes with Spark 2.1.0\. Additionally,
    if a native system BLAS is not configured on your machine, ND4J''s performance
    will be reduced. You will experience the following warning once you execute any
    simple code written in Scala:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`pom.xml`文件中显示的DL4J、ND4S和ND4J所需的全部依赖项。顺便说一句，DL4J自带Spark 2.1.0。另外，如果你的机器上没有配置本地系统BLAS，ND4J的性能将会降低。一旦你执行任何用Scala编写的简单代码，你将体验到以下警告：
- en: '[PRE1]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However, installing and configuring a BLAS, such as OpenBLAS or IntelMKL, is
    not that difficult; you can invest some time and do it. Refer to the following
    URL for further details:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，安装和配置BLAS，如OpenBLAS或IntelMKL，并不那么困难；你可以投入一些时间并完成它。有关更多详细信息，请参阅以下URL：
- en: '[http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open)'
- en: Well done! Our programming environment is ready for simple DL application development.
    Now, it's time to get our hands dirty with some sample code.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们的编程环境已准备好进行简单的深度学习应用开发。现在，是时候用一些示例代码来动手实践了。
- en: Preprocessing
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: Since we do not have any unlabeled data, I would like to select some samples
    randomly for testing. One more thing to note is that features and labels come
    in two separate files. Therefore, we can perform the necessary preprocessing and
    then join them together so that our preprocessed data will have features and labels
    together.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有未标记的数据，我想随机选择一些样本进行测试。还有一点需要注意，特征和标签在两个单独的文件中。因此，我们可以执行必要的预处理，然后将它们合并在一起，这样我们的预处理数据将包含特征和标签。
- en: 'Then, the rest of the data will be used for training. Finally, we''ll save
    the training and testing sets in a separate CSV file to be used later on. Follow
    these steps to get started:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将使用剩余的数据进行训练。最后，我们将训练和测试集保存到单独的CSV文件中，以便以后使用。按照以下步骤开始：
- en: 'First, let''s load the samples and see the statistics. Here, we use the `read()`
    method of Spark, but specify the necessary options and format too:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载样本并查看统计信息。在这里，我们使用Spark的`read()`方法，但也要指定必要的选项和格式：
- en: '[PRE2]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we will see some related statistics, such as the number of features and
    the number of samples:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将看到一些相关的统计信息，例如特征数量和样本数量：
- en: '[PRE3]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Therefore, there are `801` samples from `801` distinct patients and the dataset
    is too high in dimensions, since it has `20532` features:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有来自`801`个不同患者的`801`个样本，由于它有`20532`个特征，数据集的维度非常高：
- en: '[PRE4]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In addition, since the `id` column represents only the patient''s anonymous
    ID, so we can simply drop it:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we load the labels using the `read()` method of Spark and also specify
    the necessary options and format:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have already seen what the label DataFrame looks like. We will skip the
    `id`. However, the `Class` column is categorical. As we mentioned previously,
    DL4J does not support categorical labels that need to be predicted. Therefore,
    we have to convert it into a numeric format (an integer, to be more specific);
    for that, I would use `StringIndexer()` from Spark:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `StringIndexer()`, we apply the index operation to the `Class`
    column, and rename it `label`. Additionally, we `skip` null entries:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we perform the indexing operation by calling the `fit()` and `transform()`
    operations, as follows:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, let''s take a look at the indexed DataFrame:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding line of code should convert the `label` column in numeric format:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/583202d7-f812-471d-a3af-03284a68f1b0.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Fantastic! Now, all the columns (including features and labels) are numeric. Thus,
    we can join both features and labels into a single DataFrame. For that, we can
    use the `join()` method from Spark, as follows:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can generate both the training and test sets by randomly splitting `combinedDF`,
    as follows:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let''s see the `count` of samples in each set:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There should be 561 samples in the training set and 240 samples in the test
    set. Finally, we save them in separate CSV files to be used later on:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have the training and test sets, we can train the network with the
    training set and evaluate the model with the test set.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spark will generate CSV files under the `output` folder, under the project root.
    However, you might see a very different name. I suggest that you to rename them
    `TCGA_train.csv` and `TCGA_test.csv` for the training and test sets, respectively.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Considering the high dimensionality, I would rather try a better network such
    as LSTM, which is an improved variant of RNN. At this point, some contextual information
    about LSTM would be helpful to grasp this idea, and will be provided after the
    following section.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Dataset preparation
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we prepared the training and test sets. However, we
    need to put some extra effort into making them consumable by DL4J. To be more
    specific, DL4J expects the training data in numeric format and the last column
    to be the `label` column. The remaining data should be features.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now try to prepare our training and test sets like that. First, we
    will find the files where we saved the training and test sets:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we will define the required parameters, such as the number of features,
    number of classes, and batch size. Here, I am using `128` as the `batchSize`,
    but you can adjust it accordingly:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This dataset is used for training:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is the data we want to classify:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see from the preceding two lines of code, `readCSVDataset()` is basically
    a wrapper that reads the data in CSV format, and then the `RecordReaderDataSetIterator()`
    method converts the record reader into a dataset iterator.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: LSTM network construction
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating a neural network with DL4J starts with `MultiLayerConfiguration`,
    which organizes network layers and their hyperparameters. Then, the created layers
    are added using the `NeuralNetConfiguration.Builder()` interface. As shown in
    the following diagram, the LSTM network consists of five layers: an input layer,
    which is followed by three LSTM layers. The last layer is an RNN layer, which
    is also the output layer in this case:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6eb2b39a-02bf-43e6-bf32-2b55178d0b74.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: An LSTM network for cancer type prediction, which takes 20,531 features and
    fixed bias (that is, 1) and generates multi-class outputs
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'To create LSTM layers, DL4J provides the implementation of an LSTM class. However,
    before we start creating layers for the network, let''s define some hyperparameters,
    such as the number of input/hidden/output nodes (neurons):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then create the network by specifying layers. The first, second, and third
    layers are LSTM layers. The last layer is an RNN layer. For all the hidden LSTM
    layers, we specify the number of input and output units, and we use ReLU as the
    activation function. However, since it''s a multiclass classification problem,
    we use `SOFTMAX` as the `activation` function for the output layer, with `MCXNET`
    as the loss function:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code block, the softmax `activation` function gives a probability
    distribution over classes, and `MCXENT` is the cross-entropy loss function in
    a multiclass classification setting.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, with DL4J, we add the layers we created earlier using the `NeuralNetConfiguration.Builder()`
    interface. First, we add all the LSTM layers, which are followed by the final
    RNN output layer:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the preceding code block, we used SGD as the optimizer, which tries to optimize
    the `MCXNET` loss function. Then, we initialize the network weight using `XAVIER`,
    and `Adam` acts as the network updater with SGD. Finally, we initialize a multilayer
    network using the preceding multilayer configuration:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Additionally, we can inspect the number of hyperparameters across layers and
    in the whole network. Typically, this type of network has a lot of hyperparameters.
    Let''s print the number of parameters in the network (and for each layer):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As I stated previously, our network has 910 million parameters, which is huge.
    This also poses a great challenge while tuning hyperparameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Network training
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will create a `MultiLayerNetwork` using the preceding `MultiLayerConfiguration`.
    Then, we will initialize the network and start the training on the training set:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Finally, we also specify that we do not need to do any pre-training (which is
    typically needed in DBN or stacked autoencoders).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the training has been completed, the next task is to evaluate the model,
    which we'll do on the test set here. For the evaluation, we will be using the `Evaluation()` method.
    This method creates an evaluation object with five possible classes.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s iterate the evaluation over every test sample and get the network''s
    prediction from the trained model. Finally, the `eval()` method checks the prediction
    against the true class:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Wow! Unbelievable! Our LSTM network has accurately classified the samples.
    Finally, let''s see how the classifier predicts across each class:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The predictive accuracy for cancer type prediction using LSTM is suspiciously
    higher, isn't it? Did our model underfit? Did our model overfit?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Observing the training using Deeplearning4j UI
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our accuracy is suspiciously higher, we can observe how the training went.
    Yes, there are ways to find out if it went through overfitting, since we can observe
    the training, validation, and test losses on the DL4J UI. However, I won't discuss
    the details here. Take a look at [https://deeplearning4j.org/docs/latest/deeplearning4j-nn-visualization](https://deeplearning4j.org/docs/latest/deeplearning4j-nn-visualization) for
    more information on how to do this.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to classify cancer patients on the basis of tumor
    types from a very high-dimensional gene expression dataset curated from TCGA.
    Our LSTM architecture managed to achieve 99% accuracy, which is outstanding. Nevertheless, we
    discussed many aspects of DL4J, which will be helpful in upcoming chapters. Finally,
    we saw answers to some frequent questions related to this project, LSTM networks,
    and DL4J hyperparameters/network tuning.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: This is, more or less, the end of our little journey in developing ML projects
    using Scala and different open source frameworks. Throughout these chapters, I
    have tried to provide you with several examples of how to use these wonderful
    technologies efficiently for developing ML projects. While writing this book,
    I had to keep many constraints in my mind; for example, the page count, API availability,
    and of course, my expertise.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: However, overall, I tried to make the book simple by avoiding unnecessary details
    on the theory, as you can read about that in many books, blogs, and websites.
    I will also keep the code of this book updated on the GitHub repository at [https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide).
    Feel free to open a new issue or any pull request to improve the code and stay
    tuned.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I'll upload the solution to each chapter as Zeppelin notebooks
    so that you can run the code interactively. By the way, Zeppelin is a web-based
    notebook that enables data-driven, interactive data analytics, and collaborative
    documents with SQL and Scala. Once you have configured Zeppelin on your preferred
    platform, you can download the notebook from the GitHub repository, import them
    into Zeppelin, and get going. For more detail, you can take a look at [https://zeppelin.apache.org/](https://zeppelin.apache.org/).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
