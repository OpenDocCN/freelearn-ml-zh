- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Active Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models require large, labeled datasets, which can be expensive
    and time-consuming to obtain. **Active machine learning** (**active ML**) minimizes
    the labeling effort needed by intelligently choosing which data points a human
    should label. In this book, you will gain the necessary knowledge to understand
    active learning, including its mechanisms and applications. With these fundamentals,
    the subsequent chapters will equip you with concrete skills to implement active
    learning techniques on your own.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this book, you will have practical experience with state-of-the-art
    strategies to minimize labeling costs and maximize model performance. You will
    be able to apply active learning to enhance the efficiency and adaptability of
    your models across different application areas, such as vision and language.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, this chapter provides an introduction to active ML and explains
    how it can improve model accuracy using fewer labeled examples. By the end of
    the chapter, you will have covered the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding active machine learning systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring query strategy scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing active and passive learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding active machine learning systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Active machine learning** (**active ML**) is a powerful approach that seeks
    to create predictive models with remarkable accuracy, all while minimizing the
    number of labeled training examples required. This is achieved by employing a
    clever strategy that involves selectively choosing the most informative data points
    to be labeled by a knowledgeable oracle, such as a human annotator. By doing so,
    active learning enables models to extract the necessary knowledge they need from
    a relatively small amount of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore some definitions and the fundamental concepts that form the
    foundation of active ML.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Active learning** can be defined as a dynamic and iterative approach to machine
    learning, where the algorithm intelligently engages with an **oracle** to label
    new data points. An oracle is a source that provides labels for data points queried
    by the active learner. The oracle acts as a teacher, guiding the model by providing
    labels for its most informative queries. Typically, oracles are human annotators
    or experts who can manually assign labels to new data points. However, oracles
    can also be simulation engines, crowdsourcing services, or other systems capable
    of labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: The key objective of active ML is to select and prioritize the most informative
    data points for the model. The aim is to achieve higher accuracy levels while
    minimizing the need for extensive training labels, in comparison to traditional
    supervised learning methods, which rely on large datasets of pre-labeled examples
    to train models in predicting outcomes. On the other hand, unsupervised learning
    methods work with unlabeled data, seeking patterns or structures without explicit
    instruction on the outcomes. Active learning bridges these approaches by focusing
    on a semi-supervised learning strategy. This process allows the model to actively
    learn and adapt over time, continuously improving its predictive capabilities
    by leveraging the most relevant and significant data points. By actively engaging
    with the data and carefully choosing which samples to label, active ML optimizes
    the entire learning process. It allows the algorithm to focus on the most relevant
    and informative instances, thereby reducing the need for extensive labeling efforts.
    As a result, active ML not only saves time and resources but also enables machine
    learning models to achieve higher accuracy and better generalization. Active ML
    opens the door for more advanced and intelligent machine learning systems by effectively
    prioritizing data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Potential range of applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Active learning is a highly versatile technique that can significantly enhance
    efficiency and model performance across a wide range of applications. It does
    so by directing human labeling efforts to areas where they can have the most impact.
  prefs: []
  type: TYPE_NORMAL
- en: This approach has proven to be particularly effective in **computer vision applications**,
    such as image classification, object detection, and image segmentation. By selectively
    acquiring labels for ambiguous images that traditional sampling methods often
    miss, active learning can reduce costs and improve accuracy. It does this by identifying
    the most informative edge cases to query, allowing for accurate results with fewer
    labeled samples. For example, if we consider a self-driving car object-detection
    model that needs to identify various objects such as people, trees, and other
    cars, we can utilize active learning to prioritize the classes that it may struggle
    to learn.
  prefs: []
  type: TYPE_NORMAL
- en: In **natural language tasks**, such as document classification and translation,
    active learners play a crucial role in filling gaps in linguistic coverage. By
    querying sentences that cover rare vocabulary and structures, active learning
    improves adaptation and improves overall performance. The labeling process is
    focused only on the most useful examples, minimizing the need for extensive labeling
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Anomaly detection** is another domain where active learning proves to be
    highly effective. By targeting rare outliers and anomalies, which are critical
    for identifying issues such as fraud, active learning improves the detection of
    these important but uncommon examples. By focusing human reviews on unusual cases,
    active learning enhances the overall accuracy of anomaly detection systems.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommendation systems** heavily rely on user feedback, and active learning
    provides a framework for acquiring this feedback intelligently. By querying users
    on their preferences for certain content, active learning gathers focused signals
    that can be used to fine-tune recommendations. For example, streaming services
    can use active learning techniques to improve the accuracy and relevance of their
    video suggestions.'
  prefs: []
  type: TYPE_NORMAL
- en: In the field of **medical diagnosis**, active learning techniques play a vital
    role in minimizing physician time spent on common diagnoses. By identifying challenging
    cases that require expert input, active learning ensures that effort is focused
    on ambiguous examples that can significantly improve diagnostic model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Active learning provides both the algorithms and mechanisms necessary to efficiently
    focus human effort on useful areas across various applications. By selectively
    acquiring labels, it overcomes the inherent costs and challenges associated with
    supervised machine learning, making it an invaluable tool in the field of artificial
    intelligence. Across science, engineering, and technology, the ability to intelligently
    guide data collection and labeling can accelerate progress with minimal human
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move ahead to discuss the key components of an active learning system
    and how they apply to all the applications we have just mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: Key components of active machine learning systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Active ML systems comprise four key elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unlabeled dataset**: This pool of unlabeled data points is what the active
    learner can query from. It may contain tens, hundreds, or even millions of examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query strategy**: This is the core mechanism of active learning. It guides
    how the system selects which data points to query labels for. Different criteria
    can be used, which we will explore later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning model**: The underlying predictive model being trained,
    such as a neural network, random forest, or SVM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Oracle**: The source that provides labels. This is typically a human annotator
    who can manually label queried data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'How do the key components just mentioned interact with each other? *Figure
    1**.1* depicts the interaction between various components of an active ML loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Active ML loop](img/B21789_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Active ML loop
  prefs: []
  type: TYPE_NORMAL
- en: 'Models engage in an iterative loop, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The query strategy identifies the most useful data points to label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are labeled by the oracle (human annotator).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The newly labeled data is used to train the machine learning model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The updated model is then used to inform the next round of querying and labeling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This loop allows active learning models to intelligently explore datasets, acquiring
    new training labels that maximize information gain.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dig deeper into the query strategy step by first
    examining the various scenarios that one can choose from.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring query strategies scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Active learning can be implemented in different ways, depending on the nature
    of the unlabeled data and how the queries are performed. There are three main
    scenarios to consider when implementing active learning:'
  prefs: []
  type: TYPE_NORMAL
- en: Membership query synthesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stream-based selective sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pool-based sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These scenarios offer different ways to optimize and improve the active learning
    process. Understanding these scenarios can help you make informed decisions and
    choose the most suitable approach for your specific needs. In this section, we
    will explore each of these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Membership query synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In **membership query synthesis**, the active learner has the ability to create
    its own unlabeled data points in order to improve its training. This is done by
    generating new data points from scratch and then requesting the oracle for labels,
    as depicted in *Figure 1**.2*. By incorporating these newly labeled data points
    into its training set, the model becomes more robust and accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Membership query synthesis workflow](img/B21789_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Membership query synthesis workflow
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an image classifier as an example. With the power of synthesis,
    the active learner can create new images by combining various shapes, textures,
    and colors in different compositions. This allows the model to explore a wide
    range of possibilities and learn to recognize patterns and features that may not
    have been present in the original labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a text classifier can also benefit from membership query synthesis.
    By generating new sentences and paragraphs with specific words or structures,
    the model can expand its understanding of different language patterns and improve
    its ability to classify text accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several advantages of membership query synthesis:'
  prefs: []
  type: TYPE_NORMAL
- en: The model has complete control over the data points it queries, allowing it
    to focus on corner cases and unusual examples that normal sampling might overlook.
    This helps to reduce overfitting and improve the model’s generalization by increasing
    the diversity of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By synthesizing data, the model can actively explore its weaknesses rather than
    rely on what is in the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is useful for problems where data synthesis is straightforward, such as
    simple tabular data and sequences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, there are also several disadvantages to using this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: It requires the ability to synthesize new useful data points accurately. This
    can be extremely difficult for complex real-world data such as images, audio,
    and video.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data synthesis does not work well for high-dimensional, nuanced data. The generated
    points are often not natural.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is less practical for real-world applications today compared to pool-based
    sampling. Advances in generative modeling can improve synthesis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is computationally expensive to repeatedly generate full data points from
    scratch, especially for multimedia data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over-generating synthetic examples can lead to overfitting, wherein the model
    becomes overly fixated on classifying the synthetic instances rather than the
    actual data. As a result, the model’s accuracy may suffer when confronted with
    unfamiliar and unseen data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, membership query synthesis is mostly studied in theory and rarely applied
    in practice today. However, advances in generative modeling may increase its viability
    for real applications in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-based selective sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In **stream-based selective sampling**, the process of receiving unlabeled
    data points occurs continuously and dynamically rather than in a static and predetermined
    batch. *Figure 1**.3* shows how the active learner is presented with a constant
    flow of data points, one after another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Stream-based selective sampling workflow](img/B21789_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Stream-based selective sampling workflow
  prefs: []
  type: TYPE_NORMAL
- en: The active learner is faced with the task of making instantaneous decisions
    about whether or not to request a label for each individual point. This real-time
    decision-making process adds an element of flexibility and adaptability to the
    learning algorithm. This allows it to adjust its sampling strategy on the fly
    based on the evolving characteristics of the incoming data stream. By actively
    selecting which data points to query for labels, the active learner can optimize
    the learning process and make the most efficient use of limited labeling resources.
  prefs: []
  type: TYPE_NORMAL
- en: Stream-based selective sampling finds its applications in data, including user
    activity, live sensor data, and the data in news feeds, social media, and many
    more sources.
  prefs: []
  type: TYPE_NORMAL
- en: There is a massive amount of data being generated by user activity in the form
    of clicks, searches, and posts. By selectively labeling a fraction of user actions
    on websites and apps to train models (e.g., predicting churn and engagement),
    stream-based selective sampling avoids storing massive logs of all user actions.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, live sensor data from devices or machinery requires continual
    monitoring. To minimize this oversight, the querying of labels is performed on
    only the most critical sensor events from autonomous systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of news feeds, social media streams, and content recommendation
    systems, stream-based selective sampling helps in acquiring user feedback for
    recommending a small fraction of content items. This focused user input improves
    suggestions without overwhelming the users.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, data arrives constantly in real time. The active learning model
    evaluates each new data point and selectively queries the oracle for labels on
    the most useful examples. The less useful points are discarded rather than stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages of stream-based selective sampling are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is well-suited for real-time applications with constant live data streams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is storage efficient as the full data stream isn’t saved, only the queried
    points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is scalable because it involves efficiently managing high volumes of incoming
    data without the need for storing all of it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, it comes with a few drawbacks, too, listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The model must evaluate and make query decisions on the fly as the stream arrives.
    There is no opportunity for deep analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discarded points cannot be revisited or re-queried later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes in data distribution over time are harder to adapt to without retraining
    from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model only labels specific data types, it can introduce bias. This can
    result in a model that is optimized for those particular data types but may not
    perform well when faced with new data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effectiveness of the approach may vary depending on the streaming platform
    and its limitations, which can restrict its usefulness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, stream-based selective sampling is an efficient approach when low storage
    and real-time response are critical. It works well when the stream distribution
    is relatively stable. If the stream changes over time, pool-based sampling may
    be more effective since earlier points can be re-analyzed.
  prefs: []
  type: TYPE_NORMAL
- en: Pool-based sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of **pool-based sampling**, the active learner is given access
    to a large collection of unlabeled data points that remain static over time. The
    data points in that scenario are usually acquired from an existing unlabeled dataset
    or a labeled dataset where the labels are temporarily hidden. They can also be
    collected by scraping public sources.
  prefs: []
  type: TYPE_NORMAL
- en: Active learning selects data points from a static data pool and sends them to
    the oracle for labeling. Unlike the stream-based sampling scenario, none of the
    data points are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1**.4* depicts a pool-based sampling workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Pool-based sampling workflow](img/B21789_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Pool-based sampling workflow
  prefs: []
  type: TYPE_NORMAL
- en: The static pool serves as a dataset from which the learner can repeatedly draw
    samples, with the aim of acquiring the most informative labeled examples. By tapping
    into this pool, the learner can explore and extract valuable insights that contribute
    to the learning process. Through multiple iterations of sampling, the learner
    gains a deeper understanding and improves their ability to make informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: The pool is designed to provide users with the utmost flexibility, enabling
    them to query any point within the pool at any given time. This feature allows
    for seamless access to data points, ensuring convenience and versatility in the
    querying process. The size of the pool is determined based on computational constraints,
    with common sizes ranging from 10,000 to 1,000,000 data points. The choice of
    pool size depends on various factors, such as the computational resources available
    and the specific requirements of the model being used.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to emphasize that throughout the iterative process, the pool
    remains fixed. The model continuously queries and selects the most valuable points
    from the pool, optimizing its performance and refining its results. This dynamic
    interplay between the pool and the model is essential in achieving optimal efficiency
    and accuracy in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: By maintaining a fixed pool size while iteratively querying for valuable points,
    the model ensures that it can adapt and evolve based on the changing needs of
    the analysis. This iterative approach allows the model to continuously refine
    its understanding and improve its predictions, leading to more insightful and
    accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, potential pool datasets could include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A database table with many unlabeled rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A collection of images, audio clips, or documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An existing ML dataset with the labels temporarily removed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pool-based sampling offers several advantages, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum flexibility in sampling. Any point can be queried multiple times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sampling strategy can be adjusted and improved over multiple iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to re-query points and fine-tune over time as the model changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a few challenges as well:'
  prefs: []
  type: TYPE_NORMAL
- en: It requires sufficient storage for the full unlabeled pool dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is computationally intensive to search large, high-dimensional data pools
    for optimal queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pool does not adapt over time in the way that a live data stream does.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model’s accuracy depends on the selection method used to identify the most
    informative sample, which can reduce the model’s accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, pool-based sampling provides the most practical active learning solution
    for many real-world applications. With sufficient storage and computation, it
    offers total flexibility in iterative querying. Stream-based sampling can complement
    in cases where real-time performance is critical.
  prefs: []
  type: TYPE_NORMAL
- en: Having explored the three different types of active ML scenarios, we can now
    assess how they differ from traditional passive learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing active and passive learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional passive machine learning, models are trained on fixed and pre-existing
    labeled datasets, which are carefully assembled to include both data points and
    their respective ground truth labels. The model then goes through the dataset
    once, without any iteration or interaction, and learns the patterns and relationships
    between the features and labels. This is the passive learning approach. It’s important
    to note that the model only trains on the finite data it is provided and cannot
    actively seek out new information or modify its training based on new inputs.
    Moreover, the labeled datasets required for a passive learning approach come at
    a cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several reasons why labeling is expensive in traditional machine
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual labeling requires experts**: Accurately labeling data often demands
    the expertise of domain specialists such as doctors or ecologists. However, their
    time is limited and valuable, making their involvement expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-consuming process**: Manually labeling data such as images, audio clips,
    or text is a slow and tedious task that does not scale well. It can take minutes
    to hours to accurately label a single data point. While several annotation platforms
    integrate features and tools to make the labeling process smoother and faster,
    it still remains slow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annotation errors**: Some labels may be of lower quality due to overworked,
    rushed, or non-expert labelers. These incorrect and noisy labels can have a negative
    impact on the performance of models. To prevent this, additional oversight and
    one or more reviewing steps are often necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accumulating labeling costs**: When the cost per hour of labeling is multiplied
    by the number of examples needed, the overall expenses for data annotation can
    quickly become prohibitive, especially if several reviewing steps are needed.
    Modern deep learning models often require a large number of labeled examples,
    which can be very expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Expanding model capabilities**: As new cases emerge, there is a constant
    need to acquire new labeled data. Continuously labeling evolving data poses a
    challenge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these reasons make passive labeling an expensive bottleneck. Minimizing
    these costs is crucial for developing scalable and accurate AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: Active learning, on the other hand, takes an interactive and iterative approach.
    Instead of receiving a predefined labeled dataset, an active learning model dynamically
    chooses which data points it wants to be labeled. It analyzes a pool of unlabeled
    data and intelligently selects the most useful points to query an oracle for labels.
    The newly labeled data is then incorporated into its training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'This introduces a feedback loop between data sampling, human labeling, and
    model training. The model guides its own learning by acquiring new training data
    specifically tailored to improve its weaknesses. Human effort is focused only
    on the most informative examples through selective sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Passive learning and active learning – a comparison](img/B21789_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Passive learning and active learning – a comparison
  prefs: []
  type: TYPE_NORMAL
- en: One key advantage of active learning is that it reduces the total amount of
    labeled data required. Passive learners often need vast amounts of labeled examples
    to achieve desired performance, which can be costly and time-consuming to collect
    and prepare. Active learning minimizes this upfront labeling effort by acquiring
    only the examples that provide the maximum information value.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, active learning systems can adapt and adjust over multiple iterations
    of querying. The model can change its sampling strategy based on previous rounds,
    re-query certain examples, or increase focus on areas where it is weakest. In
    contrast, passive learning involves a static dataset without room for adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, active learning provides critical benefits of reduced labeling costs
    and flexible, adaptive training. By steering its own data collection, Active ML
    achieves higher predictive performance with significantly less reliance on vast
    labeled datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this introductory chapter, we covered the fundamentals of active ML and how
    it contrasts with passive learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'You learned what active learning is and its goal of maximizing predictive performance
    with fewer labeled training examples. We discussed the core components of an active
    learning system: the unlabeled data pool, query strategy, machine learning model,
    and the oracle labeler.'
  prefs: []
  type: TYPE_NORMAL
- en: You now understand the difference between membership query synthesis, stream-based
    sampling, and pool-based sampling scenarios. We compared active and passive learning,
    highlighting the benefits of an interactive, iterative approach in active learning.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, you now know that active learning can produce models with equal
    or greater accuracy while requiring far less labeled training data. This is critical
    for reducing the costs of modeling, as labeling is often the most expensive component.
  prefs: []
  type: TYPE_NORMAL
- en: The skills you gained in this introduction will equip you to determine when
    active learning is appropriate for a problem. You can now correctly select the
    right components when implementing an active learning system.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the fundamentals of active learning, query scenarios,
    and applications, the next step is to dive into specific query strategies. In
    the next chapter, we will explore frameworks for designing effective queries to
    identify the most valuable data points to label.
  prefs: []
  type: TYPE_NORMAL
