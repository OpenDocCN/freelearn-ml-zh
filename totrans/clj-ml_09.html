<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Large-scale Machine Learning</h1></div></div></div><p>In this chapter, we will explore a few methodologies for handling large volumes of data to train machine learning models. In the latter section of this chapter, we will also demonstrate how to use cloud-based services for machine learning.</p><div><div><div><div><h1 class="title"><a id="ch09lvl1sec58"/>Using MapReduce</h1></div></div></div><p>A data-processing methodology that is often encountered in the context of data parallelism is <strong>MapReduce</strong>.<a id="id796" class="indexterm"/> This technique is inspired by the <strong>map</strong> and <strong>reduce</strong> functions from functional programming. Although these functions serve as a basis to understand the algorithm, actual implementations of MapReduce focus more on scaling and distributing the processing of data. There are currently several active implementations of MapReduce, such as Apache Hadoop and Google Bigtable.</p><p>A MapReduce engine <a id="id797" class="indexterm"/>or program is composed of a function that performs some processing over a given record in a potentially large dataset (for more information, refer to "Map-Reduce for Machine Learning on Multicore"). This function represents the <code class="literal">Map()</code> step<a id="id798" class="indexterm"/> of the algorithm. This function is applied to all the records in the dataset and the results are then combined. The latter step of extracting the results is termed as the <code class="literal">Reduce()</code> step<a id="id799" class="indexterm"/> of the algorithm. In order to scale this process over huge datasets, the input data provided to the <code class="literal">Map()</code> step is first partitioned and then processed on different computing nodes. These nodes may or may not be on separate machines, but the processing performed by a given node is independent from that of the other nodes in the system. </p><p>Some systems follow a different design in which the code or query is sent to nodes that contain the data, instead of the other way around. This step of partitioning the input data and then forwarding the query or data to different nodes is called the <code class="literal">Partition()</code> step<a id="id800" class="indexterm"/> of the algorithm. To summarize, this method of handling a large dataset is quite different from traditional methods of iterating over the entire data as fast as possible.</p><p>MapReduce scales better than other methods because the partitions of the input data can be processed independently on physically different machines and then combined later. This gain in scalability is not only because the input is divided among several nodes, but because of an intrinsic reduction in complexity. An NP-hard problem cannot be solved for a large problem space,<a id="id801" class="indexterm"/> but can be solved if the problem space is smaller.</p><p>For problems with an algorithmic complexity of <img src="img/4351OS_09_01.jpg" alt="Using MapReduce"/> or <img src="img/4351OS_09_02.jpg" alt="Using MapReduce"/>, partitioning the problem space will actually increase the time needed to solve the given problem. However, if the algorithmic complexity is <img src="img/4351OS_09_03.jpg" alt="Using MapReduce"/>, where <img src="img/4351OS_09_04.jpg" alt="Using MapReduce"/>, partitioning the problem space will reduce the time needed to solve the problem. In case of NP-hard problems, <img src="img/4351OS_09_05.jpg" alt="Using MapReduce"/>. Thus, MapReduce decreases the time needed to solve NP-hard problems by partitioning the problem space (for more information, refer to <em>Evaluating MapReduce for Multi-core and Multiprocessor Systems</em>).</p><p>The MapReduce algorithm can be illustrated using the following diagram:</p><div><img src="img/4351OS_09_06.jpg" alt="Using MapReduce"/></div><p>In the previous diagram, the input data is first partitioned, and each partition is independently processed in the <code class="literal">Map()</code> step. Finally, the results are combined in the <code class="literal">Reduce()</code> step.</p><p>We can concisely define the MapReduce algorithm in Clojure pseudo-code, as shown in the following code:</p><div><pre class="programlisting">(defn map-reduce [f partition-size coll]
  (-&gt;&gt; coll
       (partition-all partition-size)   ; Partition
       (pmap f)                         ; Parallel Map
       (reduce concat)))                ; Reduce</pre></div><p>The <code class="literal">map-reduce</code> function<a id="id802" class="indexterm"/> defined in the previous code distributes the application of the function <code class="literal">f</code> among several<a id="id803" class="indexterm"/> processors (or threads) using the standard <code class="literal">pmap</code> (abbreviation for parallel map) function. The input data, represented by the collection <code class="literal">coll</code>, is first partitioned using the <code class="literal">partition-all</code> function<a id="id804" class="indexterm"/>, and the function <code class="literal">f</code> is then applied to each partition in parallel using the <code class="literal">pmap</code> function. The results of this <code class="literal">Map()</code> step are then combined using a composition of the standard <code class="literal">reduce</code> and <code class="literal">concat</code> functions. Note that this is possible in Clojure due to the fact the each partition of data is a sequence, and the <code class="literal">pmap</code> function will thus return a sequence of partitions that can be joined or concatenated into a single sequence to produce the result of the computation.</p><p>Of course, this is only a theoretical explanation of the core of the MapReduce algorithm. Actual implementations tend to focus more on distributing the processing among several machines, rather than <a id="id805" class="indexterm"/>among several processors or threads as shown in the <code class="literal">map-reduce</code> function defined in the previous code.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec59"/>Querying and storing datasets</h1></div></div></div><p>When dealing with large datasets,<a id="id806" class="indexterm"/> it's useful to be able to query the data based on some arbitrary conditions. Also, it's more reliable to store the data in a database rather than in a <a id="id807" class="indexterm"/>flat file or as an in-memory resource. The Incanter library provides us<a id="id808" class="indexterm"/> with several useful functions<a id="id809" class="indexterm"/> to perform these operations, as we will demonstrate in the code example that will follow.</p><div><div><h3 class="title"><a id="note49"/>Note</h3><p>The Incanter library and the MongoDB driver used in the upcoming example can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">[congomongo "0.4.1"]
[incanter "1.5.4"]</pre></div><p>For the upcoming example, the namespace declaration should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use incanter.core
        [incanter.mongodb   :only [insert-dataset
                                   fetch-dataset]]
        [somnium.congomongo :only [mongo!]]
        [incanter.datasets  :only [get-dataset]]))</pre></div><p>Also, this example requires MongoDB to be installed and running.</p></div></div><p>For this example, we will use the Iris dataset, which can be fetched using the <code class="literal">get-dataset</code> function from the <code class="literal">incanter.datasets</code> namespace. The code is as follows:</p><div><pre class="programlisting">(def iris (get-dataset :iris))</pre></div><p>As shown in the previous code, we simply bind the Iris dataset to a variable <code class="literal">iris</code>. We can perform various operations on this dataset using the <code class="literal">with-data</code> function. To view the data, we can use the <code class="literal">view</code> function along with the <code class="literal">with-data</code> function to provide a tabular representation of the dataset, as shown in the following code:</p><div><pre class="programlisting">user&gt; (with-data iris
        (view (conj-cols (range (nrow $data)) $data)))</pre></div><p>The <code class="literal">$data</code> variable is a special binding that can be used to represent the entire dataset within the scope of the <code class="literal">with-data</code> function. In the previous code, we add an extra column to represent the row number of <a id="id810" class="indexterm"/>a record to the data using a composition of the <code class="literal">conj-cols</code>, <code class="literal">nrows</code>, and <code class="literal">range</code> functions. The data is then displayed in a <a id="id811" class="indexterm"/>spreadsheet-like table using the <code class="literal">view</code> function. The previous code produces the following table that represents the dataset:</p><div><img src="img/4351OS_09_07.jpg" alt="Querying and storing datasets"/></div><p>We can also select columns<a id="id812" class="indexterm"/> we are interested in from the original dataset using<a id="id813" class="indexterm"/> the <code class="literal">$</code> function within the scope of the <code class="literal">with-data</code> function<a id="id814" class="indexterm"/>, as shown in the following code:</p><div><pre class="programlisting">user&gt; (with-data iris ($ [:Species :Sepal.Length]))


|   :Species | :Sepal.Length |
|------------+---------------|
|     setosa |           5.1 |
|     setosa |           4.9 |
|     setosa |           4.7 |
  ...
|  virginica |           6.5 |
|  virginica |           6.2 |
|  virginica |           5.9 |</pre></div><p>The <code class="literal">$</code> function selects the <code class="literal">:Species</code> and <code class="literal">:Sepal.Length</code> columns from the <code class="literal">iris</code> dataset in the code example shown previously. We can also filter the data based on a condition using the <code class="literal">$where</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (with-data iris ($ [:Species :Sepal.Length]
                         ($where {:Sepal.Length 7.7})))

|  :Species | :Sepal.Length |
|-----------+---------------|
| virginica |           7.7 |
| virginica |           7.7 |
| virginica |           7.7 |
| virginica |           7.7 |</pre></div><p>The previous example queries the <code class="literal">iris</code> dataset for records with the <code class="literal">:Sepal.Length</code> column equal to <code class="literal">7.7</code> using the <code class="literal">$where</code> function. We can also specify the lower or upper bound of the value to compare<a id="id815" class="indexterm"/> a column to using the <code class="literal">:$gt</code> and <code class="literal">:$lt</code> symbols in a map<a id="id816" class="indexterm"/> passed to <code class="literal">$where</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (with-data iris ($ [:Species :Sepal.Length]
                         ($where {:Sepal.Length {:$gt 7.0}})))

|  :Species | :Sepal.Length |
|-----------+---------------|
| virginica |           7.1 |
| virginica |           7.6 |
| virginica |           7.3 |
  ...
| virginica |           7.2 |
| virginica |           7.2 |
| virginica |           7.4 |</pre></div><p>The previous example <a id="id817" class="indexterm"/>checks for records that have a <code class="literal">:Sepal.Length</code> attribute with a value greater than <code class="literal">7</code>. To check whether a column's value<a id="id818" class="indexterm"/> lies within a given range, we can specify both the <code class="literal">:$gt</code> and <code class="literal">:$lt</code> keys in the map passed to the <code class="literal">$where</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (with-data iris ($ [:Species :Sepal.Length]
                         ($where {:Sepal.Length
                                  {:$gt 7.0 :$lt 7.5}})))

|  :Species  |:Sepal.Length |
|------------+--------------|
| virginica  |          7.1 |
| virginica  |          7.3 |
| virginica  |          7.2 |
| virginica  |          7.2 |
| virginica  |          7.2 |
| virginica  |          7.4 |</pre></div><p>The previous example checks for records that have a <code class="literal">:Sepal.Length</code> attribute within the range of <code class="literal">7.0</code> and <code class="literal">7.5</code>. We can also specify a discrete set of values using the <code class="literal">$:in</code> key, such as in the expression <code class="literal">{:$in #{7.2 7.3 7.5}}</code>. The Incanter library also provides several other functions such as <code class="literal">$join</code> and <code class="literal">$group-by</code> that can be used to express more complex queries.</p><p>The Incanter library provides<a id="id819" class="indexterm"/> functions to operate with MongoDB to persist and fetch datasets. MongoDB is a nonrelational document database that allows for storage of JSON documents with dynamic schemas. To connect to a MongoDB instance, we use the <code class="literal">mongo!</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (mongo! :db "sampledb")
true</pre></div><p>In the previous code, the <a id="id820" class="indexterm"/>database name <code class="literal">sampledb</code> is specified as a keyword argument with<a id="id821" class="indexterm"/> the key <code class="literal">:db</code> to the <code class="literal">mongo!</code> function. We can also specify the<a id="id822" class="indexterm"/> hostname and port of the instance to connect to using the <code class="literal">:host</code> and <code class="literal">:post</code> keyword arguments, respectively.</p><p>We can store datasets in the connected MongoDB instance using the <code class="literal">insert-dataset</code> function from the <code class="literal">incanter.mongodb</code> namespace. Unfortunately, MongoDB does not support the use of the dot character (.) as column names, and so we must change the names of the columns in the <code class="literal">iris</code> dataset in order to successfully store it using the <code class="literal">insert-dataset</code> function<a id="id823" class="indexterm"/>. Replacing the column names can be done using the <code class="literal">col-names</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (insert-dataset
:iris (col-names iris [:SepalLength
:SepalWidth
:PetalLength
:PetalWidth
:Species]))</pre></div><p>The previous code stores the <code class="literal">iris</code> dataset in the MongoDB instance after replacing the dot characters in the column names.</p><div><div><h3 class="title"><a id="note52"/>Note</h3><p>Note that the dataset will be stored in a collection named <code class="literal">iris</code> in the <code class="literal">sampledb</code> database. Also, MongoDB will assign a hash-based ID to each record in the dataset that was stored in the database. This column can be referred to using the <code class="literal">:_id</code> keyword.</p></div></div><p>To fetch the dataset back from the database, we use the <code class="literal">fetch-dataset</code> function, as shown in the following code. The value returned by this function can be directly used by the <code class="literal">with-data</code> function<a id="id824" class="indexterm"/> to query and view the dataset fetched.</p><div><pre class="programlisting">user&gt; (with-data (fetch-dataset :iris) ($ [:Species :_id]
                                          ($where {:SepalLength
                                                   {:$gt 7}})))

|  :Species |                     :_id |
|-----------+--------------------------|
| virginica | 52ebcc1144ae6d6725965984 |
| virginica | 52ebcc1144ae6d6725965987 |
| virginica | 52ebcc1144ae6d6725965989 |
  ...
| virginica | 52ebcc1144ae6d67259659a0 |
| virginica | 52ebcc1144ae6d67259659a1 |
| virginica | 52ebcc1144ae6d67259659a5 |</pre></div><p>We can also inspect the database after storing our dataset, using the <code class="literal">mongo</code> client, as shown in the following code. <a id="id825" class="indexterm"/>As we mentioned our database name is <code class="literal">sampledb</code>, we must <a id="id826" class="indexterm"/>select this database using the <code class="literal">use</code> command, as shown<a id="id827" class="indexterm"/> in the following terminal output:</p><div><pre class="programlisting">$ mongo
MongoDB shell version: 2.4.6
connecting to: test
Server has startup warnings:
...

&gt; use sampledb
switched to db sampledb</pre></div><p>We can view all <a id="id828" class="indexterm"/>collections in the database using the <code class="literal">show collections</code> command. The queries can be executed using the <code class="literal">find()</code> function on the appropriate property in the variable <code class="literal">db</code> instance, as shown in the following code:</p><div><pre class="programlisting">&gt; show collections
iris
system.indexes
&gt;
&gt; db.iris.find({ SepalLength: 5})

{ "_id" : ObjectId("52ebcc1144ae6d6725965922"),
  "Species" : "setosa",
  "PetalWidth" : 0.2,
  "PetalLength" : 1.4,
  "SepalWidth" : 3.6,
  "SepalLength" : 5 }
{ "_id" : ObjectId("52ebcc1144ae6d6725965925"),
  "Species" : "setosa",
  "PetalWidth" : 0.2,
  "PetalLength" : 1.5,
  "SepalWidth" : 3.4,
  "SepalLength" : 5 }

...</pre></div><p>To conclude, the Incanter library <a id="id829" class="indexterm"/>provides us with a sufficient set of tools for querying and <a id="id830" class="indexterm"/>storing datasets. Also, MongoDB can be easily<a id="id831" class="indexterm"/> used to store datasets via the<a id="id832" class="indexterm"/> Incanter library.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec60"/>Machine learning in the cloud</h1></div></div></div><p>In the modern day of web-based and cloud services, it is also possible to persist both datasets and machine learning <a id="id833" class="indexterm"/>models to online cloud storage. This is a great solution when dealing with enormous amounts of data, since cloud solutions take care of both the storage and processing of huge amounts of data. </p><p>
<strong>BigML</strong><a id="id834" class="indexterm"/> (<a class="ulink" href="http://bigml.com/">http://bigml.com/</a>) is a cloud provider<a id="id835" class="indexterm"/> for machine learning resources. BigML internally uses <strong>Classification and Regression Trees</strong> (<strong>CARTs</strong>), <a id="id836" class="indexterm"/>which are a specialization of decision trees (for more information, refer to <em>Top-down induction of decision trees classifiers-a survey</em>), as a machine learning model. </p><p>BigML provides developers with a simple REST API that can be used to work with the service from any language or platform that can send HTTP requests. The service supports several file formats such as <strong>CSV</strong> (<strong>comma-separated values</strong>)<a id="id837" class="indexterm"/>, Excel spreadsheet, and the Weka library's ARFF format, and also supports a variety of data compression formats such as TAR and GZIP. This service also takes a white-box approach, in the sense that models can be downloaded for local use, apart from the use of the models for predictions through the online web interface.</p><p>There are bindings for BigML in several languages, and we will demonstrate a Clojure client library for BigML in this section. Like other cloud services, users and developers of BigML must first register for an account. They can then use this account and a provided API key to access BigML <a id="id838" class="indexterm"/>from a client library. A new BigML account provides a few example datasets to experiment with, including the Iris dataset that we've frequently<a id="id839" class="indexterm"/> encountered in this book. </p><p>The dashboard of a BigML account provides a simple web-based user interface for all the resources available to the account.</p><div><img src="img/4351OS_09_08.jpg" alt="Machine learning in the cloud"/></div><p>BigML resources include sources, datasets, models, predictions, and evaluations. We will discuss each of these resources in the upcoming code example.</p><div><div><h3 class="title"><a id="note53"/>Note</h3><p>The BigML Clojure library can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">[bigml/clj-bigml "0.1.0"]</pre></div><p>For the upcoming example, the namespace declaration should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:require [bigml.api [core :as api]
             [source :as source]
             [dataset :as dataset]
             [model :as model]
             [prediction :as prediction]
             [evaluation :as evaluation]]))</pre></div></div></div><p>Firstly, we will have to provide authentication details<a id="id840" class="indexterm"/> for the BigML service. This is done using the <code class="literal">make-connection</code> function<a id="id841" class="indexterm"/> from the <code class="literal">bigml.api</code> namespace. We must provide a username, an API key, and a flag indicating whether we are using development or production datasets to the <code class="literal">make-connection</code> function, as shown in the following code. Note that this username and API key will be shown on your BigML account page.</p><div><pre class="programlisting">(def default-connection
  (api/make-connection
   "my-username"                               ; username
   "a3015d5fa2ee19604d8a69335a4ac66664b8b34b"  ; API key
   true))</pre></div><p>To use the connection <code class="literal">default-connection</code> defined in the previous code, we must use the <code class="literal">with-connection</code> function<a id="id842" class="indexterm"/>. We can avoid repeating the use of the <code class="literal">with-connection</code> function with the <code class="literal">default-connection</code> variable by use of a simple<a id="id843" class="indexterm"/> macro, as shown in the following code:</p><div><pre class="programlisting">(defmacro with-default-connection [&amp; body]
  '(api/with-connection default-connection
     ~@body))</pre></div><p>In effect, using <code class="literal">with-default-connection</code> is as good as using the <code class="literal">with-connection</code> function with the <code class="literal">default-connection</code> binding, thus helping us avoid repeating code.</p><p>BigML has the notion of <a id="id844" class="indexterm"/>sources to represent resources that can be converted to training data. BigML supports local files, remote files, and inline code resources as sources, and also supports multiple data types. To create a resource, we can use the <code class="literal">create</code> function from the <code class="literal">bigml.source</code> namespace, as shown in the following code:</p><div><pre class="programlisting">(def default-source
  (with-default-connection
    (source/create [["Make"  "Model"   "Year" "Weight" "MPG"]
                    ["AMC"   "Gremlin" 1970   2648     21]
                    ["AMC"   "Matador" 1973   3672     14]
                    ["AMC"   "Gremlin" 1975   2914     20]
                    ["Honda" "Civic"   1974   2489     24]
                    ["Honda" "Civic"   1976   1795     33]])))</pre></div><p>In the previous code, we define a source using some inline data. The data is actually a set of features of various car models, such as their year of manufacture and total weight. The last feature is the mileage or MPG of the car model. By convention, BigML sources treat the last column as the output or objective variable of the machine learning model.</p><p>We must now convert the<a id="id845" class="indexterm"/> source to a BigML dataset, which is a structured and indexed representation of the raw data from a source. Each feature in the data is assigned a unique integer ID in a dataset. This dataset can then be used to train a machine learning CART model, which is simply termed as a model in BigML jargon. We can create a dataset and a model using the <code class="literal">dataset/create</code> and <code class="literal">model/create</code> functions, respectively, as shown in the following code. Also, we will have to use the <code class="literal">api/get-final</code> function<a id="id846" class="indexterm"/> to finalize a resource that has been sent to the BigML cloud service for processing.</p><div><pre class="programlisting">(def default-dataset
  (with-default-connection
    (api/get-final (dataset/create default-source))))

(def default-model
  (with-default-connection
    (api/get-final (model/create default-dataset))))</pre></div><p>BigML also provides an interactive visualization of a trained CART model. For our training data, the following visualization is produced:</p><div><img src="img/4351OS_09_09.jpg" alt="Machine learning in the cloud"/></div><p>We can now use the trained model to predict the value of the output variable. Each prediction is stored in the BigML cloud service, and is shown in the <strong>Predictions</strong> tab of the dashboard. This is done using the <code class="literal">create</code> function from the <code class="literal">bigml.prediction</code> namespace, as shown in the following code:</p><div><pre class="programlisting">(def default-remote-prediction
  (with-default-connection
    (prediction/create default-model [1973 3672])))</pre></div><p>In the previous code, we attempt to predict the MPG (miles per gallon, a measure of mileage) of a car model by providing values for the year of manufacture and the weight of the car to the <code class="literal">prediction/create</code> function. The value returned by this function is a map, which contains a key <code class="literal">:prediction</code> among other things, that represents the predicted value of the output variable. <a id="id847" class="indexterm"/>The value of this key is another map that contains column IDs as keys and their predicted values as values in the map, as shown in the following code:</p><div><pre class="programlisting">user&gt; (:prediction default-remote-prediction)
{:000004 33}</pre></div><p>The MPG column, which<a id="id848" class="indexterm"/> has the ID <code class="literal">000004</code>, is predicted to have a value of <code class="literal">33</code> from the trained model, as shown in the previous code. The <code class="literal">prediction/create</code> function<a id="id849" class="indexterm"/> creates an online, or remote, prediction, and sends data to the BigML service whenever it is called. Alternatively, we can download a function from the BigML service that we can use to perform predictions locally using the <code class="literal">prediction/predictor</code> function<a id="id850" class="indexterm"/>, as shown in the following code:</p><div><pre class="programlisting">(def default-local-predictor
  (with-default-connection
    (prediction/predictor default-model)))</pre></div><p>We can now use the downloaded function, <code class="literal">default-local-predictor</code>, to perform local predictions, as shown in the following REPL output:</p><div><pre class="programlisting">user&gt; (default-local-predictor [1983])
22.4
user&gt; (default-local-predictor [1983] :details true)
{:prediction {:000004 22.4},
:confidence 24.37119,
:count 5, :id 0,
:objective_summary
 {:counts [[14 1] [20 1] [21 1] [24 1] [33 1]]}}</pre></div><p>As shown in the previous code, the local prediction function predicts the MPG of a car manufactured in <code class="literal">1983</code> as <code class="literal">22.4</code>. We can also pass the <code class="literal">:details</code> keyword argument to the <code class="literal">default-local-predictor</code> function to provide more information about the prediction.</p><p>BigML also allows us to evaluate trained CART models. We will now train a model using the Iris dataset and then cross-validate it. The <code class="literal">evaluation/create</code> function from the BigML library will create an evaluation using a trained model and some cross-validation data. This function returns a map that contains all cross-validation information about the model.</p><p>In the previous code snippets, we used the <code class="literal">api/get-final</code> function in almost all stages of training a model. In the following code example, we will attempt to avoid repeated use of this function by using a macro. We first define a function to apply the <code class="literal">api/get-final</code> and <code class="literal">with-default-connection</code> functions to an arbitrary function that takes any number of arguments.</p><div><pre class="programlisting">(defn final-with-default-connection [f &amp; xs]
  (with-default-connection
    (api/get-final (apply f xs))))</pre></div><p>Using the <code class="literal">final-with-default-connection</code> function<a id="id851" class="indexterm"/> defined in the previous code, we can define a macro that will map it to a list of values, as shown in the following code:</p><div><pre class="programlisting">(defmacro get-final-&gt; [head &amp; body]
  (let [final-body (map list
                        (repeat 'final-with-default-connection)
                        body)]
    '(-&gt;&gt; ~head
          ~@final-body)))</pre></div><p>The <code class="literal">get-final-&gt;</code> macro defined<a id="id852" class="indexterm"/> in the previous code basically uses the <code class="literal">-&gt;&gt;</code> threading macro to pass the value in the <code class="literal">head</code> argument through the functions in the <code class="literal">body</code> argument. <a id="id853" class="indexterm"/>Also, the previous macro interleaves application of the <code class="literal">final-with-default-connection</code> function to finalize the values returned by functions in the <code class="literal">body</code> argument. We can now use the <code class="literal">get-final-&gt;</code> macro to create a source, dataset, and model in a single expression, and then evaluate the model using the <code class="literal">evaluation/create</code> function<a id="id854" class="indexterm"/>, as shown in the following code:</p><div><pre class="programlisting">(def iris-model
  (get-final-&gt; "https://static.bigml.com/csv/iris.csv"
               source/create
               dataset/create
               model/create))

(def iris-evaluation
  (with-default-conection
    (api/get-final
     (evaluation/create iris-model (:dataset iris-model)))))</pre></div><p>In the previous code snippet, we use a remote file that contains the Iris sample data as a source, and pass it to the <code class="literal">source/create</code>, <code class="literal">dataset/create</code>, and <code class="literal">model/create</code> functions in sequence using the <code class="literal">get-final-&gt;</code> macro we previously defined. </p><p>The formulated model is then evaluated using a composition of the <code class="literal">api/get-final</code> and <code class="literal">evaluation/create</code> functions,<a id="id855" class="indexterm"/> and the result is stored in the variable <code class="literal">iris-evaluation</code>. Note that we use the training data itself to cross-validate the model, which doesn't really achieve anything useful. In practice, however, we should use unseen data<a id="id856" class="indexterm"/> to evaluate a trained machine<a id="id857" class="indexterm"/> learning model. Obviously, as we use the training data to cross-validate the model, the accuracy of the model is found to be a 100 percent or 1, as shown in the following code:</p><div><pre class="programlisting">user&gt; (-&gt; iris-evaluation :result :model :accuracy)
1</pre></div><p>The BigML dashboard will also provide a visualization (as shown in the following diagram) of the model formulated from the data in the previous example. This illustration depicts the CART decision tree that was formulated from the Iris sample dataset.</p><div><img src="img/4351OS_09_10.jpg" alt="Machine learning in the cloud"/></div><p>To conclude, the BigML cloud service provides us with several flexible options to estimate CARTs from large datasets in a scalable and platform-independent manner. BigML is just one of the many<a id="id858" class="indexterm"/> machine learning services available online, and the reader is encouraged to explore other cloud service providers of machine learning.</p></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec61"/>Summary</h1></div></div></div><p>In this chapter, we explored a few useful techniques to deal with huge amounts of sample data. We also described how we can use machine learning models through online services such as BigML, as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We described MapReduce and how it is used to process large volumes of data using parallel and distributed computations</li><li class="listitem" style="list-style-type: disc">We explored how we can query and persist datasets using the Incanter library and MongoDB</li><li class="listitem" style="list-style-type: disc">We briefly studied the BigML cloud service provider and how we can use this service to formulate and evaluate CARTs from sample data</li></ul></div><p>In conclusion, we described several techniques and tools that can be used to implement machine learning systems in this book. Clojure helps us build these systems in a simple and scalable manner by leveraging the power of the JVM and equally powerful libraries. We also studied how we can evaluate and improve machine learning systems. Programmers and architects can use these tools and techniques to model and learn from their users' data, as well as build machine learning systems that provide users with a better experience.</p><p>You can explore the academia and research in machine learning through the various citations and references that have been used in this book. New academic papers and articles on machine learning provide even more insight into the cutting-edge of machine learning, and you are encouraged to find and explore them.</p></div></body></html>