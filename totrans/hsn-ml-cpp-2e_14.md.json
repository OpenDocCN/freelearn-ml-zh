["```py\nsudo apt install default-jre\n```", "```py\n# make the folder where to install components\nmkdir android\ncd android\n# download command line tools\nwget https://dl.google.com/android/repository/commandlinetools-linux-9477386_latest.zip\n# unzip them and move to the correct folder\nunzip commandlinetools-linux-9477386_latest.zip\nmv cmdline-tools latest\nmkdir cmdline-tools\nmv latest cmdline-tools\n# install SDK, NDK and build tools for Android using sdkmanager utility\nyes | ./cmdline-tools/latest/bin/sdkmanager --licenses\nyes | ./cmdline-tools/latest/bin/sdkmanager \"platform-tools\"\nyes | ./cmdline-tools/latest/bin/sdkmanager \"platforms;android-35\"\nyes | ./cmdline-tools/latest/bin/sdkmanager \"build-tools;35.0.0\"\nyes | ./cmdline-tools/latest/bin/sdkmanager \"system-images;android-35;google_apis;arm64-v8a\"\nyes | ./cmdline-tools/latest/bin/sdkmanager --install \"ndk;26.1.10909125\"\n```", "```py\nandroid/ndk/26.1.10909125/\n```", "```py\ncd /home/[USER]\ngit clone https://github.com/pytorch/pytorch.git\ncd pytorch/\ngit checkout v2.3.1\ngit submodule update --init --recursive\nexport ANDROID_NDK=[Path to the installed NDK]\nexport ANDROID_ABI='arm64-v8a'\nexport ANDROID_STL_SHARED=1\n$START_DIR/android/pytorch/scripts/build_android.sh \\\n-DBUILD_CAFFE2_MOBILE=OFF \\\n-DBUILD_SHARED_LIBS=ON \\\n-DUSE_VULKAN=OFF \\\n-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())') \\\n-DPYTHON_EXECUTABLE=$(python -c 'import sys; print(sys.executable)') \\\n```", "```py\n    git clone https://github.com/ultralytics/yolov5\n    cd yolov5\n    jit script of the model optimized for mobile:\n\n    ```", "```py\n\n    ```", "```py\napp\n|--src\n|  `--main\n|    |--cpp\n|    |  |—CmakeLists.txt\n|    |  `—native-lib.cpp\n|    |--java\n|    |  `--com\n|    |    `--example\n|    |       `--objectdetection\n|    |         `--MainActivity.kt\n|    |--res\n|    |  `--layout\n|    |    `--activity_main.xml\n|    |--values\n|       |--colors.xml\n|       |--strings.xml\n|       |--styles.xml\n|          `—…\n|--build.gradle\n`--...\n```", "```py\napp\n|--src\n|  |--main\n|  |--…\n|  |--JniLibs\n|     `--arm64-v8a\n|        |--libc10.so\n|        |--libtorch_cpu.so\n|        |--libtorch_global_deps.so\n|        `—libtorch.so\n`...\n```", "```py\napp\n|--src\n|  `--main\n|     |--...\n|     |--cpp\n|     |--JniLibs\n|     |--assests\n|     |  |--yolov5.torchscript\n|     |  `--classes.txt\n|     `—...\n`...\n```", "```py\n…\n<activity\n    …\n    android:screenOrientation=\"portrait\">\n…\n```", "```py\nclass MainActivity\n    : NativeActivity(),\n      ActivityCompat.OnRequestPermissionsResultCallback {\n  …\n}\n```", "```py\noverride fun onResume() {\n  super.onResume() val cameraPermission =\n      android.Manifest.permission\n          .CAMERA if (checkSelfPermission(\n                          cameraPermission) !=\n                      PackageManager.PERMISSION_GRANTED) {\n    requestPermissions(arrayOf(cameraPermission),\n                       CAM_PERMISSION_CODE)\n  }\n  else {\n    val camId =\n        getCameraBackCameraId() if (camId.isEmpty()){\n            Toast\n                .makeText(\n                    this,\n                    \"Camera probably won't work on this\n                    device !\",\n                    Toast.LENGTH_LONG)\n                .show() finish()} initObjectDetection(camId)\n  }\n}\n```", "```py\noverride fun onRequestPermissionsResult(requestCode\n                                        : Int, permissions\n                                        : Array<out String>,\n                                          grantResults\n                                        : IntArray) {\n  super.onRequestPermissionsResult(requestCode,\n                                   permissions,\n                                   grantResults)\n    if (requestCode == CAM_PERMISSION_CODE &&\n      grantResults[0] != PackageManager.PERMISSION_GRANTED)\n    {\n       Toast.makeText(this,\n                     \"This app requires camera permission\",\n                     Toast.LENGTH_SHORT).show()\n                     finish()\n    }\n}\n```", "```py\nprivate fun getCameraBackCameraId(): String {\n  val camManager = getSystemService(\n      Context.CAMERA_SERVICE)as CameraManager\n  for (camId in camManager.cameraIdList) {\n      val characteristics =\n          camManager.getCameraCharacteristics(camId)\n      val hwLevel = characteristics.get(\n     CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL)\n      val facing = characteristics.get(\n          CameraCharacteristics.LENS_FACING)\n      if (hwLevel != INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY &&\n          facing == LENS_FACING_BACK) {\n              return camId\n      }\n  }\n  return \"\"\n}\n```", "```py\noverride fun onPause() {\n  super.onPause()\n  stopObjectDetection()\n}\n```", "```py\nprivate external fun initObjectDetection(camId: String)\nprivate external fun stopObjectDetection()\ncompanion object {\n  init {\n      System.loadLibrary(\"object-detection\")\n  }\n}\n```", "```py\n#include <jni.h>\n...\nstd::shared_ptr<ObjectDetector> object_detector_;\nextern \"C\" JNIEXPORT void JNICALL\nJava_com_example_objectdetection_MainActivity_initObjectDetection(\n    JNIEnv* env,\n    jobject /* this */,\n    jstring camId) {\n  auto camera_id = env->GetStringUTFChars(camId, nullptr);\n\nLOGI(\"Camera ID: %s\", camera_id);\n  if (object_detector_) {\n    object_detector_->allow_camera_session(camera_id);\n    object_detector_->configure_resources();\n  } else\n    LOGE(\"Object Detector object is missed!\");\n}\n```", "```py\nextern \"C\" JNIEXPORT void JNICALL\nJava_com_example_objectdetection_MainActivity_stopObjectDetection(\n    JNIEnv*,\n    jobject /* this */) {\n  if (object_detector_) {\n    object_detector_->release_resources();\n  } else\n    LOGE(\"Object Detector object is missed!\");\n}\n```", "```py\n#include <android/log.h>\n#define LOG_TAG \"OBJECT-DETECTION\"\n#define LOGI(...) __android_log_print(ANDROID_LOG_INFO,\n                                      LOG_TAG, __VA_ARGS__)\n#define LOGW(...) __android_log_print(ANDROID_LOG_WARN,\n                                      LOG_TAG, __VA_ARGS__)\n#define LOGE(...) __android_log_print(ANDROID_LOG_ERROR,\n                                      LOG_TAG, __VA_ARGS__)\n#define ASSERT(cond, fmt, ...)                                \\\n  if (!(cond))\n  {                                              \\\n     __android_log_assert(#cond, LOG_TAG, fmt, ##__VA_ARGS__); \\\n  }\n```", "```py\nextern \"C\" void android_main(struct android_app* app) {\n  LOGI(\"Native entry point\");\n  object_detector_ = std::make_shared<ObjectDetector>(app);\n  app->onAppCmd = ProcessAndroidCmd;\n  while (!app->destroyRequested) {\n    struct android_poll_source* source = nullptr;\n    auto result = ALooper_pollOnce(0, nullptr, nullptr,\n                                   (void**)&source);\n    ASSERT(result != ALOOPER_POLL_ERROR,\n           \"ALooper_pollOnce returned an error\");\n    if (source != nullptr) {\n      source->process(app, source);\n    }\n    if (object_detector_)\n      object_detector_->draw_frame();\n  }\n  object_detector_.reset();\n}\n```", "```py\n static void ProcessAndroidCmd(struct android_app* /*app*/,\n                              int32_t cmd) {\n  if (object_detector_) {\n  switch (cmd) {\n    case APP_CMD_INIT_WINDOW:\n      object_detector_->configure_resources();\n      break;\n    case APP_CMD_TERM_WINDOW:\n      object_detector_->release_resources();\n      break;\n  }\n}\n```", "```py\nObjectDetector::ObjectDetector(android_app *app) : android_app_(app) {\n  yolo_ = std::make_shared<YOLO>(app->activity->assetManager);\n}\n```", "```py\nObjectDetector::~ObjectDetector() {\n  release_resources();\n  LOGI(\"Object Detector was destroyed!\");\n}\nvoid ObjectDetector::release_resources() {\n  delete_camera();\n  delete_image_reader();\n  delete_session();\n}\n```", "```py\nvoid ObjectDetector::allow_camera_session(std::string_view camera_id) {\n  camera_id_ = camera_id;\n}\n```", "```py\nbool ObjectDetector::is_session_allowed() const {\n  return !camera_id_.empty();\n}\n```", "```py\nvoid ObjectDetector::create_camera() {\n  camera_mgr_ = ACameraManager_create();\n  ASSERT(camera_mgr_, \"Failed to create Camera Manager\");\n  ACameraManager_openCamera(camera_mgr_, camera_id_.c_str(),\n                            &camera_device_callbacks,\n                            &camera_device_);\n  ASSERT(camera_device_, \"Failed to open camera\");\n}\n```", "```py\nnamespace {\nvoid onDisconnected(\n    [[maybe_unused]] void* context,\n    [[maybe_unused]] ACameraDevice* device) {\n  LOGI(\"Camera onDisconnected\");\n}\nvoid onError([[maybe_unused]] void* context,\n             [[maybe_unused]] ACameraDevice* device,\n             int error) {\n  LOGE(\"Camera error %d\", error);\n}\nACameraDevice_stateCallbacks camera_device_callbacks = {\n    .context = nullptr,\n    .onDisconnected = onDisconnected,\n    .onError = onError,\n};\n}  // namespace\n```", "```py\nvoid ObjectDetector::configure_resources() {\n  if (!is_session_allowed() || !android_app_ ||\n      !android_app_->window) {\n          LOGE(\"Can't configure output window!\");\n      return;\n  }\n  if (!camera_device_)\n      create_camera();\n  // configure output window size and format\n  ...\n  if (!image_reader_ && !session_output_) {\n      create_image_reader();\n      create_session();\n  }\n}\n```", "```py\nACameraMetadata *metadata_obj{nullptr};\nACameraManager_getCameraCharacteristics(camera_mgr_,\n                                        camera_id_.c_str(),\n                                        &metadata_obj);\nACameraMetadata_const_entry entry;\nACameraMetadata_getConstEntry(metadata_obj,\n                              ACAMERA_SENSOR_ ORIENTATION,\n                              &entry);\norientation_ = entry.data.i32[0];\nbool is_horizontal = orientation_ == 0 || orientation_ == 270;\nauto out_width = is_horizontal ? width_ : height_;\nauto out_height = is_horizontal ? height_ : width_;\nANativeWindow_setBuffersGeometry(android_app_->window,\n                                 out_width,\n                                 out_height,\n                                 WINDOW_FORMAT_RGBA_8888);\n```", "```py\nvoid ObjectDetector::create_image_reader() {\n  constexpr int32_t MAX_BUF_COUNT = 4;\n  auto status = AImageReader_new(\n      width_, height_, AIMAGE_FORMAT_YUV_420_888,\n      MAX_BUF_COUNT, &image_reader_);\n  ASSERT(image_reader_ && status == AMEDIA_OK,\n         \"Failed to create AImageReader\");\n}\n```", "```py\nvoid ObjectDetector::create_session() {\n  ANativeWindow* output_ native_window;\n  AImageReader_getWindow(image_reader_,\n                         &output_ native_window);\n  ANativeWindow_acquire(output_native_window);\n  ACaptureSessionOutputContainer_create(&output_container_);\n  ACaptureSessionOutput_create(output_native_window,\n                               &session_ output_);\n  ACaptureSessionOutputContainer_add(output_container_,\n                                     session_output_);\n  ACameraOutputTarget_create(output_native_window,\n                             &output_target_);\n  ACameraDevice_createCaptureRequest(\n      camera_ device_, TEMPLATE_PREVIEW, &capture_request_);\n  ACaptureRequest_ addTarget(capture_request_,\n                             output_target_);\n  ACameraDevice_createCaptureSession(camera_device_,\n                                     output_container_,\n                                     &session_callbacks,\n                                     &capture_session_);\n  // Start capturing continuously\n  ACameraCaptureSession_setRepeatingRequest(capture_session_,\n                                            nullptr,\n                                            1,\n                                            &capture_request_,\n                                            nullptr);\n}\n```", "```py\nvoid ObjectDetector::draw_frame() {\n  if (image_reader_ == nullptr)\n      return;\n  AImage *image = nullptr;\n  auto status = AImageReader_acquireNextImage(image_reader_, &image);\n\n  if (status != AMEDIA_OK) {\n      return;\n  }\nANativeWindow_acquire(android_app_->window);\nANativeWindow_Buffer buf;\nif (ANativeWindow_lock(android_app_->window,\n                       &buf,\n                       nullptr) < 0) {\n  AImage_delete(image);\n  return;\n}\n    process_image(&buf, image);\n    AImage_delete(image);\n    ANativeWindow_unlockAndPost(android_app_->window);\n    ANativeWindow_release(android_app_->window);\n}\n```", "```py\nvoid ObjectDetector::process_image(\n    ANativeWindow_Buffer* buf,\n    AImage* image);\n```", "```py\nint32_t src_format = -1;\nAImage_getFormat(image, &src_format);\nASSERT(AIMAGE_FORMAT_YUV_420_888 == src_format,\n       \"Unsupported image format for displaying\");\nint32_t num_src_planes = 0;\nAImage_getNumberOfPlanes(image, &num_src_planes);\nASSERT(num_src_planes == 3,\n      \"Image for display has unsupported number of planes\");\nint32_t src_height;\nAImage_getHeight(image, &src_height);\nint32_t src_width;\nAImage_getWidth(image, &src_width);\n```", "```py\nint32_t y_stride{0};\nAImage_getPlaneRowStride(image, 0, &y_stride);\nint32_t uv_stride1{0};\nAImage_getPlaneRowStride(image, 1, &uv_stride1);\nint32_t uv_stride2{0};\nAImage_getPlaneRowStride(image, 1, &uv_stride2);\nuint8_t *y_pixel{nullptr}, *uv_pixel1{nullptr}, *uv_pixel2{nullptr};\nint32_t y_len{0}, uv_len1{0}, uv_len2{0};\nAImage_getPlaneData(image, 0, &y_pixel, &y_len);\nAImage_getPlaneData(image, 1, &uv_pixel1, &uv_len1);\nAImage_getPlaneData(image, 2, &uv_pixel2, &uv_len2);\n```", "```py\ncv::Size actual_size(src_width, src_height);\ncv::Size half_size(src_width / 2, src_height / 2);\ncv::Mat y(actual_size, CV_8UC1, y_pixel, y_stride);\ncv::Mat uv1(half_size, CV_8UC2, uv_pixel1, uv_stride1);\ncv::Mat uv2(half_size, CV_8UC2, uv_pixel2, uv_stride2);\n```", "```py\ncv::mat rgba_img_;\n...\nlong addr_diff = uv2.data - uv1.data;\nif (addr_diff > 0) {\n  cvtColorTwoPlane(y, uv1, rgba_img_, cv::COLOR_YUV2RGBA_NV12);\n} else {\n  cvtColorTwoPlane(y, uv2, rgba_img_, cv::COLOR_YUV2RGBA_NV21);\n}\n```", "```py\ncv::rotate(rgba_img_, rgba_img_, cv::ROTATE_90_CLOCKWISE);\n```", "```py\nauto results = yolo_->detect(rgba_img_);\nfor (auto& result : results) {\n  int thickness = 2;\n  rectangle(rgba_img_, result.rect.tl(), result.rect.br(),\n            cv::Scalar(255, 0, 0, 255), thickness,\n            cv::LINE_4);\n  cv::putText(rgba_ img_, result.class_name,\n              result.rect.tl(), cv::FONT_HERSHEY_DUPLEX,\n              1.0, CV_RGB(0, 255, 0), 2);\n}\n```", "```py\nstruct YOLOResult {\n  int class_index;\n  std::string class_name;\n  float score;\n  cv::Rect rect;\n};\n```", "```py\ncv::Mat buffer_mat(src_width,\n                   src_height,\n                   CV_8UC4,\n                   buf->bits,\n                   buf->stride * 4);\nrgba_img_.copyTo(buffer_mat);\n```", "```py\nYOLO::YOLO(AAssetManager* asset_manager) {\n  const std::string model_file_name = \"yolov5s.torchscript\";\n  auto model_buf = read_asset(asset_manager,\n                           model_file_name);\n  model_ = torch::jit::_load_for_mobile(\n      std::make_unique<ReadAdapter>(model_buf));\n  const std::string classes_file_name = \"classes.txt\";\n  auto classes_buf = read_asset( asset_manager,\n                             classes_file_name);\n  VectorStreamBuf<char> stream_buf(classes_buf);\n  std::istream is(&stream_buf);\n  load_classes(is);\n}\n```", "```py\nstd::vector<char> read_asset(AAssetManager* asset_manager,\n                             const std::string& name) {\n  std::vector<char> buf;\n  AAsset* asset = AAssetManager_open(\n      asset_manager, name.c_str(), AASSET_MODE_UNKNOWN);\n  if (asset != nullptr) {\n    LOGI(\"Open asset %s OK\", name.c_str());\n    off_t buf_size = AAsset_getLength(asset);\n    buf.resize(buf_size + 1, 0);\n    auto num_read =AAsset_read(\n                      asset, buf.data(), buf_size);\n    LOGI(\"Read asset %s OK\", name.c_str());\n    if (num_read == 0)\n      buf.clear();\n    AAsset_close(asset);\n    LOGI(\"Close asset %s OK\", name.c_str());\n  }\n  return buf;\n}\n```", "```py\nclass ReadAdapter\n    : public caffe2::serialize::ReadAdapterInterface {\n public:\n  explicit ReadAdapter(const std::vector<char>& buf)\n      : buf_(&buf) {}\n  size_t size() const override { return buf_->size(); }\n  size_t read(uint64_t pos,\n              void* buf,\n              size_t n,\n              const char* what) const override {\n    std::copy_n(buf_->begin() + pos, n,\n                reinterpret_cast<char*>(buf));\n    return n;\n  }\n private:\n  const std::vector<char>* buf_;\n};\n```", "```py\ntemplate<typename CharT, typename TraitsT = std::char_traits<CharT> >\nstruct VectorStreamBuf : public std::basic_streambuf<CharT, TraitsT> {\n  explicit VectorStreamBuf(std::vector<CharT>& vec) {\n    this->setg(vec.data(), vec.data(),\n               vec.data() + vec.size());\n  }\n}\n```", "```py\nvoid YOLO::load_classes(std::istream& stream) {\n  LOGI(\"Init classes start OK\");\n  classes_.clear();\n  if (stream) {\n    std::string line;\n    std::string id;\n    std::string label;\n    size_t idx = 0;\n    while (std::getline(stream, line)) {\n      auto pos = line.find_first_of(':');\n      id = line.substr(0, pos);\n      label = line.substr(pos + 1);\n      classes_.insert({idx, label});\n      ++idx;\n    }\n  }\n  LOGI(\"Init classes finish OK\");\n}\n```", "```py\n[ID] space character [class name]\n```", "```py\nstd::vector<YOLOResult> YOLO::detect(const cv::Mat& image) {\n  constexpr int input_width = 640;\n  constexpr int input_height = 640;\n  cv::cvtColor(image, rgb_img_, cv::COLOR_RGBA2RGB);\n  cv::resize(rgb_img_, rgb_img_,\n             cv::Size(input_width, input_height));\n  auto img_scale_x =\n      static_cast<float>(image.cols) / input_width;\n  auto img_scale_y =\n      static_cast<float>(image.rows) / input_height;\n  auto input_tensor = mat2tensor(rgb_img_);\n  std::vector<torch::jit::IValue> inputs;\n  inputs.emplace_back(input_tensor);\n  auto output = model_.forward(inputs).toTuple() - >\n                elements()[0].toTensor().squeeze(0);\n  output2results(output, img_scale_x, img_scale_y);\n  return non_max_suppression();\n}\n```", "```py\ntorch::Tensor mat2tensor(const cv::Mat& image) {\n  ASSERT(image.channels() == 3, \"Invalid image format\");\n  torch::Tensor tensor_image = torch::from_blob(\n      image.data,\n      {1, image.rows, image.cols, image.channels()},\n      at::kByte);\n  tensor_image = tensor_image.to(at::kFloat) / 255.;\n  tensor_image = torch::transpose(tensor_image, 1, 2);\n  tensor_image = torch::transpose(tensor_image, 1, 3);\n  return tensor_image;\n}\n```", "```py\nvoid YOLO::output2results(const torch::Tensor &output,\n                          float img_scale_x,\n                          float img_scale_y) {\n  auto outputs = output.accessor<float, 2>();\n  auto output_row = output.size(0);\n  auto output_column = output.size(1);\n  results_.clear();\n  for (int64_t i = 0; i < output_row; i++) {\n    auto score = outputs[i][4];\n    if (score > threshold) {\n      // read the bounding box\n      // calculate the class id\n      results_.push_back(YOLOResult{\n          .class_index = cls,\n          .class_name = classes_[cls],\n          .score = score,\n          .rect = cv::Rect(left, top, bw, bh),\n      });\n    }\n  }\n```", "```py\nfloat cx = outputs[i][0];\nfloat cy = outputs[i][1];\nfloat w = outputs[i][2];\nfloat h = outputs[i][3];\nint left = static_cast<int>(img_scale_x * (cx - w / 2));\nint top = static_cast<int>(img_scale_y * (cy - h / 2));\nint bw = static_cast<int>(img_scale_x * w);\nint bh = static_cast<int>(img_scale_y * h);\n```", "```py\nfloat max = outputs[i][5];\nint cls = 0;\nfor (int64_t j = 0; j < output_column - 5; j++) {\nif (outputs[i][5 + j] > max) {\n  max = outputs[i][5 + j];\n  cls = static_cast<int>(j);\n}\n}\n```", "```py\nstd::vector<YOLOResult> YOLO::non_max_suppression() {\n  // do an sort on the confidence scores, from high to low.\n    std::sort(results_.begin(), results_.end(), [](\n          auto &r1, auto &r2) {\n        return r1.score > r2.score;\n    });\n    std::vector<YOLOResult> selected;\n    std::vector<bool> active(results_.size(), true);\n    int num_active = static_cast<int>(active.size());\n    bool done = false;\n    for (size_t i = 0; i < results_.size() && !done; i++) {\n  if (active[i]) {\n    const auto& box_a = results_[i];\n    selected.push_back(box_a);\n    if (selected.size() >= nms_limit)\n      break;\n    for (size_t j = i + 1; j < results_.size(); j++) {\n      if (active[j]) {\n        const auto& box_b = results_[j];\n        if (IOU(box_a.rect, box_b.rect) > threshold) {\n          active[j] = false;\n          num_active -= 1;\n          if (num_active <= 0) {\n            done = true;\n            break;\n          }\n        }\n      }\n    }\n  }\n}\n  return selected;\n}\n```", "```py\nfloat IOU(const cv::Rect& a, const cv::Rect& b) {\n  if (a.empty() <= 0.0)\n    return 0.0f;\n  if (b.empty() <= 0.0)\n    return 0.0f;\n  auto min_x = std::max(a.x, b.x);\n  auto min_y = std::max(a.y, b.y);\n  auto max_x = std::min(a.x + a.width, b.x + b.width);\n  auto max_y = std::min(a.y + a.height, b.y + b.height);\n  auto area = std::max(max_y - min_y, 0) *\n              std::max(max_x - min_x, 0);\n  return static_cast<float>(area) /\n         static_cast<float>(a.area() + b.area() - area);\n}\n```"]