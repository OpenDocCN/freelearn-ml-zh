["```py\n[akozlov@Alexanders-MacBook-Pro ~]$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala>\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ scala\n…\nscala> import scala.sys.process._\nimport scala.sys.process._\nscala> val histogram = ( \"gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz\"  #|  \"cut -f 10\" #| \"sort\" #|  \"uniq -c\" #| \"sort -k1nr\" ).lineStream\nhistogram: Stream[String] = Stream(7731 http://www.mycompany.com/us/en_us/, ?)\nscala> histogram take(10) foreach println \n7731 http://www.mycompany.com/us/en_us/\n3843 http://mycompanyplus.mycompany.com/plus/\n2734 http://store.mycompany.com/us/en_us/?l=shop,men_shoes\n2400 http://m.mycompany.com/us/en_us/\n1750 http://store.mycompany.com/us/en_us/?l=shop,men_mycompanyid\n1556 http://www.mycompany.com/us/en_us/c/mycompanyid?sitesrc=id_redir\n1530 http://store.mycompany.com/us/en_us/\n1393 http://www.mycompany.com/us/en_us/?cp=USNS_KW_0611081618\n1379 http://m.mycompany.com/us/en_us/?ref=http%3A%2F%2Fwww.mycompany.com%2F\n1230 http://www.mycompany.com/us/en_us/c/running\n\n```", "```py\nscala> import scala.sys.process._\nimport scala.sys.process._\nscala> val nums = ( \"gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz\"  #|  \"cut -f 6\" ).lineStream\nnums: Stream[String] = Stream(0, ?) \nscala> val m = nums.map(_.toDouble).min\nm: Double = 0.0\nscala> val m = nums.map(_.toDouble).sum/nums.size\nm: Double = 3.6883642764024662\nscala> val m = nums.map(_.toDouble).max\nm: Double = 33.0\n\n```", "```py\nimport scala.util.Random\nimport util.Properties\n\nval threshold = 0.05\n\nval lines = scala.io.Source.fromFile(\"chapter01/data/iris/in.txt\").getLines\nval newLines = lines.filter(_ =>\n    Random.nextDouble() <= threshold\n)\n\nval w = new java.io.FileWriter(new java.io.File(\"out.txt\"))\nnewLines.foreach { s =>\n    w.write(s + Properties.lineSeparator)\n}\nw.close\n```", "```py\nimport scala.reflect.ClassTag\nimport scala.util.Random\nimport util.Properties\n\ndef reservoirSample[T: ClassTag](input: Iterator[T],k: Int): Array[T] = {\n  val reservoir = new Array[T](k)\n  // Put the first k elements in the reservoir.\n  var i = 0\n  while (i < k && input.hasNext) {\n    val item = input.next()\n    reservoir(i) = item\n    i += 1\n  }\n\n  if (i < k) {\n    // If input size < k, trim the array size\n    reservoir.take(i)\n  } else {\n    // If input size > k, continue the sampling process.\n    while (input.hasNext) {\n      val item = input.next\n      val replacementIndex = Random.nextInt(i)\n      if (replacementIndex < k) {\n        reservoir(replacementIndex) = item\n      }\n      i += 1\n    }\n    reservoir\n  }\n}\n\nval numLines=15\nval w = new java.io.FileWriter(new java.io.File(\"out.txt\"))\nval lines = io.Source.fromFile(\"chapter01/data/iris/in.txt\").getLines\nreservoirSample(lines, numLines).foreach { s =>\n    w.write(s + scala.util.Properties.lineSeparator)\n}\nw.close\n```", "```py\nval origLinesRdd = sc.textFile(\"file://...\")\nval keyedRdd = origLines.keyBy(r => r.split(\",\")(0))\nval fractions = keyedRdd.countByKey.keys.map(r => (r, 0.1)).toMap\nval sampledWithKey = keyedRdd.sampleByKeyExact(fractions)\nval sampled = sampledWithKey.map(_._2).collect\n```", "```py\nimport scala.util.hashing.MurmurHash3._\n\nval markLow = 0\nval markHigh = 4096\nval seed = 12345\n\ndef consistentFilter(s: String): Boolean = {\n  val hash = stringHash(s.split(\" \")(0), seed) >>> 16\n  hash >= markLow && hash < markHigh\n}\n\nval w = new java.io.FileWriter(new java.io.File(\"out.txt\"))\nval lines = io.Source.fromFile(\"chapter01/data/iris/in.txt\").getLines\nlines.filter(consistentFilter).foreach { s =>\n     w.write(s + Properties.lineSeparator)\n}\nw.close\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ wget http://s3.eu-central-1.amazonaws.com/spark-notebook/zip/spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip\n...\n[akozlov@Alexanders-MacBook-Pro]$ unzip -d ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet.zip\n...\n[akozlov@Alexanders-MacBook-Pro]$ ln -sf ~/ spark-notebook-0.6.3-scala-2.11.7-spark-1.6.1-hadoop-2.6.4-with-hive-with-parquet ~/spark-notebook\n[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/notebook/Chapter01.snb ~/spark-notebook/notebooks\n[akozlov@Alexanders-MacBook-Pro]$ cp chapter01/ data/kddcup/kddcup.parquet ~/spark-notebook\n[akozlov@Alexanders-MacBook-Pro]$ cd ~/spark-notebook\n[akozlov@Alexanders-MacBook-Pro]$ bin/spark-notebook \nPlay server process ID is 2703\n16/04/14 10:43:35 INFO play: Application started (Prod)\n16/04/14 10:43:35 INFO play: Listening for HTTP on /0:0:0:0:0:0:0:0:9000\n...\n\n```", "```py\nval labelCount = df.groupBy(\"lbl\").count().collect\nlabelCount.toList.map(row => (row.getString(0), row.getLong(1)))\n```", "```py\nsqlContext.sql(\"SELECT lbl, protocol_type, min(duration), avg(duration), stddev(duration), max(duration) FROM parquet.`kddcup.parquet` group by lbl, protocol_type\")\n```", "```py\nval pct = sqlContext.sql(\"SELECT duration FROM parquet.`kddcup.parquet` where protocol_type = 'udp'\").rdd.map(_.getLong(0)).cache\npct.top((0.05*pct.count).toInt).last\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ pbcopy < chapter01/data/iris/in.txt\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.27 \n\nscala> def logFactorial(n: Int) = { (1 to n).map(Math.log(_)).sum }\nlogFactorial: (n: Int)Double\n\nscala> def cmnp(m: Int, n: Int, p: Double) = {\n |   Math.exp(logFactorial(n) -\n |   logFactorial(m) +\n |   m*Math.log(p) -\n |   logFactorial(n-m) +\n |   (n-m)*Math.log(1-p))\n | }\ncmnp: (m: Int, n: Int, p: Double)Double\n\nscala> val p = 0.105\np: Double = 0.105\n\nscala> val n = 60\nn: Int = 60\n\nscala> var cumulative = 0.0\ncumulative: Double = 0.0\n\nscala> for(i <- 0 to 14) {\n |   val prob = cmnp(i,n,p)\n |   cumulative += prob\n |   println(f\"We expect $i wins with $prob%.6f probability $cumulative%.3f cumulative (n = $n, p = $p).\")\n | }\nWe expect 0 wins with 0.001286 probability 0.001 cumulative (n = 60, p = 0.105).\nWe expect 1 wins with 0.009055 probability 0.010 cumulative (n = 60, p = 0.105).\nWe expect 2 wins with 0.031339 probability 0.042 cumulative (n = 60, p = 0.105).\nWe expect 3 wins with 0.071082 probability 0.113 cumulative (n = 60, p = 0.105).\nWe expect 4 wins with 0.118834 probability 0.232 cumulative (n = 60, p = 0.105).\nWe expect 5 wins with 0.156144 probability 0.388 cumulative (n = 60, p = 0.105).\nWe expect 6 wins with 0.167921 probability 0.556 cumulative (n = 60, p = 0.105).\nWe expect 7 wins with 0.151973 probability 0.708 cumulative (n = 60, p = 0.105).\nWe expect 8 wins with 0.118119 probability 0.826 cumulative (n = 60, p = 0.105).\nWe expect 9 wins with 0.080065 probability 0.906 cumulative (n = 60, p = 0.105).\nWe expect 10 wins with 0.047905 probability 0.954 cumulative (n = 60, p = 0.105).\nWe expect 11 wins with 0.025546 probability 0.979 cumulative (n = 60, p = 0.105).\nWe expect 12 wins with 0.012238 probability 0.992 cumulative (n = 60, p = 0.105).\nWe expect 13 wins with 0.005301 probability 0.997 cumulative (n = 60, p = 0.105).\nWe expect 14 wins with 0.002088 probability 0.999 cumulative (n = 60, p = 0.105).\n\n```", "```py\n$ git clone https://github.com/apache/spark.git\nCloning into 'spark'...\nremote: Counting objects: 301864, done.\n...\n$ cd spark\n$sh ./ dev/change-scala-version.sh 2.11\n...\n$./make-distribution.sh --name alex-build-2.6-yarn --skip-java-test --tgz -Pyarn -Phive -Phive-thriftserver -Pscala-2.11 -Phadoop-2.6\n...\n\n```", "```py\n$ bin/spark-shell --master spark://<master-address>:7077\n\n```", "```py\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = localhost\na1.sources.r1.port = 4987\n\n# Describe the sink (the instructions to configure and start HDFS are provided in the Appendix)\na1.sinks.k1.type=hdfs\na1.sinks.k1.hdfs.path=hdfs://localhost:8020/flume/netcat/data\na1.sinks.k1.hdfs.filePrefix=chapter03.example\na1.sinks.k1.channel=c1\na1.sinks.k1.hdfs.writeFormat = Text\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```", "```py\n$ wget http://mirrors.ocf.berkeley.edu/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz\n$ md5sum apache-flume-1.6.0-bin.tar.gz\nMD5 (apache-flume-1.6.0-bin.tar.gz) = defd21ad8d2b6f28cc0a16b96f652099\n$ tar xf apache-flume-1.6.0-bin.tar.gz\n$ cd apache-flume-1.6.0-bin\n$ ./bin/flume-ng agent -Dlog.dir=. -Dflume.log.level=DEBUG,console -n a1 -f ../chapter03/conf/flume.conf\nInfo: Including Hadoop libraries found via (/Users/akozlov/hadoop-2.6.4/bin/hadoop) for HDFS access\nInfo: Excluding /Users/akozlov/hadoop-2.6.4/share/hadoop/common/lib/slf4j-api-1.7.5.jar from classpath\nInfo: Excluding /Users/akozlov/hadoop-2.6.4/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar from classpath\n...\n\n```", "```py\n$ nc localhost 4987\nHello\nOK\nWorld\nOK\n\n...\n\n```", "```py\n$ bin/hdfs dfs -text /flume/netcat/data/chapter03.example.1463052301372\n16/05/12 04:27:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n1463052302380  Hello\n1463052304307  World\n\n```", "```py\nval lines = scala.io.Source.fromFile(\"...\").getLines.toSeq\nval counts = lines.flatMap(line => line.split(\"\\\\W+\")).sorted.\n  foldLeft(List[(String,Int)]()){ (r,c) =>\n    r match {\n      case (key, count) :: tail =>\n        if (key == c) (c, count+1) :: tail\n        else (c, 1) :: r\n        case Nil =>\n          List((c, 1))\n  }\n}\n```", "```py\nval linesRdd = sc.textFile(\"hdfs://...\")\nval counts = linesRdd.flatMap(line => line.split(\"\\\\W+\"))\n    .map(_.toLowerCase)\n    .map(word => (word, 1)).\n    .reduceByKey(_+_)\ncounts.collect\n```", "```py\n$ wget http://mirrors.sonic.net/apache/spark/spark-1.6.1/spark-1.6.1-bin-hadoop2.6.tgz\n$ tar xvf spark-1.6.1-bin-hadoop2.6.tgz\n$ cd spark-1.6.1-bin-hadoop2.6\n$ mkdir leotolstoy\n$ (cd leotolstoy; wget http://www.gutenberg.org/files/1399/1399-0.txt)\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\nscala> val linesRdd = sc.textFile(\"leotolstoy\", minPartitions=10)\nlinesRdd: org.apache.spark.rdd.RDD[String] = leotolstoy MapPartitionsRDD[3] at textFile at <console>:27\n\n```", "```py\nscala> val countsRdd = linesRdd.flatMap(line => line.split(\"\\\\W+\")).\n | map(_.toLowerCase).\n | map(word => (word, 1)).\n | reduceByKey(_+_)\ncountsRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[5] at reduceByKey at <console>:31\n\n```", "```py\nscala> countsRdd.collect.filter(_._2 > 99)\nres3: Array[(String, Int)] = Array((been,1061), (them,841), (found,141), (my,794), (often,105), (table,185), (this,1410), (here,364), (asked,320), (standing,132), (\"\",13514), (we,592), (myself,140), (is,1454), (carriage,181), (got,277), (won,153), (girl,117), (she,4403), (moment,201), (down,467), (me,1134), (even,355), (come,667), (new,319), (now,872), (upon,207), (sister,115), (veslovsky,110), (letter,125), (women,134), (between,138), (will,461), (almost,124), (thinking,159), (have,1277), (answer,146), (better,231), (men,199), (after,501), (only,654), (suddenly,173), (since,124), (own,359), (best,101), (their,703), (get,304), (end,110), (most,249), (but,3167), (was,5309), (do,846), (keep,107), (having,153), (betsy,111), (had,3857), (before,508), (saw,421), (once,334), (side,163), (ough...\n\n```", "```py\n# The sink is Spark\na1.sinks.k1.type=org.apache.spark.streaming.flume.sink.SparkSink\na1.sinks.k1.hostname=localhost\na1.sinks.k1.port=4989\n\n```", "```py\nobject FlumeWordCount {\n  def main(args: Array[String]) {\n    // Create the context with a 2 second batch size\n    val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"FlumeWordCount\")\n    val ssc = new StreamingContext(sparkConf, Seconds(2))\n    ssc.checkpoint(\"/tmp/flume_check\")\n    val hostPort=args(0).split(\":\")\n    System.out.println(\"Opening a sink at host: [\" + hostPort(0) + \"] port: [\" + hostPort(1).toInt + \"]\")\n    val lines = FlumeUtils.createPollingStream(ssc, hostPort(0), hostPort(1).toInt, StorageLevel.MEMORY_ONLY)\n    val words = lines\n      .map(e => new String(e.event.getBody.array)).map(_.toLowerCase).flatMap(_.split(\"\\\\W+\"))\n      .map(word => (word, 1L))\n      .reduceByKeyAndWindow(_+_, _-_, Seconds(6), Seconds(2)).print\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```", "```py\n$ ./bin/flume-ng agent -Dflume.log.level=DEBUG,console -n a1 –f ../chapter03/conf/flume-spark.conf\n...\n\n```", "```py\n$ cd ../chapter03\n$ sbt \"run-main org.akozlov.chapter03.FlumeWordCount localhost:4989\n...\n\n```", "```py\n$ echo \"Happy families are all alike; every unhappy family is unhappy in its own way\" | nc localhost 4987\n...\n-------------------------------------------\nTime: 1464161488000 ms\n-------------------------------------------\n(are,1)\n(is,1)\n(its,1)\n(family,1)\n(families,1)\n(alike,1)\n(own,1)\n(happy,1)\n(unhappy,2)\n(every,1)\n...\n\n-------------------------------------------\nTime: 1464161490000 ms\n-------------------------------------------\n(are,1)\n(is,1)\n(its,1)\n(family,1)\n(families,1)\n(alike,1)\n(own,1)\n(happy,1)\n(unhappy,2)\n(every,1)\n...\n\n```", "```py\nobject KafkaWordCount {\n  def main(args: Array[String]) {\n    // Create the context with a 2 second batch size\n    val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"KafkaWordCount\")\n    val ssc = new StreamingContext(sparkConf, Seconds(2))\n    ssc.checkpoint(\"/tmp/kafka_check\")\n    System.out.println(\"Opening a Kafka consumer at zk:[\" + args(0) + \"] for group group-1 and topic example\")\n    val lines = KafkaUtils.createStream(ssc, args(0), \"group-1\", Map(\"example\" -> 1), StorageLevel.MEMORY_ONLY)\n    val words = lines\n      .flatMap(_._2.toLowerCase.split(\"\\\\W+\"))\n      .map(word => (word, 1L))\n      .reduceByKeyAndWindow(_+_, _-_, Seconds(6), Seconds(2)).print\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n```", "```py\n$ wget http://apache.cs.utah.edu/kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz\n...\n$ tar xf kafka_2.11-0.9.0.1.tgz\n$ bin/zookeeper-server-start.sh config/zookeeper.properties\n...\n\n```", "```py\n$ bin/kafka-server-start.sh config/server.properties\n...\n\n```", "```py\n$ sbt \"run-main org.akozlov.chapter03.KafkaWordCount localhost:2181\"\n...\n\n```", "```py\n$ echo \"Happy families are all alike; every unhappy family is unhappy in its own way\" | ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic example\n...\n\n$ sbt \"run-main org.akozlov.chapter03.FlumeWordCount localhost:4989\n...\n-------------------------------------------\nTime: 1464162712000 ms\n-------------------------------------------\n(are,1)\n(is,1)\n(its,1)\n(family,1)\n(families,1)\n(alike,1)\n(own,1)\n(happy,1)\n(unhappy,2)\n(every,1)\n\n```", "```py\n$ ./bin/spark-sql\n…\nspark-sql> select min(duration), max(duration), avg(duration) from kddcup;\n…\n0  58329  48.34243046395876\nTime taken: 11.073 seconds, Fetched 1 row(s)\n\n```", "```py\n$ ./bin/spark-shell\n…\nscala> val df = sqlContext.sql(\"select min(duration), max(duration), avg(duration) from kddcup\"\n16/05/12 13:35:34 INFO parse.ParseDriver: Parsing command: select min(duration), max(duration), avg(duration) from alex.kddcup_parquet\n16/05/12 13:35:34 INFO parse.ParseDriver: Parse Completed\ndf: org.apache.spark.sql.DataFrame = [_c0: bigint, _c1: bigint, _c2: double]\nscala> df.collect.foreach(println)\n…\n16/05/12 13:36:32 INFO scheduler.DAGScheduler: Job 2 finished: collect at <console>:22, took 4.593210 s\n[0,58329,48.34243046395876]\n\n```", "```py\n    $ wget ftp://apache.cs.utah.edu/apache.org/hadoop/common/h/hadoop-2.6.4.tar.gz\n    --2016-05-12 00:10:55--  ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz\n     => 'hadoop-2.6.4.tar.gz.1'\n    Resolving apache.cs.utah.edu... 155.98.64.87\n    Connecting to apache.cs.utah.edu|155.98.64.87|:21... connected.\n    Logging in as anonymous ... Logged in!\n    ==> SYST ... done.    ==> PWD ... done.\n    ==> TYPE I ... done.  ==> CWD (1) /apache.org/hadoop/common/hadoop-2.6.4 ... done.\n    ==> SIZE hadoop-2.6.4.tar.gz ... 196015975\n    ==> PASV ... done.    ==> RETR hadoop-2.6.4.tar.gz ... done.\n    ...\n    $ wget ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz.mds\n    --2016-05-12 00:13:58--  ftp://apache.cs.utah.edu/apache.org/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz.mds\n     => 'hadoop-2.6.4.tar.gz.mds'\n    Resolving apache.cs.utah.edu... 155.98.64.87\n    Connecting to apache.cs.utah.edu|155.98.64.87|:21... connected.\n    Logging in as anonymous ... Logged in!\n    ==> SYST ... done.    ==> PWD ... done.\n    ==> TYPE I ... done.  ==> CWD (1) /apache.org/hadoop/common/hadoop-2.6.4 ... done.\n    ==> SIZE hadoop-2.6.4.tar.gz.mds ... 958\n    ==> PASV ... done.    ==> RETR hadoop-2.6.4.tar.gz.mds ... done.\n    ...\n    $ shasum -a 512 hadoop-2.6.4.tar.gz\n    493cc1a3e8ed0f7edee506d99bfabbe2aa71a4776e4bff5b852c6279b4c828a0505d4ee5b63a0de0dcfecf70b4bb0ef801c767a068eaeac938b8c58d8f21beec  hadoop-2.6.4.tar.gz\n    $ cat !$.mds\n    hadoop-2.6.4.tar.gz:    MD5 = 37 01 9F 13 D7 DC D8 19  72 7B E1 58 44 0B 94 42\n    hadoop-2.6.4.tar.gz:   SHA1 = 1E02 FAAC 94F3 35DF A826  73AC BA3E 7498 751A 3174\n    hadoop-2.6.4.tar.gz: RMD160 = 2AA5 63AF 7E40 5DCD 9D6C  D00E EBB0 750B D401 2B1F\n    hadoop-2.6.4.tar.gz: SHA224 = F4FDFF12 5C8E754B DAF5BCFC 6735FCD2 C6064D58\n     36CB9D80 2C12FC4D\n    hadoop-2.6.4.tar.gz: SHA256 = C58F08D2 E0B13035 F86F8B0B 8B65765A B9F47913\n     81F74D02 C48F8D9C EF5E7D8E\n    hadoop-2.6.4.tar.gz: SHA384 = 87539A46 B696C98E 5C7E352E 997B0AF8 0602D239\n     5591BF07 F3926E78 2D2EF790 BCBB6B3C EAF5B3CF\n     ADA7B6D1 35D4B952\n    hadoop-2.6.4.tar.gz: SHA512 = 493CC1A3 E8ED0F7E DEE506D9 9BFABBE2 AA71A477\n     6E4BFF5B 852C6279 B4C828A0 505D4EE5 B63A0DE0\n     DCFECF70 B4BB0EF8 01C767A0 68EAEAC9 38B8C58D\n     8F21BEEC\n\n    $ tar xf hadoop-2.6.4.tar.gz\n    $ cd hadoop-2.6.4\n\n    ```", "```py\n    $ cat << EOF > etc/hadoop/core-site.xml\n    <configuration>\n     <property>\n     <name>fs.defaultFS</name>\n     <value>hdfs://localhost:8020</value>\n     </property>\n    </configuration>\n    EOF\n    $ cat << EOF > etc/hadoop/hdfs-site.xml\n    <configuration>\n     <property>\n     <name>dfs.replication</name>\n     <value>1</value>\n     </property>\n    </configuration>\n    EOF\n\n    ```", "```py\n    $ bin/hdfs namenode -format\n    16/05/12 00:55:40 INFO namenode.NameNode: STARTUP_MSG: \n    /************************************************************\n    STARTUP_MSG: Starting NameNode\n    STARTUP_MSG:   host = alexanders-macbook-pro.local/192.168.1.68\n    STARTUP_MSG:   args = [-format]\n    STARTUP_MSG:   version = 2.6.4\n    STARTUP_MSG:   classpath =\n    ...\n\n    ```", "```py\n    $ bin/hdfs namenode &\n    ...\n    $ bin/hdfs secondarynamenode &\n    ...\n    $ bin/hdfs datanode &\n    ...\n\n    ```", "```py\n    $ date | bin/hdfs dfs –put – date.txt\n    ...\n    $ bin/hdfs dfs –ls\n    Found 1 items\n    -rw-r--r-- 1 akozlov supergroup 29 2016-05-12 01:02 date.txt\n    $ bin/hdfs dfs -text date.txt\n    Thu May 12 01:02:36 PDT 2016\n\n    ```", "```py\n    $ cat /tmp/hadoop-akozlov/dfs/data/current/BP-1133284427-192.168.1.68-1463039756191/current/finalized/subdir0/subdir0/blk_1073741827\n    Thu May 12 01:02:36 PDT 2016\n\n    ```", "```py\nawk -F, '/setosa/ {print \"0 1:\"$1\" 2:\"$2\" 3:\"$3\" 4:\"$4;}; /versicolor/ {print \"1 1:\"$1\" 2:\"$2\" 3:\"$3\" 4:\"$4;}; /virginica/ {print \"1 1:\"$1\" 2:\"$2\" 3:\"$3\" 4:\"$4;};' iris.csv > iris-libsvm.txt\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nscala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\nscala> val splits = data.randomSplit(Array(0.6, 0.4), seed = 123L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:26, MapPartitionsRDD[8] at randomSplit at <console>:26)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:26\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:26\nscala> val numIterations = 100\nnumIterations: Int = 100\nscala> val model = SVMWithSGD.train(training, numIterations)\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = 0.0\nscala> model.clearThreshold()\nres0: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 4, numClasses = 2, threshold = None\nscala> val scoreAndLabels = test.map { point =>\n |   val score = model.predict(point.features)\n |   (score, point.label)\n | }\nscoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[212] at map at <console>:36\nscala> val metrics = new BinaryClassificationMetrics(scoreAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@692e4a35\nscala> val auROC = metrics.areaUnderROC()\nauROC: Double = 1.0\n\nscala> println(\"Area under ROC = \" + auROC)\nArea under ROC = 1.0\nscala> model.save(sc, \"model\")\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\n```", "```py\nscala> model.intercept\nres5: Double = 0.0\n\nscala> model.weights\nres6: org.apache.spark.mllib.linalg.Vector = [-0.2469448809675877,-1.0692729424287566,1.7500423423258127,0.8105712661836376]\n\n```", "```py\n$ parquet-tools dump model/data/part-r-00000-7a86b825-569d-4c80-8796-8ee6972fd3b1.gz.parquet\n…\nDOUBLE weights.values.array \n----------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:3 V:-0.2469448809675877\nvalue 2: R:1 D:3 V:-1.0692729424287566\nvalue 3: R:1 D:3 V:1.7500423423258127\nvalue 4: R:1 D:3 V:0.8105712661836376\n\nDOUBLE intercept \n----------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 1 *** \nvalue 1: R:0 D:1 V:0.0\n…\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext\nscala> import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\nimport org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\nscala> import org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nscala> import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LabeledPoint\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm-3.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\nscala> val splits = data.randomSplit(Array(0.6, 0.4))\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:29, MapPartitionsRDD[8] at randomSplit at <console>:29)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:29\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:29\nscala> val model = new LogisticRegressionWithLBFGS().setNumClasses(3).run(training)\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 8, numClasses = 3, threshold = 0.5\nscala> val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n |   val prediction = model.predict(features)\n |   (prediction, label)\n | }\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[67] at map at <console>:37\nscala> val metrics = new MulticlassMetrics(predictionAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@6d5254f3\nscala> val precision = metrics.precision\nprecision: Double = 0.9516129032258065\nscala> println(\"Precision = \" + precision)\nPrecision = 0.9516129032258065\nscala> model.intercept\nres5: Double = 0.0\nscala> model.weights\nres7: org.apache.spark.mllib.linalg.Vector = [10.644978886788556,-26.850171485157578,3.852594349297618,8.74629386938248,4.288703063075211,-31.029289381858273,9.790312529377474,22.058196856491996]\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.DecisionTree\nscala> import org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> import org.apache.spark.mllib.tree.configuration.Strategy\nimport org.apache.spark.mllib.tree.configuration.Strategy\nscala> import org.apache.spark.mllib.tree.configuration.Algo.Classification\nimport org.apache.spark.mllib.tree.configuration.Algo.Classification\nscala> import org.apache.spark.mllib.tree.impurity.{Entropy, Gini}\nimport org.apache.spark.mllib.tree.impurity.{Entropy, Gini}\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm-3.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\n\nscala> val splits = data.randomSplit(Array(0.7, 0.3), 11L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:30, MapPartitionsRDD[8] at randomSplit at <console>:30)\nscala> val (trainingData, testData) = (splits(0), splits(1))\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:30\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:30\nscala> val strategy = new Strategy(Classification, Gini, 10, 3, 10)\nstrategy: org.apache.spark.mllib.tree.configuration.Strategy = org.apache.spark.mllib.tree.configuration.Strategy@4110e631\nscala> val dt = new DecisionTree(strategy)\ndt: org.apache.spark.mllib.tree.DecisionTree = org.apache.spark.mllib.tree.DecisionTree@33d89052\nscala> val model = dt.run(trainingData)\nmodel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 6 with 21 nodes\nscala> val labelAndPreds = testData.map { point =>\n |   val prediction = model.predict(point.features)\n |   (point.label, prediction)\n | }\nlabelAndPreds: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[32] at map at <console>:36\nscala> val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\ntestErr: Double = 0.02631578947368421\nscala> println(\"Test Error = \" + testErr)\nTest Error = 0.02631578947368421\n\nscala> println(\"Learned classification tree model:\\n\" + model.toDebugString)\nLearned classification tree model:\nDecisionTreeModel classifier of depth 6 with 21 nodes\n If (feature 3 <= 0.4)\n Predict: 0.0\n Else (feature 3 > 0.4)\n If (feature 3 <= 1.7)\n If (feature 2 <= 4.9)\n If (feature 0 <= 5.3)\n If (feature 1 <= 2.8)\n If (feature 2 <= 3.9)\n Predict: 1.0\n Else (feature 2 > 3.9)\n Predict: 2.0\n Else (feature 1 > 2.8)\n Predict: 0.0\n Else (feature 0 > 5.3)\n Predict: 1.0\n Else (feature 2 > 4.9)\n If (feature 0 <= 6.0)\n If (feature 1 <= 2.4)\n Predict: 2.0\n Else (feature 1 > 2.4)\n Predict: 1.0\n Else (feature 0 > 6.0)\n Predict: 2.0\n Else (feature 3 > 1.7)\n If (feature 2 <= 4.9)\n If (feature 1 <= 3.0)\n Predict: 2.0\n Else (feature 1 > 3.0)\n Predict: 1.0\n Else (feature 2 > 4.9)\n Predict: 2.0\nscala> model.save(sc, \"dt-model\")\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\n```", "```py\n$ bin/spark-shell\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nscala> val iris = sc.textFile(\"iris.txt\")\niris: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at textFile at <console>:23\n\nscala> val vectors = data.map(s => Vectors.dense(s.split('\\t').map(_.toDouble))).cache()\nvectors: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[5] at map at <console>:25\n\nscala> val numClusters = 3\nnumClusters: Int = 3\nscala> val numIterations = 20\nnumIterations: Int = 20\nscala> val clusters = KMeans.train(vectors, numClusters, numIterations)\nclusters: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@5dc9cb99\nscala> val centers = clusters.clusterCenters\ncenters: Array[org.apache.spark.mllib.linalg.Vector] = Array([5.005999999999999,3.4180000000000006,1.4640000000000002,0.2439999999999999], [6.8538461538461535,3.076923076923076,5.715384615384614,2.0538461538461537], [5.883606557377049,2.740983606557377,4.388524590163936,1.4344262295081966])\nscala> val SSE = clusters.computeCost(vectors)\nWSSSE: Double = 78.94506582597859\nscala> vectors.collect.map(x => clusters.predict(x))\nres18: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2)\nscala> println(\"Sum of Squared Errors = \" + SSE)\nSum of Squared Errors = 78.94506582597859\nscala> clusters.save(sc, \"model\")\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n\n```", "```py\nscala> 1.to(10).foreach(i => println(\"i: \" + i + \" SSE: \" + KMeans.train(vectors, i, numIterations).computeCost(vectors)))\ni: 1 WSSSE: 680.8244\ni: 2 WSSSE: 152.3687064773393\ni: 3 WSSSE: 78.94506582597859\ni: 4 WSSSE: 57.47327326549501\ni: 5 WSSSE: 46.53558205128235\ni: 6 WSSSE: 38.9647878510374\ni: 7 WSSSE: 34.311167589868646\ni: 8 WSSSE: 32.607859500805034\ni: 9 WSSSE: 28.231729411088438\ni: 10 WSSSE: 29.435054384424078\n\n```", "```py\nscala> for (i <- 1.to(10)) println(i + \" -> \" + ((KMeans.train(vectors, i, numIterations).computeCost(vectors)) + 680 * scala.math.log(i) / scala.math.log(150)))\n1 -> 680.8244\n2 -> 246.436635016484\n3 -> 228.03498068120865\n4 -> 245.48126639400738\n5 -> 264.9805962616268\n6 -> 285.48857890531764\n7 -> 301.56808340425164\n8 -> 315.321639004243\n9 -> 326.47262191671723\n10 -> 344.87130979355675\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LabeledPoint\nscala> import org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.feature.PCA\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala> val pca = new PCA(2).fit(data.map(_.features))\npca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@4eee0b1a\n\nscala> val reduced = data.map(p => p.copy(features = pca.transform(p.features)))\nreduced: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[311] at map at <console>:39\nscala> reduced.collect().take(10)\nres4: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[-2.827135972679021,-5.641331045573367]), (0.0,[-2.7959524821488393,-5.145166883252959]), (0.0,[-2.621523558165053,-5.177378121203953]), (0.0,[-2.764905900474235,-5.0035994150569865]), (0.0,[-2.7827501159516546,-5.6486482943774305]), (0.0,[-3.231445736773371,-6.062506444034109]), (0.0,[-2.6904524156023393,-5.232619219784292]), (0.0,[-2.8848611044591506,-5.485129079769268]), (0.0,[-2.6233845324473357,-4.743925704477387]), (0.0,[-2.8374984110638493,-5.208032027056245]))\n\nscala> import scala.language.postfixOps\nimport scala.language.postfixOps\n\nscala> pca pc\nres24: org.apache.spark.mllib.linalg.DenseMatrix = \n-0.36158967738145065  -0.6565398832858496 \n0.08226888989221656   -0.7297123713264776 \n-0.856572105290527    0.17576740342866465 \n-0.35884392624821626  0.07470647013502865\n\nscala> import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nscala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nscala> val splits = reduced.randomSplit(Array(0.6, 0.4), seed = 1L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[312] at randomSplit at <console>:44, MapPartitionsRDD[313] at randomSplit at <console>:44)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[312] at randomSplit at <console>:44\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[313] at randomSplit at <console>:44\nscala> val numIterations = 100\nnumIterations: Int = 100\nscala> val model = SVMWithSGD.train(training, numIterations)\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = 0.0\nscala> model.clearThreshold()\nres30: model.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 2, numClasses = 2, threshold = None\nscala> val scoreAndLabels = test.map { point =>\n |   val score = model.predict(point.features)\n |   (score, point.label)\n | }\nscoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[517] at map at <console>:54\nscala> val metrics = new BinaryClassificationMetrics(scoreAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@27f49b8c\n\nscala> val auROC = metrics.areaUnderROC()\nauROC: Double = 1.0\nscala> println(\"Area under ROC = \" + auROC)\nArea under ROC = 1.0\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ scala\n\nWelcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import scala.util.Random\nimport scala.util.Random\n\nscala> val x = -5 to 5\nx: scala.collection.immutable.Range.Inclusive = Range(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n\nscala> val y = x.map(_ * 2 + 4 + Random.nextGaussian)\ny: scala.collection.immutable.IndexedSeq[Double] = Vector(-4.317116812989753, -4.4056031270948015, -2.0376543660274713, 0.0184679796245639, 1.8356532746253016, 3.2322795591658644, 6.821999810895798, 7.7977904139852035, 10.288549406814154, 12.424126535332453, 13.611442206874917)\n\nscala> val a = (x, y).zipped.map(_ * _).sum / x.map(x => x * x).sum\na: Double = 1.9498665133868092\n\nscala> val b = y.sum / y.size\nb: Double = 4.115448625564203\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import scala.util.Random.nextGaussian\nimport scala.util.Random.nextGaussian\n\nscala> val x0 = Vector.fill(201)(100 * nextGaussian)\nx0: scala.collection.immutable.IndexedSeq[Double] = Vector(168.28831870102465, -40.56031270948016, -3.7654366027471324, 1.84679796245639, -16.43467253746984, -76.77204408341358, 82.19998108957988, -20.22095860147962, 28.854940681415442, 42.41265353324536, -38.85577931250823, -17.320873680820082, 64.19368427702135, -8.173507833084892, -198.6064655461397, 40.73700995880357, 32.36849515282444, 0.07758364225363915, -101.74032407199553, 34.789280276495646, 46.29624756866302, 35.54024768650289, 24.7867839701828, -11.931948933554782, 72.12437623460166, 30.51440227306552, -80.20756177356768, 134.2380548346385, 96.14401034937691, -205.48142161773896, -73.48186022765427, 2.7861465340245215, 39.49041527572774, 12.262899592863906, -118.30408039749234, -62.727048950163855, -40.58557796128219, -23.42...\nscala> val y0 = Vector.fill(201)(30 * nextGaussian)\ny0: scala.collection.immutable.IndexedSeq[Double] = Vector(-51.675658534203876, 20.230770706186128, 32.47396891906855, -29.35028743620815, 26.7392929946199, 49.85681312583139, 24.226102932450917, 31.19021547086266, 26.169544117916704, -4.51435617676279, 5.6334117227063985, -59.641661744341775, -48.83082934374863, 29.655750956280304, 26.000847703123497, -17.43319605936741, 0.8354318740518344, 11.44787080976254, -26.26312164695179, 88.63863939038357, 45.795968719043785, 88.12442528090506, -29.829048945601635, -1.0417034396751037, -27.119245702417494, -14.055969115249258, 6.120344305721601, 6.102779172838027, -6.342516875566529, 0.06774080659895702, 46.364626315486014, -38.473161588561, -43.25262339890197, 19.77322736359687, -33.78364440355726, -29.085765762613683, 22.87698648100551, 30.53...\nscala> val x1 = (x0, y0).zipped.map((a,b) => 0.5 * (a + b) )\nx1: scala.collection.immutable.IndexedSeq[Double] = Vector(58.30633008341039, -10.164771001647015, 14.354266158160707, -13.75174473687588, 5.152310228575029, -13.457615478791094, 53.213042011015396, 5.484628434691521, 27.51224239966607, 18.949148678241286, -16.611183794900917, -38.48126771258093, 7.681427466636357, 10.741121561597705, -86.3028089215081, 11.651906949718079, 16.601963513438136, 5.7627272260080895, -64.00172285947366, 61.71395983343961, 46.0461081438534, 61.83233648370397, -2.5211324877094174, -6.486826186614943, 22.50256526609208, 8.229216578908131, -37.04360873392304, 70.17041700373827, 44.90074673690519, -102.70684040557, -13.558616956084126, -17.843507527268237, -1.8811040615871129, 16.01806347823039, -76.0438624005248, -45.90640735638877, -8.85429574013834, 3.55536787...\nscala> val y1 = (x0, y0).zipped.map((a,b) => 0.5 * (a - b) )\ny1: scala.collection.immutable.IndexedSeq[Double] = Vector(109.98198861761426, -30.395541707833143, -18.11970276090784, 15.598542699332269, -21.58698276604487, -63.31442860462248, 28.986939078564482, -25.70558703617114, 1.3426982817493691, 23.463504855004075, -22.244595517607316, 21.160394031760845, 56.51225681038499, -18.9146293946826, -112.3036566246316, 29.08510300908549, 15.7665316393863, -5.68514358375445, -37.73860121252187, -26.924679556943964, 0.2501394248096176, -26.292088797201085, 27.30791645789222, -5.445122746939839, 49.62181096850958, 22.28518569415739, -43.16395303964464, 64.06763783090022, 51.24326361247172, -102.77458121216895, -59.92324327157014, 20.62965406129276, 41.37151933731485, -3.755163885366482, -42.26021799696754, -16.820641593775086, -31.73128222114385, -26.9...\nscala> val a = (x1, y1).zipped.map(_ * _).sum / x1.map(x => x * x).sum\na: Double = 0.8119662470457414\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vector\n\nscala> import org.apache.spark.util._\nimport org.apache.spark.util._\n\nscala> import org.apache.spark.mllib.util._\nimport org.apache.spark.mllib.util._\n\nscala> val data = MLUtils.loadLibSVMFile(sc, \"data/iris/iris-libsvm.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[291] at map at MLUtils.scala:112\n\nscala> var w = Vector.random(4)\nw: org.apache.spark.util.Vector = (0.9515155226069267, 0.4901713461728122, 0.4308861351586426, 0.8030814804136821)\n\nscala> for (i <- 1.to(10)) println { val gradient = data.map(p => ( - p.label / (1+scala.math.exp(p.label*(Vector(p.features.toDense.values) dot w))) * Vector(p.features.toDense.values) )).reduce(_+_); w -= 0.1 * gradient; w }\n(-24.056553839570114, -16.585585503253142, -6.881629923278653, -0.4154730884796032)\n(38.56344616042987, 12.134414496746864, 42.178370076721365, 16.344526911520397)\n(13.533446160429868, -4.95558550325314, 34.858370076721364, 15.124526911520398)\n(-11.496553839570133, -22.045585503253143, 27.538370076721364, 13.9045269115204)\n(-4.002010810020908, -18.501520148476196, 32.506256310962314, 15.455945245916512)\n(-4.002011353029471, -18.501520429824225, 32.50625615219947, 15.455945209971787)\n(-4.002011896036225, -18.501520711171313, 32.50625599343715, 15.455945174027184)\n(-4.002012439041171, -18.501520992517463, 32.506255834675365, 15.455945138082699)\n(-4.002012982044308, -18.50152127386267, 32.50625567591411, 15.455945102138333)\n(-4.002013525045636, -18.501521555206942, 32.506255517153384, 15.455945066194088)\n\nscala> w *= 0.24 / 4\nw: org.apache.spark.util.Vector = (-0.24012081150273815, -1.1100912933124165, 1.950375331029203, 0.9273567039716453)\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.DecisionTree\n\nscala> import org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\n\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\n\nscala> // Load and parse the data file.\n\nscala> val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\n\nscala> // Split the data into training and test sets (30% held out for testing)\n\nscala> val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:26\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:26\n\nscala> val categoricalFeaturesInfo = Map[Int, Int]()\ncategoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n\nscala> val impurity = \"variance\"\nimpurity: String = variance\n\nscala> val maxDepth = 5\nmaxDepth: Int = 5\n\nscala> val maxBins = 32\nmaxBins: Int = 32\n\nscala> val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\nmodel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel regressor of depth 2 with 5 nodes\n\nscala> val labelsAndPredictions = testData.map { point =>\n |   val prediction = model.predict(point.features)\n |   (point.label, prediction)\n | }\nlabelsAndPredictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[20] at map at <console>:36\n\nscala> val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()\ntestMSE: Double = 0.07407407407407407\n\nscala> println(s\"Test Mean Squared Error = $testMSE\")\nTest Mean Squared Error = 0.07407407407407407\n\nscala> println(\"Learned regression tree model:\\n\" + model.toDebugString)\nLearned regression tree model:\nDecisionTreeModel regressor of depth 2 with 5 nodes\n If (feature 378 <= 71.0)\n If (feature 100 <= 165.0)\n Predict: 0.0\n Else (feature 100 > 165.0)\n Predict: 1.0\n Else (feature 378 > 71.0)\n Predict: 1.0\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n\nscala> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\n\nscala> \n\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm-3.txt\").toDF()\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector] \n\nscala> \n\nscala> val Array(train, test) = data.randomSplit(Array(0.6, 0.4), seed = 13L)\ntrain: org.apache.spark.sql.DataFrame = [label: double, features: vector]\ntest: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nscala> // specify layers for the neural network: \n\nscala> // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n\nscala> val layers = Array(4, 5, 4, 3)\nlayers: Array[Int] = Array(4, 5, 4, 3)\n\nscala> // create the trainer and set its parameters\n\nscala> val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(13L).setMaxIter(100)\ntrainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_b5f2c25196f9\n\nscala> // train the model\n\nscala> val model = trainer.fit(train)\nmodel: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_b5f2c25196f9\n\nscala> // compute precision on the test set\n\nscala> val result = model.transform(test)\nresult: org.apache.spark.sql.DataFrame = [label: double, features: vector, prediction: double]\n\nscala> val predictionAndLabels = result.select(\"prediction\", \"label\")\npredictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]\n\nscala> val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"precision\")\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_55757d35e3b0\n\nscala> println(\"Precision = \" + evaluator.evaluate(predictionAndLabels))\nPrecision = 0.9375\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nWha\nscala> \n\nscala> val points = Array(\n |    LabeledPoint(0.0, Vectors.sparse(3, Array(1), Array(1.0))),\n |    LabeledPoint(1.0, Vectors.dense(0.0, 2.0, 0.0)),\n |    LabeledPoint(2.0, Vectors.sparse(3, Array((1, 3.0)))),\n |    LabeledPoint.parse(\"(3.0,[0.0,4.0,0.0])\"));\npts: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]), (2.0,(3,[1],[3.0])), (3.0,[0.0,4.0,0.0]))\nscala> \n\nscala> val rdd = sc.parallelize(points)\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = ParallelCollectionRDD[0] at parallelize at <console>:25\n\nscala> \n\nscala> val df = rdd.repartition(1).toDF\ndf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nscala> df.write.parquet(\"points\")\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ wget -O - http://archive.cloudera.com/cdh5/cdh/5/parquet-1.5.0-cdh5.5.0.tar.gz | tar xzvf -\n\nakozlov@Alexanders-MacBook-Pro$ cd parquet-1.5.0-cdh5.5.0/parquet-tools\n\nakozlov@Alexanders-MacBook-Pro$ tar xvf xvf parquet-1.5.0-cdh5.5.0/parquet-tools/target/parquet-tools-1.5.0-cdh5.5.0-bin.tar.gz\n\nakozlov@Alexanders-MacBook-Pro$ cd parquet-tools-1.5.0-cdh5.5.0\n\nakozlov@Alexanders-MacBook-Pro $ ./parquet-schema ~/points/*.parquet \nmessage spark_schema {\n optional double label;\n optional group features {\n required int32 type (INT_8);\n optional int32 size;\n optional group indices (LIST) {\n repeated group list {\n required int32 element;\n }\n }\n optional group values (LIST) {\n repeated group list {\n required double element;\n }\n }\n }\n}\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro $ ./parquet-dump ~/points/*.parquet \nrow group 0 \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nlabel:       DOUBLE GZIP DO:0 FPO:4 SZ:78/79/1.01 VC:4 ENC:BIT_PACKED,PLAIN,RLE\nfeatures: \n.type:       INT32 GZIP DO:0 FPO:82 SZ:101/63/0.62 VC:4 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE\n.size:       INT32 GZIP DO:0 FPO:183 SZ:97/59/0.61 VC:4 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE\n.indices: \n..list: \n...element:  INT32 GZIP DO:0 FPO:280 SZ:100/65/0.65 VC:4 ENC:PLAIN_DICTIONARY,RLE\n.values: \n..list: \n...element:  DOUBLE GZIP DO:0 FPO:380 SZ:125/111/0.89 VC:8 ENC:PLAIN_DICTIONARY,RLE\n\n label TV=4 RL=0 DL=1\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:38 VC:4\n\n features.type TV=4 RL=0 DL=1 DS:                 2 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:9 VC:4\n\n features.size TV=4 RL=0 DL=2 DS:                 1 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:9 VC:4\n\n features.indices.list.element TV=4 RL=1 DL=3 DS: 1 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY SZ:15 VC:4\n\n features.values.list.element TV=8 RL=1 DL=3 DS:  5 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY SZ:17 VC:8\n\nDOUBLE label \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:1 V:0.0\nvalue 2: R:0 D:1 V:1.0\nvalue 3: R:0 D:1 V:2.0\nvalue 4: R:0 D:1 V:3.0\n\nINT32 features.type \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:1 V:0\nvalue 2: R:0 D:1 V:1\nvalue 3: R:0 D:1 V:0\nvalue 4: R:0 D:1 V:1\n\nINT32 features.size \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:2 V:3\nvalue 2: R:0 D:1 V:<null>\nvalue 3: R:0 D:2 V:3\nvalue 4: R:0 D:1 V:<null>\n\nINT32 features.indices.list.element \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:3 V:1\nvalue 2: R:0 D:1 V:<null>\nvalue 3: R:0 D:3 V:1\nvalue 4: R:0 D:1 V:<null>\n\nDOUBLE features.values.list.element \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 8 *** \nvalue 1: R:0 D:3 V:1.0\nvalue 2: R:0 D:3 V:0.0\nvalue 3: R:1 D:3 V:2.0\nvalue 4: R:1 D:3 V:0.0\nvalue 5: R:0 D:3 V:3.0\nvalue 6: R:0 D:3 V:0.0\nvalue 7: R:1 D:3 V:4.0\nvalue 8: R:1 D:3 V:0.0\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> val df = sqlContext.read.parquet(\"points\")\ndf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nscala> val df = sqlContext.read.parquet(\"points\").collect\ndf: Array[org.apache.spark.sql.Row] = Array([0.0,(3,[1],[1.0])], [1.0,[0.0,2.0,0.0]], [2.0,(3,[1],[3.0])], [3.0,[0.0,4.0,0.0]])\n\nscala> val rdd = df.map(x => LabeledPoint(x(0).asInstanceOf[scala.Double], x(1).asInstanceOf[org.apache.spark.mllib.linalg.Vector]))\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[16] at map at <console>:25\n\nscala> rdd.collect\nres12: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]), (2.0,(3,[1],[3.0])), (3.0,[0.0,4.0,0.0]))\n\nscala> rdd.filter(_.features(1) <= 2).collect\nres13: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]))\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> case class Person(id: String, visits: Array[String]) { override def toString: String = { val vsts = visits.mkString(\",\"); s\"($id -> $vsts)\" } }\ndefined class Person\n\nscala> val p1 = Person(\"Phil\", Array(\"http://www.google.com\", \"http://www.facebook.com\", \"http://www.linkedin.com\", \"http://www.homedepot.com\"))\np1: Person = (Phil -> http://www.google.com,http://www.facebook.com,http://www.linkedin.com,http://www.homedepot.com)\n\nscala> val p2 = Person(\"Emily\", Array(\"http://www.victoriassecret.com\", \"http://www.pacsun.com\", \"http://www.abercrombie.com/shop/us\", \"http://www.orvis.com\"))\np2: Person = (Emily -> http://www.victoriassecret.com,http://www.pacsun.com,http://www.abercrombie.com/shop/us,http://www.orvis.com)\n\nscala> sc.parallelize(Array(p1,p2)).repartition(1).toDF.write.parquet(\"history\")\n\nscala> import scala.collection.mutable.WrappedArray\nimport scala.collection.mutable.WrappedArray\n\nscala> val df = sqlContext.read.parquet(\"history\")\ndf: org.apache.spark.sql.DataFrame = [id: string, visits: array<string>]\n\nscala> val rdd = df.map(x => Person(x(0).asInstanceOf[String], x(1).asInstanceOf[WrappedArray[String]].toArray[String]))\nrdd: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[27] at map at <console>:28\n\nscala> rdd.collect\nres9: Array[Person] = Array((Phil -> http://www.google.com,http://www.facebook.com,http://www.linkedin.com,http://www.homedepot.com), (Emily -> http://www.victoriassecret.com,http://www.pacsun.com,http://www.abercrombie.com/shop/us,http://www.orvis.com))\n\n```", "```py\nscala> :paste\n// Entering paste mode (ctrl-D to finish)\n\nimport com.esotericsoftware.kryo.Kryo\nimport org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}\n\nclass MyKryoRegistrator extends KryoRegistrator {\n override def registerClasses(kryo: Kryo) {\n kryo.register(classOf[Person])\n }\n}\n\nobject MyKryoRegistrator {\n def register(conf: org.apache.spark.SparkConf) {\n conf.set(\"spark.serializer\", classOf[KryoSerializer].getName)\n conf.set(\"spark.kryo.registrator\", classOf[MyKryoRegistrator].getName)\n }\n}\n^D\n\n// Exiting paste mode, now interpreting.\n\nimport com.esotericsoftware.kryo.Kryo\nimport org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}\ndefined class MyKryoRegistrator\ndefined module MyKryoRegistrator\n\nscala>\n\n```", "```py\nrow_format\n  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]\n    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\n    [NULL DEFINED AS char]\n```", "```py\n$ cat data\n0^A1^B1^D1.0$\n2^A1^B1^D3.0$\n1^A0^B0.0^C2.0^C0.0$\n3^A0^B0.0^C4.0^C0.0$\n\n```", "```py\n$ tar xf hive-1.1.0-cdh5.5.0.tar.gz \n$ cd hive-1.1.0-cdh5.5.0\n$ bin/hive\n…\nhive> CREATE TABLE LABELED_POINT ( LABEL INT, VECTOR UNIONTYPE<ARRAY<DOUBLE>, MAP<INT,DOUBLE>> ) STORED AS TEXTFILE;\nOK\nTime taken: 0.453 seconds\nhive> LOAD DATA LOCAL INPATH './data' OVERWRITE INTO TABLE LABELED_POINT;\nLoading data to table alexdb.labeled_point\nTable labeled_point stats: [numFiles=1, numRows=0, totalSize=52, rawDataSize=0]\nOK\nTime taken: 0.808 seconds\nhive> select * from labeled_point;\nOK\n0  {1:{1:1.0}}\n2  {1:{1:3.0}}\n1  {0:[0.0,2.0,0.0]}\n3  {0:[0.0,4.0,0.0]}\nTime taken: 0.569 seconds, Fetched: 4 row(s)\nhive>\n\n```", "```py\n(id, timestamp, path)\n```", "```py\nSELECT id, timestamp, path \n  ANALYTIC_FUNCTION(path) OVER (PARTITION BY id ORDER BY timestamp) AS agg\nFROM log_table;\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> :paste\n// Entering paste mode (ctrl-D to finish)\n\nimport java.io._\n\n// a basic page view structure\n@SerialVersionUID(123L)\ncase class PageView(ts: String, path: String) extends Serializable with Ordered[PageView] {\n override def toString: String = {\n s\"($ts :$path)\"\n }\n def compare(other: PageView) = ts compare other.ts\n}\n\n// represent a session\n@SerialVersionUID(456L)\ncase class Session[A  <: PageView](id: String, visits: Seq[A]) extends Serializable {\n override def toString: String = {\n val vsts = visits.mkString(\"[\", \",\", \"]\")\n s\"($id -> $vsts)\"\n }\n}^D\n// Exiting paste mode, now interpreting.\n\nimport java.io._\ndefined class PageView\ndefined class Session\n\n```", "```py\nscala> val rdd = sc.textFile(\"log.csv\").map(x => { val z = x.split(\",\",3); (z(1), new PageView(z(0), z(2))) } ).groupByKey.map( x => { new Session(x._1, x._2.toSeq.sorted) } ).persist\nrdd: org.apache.spark.rdd.RDD[Session] = MapPartitionsRDD[14] at map at <console>:31\n\nscala> rdd.take(3).foreach(println)\n(189.248.74.238 -> [(2015-08-23 23:09:16 :mycompanycom>homepage),(2015-08-23 23:11:00 :mycompanycom>homepage),(2015-08-23 23:11:02 :mycompanycom>running:slp),(2015-08-23 23:12:01 :mycompanycom>running:slp),(2015-08-23 23:12:03 :mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp),(2015-08-23 23:12:08 :mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp),(2015-08-23 23:12:08 :mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp),(2015-08-23 23:12:42 :mycompanycom>running:slp),(2015-08-23 23:13:25 :mycompanycom>homepage),(2015-08-23 23:14:00 :mycompanycom>homepage),(2015-08-23 23:14:06 :mycompanycom:mobile>mycompany photoid>landing),(2015-08-23 23:14:56 :mycompanycom>men>shoes:segmentedgrid),(2015-08-23 23:15:10 :mycompanycom>homepage)])\n(82.166.130.148 -> [(2015-08-23 23:14:27 :mycompanycom>homepage)])\n(88.234.248.111 -> [(2015-08-23 22:36:10 :mycompanycom>plus>home),(2015-08-23 22:36:20 :mycompanycom>plus>home),(2015-08-23 22:36:28 :mycompanycom>plus>home),(2015-08-23 22:36:30 :mycompanycom>plus>onepluspdp>sport band),(2015-08-23 22:36:52 :mycompanycom>onsite search>results found),(2015-08-23 22:37:19 :mycompanycom>plus>onepluspdp>sport band),(2015-08-23 22:37:21 :mycompanycom>plus>home),(2015-08-23 22:37:39 :mycompanycom>plus>home),(2015-08-23 22:37:43 :mycompanycom>plus>home),(2015-08-23 22:37:46 :mycompanycom>plus>onepluspdp>sport watch),(2015-08-23 22:37:50 :mycompanycom>gear>mycompany+ sportwatch:standardgrid),(2015-08-23 22:38:14 :mycompanycom>homepage),(2015-08-23 22:38:35 :mycompanycom>homepage),(2015-08-23 22:38:37 :mycompanycom>plus>products landing),(2015-08-23 22:39:01 :mycompanycom>homepage),(2015-08-23 22:39:24 :mycompanycom>homepage),(2015-08-23 22:39:26 :mycompanycom>plus>whatismycompanyfuel)])\n\n```", "```py\nscala> import java.time.ZoneOffset\nimport java.time.ZoneOffset\n\nscala> import java.time.LocalDateTime\nimport java.time.LocalDateTime\n\nscala> import java.time.format.DateTimeFormatter\nimport java.time.format.DateTimeFormatter\n\nscala> \nscala> def toEpochSeconds(str: String) : Long = { LocalDateTime.parse(str, DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")).toEpochSecond(ZoneOffset.UTC) }\ntoEpochSeconds: (str: String)Long\n\nscala> val checkoutPattern = \".*>checkout.*\".r.pattern\ncheckoutPattern: java.util.regex.Pattern = .*>checkout.*\n\nscala> val lengths = rdd.map(x => { val pths = x.visits.map(y => y.path); val pchs = pths.indexWhere(checkoutPattern.matcher(_).matches); (x.id, x.visits.map(y => y.ts).min, x.visits.map(y => y.ts).max, x.visits.lastIndexWhere(_ match { case PageView(ts, \"mycompanycom>homepage\") => true; case _ => false }, pchs), pchs, x.visits) } ).filter(_._4>0).filter(t => t._5>t._4).map(t => (t._5 - t._4, toEpochSeconds(t._6(t._5).ts) - toEpochSeconds(t._6(t._4).ts)))\n\nscala> lengths.toDF(\"cnt\", \"sec\").agg(avg($\"cnt\"),min($\"cnt\"),max($\"cnt\"),avg($\"sec\"),min($\"sec\"),max($\"sec\")).show\n+-----------------+--------+--------+------------------+--------+--------+\n\n|         avg(cnt)|min(cnt)|max(cnt)|          avg(sec)|min(sec)|max(sec)|\n+-----------------+--------+--------+------------------+--------+--------+\n|19.77570093457944|       1|     121|366.06542056074767|      15|    2635|\n+-----------------+--------+--------+------------------+--------+--------+\n\nscala> lengths.map(x => (x._1,1)).reduceByKey(_+_).sortByKey().collect\nres18: Array[(Int, Int)] = Array((1,1), (2,8), (3,2), (5,6), (6,7), (7,9), (8,10), (9,4), (10,6), (11,4), (12,4), (13,2), (14,3), (15,2), (17,4), (18,6), (19,1), (20,1), (21,1), (22,2), (26,1), (27,1), (30,2), (31,2), (35,1), (38,1), (39,2), (41,1), (43,2), (47,1), (48,1), (49,1), (65,1), (66,1), (73,1), (87,1), (91,1), (103,1), (109,1), (121,1))\n\n```", "```py\ndef splitSession(session: Session[PageView]) : Seq[Session[PageView]] = { … }\n```", "```py\nval newRdd = rdd.flatMap(splitSession)\n```", "```py\nscala> trait Epoch {\n |   this: PageView =>\n |   def epoch() : Long = { LocalDateTime.parse(ts, DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")).toEpochSecond(ZoneOffset.UTC) }\n | }\ndefined trait Epoch\n\n```", "```py\nscala> val rddEpoch = rdd.map(x => new Session(x.id, x.visits.map(x => new PageView(x.ts, x.path) with Epoch)))\nrddEpoch: org.apache.spark.rdd.RDD[Session[PageView with Epoch]] = MapPartitionsRDD[20] at map at <console>:31\n\n```", "```py\nscala> rddEpoch.map(x => (x.id, x.visits.zip(x.visits.tail).map(x => (x._2.path, x._2.epoch - x._1.epoch)).mkString(\"[\", \",\", \"]\"))).take(3).foreach(println)\n(189.248.74.238,[(mycompanycom>homepage,104),(mycompanycom>running:slp,2),(mycompanycom>running:slp,59),(mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp,2),(mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp,5),(mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp,0),(mycompanycom>running:slp,34),(mycompanycom>homepage,43),(mycompanycom>homepage,35),(mycompanycom:mobile>mycompany photoid>landing,6),(mycompanycom>men>shoes:segmentedgrid,50),(mycompanycom>homepage,14)])\n(82.166.130.148,[])\n(88.234.248.111,[(mycompanycom>plus>home,10),(mycompanycom>plus>home,8),(mycompanycom>plus>onepluspdp>sport band,2),(mycompanycom>onsite search>results found,22),(mycompanycom>plus>onepluspdp>sport band,27),(mycompanycom>plus>home,2),(mycompanycom>plus>home,18),(mycompanycom>plus>home,4),(mycompanycom>plus>onepluspdp>sport watch,3),(mycompanycom>gear>mycompany+ sportwatch:standardgrid,4),(mycompanycom>homepage,24),(mycompanycom>homepage,21),(mycompanycom>plus>products landing,2),(mycompanycom>homepage,24),(mycompanycom>homepage,23),(mycompanycom>plus>whatismycompanyfuel,2)])\n\n```", "```py\nscala> def findAllMatchedSessions(h: Seq[Session[PageView]], s: Session[PageView]) : Seq[Session[PageView]] = {\n |     def matchSessions(h: Seq[Session[PageView]], id: String, p: Seq[PageView]) : Seq[Session[PageView]] = {\n |       p match {\n |         case Nil => Nil\n |         case PageView(ts1, \"mycompanycom>homepage\") :: PageView(ts2, \"mycompanycom>plus>products landing\") :: tail =>\n |           matchSessions(h, id, tail).+:(new Session(id, p))\n |         case _ => matchSessions(h, id, p.tail)\n |       }\n |     }\n |    matchSessions(h, s.id, s.visits)\n | }\nfindAllSessions: (h: Seq[Session[PageView]], s: Session[PageView])Seq[Session[PageView]]\n\n```", "```py\nscala> rdd.flatMap(x => findAllMatchedSessions(Nil, x)).take(10).foreach(println)\n(88.234.248.111 -> [(2015-08-23 22:38:35 :mycompanycom>homepage),(2015-08-23 22:38:37 :mycompanycom>plus>products landing),(2015-08-23 22:39:01 :mycompanycom>homepage),(2015-08-23 22:39:24 :mycompanycom>homepage),(2015-08-23 22:39:26 :mycompanycom>plus>whatismycompanyfuel)])\n(148.246.218.251 -> [(2015-08-23 22:52:09 :mycompanycom>homepage),(2015-08-23 22:52:16 :mycompanycom>plus>products landing),(2015-08-23 22:52:23 :mycompanycom>homepage),(2015-08-23 22:52:32 :mycompanycom>homepage),(2015-08-23 22:52:39 :mycompanycom>running:slp)])\n(86.30.116.229 -> [(2015-08-23 23:15:00 :mycompanycom>homepage),(2015-08-23 23:15:02 :mycompanycom>plus>products landing),(2015-08-23 23:15:12 :mycompanycom>plus>products landing),(2015-08-23 23:15:18 :mycompanycom>language tunnel>load),(2015-08-23 23:15:23 :mycompanycom>language tunnel>geo selected),(2015-08-23 23:15:24 :mycompanycom>homepage),(2015-08-23 23:15:27 :mycompanycom>homepage),(2015-08-23 23:15:30 :mycompanycom>basketball:slp),(2015-08-23 23:15:38 :mycompanycom>basketball>lebron-10:cdp),(2015-08-23 23:15:50 :mycompanycom>basketball>lebron-10:cdp),(2015-08-23 23:16:05 :mycompanycom>homepage),(2015-08-23 23:16:09 :mycompanycom>homepage),(2015-08-23 23:16:11 :mycompanycom>basketball:slp),(2015-08-23 23:16:29 :mycompanycom>onsite search>results found),(2015-08-23 23:16:39 :mycompanycom>onsite search>no results)])\n(204.237.0.130 -> [(2015-08-23 23:26:23 :mycompanycom>homepage),(2015-08-23 23:26:27 :mycompanycom>plus>products landing),(2015-08-23 23:26:35 :mycompanycom>plus>fuelband activity>summary>wk)])\n(97.82.221.34 -> [(2015-08-23 22:36:24 :mycompanycom>homepage),(2015-08-23 22:36:32 :mycompanycom>plus>products landing),(2015-08-23 22:37:09 :mycompanycom>plus>plus activity>summary>wk),(2015-08-23 22:37:39 :mycompanycom>plus>products landing),(2015-08-23 22:44:17 :mycompanycom>plus>home),(2015-08-23 22:44:33 :mycompanycom>plus>home),(2015-08-23 22:44:34 :mycompanycom>plus>home),(2015-08-23 22:44:36 :mycompanycom>plus>home),(2015-08-23 22:44:43 :mycompanycom>plus>home)])\n(24.230.204.72 -> [(2015-08-23 22:49:58 :mycompanycom>homepage),(2015-08-23 22:50:00 :mycompanycom>plus>products landing),(2015-08-23 22:50:30 :mycompanycom>homepage),(2015-08-23 22:50:38 :mycompanycom>homepage),(2015-08-23 22:50:41 :mycompanycom>training:cdp),(2015-08-23 22:51:56 :mycompanycom>training:cdp),(2015-08-23 22:51:59 :mycompanycom>store locator>start),(2015-08-23 22:52:28 :mycompanycom>store locator>landing)])\n(62.248.72.18 -> [(2015-08-23 23:14:27 :mycompanycom>homepage),(2015-08-23 23:14:30 :mycompanycom>plus>products landing),(2015-08-23 23:14:33 :mycompanycom>plus>products landing),(2015-08-23 23:14:40 :mycompanycom>plus>products landing),(2015-08-23 23:14:47 :mycompanycom>store homepage),(2015-08-23 23:14:50 :mycompanycom>store homepage),(2015-08-23 23:14:55 :mycompanycom>men:clp),(2015-08-23 23:15:08 :mycompanycom>men:clp),(2015-08-23 23:15:15 :mycompanycom>men:clp),(2015-08-23 23:15:16 :mycompanycom>men:clp),(2015-08-23 23:15:24 :mycompanycom>men>sportswear:standardgrid),(2015-08-23 23:15:41 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:45 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:45 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:49 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:50 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:56 :mycompanycom>men>sportswear:standardgrid),(2015-08-23 23:18:41 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:42 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:53 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:55 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:57 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:19:04 :mycompanycom>men>sportswear:standardgrid),(2015-08-23 23:20:12 :mycompanycom>men>sportswear>silver:standardgrid),(2015-08-23 23:28:20 :mycompanycom>onsite search>no results),(2015-08-23 23:28:33 :mycompanycom>onsite search>no results),(2015-08-23 23:28:36 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:40 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:41 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:43 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:43 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:29:00 :mycompanycom>pdp:mycompanyid>mycompany blazer low id shoe)])\n(46.5.127.21 -> [(2015-08-23 22:58:00 :mycompanycom>homepage),(2015-08-23 22:58:01 :mycompanycom>plus>products landing)])\n(200.45.228.1 -> [(2015-08-23 23:07:33 :mycompanycom>homepage),(2015-08-23 23:07:39 :mycompanycom>plus>products landing),(2015-08-23 23:07:42 :mycompanycom>plus>products landing),(2015-08-23 23:07:45 :mycompanycom>language tunnel>load),(2015-08-23 23:07:59 :mycompanycom>homepage),(2015-08-23 23:08:15 :mycompanycom>homepage),(2015-08-23 23:08:26 :mycompanycom>onsite search>results found),(2015-08-23 23:08:43 :mycompanycom>onsite search>no results),(2015-08-23 23:08:49 :mycompanycom>onsite search>results found),(2015-08-23 23:08:53 :mycompanycom>language tunnel>load),(2015-08-23 23:08:55 :mycompanycom>plus>products landing),(2015-08-23 23:09:04 :mycompanycom>homepage),(2015-08-23 23:11:34 :mycompanycom>running:slp)])\n(37.78.203.213 -> [(2015-08-23 23:18:10 :mycompanycom>homepage),(2015-08-23 23:18:12 :mycompanycom>plus>products landing),(2015-08-23 23:18:14 :mycompanycom>plus>products landing),(2015-08-23 23:18:22 :mycompanycom>plus>products landing),(2015-08-23 23:18:25 :mycompanycom>store homepage),(2015-08-23 23:18:31 :mycompanycom>store homepage),(2015-08-23 23:18:34 :mycompanycom>men:clp),(2015-08-23 23:18:50 :mycompanycom>store homepage),(2015-08-23 23:18:51 :mycompanycom>footwear:segmentedgrid),(2015-08-23 23:19:12 :mycompanycom>men>footwear:segmentedgrid),(2015-08-23 23:19:12 :mycompanycom>men>footwear:segmentedgrid),(2015-08-23 23:19:26 :mycompanycom>men>footwear>new releases:standardgrid),(2015-08-23 23:19:26 :mycompanycom>men>footwear>new releases:standardgrid),(2015-08-23 23:19:35 :mycompanycom>pdp>mycompany cheyenne 2015 men's shoe),(2015-08-23 23:19:40 :mycompanycom>men>footwear>new releases:standardgrid)])\n\n```", "```py\ndef randomeProjecton(data: NestedStructure) : Vector = { … }\n\n```", "```py\n$ port install sbt\n\n```", "```py\n$ brew install sbt\n\n```", "```py\n$ bin/create_project.sh\n\n```", "```py\n$ cat >> project.plugin.sbt << EOF\naddSbtPlugin(\"org.scalastyle\" %% \"scalastyle-sbt-plugin\" % \"0.8.0\")\nEOF\n\n```", "```py\n$ sbt\n [info] Loading global plugins from /Users/akozlov/.sbt/0.13/plugins\n[info] Set current project to My Graph Project (in build file:/Users/akozlov/Scala/graph/)\n> help\n\n help                                    Displays this help message or prints detailed help on requested commands (run 'help <command>').\nFor example, `sbt package` will build a Java jar, as follows:\n$  sbt package\n[info] Loading global plugins from /Users/akozlov/.sbt/0.13/plugins\n[info] Loading project definition from /Users/akozlov/Scala/graph/project\n[info] Set current project to My Graph Project (in build file:/Users/akozlov/Scala/graph/)\n[info] Updating {file:/Users/akozlov/Scala/graph/}graph...\n[info] Resolving jline#jline;2.12.1 ...\n[info] Done updating.\n$ ls -1 target/scala-2.11/\nclasses\nmy-graph-project_2.11-1.0.jar\n\n```", "```py\nimport scalax.collection.Graph\nimport scalax.collection.edge._\nimport scalax.collection.GraphPredef._\nimport scalax.collection.GraphEdge._\n\nimport scalax.collection.edge.Implicits._\n\nobject InfluenceDiagram extends App {\n  var g = Graph[String, LDiEdge]((\"'Weather'\"~+>\"'Weather Forecast'\")(\"Forecast\"), (\"'Weather Forecast'\"~+>\"'Vacation Activity'\")(\"Decision\"), (\"'Vacation Activity'\"~+>\"'Satisfaction'\")(\"Deterministic\"), (\"'Weather'\"~+>\"'Satisfaction'\")(\"Deterministic\"))\n  println(g.mkString(\";\"))\n  println(g.isDirected)\n  println(g.isAcyclic)\n}\n```", "```py\n[akozlov@Alexanders-MacBook-Pro chapter07(master)]$ sbt\n[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter07/project\n[info] Set current project to Working with Graph Algorithms (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter07/)\n> run\n[warn] Multiple main classes detected.  Run 'show discoveredMainClasses' to see the list\n\nMultiple main classes detected, select one to run:\n\n [1] org.akozlov.chapter07.ConstranedDAG\n [2] org.akozlov.chapter07.EnronEmail\n [3] org.akozlov.chapter07.InfluenceDiagram\n [4] org.akozlov.chapter07.InfluenceDiagramToJson\n\nEnter number: 3\n\n[info] Running org.akozlov.chapter07.InfluenceDiagram \n'Weather';'Vacation Activity';'Satisfaction';'Weather Forecast';'Weather'~>'Weather Forecast' 'Forecast;'Weather'~>'Satisfaction' 'Deterministic;'Vacation Activity'~>'Satisfaction' 'Deterministic;'Weather Forecast'~>'Vacation Activity' 'Decision\nDirected: true\nAcyclic: true\n'Weather';'Vacation Activity';'Satisfaction';'Recommend to a Friend';'Weather Forecast';'Weather'~>'Weather Forecast' 'Forecast;'Weather'~>'Satisfaction' 'Deterministic;'Vacation Activity'~>'Satisfaction' 'Deterministic;'Satisfaction'~>'Recommend to a Friend' 'Probabilistic;'Weather Forecast'~>'Vacation Activity' 'Decision\nDirected: true\nAcyclic: true\n\n```", "```py\ng += (\"'Satisfaction'\" ~+> \"'Recommend to a Friend'\")(\"Probabilistic\")\n```", "```py\n'Weather';'Vacation Activity';'Satisfaction';'Recommend to a Friend';'Weather Forecast';'Weather'~>'Weather Forecast' 'Forecast;'Weather'~>'Satisfaction' 'Deterministic;'Vacation Activity'~>'Satisfaction' 'Deterministic;'Satisfaction'~>'Recommend to a Friend' 'Probabilistic;'Weather Forecast'~>'Vacation Activity' 'Decision\nDirected: true\nAcyclic: true\n\n```", "```py\nprintln((g get \"'Recommend to a Friend'\").incoming)\n\nSet('Satisfaction'~>'Recommend to a Friend' 'Probabilistic)\n```", "```py\ng += (\"'Satisfaction'\" ~+> \"'Weather'\")(\"Cyclic\")\nprintln(g.mkString(\";\")) println(\"Directed: \" + g.isDirected)\nprintln(\"Acyclic: \" + g.isAcyclic)\n\n'Weather';'Vacation Activity';'Satisfaction';'Recommend to a Friend';'Weather Forecast';'Weather'~>'Weather Forecast' 'Forecast;'Weather'~>'Satisfaction' 'Deterministic;'Vacation Activity'~>'Satisfaction' 'Deterministic;'Satisfaction'~>'Recommend to a Friend' 'Probabilistic;'Satisfaction'~>'Weather' 'Cyclic;'Weather Forecast'~>'Vacation Activity' 'Decision\nDirected: true\nAcyclic: false\n```", "```py\n var n, m = 0; val f = Graph.fill(45){ m = if (m < 9) m + 1 else { n = if (n < 8) n + 1 else 8; n + 1 }; m ~ n }\n\n  println(f.nodes)\n  println(f.edges)\n  println(f)\n\n  println(\"Directed: \" + f.isDirected)\n  println(\"Acyclic: \" + f.isAcyclic)\n\nNodeSet(0, 9, 1, 5, 2, 6, 3, 7, 4, 8)\nEdgeSet(9~0, 9~1, 9~2, 9~3, 9~4, 9~5, 9~6, 9~7, 9~8, 1~0, 5~0, 5~1, 5~2, 5~3, 5~4, 2~0, 2~1, 6~0, 6~1, 6~2, 6~3, 6~4, 6~5, 3~0, 3~1, 3~2, 7~0, 7~1, 7~2, 7~3, 7~4, 7~5, 7~6, 4~0, 4~1, 4~2, 4~3, 8~0, 8~1, 8~2, 8~3, 8~4, 8~5, 8~6, 8~7)\nGraph(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1~0, 2~0, 2~1, 3~0, 3~1, 3~2, 4~0, 4~1, 4~2, 4~3, 5~0, 5~1, 5~2, 5~3, 5~4, 6~0, 6~1, 6~2, 6~3, 6~4, 6~5, 7~0, 7~1, 7~2, 7~3, 7~4, 7~5, 7~6, 8~0, 8~1, 8~2, 8~3, 8~4, 8~5, 8~6, 8~7, 9~0, 9~1, 9~2, 9~3, 9~4, 9~5, 9~6, 9~7, 9~8)\nDirected: false\nAcyclic: false\n```", "```py\npackage org.akozlov.chapter07\n\nimport scalax.collection.GraphPredef._, scalax.collection.GraphEdge._\nimport scalax.collection.constrained.{Config, ConstraintCompanion, Graph => DAG}\nimport scalax.collection.constrained.constraints.{Connected, Acyclic}\n\nobject AcyclicWithSideEffect extends ConstraintCompanion[Acyclic] {\n  def apply [N, E[X] <: EdgeLikeIn[X]] (self: DAG[N,E]) =\n    new Acyclic[N,E] (self) {\n      override def onAdditionRefused(refusedNodes: Iterable[N],\n        refusedEdges: Iterable[E[N]],\n        graph:        DAG[N,E]) = {\n          println(\"Addition refused: \" + \"nodes = \" + refusedNodes + \", edges = \" + refusedEdges)\n          true\n        }\n    }\n}\n\nobject ConnectedWithSideEffect extends ConstraintCompanion[Connected] {\n  def apply [N, E[X] <: EdgeLikeIn[X]] (self: DAG[N,E]) =\n    new Connected[N,E] (self) {\n      override def onSubtractionRefused(refusedNodes: Iterable[DAG[N,E]#NodeT],\n        refusedEdges: Iterable[DAG[N,E]#EdgeT],\n        graph:        DAG[N,E]) = {\n          println(\"Subtraction refused: \" + \"nodes = \" + refusedNodes + \", edges = \" + refusedEdges)\n        true\n      }\n    }\n}\n\nclass CycleException(msg: String) extends IllegalArgumentException(msg)\nobject ConstranedDAG extends App {\n  implicit val conf: Config = ConnectedWithSideEffect && AcyclicWithSideEffect\n  val g = DAG(1~>2, 1~>3, 2~>3, 3~>4) // Graph()\n  println(g ++ List(1~>4, 3~>1))\n  println(g - 2~>3)\n  println(g - 2)\n  println((g + 4~>5) - 3)\n}\n```", "```py\n[akozlov@Alexanders-MacBook-Pro chapter07(master)]$ sbt \"run-main org.akozlov.chapter07.ConstranedDAG\"\n[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter07/project\n[info] Set current project to Working with Graph Algorithms (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter07/)\n[info] Running org.akozlov.chapter07.ConstranedDAG \nAddition refused: nodes = List(), edges = List(1~>4, 3~>1)\nGraph(1, 2, 3, 4, 1~>2, 1~>3, 2~>3, 3~>4)\nSubtraction refused: nodes = Set(), edges = Set(2~>3)\nGraph(1, 2, 3, 4, 1~>2, 1~>3, 2~>3, 3~>4)\nGraph(1, 3, 4, 1~>3, 3~>4)\nSubtraction refused: nodes = Set(3), edges = Set()\nGraph(1, 2, 3, 4, 5, 1~>2, 1~>3, 2~>3, 3~>4, 4~>5)\n[success] Total time: 1 s, completed May 1, 2016 1:53:42 PM \n\n```", "```py\nobject InfluenceDiagramToJson extends App {\n\n  val g = Graph[String,LDiEdge]((\"'Weather'\" ~+> \"'Weather Forecast'\")(\"Forecast\"), (\"'Weather Forecast'\" ~+> \"'Vacation Activity'\")(\"Decision\"), (\"'Vacation Activity'\" ~+> \"'Satisfaction'\")(\"Deterministic\"), (\"'Weather'\" ~+> \"'Satisfaction'\")(\"Deterministic\"), (\"'Satisfaction'\" ~+> \"'Recommend to a Friend'\")(\"Probabilistic\"))\n\n  import scalax.collection.io.json.descriptor.predefined.{LDi}\n  import scalax.collection.io.json.descriptor.StringNodeDescriptor\n  import scalax.collection.io.json._\n\n  val descriptor = new Descriptor[String](\n    defaultNodeDescriptor = StringNodeDescriptor,\n    defaultEdgeDescriptor = LDi.descriptor[String,String](\"Edge\")\n  )\n\n  val n = g.toJson(descriptor)\n  println(n)\n  import net.liftweb.json._\n  println(Printer.pretty(JsonAST.render(JsonParser.parse(n))))\n}\n```", "```py\n[kozlov@Alexanders-MacBook-Pro chapter07(master)]$ sbt \"run-main org.akozlov.chapter07.InfluenceDiagramToJson\"\n[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter07/project\n[info] Set current project to Working with Graph Algorithms (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter07/)\n[info] Running org.akozlov.chapter07.InfluenceDiagramToJson \n{\n \"nodes\":[[\"'Recommend to a Friend'\"],[\"'Satisfaction'\"],[\"'Vacation Activity'\"],[\"'Weather Forecast'\"],[\"'Weather'\"]],\n \"edges\":[{\n \"n1\":\"'Weather'\",\n \"n2\":\"'Weather Forecast'\",\n \"label\":\"Forecast\"\n },{\n \"n1\":\"'Vacation Activity'\",\n \"n2\":\"'Satisfaction'\",\n \"label\":\"Deterministic\"\n },{\n \"n1\":\"'Weather'\",\n \"n2\":\"'Satisfaction'\",\n \"label\":\"Deterministic\"\n },{\n \"n1\":\"'Weather Forecast'\",\n \"n2\":\"'Vacation Activity'\",\n \"label\":\"Decision\"\n },{\n \"n1\":\"'Satisfaction'\",\n \"n2\":\"'Recommend to a Friend'\",\n \"label\":\"Probabilistic\"\n }]\n}\n[success] Total time: 1 s, completed May 1, 2016 1:55:30 PM\n\n```", "```py\npackage org.akozlov.chapter07\n\nimport scala.io.Source\n\nimport scala.util.hashing.{MurmurHash3 => Hash}\nimport scala.util.matching.Regex\n\nimport java.util.{Date => javaDateTime}\n\nimport java.io.File\nimport net.liftweb.json._\nimport Extraction._\nimport Serialization.{read, write}\n\nobject EnronEmail {\n\n  val emailRe = \"\"\"[a-zA-Z0-9_.+\\-]+@enron.com\"\"\".r.unanchored\n\n  def emails(s: String) = {\n    for (email <- emailRe findAllIn s) yield email\n  }\n\n  def hash(s: String) = {\n    java.lang.Integer.MAX_VALUE.toLong + Hash.stringHash(s)\n  }\n\n  val messageRe =\n    \"\"\"(?:Message-ID:\\s+)(<[A-Za-z0-9_.+\\-@]+>)(?s)(?:.*?)(?m)\n      |(?:Date:\\s+)(.*?)$(?:.*?)\n      |(?:From:\\s+)([a-zA-Z0-9_.+\\-]+@enron.com)(?:.*?)\n      |(?:Subject: )(.*?)$\"\"\".stripMargin.r.unanchored\n\n  case class Relation(from: String, fromId: Long, to: String, toId: Long, source: String, messageId: String, date: javaDateTime, subject: String)\n\n  implicit val formats = Serialization.formats(NoTypeHints)\n\n  def getFileTree(f: File): Stream[File] =\n    f #:: (if (f.isDirectory) f.listFiles().toStream.flatMap(getFileTree) else Stream.empty)\n\n  def main(args: Array[String]) {\n    getFileTree(new File(args(0))).par.map {\n      file => {\n        \"\\\\.$\".r findFirstIn file.getName match {\n          case Some(x) =>\n          try {\n            val src = Source.fromFile(file, \"us-ascii\")\n            val message = try src.mkString finally src.close()\n            message match {\n              case messageRe(messageId, date, from , subject) =>\n              val fromLower = from.toLowerCase\n              for (to <- emails(message).filter(_ != fromLower).toList.distinct)\n              println(write(Relation(fromLower, hash(fromLower), to, hash(to), file.toString, messageId, new javaDateTime(date), subject)))\n                case _ =>\n            }\n          } catch {\n            case e: Exception => System.err.println(e)\n          }\n          case _ =>\n        }\n      }\n    }\n  }\n}\n```", "```py\n# (mkdir Enron; cd Enron; wget -O - http://www.cs.cmu.edu/~./enron/enron_mail_20150507.tgz | tar xzvf -)\n...\n# sbt --error \"run-main org.akozlov.chapter07.EnronEmail Enron/maildir\" > graph.json\n\n# spark --driver-memory 2g --executor-memory 2g\n...\nscala> val df = sqlContext.read.json(\"graph.json\")\ndf: org.apache.spark.sql.DataFrame = [[date: string, from: string, fromId: bigint, messageId: string, source: string, subject: string, to: string, toId: bigint]\n\n```", "```py\nscala> df.select(\"_corrupt_record\").collect.foreach(println)\n...\n\n```", "```py\nscala> import org.apache.spark._\n...\nscala> import org.apache.spark.graphx._\n...\nscala> import org.apache.spark.rdd.RDD\n...\nscala> val people: RDD[(VertexId, String)] = df.select(\"fromId\", \"from\").unionAll(df.select(\"toId\", \"to\")).na.drop.distinct.map( x => (x.get(0).toString.toLong, x.get(1).toString))\npeople: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, String)] = MapPartitionsRDD[146] at map at <console>:28\n\nscala> val relationships = df.select(\"fromId\", \"toId\", \"messageId\", \"subject\").na.drop.distinct.map( x => Edge(x.get(0).toString.toLong, x.get(1).toString.toLong, (x.get(2).toString, x.get(3).toString)))\nrelationships: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[(String, String)]] = MapPartitionsRDD[156] at map at <console>:28\n\nscala> val graph = Graph(people, relationships).cache\ngraph: org.apache.spark.graphx.Graph[String,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@7b59aa7b\n\n```", "```py\nscala> graph.vertices.getNumPartitions\nres1: Int = 200\n\nscala> graph.edges.getNumPartitions\nres2: Int = 200\n\n```", "```py\nscala> val graph = Graph(people.coalesce(6), relationships.coalesce(6))\ngraph: org.apache.spark.graphx.Graph[String,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@5dc7d016\n\nscala> graph.vertices.getNumPartitions\nres10: Int = 6\n\nscala> graph.edges.getNumPartitions\nres11: Int = 6\n\n```", "```py\nscala> graph.cache\nres12: org.apache.spark.graphx.Graph[String,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@5dc7d016\n\n```", "```py\nscala> people.join(graph.inDegrees).sortBy(_._2._2, ascending=false).take(10).foreach(println)\n(268746271,(richard.shapiro@enron.com,18523))\n(1608171805,(steven.kean@enron.com,15867))\n(1578042212,(jeff.dasovich@enron.com,13878))\n(960683221,(tana.jones@enron.com,13717))\n(3784547591,(james.steffes@enron.com,12980))\n(1403062842,(sara.shackleton@enron.com,12082))\n(2319161027,(mark.taylor@enron.com,12018))\n(969899621,(mark.guzman@enron.com,10777))\n(1362498694,(geir.solberg@enron.com,10296))\n(4151996958,(ryan.slinger@enron.com,10160))\n\n```", "```py\nscala> people.join(graph.outDegrees).sortBy(_._2._2, ascending=false).take(10).foreach(println)\n(1578042212,(jeff.dasovich@enron.com,139786))\n(2822677534,(veronica.espinoza@enron.com,106442))\n(3035779314,(pete.davis@enron.com,94666))\n(2346362132,(rhonda.denton@enron.com,90570))\n(861605621,(cheryl.johnson@enron.com,74319))\n(14078526,(susan.mara@enron.com,58797))\n(2058972224,(jae.black@enron.com,58718))\n(871077839,(ginger.dernehl@enron.com,57559))\n(3852770211,(lorna.brennan@enron.com,50106))\n(241175230,(mary.hain@enron.com,40425))\n…\n\n```", "```py\nscala> val groups = org.apache.spark.graphx.lib.ConnectedComponents.run(graph).vertices.map(_._2).distinct.cache\ngroups: org.apache.spark.rdd.RDD[org.apache.spark.graphx.VertexId] = MapPartitionsRDD[2404] at distinct at <console>:34\n\nscala> groups.count\nres106: Long = 18\n\nscala> people.join(groups.map( x => (x, x))).map(x => (x._1, x._2._1)).sortBy(_._1).collect.foreach(println)\n(332133,laura.beneville@enron.com)\n(81833994,gpg.me-q@enron.com)\n(115247730,dl-ga-enron_debtor@enron.com)\n(299810291,gina.peters@enron.com)\n(718200627,techsupport.notices@enron.com)\n(847455579,paul.de@enron.com)\n(919241773,etc.survey@enron.com)\n(1139366119,enron.global.services.-.us@enron.com)\n(1156539970,shelley.ariel@enron.com)\n(1265773423,dl-ga-all_ews_employees@enron.com)\n(1493879606,chairman.ees@enron.com)\n(1511379835,gary.allen.-.safety.specialist@enron.com)\n(2114016426,executive.robert@enron.com)\n(2200225669,ken.board@enron.com)\n(2914568776,ge.americas@enron.com)\n(2934799198,yowman@enron.com)\n(2975592118,tech.notices@enron.com)\n(3678996795,mail.user@enron.com)\n\n```", "```py\nscala> df.filter(\"fromId = 919241773 or toId = 919241773\").select(\"date\",\"from\",\"to\",\"subject\",\"source\").collect.foreach(println)\n[2000-09-19T18:40:00.000Z,survey.test@enron.com,etc.survey@enron.com,NO ACTION REQUIRED - TEST,Enron/maildir/dasovich-j/all_documents/1567.]\n[2000-09-19T18:40:00.000Z,survey.test@enron.com,etc.survey@enron.com,NO ACTION REQUIRED - TEST,Enron/maildir/dasovich-j/notes_inbox/504.]\n\n```", "```py\nscala> val unedges = graph.edges.map(e => if (e.srcId < e.dstId) (e.srcId, e.dstId) else (e.dstId, e.srcId)).map( x => Edge(x._1, x._2, 1)).cache\nunedges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Int]] = MapPartitionsRDD[87] at map at <console>:48\n\nscala> val ungraph = Graph(people, unedges).partitionBy(org.apache.spark.graphx.PartitionStrategy.EdgePartition1D, 10).cache\nungraph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@77274fff\n\nscala> val triangles = org.apache.spark.graphx.lib.TriangleCount.run(ungraph).cache\ntriangles: org.apache.spark.graphx.Graph[Int,Int] = org.apache.spark.graphx.impl.GraphImpl@6aec6da1\n\nscala> people.join(triangles.vertices).map(t => (t._2._2,t._2._1)).sortBy(_._1, ascending=false).take(10).foreach(println)\n(31761,sally.beck@enron.com)\n(24101,louise.kitchen@enron.com)\n(23522,david.forster@enron.com)\n(21694,kenneth.lay@enron.com)\n(20847,john.lavorato@enron.com)\n(18460,david.oxley@enron.com)\n(17951,tammie.schoppe@enron.com)\n(16929,steven.kean@enron.com)\n(16390,tana.jones@enron.com)\n(16197,julie.clyatt@enron.com)\n\n```", "```py\nscala> val components = org.apache.spark.graphx.lib.StronglyConnectedComponents.run(graph, 100).cache\ncomponents: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,(String, String)] = org.apache.spark.graphx.impl.GraphImpl@55913bc7\n\nscala> components.vertices.map(_._2).distinct.count\nres2: Long = 17980\n\nscala> people.join(components.vertices.map(_._2).distinct.map( x => (x, x))).map(x => (x._1, x._2._1)).sortBy(_._1).collect.foreach(println)\n(332133,laura.beneville@enron.com) \n(466265,medmonds@enron.com)\n(471258,.jane@enron.com)\n(497810,.kimberly@enron.com)\n(507806,aleck.dadson@enron.com)\n(639614,j..bonin@enron.com)\n(896860,imceanotes-hbcamp+40aep+2ecom+40enron@enron.com)\n(1196652,enron.legal@enron.com)\n(1240743,thi.ly@enron.com)\n(1480469,ofdb12a77a.a6162183-on86256988.005b6308@enron.com)\n(1818533,fran.i.mayes@enron.com)\n(2337461,michael.marryott@enron.com)\n(2918577,houston.resolution.center@enron.com)\n\n```", "```py\nscala> val ranks = graph.pageRank(0.001).vertices\nranks: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[955] at RDD at VertexRDD.scala:57\n\nscala> people.join(ranks).map(t => (t._2._2,t._2._1)).sortBy(_._1, ascending=false).take(10).foreach(println)\n\nscala> val ranks = graph.pageRank(0.001).vertices\nranks: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[955] at RDD at VertexRDD.scala:57\n\nscala> people.join(ranks).map(t => (t._2._2,t._2._1)).sortBy(_._1, ascending=false).take(10).foreach(println)\n(32.073722548483325,tana.jones@enron.com)\n(29.086568868043248,sara.shackleton@enron.com)\n(28.14656912897315,louise.kitchen@enron.com)\n(26.57894933459292,vince.kaminski@enron.com)\n(25.865486865014493,sally.beck@enron.com)\n(23.86746232662471,john.lavorato@enron.com)\n(22.489814482022275,jeff.skilling@enron.com)\n(21.968039409295585,mark.taylor@enron.com)\n(20.903053536275547,kenneth.lay@enron.com)\n(20.39124651779771,gerald.nemec@enron.com)\n\n```", "```py\nscala> val rgraph = graph.partitionBy(org.apache.spark.graphx.PartitionStrategy.EdgePartition1D, 10).mapEdges(e => 1).groupEdges(_+_).cache\nrgraph: org.apache.spark.graphx.Graph[String,Int] = org.apache.spark.graphx.impl.GraphImpl@2c1a48d6\n\nscala> val redges = rgraph.edges.map( e => Edge(-e.srcId, e.dstId, Math.log(e.attr.toDouble)) ).cache\nredges: org.apache.spark.rdd.RDD[org.apache.spark.graphx.Edge[Double]] = MapPartitionsRDD[57] at map at <console>:36\n\nscala> import org.apache.spark.graphx.lib.SVDPlusPlus\nimport org.apache.spark.graphx.lib.SVDPlusPlus\n\nscala> implicit val conf = new SVDPlusPlus.Conf(10, 50, 0.0, 10.0, 0.007, 0.007, 0.005, 0.015)\nconf: org.apache.spark.graphx.lib.SVDPlusPlus.Conf = org.apache.spark.graphx.lib.SVDPlusPlus$Conf@15cdc117\n\nscala> val (svd, mu) = SVDPlusPlus.run(redges, conf)\nsvd: org.apache.spark.graphx.Graph[(Array[Double], Array[Double], Double, Double),Double] = org.apache.spark.graphx.impl.GraphImpl@3050363d\nmu: Double = 1.3773578970633769\n\nscala> val svdRanks = svd.vertices.filter(_._1 > 0).map(x => (x._2._3, x._1))\nsvdRanks: org.apache.spark.rdd.RDD[(Double, org.apache.spark.graphx.VertexId)] = MapPartitionsRDD[1517] at map at <console>:31\n\nscala> val svdRanks = svd.vertices.filter(_._1 > 0).map(x => (x._1, x._2._3))\nsvdRanks: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, Double)] = MapPartitionsRDD[1520] at map at <console>:31\n\nscala> people.join(svdRanks).sortBy(_._2._2, ascending=false).map(x => (x._2._2, x._2._1)).take(10).foreach(println)\n(8.864218804309887,jbryson@enron.com)\n(5.935146713012661,dl-ga-all_enron_worldwide2@enron.com)\n(5.740242927715701,houston.report@enron.com)\n(5.441934324464593,a478079f-55e1f3b0-862566fa-612229@enron.com)\n(4.910272928389445,pchoi2@enron.com)\n(4.701529779800544,dl-ga-all_enron_worldwide1@enron.com)\n(4.4046392452058045,eligible.employees@enron.com)\n(4.374738019256556,all_ena_egm_eim@enron.com)\n(4.303078586979311,dl-ga-all_enron_north_america@enron.com)\n(3.8295412053860867,the.mailout@enron.com)\n\n```", "```py\nscala> import com.github.fommil.netlib.BLAS.{getInstance => blas}\n\nscala> def topN(uid: Long, num: Int) = {\n |    val usr = svd.vertices.filter(uid == -_._1).collect()(0)._2\n |    val recs = svd.vertices.filter(_._1 > 0).map( v => (v._1, mu + usr._3 + v._2._3 + blas.ddot(usr._2.length, v._2._1, 1, usr._2, 1)))\n |    people.join(recs).sortBy(_._2._2, ascending=false).map(x => (x._2._2, x._2._1)).take(num)\n | }\ntopN: (uid: Long, num: Int)Array[(Double, String)]\n\nscala> def top5(x: Long) : Array[(Double, String)] = topN(x, 5)\ntop5: (x: Long)Array[(Double, String)]\n\nscala> people.join(graph.inDegrees).sortBy(_._2._2, ascending=false).map(x => (x._1, x._2._1)).take(10).toList.map(t => (t._2, top5(t._1).toList)).foreach(println)\n(richard.shapiro@enron.com,List((4.866184418005094E66,anne.bertino@enron.com), (3.9246829664352734E66,kgustafs@enron.com), (3.9246829664352734E66,gweiss@enron.com), (3.871029763863491E66,hill@enron.com), (3.743135924382312E66,fraser@enron.com)))\n(steven.kean@enron.com,List((2.445163626935533E66,anne.bertino@enron.com), (1.9584692804232504E66,hill@enron.com), (1.9105427465629028E66,kgustafs@enron.com), (1.9105427465629028E66,gweiss@enron.com), (1.8931872324048717E66,fraser@enron.com)))\n(jeff.dasovich@enron.com,List((2.8924566115596135E66,anne.bertino@enron.com), (2.3157345904446663E66,hill@enron.com), (2.2646318970030287E66,gweiss@enron.com), (2.2646318970030287E66,kgustafs@enron.com), (2.2385865127706285E66,fraser@enron.com)))\n(tana.jones@enron.com,List((6.1758464471309754E66,elizabeth.sager@enron.com), (5.279291610047078E66,tana.jones@enron.com), (4.967589820856654E66,tim.belden@enron.com), (4.909283344915057E66,jeff.dasovich@enron.com), (4.869177440115682E66,mark.taylor@enron.com)))\n(james.steffes@enron.com,List((5.7702834706832735E66,anne.bertino@enron.com), (4.703038082326939E66,gweiss@enron.com), (4.703038082326939E66,kgustafs@enron.com), (4.579565962089777E66,hill@enron.com), (4.4298763869135494E66,george@enron.com)))\n(sara.shackleton@enron.com,List((9.198688613290757E67,louise.kitchen@enron.com), (8.078107057848099E67,john.lavorato@enron.com), (6.922806078209984E67,greg.whalley@enron.com), (6.787266892881456E67,elizabeth.sager@enron.com), (6.420473603137515E67,sally.beck@enron.com)))\n(mark.taylor@enron.com,List((1.302856119148208E66,anne.bertino@enron.com), (1.0678968544568682E66,hill@enron.com), (1.031255083546722E66,fraser@enron.com), (1.009319696608474E66,george@enron.com), (9.901391892701356E65,brad@enron.com)))\n(mark.guzman@enron.com,List((9.770393472845669E65,anne.bertino@enron.com), (7.97370292724488E65,kgustafs@enron.com), (7.97370292724488E65,gweiss@enron.com), (7.751983820970696E65,hill@enron.com), (7.500175024539423E65,george@enron.com)))\n(geir.solberg@enron.com,List((6.856103529420811E65,anne.bertino@enron.com), (5.611272903720188E65,gweiss@enron.com), (5.611272903720188E65,kgustafs@enron.com), (5.436280144720843E65,hill@enron.com), (5.2621103015001885E65,george@enron.com)))\n(ryan.slinger@enron.com,List((5.0579114162531735E65,anne.bertino@enron.com), (4.136838933824579E65,kgustafs@enron.com), (4.136838933824579E65,gweiss@enron.com), (4.0110663808847004E65,hill@enron.com), (3.8821438267917902E65,george@enron.com)))\n\nscala> people.join(graph.outDegrees).sortBy(_._2._2, ascending=false).map(x => (x._1, x._2._1)).take(10).toList.map(t => (t._2, top5(t._1).toList)).foreach(println)\n(jeff.dasovich@enron.com,List((2.8924566115596135E66,anne.bertino@enron.com), (2.3157345904446663E66,hill@enron.com), (2.2646318970030287E66,gweiss@enron.com), (2.2646318970030287E66,kgustafs@enron.com), (2.2385865127706285E66,fraser@enron.com)))\n(veronica.espinoza@enron.com,List((3.135142195254243E65,gweiss@enron.com), (3.135142195254243E65,kgustafs@enron.com), (2.773512892785554E65,anne.bertino@enron.com), (2.350799070225962E65,marcia.a.linton@enron.com), (2.2055288158758267E65,robert@enron.com)))\n(pete.davis@enron.com,List((5.773492048248794E66,louise.kitchen@enron.com), (5.067434612038159E66,john.lavorato@enron.com), (4.389028076992449E66,greg.whalley@enron.com), (4.1791711984241975E66,sally.beck@enron.com), (4.009544764149938E66,elizabeth.sager@enron.com)))\n(rhonda.denton@enron.com,List((2.834710591578977E68,louise.kitchen@enron.com), (2.488253676819922E68,john.lavorato@enron.com), (2.1516048969715738E68,greg.whalley@enron.com), (2.0405329247770104E68,sally.beck@enron.com), (1.9877213034021861E68,elizabeth.sager@enron.com)))\n(cheryl.johnson@enron.com,List((3.453167402163105E64,mary.dix@enron.com), (3.208849221485621E64,theresa.byrne@enron.com), (3.208849221485621E64,sandy.olofson@enron.com), (3.0374270093157086E64,hill@enron.com), (2.886581252384442E64,fraser@enron.com)))\n(susan.mara@enron.com,List((5.1729089729525785E66,anne.bertino@enron.com), (4.220843848723133E66,kgustafs@enron.com), (4.220843848723133E66,gweiss@enron.com), (4.1044435240204605E66,hill@enron.com), (3.9709951893268635E66,george@enron.com)))\n(jae.black@enron.com,List((2.513139130001457E65,anne.bertino@enron.com), (2.1037756300035247E65,hill@enron.com), (2.0297519350719265E65,fraser@enron.com), (1.9587139280519927E65,george@enron.com), (1.947164483486155E65,brad@enron.com)))\n(ginger.dernehl@enron.com,List((4.516267307013845E66,anne.bertino@enron.com), (3.653408921875843E66,gweiss@enron.com), (3.653408921875843E66,kgustafs@enron.com), (3.590298037045689E66,hill@enron.com), (3.471781765250177E66,fraser@enron.com)))\n(lorna.brennan@enron.com,List((2.0719309635087482E66,anne.bertino@enron.com), (1.732651408857978E66,kgustafs@enron.com), (1.732651408857978E66,gweiss@enron.com), (1.6348480059915056E66,hill@enron.com), (1.5880693846486309E66,george@enron.com)))\n(mary.hain@enron.com,List((5.596589595417286E66,anne.bertino@enron.com), (4.559474243930487E66,kgustafs@enron.com), (4.559474243930487E66,gweiss@enron.com), (4.4421474044331\n\n```", "```py\n# apt-get update\n...\n# apt-get install r-base r-base-dev\n...\n\n```", "```py\n# apt-cache search \"^r-.*\" | sort\n...\n\n```", "```py\n# apt-cache rdepends r-base-core\n\n```", "```py\n# apt-get install r-base-dev\n\n```", "```py\n> install.packages(\"ggplot2\")\n--- Please select a CRAN mirror for use in this session ---\nalso installing the dependencies 'stringi', 'magrittr', 'colorspace', 'Rcpp', 'stringr', 'RColorBrewer', 'dichromat', 'munsell', 'labeling', 'digest', 'gtable', 'plyr', 'reshape2', 'scales'\n\n```", "```py\n$ cat >> ~/.Rprofile << EOF\nr = getOption(\"repos\") # hard code the Berkeley repo for CRAN\nr[\"CRAN\"] = \"http://cran.cnr.berkeley.edu\"\noptions(repos = r)\nrm(r)\n\nEOF\n\n```", "```py\n$ export R_LIBS_SITE=${R_LIBS_SITE:-/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library}\n$ export R_LIBS_USER=${R_LIBS_USER:-$HOME/R/$(uname -i)-library/$( R --version | grep -o -E [0-9]+\\.[\n0-9]+ | head -1)}\n\n```", "```py\n$ pkgutil --check-signature R-3.2.3.pkg\nPackage \"R-3.2.3.pkg\":\n Status: signed by a certificate trusted by Mac OS X\n Certificate Chain:\n 1\\. Developer ID Installer: Simon Urbanek\n SHA1 fingerprint: B7 EB 39 5E 03 CF 1E 20 D1 A6 2E 9F D3 17 90 26 D8 D6 3B EF\n -----------------------------------------------------------------------------\n 2\\. Developer ID Certification Authority\n SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86\n -----------------------------------------------------------------------------\n 3\\. Apple Root CA\n SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60\n\n```", "```py\n$ git clone https://github.com/apache/spark.git\nCloning into 'spark'...\nremote: Counting objects: 301864, done.\n...\n$ cp –r R/{install-dev.sh,pkg) $SPARK_HOME/R\n...\n$ cd $SPARK_HOME\n$ ./R/install-dev.sh\n* installing *source* package 'SparkR' ...\n** R\n** inst\n** preparing package for lazy loading\nCreating a new generic function for 'colnames' in package 'SparkR'\n...\n$ bin/sparkR\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-redhat-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nLaunching java with spark-submit command /home/alex/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   \"sparkr-shell\" /tmp/RtmpgdTfmU/backend_port22446d0391e8 \n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /___/ .__/\\_,_/_/ /_/\\_\\   version  1.6.1 \n /_/ \n\n Spark context is available as sc, SQL context is available as sqlContext>\n\n```", "```py\nR> library(SparkR, lib.loc = c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\")))\n...\nR> sc <- sparkR.init(master = Sys.getenv(\"SPARK_MASTER\"), sparkEnvir = list(spark.driver.memory=\"1g\"))\n...\nR> sqlContext <- sparkRSQL.init(sc)\n\n```", "```py\n$ wget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_7.zip\n--2016-01-23 15:40:02--  http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_7.zip\nResolving www.transtats.bts.gov... 204.68.194.70\nConnecting to www.transtats.bts.gov|204.68.194.70|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 26204213 (25M) [application/x-zip-compressed]\nSaving to: \"On_Time_On_Time_Performance_2015_7.zip\"\n\n100%[====================================================================================================================================================================================>] 26,204,213   966K/s   in 27s \n\n2016-01-23 15:40:29 (956 KB/s) - \"On_Time_On_Time_Performance_2015_7.zip\" saved [26204213/26204213]\n\n$ unzip -d flights On_Time_On_Time_Performance_2015_7.zip\nArchive:  On_Time_On_Time_Performance_2015_7.zip\n inflating: flights/On_Time_On_Time_Performance_2015_7.csv \n inflating: flights/readme.html\n\n```", "```py\n$ hadoop fs –put flights .\n\n```", "```py\n$ bin/sparkR --master local[8]\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\n[Previously saved workspace restored]\n\nLaunching java with spark-submit command /Users/akozlov/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   \"--master\" \"local[8]\" \"sparkr-shell\" /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T//RtmpD42eTz/backend_port682e58e2c5db \n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /___/ .__/\\_,_/_/ /_/\\_\\   version  1.6.1 \n /_/ \n\n Spark context is available as sc, SQL context is available as sqlContext\n> flights <- read.table(unz(\"On_Time_On_Time_Performance_2015_7.zip\", \"On_Time_On_Time_Performance_2015_7.csv\"), nrows=1000000, header=T, quote=\"\\\"\", sep=\",\")\n> sfoFlights <- flights[flights$Dest == \"SFO\", ]\n> attach(sfoFlights)\n> delays <- aggregate(ArrDelayMinutes ~ DayOfWeek + Origin + UniqueCarrier, FUN=mean, na.rm=TRUE)\n> tail(delays[order(delays$ArrDelayMinutes), ])\n DayOfWeek Origin UniqueCarrier ArrDelayMinutes\n220         4    ABQ            OO           67.60\n489         4    TUS            OO           71.80\n186         5    IAH            F9           77.60\n696         3    RNO            UA           79.50\n491         6    TUS            OO          168.25\n84          7    SLC            AS          203.25\n\n```", "```py\n> sparkDf <- createDataFrame(sqlContext, flights)\n\n```", "```py\nsparkDf <- createDataFrame(sqlContext, subset(flights, select = c(\"ArrDelayMinutes\", \"DayOfWeek\", \"Origin\", \"Dest\", \"UniqueCarrier\")))\n\n```", "```py\n> rDf <- as.data.frame(sparkDf)\n\n```", "```py\n> $ ./bin/sparkR --packages com.databricks:spark-csv_2.10:1.3.0 --master local[8]\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-redhat-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nWarning: namespace 'SparkR' is not available and has been replaced\nby .GlobalEnv when processing object 'sparkDf'\n[Previously saved workspace restored]\n\nLaunching java with spark-submit command /home/alex/spark-1.6.1-bin-hadoop2.6/bin/spark-submit   \"--master\" \"local[8]\" \"--packages\" \"com.databricks:spark-csv_2.10:1.3.0\" \"sparkr-shell\" /tmp/RtmpfhcUXX/backend_port1b066bea5a03 \nIvy Default Cache set to: /home/alex/.ivy2/cache\nThe jars for the packages stored in: /home/alex/.ivy2/jars\n:: loading settings :: url = jar:file:/home/alex/spark-1.6.1-bin-hadoop2.6/lib/spark-assembly-1.6.1-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n confs: [default]\n found com.databricks#spark-csv_2.10;1.3.0 in central\n found org.apache.commons#commons-csv;1.1 in central\n found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 189ms :: artifacts dl 4ms\n :: modules in use:\n com.databricks#spark-csv_2.10;1.3.0 from central in [default]\n com.univocity#univocity-parsers;1.5.1 from central in [default]\n org.apache.commons#commons-csv;1.1 from central in [default]\n ---------------------------------------------------------------------\n |                  |            modules            ||   artifacts   |\n |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n ---------------------------------------------------------------------\n |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n confs: [default]\n 0 artifacts copied, 3 already retrieved (0kB/7ms)\n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /___/ .__/\\_,_/_/ /_/\\_\\   version  1.6.1 \n /_/ \n\n Spark context is available as sc, SQL context is available as sqlContext\n> sparkDf <- read.df(sqlContext, \"./flights\", \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"false\")\n> sfoFlights <- select(filter(sparkDf, sparkDf$Dest == \"SFO\"), \"DayOfWeek\", \"Origin\", \"UniqueCarrier\", \"ArrDelayMinutes\")\n> aggs <- agg(group_by(sfoFlights, \"DayOfWeek\", \"Origin\", \"UniqueCarrier\"), count(sparkDf$ArrDelayMinutes), avg(sparkDf$ArrDelayMinutes))\n> head(arrange(aggs, c('avg(ArrDelayMinutes)'), decreasing = TRUE), 10)\n DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes) \n1          7    SLC            AS                      4               203.25\n2          6    TUS            OO                      4               168.25\n3          3    RNO            UA                      8                79.50\n4          5    IAH            F9                      5                77.60\n5          4    TUS            OO                      5                71.80\n6          4    ABQ            OO                      5                67.60\n7          2    ABQ            OO                      4                66.25\n8          1    IAH            F9                      4                61.25\n9          4    DAL            WN                      5                59.20\n10         3    SUN            OO                      5                59.00\n\n```", "```py\n$ for i in $(seq 1 6); do wget http://www.transtats.bts.gov/Download/On_Time_On_Time_Performance_2015_$i.zip; unzip -d flights On_Time_On_Time_Performance_2015_$i.zip; hadoop fs -put -f flights/On_Time_On_Time_Performance_2015_$i.csv flights; done\n\n$ hadoop fs -ls flights\nFound 7 items\n-rw-r--r--   3 alex eng  211633432 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_1.csv\n-rw-r--r--   3 alex eng  192791767 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_2.csv\n-rw-r--r--   3 alex eng  227016932 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_3.csv\n-rw-r--r--   3 alex eng  218600030 2016-02-16 03:28 flights/On_Time_On_Time_Performance_2015_4.csv\n-rw-r--r--   3 alex eng  224003544 2016-02-16 03:29 flights/On_Time_On_Time_Performance_2015_5.csv\n-rw-r--r--   3 alex eng  227418780 2016-02-16 03:29 flights/On_Time_On_Time_Performance_2015_6.csv\n-rw-r--r--   3 alex eng  235037955 2016-02-15 21:56 flights/On_Time_On_Time_Performance_2015_7.csv\n\n```", "```py\n> sparkDf <- read.df(sqlContext, \"./flights\", \"com.databricks.spark.csv\", header=\"true\")\n> sfoFlights <- select(filter(sparkDf, sparkDf$Dest == \"SFO\"), \"DayOfWeek\", \"Origin\", \"UniqueCarrier\", \"ArrDelayMinutes\")\n> aggs <- cache(agg(group_by(sfoFlights, \"DayOfWeek\", \"Origin\", \"UniqueCarrier\"), count(sparkDf$ArrDelayMinutes), avg(sparkDf$ArrDelayMinutes)))\n> head(arrange(aggs, c('avg(ArrDelayMinutes)'), decreasing = TRUE), 10)\n DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes) \n1          6    MSP            UA                      1            122.00000\n2          3    RNO            UA                      8             79.50000\n3          1    MSP            UA                     13             68.53846\n4          7    SAT            UA                      1             65.00000\n5          7    STL            UA                      9             64.55556\n6          1    ORD            F9                     13             55.92308\n7          1    MSO            OO                      4             50.00000\n8          2    MSO            OO                      4             48.50000\n9          5    CEC            OO                     28             45.86957\n10         3    STL            UA                     13             43.46154\n\n```", "```py\n> head(arrange(filter(filter(aggs, aggs$Origin == \"SLC\"), aggs$UniqueCarrier == \"AS\"), c('avg(ArrDelayMinutes)'), decreasing = TRUE), 100)\n DayOfWeek Origin UniqueCarrier count(ArrDelayMinutes) avg(ArrDelayMinutes)\n1         7    SLC            AS                     30            32.600000\n2         2    SLC            AS                     30            10.200000\n3         4    SLC            AS                     31             9.774194\n4         1    SLC            AS                     30             9.433333\n5         3    SLC            AS                     30             5.866667\n6         5    SLC            AS                     31             5.516129\n7         6    SLC            AS                     30             2.133333\n\n```", "```py\nR> attach(iris)\nR> lm(Sepal.Length ~ Sepal.Width)\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width)\n\nCoefficients:\n(Intercept)  Sepal.Width\n 6.5262      -0.2234\n\n```", "```py\nR> model <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)\nR> summary(model)\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width)\n\nResiduals:\n Min       1Q   Median       3Q      Max \n-0.82816 -0.21989  0.01875  0.19709  0.84570 \n\nCoefficients:\n Estimate Std. Error t value Pr(>|t|) \n(Intercept)   1.85600    0.25078   7.401 9.85e-12 ***\nSepal.Width   0.65084    0.06665   9.765  < 2e-16 ***\nPetal.Length  0.70913    0.05672  12.502  < 2e-16 ***\nPetal.Width  -0.55648    0.12755  -4.363 2.41e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3145 on 146 degrees of freedom\nMultiple R-squared:  0.8586,  Adjusted R-squared:  0.8557 \nF-statistic: 295.5 on 3 and 146 DF,  p-value: < 2.2e-16\n\n```", "```py\nR> aov <- aov(Sepal.Length ~ Species)\nR> summary(aov)\n Df Sum Sq Mean Sq F value Pr(>F) \nSpecies       2  63.21  31.606   119.3 <2e-16 ***\nResiduals   147  38.96   0.265 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n```", "```py\nR> flights <- read.table(unz(\"On_Time_On_Time_Performance_2015_7.zip\", \"On_Time_On_Time_Performance_2015_7.csv\"), nrows=1000000, header=T, quote=\"\\\"\", sep=\",\")\nR> flights$DoW_ <- factor(flights$DayOfWeek,levels=c(1,2,3,4,5,6,7), labels=c(\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"))\nR> attach(flights)\nR> system.time(model <- glm(ArrDel15 ~ UniqueCarrier + DoW_ + Origin + Dest, flights, family=\"binomial\"))\n\n```", "```py\nsparkR> cache(sparkDf <- read.df(sqlContext, \"./flights\", \"com.databricks.spark.csv\", header=\"true\", inferSchema=\"true\"))\nDataFrame[Year:int, Quarter:int, Month:int, DayofMonth:int, DayOfWeek:int, FlightDate:string, UniqueCarrier:string, AirlineID:int, Carrier:string, TailNum:string, FlightNum:int, OriginAirportID:int, OriginAirportSeqID:int, OriginCityMarketID:int, Origin:string, OriginCityName:string, OriginState:string, OriginStateFips:int, OriginStateName:string, OriginWac:int, DestAirportID:int, DestAirportSeqID:int, DestCityMarketID:int, Dest:string, DestCityName:string, DestState:string, DestStateFips:int, DestStateName:string, DestWac:int, CRSDepTime:int, DepTime:int, DepDelay:double, DepDelayMinutes:double, DepDel15:double, DepartureDelayGroups:int, DepTimeBlk:string, TaxiOut:double, WheelsOff:int, WheelsOn:int, TaxiIn:double, CRSArrTime:int, ArrTime:int, ArrDelay:double, ArrDelayMinutes:double, ArrDel15:double, ArrivalDelayGroups:int, ArrTimeBlk:string, Cancelled:double, CancellationCode:string, Diverted:double, CRSElapsedTime:double, ActualElapsedTime:double, AirTime:double, Flights:double, Distance:double, DistanceGroup:int, CarrierDelay:double, WeatherDelay:double, NASDelay:double, SecurityDelay:double, LateAircraftDelay:double, FirstDepTime:int, TotalAddGTime:double, LongestAddGTime:double, DivAirportLandings:int, DivReachedDest:double, DivActualElapsedTime:double, DivArrDelay:double, DivDistance:double, Div1Airport:string, Div1AirportID:int, Div1AirportSeqID:int, Div1WheelsOn:int, Div1TotalGTime:double, Div1LongestGTime:double, Div1WheelsOff:int, Div1TailNum:string, Div2Airport:string, Div2AirportID:int, Div2AirportSeqID:int, Div2WheelsOn:int, Div2TotalGTime:double, Div2LongestGTime:double, Div2WheelsOff:string, Div2TailNum:string, Div3Airport:string, Div3AirportID:string, Div3AirportSeqID:string, Div3WheelsOn:string, Div3TotalGTime:string, Div3LongestGTime:string, Div3WheelsOff:string, Div3TailNum:string, Div4Airport:string, Div4AirportID:string, Div4AirportSeqID:string, Div4WheelsOn:string, Div4TotalGTime:string, Div4LongestGTime:string, Div4WheelsOff:string, Div4TailNum:string, Div5Airport:string, Div5AirportID:string, Div5AirportSeqID:string, Div5WheelsOn:string, Div5TotalGTime:string, Div5LongestGTime:string, Div5WheelsOff:string, Div5TailNum:string, :string]\nsparkR> noNulls <- cache(dropna(selectExpr(filter(sparkDf, sparkDf$Cancelled == 0), \"ArrDel15\", \"UniqueCarrier\", \"format_string('%d', DayOfWeek) as DayOfWeek\", \"Origin\", \"Dest\"), \"any\"))\nsparkR> sparkModel = glm(ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest, noNulls, family=\"binomial\")\n\n```", "```py\n> summary(sparkModel)\n$coefficients\n Estimate\n(Intercept)      -1.518542340\nUniqueCarrier_WN  0.382722232\nUniqueCarrier_DL -0.047997652\nUniqueCarrier_OO  0.367031995\nUniqueCarrier_AA  0.046737727\nUniqueCarrier_EV  0.344539788\nUniqueCarrier_UA  0.299290120\nUniqueCarrier_US  0.069837542\nUniqueCarrier_MQ  0.467597761\nUniqueCarrier_B6  0.326240578\nUniqueCarrier_AS -0.210762769\nUniqueCarrier_NK  0.841185903\nUniqueCarrier_F9  0.788720078\nUniqueCarrier_HA -0.094638586\nDayOfWeek_5       0.232234937\nDayOfWeek_4       0.274016179\nDayOfWeek_3       0.147645473\nDayOfWeek_1       0.347349366\nDayOfWeek_2       0.190157420\nDayOfWeek_7       0.199774806\nOrigin_ATL       -0.180512251\n...\n\n```", "```py\nR> summary(model)\n\nCall:\nglm(formula = ArrDel15 ~ UniqueCarrier + DoW + Origin + Dest, \n family = \"binomial\", data = dow)\n\nDeviance Residuals: \n Min       1Q   Median       3Q      Max \n-1.4205  -0.7274  -0.6132  -0.4510   2.9414 \n\nCoefficients:\n Estimate Std. Error z value Pr(>|z|) \n(Intercept)     -1.817e+00  2.402e-01  -7.563 3.95e-14 ***\nUniqueCarrierAS -3.296e-01  3.413e-02  -9.658  < 2e-16 ***\nUniqueCarrierB6  3.932e-01  2.358e-02  16.676  < 2e-16 ***\nUniqueCarrierDL -6.602e-02  1.850e-02  -3.568 0.000359 ***\nUniqueCarrierEV  3.174e-01  2.155e-02  14.728  < 2e-16 ***\nUniqueCarrierF9  6.754e-01  2.979e-02  22.668  < 2e-16 ***\nUniqueCarrierHA  7.883e-02  7.058e-02   1.117 0.264066 \nUniqueCarrierMQ  2.175e-01  2.393e-02   9.090  < 2e-16 ***\nUniqueCarrierNK  7.928e-01  2.702e-02  29.343  < 2e-16 ***\nUniqueCarrierOO  4.001e-01  2.019e-02  19.817  < 2e-16 ***\nUniqueCarrierUA  3.982e-01  1.827e-02  21.795  < 2e-16 ***\nUniqueCarrierVX  9.723e-02  3.690e-02   2.635 0.008423 ** \nUniqueCarrierWN  6.358e-01  1.700e-02  37.406  < 2e-16 ***\ndowTue           1.365e-01  1.313e-02  10.395  < 2e-16 ***\ndowWed           1.724e-01  1.242e-02  13.877  < 2e-16 ***\ndowThu           4.593e-02  1.256e-02   3.656 0.000256 ***\ndowFri          -2.338e-01  1.311e-02 -17.837  < 2e-16 ***\ndowSat          -2.413e-01  1.458e-02 -16.556  < 2e-16 ***\ndowSun          -3.028e-01  1.408e-02 -21.511  < 2e-16 ***\nOriginABI       -3.355e-01  2.554e-01  -1.314 0.188965 \n...\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ cat examples/src/main/resources/people.json \n{\"name\":\"Michael\"}\n{\"name\":\"Andy\", \"age\":30}\n{\"name\":\"Justin\", \"age\":19}\n\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ bin/sparkR\n...\n\n> people = read.json(sqlContext, \"examples/src/main/resources/people.json\")\n> dtypes(people)\n[[1]]\n[1] \"age\"    \"bigint\"\n\n[[2]]\n[1] \"name\"   \"string\"\n\n> schema(people)\nStructType\n|-name = \"age\", type = \"LongType\", nullable = TRUE\n|-name = \"name\", type = \"StringType\", nullable = TRUE\n> showDF(people)\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n```", "```py\n> write.parquet(sparkDf, \"parquet\")\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ ls –l On_Time_On_Time_Performance_2015_7.zip parquet/ flights/\n-rw-r--r--  1 akozlov  staff  26204213 Sep  9 12:21 /Users/akozlov/spark/On_Time_On_Time_Performance_2015_7.zip\n\nflights/:\ntotal 459088\n-rw-r--r--  1 akozlov  staff  235037955 Sep  9 12:20 On_Time_On_Time_Performance_2015_7.csv\n-rw-r--r--  1 akozlov  staff      12054 Sep  9 12:20 readme.html\n\nparquet/:\ntotal 848\n-rw-r--r--  1 akozlov  staff       0 Jan 24 22:50 _SUCCESS\n-rw-r--r--  1 akozlov  staff   10000 Jan 24 22:50 _common_metadata\n-rw-r--r--  1 akozlov  staff   23498 Jan 24 22:50 _metadata\n-rw-r--r--  1 akozlov  staff  394418 Jan 24 22:50 part-r-00000-9e2d0004-c71f-4bf5-aafe-90822f9d7223.gz.parquet\n\n```", "```py\nR> scala <- scalaInterpreter()\nR> scala %~% 'def pri(i: Stream[Int]): Stream[Int] = i.head #:: pri(i.tail filter  { x => { println(\"Evaluating \" + x + \"%\" + i.head); x % i.head != 0 } } )'\nScalaInterpreterReference... engine: javax.script.ScriptEngine\nR> scala %~% 'val primes = pri(Stream.from(2))'\nScalaInterpreterReference... primes: Stream[Int]\nR> scala %~% 'primes take 5 foreach println'\n2\nEvaluating 3%2\n3\nEvaluating 4%2\nEvaluating 5%2\nEvaluating 5%3\n5\nEvaluating 6%2\nEvaluating 7%2\nEvaluating 7%3\nEvaluating 7%5\n7\nEvaluating 8%2\nEvaluating 9%2\nEvaluating 9%3\nEvaluating 10%2\nEvaluating 11%2\nEvaluating 11%3\nEvaluating 11%5\nEvaluating 11%7\n11\nR> scala %~% 'primes take 5 foreach println'\n2\n3\n5\n7\n11\nR> scala %~% 'primes take 7 foreach println'\n2\n3\n5\n7\n11\nEvaluating 12%2\nEvaluating 13%2\nEvaluating 13%3\nEvaluating 13%5\nEvaluating 13%7\nEvaluating 13%11\n13\nEvaluating 14%2\nEvaluating 15%2\nEvaluating 15%3\nEvaluating 16%2\nEvaluating 17%2\nEvaluating 17%3\nEvaluating 17%5\nEvaluating 17%7\nEvaluating 17%11\nEvaluating 17%13\n17\nR> \n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro ~]$ cat << EOF > rdate.R\n> #!/usr/local/bin/Rscript\n> \n> write(date(), stdout())\n> EOF\n[akozlov@Alexanders-MacBook-Pro ~]$ chmod a+x rdate.R\n[akozlov@Alexanders-MacBook-Pro ~]$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import sys.process._\nimport sys.process._\n\nscala> val date = Process(Seq(\"./rdate.R\")).!!\ndate: String =\n\"Wed Feb 24 02:20:09 2016\n\"\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro ~]$ wget http://www.rforge.net/Rserve/snapshot/Rserve_1.8-5.tar.gz\n\n[akozlov@Alexanders-MacBook-Pro ~]$ R CMD INSTALL Rserve_1.8-5.tar\n.gz\n...\n[akozlov@Alexanders-MacBook-Pro ~]$ R CMD INSTALL Rserve_1.8-5.tar.gz\n\n[akozlov@Alexanders-MacBook-Pro ~]$ $ R -q CMD Rserve\n\nR version 3.2.3 (2015-12-10) -- \"Wooden Christmas-Tree\"\nCopyright (C) 2015 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin13.4.0 (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nRserv started in daemon mode.\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark(master)]$ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n==> This script will install:\n/usr/local/bin/brew\n/usr/local/Library/...\n/usr/local/share/man/man1/brew.1\n…\n[akozlov@Alexanders-MacBook-Pro spark(master)]$ brew install python\n…\n\n```", "```py\n$ export PYTHON_VERSION=2.7.11\n$ wget -O - https://www.python.org/ftp/python/$PYTHON_VERSION/Python-$PYTHON_VERSION.tgz | tar xzvf -\n$ cd $HOME/Python-$PYTHON_VERSION\n$ ./configure--prefix=/usr/local --enable-unicode=ucs4--enable-shared LDFLAGS=\"-Wl,-rpath /usr/local/lib\"\n$ make; sudo make altinstall\n$ sudo ln -sf /usr/local/bin/python2.7 /usr/local/bin/python\n\n```", "```py\n$ wget https://bootstrap.pypa.io/ez_setup.py\n$ sudo /usr/local/bin/python ez_setup.py\n$ sudo /usr/local/bin/easy_install-2.7 pip\n$ sudo /usr/local/bin/pip install --upgrade avro nose numpy scipy pandas statsmodels scikit-learn iso8601 python-dateutil python-snappy\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ export PYSPARK_PYTHON=/usr/local/bin/python\n[akozlov@Alexanders-MacBook-Pro spark-1.6.1-bin-hadoop2.6]$ bin/pyspark \nPython 2.7.11 (default, Jan 23 2016, 20:14:24) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Python version 2.7.11 (default, Jan 23 2016 20:14:24)\nSparkContext available as sc, HiveContext available as sqlContext.\n>>>\n\n```", "```py\n>>> sfoFlights = sqlContext.sql(\"SELECT Dest, UniqueCarrier, ArrDelayMinutes FROM parquet.parquet\")\n>>> sfoFlights.groupBy([\"Dest\", \"UniqueCarrier\"]).agg(func.avg(\"ArrDelayMinutes\"), func.count(\"ArrDelayMinutes\")).sort(\"avg(ArrDelayMinutes)\", ascending=False).head(5)\n[Row(Dest=u'HNL', UniqueCarrier=u'HA', avg(ArrDelayMinutes)=53.70967741935484, count(ArrDelayMinutes)=31), Row(Dest=u'IAH', UniqueCarrier=u'F9', avg(ArrDelayMinutes)=43.064516129032256, count(ArrDelayMinutes)=31), Row(Dest=u'LAX', UniqueCarrier=u'DL', avg(ArrDelayMinutes)=39.68691588785047, count(ArrDelayMinutes)=214), Row(Dest=u'LAX', UniqueCarrier=u'WN', avg(ArrDelayMinutes)=29.704453441295545, count(ArrDelayMinutes)=247), Row(Dest=u'MSO', UniqueCarrier=u'OO', avg(ArrDelayMinutes)=29.551724137931036, count(ArrDelayMinutes)=29)]\n\n```", "```py\nscala> import sys.process._\nimport sys.process._\n\nscala> val retCode = Process(Seq(\"/usr/local/bin/python\", \"-c\", \"import socket; print(socket.gethostname())\")).!\nAlexanders-MacBook-Pro.local\nretCode: Int = 0\n\nscala> val lines = Process(Seq(\"/usr/local/bin/python\", \"-c\", \"\"\"from datetime import datetime, timedelta; print(\"Yesterday was {}\".format(datetime.now()-timedelta(days=1)))\"\"\")).!!\nlines: String =\n\"Yesterday was 2016-02-12 16:24:53.161853\n\"\n\n```", "```py\n#!/usr/bin/env python\n\nimport sys\nimport os\nimport re\n\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.linalg import svd\n\nnp.set_printoptions(linewidth=10000)\n\ndef process_line(input):\n    inp = input.rstrip(\"\\r\\n\")\n    if len(inp) > 1:\n        try:\n            (mat, rank) = inp.split(\"|\")\n            a = np.matrix(mat)\n            r = int(rank)\n        except:\n            a = np.matrix(inp)\n            r = 1\n        U, s, Vh = linalg.svd(a, full_matrices=False)\n        for i in xrange(r, s.size):\n            s[i] = 0\n        S = linalg.diagsvd(s, s.size, s.size)\n        print(str(np.dot(U, np.dot(S, Vh))).replace(os.linesep, \";\"))\n\nif __name__ == '__main__':\n    map(process_line, sys.stdin)\n```", "```py\n$ echo -e \"1,2,3;2,1,2;3,2,1;7,8,9|3\" | ./svd.py\n[[ 1\\.  2\\.  3.]; [ 2\\.  1\\.  2.]; [ 3\\.  2\\.  1.]; [ 7\\.  8\\.  9.]]\n\n```", "```py\nscala> implicit class RunCommand(command: String) {\n |   def #<<< (input: String)(implicit buffer: StringBuilder) =  {\n |     val process = Process(command)\n |     val io = new ProcessIO (\n |       in  => { in.write(input getBytes \"UTF-8\"); in.close},\n |       out => { buffer append scala.io.Source.fromInputStream(out).getLines.mkString(\"\\n\"); buffer.append(\"\\n\"); out.close() },\n |       err => { scala.io.Source.fromInputStream(err).getLines().foreach(System.err.println) })\n |     (process run io).exitValue\n |   }\n | }\ndefined class RunCommand\n\n```", "```py\nscala> implicit val buffer = new StringBuilder()\nbuffer: StringBuilder =\n\nscala> if (\"./svd.py\" #<<< \"1,2,3;2,1,2;3,2,1;7,8,9|1\" == 0)  Some(buffer.toString) else None\nres77: Option[String] = Some([[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]])\n\n```", "```py\nscala> if (\"./svd.py\" #<<< \"\"\"\n | 1,2,3;2,1,2;3,2,1;7,8,9|0\n | 1,2,3;2,1,2;3,2,1;7,8,9|1\n | 1,2,3;2,1,2;3,2,1;7,8,9|2\n | 1,2,3;2,1,2;3,2,1;7,8,9|3\"\"\" == 0) Some(buffer.toString) else None\nres80: Option[String] =\nSome([[ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]]\n[[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]]\n[[ 0.9905897   2.02161614  2.98849663]; [ 1.72361156  1.63488399  1.66213642]; [ 3.04783513  1.89011928  1.05847477]; [ 7.04822694  7.88921926  9.05895373]]\n[[ 1\\.  2\\.  3.]; [ 2\\.  1\\.  2.]; [ 3\\.  2\\.  1.]; [ 7\\.  8\\.  9.]])\n\n```", "```py\nscala> sc.parallelize(List(\"1,2,3;2,1,2;3,2,1;7,8,9|0\", \"1,2,3;2,1,2;3,2,1;7,8,9|1\", \"1,2,3;2,1,2;3,2,1;7,8,9|2\", \"1,2,3;2,1,2;3,2,1;7,8,9|3\"),4).pipe(\"./svd.py\").collect.foreach(println)\n[[ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]; [ 0\\.  0\\.  0.]]\n[[ 1.84716691  2.02576751  2.29557674]; [ 1.48971176  1.63375041  1.85134741]; [ 1.71759947  1.88367234  2.13455611]; [ 7.19431647  7.88992728  8.94077601]]\n[[ 0.9905897   2.02161614  2.98849663]; [ 1.72361156  1.63488399  1.66213642]; [ 3.04783513  1.89011928  1.05847477]; [ 7.04822694  7.88921926  9.05895373]]\n[[ 1\\.  2\\.  3.]; [ 2\\.  1\\.  2.]; [ 3\\.  2\\.  1.]; [ 7\\.  8\\.  9.]]\n\n```", "```py\n$ wget -O jython-standalone-2.7.0.jar http://search.maven.org/remotecontent?filepath=org/python/jython-standalone/2.7.0/jython-standalone-2.7.0.jar\n\n[akozlov@Alexanders-MacBook-Pro Scala]$ scala -cp jython-standalone-2.7.0.jar \nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import javax.script.ScriptEngine;\n...\nscala> import javax.script.ScriptEngineManager;\n...\nscala> import javax.script.ScriptException;\n...\nscala> val manager = new ScriptEngineManager();\nmanager: javax.script.ScriptEngineManager = javax.script.ScriptEngineManager@3a03464\n\nscala> val engines = manager.getEngineFactories();\nengines: java.util.List[javax.script.ScriptEngineFactory] = [org.python.jsr223.PyScriptEngineFactory@4909b8da, jdk.nashorn.api.scripting.NashornScriptEngineFactory@68837a77, scala.tools.nsc.interpreter.IMain$Factory@1324409e]\n\n```", "```py\nscala> val engine = new ScriptEngineManager().getEngineByName(\"jython\");\nengine: javax.script.ScriptEngine = org.python.jsr223.PyScriptEngine@6094de13\n\nscala> engine.eval(\"from datetime import datetime, timedelta; yesterday = str(datetime.now()-timedelta(days=1))\")\nres15: Object = null\n\nscala> engine.get(\"yesterday\")\nres16: Object = 2016-02-12 23:26:38.012000\n\n```", "```py\nscala> val startTime = System.nanoTime\nstartTime: Long = 54384084381087\n\nscala> for (i <- 1 to 100) {\n |   engine.eval(\"from datetime import datetime, timedelta; yesterday = str(datetime.now()-timedelta(days=1))\")\n |   val yesterday = engine.get(\"yesterday\")\n | }\n\nscala> val elapsed = 1e-9 * (System.nanoTime - startTime)\nelapsed: Double = 0.270837934\n\nscala> val startTime = System.nanoTime\nstartTime: Long = 54391560460133\n\nscala> for (i <- 1 to 100) {\n |   val yesterday = Process(Seq(\"/usr/local/bin/python\", \"-c\", \"\"\"from datetime import datetime, timedelta; print(datetime.now()-timedelta(days=1))\"\"\")).!!\n | }\n\nscala> val elapsed = 1e-9 * (System.nanoTime - startTime)\nelapsed: Double = 2.221937263\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  ''_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> val leotolstoy = sc.textFile(\"leotolstoy\").cache\nleotolstoy: org.apache.spark.rdd.RDD[String] = leotolstoy MapPartitionsRDD[1] at textFile at <console>:27\n\nscala> leotolstoy.flatMap(_.split(\"\\\\W+\")).count\nres1: Long = 1318234 \n\nscala> val shakespeare = sc.textFile(\"shakespeare\").cache\nshakespeare: org.apache.spark.rdd.RDD[String] = shakespeare MapPartitionsRDD[7] at textFile at <console>:27\n\nscala> shakespeare.flatMap(_.split(\"\\\\W+\")).count\nres2: Long = 1051958\n\n```", "```py\nscala> :silent\n\nscala> val shakespeareBag = shakespeare.flatMap(_.split(\"\\\\W+\")).map(_.toLowerCase).distinct\n\nscala> val leotolstoyBag = leotolstoy.flatMap(_.split(\"\\\\W+\")).map(_.toLowerCase).distinct\nleotolstoyBag: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[27] at map at <console>:29\n\nscala> println(\"The bags intersection is \" + leotolstoyBag.intersection(shakespeareBag).count)\nThe bags intersection is 11552\n\n```", "```py\n$ (mkdir bible; cd bible; wget http://www.gutenberg.org/cache/epub/10/pg10.txt)\n\nscala> val bible = sc.textFile(\"bible\").cache\n\nscala> val bibleBag = bible.flatMap(_.split(\"\\\\W+\")).map(_.toLowerCase).distinct\n\nscala>:silent\n\nscala> bibleBag.intersection(shakespeareBag).count\nres5: Long = 7250\n\nscala> bibleBag.intersection(leotolstoyBag).count\nres24: Long = 6611\n\n```", "```py\n$ (mkdir chekhov; cd chekhov;\n wget http://www.gutenberg.org/cache/epub/7986/pg7986.txt\n wget http://www.gutenberg.org/cache/epub/1756/pg1756.txt\n wget http://www.gutenberg.org/cache/epub/1754/1754.txt\n wget http://www.gutenberg.org/cache/epub/13415/pg13415.txt)\n\nscala> val chekhov = sc.textFile(\"chekhov\").cache\nchekhov: org.apache.spark.rdd.RDD[String] = chekhov MapPartitionsRDD[61] at textFile at <console>:27\n\nscala> val chekhovBag = chekhov.flatMap(_.split(\"\\\\W+\")).map(_.toLowerCase).distinct\nchekhovBag: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[66] at distinct at <console>:29\n\nscala> chekhovBag.intersection(leotolstoyBag).count\nres8: Long = 8263\n\nscala> chekhovBag.intersection(shakespeareBag).count\nres9: Long = 6457 \n\n```", "```py\ndef main(args: Array[String]) {\n\n    val stemmer = new Stemmer\n\n    val conf = new SparkConf().\n      setAppName(\"Stemmer\").\n      setMaster(args(0))\n\n    val sc = new SparkContext(conf)\n\n    val stopwords = scala.collection.immutable.TreeSet(\n      \"\", \"i\", \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"from\", \"had\", \"has\", \"he\", \"her\", \"him\", \"his\", \"in\", \"is\", \"it\", \"its\", \"my\", \"not\", \"of\", \"on\", \"she\", \"that\", \"the\", \"to\", \"was\", \"were\", \"will\", \"with\", \"you\"\n    ) map { stemmer.stem(_) }\n\n    val bags = for (name <- args.slice(1, args.length)) yield {\n      val rdd = sc.textFile(name).map(_.toLowerCase)\n      if (name == \"nytimes\" || name == \"nips\" || name == \"enron\")\n        rdd.filter(!_.startsWith(\"zzz_\")).flatMap(_.split(\"_\")).map(stemmer.stem(_)).distinct.filter(!stopwords.contains(_)).cache\n      else {\n        val withCounts = rdd.flatMap(_.split(\"\\\\W+\")).map(stemmer.stem(_)).filter(!stopwords.contains(_)).map((_, 1)).reduceByKey(_+_)\n        val minCount = scala.math.max(1L, 0.0001 * withCounts.count.toLong)\n        withCounts.filter(_._2 > minCount).map(_._1).cache\n      }\n    }\n\n    val cntRoots = (0 until { args.length - 1 }).map(i => Math.sqrt(bags(i).count.toDouble))\n\n    for(l <- 0 until { args.length - 1 }; r <- l until { args.length - 1 }) {\n      val cnt = bags(l).intersection(bags(r)).count\n      println(\"The intersect \" + args(l+1) + \" x \" + args(r+1) + \" is: \" + cnt + \" (\" + (cnt.toDouble / cntRoots(l) / cntRoots(r)) + \")\")\n    }\n\n    sc.stop\n    }\n}\n```", "```py\n$ sbt \"run-main org.akozlov.examples.Stemmer local[2] shakespeare leotolstoy chekhov nytimes nips enron bible\"\n[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter09/project\n[info] Set current project to NLP in Scala (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter09/)\n[info] Running org.akozlov.examples.Stemmer local[2] shakespeare leotolstoy chekhov nytimes nips enron bible\nThe intersect shakespeare x shakespeare is: 10533 (1.0)\nThe intersect shakespeare x leotolstoy is: 5834 (0.5293670391596142)\nThe intersect shakespeare x chekhov is: 3295 (0.4715281914492153)\nThe intersect shakespeare x nytimes is: 7207 (0.4163369701270161)\nThe intersect shakespeare x nips is: 2726 (0.27457329089479504)\nThe intersect shakespeare x enron is: 5217 (0.34431535832271265)\nThe intersect shakespeare x bible is: 3826 (0.45171392986714726)\nThe intersect leotolstoy x leotolstoy is: 11531 (0.9999999999999999)\nThe intersect leotolstoy x chekhov is: 4099 (0.5606253333241973)\nThe intersect leotolstoy x nytimes is: 8657 (0.47796976891152176)\nThe intersect leotolstoy x nips is: 3231 (0.3110369262979765)\nThe intersect leotolstoy x enron is: 6076 (0.38326210407266764)\nThe intersect leotolstoy x bible is: 3455 (0.3898604013063757)\nThe intersect chekhov x chekhov is: 4636 (1.0)\nThe intersect chekhov x nytimes is: 3843 (0.33463022711780555)\nThe intersect chekhov x nips is: 1889 (0.28679311682962116)\nThe intersect chekhov x enron is: 3213 (0.31963226496874225)\nThe intersect chekhov x bible is: 2282 (0.40610513998395287)\nThe intersect nytimes x nytimes is: 28449 (1.0)\nThe intersect nytimes x nips is: 4954 (0.30362042173997206)\nThe intersect nytimes x enron is: 11273 (0.45270741164576034)\nThe intersect nytimes x bible is: 3655 (0.2625720159205085)\nThe intersect nips x nips is: 9358 (1.0000000000000002)\nThe intersect nips x enron is: 4888 (0.3422561629856124)\nThe intersect nips x bible is: 1615 (0.20229053645165143)\nThe intersect enron x enron is: 21796 (1.0)\nThe intersect enron x bible is: 2895 (0.23760453654690084)\nThe intersect bible x bible is: 6811 (1.0)\n[success] Total time: 12 s, completed May 17, 2016 11:00:38 PM\n\n```", "```py\nscala> val bags = for (name <- List(\"shakespeare\", \"leotolstoy\", \"chekhov\", \"nytimes\", \"enron\", \"bible\")) yield {\n |     sc textFile(name) flatMap { _.split(\"\\\\W+\") } map { _.toLowerCase } map { stemmer.stem(_) } filter { ! stopwords.contains(_) } cache()\n | }\nbags: List[org.apache.spark.rdd.RDD[String]] = List(MapPartitionsRDD[93] at filter at <console>:36, MapPartitionsRDD[98] at filter at <console>:36, MapPartitionsRDD[103] at filter at <console>:36, MapPartitionsRDD[108] at filter at <console>:36, MapPartitionsRDD[113] at filter at <console>:36, MapPartitionsRDD[118] at filter at <console>:36)\n\nscala> bags reduceLeft { (a, b) => a.union(b) } map { (_, 1) } reduceByKey { _+_ } collect() sortBy(- _._2) map { x => scala.math.log(x._2) }\nres18: Array[Double] = Array(10.27759958298627, 10.1152465449837, 10.058652004037477, 10.046635061754612, 9.999615579630348, 9.855399641729074, 9.834405391348684, 9.801233318497372, 9.792667717430884, 9.76347807952779, 9.742496866444002, 9.655474810542554, 9.630365631415676, 9.623244409181346, 9.593355351246755, 9.517604459155686, 9.515837804297965, 9.47231994707559, 9.45930760329985, 9.441531454869693, 9.435561763085358, 9.426257878198653, 9.378985497953893, 9.355997944398545, 9.34862295977619, 9.300820725104558, 9.25569607369698, 9.25320827220336, 9.229162126216771, 9.20391980417326, 9.19917830726999, 9.167224080902555, 9.153875834995056, 9.137877200242468, 9.129889247578555, 9.090430075303626, 9.090091799380007, 9.083075020930307, 9.077722847361343, 9.070273383079064, 9.0542711863262...\n...\n\n```", "```py\nscala> import org.apache.spark.mllib.feature.HashingTF\nimport org.apache.spark.mllib.feature.HashingTF\n\nscala> import org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vector\n\nscala> val hashingTF = new HashingTF\nhashingTF: org.apache.spark.mllib.feature.HashingTF = org.apache.spark.mllib.feature.HashingTF@61b975f7\n\nscala> val documents: RDD[Seq[String]] = sc.textFile(\"shakepeare\").map(_.split(\"\\\\W+\").toSeq)\ndocuments: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[263] at map at <console>:34\n\nscala> val tf = hashingTF transform documents\ntf: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[264] at map at HashingTF.scala:76\n\n```", "```py\nscala> tf.cache\nres26: tf.type = MapPartitionsRDD[268] at map at HashingTF.scala:76\n\nscala> import org.apache.spark.mllib.feature.IDF\nimport org.apache.spark.mllib.feature.IDF\n\nscala> val idf = new IDF(minDocFreq = 2) fit tf\nidf: org.apache.spark.mllib.feature.IDFModel = org.apache.spark.mllib.feature.IDFModel@514bda2d\n\nscala> val tfidf = idf transform tf\ntfidf: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = MapPartitionsRDD[272] at mapPartitions at IDF.scala:178\n\nscala> tfidf take(10) foreach println\n(1048576,[3159,3543,84049,582393,787662,838279,928610,961626,1021219,1021273],[3.9626355004005083,4.556357737874695,8.380602528651274,8.157736974683708,11.513471982269106,9.316247404932888,10.666174121881904,11.513471982269106,8.07948477778396,11.002646358503116])\n(1048576,[267794,1021219],[8.783442874448122,8.07948477778396])\n(1048576,[0],[0.5688129477150906])\n(1048576,[3123,3370,3521,3543,96727,101577,114801,116103,497275,504006,508606,843002,962509,980206],[4.207164322003765,2.9674322162952897,4.125144122691999,2.2781788689373474,2.132236195047438,3.2951341639027754,1.9204575904855747,6.318664992090735,11.002646358503116,3.1043838099579815,5.451238364272918,11.002646358503116,8.43769700104158,10.30949917794317])\n(1048576,[0,3371,3521,3555,27409,89087,104545,107877,552624,735790,910062,943655,962421],[0.5688129477150906,3.442878442319589,4.125144122691999,4.462482535201062,5.023254392629403,5.160262034409286,5.646060083831103,4.712188947797486,11.002646358503116,7.006282204641219,6.216822672821767,11.513471982269106,8.898512204232908])\n(1048576,[3371,3543,82108,114801,149895,279256,582393,597025,838279,915181],[3.442878442319589,2.2781788689373474,6.017670811187438,3.8409151809711495,7.893585399642122,6.625632265652778,8.157736974683708,10.414859693600997,9.316247404932888,11.513471982269106])\n(1048576,[3123,3555,413342,504006,690950,702035,980206],[4.207164322003765,4.462482535201062,3.4399651117812313,3.1043838099579815,11.513471982269106,11.002646358503116,10.30949917794317])\n(1048576,[0],[0.5688129477150906])\n(1048576,[97,1344,3370,100898,105489,508606,582393,736902,838279,1026302],[2.533299776544098,23.026943964538212,2.9674322162952897,0.0,11.225789909817326,5.451238364272918,8.157736974683708,10.30949917794317,9.316247404932888,11.513471982269106])\n(1048576,[0,1344,3365,114801,327690,357319,413342,692611,867249,965170],[4.550503581720725,23.026943964538212,2.7455719545259836,1.9204575904855747,8.268278849083533,9.521041817578901,3.4399651117812313,0.0,6.661441718349489,0.0])\n\n```", "```py\n$ mkdir enron\n$ cat /dev/null > enron/all.txt\n$ for f in $(find maildir -name \\*\\. -print); do cat $f | sed '1,/^$/d;/^$/d' | tr \"\\n\\r\" \"  \" >> enron/all.txt; echo \"\" >> enron/all.txt; done\n$\n\n```", "```py\n$ spark-shell --driver-memory 8g --executor-memory 8g --packages com.github.fommil.netlib:all:1.1.2\nIvy Default Cache set to: /home/alex/.ivy2/cache\nThe jars for the packages stored in: /home/alex/.ivy2/jars\n:: loading settings :: url = jar:file:/opt/cloudera/parcels/CDH-5.5.2-1.cdh5.5.2.p0.4/jars/spark-assembly-1.5.0-cdh5.5.2-hadoop2.6.0-cdh5.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.github.fommil.netlib#all added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n confs: [default]\n found com.github.fommil.netlib#all;1.1.2 in central\n found net.sourceforge.f2j#arpack_combined_all;0.1 in central\n found com.github.fommil.netlib#core;1.1.2 in central\n found com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 in central\n found com.github.fommil.netlib#native_ref-java;1.1 in central\n found com.github.fommil#jniloader;1.1 in central\n found com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 in central\n found com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 in central\n found com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 in central\n found com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 in central\n found com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 in central\n found com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 in central\n found com.github.fommil.netlib#native_system-java;1.1 in central\n found com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 in central\n found com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 in central\n found com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 in central\n found com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 in central\n found com.github.fommil.netlib#netlib-native_system-win-i686;1.1 in central\ndownloading https://repo1.maven.org/maven2/net/sourceforge/f2j/arpack_combined_all/0.1/arpack_combined_all-0.1-javadoc.jar ...\n [SUCCESSFUL ] net.sourceforge.f2j#arpack_combined_all;0.1!arpack_combined_all.jar (513ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/core/1.1.2/core-1.1.2.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#core;1.1.2!core.jar (18ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-osx-x86_64/1.1/netlib-native_ref-osx-x86_64-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1!netlib-native_ref-osx-x86_64.jar (167ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-x86_64/1.1/netlib-native_ref-linux-x86_64-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1!netlib-native_ref-linux-x86_64.jar (159ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-i686/1.1/netlib-native_ref-linux-i686-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1!netlib-native_ref-linux-i686.jar (131ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-win-x86_64/1.1/netlib-native_ref-win-x86_64-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1!netlib-native_ref-win-x86_64.jar (210ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-win-i686/1.1/netlib-native_ref-win-i686-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-win-i686;1.1!netlib-native_ref-win-i686.jar (167ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_ref-linux-armhf/1.1/netlib-native_ref-linux-armhf-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1!netlib-native_ref-linux-armhf.jar (110ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-osx-x86_64/1.1/netlib-native_system-osx-x86_64-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1!netlib-native_system-osx-x86_64.jar (54ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-x86_64/1.1/netlib-native_system-linux-x86_64-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1!netlib-native_system-linux-x86_64.jar (47ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-i686/1.1/netlib-native_system-linux-i686-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-i686;1.1!netlib-native_system-linux-i686.jar (44ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-linux-armhf/1.1/netlib-native_system-linux-armhf-1.1-natives.jar ...\n[SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1!netlib-native_system-linux-armhf.jar (35ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-win-x86_64/1.1/netlib-native_system-win-x86_64-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1!netlib-native_system-win-x86_64.jar (62ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/netlib-native_system-win-i686/1.1/netlib-native_system-win-i686-1.1-natives.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#netlib-native_system-win-i686;1.1!netlib-native_system-win-i686.jar (55ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/native_ref-java/1.1/native_ref-java-1.1.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#native_ref-java;1.1!native_ref-java.jar (24ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/jniloader/1.1/jniloader-1.1.jar ...\n [SUCCESSFUL ] com.github.fommil#jniloader;1.1!jniloader.jar (3ms)\ndownloading https://repo1.maven.org/maven2/com/github/fommil/netlib/native_system-java/1.1/native_system-java-1.1.jar ...\n [SUCCESSFUL ] com.github.fommil.netlib#native_system-java;1.1!native_system-java.jar (7ms)\n:: resolution report :: resolve 3366ms :: artifacts dl 1821ms\n :: modules in use:\n com.github.fommil#jniloader;1.1 from central in [default]\n com.github.fommil.netlib#all;1.1.2 from central in [default]\n com.github.fommil.netlib#core;1.1.2 from central in [default]\n com.github.fommil.netlib#native_ref-java;1.1 from central in [default]\n com.github.fommil.netlib#native_system-java;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_ref-linux-armhf;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_ref-linux-i686;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_ref-linux-x86_64;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_ref-osx-x86_64;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_ref-win-i686;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_ref-win-x86_64;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_system-linux-armhf;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_system-linux-i686;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_system-linux-x86_64;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_system-osx-x86_64;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_system-win-i686;1.1 from central in [default]\n com.github.fommil.netlib#netlib-native_system-win-x86_64;1.1 from central in [default]\n net.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n :: evicted modules:\n com.github.fommil.netlib#core;1.1 by [com.github.fommil.netlib#core;1.1.2] in [default]\n --------------------------------------------------------------------\n |                  |            modules            ||   artifacts   |\n |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n ---------------------------------------------------------------------\n |      default     |   19  |   18  |   18  |   1   ||   17  |   17  |\n ---------------------------------------------------------------------\n...\nscala> val enron = sc textFile(\"enron\")\nenron: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at textFile at <console>:21\n\nscala> enron.flatMap(_.split(\"\\\\W+\")).map(_.toLowerCase).distinct.count\nres0: Long = 529199 \n\nscala> val stopwords = scala.collection.immutable.TreeSet (\"\", \"i\", \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"from\", \"had\", \"has\", \"he\", \"her\", \"him\", \"his\", \"in\", \"is\", \"it\", \"its\", \"not\", \"of\", \"on\", \"she\", \"that\", \"the\", \"to\", \"was\", \"were\", \"will\", \"with\", \"you\")\nstopwords: scala.collection.immutable.TreeSet[String] = TreeSet(, a, an, and, are, as, at, be, but, by, for, from, had, has, he, her, him, his, i, in, is, it, its, not, of, on, she, that, the, to, was, were, will, with, you)\nscala> \n\nscala> val terms = enron.flatMap(x => if (x.length < 8192) x.toLowerCase.split(\"\\\\W+\") else Nil).filterNot(stopwords).map(_,1).reduceByKey(_+_).collect.sortBy(- _._2).slice(0, 1000).map(_._1)\nterms: Array[String] = Array(enron, ect, com, this, hou, we, s, have, subject, or, 2001, if, your, pm, am, please, cc, 2000, e, any, me, 00, message, 1, corp, would, can, 10, our, all, sent, 2, mail, 11, re, thanks, original, know, 12, 713, http, may, t, do, 3, time, 01, ees, m, new, my, they, no, up, information, energy, us, gas, so, get, 5, about, there, need, what, call, out, 4, let, power, should, na, which, one, 02, also, been, www, other, 30, email, more, john, like, these, 03, mark, 04, attached, d, enron_development, their, see, 05, j, forwarded, market, some, agreement, 09, day, questions, meeting, 08, when, houston, doc, contact, company, 6, just, jeff, only, who, 8, fax, how, deal, could, 20, business, use, them, date, price, 06, week, here, net, 15, 9, 07, group, california,...\nscala> def getBagCounts(bag: Seq[String]) = { for(term <- terms) yield { bag.count(_==term) } }\ngetBagCounts: (bag: Seq[String])Array[Int]\n\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\n\nscala> val corpus = enron.map(x => { if (x.length < 8192) Some(x.toLowerCase.split(\"\\\\W+\").toSeq) else None } ).map(x => { Vectors.dense(getBagCounts(x.getOrElse(Nil)).map(_.toDouble).toArray) }).zipWithIndex.map(_.swap).cache\ncorpus: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[14] at map at <console>:30\n\nscala> import org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}\nimport org.apache.spark.mllib.clustering.{LDA, DistributedLDAModel}\n\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\n\nscala> val ldaModel = new LDA().setK(10).run(corpus)\n...\nscala> ldaModel.topicsMatrix.transpose\nres2: org.apache.spark.mllib.linalg.Matrix = \n207683.78495933366  79745.88417942637   92118.63972404732   ... (1000 total)\n35853.48027575886   4725.178508682296   111214.8860582083   ...\n135755.75666585402  54736.471356209106  93289.65563593085   ...\n39445.796099155996  6272.534431534215   34764.02707696523   ...\n329786.21570967307  602782.9591026317   42212.22143362559   ...\n62235.09960154089   12191.826543794878  59343.24100019015   ...\n210049.59592560542  160538.9650732507   40034.69756641789   ...\n53818.14660186875   6351.853448001488   125354.26708575874  ...\n44133.150537842856  4342.697652158682   154382.95646078113  ...\n90072.97362336674   21132.629704311104  93683.40795807641   ...\n\n```", "```py\nscala> ldaModel.describeTopics foreach { x : (Array[Int], Array[Double]) => { print(x._1.slice(0,10).map(terms(_)).mkString(\":\")); print(\"-> \"); print(x._2.slice(0,10).map(_.toFloat).mkString(\":\")); println } }\ncom:this:ect:or:if:s:hou:2001:00:we->0.054606363:0.024220783:0.02096761:0.013669214:0.0132700335:0.012969772:0.012623918:0.011363528:0.010114557:0.009587474\ns:this:hou:your:2001:or:please:am:com:new->0.029883621:0.027119286:0.013396418:0.012856948:0.01218803:0.01124849:0.010425644:0.009812181:0.008742722:0.0070441025\ncom:this:s:ect:hou:or:2001:if:your:am->0.035424445:0.024343235:0.015182628:0.014283071:0.013619815:0.012251413:0.012221165:0.011411696:0.010284024:0.009559739\nwould:pm:cc:3:thanks:e:my:all:there:11->0.047611523:0.034175437:0.022914853:0.019933242:0.017208714:0.015393614:0.015366959:0.01393391:0.012577525:0.011743208\nect:com:we:can:they:03:if:also:00:this->0.13815293:0.0755843:0.065043546:0.015290086:0.0121941045:0.011561104:0.011326733:0.010967959:0.010653805:0.009674695\ncom:this:s:hou:or:2001:pm:your:if:cc->0.016605735:0.015834121:0.01289918:0.012708308:0.0125788655:0.011726159:0.011477625:0.010578845:0.010555539:0.009609056\ncom:ect:we:if:they:hou:s:00:2001:or->0.05537054:0.04231919:0.023271963:0.012856676:0.012689817:0.012186356:0.011350313:0.010887237:0.010778923:0.010662295\nthis:s:hou:com:your:2001:or:please:am:if->0.030830953:0.016557815:0.014236835:0.013236604:0.013107091:0.0126846135:0.012257128:0.010862533:0.01027849:0.008893094\nthis:s:or:pm:com:your:please:new:hou:2001->0.03981197:0.013273305:0.012872894:0.011672661:0.011380969:0.010689667:0.009650983:0.009605533:0.009535899:0.009165275\nthis:com:hou:s:or:2001:if:your:am:please->0.024562683:0.02361607:0.013770585:0.013601272:0.01269994:0.012360005:0.011348433:0.010228578:0.009619628:0.009347991\n\n```", "```py\nscala> ldaModel.save(sc, \"ldamodel\")\n\nscala> val sameModel = DistributedLDAModel.load(sc, \"ldamode2l\")\n\nscala> sameModel.topDocumentsPerTopic(10) foreach { x : (Array[Long], Array[Double]) => { print(x._1.mkString(\":\")); print(\"-> \"); print(x._2.map(_.toFloat).mkString(\":\")); println } }\n59784:50745:52479:60441:58399:49202:64836:52490:67936:67938-> 0.97146696:0.9713364:0.9661418:0.9661132:0.95249915:0.9519995:0.94945914:0.94944507:0.8977366:0.8791358\n233009:233844:233007:235307:233842:235306:235302:235293:233020:233857-> 0.9962034:0.9962034:0.9962034:0.9962034:0.9962034:0.99620336:0.9954057:0.9954057:0.9954057:0.9954057\n14909:115602:14776:39025:115522:288507:4499:38955:15754:200876-> 0.83963907:0.83415157:0.8319566:0.8303818:0.8291597:0.8281472:0.82739806:0.8272517:0.82579833:0.8243338\n237004:71818:124587:278308:278764:278950:233672:234490:126637:123664-> 0.99929106:0.9968135:0.9964454:0.99644524:0.996445:0.99644494:0.99644476:0.9964447:0.99644464:0.99644417\n156466:82237:82252:82242:341376:82501:341367:340197:82212:82243-> 0.99716955:0.94635135:0.9431836:0.94241136:0.9421047:0.9410431:0.94075173:0.9406304:0.9402021:0.94014835\n335708:336413:334075:419613:417327:418484:334157:335795:337573:334160-> 0.987011:0.98687994:0.9865438:0.96953565:0.96953565:0.96953565:0.9588571:0.95852506:0.95832515:0.9581657\n243971:244119:228538:226696:224833:207609:144009:209548:143066:195299-> 0.7546907:0.7546907:0.59146744:0.59095955:0.59090924:0.45532238:0.45064417:0.44945204:0.4487876:0.44833568\n242260:214359:126325:234126:123362:233304:235006:124195:107996:334829-> 0.89615464:0.8961442:0.8106028:0.8106027:0.8106023:0.8106023:0.8106021:0.8106019:0.76834095:0.7570231\n209751:195546:201477:191758:211002:202325:197542:193691:199705:329052-> 0.913124:0.9130985:0.9130918:0.9130672:0.5525752:0.5524637:0.5524494:0.552405:0.55240136:0.5026157\n153326:407544:407682:408098:157881:351230:343651:127848:98884:129351-> 0.97206575:0.97206575:0.97206575:0.97206575:0.97206575:0.9689198:0.968068:0.9659192:0.9657442:0.96553063\n\n```", "```py\n$ git clone https://github.com/factorie/factorie.git\n...\n$ cd factorie\n$ git checkout factorie_2.11-1.2\n...\n$ mvn package -Pnlp-jar-with-dependencies\n\n```", "```py\n$ $ bin/fac nlp --wsj-forward-pos --conll-chain-ner\njava -Xmx6g -ea -Djava.awt.headless=true -Dfile.encoding=UTF-8 -server -classpath ./src/main/resources:./target/classes:./target/factorie_2.11-1.2-nlp-jar-with-dependencies.jar\nfound model\n18232\nListening on port 3228\n...\n\n```", "```py\n$ telnet localhost 3228\nTrying ::1...\nConnected to localhost.\nEscape character is '^]'.\nBut I warn you, if you don't tell me that this means war, if you still try to defend the infamies and horrors perpetrated by that Antichrist--I really believe he is Antichrist--I will have nothing more to do with you and you are no longer my friend, no longer my 'faithful slave,' as you call yourself! But how do you do? I see I have frightened you--sit down and tell me all the news.\n\n1  1  But  CC  O\n2  2  I    PRP  O\n3  3  warn    VBP  O\n4  4  you    PRP  O\n5  5  ,      O\n6  6  if    IN  O\n7  7  you    PRP  O\n8  8  do    VBP  O\n9  9  n't    RB  O\n10  10  tell    VB  O\n11  11  me    PRP  O\n12  12  that    IN  O\n13  13  this    DT  O\n14  14  means    VBZ  O\n15  15  war    NN  O\n16  16  ,    ,  O\n17  17  if    IN  O\n18  18  you  PRP  O\n19  19  still    RB  O\n20  20  try    VBP  O\n21  21  to    TO  O\n22  22  defend    VB  O\n23  23  the    DT  O\n24  24  infamies    NNS  O\n25  25  and    CC  O\n26  26  horrors    NNS  O\n27  27  perpetrated    VBN  O\n28  28  by    IN  O\n29  29  that    DT  O\n30  30  Antichrist    NNP  O\n31  31  --    :  O\n32  1  I  PRP  O\n33  2  really    RB  O\n34  3  believe    VBP  O\n35  4  he    PRP  O\n36  5  is    VBZ  O\n37  6  Antichrist    NNP  U-MISC\n38  7  --    :  O\n39  1  I    PRP  O\n40  2  will    MD  O\n41  3  have    VB  O\n42  4  nothing    NN  O\n43  5  more    JJR  O\n44  6  to    TO  O\n45  7  do    VB  O\n46  8  with    IN  O\n47  9  you    PRP  O\n48  10  and    CC  O\n49  11  you    PRP  O\n50  12  are    VBP  O\n51  13  no    RB  O\n52  14  longer    RBR  O\n53  15  my    PRP$  O\n54  16  friend    NN  O\n55  17  ,    ,  O\n56  18  no    RB  O\n57  19  longer    RB  O\n58  20  my  PRP$  O\n59  21  '    POS  O\n60  22  faithful    NN  O\n61  23  slave    NN  O\n62  24  ,    ,  O\n63  25  '    ''  O\n64  26  as    IN  O\n65  27  you    PRP  O\n66  28  call    VBP  O\n67  29  yourself    PRP  O\n68  30  !    .  O\n69  1  But    CC  O\n70  2  how    WRB  O\n71  3  do    VBP  O\n72  4  you    PRP  O\n73  5  do    VB  O\n74  6  ?    .  O\n75  1  I    PRP  O\n76  2  see    VBP  O\n77  3  I    PRP  O\n78  4  have    VBP  O\n79  5  frightened    VBN  O\n80  6  you    PRP  O\n81  7  --    :  O\n82  8  sit    VB  O\n83  9  down    RB  O\n84  10  and    CC  O\n85  11  tell    VB  O\n86  12  me    PRP  O\n87  13  all    DT  O\n88  14  the    DT  O\n89  15  news    NN  O\n90  16  .    .  O\n\n```", "```py\nscala> val word2vec = new Word2Vec\nword2vec: org.apache.spark.mllib.feature.Word2Vec = org.apache.spark.mllib.feature.Word2Vec@58bb4dd\n\nscala> val model = word2vec.fit(sc.textFile(\"warandpeace\").map(_.split(\"\\\\W+\").toSeq)\nmodel: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@6f61b9d7\n\nscala> val synonyms = model.findSynonyms(\"life\", 10)\nsynonyms: Array[(String, Double)] = Array((freedom,1.704344822168997), (universal,1.682276637692245), (conception,1.6776193389148586), (relation,1.6760497906519414), (humanity,1.67601036253831), (consists,1.6637604144872544), (recognition,1.6526169382380496), (subjection,1.6496559771230317), (activity,1.646671198014248), (astronomy,1.6444424059160712))\n\nscala> synonyms foreach println\n(freedom,1.704344822168997)\n(universal,1.682276637692245)\n(conception,1.6776193389148586)\n(relation,1.6760497906519414)\n(humanity,1.67601036253831)\n(consists,1.6637604144872544)\n(recognition,1.6526169382380496)\n(subjection,1.6496559771230317)\n(activity,1.646671198014248)\n(astronomy,1.6444424059160712)\n\n```", "```py\nscala> val a = model.getVectors.filter(_._1 == \"monarchs\").map(_._2).head\na: Array[Float] = Array(-0.0044642715, -0.0013227836, -0.011506443, 0.03691717, 0.020431392, 0.013427449, -0.0036369907, -0.013460356, -3.8938568E-4, 0.02432113, 0.014533845, 0.004130258, 0.00671316, -0.009344602, 0.006229065, -0.005442078, -0.0045390734, -0.0038824948, -6.5973646E-4, 0.021729799, -0.011289608, -0.0030690092, -0.011423801, 0.009100784, 0.011765533, 0.0069619063, 0.017540144, 0.011198071, 0.026103685, -0.017285397, 0.0045515243, -0.0044477824, -0.0074411617, -0.023975836, 0.011371289, -0.022625357, -2.6478301E-5, -0.010510282, 0.010622139, -0.009597833, 0.014937023, -0.01298345, 0.0016747514, 0.01172987, -0.001512275, 0.022340108, -0.009758578, -0.014942565, 0.0040697413, 0.0015349758, 0.010246878, 0.0021413323, 0.008739062, 0.007845526, 0.006857361, 0.01160148, 0.008595...\nscala> val b = model.getVectors.filter(_._1 == \"princess\").map(_._2).head\nb: Array[Float] = Array(0.13265875, -0.04882792, -0.08409957, -0.04067986, 0.009084379, 0.121674284, -0.11963971, 0.06699862, -0.20277102, 0.26296946, -0.058114383, 0.076021515, 0.06751665, -0.17419271, -0.089830205, 0.2463593, 0.062816426, -0.10538805, 0.062085453, -0.2483566, 0.03468293, 0.20642486, 0.3129267, -0.12418643, -0.12557726, 0.06725172, -0.03703333, -0.10810595, 0.06692443, -0.046484336, 0.2433963, -0.12762263, -0.18473054, -0.084376186, 0.0037174677, -0.0040220995, -0.3419341, -0.25928706, -0.054454487, 0.09521076, -0.041567303, -0.13727514, -0.04826158, 0.13326299, 0.16228828, 0.08495835, -0.18073058, -0.018380836, -0.15691829, 0.056539804, 0.13673553, -0.027935665, 0.081865616, 0.07029694, -0.041142456, 0.041359138, -0.2304657, -0.17088272, -0.14424285, -0.0030700471, -0...\nscala> val c = model.getVectors.filter(_._1 == \"individual\").map(_._2).head\nc: Array[Float] = Array(-0.0013353615, -0.01820516, 0.007949033, 0.05430816, -0.029520465, -0.030641818, -6.607431E-4, 0.026548808, 0.04784935, -0.006470232, 0.041406438, 0.06599842, 0.0074243015, 0.041538745, 0.0030222891, -0.003932073, -0.03154199, -0.028486902, 0.022139633, 0.05738223, -0.03890591, -0.06761177, 0.0055152955, -0.02480924, -0.053222697, -0.028698998, -0.005315235, 0.0582403, -0.0024816995, 0.031634405, -0.027884213, 6.0290704E-4, 1.9750209E-4, -0.05563172, 0.023785716, -0.037577976, 0.04134448, 0.0026664822, -0.019832063, -0.0011898747, 0.03160933, 0.031184288, 0.0025268437, -0.02718441, -0.07729341, -0.009460656, 0.005344515, -0.05110715, 0.018468754, 0.008984449, -0.0053139487, 0.0053904117, -0.01322933, -0.015247412, 0.009819351, 0.038043085, 0.044905875, 0.00402788...\nscala> model.findSynonyms(new DenseVector((for(i <- 0 until 100) yield (a(i) - b(i) + c(i)).toDouble).toArray), 10) foreach println\n(achievement,0.9432423663884002)\n(uncertainty,0.9187759184842362)\n(leader,0.9163721499105207)\n(individual,0.9048367510621271)\n(instead,0.8992079672038455)\n(cannon,0.8947818781378154)\n(arguments,0.8883634101905679)\n(aims,0.8725107984356915)\n(ants,0.8593842583047755)\n(War,0.8530727227924755)\n\n```", "```py\n  def step1(s: String) = {\n    b = s\n    // step 1a\n    processSubList(List((\"sses\", \"ss\"), (\"ies\",\"i\"),(\"ss\",\"ss\"), (\"s\", \"\")), _>=0)\n    // step 1b\n    if (!(replacer(\"eed\", \"ee\", _>0)))\n    {\n      if ((vowelInStem(\"ed\") && replacer(\"ed\", \"\", _>=0)) || (vowelInStem(\"ing\") && replacer(\"ing\", \"\", _>=0)))\n      {\n        if (!processSubList(List((\"at\", \"ate\"), (\"bl\",\"ble\"), (\"iz\",\"ize\")), _>=0 ) )\n        {\n          // if this isn't done, then it gets more confusing.\n          if (doublec() && b.last != 'l' && b.last != 's' && b.last != 'z') { b = b.substring(0, b.length - 1) }\n          else\n            if (calcM(b.length) == 1 && cvc(\"\")) { b = b + \"e\" }\n        }\n      }\n    }\n    // step 1c\n    (vowelInStem(\"y\") && replacer(\"y\", \"i\", _>=0))\n    this\n  }\n```", "```py\nclass SimpleServlet extends Servlet {\n  val logger = LoggerFactory.getLogger(getClass)\n  var hwCounter: Long = 0L\n  val hwLookup: scala.collection.mutable.Map[String,Long] = scala.collection.mutable.Map() \n  val defaultName = \"Stranger\"\n  def response(name: String, id: Long) = { \"Hello %s! Your id should be %d.\".format(if (name.length > 0) name else defaultName, id) }\n  get(\"/hw/:name\") {\n    val name = params(\"name\")\n    val startTime = System.nanoTime\n    val retVal = response(name, synchronized { hwLookup.get(name) match { case Some(id) => id; case _ => hwLookup += name -> { hwCounter += 1; hwCounter } ; hwCounter } } )\n    logger.info(\"It took [\" + name + \"] \" + (System.nanoTime - startTime) + \" \" + TimeUnit.NANOSECONDS)\n    retVal\n  }\n}\n```", "```py\n[akozlov@Alexanders-MacBook-Pro chapter10]$ sbt\n[info] Loading project definition from /Users/akozlov/Src/Book/ml-in-scala/chapter10/project\n[info] Compiling 1 Scala source to /Users/akozlov/Src/Book/ml-in-scala/chapter10/project/target/scala-2.10/sbt-0.13/classes...\n[info] Set current project to Advanced Model Monitoring (in build file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/)\n> ~;jetty:stop;jetty:start\n[success] Total time: 0 s, completed May 15, 2016 12:08:31 PM\n[info] Compiling Templates in Template Directory: /Users/akozlov/Src/Book/ml-in-scala/chapter10/src/main/webapp/WEB-INF/templates\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n[info] starting server ...\n[success] Total time: 1 s, completed May 15, 2016 12:08:32 PM\n1\\. Waiting for source changes... (press enter to interrupt)\n2016-05-15 12:08:32.578:INFO::main: Logging initialized @119ms\n2016-05-15 12:08:32.586:INFO:oejr.Runner:main: Runner\n2016-05-15 12:08:32.666:INFO:oejs.Server:main: jetty-9.2.1.v20140609\n2016-05-15 12:08:34.650:WARN:oeja.AnnotationConfiguration:main: ServletContainerInitializers: detected. Class hierarchy: empty\n2016-15-05 12:08:34.921: [main] INFO  o.scalatra.servlet.ScalatraListener - The cycle class name from the config: ScalatraBootstrap\n2016-15-05 12:08:34.973: [main] INFO  o.scalatra.servlet.ScalatraListener - Initializing life cycle class: ScalatraBootstrap\n2016-15-05 12:08:35.213: [main] INFO  o.f.s.servlet.ServletTemplateEngine - Scalate template engine using working directory: /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T/scalate-6339535024071976693-workdir\n2016-05-15 12:08:35.216:INFO:oejsh.ContextHandler:main: Started o.e.j.w.WebAppContext@1ef7fe8e{/,file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/,AVAILABLE}{file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/}\n2016-05-15 12:08:35.216:WARN:oejsh.RequestLogHandler:main: !RequestLog\n2016-05-15 12:08:35.237:INFO:oejs.ServerConnector:main: Started ServerConnector@68df9280{HTTP/1.1}{0.0.0.0:8080}\n2016-05-15 12:08:35.237:INFO:oejs.Server:main: Started @2795ms2016-15-05 12:03:52.385: [main] INFO  o.f.s.servlet.ServletTemplateEngine - Scalate template engine using working directory: /var/folders/p1/y7ygx_4507q34vhd60q115p80000gn/T/scalate-3504767079718792844-workdir\n2016-05-15 12:03:52.387:INFO:oejsh.ContextHandler:main: Started o.e.j.w.WebAppContext@1ef7fe8e{/,file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/,AVAILABLE}{file:/Users/akozlov/Src/Book/ml-in-scala/chapter10/target/webapp/}\n2016-05-15 12:03:52.388:WARN:oejsh.RequestLogHandler:main: !RequestLog\n2016-05-15 12:03:52.408:INFO:oejs.ServerConnector:main: Started ServerConnector@68df9280{HTTP/1.1}{0.0.0.0:8080}\n2016-05-15 12:03:52.408:INFO:oejs.Server:main: Started @2796mss\n\n```", "```py\n2016-15-05 13:10:06.240: [qtp1747585824-26] INFO  o.a.examples.ServletWithMetrics - It took [Joe] 133225 NANOSECONDS\n\n```", "```py\nimport org.akozlov.examples._\n\nimport javax.servlet.ServletContext\nimport org.scalatra.LifeCycle\nimport org.scalatra.metrics.MetricsSupportExtensions._\nimport org.scalatra.metrics._\n\nclass ScalatraBootstrap extends LifeCycle with MetricsBootstrap {\n  override def init(context: ServletContext) = {\n    context.mount(new ServletWithMetrics, \"/\")\n    context.mountMetricsAdminServlet(\"/admin\")\n    context.mountHealthCheckServlet(\"/health\")\n    context.installInstrumentedFilter(\"/*\")\n  }\n}\n```", "```py\npackage org.akozlov.examples\n\nimport org.scalatra._\nimport scalate.ScalateSupport\nimport org.scalatra.ScalatraServlet\nimport org.scalatra.metrics.{MetricsSupport, HealthChecksSupport}\nimport java.util.concurrent.atomic.AtomicLong\nimport java.util.concurrent.TimeUnit\nimport org.slf4j.{Logger, LoggerFactory}\n\nclass ServletWithMetrics extends Servlet with MetricsSupport with HealthChecksSupport {\n  val logger = LoggerFactory.getLogger(getClass)\n  val defaultName = \"Stranger\"\n  var hwCounter: Long = 0L\n  val hwLookup: scala.collection.mutable.Map[String,Long] = scala.collection.mutable.Map()  val hist = histogram(\"histogram\")\n  val cnt =  counter(\"counter\")\n  val m = meter(\"meter\")\n  healthCheck(\"response\", unhealthyMessage = \"Ouch!\") { response(\"Alex\", 2) contains \"Alex\" }\n  def response(name: String, id: Long) = { \"Hello %s! Your id should be %d.\".format(if (name.length > 0) name else defaultName, id) }\n\n  get(\"/hw/:name\") {\n    cnt += 1\n    val name = params(\"name\")\n    hist += name.length\n    val startTime = System.nanoTime\n    val retVal = response(name, synchronized { hwLookup.get(name) match { case Some(id) => id; case _ => hwLookup += name -> { hwCounter += 1; hwCounter } ; hwCounter } } )s\n    val elapsedTime = System.nanoTime - startTime\n    logger.info(\"It took [\" + name + \"] \" + elapsedTime + \" \" + TimeUnit.NANOSECONDS)\n    m.mark(1)\n    retVal\n  }\n```", "```py\n  get(\"/time\") {\n    val sleepTime = scala.util.Random.nextInt(1000)\n    val startTime = System.nanoTime\n    timer(\"timer\") {\n      Thread.sleep(sleepTime)\n      Thread.sleep(sleepTime)\n      Thread.sleep(sleepTime)\n    }\n    logger.info(\"It took [\" + sleepTime + \"] \" + (System.nanoTime - startTime) + \" \" + TimeUnit.NANOSECONDS)\n    m.mark(1)\n  }\n```"]