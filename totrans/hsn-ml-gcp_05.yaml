- en: Transforming Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Real-world datasets are very varied: variables can be textual, numerical, or
    categorical, and observations can be missing, false, or wrong (outliers). To perform
    a proper data analysis, we will understand how to correctly parse data, clean
    it, and create an output matrix optimally built for machine learning analysis.
    To extract knowledge, it is essential that the reader is able to create an observation
    matrix using different techniques of data analysis and cleaning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll present Cloud Dataprep, a service useful to preprocess
    the data, extract features, and clean up the records. We'll also cover Cloud Dataflow,
    a service to implement streaming and batch processing. We'll go into some practical
    details with real-life examples. We'll start from discovering different ways to
    transform data and the degree of cleaning data. We will analyze the techniques
    available for preparing the most suitable data for analysis and modeling, which
    includes imputation of missing data, detecting and eliminating outliers, and adding
    derived variables. Then we will learn how to normalize the data, in which data
    units are eliminated, allowing us to easily compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Different ways to transform data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to organize data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the chapter, we will be able to perform data preparation so that
    its information content is best exposed to the regression tools. We'll learn how
    to apply transforming methods to our own data and how these techniques work. We'll
    discover how to clean the data, identify missing data, and work with outliers
    and missing entries. We'll also learn how to use normalization techniques to compare
    data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: How to clean and prepare the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A novice may think that once we complete collecting data and it is imported
    into Google Cloud, it is finally time to start the analysis process. Conversely,
    we must first proceed with the preparation of data (data wrangling).
  prefs: []
  type: TYPE_NORMAL
- en: Data wrangling is the process of the transformation and mapping of data, turning
    raw data into formatted data, with the intent of making it more appropriate for
    subsequent analysis operations.
  prefs: []
  type: TYPE_NORMAL
- en: This process can take a long time and it is very cumbersome, in some cases taking
    up about 80 percent of the entire data analysis process.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is a fundamental prerequisite for the rest of the data analysis
    workflow; so it is essential to acquire the best practices in such techniques.
    Before submitting our data to any machine learning algorithm, we must be able
    to evaluate the quality and accuracy of our observations. If we do not know how
    to switch from raw data to something that can be analyzed, we cannot go ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataprep
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For proper preparation of our data, it is necessary to perform a series of
    operations involving the use of different algorithms. As we have anticipated,
    this work can take a long time and uses many resources. Google, within the Cloud
    service, offers the ability to do this job in a simple and immediate way: Google
    Cloud Dataprep.'
  prefs: []
  type: TYPE_NORMAL
- en: It is an intelligent data service that allows you to visually explore, clean
    up, and prepare for structured and unstructured data analysis. Google Cloud Dataprep
    is serverless and works on any scale. It is not necessary to distribute or manage
    any infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataprep helps to quickly prepare data for immediate analysis or
    for training machine learning models. Normally, the data has to be manually cleaned
    up; however, Google Cloud Dataprep makes the process extremely simple by automatically
    detecting schemas, types, joins, and anomalies such as missing values. With regard
    to machine learning, different ways of data cleaning are suggested that can make
    the process of data preparation quicker and less prone to errors.
  prefs: []
  type: TYPE_NORMAL
- en: In Google Cloud Dataprep, it is possible to define data preparation rules by
    interacting with a sample of the data. Use of the application is free. Once a
    data preparation flow has been defined, you can export the sample for free or
    run the stream as a Cloud Dataprep job, which will be subject to additional costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the use of Google Cloud Dataprep, you can perform the following operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Import data from different sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and remove or modify missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify anomalous values (outliers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform searches from a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the values in the fields in the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merge datasets with joins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add one dataset to another through merge operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These operations can be performed without the need for technological infrastructure
    in a very short time.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Dataprep console
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first time you log into the Google Cloud Dataprep console, you will be
    asked to accept the terms of service, log into your Google account, and choose
    a cloud storage bucket to use with Cloud Dataprep. You will also be asked to allow
    Trifacta, the third-party application host, to access project data. After completing
    these steps, you will be offered the Cloud Dataprep home page with the Flows screen
    opened, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0b5e5c5-28d7-46ec-b26e-7194ffabb172.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For now, the console appears empty because we have not yet performed any operation;
    it will later be populated by our operations. At the top of the Google Cloud Dataprep
    console, you can find three links that open the following pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FLOWS**: This page shows the flows you have access to and allows you to create,
    review, and manage them. A flow is an object that allows us to gather and organize
    datasets to generate results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DATASETS**: In this page, we can review the import and reference datasets
    to which we have access.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JOBS**: In this page, we can track the status of all of our running, complete,
    or failed jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By default, a Flows page is open. To create a new flow, click on the Create
    Flow button. The following window opens, where we can set a name and description
    for the new flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aee494ae-1dfc-4270-8ef9-33f2643efdae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we have done this, on the flows page, the new flow just created appears
    with suggestions of datasets that can be added to this flow to start wrangling.
    To analyze how a Google Cloud Dataprep example works, we will use a specially
    designed file that contains the data for a small sample of observation; it lists
    the results of a test. We''ll grab `CleaningData.csv`, a spreadsheet that contains
    some of the issues we just listed. After correctly identifying this file on our
    computer and having it uploaded, the following window will be displayed (transformer
    page):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90edebe7-dcf3-4f1e-978e-152ac39693ee.png)'
  prefs: []
  type: TYPE_IMG
- en: In this page, we can easily identify the data we have imported. At this point,
    we can program the transformations that we want to apply to this data. Once we
    have established what we want to do, we can immediately preview the results before
    making changes to the entire final dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Transformer page, two panels are available: Grid and Columns. By default,
    the Transformer page displays the grid panel, in which the previews of the columns
    in a columnar grid are displayed. In the columns panel, additional statistical
    information on individual columns is returned. This panel is particularly useful
    for managing anomalous values, to review average, minimum, and maximum values.'
  prefs: []
  type: TYPE_NORMAL
- en: From a first glance at the transformer page, you can get an idea of the data
    we have imported. In fact, we can view all fields and all observations.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have uploaded a file with just a few fields and observations. More
    generally, in the lower left part, we are given a summary of the number of rows,
    columns, and data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the analysis of the returned window, we can extract useful information.
    Start from the name of each column, to the left of which appears a symbol that
    identifies the type of data automatically attributed to it. In this way we can
    verify that Dataprep has cataloged the variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: Name (String)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gender`: Sex (Gender)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: Age (Integer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`right1`: Percentage of right answers (Integer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wrong`: Percentage of wrong answers (Integer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It seems that the attribution of the data type is correct. This information
    is returned to us using the cursor. In fact, as you move the cursor around the
    page, the cursor changes when it is over a selected data element. Immediately
    down the column header, there are useful summary graphs. In the column''s data
    quality bar, the categories of values are shown. The following three types of
    values are available:'
  prefs: []
  type: TYPE_NORMAL
- en: Valid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mismatched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, histograms are shown to provide us with statistics about the data contained
    in each column. Further information is returned to us by passing the cursor over
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Since the values are few, we can also have a preview of all the data. Let's
    take a look to visually identify anomalies. Already, we can see that the age variable
    has one missing value. Missing values of any type of variable are indicated by
    the NA code, which means not available. The **Not a Number** (**NaN**) code, on
    the other hand, indicates invalid numeric values, such as a numeric value divided
    by zero. If a variable contains missing values, GCP cannot apply some functions
    to it. For this reason, it is necessary to process the missing values in advance.
  prefs: []
  type: TYPE_NORMAL
- en: On the transformation page, you can see that when you hover your mouse over
    the areas and related panels, a light bulb icon appears next to the cursor to
    indicate that suggestions are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left of each column header, you can see a drop-down column menu. With
    the help of this menu, we can perform several actions on the column data such
    as changing its data type, depending on the column data type. The following actions
    are available:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename: Rename the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Change type: Change the data type of the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Move: Move the column to the beginning or end, or to a specified location in
    the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Edit column: Perform a series of edits to the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column Details: Explore the interactive profile of the column details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Show related steps: Highlight steps in the Recipe panel where the selected
    column is referenced'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Find: Find and replace specific values or extract patterned values from the
    column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Filter: Filter the rows of the dataset based on literal or computed values
    from the column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clean: Clean the mismatched or missing values in the column, replacing values
    with fixed or computed values or removing the rows altogether'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Formula: Generate a new column containing the values computed from the source
    column based on the selected function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggregate: Generate a summary table based on computations aggregated across
    groups, or add summary data as a new column in the current table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Restructure: Change the structure of the dataset based on the values of the
    column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lookup: Perform a lookup of the column values against a set of values in another
    column of another dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delete: Remove the column from the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Column menu for the gender column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9022727-6992-42c2-904b-d939955357d5.png)'
  prefs: []
  type: TYPE_IMG
- en: When selecting a column, a new suggestion panel opens to the right of the Transformer
    page. This panel shows a series of suggestions relevant to the type of data contained
    in the specific column. Suggestions vary depending on the selected data. By hovering
    over any of the suggestions, you can preview the results in the data grid to ensure
    that the proposed transformation works for the dataset. In the next section, we
    will be able to expand the use of the suggestion panel.
  prefs: []
  type: TYPE_NORMAL
- en: Removing empty cells
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From a visual analysis of the imported data, we detect an empty cell at the
    third row and second column; this indicates the presence of a missing value. It
    is necessary to eliminate this anomaly before you can analyze the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, identifying the empty cell was particularly easy given the small
    amount of data; in the case of large datasets, visual analysis does not work.
    Therefore, to identify missing values, ​​we can analyze the data quality bar.
    Here, the missing values ​​are identified in black.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a preview on the number of missing data, we can move our cursor over
    the black part of the data quality bar; the number of missing data is returned,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3566a19-5274-47b8-91e0-437ef7d89e47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have confirmed the presence of a missing value in the gender field. Recall
    that a missing value is a value that contains no content or is non-existent. These
    missing values may be due to a series of occurrences:'
  prefs: []
  type: TYPE_NORMAL
- en: Errors in the creation of the dataset; the values have been entered incorrectly,
    leaving empty cells
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dataset contains fields created automatically with cells that do not contain
    values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of an impossible calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, if we select the gender column, a new suggestion panel opens to the right
    of the Transformer page. As stated in the previous section, this panel shows a
    series of suggestions relevant to the type of data contained in the specific column.
    Several suggestions are proposed:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rename
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregate and group data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values to columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we hover over different suggestions, a mini-preview appears to the left
    of each suggestion, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93e688d2-4b81-48f0-b484-1858ebb93b14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the suggestion panel, a specific text called missing value to appears, meaning
    that the missing values identified in the column can be replaced with something
    else. By clicking on the Set item, two buttons appear: Edit and Add. By clicking
    on the Edit button a new box is opened; a new formula is proposed in this box
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The missing function writes out a specified value if the source value is a
    null or missing value. Upon inserting the `NA` string between the quotation marks,
    every time a missing value is identified in the column, it will be replaced by
    the `NA` value. After having modified the formula, we will be able to see a preview
    of the column in real time as modified by the formula applied; this is shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b58b06cc-b3c5-46a6-8633-ba7a710510b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, to apply this suggestion, just click on the Add button in the suggestions
    panel. In this way, a new step will be added to the Recipe panel.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Recipe panel allows us to review and modify the steps of the recipe
    we have created so far, but, at the same time, it allows us to add new ones. If
    the panel is not displayed, to make it appear, just click on the icon (Recipe)
    at the top left of the Run Job button.
  prefs: []
  type: TYPE_NORMAL
- en: To generate a new set of suggestions on different columns, click on Cancel.
    Then, select a different set of columns or values within a column; in this way,
    a new suggestions panel will be opened.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing incorrect values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next step will allow us to replace the incorrect value indicators. If we
    take a look at the data again, we will see that in the age column, the value `-19`
    is displayed. That''s obviously an incorrect value since for that variable, permissible
    values are greater than zero (this is an age):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can replace this value with the missing value indicator, `NA`. To do this,
    we will write the following formula in the set suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: A new step will be added to the Recipe panel immediately after the one created
    in the previous section. At the same time, we can see a preview of the changes
    made on the dataset. The incorrect value is no longer present; in its place, there
    is a further `NA` value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same result can be obtained by operating directly in the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that to open the Recipe panel, you can just click on the icon (Recipe)
    at the top left of the Run Job button
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the New Step button; the Transform Builder is opened
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Transformation drop-down menu, select Apply formula in the list of available
    transforms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the Columns (age)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Edit the formula required in the Formula box
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Add button
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new step is added to the Recipe panel
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see the Transform Builder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad88eb70-7ee6-4c82-882c-fefef615521c.png)'
  prefs: []
  type: TYPE_IMG
- en: The result obtained is identical to the result we obtained by writing the formula
    in the set suggestions; even in this case, a negative value will no longer be
    present in the age column.
  prefs: []
  type: TYPE_NORMAL
- en: Mismatched values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A mismatched value is any value that seems to be of a different data type than
    the type specified for the column. In the database we are analyzing at this point,
    there are several values that seem to deviate from the type attributed to the
    column. For example, in the right column is a cell that contains a dot; it is
    clear that this is an error in the phase of populating the database. It is equally
    clear that such a value can cause many problems during the analysis phase, which
    is why it must be appropriately dealt with.
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen for missing values, mismatched values are also represented in the data
    quality bar at the top of each column. In the data quality bar, mismatched values
    are identified in red, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18b327ab-a0fd-4219-951c-716eca582790.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To fix mismatched data, there are several options available:'
  prefs: []
  type: TYPE_NORMAL
- en: Change the data type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the values with constant values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the values with other columns' values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transform the data with functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hide the column for now
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drop the column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, since two of the selected lines contain mismatched data in other
    columns too, we will eliminate all three columns. To do this, simply click on
    the Add button in the Delete rows area of the suggestions panel.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the dataset, we can see that mismatched data is still present. In
    fact, the data quality bar in the age column has an area in red. We try to fix
    this problem too. This time it is not appropriate to delete the entire row. In
    fact, by analyzing the first column, we can see that the `NA` value refers to
    a line whose name is clearly referring to a female (Olivia). So the most appropriate
    solution is to replace this value with a known value, in this case with `'F'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will write the following formula in the set item of the suggestions
    panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A new step will be added to the Recipe panel. Once again, we can see a preview
    of the changes made to the dataset. In fact, we can see that the wrong value is
    no longer present; in its place, there is an `'F'`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have so far adjusted several things, but at first glance, there is still
    something to be done. If we pay attention to the right1 column, which represents
    the percentage of correct answers provided, we notice that the range of values
    is as follows: -19 to 98\. But -19 is obviously an incorrect value since for that
    variable, the permissible values are between 0 and 100 (this is a percentage).
    We can assume that a minus sign was added by mistake when creating the dataset.
    We can then modify this value, leaving only the value 19.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Recipe panel. Just click on the icon (Recipe) at the top left of the
    Run Job button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the New Step button. The Transform Builder is opened.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the Transformation drop-down menu, select Apply formula in the list of available
    transforms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify the Columns (right).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Edit the following formula in the formula box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`IF($col==-19,19,$col)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the Add button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new step is added to the Recipe panel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The operations we have added to the Recipe panel are five, as shown in this
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c39eaff-8aa3-476d-b292-b8d4016d7185.png)'
  prefs: []
  type: TYPE_IMG
- en: By analyzing the Recipe panel, we then have a summary of the actions we have
    planned on the dataset. Moreover, a visual analysis of the dataset preview with
    the changes made does not show any anomalies to be fixed. In the data quality
    bar at the top of each column, no red/black zones are highlighted, although it's
    too early to party yet! The data preparation work is far from done.
  prefs: []
  type: TYPE_NORMAL
- en: Finding outliers in the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outliers are the values that, compared to others, are particularly extreme (a
    value clearly distant from the other available observations). The presence of
    outliers causes a hindrance because they tend to distort the results of data analysis,
    in particular in descriptive statistics and correlations. It is ideal to identify
    these outliers in the data cleaning phase itself; however, they can also be dealt
    with in the next step of the data analysis. Outliers can be univariate when they
    have an extreme value for a single variable, or multivariate when they have an
    unusual combination of values for a number of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Outliers are the extreme values of a distribution that are characterized by
    being extremely high or extremely low compared to the rest of the distribution,
    thus representing isolated cases in respect to the rest of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: There are different methods to detect outliers. Google Cloud Dataprep uses Tukey's
    method, which uses the **interquartile range** (**IQR**) approach. This method
    is not dependent on the distribution of the data and ignores the mean and the
    standard deviation, which are influenced by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: As said before, to determine the outlier values, refer to the IQR given by the
    difference between the 25th percentile and the 75th percentile, that is, the amplitude
    of the range within which it falls. These 50 percent of observations occupy the
    central positions in the ordered series of data. An outlier is a value with positive
    deviation from the 75th percentile greater than two times the IQR or, symmetrically,
    a value with a negative deviation from the 25th percentile (in absolute value)
    greater than two times the IQR.
  prefs: []
  type: TYPE_NORMAL
- en: 'Practically, an outlier value is either of these two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To identify outliers in individual columns, Google Cloud Dataprep has visual
    functionality and statistical information.
  prefs: []
  type: TYPE_NORMAL
- en: Visual functionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we have talked about the histogram at the top of each column. This
    graph displays the count of each value detected in the column (for string data)
    or the count of values ​​within a numeric range (for numerical data).
  prefs: []
  type: TYPE_NORMAL
- en: The visual functionality offered by Google Cloud Dataprep refers precisely to
    the use of these histograms to identify unusual values ​​or outliers, which should
    be removed or corrected before performing any analysis on the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The type of histogram returned depends on the type of data contained in the
    column. In fact, for numeric data, each bar refers to a range of values ​​and
    the bars are sorted in numerical order. For categorical types, each vertical bar
    covers a single value, ordered by the values ​​that occur most frequently.
  prefs: []
  type: TYPE_NORMAL
- en: By moving the mouse over a bar of the histogram, a series of information is
    returned. In this way, we can highlight specific values, obtaining the count of
    a value and the percentage that that value represents in the total count of values
    ​​in the column. We can also select a specific bar; in this case, the rows containing
    them are highlighted and the suggestion panel is displayed for the management
    of these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To select different bars at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: Use *Ctrl* + click to select multiple bars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click and drag over a range of bars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To better understand the usefulness of this visual functionality, let us refer
    to an example. In particular, we will analyze the dataset already used in the
    previous sections, which we have already had the opportunity to modify. Let''s
    take a look at the age column, as it is currently presented in the preview offer
    with the changes already made effective. We can see that the histogram has four
    bars grouped on the left and only one bar isolated on the right. This particular
    form of the histogram is identifying the presence of an outlier, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a952016-9736-401b-8b8d-57f5a358f8c9.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the presence of the anomalous value is evident; more generally,
    the presence of bars isolated at the ends of the graph must alert us.
  prefs: []
  type: TYPE_NORMAL
- en: If a dataset contains multiple instances of outliers, it is necessary to further
    investigate. Generally, if the dataset contains a large number of outliers, it
    is necessary to review these values and their data in other columns before performing
    operations to modify or remove these rows, since the removal of these values can
    become statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The visual analysis we have done so far does not allow us to easily identify
    the presence of outliers in some cases. To obtain more information, we can examine
    detailed statistics of the values in the currently selected column, including
    data on outliers, available in the Column Details panel. To open the Column Details
    panel, just select Column Details from the drop-down menu of a column; the following
    panel is opened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a724a0ff-cc64-4690-890d-80a5c9779b4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this panel, a lot of information is available, some of which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Summary stats the count of valid (unique and outliers), mismatched, and missing
    values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics means you get many options such as, min, max, average, lowest and
    highest quartiles, median, and standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Value histogram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, any outliers are clearly identified and indicated. Furthermore,
    the histogram is now much more precise, clearly indicating the arrangement of
    the bars.
  prefs: []
  type: TYPE_NORMAL
- en: Removing outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen different techniques for identifying possible outliers.
    What should we do after identifying them? After identifying the values ​​that
    are outliers in the column, you need to determine whether these values ​​are valid
    or invalid for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If these are invalid values ​​due to an error in the population phase of the
    dataset, then we must correct them. This operation may involve the replacement
    of this value with a presumably valid one or the removal of the entire row. In
    this latter case, we must pay attention to the weight that this action can have
    on the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To replace the value `100`, which seems to us an invalid value in all respects
    (maybe it was `10` and an extra zero was added), we can insert the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead, if the removal of this record does not assume statistically significant
    importance, we can adopt a simple erasing instruction, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If the data seems valid (in reality, a 100-year age is possible for human beings),
    we can leave it as it is. Or we can convert it into a value that seems to us to
    be statistically more significant. For example, we can decide to replace this
    value with the average value of the entire column so as to preserve at least the
    information of this observation derived from the other columns. To replace 100
    with the average value of the column, use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Ultimately we choose the first option, so `100` will be replaced with `10`
    in the age column. To do this, we insert (as always) the formula just proposed
    in a new step, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af8b0974-b7d0-4420-9d07-e9a64ada601d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To view a preview of the change made, click on the Add button and a new step
    will be added to our Recipe panel. In the screenshot, we can also verify that
    now the range of values has been significantly reduced: 10 to 32 instead of 15
    to 100\. Not only that, but also the histogram no longer has bars isolated at
    the end.'
  prefs: []
  type: TYPE_NORMAL
- en: Run Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have planned several operations on our database: it is time to make these
    changes. To do this, just click on Run Job on the Transformer page. In this way,
    the Run Job page will be open, where we can specify transformation and profiling
    jobs for the currently loaded dataset. Available options include output formats
    and output destinations.'
  prefs: []
  type: TYPE_NORMAL
- en: The Profile Results option allows us to generate a visual result profile. The
    visual profile is very useful for examining the problems of our recipe and iterating,
    even if it is a process that requires a lot of resources. If the dataset we are
    processing is large, disabling the profiling of the results can improve the overall
    execution speed of the job.
  prefs: []
  type: TYPE_NORMAL
- en: After setting the available options correctly, we can queue the specified job
    for execution by simply clicking Run Job. Once this is done, the job is queued
    for processing. At the end of the process, you can view the results of successful
    runs using the Dataset Details page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flow processing times depend on the availability of the server and the size
    of the dataset. In our case, the dataset is really small, so we just have to wait
    for the service to become available. Click on View Results to open the job on
    the Job Results page, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccfb166f-33cb-472e-9e6f-5ac822879874.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous screenshot (Job Results page), it is possible to review the
    effects of the transformation recipe on the entire dataset. Statistics and data
    histograms are available that provide general visibility on the quality of our
    transformation recipe.
  prefs: []
  type: TYPE_NORMAL
- en: At the top of the screenshot on the left, you can find a series of summary information
    on the data contained in the generated dataset. In particular, the counts of valid
    values, non-corresponding values, and missing values are shown. These values are
    shown in full for the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A series of summary information on the work performed always appears in the
    upper part of the screenshot, but this time you can find it on the right.
  prefs: []
  type: TYPE_NORMAL
- en: In the lower part of the screenshot, we can visualize the details of the transformations
    made on the single columns. Depending on the data type of the column, the information
    about the variables is displayed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in the upper part of the screenshot, on the right, there are two buttons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**View Dependencies**: To see the recipes and datasets on which the job depends'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Export Results**: To export the results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale of features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scaling is a preprocessing technique usually employed before feature selection
    and classification. Many artificial intelligence-based systems use features that
    are generated by many different feature extraction algorithms, with different
    kinds of sources. These features may have different dynamic ranges. Popular distance
    measures, such as Euclidean distance, implicitly assign more weighting to features
    with large ranges than those with small ranges. Feature scaling is thus required
    to approximately equalize ranges of the features and make them have approximately
    the same effect in the computation of similarity.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in several data mining applications with huge numbers of features
    with large dynamic ranges, feature scaling may improve the performance of the
    fitting model. However, the appropriate choice of these techniques is an important
    issue. This is because applying scaling on the input could change the structure
    of data and thereby affect the outcome of multivariate analysis used in data mining.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have worked on the data to correct any errors or omissions. We can
    say that at this point all variables contained in the dataset are complete with
    consistent data. What about different variables characterized by different ranges
    and units? There can be variables in a data frame where values for one feature
    could range from 1 to 10 and values for another feature could range from 1 to
    1,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'In data frames such as these, owing to mere greater numeric range, the impact
    on response variables by the feature having greater numeric range could be more
    than the one having less numeric range, and this could, in turn, impact prediction
    accuracy. Our goal is to improve predictive accuracy and not allow a particular
    feature to impact the prediction due to a large numeric value range. Thus, we
    may need to scale values under different features such that they fall under a
    common range. Through this statistical procedure, it is possible to compare identical
    variables belonging to different distributions, but also different variables,
    or variables expressed in different units. Two methods are usually well known
    for rescaling data: normalization and standardization.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to rescale the data before training a machine
    learning algorithm. With rescaling, data units are eliminated, allowing you to
    easily compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Min–max normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Min-max normalization (usually called **feature scaling**) performs a linear
    transformation on the original data. This technique gets all the scaled data in
    the range (0, 1). The formula to achieve this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/545d977f-9de2-4280-b08a-0436149980c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Min-max normalization preserves the relationships among the original data values.
    The cost of having this bounded range is that we will end up with smaller standard
    deviations, which can suppress the effect of outliers.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how to perform a min-max normalization, just analyze an
    example. We will use a dataset contained in the `Airquality.csv` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset is available at the UCI machine learning repository, a large collection
    of data, at the following link: [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php).'
  prefs: []
  type: TYPE_NORMAL
- en: 'S. De Vito, E. Massera, M. Piga, L. Martinotto, G. Di Francia, *On field calibration
    of an electronic nose for benzene estimation in an urban pollution monitoring
    scenario*, Sensors and Actuators B: Chemical, Volume 129, Issue 2, 22 February
    2008, Pages 750-757, ISSN 0925-4005 at: [https://www.sciencedirect.com/science/article/pii/S0925400507007691](https://www.sciencedirect.com/science/article/pii/S0925400507007691).'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the daily readings of the following air quality values for May 1,
    1973 (a Tuesday) to September 30, 1973:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ozone**: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt
    island'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solar.R**: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms
    from 0800 to 1200 hours at central park'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wind**: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia
    airport'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temp**: Maximum daily temperature in degrees Fahrenheit at LaGuardia airport'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data was obtained from the New York State Department of Conservation (ozone
    data) and the **National Weather Service** or **NWS** (meteorological data).
  prefs: []
  type: TYPE_NORMAL
- en: 'A data frame consists of 154 observations on 6 variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Type** | **Units** |'
  prefs: []
  type: TYPE_TB
- en: '| Ozone | numeric | ppb |'
  prefs: []
  type: TYPE_TB
- en: '| Solar | numeric | lang |'
  prefs: []
  type: TYPE_TB
- en: '| Wind | numeric | mph |'
  prefs: []
  type: TYPE_TB
- en: '| Temp | numeric | degrees F |'
  prefs: []
  type: TYPE_TB
- en: '| Month | numeric | 1 to 12 |'
  prefs: []
  type: TYPE_TB
- en: '| Day | numeric | 1 to 31 |'
  prefs: []
  type: TYPE_TB
- en: 'As can be seen, the six variables are characterized by different units of measurement.
    As we did in the previous sections, even in this case, we start preparing the
    data by creating a new flow from the flows screen, and then we import the `.csv`
    file into Google Cloud Dataprep. The following window opens:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/777479ea-23cf-46f0-b15b-b625bcf8053b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we have anticipated, the four variables have different units of measure
    this implies that the ranges of values are very different. In fact, by analyzing
    the upper part of each column in the previous screenshot, we can obtain the following
    ranges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ozone**: 1 to 168'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solar.R**: 7 to 334'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wind**: 2 to 21'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temp**: 56 to 97'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before proceeding with standardization, we eliminate some problems highlighted
    in the data quality bar. Mismatched values have been identified. To be precise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ozone**: 37 mismatched values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solar.R**: 7 mismatched values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First of all, we will try to fix these problems as we have learned to do in
    the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the Ozone column; we proceed to click on the red area of the
    data quality bar. In this way, all 37 mismatched values are highlighted. At the
    same time, the suggestions panel is opened on the right of the window. In particular,
    the first suggestion advises us to delete the lines with mismatched values in
    Ozone. We simply click on the Add button. The following line will be added to
    the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We perform the same operation for the `Solar_R` column; the following line
    will be added to the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In both cases, the preview window shows us that no mismatched value is now
    present in the data. At this point, we can take care of the normalization. As
    we have specified earlier, the variables ranges are very varied. We want to eliminate
    this feature through min-max normalization. As stated earlier, to apply this procedure
    we have to calculate the minimum and maximum for each variable. To do this, we
    can apply two functions available in Google Cloud Dataprep: `MIN` and `MAX`.'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataprep support functions typically found in most desktop spreadsheet
    packages. Functions can be used to create formulas that manipulate data in the
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Previously, we have proposed the formula for normalization; to apply it to
    a column, we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Recipe panel, click on the NEW STEP button. Remember? To open the Recipe
    panel, just click on the icon (Recipe) at the top left of the Run Job button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the Transformation drop-down menu, select Apply formula item (this item
    sets the values of one or more columns to the result of a formula).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the column box, select the Ozone column, for example (the same procedure
    can be applied to all the dataset variables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Formula box, edit the following formula: `(Ozone-MIN(Ozone))/(MAX(Ozone)-MIN(Ozone))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just click on the Add button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following statement is added to the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We follow the same procedure for the other three variables: `Solar_R`, `Wind`,
    and `Temp`. At the end, we will have added the following lines to the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Transformer page has been changed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6c98f0f-7342-4faa-b258-62bc105960d0.png)'
  prefs: []
  type: TYPE_IMG
- en: It is apparent that now the data is all between zero and one; this happens for
    each column of the dataset and then for each variable. The scale differences due
    to the different units of measurement have therefore been removed.
  prefs: []
  type: TYPE_NORMAL
- en: z score standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This technique consists of subtracting the mean of the column from each value
    in a column, and then dividing the result by the standard deviation of the column.
    The formula to achieve this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a9d8cb9-10f7-43b5-b52f-865fbbb0b69e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The result of standardization is that the features will be rescaled so that
    they’ll have the properties of a standard normal distribution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*μ=0*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ=1*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: μ is the mean and *σ* is the standard deviation from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the z score (also called the **standard score**) represents the
    number of standard deviations with which the value of an observation point or
    data differ than the mean value of what is observed or measured. Values more than
    the mean have positive z scores, while values less than the mean have negative
    z scores. The z score is a quantity without dimension, obtained by subtracting
    the population mean from a single rough score and then dividing the difference
    for the standard deviation of the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, to standardize the data, we will use the same procedure used to
    min-max normalization. This time the two functions are changed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AVERAGE`: Calculates the average for each column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STDEV`: Calculates the standard deviation for each column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To perform a z score standardization, just analyze the same dataset used for
    min-max normalization. I refer to dataset called Airquality.csv, which contains
    daily readings of the following air quality values for May 1, 1973 (a Tuesday)
    to September 30, 1973.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply z score standardization to a dataset column, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Recipe panel, click on the NEW STEP button. Again, to open the Recipe
    panel, just click on the icon (Recipe) at the top left of the Run Job button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the Transformation drop-down menu, select Apply formula (this item sets
    the values of one or more columns to the result of a formula).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the column box, select the Ozone column (the same procedure can be applied
    to all the dataset variables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Formula box, edit the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`(Ozone- AVERAGE(Ozone))/STDEV(Ozone)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just click on the ADD button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following statement is added to the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We follow the same procedure for the other three variables: `Solar_R`, `Wind`,
    and `Temp`. At the end, we will have added the following lines to the Recipe panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformer page has been changed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47b95f55-7b8c-43b2-be42-b1d71a872987.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The modification made by the z score standardization is evident: the data ranges
    are quite similar. This happens for each column of the dataset, and then for each
    variable. The differences in scale due to the different units of measurement have
    therefore been removed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the assumptions, all variables must have `average= 0` and `stdev
    =1`. Let''s verify that. To do so, just use the statistical information available
    in the Column Details panel, already used in the previous sections. To open the
    Column Details panel, select Column Details from the drop-down menu of a column;
    the following panel is opened:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3695a5e9-314b-41b8-9b9b-7034543dda15.png)'
  prefs: []
  type: TYPE_IMG
- en: So, we have verified that the Ozone variable has an average of zero and a standard
    deviation of one. The same check can be executed for the other variables.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Cloud Dataflow is a fully managed service for creating data pipelines
    that transform, enrich, and analyze data in batch and streaming modes. Google
    Cloud Dataflow extracts useful information from data, reducing operating costs
    without the hassle of implementing, maintaining, or resizing the data infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline is a set of data processing elements connected in series, in which
    the output of one element is the input of the next. The data pipeline is implemented
    to increase throughput, which is the number of instructions executed in a given
    amount of time, parallelizing the processing flows of multiple instructions.
  prefs: []
  type: TYPE_NORMAL
- en: By appropriately defining a process management flow, significant resources can
    be saved in extracting knowledge from the data. Thanks to a serverless approach
    to provisioning and managing resources, Dataflow offers virtually unlimited capacity
    to solve the most serious data processing problems, but you only pay for what
    you use.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataflow automates the provisioning and management of processing
    resources to reduce latency times and optimize utilization. It is no longer necessary
    to activate the instances manually or to reserve them. Automatic and optimized
    partitioning allows the pending job to be dynamically redistributed. You do not
    need to go for keyboard shortcuts or preprocess your input data. Cloud Dataflow
    supports rapid and simplified pipeline development using expressive Java and Python
    APIs in the Apache Beam SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Dataflow jobs are billed per minute, based on the actual use of workers
    in batch mode or streaming of Cloud Dataflow. Jobs that use other GCP resources,
    such as Cloud Storage or Cloud Pub/Sub, are billed based on the price of the corresponding
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored Google Cloud Dataprep, a service useful to preprocess
    the data, extract features, and clean up records. We got into practical details
    with real-life examples. We started by taking a look at the Cloud application
    interface to discover some preliminary information needed to access the platform.
    We then analyzed the techniques available for the preparation of the most suitable
    data for analysis and modeling, which includes the imputation of missing data,
    detecting and eliminating outliers, and mismatched values treatment. We discovered
    different ways to transform data and the degree of cleaning data. Then we learned
    how to normalize our data, in which data units are eliminated, allowing us to
    easily compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
