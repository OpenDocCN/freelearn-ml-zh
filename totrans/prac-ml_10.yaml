- en: Chapter 10. Regression based learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regression analysis allows us to mathematically model the relationship between
    two variables using simple algebra. In this chapter, we will focus on covering
    another supervised learning technique: regression analysis or regression-based
    learning. In the previous chapter, we covered the basics of statistics that will
    be of use in this chapter. We will start with understanding how multiple variables
    can influence the outcome, and how statistical adjustment techniques can be used
    to arbitrate this influence, understand correlation and regression analysis using
    real world examples, and take a deep dive into concepts such as confounding and
    effect modification.'
  prefs: []
  type: TYPE_NORMAL
- en: You will learn the basic and advanced concepts of this technique and get hands-on
    implementation guidance in simple, multiple linear regression, polynomial regression
    and logistic regression using Apache Mahout, R, Julia, Apache Spark, and Python.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, readers will have understood the uses and limitations
    of regression models, learned how to fit linear and logistic regression models
    to data, statistically inferencing the results and finally, assessing and diagnosing
    the performance of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts different learning models covered in this book,
    and the techniques highlighted in orange will be dealt in detail in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression based learning](img/B03980_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The topics listed here are covered in depth in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to correlation and regression analysis; revision of additional
    statistical concepts such as covariance and correlation coefficients. We will
    cover the properties of expectation, variance, and covariance in the context of
    regression models and ANOVA model and diagnostics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will learn simple, linear and multiple linear regressions: linear relationships,
    linear models, basic assumptions (normality, homoscedasticity, linearity, and
    independence), and least squares estimation. Overall, you will learn model diagnostics
    and selection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will be presented with an overview of generalized linear models (GLMs) and
    a listing of the regression algorithms under GLM. Also, the phenomena of confounding
    and effect modification will be presented, and hence realization and adjustments
    for the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to logistic regression, understanding odds and risk ratios,
    model building logistic regression models, and assessing the same will be covered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample implementation using Apache Mahout, R, Apache Spark, Julia, and Python
    (scikit-learn) libraries and modules will also be covered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Under supervised learning techniques, the learning models that are categorized
    under statistical methods are instance-based learning methods, Bayesian learning
    methods, and regression analysis. In this chapter, we will focus on regression
    analysis and other related regression models. Regression analysis is known to
    be one of the most important statistical techniques. As mentioned, it is a statistical
    methodology that is used to measure the relationship and check the validity and
    strength of the relationship between two or more variables.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, researchers, analysts, and traders have been using regression
    analysis to build trading strategies to understand the risk contained in a portfolio.
    Regression methods are used to address both classification and prediction problems.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered some key statistical concepts in the earlier chapter; in this
    chapter, we will cover some more concepts that are quite relevant in the context
    of regression analysis. To name a few concepts, we have the measurement of variability,
    linearity, covariance, coefficients, standard errors, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression analysis](img/B03980_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Revisiting statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the earlier chapter, where we learned the Bayesian learning methods, we covered
    some core statistical measures such as mean, median, mode, and standard deviation.
    Let's now extend this to some more measures such as variance, covariance, correlation,
    and the first and second moments of the distribution of a random variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Variance** is the square of standard deviation. If you recollect what standard
    deviation is, it is an average measure of how much each measurement in the sample
    deviates from the mean. It is also called the standard deviation of the mean.
    We can theoretically compute standard deviations for mode and median.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The range** is defined as a span of values over which the dataset is spread.
    Range usually is represented as minimum and maximum values.'
  prefs: []
  type: TYPE_NORMAL
- en: Quartiles, deciles, and percentiles subdivide a distribution of measurements
    that are similar to the median. The median is known to divide the distribution
    into half while quartile, decile, and percentile divide the distribution into
    1/25, 1/10 and 1/100 respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**First quartile** (designated Q1) OR lower quartile is the 25th percentile.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Third quartile** (designated Q3) OR upper quartile is the 75th percentile.'
  prefs: []
  type: TYPE_NORMAL
- en: interquartile range = third quartile – first quartile
  prefs: []
  type: TYPE_NORMAL
- en: '**Symmetric** and **skewed data**: Median, mean and mode for symmetric, positively,
    and negatively skewed data is represented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Symmetric distribution has equal mean and median values. For a positively skewed
    distribution, the median is greater than the mean and for a negatively skewed
    distribution, the mean value is greater than the median's value.
  prefs: []
  type: TYPE_NORMAL
- en: The outlier is an observation that is separated from the main cluster of the
    data. It can impact measures such as mean in a very significant way. Let's take
    an example to understand this better. We want to understand the average wealth
    of a group of five people. Say, the individual assets are valued at USD 1M, USD
    1.2M, USD 0.9M, USD 1.1M, and USD 12M.
  prefs: []
  type: TYPE_NORMAL
- en: 1+1.2+0.9+1.1+12=16.2
  prefs: []
  type: TYPE_NORMAL
- en: 16.2/5=3.24
  prefs: []
  type: TYPE_NORMAL
- en: 'The last observation had an unrealistic impact on the measurement. Let''s now
    see how the median is impacted. Let''s sort the assets in ascending order: 0.9M,
    1.0M, 1.1M, 1.2M, and 12M. The median is 1.1M. There are two important concepts
    we must understand. Outliers influence mean more significantly than the median.'
  prefs: []
  type: TYPE_NORMAL
- en: So, you should check the data carefully before choosing the correct statistical
    quantity.
  prefs: []
  type: TYPE_NORMAL
- en: Mean represents the average value of the variable and median represents the
    value of the average variable.
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance** is when there are two or more variables of interest (such as
    stocks of companies, physical properties of materials, etc.); it becomes important
    to understand whether there is any relation between them. Precisely, what we want
    to understand is if one of them is varying how does the other variable vary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In statistics, two terms explain this behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is known as covariance. For a data set comprising *n* points
    of two variables *x* and *y*, the following equation depicts the computation of
    covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: However, covariance can be a very large number. It is best to express it as
    a normalized number between -1 and 1 to understand the relation between the quantities.
    This is achieved by normalizing covariance with standard deviations of both the
    variables (*sx* and *sy*).
  prefs: []
  type: TYPE_NORMAL
- en: This is called **correlation coefficient** between *x* and *y*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Correlation** measures the strength of linear dependence between *X* and
    *Y* and lies between -1 and 1\. The following graph gives you a visual understanding
    of how the correlation impacts the linear dependence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Before we get into the specifics of various regression models, let's first look
    at the steps for implementing a regression model and analyzing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean and variances are the *first and second moments* of the probability
    distribution functions of random variables. They are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: After computing the probability distribution for a given random variable, we
    will compute the mean variance through simple integration.
  prefs: []
  type: TYPE_NORMAL
- en: Let's compute all these measures using a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the data of the stock prices of three companies (company A, company
    B, and company C) during a period of 14 days. First, compute the returns using
    the next formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Returns = (current day's price-yesterday's price)/yesterday's price
  prefs: []
  type: TYPE_NORMAL
- en: From this return, compute mean, median, and pairwise correlation. Do not use
    the built-in libraries. Use the base formulae even if you use Excel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First, let's compute the returns using the formula given previously.
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we had to compute the mean, the values would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To find the median, we will first sort the return values in ascending order
    and then mark the mid value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Finally, let's compute the covariance and then correlations using the formulae
    given in the previous covariance section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Revisiting statistics](img/B03980_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Properties of expectation, variance, and covariance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's combine the understanding of the previous and current chapters and conclude
    them.
  prefs: []
  type: TYPE_NORMAL
- en: This distribution of a variable is the probability of taking a particular value.
    The expectation is the population mean (which is the probability of the weighted
    average).
  prefs: []
  type: TYPE_NORMAL
- en: We can define a variance and standard deviation of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if we are looking at two different variables, we can define covariance
    and correlations. Now, let''s understand how the expectations and variance of
    two groups can be computed. This becomes particularly useful in the next sections
    where we will analyze two variables together for linear regression is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(x+y) = E(x) + E(y)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(x+a) = E(x) + E(a) = a + E(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(kx) = kE(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a very interesting rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Properties of expectation, variance, and covariance](img/B03980_10_49.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Essentially, this rule says that if we have a portfolio of properties in a
    given fraction, then the total expectation is the weighted sum of the individual
    expectations. This is a crucial concept in the portfolio of analytics. If there
    is a portfolio of 30% company A, 50%, company B, and 20% company C stocks, the
    expected return of our portfolio is:'
  prefs: []
  type: TYPE_NORMAL
- en: E (Portfolio) = 0.3 E(Company A) + 0.5 E(Company A) + 0.2 E (Company A)
  prefs: []
  type: TYPE_NORMAL
- en: Properties of variance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given *X*, a random variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Var(X+Y) = Var(X)+Var(Y)+2Cov(X,Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*V(x+a) = V(x)* (the variance does not change when a constant is added)'
  prefs: []
  type: TYPE_NORMAL
- en: '*V(ax) = a2 V(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s prove this as it is not obvious:'
  prefs: []
  type: TYPE_NORMAL
- en: Say, *Y= aX*
  prefs: []
  type: TYPE_NORMAL
- en: '*E(Y) = an E(X)* (from the previous set of relations)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y-E(Y) = a(X-E(X))*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Squaring both sides and taking expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(Y-E(Y))² = a² E(X-E(x))²*'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the left-hand side is the variance of *Y,* and the right-hand side
    is the variance of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Var (Y) = a²Var(X)*'
  prefs: []
  type: TYPE_NORMAL
- en: Another couple of interesting properties of variance can be derived from the
    above. It follows directly that
  prefs: []
  type: TYPE_NORMAL
- en: '*Var (-y) = Var (y)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the variance of the portfolio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Properties of variance](img/B03980_10_50.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, if you have a portfolio of three stocks, the variance of your portfolio
    (or the standard deviation that is its square root) varies as shown previously.
    The standard deviation is often called the risk of the portfolio. Ideally, it
    needs to be as low as possible. From the previous formula, this can be done in
    two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By selecting the elements whose variance is very low
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By selecting the elements whose covariance is very negative
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a crucial approach to a successful investment.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of covariance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following are the properties of covariance:'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(X, Y) = E[XY] − E[X]E[Y]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(x, a) = 0*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(x, x) = var(x)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(y, x) = cov(x, y)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(ax, by) =abcov(x, y)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(X+a, Y+b) = cov (X, Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cov(aX+bY, cW+dV) = accov(X,W) + adcov(X,V) + bccov(Y,W) + bdcov(Y,V)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cor(X,Y) = E[XY]/σXσY*'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see this using a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two of your best friends, Ana and Daniel, are planning to invest in stock markets.
    As you are the most experienced investor in your friends circle, they approached
    you for advice. You know Daniel can handle a 10% risk whereas Ana wants the least
    possible risk. You obviously want to maximize the returns for both. They both
    want to invest in three items: gold bonds, a top IT company, and a top bank.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/B03980_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: SD—Standard Deviation
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlations can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/B03980_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's derive the advice systematically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create a list of all the possible weights (assuming you need to
    compute up to a single decimal point) for three assets. There can be approximately
    66 values possible. This means that our friends must pick from one of these choices
    to invest. Now, calculate the returns for each possible portfolio (a unique combination
    of weights) using the following formula (again use any language you like):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Return from portfolio = W*[g] *X R*[g] *+ W*[i] *X R*[i] *+ w*[b] *X R*[b]*W*[g]'
  prefs: []
  type: TYPE_NORMAL
- en: '*W*[i]*, W*[b] *= weights* and'
  prefs: []
  type: TYPE_NORMAL
- en: '*R*[i]*, R*[g]*, R*[b] *= returns*'
  prefs: []
  type: TYPE_NORMAL
- en: This is because the expectation of a portfolio is the summation of the expectations
    of individual portfolio multiplied by individual weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values for the first five portfolios are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/B03980_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Compute all the other values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the risk of each portfolio using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Return = Sqrt ((wg*sdg)² + (Wi*sdi)² + (Wb*sdb)2 + (2*Wg*sdg*Wi*sdi*rgi)+ (2*Wi*sdi*Wb*sdb*rib)
    + (2*Wb*sdb*wg*sdg*rbg))
  prefs: []
  type: TYPE_NORMAL
- en: '*sdg, sdb, sdi = Risks and rij = correlations of i and j*'
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly the same formula for the variance of a portfolio as given in
    one of the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/B03980_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now lets compute all the other values.
  prefs: []
  type: TYPE_NORMAL
- en: Now, all that is needed is to recommend the balanced portfolios for both Ana
    and Daniel as their risk appetites are known to you. As Ana prefers zero risk,
    we will pick the point that corresponds to 17.2% returns and 0.87 risks. You can
    look up in the table and confirm that this is obtained with the portfolio of 0.7,
    0.2, and 0.1 (Gold, IT, and Bank). As Daniel can take 10% risk, we will see the
    portfolio that corresponds to 10% risk, which has the highest return.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this can be read as 0.2, 0.7, and 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA and F Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In cases like bivariate and multivariate distributions, a good quantity to understand
    is the way the variance is distributed within the populations or groups and between
    the populations or groups. This is the process of grouping data into multiple
    subsets. As you can clearly see, in such situations it really helps to know how
    variance is distributed among them. Such an analysis is called the **ANOVA** (**Analysis
    of Variance**). The calculations involved are fairly straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take three samples that have their own mean and distribution as depicted
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA and F Statistics](img/B03980_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And in terms of an example, see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sample 1= {3, 2, 1}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sample 2= {5, 3, 4}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sample 3= {5, 6, 7}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean for Sample 1 = 2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean for Sample 2 = 4*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mean for Sample 3 = 6*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Overall grand mean = (3+2+1+5+3+4+5+6+7) / 9 = 4*'
  prefs: []
  type: TYPE_NORMAL
- en: The grand mean (which will be the population mean if the groups cover the entire
    population) is equal to the mean of means.
  prefs: []
  type: TYPE_NORMAL
- en: Is it possible that the three means come from the same population? If one mean
    value is very different or far from the others, would that mean they are not from
    the same population? Or are they equally far apart?
  prefs: []
  type: TYPE_NORMAL
- en: All the previous samples are about relative distance measures from the grand
    mean.
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA and F Statistics](img/B03980_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now compute the sum of the squares of the entire sample set:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(3 − 4)² + (2 − 4)² + … = 30*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have calculated the variance by dividing the previously mentioned
    quantity with the degrees of freedom *(n*m-1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n*—number of elements in each sample'
  prefs: []
  type: TYPE_NORMAL
- en: '*m*—number of samples'
  prefs: []
  type: TYPE_NORMAL
- en: 'The property that we are trying to establish does not change. Hence, let''s
    just stick with the sum of squares instead of the variance. Now, let''s compute
    two quantities: the sum of squares of the group and between the groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The sum of squares of the group**: Let''s take the first group (3, 2, and
    1) where the mean is 2\. The variation (we are not calling it variance. But, it
    is definitely a measure of variance) within the group is equal to *(3-2)2+…=2*.
    Similarly, variation within group 2 and group 3 are equal to *2* and *2*. So,
    the total variation contributed within the groups is 6\. The total number of the
    degrees of freedom within each group is *n-1*. The total degrees of freedom is
    *(n-1)*m*. This is 6 in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The sum of squares between the groups**: This is measured as the distances
    between the mean of the group, and the grand mean, which is multiplied by the
    number of elements in the group mean of group 1 is 2, and grand mean is 4\. So,
    the variation of this group from the grand mean is *(2-4)2 * 3 = 12*. The variation
    for the second group is 0 and for the third is 12\. So, the variation between
    the groups is 24\. The degree of freedom, in this case, is *m-1 = 2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, let''s document this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA and F Statistics](img/B03980_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, we see that of the total variation of 30, 6 is contributed by variation
    within and 24 is contributed by variation between the groups. So most likely,
    it makes sense to group them separately. Now, let's do some kind of inferential
    statistics here. Let's assume that the previous values are the ranks obtained
    by three coaching centers. We want to know whether putting people in a coaching
    center actually has an impact on their final rank.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a hypothetical argument.
  prefs: []
  type: TYPE_NORMAL
- en: '**Null Hypothesis** is that coaching centers do not have an impact on the rank.
    Alternative coaching centers do have an impact on the rank.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ANOVA and F Statistics](img/B03980_10_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we observe, this measure is not about the values being equal, but it would
    be a check if the samples come from the same larger population. This measure is
    called the variability among or between the sample means.
  prefs: []
  type: TYPE_NORMAL
- en: 'So in short, ANOVA is a variability ratio represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA = Variance Between / Variance Within = Distance between the overall mean
    / internal spread
  prefs: []
  type: TYPE_NORMAL
- en: Total Variance = Variance Between + Variance Within
  prefs: []
  type: TYPE_NORMAL
- en: 'This process of separating total variance into two components is called partitioning:'
  prefs: []
  type: TYPE_NORMAL
- en: If the variance between the means is > variance within the means, it will mean
    that the variability ratio is > 1\. Hence, we can conclude that the samples do
    not belong to the same population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the variance between the means and within the means is similar, then the
    ratio almost becomes 1, and this would indicate an overlap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the variance between the means < the variance with the means, it will mean
    that the samples are close to the overall mean or the distributions *melt* together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, as we can see while dealing with multiple variables that there can be many
    factors that influence the outcome. Each of these variables will need to be assessed
    for the independent effect on the relationship between variables. In the next
    section, two concepts, **confounding** and **effect modification**, will explain
    the different types of influence factors on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Confounding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start understanding what confounding is using an example. Let's assume
    we are doing a study where we want to determine if the risk of developing heart
    disease has anything to do with smoking. When a study was done on sample data
    that had a mix of smokers and non-smokers and those who were detected to have
    a heart disease over a period of time, a measure of association such as a *risk
    ratio* was done, and it was found to be 2.0\. This can be interpreted as the risk
    of a smoker developing a heart disease being twice as much as that of a non-smoker.
    Now, when we look closely at the data, let's assume that we find the age distribution
    among the smokers and non-smokers is not the same, and it turns out that the age
    of smokers in the sample is much higher than the age of non-smokers. If we had
    to correlate this piece of information, is the outcome of developing heart disease
    to do with the old age, smoking or both?
  prefs: []
  type: TYPE_NORMAL
- en: An ideal way of measuring the quantitative effect of smoking on developing heart
    disease is to take a sample of people, observe them smoke over a period of time,
    collect the data on heart disease development, use the same set of people, and
    go back in time to run the same assessment when they are not smoking. This would
    help measure the counterfactual outcomes. The same group of people represents
    both smokers and non-smokers. Since this is not a possibility, we need to assume
    there is *exchangeability*. Non-smokers describe smokers if they ever smoke and
    vice-versa. This, in other words, means the two groups are comparable in all respects/aspects.
    In the cases where the data samples are not comparable, the condition is termed
    as confounding, and the property that is responsible for it (in this case, age)
    is called the **confounder**. If we have to explain this with an example, the
    fact that all non-smokers are younger, the non-smokers will under-estimate the
    outcome of older smokers had they not smoked.
  prefs: []
  type: TYPE_NORMAL
- en: 'This condition can be represented as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Confounding](img/B03980_10_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What we observe is that there is a backdoor pathway (through the age property).
    Confounding can thus be defined in a much simpler term, that is, *the existence
    of a backdoor pathway*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Confounding is a situation in which the effect or association between exposure
    and outcome is distorted by the presence of another variable.
  prefs: []
  type: TYPE_NORMAL
- en: Effect modification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Effect modification is the condition when exposures have different values for
    different groups. This can be observed when the measures of association estimation,
    like odds ratio, rate ratio, and risk ratio values, are very close to a weighted
    average of group-specific estimates from the association.
  prefs: []
  type: TYPE_NORMAL
- en: The effect modifier is the variable that differentially (this can mean positively
    or negatively) modifies the observed effect on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example. Breast cancer can occur both in men and women; the
    ratio occurs in both men and women, but the rate at which it occurs in women is
    800 times more than men, and the gender factor is a differentiating one for obvious
    reasons.
  prefs: []
  type: TYPE_NORMAL
- en: If the effect modifier is not properly identified, this could result in an incorrect
    crude estimate, and this results in missing the opportunity to understand the
    relationship between the risk factor and the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps need to be followed to study the effect modification for
    analyzing the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather information on potential effect modifiers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Study the effect of the effect modifier, measure the difference, and hold on
    from matching the values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stratify the data by potential effect modifiers and calculate estimates of the
    effect of the risk on the outcome. Determine if effect modification is present.
    If so, the estimates can be presented/used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To review, confounders mask a true effect and effect modifiers mean that there
    is a different effect for different groups.
  prefs: []
  type: TYPE_NORMAL
- en: Regression methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we learned, regression allows us to model the relationship between two or
    more variables, especially when a continuous dependent variable is predicted,
    based on several independent variables. The independent variables used in regression
    can be either continuous or dichotomous. In cases where the dependent variable
    is dichotomous, logistic regression is applied. In cases where the split between
    the two levels of dependent variables is equal, then both linear and logistic
    regression would fetch the same results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression methods](img/B03980_10_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Assumptions of regression (most apply to linear regression model family)
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample cases size**: In order to apply regression models, the cases-to-**Independent
    Variables** (**IVs**) ratio should ideally be 20:1 (for every IV in the model,
    there need to be 20 cases), the least being 5:1(5 cases for every IV in the model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data accuracy**: Regression assumes the basic validity of data, and it is
    expected to run basic data validations before running regression methods. For
    example, if a variable can have values between 1-5, any value not in the range
    will need to be corrected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers**: As we learned, outliers are those data points that usually have
    extreme values and don''t naturally appear to be a part of the population. Regression
    assumes that the outlier values are handled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing data**: It is important to look for missing data and address the
    same. If a specific variable has many missing values, it might be good to eliminate
    the variable unless there are too many variables with many missing values. Once
    the regression process is run, the variable that has no values can be a candidate
    for exclusion. And to avoid the risk of losing data through elimination, missing
    value techniques will need to be applied'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normal distribution**: It is necessary for the data to be checked to ensure
    that your data is normally distributed. Plotting data on a histogram is a way
    to check if the data is normally distributed. The following histogram is an example
    of normal distribution:![Regression methods](img/B03980_10_24.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear behavior**: Linear behavior is, in simple terms, seeing a straight
    line relationship between the dependent and independent variables. Any non-linear
    relationship between the IV and DV is ignored. A bivariate scatterplot is used
    to test for linearity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homoscedasticity**: Homoscedasticity refers to the constant changes to an
    independent variable for a change in the dependent variable. The following scatter
    plot is an example of data being homoscedastic, and we can see the concentration
    of plottings in the center:![Regression methods](img/B03980_10_25.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to the assumption of linearity, violation of the assumption of homoscedasticity
    does not invalidate regression but weakens it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multicollinearity and singularity**: Multicollinearity is a case where independent
    variables are highly correlated. In the case of singularity, the independent variables
    are perfectly correlated and, usually, one IV is a combination of one or more
    other IVs. Both multicollinearity and singularity can be easily identified using
    the correlation between IVs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the following section onwards, we will cover each of the regression methods
    in depth as listed in the concept map here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression methods](img/B03980_10_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Simple regression or simple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this case, we will be working with just two variables; one dependent variable
    and another independent variable. Simple linear regression is all about comparing
    two models; one where there is no independent variable and the best fit line is
    formed using the dependent variable, and the other that uses the best-fit regression
    line. Now let's look at an example to understand the best fit line and regression
    line definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a real-world example. Let''s assume there is a real-estate
    dealer and for every real-estate transaction that he does, he gets a commission.
    Very obviously, the commission amount dependents on the value of the transaction;
    the higher the value of the transaction, the higher the commission. So in this
    case, the commission becomes a dependent variable, and the transaction amount
    becomes an independent variable. In order to predict what could possibly be the
    next commission amount, let''s consider the sample data of the last six transactions
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s assume that we do not have data for the overall transaction amount.
    If we were to predict the next commission given in the previous data, we start
    by plotting it on a graph as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One of the options we have to identify the next commission amount that is given
    in the data is to compute the mean, which is the best prediction for the sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot this point on the graph and this would become the *best* fit. Plotting
    the mean value on the previous graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Computing the distance for each point from the mean gives the values that are
    shown in the next graph. This distance measure is called error or residual. The
    sum of the error for all the points is always found to be zero, and this is the
    measure of the goodness of the fit.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Plotting the distance on the graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We have learned in our earlier chapters about the **SSE** (**Sum Squared Error**)
    value. The error is squared because it makes the value positive and also emphasizes
    larger deviations. The following table shows the SSE values computed for the sample
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The overall goal of a simple linear regression is to build a model that minimizes
    SSE to a maximum extent. Until now, we have seen the best fit using a single variable,
    which is the dependent variable. Now, let's assume we get the data for another
    independent variable in our example. This, in fact, gets us a new regression line
    that is different from the best fit line that we arrived at previously. It is
    expected that the new independent variable should significantly reduce the SSE
    value. In other words, this new regression line should be a better fit for the
    given data.
  prefs: []
  type: TYPE_NORMAL
- en: If there is no difference in the earlier best-fit line and the regression line,
    this would mean that the identified independent variable has no influence on the
    outcome. Overall, simple linear regression is designed to find the best fitting
    line using the data that would have the least amount of SSE value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now add the independent variable data into our analysis—the real-estate
    transaction value, as shown in the table here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will plot a scatter plot between the dependent and the independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There could be multiple lines/equations possible in this context as shown in
    the next graph. In case the data seems to be falling in line, we can proceed.
    If the data points are scattered all over the place, this is an indication that
    there is no linearity in data, and we could choose to stop deriving the regression
    line. We could choose to compute the correlation coefficient here as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: r = 0.866
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that the relationship between the two variables is strong, and
    we can proceed to build the regression model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now compute the mean for the *x* and *y*-axis; here are the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These mean values are to then be plotted as a centroid onto the scattered plot,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The best-fit regression line has to go through the centroid that comprises
    the mean of the *x* and *y* variables. The calculations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_51.jpg)![Simple
    regression or simple linear regression](img/B03980_10_52.jpg)![Simple regression
    or simple linear regression](img/B03980_10_53.jpg)![Simple regression or simple
    linear regression](img/B03980_10_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The final regression line equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Plotting the previous equation on the scatter plot looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple regression or simple linear regression](img/B03980_10_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multiple regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiple regression is an extension of simple linear regression with one important
    difference, that there can be two or more independent variables used for predicting
    or explaining the variance in one dependent variable. Adding more independent
    variables does not necessarily make the regression better. There could potentially
    be two problems that could arise, one of which is over-fitting. We have covered
    this in the earlier chapters. Too many independent variables can add to the variance
    but in reality, they add nothing to the model thus causing over-fitting. Also,
    adding more independent variables adds more relationships. It is not only that
    the independent variables are potentially related to the dependent variables,
    but also there could be a dependency between the independent variables themselves.
    This condition is called multicollinearity. The ideal expectation is that the
    independent variables are correlated with the dependent variables, but not with
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of over-fitting and multicollinearity issues, there is a need for
    preparatory work before a multiple regression analysis work is to be started.
    The preparatory work can include computing correlations, mapping scatter plots,
    and running simple linear regression among others.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say, we have one dependent variable and four independent variables, and
    there is a multicollinearity risk. This means there are four relationships between
    the four independent variables and one dependent variable, and among the independent
    variables, there could be six more. So, there are 10 relationships to consider
    as shown here. DV stands for dependent variable and IV stands for independent
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple regression](img/B03980_10_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some independent variables are better than others for predicting the dependent
    variable, and some might not contribute anything to the prediction. There is a
    need to decide which one of the dependent variables to consider.
  prefs: []
  type: TYPE_NORMAL
- en: In multiple regression, each coefficient is interpreted as the estimated change
    in *y* corresponding to a one-unit change in the variable, while the rest of the
    variables are assumed constant.
  prefs: []
  type: TYPE_NORMAL
- en: The following are the multiple regression equations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Multiple regression](img/B03980_10_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's say we want to fit an independent variable as a function of a lot of variables
    (*x*, *y*, and *x*²). We can follow a simple procedure to get the coefficients
    of all the variables. This is applicable for linear, quadratic, and cubic functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the step-by-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Order all the points of each variable in a separate column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine all the columns of the independent variables to be represented as a
    matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a column to the 1's at the beginning of the matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name this matrix as *X* Matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a separate column matrix of all independent variables and call it *Y* Matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the coefficients using the formula here (this is the least square regression):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*B = (X*^T*X)*^(-1)*X*^T*Y*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is a matrix operation, and the resulting vector is the coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: In multiple regression, a lot of preparatory work needs to be done before running
    the regression model. It is necessary to step back and perform some analysis on
    the variables in consideration. Some basic scatter plots can be plotted to check
    for any correlations and to analyze the relationships between the dependent variables.
    Techniques like scatter plots, correlation analysis, and individual or group regressions
    can be used. In case there are any qualitative or categorical variables, we will
    need to use dummy variables to build the regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial (non-linear) regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the linear regression model *y = Xβ + ε* is a general model that will
    fit any linear relationship in the unknown parameter *β*, polynomial models are
    applicable in cases where the analyst knows that curvilinear effects are present
    in the true response function. Polynomial models are also used as approximating
    functions to the unknown and possibly very complex nonlinear relationship. The
    polynomial model is the Taylor series expansion of the unknown function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the two variables are linearly related, the scatterplot looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial (non-linear) regression](img/B03980_10_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous bivariate scatterplot, it is clear that there is a linear
    relationship between friends and happiness. The graph says *more friends, more
    happiness*. What if we talk about a curvilinear relationship between the variables,
    the number of friends and happiness? This means as the number of friends grows,
    the happiness grows but only to a certain point. The following graph shows this
    behavior in data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polynomial (non-linear) regression](img/B03980_10_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the data is not linear, then the process is to make it linear by transforming
    IVs or the DV so that there is a linear relationship between them. This transformation
    will not always work as there might be a genuine non-linearity in data and behavior.
    In this case, we will need to include the square of the independent variables
    in the regression. This is also known as a polynomial/quadratic regression. The
    **least squares** method is used to fit a polynomial regression model as it minimizes
    the variance in the estimation of the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Generalized Linear Models (GLM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the reasons why a linear regression model does not work.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression is a quantitative variable predicting another, multiple
    regression. It is an extended simple linear regression, but with more independent
    variables and finally, a nonlinear or polynomial regression is the case where
    there are two quantitative variables, but the data is curvilinear.
  prefs: []
  type: TYPE_NORMAL
- en: Now, running a typical linear regression, in the same way, has some problems.
    Binary data does not have a normal distribution. This is where the need for other
    regression models comes in. Secondly, the predicted values of the dependent variable
    can be beyond 0 and 1, which is against the concept of probability. Finally, probabilities
    are often non-linear and can take majorly low or high values at the extremes.
  prefs: []
  type: TYPE_NORMAL
- en: GLM is a generalization of linear regression that supports cases where the independent
    variables can have distribution error models other than normal distribution. GLM
    generalizes linear regression as it allows the linear model to be related to the
    independent variable through a link function, and it also allows the degree of
    the variance of each measure is a function of its predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: In short, GLM generalizes linear, logistic, and Poisson regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression (logit link)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression is an extension of linear regression where the dependent
    variable is a categorical variable that is responsible for the classification
    of the observations.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if *Y* denotes whether a particular customer is likely to purchase
    a product (1) or unlikely to purchase (0), we have a categorical variable with
    two categories or classes (0 and 1). Logistic regression can solve a classification
    problem where the class is unknown. This is done using the predictor values classifying
    a new observation, where the class is unknown, into one of the classes, based
    on the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying customers as returning (1) or non-returning (0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting if a loan would be approved or rejected, given the credit score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the important uses can be to find similarities between predictor values.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start taking a deep dive into logistic regression, let's revisit the
    concept of probability and the odds that were covered in the earlier chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Probability = outcomes of interest / all possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a fair coin is tossed, *P(heads) = ½ = 0.5*. When a dice is
    rolled, *P(1 or 2) = 2/6 = 1/3 = 0.33*. In a deck of cards, *P(diamond card) =
    13/52 = ¼ = 0.25*.
  prefs: []
  type: TYPE_NORMAL
- en: Odds = P(something happening)/P(something not happening) = p/1-p
  prefs: []
  type: TYPE_NORMAL
- en: For example, when a coin is tossed, *odds(heads = 0.5/0.5= 1)*. When a dice
    is rolled, *odds(1 or 2) = 0.333/0.666 = ½ = 0.5*. In a deck of cards, *odds(diamond
    card) = 0.25/0.75 = 1/3 = 0.333*.
  prefs: []
  type: TYPE_NORMAL
- en: The odds ratio is the ratio of two odds.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, when a coin is tossed, in case of a fair flip:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(heads) = ½= 0.5* and *odds(heads) = 0.5/0.5 = 1 = 1:1*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In case of a loaded coin flip:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(heads) = 0.7 and odds(heads) = 0.7/0.3 = 2.333*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Odds ratio= odds1/odds0 = 2.333/1 = 2.333*'
  prefs: []
  type: TYPE_NORMAL
- en: This means the odds of getting a heads when a loaded coin is flipped is 2.333
    times greater than a fair coin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, logistic regression seeks to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model** the probability of the event occurring depending on the values of
    the independent variables, which can be categorical or numerical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimate** the probability of an event occurring versus not occurring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predict** the effect of a set of variables on a binary response variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classify** the observations to belong to a particular category based on the
    probability estimation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Odds ratio in logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The odds ratio for a variable in logistic regression denotes how the odds for
    one variable changes with the increase of a unit in that variable, keeping the
    rest of the variables constant.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example to understand this—whether the body weight is dependent
    on sleep apnea or not. Let's assume that the body weight variable has an odds
    ratio of 1.07\. This means one pound increased in weight could potentially increase
    the odds of having slept apnea by 1.07 times. This might not be significant. In
    the case of a 10-pound increase in weight, the odds increase to 1.98, which doubles
    the odds of the person having slept apnea. It is important that we separate the
    probability and the odds measures. For example, though the increase in weight
    by 20 pounds increases the odds of the person having slept by 4 times, the probability
    that the person's weight has increased by 20 pounds could potentially be very
    low.
  prefs: []
  type: TYPE_NORMAL
- en: 'In logistic regression, there are two important steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the probability of belonging to a particular class. So, if *Y = 0* or
    *1*, the probability of belonging to *class 1* is *P(Y=1)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will need to use the cut-off values of the probabilities to ensure that each
    case gets into one of the classes. In case of binary cut-off, a *P(Y=1) > 0.5*
    will be categorized as *1* and *P(Y=0) < 0.5* will be categorized as *0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*y*[i] is normally distributed and takes the value of either 0 or 1 for *i
    = 0,1,…,n*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[i] is equal to {0, 1} where *P(y*[i] *= 1) = p* and *P(y*[i] *= 0) = 1-p*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Y = a + bx for P(y*[i]*= 1)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*[i] *= a + bx*[i]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that *p*[i] will not take values between (0, 1). This is fixed by using
    a non-linear function of predictors such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/B03980_10_46.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, this takes a value between 0 and 1 as *x* varies from -∞ to ∞. From
    this, *a+bx*[i] can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/B03980_10_47.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following curve shows how the function varies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/B03980_10_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Poisson regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Poisson regression, in the context of GLM, is a count of data with the independent
    variable having Poisson distribution and the link function applied is a logarithm
    of the response that can be modeled using a linear combination of unknown parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing linear and logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the source code provided for this chapter for implementing linear regression.
    (source code path `.../chapter10/...` under each of the folders for the technology)
  prefs: []
  type: TYPE_NORMAL
- en: Using Mahout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter10/linearregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../mahout/chapter10/logisticregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter10/linearregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../r/chapter10/logisticregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter10/linearregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../spark/chapter10/logisticregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter10/linearregressionexample/`
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../python-scikit-learn/chapter10/logisticregressionexample/`
  prefs: []
  type: TYPE_NORMAL
- en: Using Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter10/linearregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the folder `.../julia/chapter10/logisticregressionexample/`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned regression analysis-based machine learning and,
    in particular, how to implement linear and logistic regression models using Mahout,
    R, Python, Julia, and Spark. Additionally, we covered other related concepts of
    statistics such as variance, covariance, and ANOVA among others. We covered regression
    models in depth with examples to understand how to apply them to real-world problems.
    In the next chapter, we will cover deep learning methods.
  prefs: []
  type: TYPE_NORMAL
