- en: '*Chapter 13*: Inference'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will take an in-depth look at all of the fascinating things
    you can do with trained supervised models in the Elastic Stack. First, we will
    see how to use the Trained Models API to view information about the models available
    in our cluster, to see details about individual models, and to export models so
    that they can be ported to other Elasticsearch clusters. We will also take a brief
    look at how to use eland to import external models, such as those trained by third-party
    machine learning libraries, into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this chapter, we will go in-depth into how to use trained
    supervised models with inference in a variety of contexts to enrich data. To do
    this, we will learn about inference processors and ingest pipelines and how these
    can be combined with continuous transforms, reindexing, and at ingest time when
    using various beats or otherwise ingesting data into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Examining, importing, and exporting trained machine learning models using the
    Trained Models API and Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding inference processors and ingest pipelines and how to configure
    and use them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing external models into Elasticsearch using eland
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The material in this chapter will require an Elasticsearch cluster version
    7.10 or later, and an installation of Python 3.7 or later with the `eland`, `elasticsearch-py`,
    and `scikit-learn` libraries installed. For detailed instructions on how to configure
    your Python installation to work with this chapter, please see the README section
    in the `Chapter 13 - Inference and Advanced Transforms` folder in the book''s
    GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference).'
  prefs: []
  type: TYPE_NORMAL
- en: Examining, exporting, and importing your trained models with the Trained Models
    API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have prepared your dataset, trained your classification or regression model,
    looked at its performance, and determined that you would like to use it to enrich
    your production datasets. Before you can dive into ingest pipelines, inference
    processors, and the multitude of other components that you can configure to use
    your trained models, it is good to become familiar with the **Trained Models API**
    ([https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html)),
    a set of REST API endpoints that you can use to find out information about your
    models and even export them to other clusters. Let's take a tour of this API to
    see what it can tell us about our models.
  prefs: []
  type: TYPE_NORMAL
- en: A tour of the Trained Models API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will take a practical look at using the Kibana Dev Console
    to examine things about our trained supervised models:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start in the Kibana Dev Console. Briefly, for those not yet familiar with
    this tool, the Kibana Dev Console, which you can access by clicking through the
    left-hand side slide-out menu and scrolling down to the `model_id`. This field
    contains the unique identifier that is assigned to each model stored in the cluster
    and that is used to reference the model when it is later used in inference processors
    and ingest pipelines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Another piece of information provided by the Trained Models API is `analyzed_fields`,
    a dictionary that includes a list of included or excluded fields. It is good to
    double-check this as a sanity check to make sure that only the fields you intended
    to use for training were included in the training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1 – A snippet of the Inference API response illustrating the number
    of trained models in the cluster as well as information about one of the models'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.1 – A snippet of the Inference API response illustrating the number
    of trained models in the cluster as well as information about one of the models
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our demo cluster contains just three trained models, so the amount of information
    returned from the API is not overwhelming, but in case you are working on a cluster
    with tens or hundreds of models, it can be helpful to view the details of a single
    model at a time using the API call with the full name of the model. Note, if you
    are following along and want to run the subsequent API call in your specific instance
    of Kibana, you will have to look up and use the `model_id` of the model located
    in your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, use an API call with a wildcard:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A less verbose summary is available through the `_cat` API. We can use the
    following API call to see a brief summary of the models available:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response we receive from our cluster is displayed in *Figure 13.2*. You
    will notice that there are two models trained on the breast cancer dataset as
    well as a third model whose identifier is `lang_ident_model_1`. This is a language
    identification model that ships by default with Elasticsearch and can be used
    to identify which language a given string is likely to be in. We will look at
    how this language identification model works and how to use it later in this chapter:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.2 – The response from the _cat API'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_13_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – The response from the _cat API
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have taken a brief look at how we can examine and extract information
    about the trained models available in our cluster, let's take a closer look at
    one last powerful function of the Trained Models API – that of exporting the model
    from an Elasticsearch cluster. Since this procedure involves a few more moving
    parts than the last one, we have dedicated the next section to it.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting and importing trained models with the Trained Models API and Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why might you want to export a model trained in Elasticsearch? You may want
    to export your model so that you can either store it externally, share it with
    colleagues, or import it later into another Elasticsearch cluster. Since training
    machine learning models can be resource-intensive, you might want to provision
    one transient Elasticsearch cluster for training, train and evaluate the model
    on this cluster, and then export and re-import the model into another, smaller
    cluster so that you can perform inference – a less resource-intensive procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To export a model using Python and to follow the steps, you will need a Python
    installation using 3.7 or later and the `elasticsearch-py` library version 7.10.1\.
    For detailed instructions and further resources on how to configure a Python environment
    and install the required dependencies, please follow the instructions in this
    README file ([https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference))
    in the book''s GitHub repository. All the steps and logic required to export a
    model from an Elasticsearch cluster will be available in the `export_model.py`
    Python script at, [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export),
    in the book''s GitHub repository. In this section, we will look at each of the
    steps in the script to understand the components necessary to export a model.
    Hopefully, this treatment will give you the building blocks to use the `elasticsearch-py`
    client to construct your own machine learning workflows in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The building block of nearly all Python scripts that interact with an Elasticsearch
    cluster is the construction of the Elasticsearch client object. The client object''s
    class must first be imported from the library like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the class has been imported, we can create an instance of the object and
    assign it to the `es_client` variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we pass into the object's constructor the variable that holds the
    URL of the Elasticseach instance. This can either be something like `localhost:9200`
    if you are running an instance of Elasticsearch on your local machine, or a longer
    URL for cloud-based deployments. Additionally, we pass in two variables, `ES_USERNAME`
    and `ES_PASSWORD`, that hold the username and password of our Elasticsearch instance.
    This is not required if you are running an unprotected Elasticsearch cluster locally
    for development purposes, but please note that running an unsecured Elasticsearch
    cluster in production is extremely dangerous.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To interact with the Machine Learning APIs, which we will need to export our
    model from the cluster, we need to import the `MlClient` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create an instance of it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have these two clients created, we can proceed to export our model.
    To do this, we will use the `get_trained_models` method. The documentation for
    the method is at here, [https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack](https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack),
    and it is good to keep this reference handy when working with this library in
    case you need to double-check the meaning of parameters and configuration options.
    There are three important parameters to pass into the method: `model_id`, which
    specifies the name of the model you wish to export, the `decompress_definition`
    flag, which should be set to `False`, and the `include_model_definition` flag,
    which should be set to `True`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 13.3 – A snippet of the Python dictionary that captures the compressed
    definition of a model as well as metadata'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.3 – A snippet of the Python dictionary that captures the compressed
    definition of a model as well as metadata
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once we have this model definition stored in a `compressed_model` variable in
    our Python script, we can convert the dictionary to a JSON-formatted string and
    write it out to a file, which can be stored in version control or imported into
    another Elasticsearch cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To convert the dictionary to JSON format, we must import the built-in Python
    `json` library:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After this, we can write the exported model to a file whose path we have stored
    in the `filename` variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All of the preceding steps are summarized in the `export_model.py` script,
    which is available in the book''s GitHub repository here: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have seen how to export a trained model from an Elasticsearch cluster,
    let''s look at how to import a model from a file. As previously, we will break
    down the logic into steps, but the full working script will be stored in the book''s
    GitHub repository here: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the steps in this procedure are like the export script we stepped through
    in detail previously. In particular, the creation of the `Elasticsearch` and `MlClient`
    objects, as well as the parsing of the command-line arguments, follow similar
    steps as the preceding script, so we will not be explaining them in detail. Thus,
    the first step is to read the model file and convert the string contents into
    a Python dictionary using the `loads` file from the built-in `json` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we have the compressed `model_definition` and the required metadata loaded
    into our Python dictionary, we can use the `put_trained_model` method to upload
    it into our cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, navigate to the Kibana instance of your cluster and use the Trained
    Models API to double-check that the model has indeed been imported into your cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how to view details about a model as well as how to
    export and import models, we are ready to move on to building more complicated
    machine learning infrastructures without models. Once you have a trained model,
    the possibilities for the model are nearly endless – you can combine the model
    with transforms, use it to enrich your data at ingest time, and much more. The
    building blocks of this infrastructure are inference processors and ingest pipelines.
    We will take a detailed look at each of these two in the next chapter to get you
    ready for building your own machine learning infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding inference processors and ingest pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have a trained machine learning model, so now what? Remember from [*Chapter
    11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209), *Classification Analysis*, and
    [*Chapter 12*](B17040_12_Epub_AM.xhtml#_idTextAnchor230), *Regression*, that one
    of the exciting things about machine learning models is that they learn from a
    labeled training dataset and then, in a way, encode the knowledge so that they
    can be used to make predictions on previously unseen data points. This process
    of labeling or making predictions for previously unseen data points is what we
    call **inference**.
  prefs: []
  type: TYPE_NORMAL
- en: How does this happen in practice in the Elastic Stack?
  prefs: []
  type: TYPE_NORMAL
- en: There are a multitude of different architectures that you might build to make
    use of inference in the Elastic Stack, but the basic building blocks of all of
    them are inference processors and ingest pipelines. These are the main subjects
    of our exploration in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: An **ingest pipeline** is a special component that lets you manipulate and transform
    your data in various ways before it is written to an Elasticsearch index. Ingest
    pipelines are normally composed of various processors, which are sub-units or
    configurable tasks that each perform a single type of manipulation or transformation
    on the data being ingested. Ingest pipelines can consist of multiple processors
    that are performed sequentially on each incoming data document as it is being
    ingested.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a typical ingest pipeline architecture might involve a script processor,
    which is able to execute a Painelss script on each document ingested through the
    pipeline, followed by an inference processor followed by another script processor.
    For many machine learning applications, such as the ones we will look at a little
    bit later in this chapter, this is the perfect place to perform feature engineering
    or transform features into a suitable format for consumption by the machine learning
    model or to remove unnecessary fields before a document is ingested into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a variety of built-in processors that can be combined and customized
    to create complex data transformation pipelines. For example, the **GeoIP** processor
    adds geographical information about IP addresses, the script processor allows
    users to write custom painless code to perform calculations and manipulations
    on existing document fields, and the **CSV** processor enables parsing and the
    extraction of data from CSV values to create fields. The full list of processors
    is available in the Elasticsearch documentation here: [https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html).
    We encourage you to take a look to see the kinds of possible data architecture
    you can build with them.'
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, in terms of exploring machine learning, the most important
    processor to study is the inference processor. When documents pass through this
    processor, they are annotated with predictions by the machine learning model referenced
    in the processor's configuration. Let's take a look at how to configure our own
    inference processor and use it in an ingest pipeline with the help of a practical
    example.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will be using the fictitious social media dataset that we
    first examined in [*Chapter 9*](B17040_09_Epub_AM.xhtml#_idTextAnchor162), *Introducing
    Data Frame Analytics*. This time, we will be using a language identification model
    to identify which language the text in these fictional microblogging site posts
    is written in. Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: If you have been playing around with the Trained Models API we discussed at
    the beginning of this chapter, you may have noticed that even if you have not
    trained any models in a particular Elasticsearch cluster, you will still see a
    single model, `lang_ident_model_1`, available in the cluster. The metadata associated
    with this model returned by the Trained Models API is shown in *Figure 13.4*:![Figure
    13.4 – The language identification model lang_ident_model_1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17040_13_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.4 – The language identification model lang_ident_model_1
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The model ships by default with an Elasticsearch cluster and can be used in
    inference processors and ingest pipelines just like any other model you might
    train yourself on an Elasticsearch cluster!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, let's see how we can create an ingest pipeline configuration with an inference
    processor that references this language identification model. Recall that processors
    are the sub-units within an ingest pipeline that process each document as it enters
    the pipeline and before it is written to an Elasticsearch index. Even though you
    can never use an inference processor as a standalone functional unit in Elasticsearch
    – it must always be a part of a pipeline – let's first examine the configuration
    of the processor in isolation and then see how it fits into an Ingest pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet shows the configuration of an inference processor
    for our planned language identification text pipeline:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a moment to go over the most important configuration parameters
    and what they mean. For a full API reference of all the available configuration
    options for inference processors, please look at the Elasticsearch documentation
    here: [https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The key part of any inference processor is the trained machine learning model
    that will be used to make predictions on our text documents. The inference processor
    becomes aware of which model it should use to classify incoming documents through
    the `model_id` configuration field. In our case, the `model_id` (which is available
    in the metadata of the model that we can view by using the Trained Models API)
    is `lang_ident_model_1`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The field name listed under `field_names` in the model's metadata specifies
    that the name of the feature field used in the model is text, which means that
    the features we want to use in our inference documents need to have a field of
    this name that contains the text to which we wish to apply language identification.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since the training of the model is decoupled from the inference process, it
    is entirely possible that the field names chosen for the features during the training
    process are not available in the data that we wish to pass through our inference
    processor. In this case, if it is not possible or desirable to alter the field
    names of our data, we can use the `field_map` configuration block in our inference
    processor to map from the field name in the data we will be using for inference
    to the field name that our supervised model expects. Below, we have configured
    a mapping between the `post` field name that contains the text in our fictional
    microblogging social media dataset and the `text` field name that the model expects:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, we've come to the last part of the configuration – the `inference_config`
    block. The configuration options for this block determine whether we are using
    classification or regression. Since, in the case of language identification, we
    are working with multiclass classification, we will select classification and
    leave all the other configuration options as their default settings. Slightly
    later in this section, we will take a closer look at the available fields in `inference_config`
    and how adjusting them determines the final format of the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have examined the pieces that are part of configuring the inference
    processor, let's move on to configuring the ingest pipeline. This is the top-level
    container, if you will, or component that will house our inference processor and
    potentially others as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The configuration for an ingest pipeline that contains the inference processor
    we configured looks like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The bulk of the configuration is taken up by the specifications for the inference
    processor that we studied in detail previously. The only additional noteworthy
    features in the configuration of the ingest pipeline are the name of the pipeline,
    which is a part of the REST API endpoint, and the `processors` array in the body
    of the configuration. In this case, we have chosen to call the pipeline `language-identification-pipeline`.
    The `processors` array contains the configuration specifications for our processors.
    In this case, we only have one processor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can either copy and paste the following configuration into the Kibana Dev
    Console or, alternatively, use the Ingest Pipelines wizard, which is available
    in the **Stack** **Management** panel in Kibana, as shown in *Figure 13.5*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.5 – The Create pipeline wizard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.5 – The Create pipeline wizard
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once we have our pipeline configured – either through the Dev Console or the
    wizard, we are ready to start using it to identify the language in our fictional
    social media microblogging platform posts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Although normally, we would use an ingest pipeline together with a transform
    or a beat such as `packetbeat`, in this case, we are going to ingest documents
    into an index using the Kibana Dev Console since it is easier to illustrate the
    concepts we want to teach there. Slightly later in the chapter, we will look at
    more advanced and, hence, more realistic examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s index our first document through the pipeline. The REST API command
    that achieved this looks like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are sending a `POST` request to index the document in the body of the request
    and are passing the name of the pipeline we created in the preceding steps as
    the argument for the optional `pipeline` parameter. In this case, a user, "Sanna,"
    has written an update (as seen in the post field in Finnish). Let's examine the
    `social-media-feed-inference` index to see what the ingested document looks like.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you have not already done so, create an index pattern for the `social-media-feed-inference`
    index and navigate to the Discover app in Kibana. Now, the `social-media-feed-inference`
    index contains only the one document we indexed using the REST API call shown
    previously. The document is shown in *Figure 13.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.6 – An ingested document in the social-media-feed-inference index'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.6 – An ingested document in the social-media-feed-inference index
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we can see from the document in *Figure 13.6*, the inference processor has
    added four new fields alongside the original fields in the document. All these
    fields are prefixed with the name `text_language_prediction_model`, which is what
    we configured in our inference processor configuration. As we can see, the fields
    record the `model_id` of the model used to make the prediction, `predicted_value`,
    which in this case will contain an identifier for the language the model predicts
    the post to be in, `prediction_probability` as well as `prediction_score`. These
    were previously covered in [*Chapter 11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209),
    *Classification Analysis*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we can see, in this case, the model has determined correctly that the original
    post was written in the Finnish language.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the previous example, we created the inference processor and ingest pipeline
    configurations, and proceeded directly to index documents into our index through
    the pipeline. If we wish to first see a few dry runs of our pipeline before indexing,
    we can use the `_simulate` endpoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The response returned by the API for this call contains the results of the
    model''s predictions, as you can see in the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, it is also possible to use the Ingest Pipeline UI to test out the documents
    before beginning to ingest to make sure that everything is working as intended.
    Unfortunately, in the UI, this can only be performed during the creation of a
    new ingest pipeline and not with an existing one, so, for the purposes of this
    demonstration, you can use the wizard to start creating a clone of the `language-identification-pipeline`
    we created previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, to the right of the **Processors** selector in the wizard, as shown in
    *Figure 13.7*, locate the **Test pipeline** text and click on the **Add documents**
    link:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.7 – The Create Ingest pipeline wizard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_13_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.7 – The Create Ingest pipeline wizard
  prefs: []
  type: TYPE_NORMAL
- en: 'This will trigger a menu on the right-hand side of the wizard that allows you
    to either add a document from an index or manually specify it in the textbox provided.
    In this case, we are going to manually add our Finnish language test document
    to the textbox, as shown in *Figure 13.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8 – A sample document in the ingest pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_13_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.8 – A sample document in the ingest pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Once you have configured your documents, click the **Run the pipeline** button
    and you should see a preview of your documents after they have passed through
    the inference pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing or corrupted data in ingest pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many, or maybe even most, real-world applications will not have neat datasets.
    Instead, data will be missing, mislabeled, and potentially even corrupted. It''s
    important to take a moment to look at what happens in such cases with inference
    processors so that you can recognize and mitigate these issues in your own pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with the fictitious microblogging platform we used as our example
    previously and suppose that due to a misconfiguration error, we rename the `post`
    field, which contains the text string whose language we wish to detect, to `post_text`,
    as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What happens once we send this text through the `language-identification-pipeline`?
    Let's perform the REST API call shown previously and then look at the ingested
    document in the **Discover** tab, as we did in the previous section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As we can see from the document as shown in *Figure 13.9*, the model was not
    able to make a correct prediction about the language in which the text in the
    `post_text` field was written:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.9 – A warning message in an ingested document'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_13_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.9 – A warning message in an ingested document
  prefs: []
  type: TYPE_NORMAL
- en: Real-world use cases and datasets are often messy and contain missing and corrupt
    data, so be on the lookout for this message to catch and rectify potential errors
    in your inference setup!
  prefs: []
  type: TYPE_NORMAL
- en: The first step to troubleshoot why this message appears is to compare the fields
    in the documents you are trying to ingest through the ingest pipeline with the
    analyzed_fields stored in the model’s metadata. Refer back to the section *Examining,
    Exporting and Importing your Trained Models with the Trained Models API* in this
    chapter for tips on how to view a model’s metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Using inference processor configuration options to gain more insight into your
    predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous sections, we configured our inference processors in such a
    way that the documents processed by this processor only contained four fields:
    the predicted class label of the document, the probability of the prediction,
    the score of the prediction, and the model ID. However, what happens in cases
    where you can see that the model has made an incorrect prediction, or you would
    like to have more information about the probabilities that the model was assigned
    to other potential classes? This can be useful for debugging.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we configure the inference processor to provide us with more information?
    Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by going back to the inference processor configuration that we
    saw in the previous section and taking a closer look at the `inference_config`
    configuration block, which we left empty. In this case, since we want to see a
    more detailed breakdown of different probabilities, we want to add a `num_top_classes`
    field to the configuration block. This configuration parameter controls the number
    of classes for which the probabilities are written out. For example, if we set
    it to 3, each document will contain the probabilities for the top 3 classes it
    was most likely to belong to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s ingest a document through this new pipeline, `language-identification-pipeline-v2`,
    using the following REST API call:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will see that as a result, the inference processor writes out a detailed
    breakdown of the possible languages (or classes if we are using classification
    terminology) to which the post belongs, as shown in *Figure 13.10*. Possible candidates
    are Finnish, denoted by the keyword **fi**, Swedish, denoted by the keyword **sv**,
    and Estonian, denoted by the keyword **eo**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.10 – A detailed breakdown of the potential classes a given document
    belongs to along with the associated probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17040_13_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.10 – A detailed breakdown of the potential classes a given document
    belongs to along with the associated probabilities
  prefs: []
  type: TYPE_NORMAL
- en: We'll now move on to importing models using eland.
  prefs: []
  type: TYPE_NORMAL
- en: Importing external models into Elasticsearch using eland
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose you already have a model trained using one of the other frameworks.
    Is it possible to re-use the building blocks we discussed in the previous section
    to deploy your own externally trained models? The answer is yes, with a few limitations.
    In this section, we will take a look at how to use the **eland** library, along
    with **scikit-learn**, another machine learning library for creating and training
    external machine learning models and importing them into Elasticsearch for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about supported external models in eland
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, the inference functionality in the Elastic Stack does not yet
    have support for importing an externally trained machine learning model from any
    library (though it might at some point in the future!). Instead, the eland documentation
    ([https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model](https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model))
    contains a list of third-party libraries that produce supported models. As it
    stands currently, the list of supported model types is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn.tree.DecisionTreeClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.tree.DecisionTreeRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.ensemble.RandomForestRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.ensemble.RandomForestClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lightgbm.LGBMRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lightgbm.LGBMClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xgboost.XGBClassifier`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xgboost.XGBRegressor`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Please note that there are some additional restrictions to models generated
    with the preceding libraries that pertain to the type of objective function that
    is selected or the type of encoding that must be enforced on the features, so
    please do make sure you check the eland documentation for the most up-to-date
    information of supported third-party models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training a scikit-learn DecisionTreeClassifier and importing it into Elasticsearch
    using eland
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have learned about the important preliminaries, let''s hit the
    ground running and take a look at how we can train an external machine learning
    model using the scikit-learn library. All of the code examples used in this walk-through
    will be available in the Jupyter notebook in the book''s GitHub repository here:
    [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models):'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in our project to import an external model into Elasticsearch
    will be to retrieve some training data and use it to train a decision tree model.
    The scikit-learn library has a great collection of built-in datasets that can
    be used for learning and quick prototyping. To continue with the same data theme
    that we have been developing in [*Chapter 11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209),
    *Classification Analysis* and in this chapter, we will be using the built-in Wisconsin
    Breast Cancer dataset (which is a variation of the dataset that we have been using).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Before we begin, let''s make sure that we import all of the required functions
    and libraries into our Python script (or Jupyter notebook):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have our imports, let's load the dataset by calling the `load_breast_cancer`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that this function returns two values, which we store in the variables
    `X` and `y`. The way that Elasticsearch organizes its training data is different
    from the conventions in scikit-learn. In Elasticsearch, our training data was
    stored in a single Elasticsearch index. Each document in the index represents
    one data point and is a combination of fields that represent the features and
    the field that represents the dependent variable (see [*Chapter 11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209),
    *Classification Analysis* and [*Chapter 12*](B17040_12_Epub_AM.xhtml#_idTextAnchor230),
    *Regression*, for further information about dependent variables and why they are
    important in supervised learning).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In contrast with Elasticsearch''s approach, scikit-learn represents each data
    point using a vector that contains all of the feature values. These vectors make
    up the matrix stored in the variable `X`. We can use Python''s slicing syntax
    to see what a sample data point would look like. An example is shown in *Figure
    13.11*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.11 – A data point is represented as a vector of field values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.11 – A data point is represented as a vector of field values
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The dependent variables are stored in the separate variable `y`. In a similar
    fashion to our previous example, we can use Python''s slicing syntax to see which
    class the data point whose feature values (or **feature vector**) are depicted
    belongs to. This is shown in *Figure 13.12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.12 – The class label for the first data point is 0'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.12 – The class label for the first data point is 0
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have imported our dataset and verified that it looks acceptable,
    we can move on to the next step, which is training our decision tree model. While
    Elasticsearch automatically splits our training data into a training and a testing
    dataset, in scikit-learn, we have to perform this step manually. Although it''s
    not strictly necessary to do this in this case, since we are not overly interested
    in systematically measuring the performance of our model, we will still show this
    as an example for interested readers who will need to do this in their own projects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see from the preceding code snippet, we pass in the variable containing
    the feature vectors for every data point and the variable containing the dependent
    variable values or class labels into the scikit-learn function, `train_test_split`,
    and the function returns the feature vectors and dependent variables that belong
    to the training set (`X_train` and `y_train`) and testing set (`X_test`, `y_test)`,
    respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have our training and test dataset generated, we can move on to
    training the decision tree classifier. The first step is to create an instance
    of the `DecisionTreeClassifier` class and fit it with the feature vectors and
    class labels from the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The trained model is referenced by the `dec_tree` variable. This is the variable
    we will serialize and upload to Elasticsearch using eland slightly later in this
    tutorial. First, however, let''s run a quick check on our model by asking it to
    classify a data point from the testing dataset (remember, these are data points
    that the model has not seen previously during the training phase) as shown in
    *Figure 13.13*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Predictions from the trained decision tree model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.13 – Predictions from the trained decision tree model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The model predicts that the first data point in our testing dataset belongs
    to class 1\. We can double-check whether or not this is the actual label of the
    data point by checking the first element in the `y_test` variable, as shown in
    *Figure 13.14*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.14 – The value of the dependent variable for the first data point
    in the testing set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17040_13_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 13.14 – The value of the dependent variable for the first data point
    in the testing set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, the model's prediction matches the actual label of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, let''s prepare to upload this model into our Elasticsearch cluster
    using eland. First, we have to import the required `MLModel` class, as shown in
    the following code sample:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we have this class in our script or Jupyter notebook, we can proceed to
    the next step, which is retrieving the feature names from the original scikit-learn
    dataset. The steps required to do this are shown in the following code sample:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The interested reader can print out the `feature_names` variable (or take a
    look at the Jupyter notebook accompanying this walk-through) to see what kinds
    of features are included. In the interest of saving space, we will leave out the
    feature names list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we will call the `import_model` method on the `MLModel` class, as
    shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, this is a method that requires quite a number of parameters.
    The first parameter, `es_client`, is an instance of the Elasticsearch client object,
    which specifies how to connect to our Elasticsearch cluster. This is discussed
    in further detail in the *Exporting and importing trained models with the Inference
    API and Python* section in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The second parameter is `model_id`, an identifier that will be used to identify
    the model once it has been uploaded into the Elasticsearch cluster. In this case,
    we have set the `model_id` variable, as shown in the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Of course, however, it is possible to set this identifier to be one of your
    choosing. Finally, we pass in the variable name that contains a reference to our
    trained model, `dec_tree`, the list of `feature_names` that we retrieved from
    the original dataset, and set the `es_if_exists` flag to `'replace'`, which means
    that if we run the code snippet more than once, an existing model with the same
    `model_id` will be overwritten. There are cases where this might not be the desired
    behavior, but in this case, since we are prototyping, it is a useful flag to set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the command discussed in the preceding section has been run, we can use
    the Trained Models API to determine whether or not this model has been imported
    into our cluster successfully. To do this, we will run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, based on the returned API response, our model has indeed been
    successfully imported into the cluster and is now ready to use in ingest pipelines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reminder
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'All of the code samples used in the figures are available in the Jupyter notebook
    that is linked in the book''s GitHub repository here: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at the various options available for using supervised
    models trained in Elasticsearch and external libraries such as scikit-learn. We
    have learned about the Trained Models API, which is useful when managing and examining
    trained supervised learning models in an Elasticsearch cluster and how to make
    use of these models to make predictions on previously unseen examples with the
    help of inference processors and ingest pipelines. In the appendix following this
    chapter, we will provide some tips and tricks that make it easier to work with
    the Elastic Machine Learning stack.
  prefs: []
  type: TYPE_NORMAL
