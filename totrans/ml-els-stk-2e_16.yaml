- en: '*Chapter 13*: Inference'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：推理'
- en: In this chapter, we will take an in-depth look at all of the fascinating things
    you can do with trained supervised models in the Elastic Stack. First, we will
    see how to use the Trained Models API to view information about the models available
    in our cluster, to see details about individual models, and to export models so
    that they can be ported to other Elasticsearch clusters. We will also take a brief
    look at how to use eland to import external models, such as those trained by third-party
    machine learning libraries, into Elasticsearch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨在Elastic Stack中使用训练好的监督模型所能做的所有令人着迷的事情。首先，我们将看到如何使用训练模型API来查看我们集群中可用的模型信息，查看单个模型的详细信息，以及导出模型以便它们可以被移植到其他Elasticsearch集群。我们还将简要地看看如何使用eland将外部模型，例如由第三方机器学习库训练的模型，导入到Elasticsearch中。
- en: In the second part of this chapter, we will go in-depth into how to use trained
    supervised models with inference in a variety of contexts to enrich data. To do
    this, we will learn about inference processors and ingest pipelines and how these
    can be combined with continuous transforms, reindexing, and at ingest time when
    using various beats or otherwise ingesting data into Elasticsearch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将深入探讨如何在各种环境中使用训练好的监督模型进行推理，以丰富数据。为此，我们将学习推理处理器和摄取管道，以及这些如何与连续转换、重新索引以及在使用各种beats或其他将数据摄取到Elasticsearch时的时间进行组合。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Examining, importing, and exporting trained machine learning models using the
    Trained Models API and Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练模型API和Python检查、导入和导出训练好的机器学习模型
- en: Understanding inference processors and ingest pipelines and how to configure
    and use them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解推理处理器和摄取管道以及如何配置和使用它们
- en: Importing external models into Elasticsearch using eland
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用eland将外部模型导入Elasticsearch
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The material in this chapter will require an Elasticsearch cluster version
    7.10 or later, and an installation of Python 3.7 or later with the `eland`, `elasticsearch-py`,
    and `scikit-learn` libraries installed. For detailed instructions on how to configure
    your Python installation to work with this chapter, please see the README section
    in the `Chapter 13 - Inference and Advanced Transforms` folder in the book''s
    GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的材料需要Elasticsearch集群版本7.10或更高版本，以及安装Python 3.7或更高版本，并已安装`eland`、`elasticsearch-py`和`scikit-learn`库。有关如何配置Python安装以与本章一起使用的详细说明，请参阅书中GitHub仓库中的`Chapter
    13 - Inference and Advanced Transforms`文件夹下的README部分：[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference)。
- en: Examining, exporting, and importing your trained models with the Trained Models
    API
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练模型API检查、导出和导入您的训练模型
- en: You have prepared your dataset, trained your classification or regression model,
    looked at its performance, and determined that you would like to use it to enrich
    your production datasets. Before you can dive into ingest pipelines, inference
    processors, and the multitude of other components that you can configure to use
    your trained models, it is good to become familiar with the **Trained Models API**
    ([https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html)),
    a set of REST API endpoints that you can use to find out information about your
    models and even export them to other clusters. Let's take a tour of this API to
    see what it can tell us about our models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经准备好了您的数据集，训练了您的分类或回归模型，检查了其性能，并确定您希望使用它来丰富您的生产数据集。在您深入到摄取管道、推理处理器以及您可以为使用训练模型配置的其他众多组件之前，熟悉**训练模型API**（[https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html](https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html)），一组REST
    API端点，您可以使用它来获取有关模型的信息，甚至将它们导出到其他集群。让我们来浏览一下这个API，看看它能告诉我们关于我们模型的信息。
- en: A tour of the Trained Models API
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型API的概览
- en: 'In this section, we will take a practical look at using the Kibana Dev Console
    to examine things about our trained supervised models:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过 Kibana Dev Console 实际查看关于我们的训练有监督模型的一些信息：
- en: Let's start in the Kibana Dev Console. Briefly, for those not yet familiar with
    this tool, the Kibana Dev Console, which you can access by clicking through the
    left-hand side slide-out menu and scrolling down to the `model_id`. This field
    contains the unique identifier that is assigned to each model stored in the cluster
    and that is used to reference the model when it is later used in inference processors
    and ingest pipelines.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从 Kibana Dev Console 开始。简而言之，对于那些还不熟悉这个工具的人来说，Kibana Dev Console 可以通过点击左侧滑动菜单并向下滚动到
    `model_id` 来访问。这个字段包含分配给集群中每个模型的唯一标识符，并在稍后用于在推理处理器和摄取管道中使用模型时引用该模型：
- en: 'Another piece of information provided by the Trained Models API is `analyzed_fields`,
    a dictionary that includes a list of included or excluded fields. It is good to
    double-check this as a sanity check to make sure that only the fields you intended
    to use for training were included in the training:'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Trained Models API 还提供了一项信息，即 `analyzed_fields`，这是一个包含包含或排除字段列表的字典。最好进行双重检查，以确保只包含用于训练的预期字段：
- en: '![Figure 13.1 – A snippet of the Inference API response illustrating the number
    of trained models in the cluster as well as information about one of the models'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 13.1 – Inference API 响应片段，展示了集群中训练模型的数量以及其中一个模型的信息'
- en: '](img/B17040_13_1.jpg)'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_13_1.jpg)'
- en: Figure 13.1 – A snippet of the Inference API response illustrating the number
    of trained models in the cluster as well as information about one of the models
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.1 – Inference API 响应片段，展示了集群中训练模型的数量以及其中一个模型的信息
- en: 'Our demo cluster contains just three trained models, so the amount of information
    returned from the API is not overwhelming, but in case you are working on a cluster
    with tens or hundreds of models, it can be helpful to view the details of a single
    model at a time using the API call with the full name of the model. Note, if you
    are following along and want to run the subsequent API call in your specific instance
    of Kibana, you will have to look up and use the `model_id` of the model located
    in your cluster:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的演示集群仅包含三个训练好的模型，因此 API 返回的信息量并不大，但如果你正在处理包含数十个或数百个模型的集群，使用 API 调用查看单个模型的详细信息可能会有所帮助。注意，如果你正在跟随教程并想在你的
    Kibana 实例中运行后续的 API 调用，你必须查找并使用位于你集群中的模型的 `model_id`：
- en: '[PRE0]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, use an API call with a wildcard:'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，使用带有通配符的 API 调用：
- en: '[PRE1]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A less verbose summary is available through the `_cat` API. We can use the
    following API call to see a brief summary of the models available:'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以通过 `_cat` API 获取一个更简洁的摘要。我们可以使用以下 API 调用来查看可用的模型简要信息：
- en: '[PRE2]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The response we receive from our cluster is displayed in *Figure 13.2*. You
    will notice that there are two models trained on the breast cancer dataset as
    well as a third model whose identifier is `lang_ident_model_1`. This is a language
    identification model that ships by default with Elasticsearch and can be used
    to identify which language a given string is likely to be in. We will look at
    how this language identification model works and how to use it later in this chapter:'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从集群收到的响应显示在 *图 13.2* 中。你会注意到，在乳腺癌数据集上训练了两个模型，还有一个标识为 `lang_ident_model_1`
    的第三个模型。这是一个默认随 Elasticsearch 一起提供的语言识别模型，可以用来识别给定字符串可能属于哪种语言。我们将在本章后面部分探讨这个语言识别模型的工作原理以及如何使用它：
- en: '![Figure 13.2 – The response from the _cat API'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.2 – _cat API 的响应'
- en: '](img/B17040_13_2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17040_13_2.jpg)'
- en: Figure 13.2 – The response from the _cat API
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – _cat API 的响应
- en: Now that we have taken a brief look at how we can examine and extract information
    about the trained models available in our cluster, let's take a closer look at
    one last powerful function of the Trained Models API – that of exporting the model
    from an Elasticsearch cluster. Since this procedure involves a few more moving
    parts than the last one, we have dedicated the next section to it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经简要地了解了如何检查和提取集群中可用的训练模型的信息，让我们更深入地研究 Trained Models API 的最后一个强大功能——从
    Elasticsearch 集群中导出模型。由于这个程序比上一个程序涉及更多的部分，我们专门用下一节来介绍它。
- en: Exporting and importing trained models with the Trained Models API and Python
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Trained Models API 和 Python 导出和导入训练好的模型
- en: Why might you want to export a model trained in Elasticsearch? You may want
    to export your model so that you can either store it externally, share it with
    colleagues, or import it later into another Elasticsearch cluster. Since training
    machine learning models can be resource-intensive, you might want to provision
    one transient Elasticsearch cluster for training, train and evaluate the model
    on this cluster, and then export and re-import the model into another, smaller
    cluster so that you can perform inference – a less resource-intensive procedure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么可能想要导出在 Elasticsearch 中训练好的模型呢？你可能想要导出你的模型以便将其存储在外部、与同事分享，或者稍后导入到另一个 Elasticsearch
    集群中。由于训练机器学习模型可能需要大量资源，你可能想要为训练配置一个临时的 Elasticsearch 集群，在这个集群上训练和评估模型，然后将模型导出并重新导入到另一个较小的集群中，以便执行推理——这是一个资源消耗较少的过程。
- en: 'To export a model using Python and to follow the steps, you will need a Python
    installation using 3.7 or later and the `elasticsearch-py` library version 7.10.1\.
    For detailed instructions and further resources on how to configure a Python environment
    and install the required dependencies, please follow the instructions in this
    README file ([https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference))
    in the book''s GitHub repository. All the steps and logic required to export a
    model from an Elasticsearch cluster will be available in the `export_model.py`
    Python script at, [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export),
    in the book''s GitHub repository. In this section, we will look at each of the
    steps in the script to understand the components necessary to export a model.
    Hopefully, this treatment will give you the building blocks to use the `elasticsearch-py`
    client to construct your own machine learning workflows in Python:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Python 导出模型并遵循步骤，你需要安装 Python 3.7 或更高版本，以及 `elasticsearch-py` 库版本 7.10.1。有关如何配置
    Python 环境和安装所需依赖项的详细说明和进一步资源，请参阅本书 GitHub 仓库中的此 README 文件（[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference)）。从
    Elasticsearch 集群中导出模型所需的所有步骤和逻辑都将在本书 GitHub 仓库中的 `export_model.py` Python 脚本中提供，[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export)。在本节中，我们将查看脚本中的每个步骤，以了解导出模型所需的组件。希望这种处理能为你提供使用
    `elasticsearch-py` 客户端在 Python 中构建自己的机器学习工作流的构建块：
- en: 'The building block of nearly all Python scripts that interact with an Elasticsearch
    cluster is the construction of the Elasticsearch client object. The client object''s
    class must first be imported from the library like this:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几乎所有与 Elasticsearch 集群交互的 Python 脚本的构建块是构建 Elasticsearch 客户端对象。首先必须从库中导入客户端对象的类，如下所示：
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the class has been imported, we can create an instance of the object and
    assign it to the `es_client` variable:'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦类被导入，我们就可以创建该对象的实例并将其分配给 `es_client` 变量：
- en: '[PRE4]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that we pass into the object's constructor the variable that holds the
    URL of the Elasticseach instance. This can either be something like `localhost:9200`
    if you are running an instance of Elasticsearch on your local machine, or a longer
    URL for cloud-based deployments. Additionally, we pass in two variables, `ES_USERNAME`
    and `ES_PASSWORD`, that hold the username and password of our Elasticsearch instance.
    This is not required if you are running an unprotected Elasticsearch cluster locally
    for development purposes, but please note that running an unsecured Elasticsearch
    cluster in production is extremely dangerous.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们将包含 Elasticsearch 实例 URL 的变量传递给对象的构造函数。这可以是类似于 `localhost:9200` 的内容，如果你在你的本地机器上运行
    Elasticsearch 实例，或者对于基于云的部署，是一个更长的 URL。此外，我们传递了两个变量，`ES_USERNAME` 和 `ES_PASSWORD`，它们包含我们
    Elasticsearch 实例的用户名和密码。如果你在本地运行一个未受保护的 Elasticsearch 集群以进行开发目的，则这不是必需的，但请注意，在生产环境中运行未加密的
    Elasticsearch 集群是极其危险的。
- en: 'To interact with the Machine Learning APIs, which we will need to export our
    model from the cluster, we need to import the `MlClient` class:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要与机器学习 API 交互，这些 API 我们需要从集群中导出模型，我们需要导入 `MlClient` 类：
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, create an instance of it:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，创建它的一个实例：
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we have these two clients created, we can proceed to export our model.
    To do this, we will use the `get_trained_models` method. The documentation for
    the method is at here, [https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack](https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack),
    and it is good to keep this reference handy when working with this library in
    case you need to double-check the meaning of parameters and configuration options.
    There are three important parameters to pass into the method: `model_id`, which
    specifies the name of the model you wish to export, the `decompress_definition`
    flag, which should be set to `False`, and the `include_model_definition` flag,
    which should be set to `True`:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦创建了这两个客户端，我们就可以继续导出我们的模型。为此，我们将使用 `get_trained_models` 方法。该方法的文档在此处，[https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack](https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack)，在处理此库时，最好将此参考信息随时备好，以防需要双重检查参数和配置选项的含义。需要传递给该方法的三个重要参数是：`model_id`，它指定了你希望导出的模型的名称，`decompress_definition`
    标志，应设置为 `False`，以及 `include_model_definition` 标志，应设置为 `True`：
- en: '[PRE7]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 13.3 – A snippet of the Python dictionary that captures the compressed
    definition of a model as well as metadata'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 13.3 – 捕获模型压缩定义以及元数据的 Python 字典片段'
- en: '](img/B17040_13_3.jpg)'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_13_3.jpg)'
- en: Figure 13.3 – A snippet of the Python dictionary that captures the compressed
    definition of a model as well as metadata
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.3 – 捕获模型压缩定义以及元数据的 Python 字典片段
- en: Once we have this model definition stored in a `compressed_model` variable in
    our Python script, we can convert the dictionary to a JSON-formatted string and
    write it out to a file, which can be stored in version control or imported into
    another Elasticsearch cluster.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们将此模型定义存储在我们的 Python 脚本中的 `compressed_model` 变量中，我们就可以将字典转换为 JSON 格式的字符串，并将其写入文件，该文件可以存储在版本控制中或导入到另一个
    Elasticsearch 集群中。
- en: 'To convert the dictionary to JSON format, we must import the built-in Python
    `json` library:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要将字典转换为 JSON 格式，我们必须导入内置的 Python `json` 库：
- en: '[PRE8]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After this, we can write the exported model to a file whose path we have stored
    in the `filename` variable:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们可以将导出的模型写入存储在 `filename` 变量中的文件路径：
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'All of the preceding steps are summarized in the `export_model.py` script,
    which is available in the book''s GitHub repository here: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面的步骤都总结在 `export_model.py` 脚本中，该脚本可在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export)。
- en: 'Now that we have seen how to export a trained model from an Elasticsearch cluster,
    let''s look at how to import a model from a file. As previously, we will break
    down the logic into steps, but the full working script will be stored in the book''s
    GitHub repository here: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何从 Elasticsearch 集群中导出训练好的模型，让我们看看如何从文件中导入模型。与之前一样，我们将逻辑分解成步骤，但完整的有效脚本将存储在本书的
    GitHub 仓库中：[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference)：
- en: 'Many of the steps in this procedure are like the export script we stepped through
    in detail previously. In particular, the creation of the `Elasticsearch` and `MlClient`
    objects, as well as the parsing of the command-line arguments, follow similar
    steps as the preceding script, so we will not be explaining them in detail. Thus,
    the first step is to read the model file and convert the string contents into
    a Python dictionary using the `loads` file from the built-in `json` library:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个流程中的许多步骤与我们在之前详细介绍的导出脚本类似。特别是，创建`Elasticsearch`和`MlClient`对象，以及解析命令行参数，都遵循与前面脚本相似的步骤，因此我们不会详细解释它们。因此，第一步是读取模型文件，并使用内置`json`库的`loads`函数将字符串内容转换为Python字典：
- en: '[PRE10]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once we have the compressed `model_definition` and the required metadata loaded
    into our Python dictionary, we can use the `put_trained_model` method to upload
    it into our cluster.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们将压缩的`model_definition`和所需的元数据加载到我们的Python字典中，我们就可以使用`put_trained_model`方法将其上传到我们的集群。
- en: Finally, navigate to the Kibana instance of your cluster and use the Trained
    Models API to double-check that the model has indeed been imported into your cluster.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，导航到你的集群的Kibana实例，并使用训练模型API来双重检查模型是否确实已导入到你的集群中。
- en: Now that we have learned how to view details about a model as well as how to
    export and import models, we are ready to move on to building more complicated
    machine learning infrastructures without models. Once you have a trained model,
    the possibilities for the model are nearly endless – you can combine the model
    with transforms, use it to enrich your data at ingest time, and much more. The
    building blocks of this infrastructure are inference processors and ingest pipelines.
    We will take a detailed look at each of these two in the next chapter to get you
    ready for building your own machine learning infrastructures.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何查看模型的详细信息，以及如何导出和导入模型，我们可以继续学习构建没有模型的更复杂的机器学习基础设施。一旦你有一个训练好的模型，模型的可能性几乎是无限的——你可以将模型与转换结合使用，在摄取时使用它来丰富你的数据，等等。这个基础设施的构建块是推理处理器和摄取管道。我们将在下一章详细探讨这两个构建块，以便你为构建自己的机器学习基础设施做好准备。
- en: Understanding inference processors and ingest pipelines
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解推理处理器和摄取管道
- en: You have a trained machine learning model, so now what? Remember from [*Chapter
    11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209), *Classification Analysis*, and
    [*Chapter 12*](B17040_12_Epub_AM.xhtml#_idTextAnchor230), *Regression*, that one
    of the exciting things about machine learning models is that they learn from a
    labeled training dataset and then, in a way, encode the knowledge so that they
    can be used to make predictions on previously unseen data points. This process
    of labeling or making predictions for previously unseen data points is what we
    call **inference**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经有一个训练好的机器学习模型，接下来该做什么呢？记得从[*第11章*](B17040_11_Epub_AM.xhtml#_idTextAnchor209)，*分类分析*，和[*第12章*](B17040_12_Epub_AM.xhtml#_idTextAnchor230)，*回归*，机器学习模型令人兴奋的一点是，它们可以从标记的训练数据集中学习，然后以某种方式编码知识，以便它们可以用于对先前未见过的数据点进行预测。对先前未见过的数据点进行标记或预测的过程就是我们所说的**推理**。
- en: How does this happen in practice in the Elastic Stack?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这在Elastic Stack中是如何实际发生的？
- en: There are a multitude of different architectures that you might build to make
    use of inference in the Elastic Stack, but the basic building blocks of all of
    them are inference processors and ingest pipelines. These are the main subjects
    of our exploration in this chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能构建出许多不同的架构来利用Elastic Stack中的推理功能，但所有这些架构的基本构建块都是推理处理器和摄取管道。这是我们本章探索的主要主题。
- en: An **ingest pipeline** is a special component that lets you manipulate and transform
    your data in various ways before it is written to an Elasticsearch index. Ingest
    pipelines are normally composed of various processors, which are sub-units or
    configurable tasks that each perform a single type of manipulation or transformation
    on the data being ingested. Ingest pipelines can consist of multiple processors
    that are performed sequentially on each incoming data document as it is being
    ingested.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**摄取管道**是一个特殊组件，它允许你在数据写入Elasticsearch索引之前以各种方式操纵和转换你的数据。摄取管道通常由各种处理器组成，这些处理器是子单元或可配置的任务，每个处理器对正在摄取的数据执行单一类型的操纵或转换。摄取管道可以由多个处理器组成，这些处理器按顺序对每个正在摄取的数据文档执行。'
- en: For example, a typical ingest pipeline architecture might involve a script processor,
    which is able to execute a Painelss script on each document ingested through the
    pipeline, followed by an inference processor followed by another script processor.
    For many machine learning applications, such as the ones we will look at a little
    bit later in this chapter, this is the perfect place to perform feature engineering
    or transform features into a suitable format for consumption by the machine learning
    model or to remove unnecessary fields before a document is ingested into Elasticsearch.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，典型的摄取管道架构可能涉及一个脚本处理器，它能够对通过管道摄取的每个文档执行 Painelss 脚本，然后是推理处理器，再然后是另一个脚本处理器。对于许多机器学习应用，例如我们将在本章稍后探讨的应用，这是执行特征工程或将特征转换为机器学习模型可消费的适当格式的完美位置，或者在文档被摄取到
    Elasticsearch 之前删除不必要的字段。
- en: 'There are a variety of built-in processors that can be combined and customized
    to create complex data transformation pipelines. For example, the **GeoIP** processor
    adds geographical information about IP addresses, the script processor allows
    users to write custom painless code to perform calculations and manipulations
    on existing document fields, and the **CSV** processor enables parsing and the
    extraction of data from CSV values to create fields. The full list of processors
    is available in the Elasticsearch documentation here: [https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html).
    We encourage you to take a look to see the kinds of possible data architecture
    you can build with them.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种内置处理器可以组合和定制，以创建复杂的数据转换管道。例如，**GeoIP** 处理器可以添加关于 IP 地址的地理位置信息，脚本处理器允许用户编写自定义的简便代码，以对现有文档字段进行计算和操作，而
    **CSV** 处理器可以解析并从 CSV 值中提取数据以创建字段。完整的处理器列表可在 Elasticsearch 文档中找到：[https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html)。我们鼓励您查看一下，看看您可以使用它们构建哪些可能的数据架构。
- en: For our purposes, in terms of exploring machine learning, the most important
    processor to study is the inference processor. When documents pass through this
    processor, they are annotated with predictions by the machine learning model referenced
    in the processor's configuration. Let's take a look at how to configure our own
    inference processor and use it in an ingest pipeline with the help of a practical
    example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，在探索机器学习方面，最重要的处理器是推理处理器。当文档通过此处理器时，它们会根据处理器配置中引用的机器学习模型的预测进行标注。让我们通过一个实际示例来看看如何配置我们自己的推理处理器，并在摄取管道中使用它。
- en: In this example, we will be using the fictitious social media dataset that we
    first examined in [*Chapter 9*](B17040_09_Epub_AM.xhtml#_idTextAnchor162), *Introducing
    Data Frame Analytics*. This time, we will be using a language identification model
    to identify which language the text in these fictional microblogging site posts
    is written in. Let's get started!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用我们在 [*第 9 章*](B17040_09_Epub_AM.xhtml#_idTextAnchor162)，“介绍数据帧分析”中首次考察的虚构社交媒体数据集。这次，我们将使用语言识别模型来识别这些虚构微博网站帖子中的文本是用哪种语言编写的。让我们开始吧！
- en: If you have been playing around with the Trained Models API we discussed at
    the beginning of this chapter, you may have noticed that even if you have not
    trained any models in a particular Elasticsearch cluster, you will still see a
    single model, `lang_ident_model_1`, available in the cluster. The metadata associated
    with this model returned by the Trained Models API is shown in *Figure 13.4*:![Figure
    13.4 – The language identification model lang_ident_model_1
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您在本章开头讨论的 Trained Models API 上进行过尝试，您可能会注意到，即使您在特定的 Elasticsearch 集群中没有训练任何模型，您仍然会在集群中看到一个模型，`lang_ident_model_1`。由
    Trained Models API 返回的与此模型相关的元数据在 *图 13.4* 中显示：![图 13.4 – 语言识别模型 lang_ident_model_1
- en: '](img/B17040_13_4.jpg)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_13_4.jpg]'
- en: Figure 13.4 – The language identification model lang_ident_model_1
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.4 – 语言识别模型 lang_ident_model_1
- en: The model ships by default with an Elasticsearch cluster and can be used in
    inference processors and ingest pipelines just like any other model you might
    train yourself on an Elasticsearch cluster!
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型默认随 Elasticsearch 集群一起提供，可以在推理处理器和摄取管道中使用，就像您在 Elasticsearch 集群上训练的任何其他模型一样！
- en: Next, let's see how we can create an ingest pipeline configuration with an inference
    processor that references this language identification model. Recall that processors
    are the sub-units within an ingest pipeline that process each document as it enters
    the pipeline and before it is written to an Elasticsearch index. Even though you
    can never use an inference processor as a standalone functional unit in Elasticsearch
    – it must always be a part of a pipeline – let's first examine the configuration
    of the processor in isolation and then see how it fits into an Ingest pipeline.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何创建一个包含引用此语言识别模型的推理处理器的摄取管道配置。回想一下，处理器是摄取管道内的子单元，它处理每个进入管道的文档，并在将其写入Elasticsearch索引之前。尽管你永远不能将推理处理器作为一个独立的单元在Elasticsearch中使用——它必须始终是管道的一部分——让我们首先单独检查处理器的配置，然后看看它是如何适应摄取管道的。
- en: 'The following code snippet shows the configuration of an inference processor
    for our planned language identification text pipeline:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码片段显示了为我们的计划语言识别文本管道配置的推理处理器：
- en: '[PRE11]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s take a moment to go over the most important configuration parameters
    and what they mean. For a full API reference of all the available configuration
    options for inference processors, please look at the Elasticsearch documentation
    here: [https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html).'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们花点时间回顾一下最重要的配置参数及其含义。有关推理处理器的所有可用配置选项的完整API参考，请参阅以下Elasticsearch文档：[https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html](https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html)。
- en: The key part of any inference processor is the trained machine learning model
    that will be used to make predictions on our text documents. The inference processor
    becomes aware of which model it should use to classify incoming documents through
    the `model_id` configuration field. In our case, the `model_id` (which is available
    in the metadata of the model that we can view by using the Trained Models API)
    is `lang_ident_model_1`.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任何推理处理器的关键部分是用于在我们文本文档上做出预测的训练机器学习模型。推理处理器通过`model_id`配置字段来了解它应该使用哪个模型来分类传入的文档。在我们的案例中，`model_id`（可以在通过使用训练模型API查看的模型元数据中找到）是`lang_ident_model_1`。
- en: '[PRE12]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The field name listed under `field_names` in the model's metadata specifies
    that the name of the feature field used in the model is text, which means that
    the features we want to use in our inference documents need to have a field of
    this name that contains the text to which we wish to apply language identification.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在模型元数据下列出的`field_names`字段名指定了在模型中使用的特征字段名为文本，这意味着我们希望在推理文档中使用的特征需要有一个包含我们希望应用语言识别的文本的字段。
- en: 'Since the training of the model is decoupled from the inference process, it
    is entirely possible that the field names chosen for the features during the training
    process are not available in the data that we wish to pass through our inference
    processor. In this case, if it is not possible or desirable to alter the field
    names of our data, we can use the `field_map` configuration block in our inference
    processor to map from the field name in the data we will be using for inference
    to the field name that our supervised model expects. Below, we have configured
    a mapping between the `post` field name that contains the text in our fictional
    microblogging social media dataset and the `text` field name that the model expects:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于模型的训练与推理过程解耦，完全有可能在训练过程中为特征选择的字段名在我们希望通过推理处理器传递的数据中不可用。在这种情况下，如果无法或不想更改我们数据的字段名，我们可以在推理处理器的`field_map`配置块中使用它来将数据中我们将用于推理的字段名映射到我们的监督模型期望的字段名。以下是我们配置的映射，将包含我们虚构的微博社交媒体数据集中的文本的`post`字段名与模型期望的`text`字段名之间进行映射：
- en: '[PRE13]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, we've come to the last part of the configuration – the `inference_config`
    block. The configuration options for this block determine whether we are using
    classification or regression. Since, in the case of language identification, we
    are working with multiclass classification, we will select classification and
    leave all the other configuration options as their default settings. Slightly
    later in this section, we will take a closer look at the available fields in `inference_config`
    and how adjusting them determines the final format of the results.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们来到了配置的最后部分——`inference_config`块。这个块的配置选项决定了我们是使用分类还是回归。由于在语言识别的情况下，我们进行的是多类分类，因此我们将选择分类，并将所有其他配置选项保留为它们的默认设置。在本节稍后，我们将更详细地查看`inference_config`中可用的字段以及调整它们如何确定结果的最终格式。
- en: Now that we have examined the pieces that are part of configuring the inference
    processor, let's move on to configuring the ingest pipeline. This is the top-level
    container, if you will, or component that will house our inference processor and
    potentially others as well.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经检查了配置推理处理器所涉及的各个部分，让我们继续配置摄取管道。这将是顶级容器，或者说组件，它将容纳我们的推理处理器，以及可能的其他处理器。
- en: 'The configuration for an ingest pipeline that contains the inference processor
    we configured looks like the following:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含我们配置的推理处理器的摄取管道的配置如下：
- en: '[PRE14]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The bulk of the configuration is taken up by the specifications for the inference
    processor that we studied in detail previously. The only additional noteworthy
    features in the configuration of the ingest pipeline are the name of the pipeline,
    which is a part of the REST API endpoint, and the `processors` array in the body
    of the configuration. In this case, we have chosen to call the pipeline `language-identification-pipeline`.
    The `processors` array contains the configuration specifications for our processors.
    In this case, we only have one processor.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置的大部分内容是由我们之前详细研究的推理处理器的规格组成。在摄取管道的配置中，唯一值得注意的附加功能是管道的名称，它是REST API端点的一部分，以及配置体中的`processors`数组。在这种情况下，我们选择将管道命名为`language-identification-pipeline`。`processors`数组包含我们处理器的配置规格。在这种情况下，我们只有一个处理器。
- en: 'You can either copy and paste the following configuration into the Kibana Dev
    Console or, alternatively, use the Ingest Pipelines wizard, which is available
    in the **Stack** **Management** panel in Kibana, as shown in *Figure 13.5*:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以将以下配置复制并粘贴到Kibana Dev Console中，或者，作为替代，使用Kibana中的**堆栈管理**面板中可用的摄取管道向导，如图*13.5*所示：
- en: '![Figure 13.5 – The Create pipeline wizard'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图13.5 – 创建管道向导'
- en: '](img/B17040_13_5.jpg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17040_13_5.jpg)'
- en: Figure 13.5 – The Create pipeline wizard
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.5 – 创建管道向导
- en: Once we have our pipeline configured – either through the Dev Console or the
    wizard, we are ready to start using it to identify the language in our fictional
    social media microblogging platform posts.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们配置了我们的管道——无论是通过Dev Console还是向导，我们就准备好开始使用它来识别我们虚构的社会媒体微博平台帖子中的语言。
- en: Although normally, we would use an ingest pipeline together with a transform
    or a beat such as `packetbeat`, in this case, we are going to ingest documents
    into an index using the Kibana Dev Console since it is easier to illustrate the
    concepts we want to teach there. Slightly later in the chapter, we will look at
    more advanced and, hence, more realistic examples.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然通常我们会使用摄取管道与转换或`packetbeat`之类的beat一起使用，但在这个案例中，我们将使用Kibana Dev Console将文档摄取到索引中，因为它更容易在那里说明我们想要教授的概念。在本章稍后，我们将查看更高级和更现实的示例。
- en: 'Let''s index our first document through the pipeline. The REST API command
    that achieved this looks like the following:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们通过管道索引我们的第一个文档。实现这一点的REST API命令如下：
- en: '[PRE15]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We are sending a `POST` request to index the document in the body of the request
    and are passing the name of the pipeline we created in the preceding steps as
    the argument for the optional `pipeline` parameter. In this case, a user, "Sanna,"
    has written an update (as seen in the post field in Finnish). Let's examine the
    `social-media-feed-inference` index to see what the ingested document looks like.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们正在向索引文档发送`POST`请求，并将我们在前面的步骤中创建的管道名称作为可选的`pipeline`参数的参数传递。在这种情况下，用户"Sanna"已经写了一条更新（如芬兰语帖子字段所示）。让我们检查`social-media-feed-inference`索引，看看摄取的文档看起来像什么。
- en: 'If you have not already done so, create an index pattern for the `social-media-feed-inference`
    index and navigate to the Discover app in Kibana. Now, the `social-media-feed-inference`
    index contains only the one document we indexed using the REST API call shown
    previously. The document is shown in *Figure 13.6*:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请为 `social-media-feed-inference` 索引创建一个索引模式，并导航到 Kibana 中的发现应用。现在，`social-media-feed-inference`
    索引中只包含我们之前使用 REST API 调用索引的一个文档。文档如图 13.6 所示：
- en: '![Figure 13.6 – An ingested document in the social-media-feed-inference index'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 13.6 – 在 social-media-feed-inference 索引中的摄取文档'
- en: '](img/B17040_13_6.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B17040_13_6.jpg]'
- en: Figure 13.6 – An ingested document in the social-media-feed-inference index
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.6 – 在 social-media-feed-inference 索引中的摄取文档
- en: As we can see from the document in *Figure 13.6*, the inference processor has
    added four new fields alongside the original fields in the document. All these
    fields are prefixed with the name `text_language_prediction_model`, which is what
    we configured in our inference processor configuration. As we can see, the fields
    record the `model_id` of the model used to make the prediction, `predicted_value`,
    which in this case will contain an identifier for the language the model predicts
    the post to be in, `prediction_probability` as well as `prediction_score`. These
    were previously covered in [*Chapter 11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209),
    *Classification Analysis*.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们从图 13.6 中的文档中可以看到，推理处理器在文档的原始字段旁边添加了四个新字段。所有这些字段都以 `text_language_prediction_model`
    为前缀，这是我们推理处理器配置中设置的。如我们所见，这些字段记录了用于预测的模型的 `model_id`、`predicted_value`（在这种情况下将包含模型预测帖子所在语言的标识符）、`prediction_probability`
    以及 `prediction_score`。这些在 [*第 11 章*](B17040_11_Epub_AM.xhtml#_idTextAnchor209)
    *分类分析* 中已有介绍。
- en: As we can see, in this case, the model has determined correctly that the original
    post was written in the Finnish language.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，在这种情况下，模型已经正确地确定原始帖子是用芬兰语编写的。
- en: 'In the previous example, we created the inference processor and ingest pipeline
    configurations, and proceeded directly to index documents into our index through
    the pipeline. If we wish to first see a few dry runs of our pipeline before indexing,
    we can use the `_simulate` endpoint:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们创建了推理处理器和摄取管道配置，然后直接通过管道将文档索引到我们的索引中。如果我们希望在索引之前先查看我们管道的一些干运行，我们可以使用
    `_simulate` 端点：
- en: '[PRE16]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The response returned by the API for this call contains the results of the
    model''s predictions, as you can see in the following code snippet:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: API 对此调用的响应包含模型预测的结果，如下面的代码片段所示：
- en: '[PRE17]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Finally, it is also possible to use the Ingest Pipeline UI to test out the documents
    before beginning to ingest to make sure that everything is working as intended.
    Unfortunately, in the UI, this can only be performed during the creation of a
    new ingest pipeline and not with an existing one, so, for the purposes of this
    demonstration, you can use the wizard to start creating a clone of the `language-identification-pipeline`
    we created previously.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在开始摄取之前，也可以使用摄取管道 UI 测试文档，以确保一切按预期工作。不幸的是，在 UI 中，这只能在创建新的摄取管道时进行，而不能在现有的管道中进行，因此，为了演示的目的，您可以使用向导开始创建我们之前创建的
    `language-identification-pipeline` 的克隆。
- en: 'Then, to the right of the **Processors** selector in the wizard, as shown in
    *Figure 13.7*, locate the **Test pipeline** text and click on the **Add documents**
    link:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，在向导中 **处理器** 选择器的右侧，如图 13.7 所示，找到 **测试管道** 文本，并点击 **添加文档** 链接：
- en: '![Figure 13.7 – The Create Ingest pipeline wizard'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 13.7 – 创建摄取管道向导'
- en: '](img/B17040_13_7.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_13_7.jpg]'
- en: Figure 13.7 – The Create Ingest pipeline wizard
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 – 创建摄取管道向导
- en: 'This will trigger a menu on the right-hand side of the wizard that allows you
    to either add a document from an index or manually specify it in the textbox provided.
    In this case, we are going to manually add our Finnish language test document
    to the textbox, as shown in *Figure 13.8*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在向导的右侧触发一个菜单，允许您从索引中添加文档或手动在提供的文本框中指定它。在这种情况下，我们将手动将我们的芬兰语测试文档添加到文本框中，如图 13.8
    所示：
- en: '![Figure 13.8 – A sample document in the ingest pipeline'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 13.8 – 摄取管道中的样本文档'
- en: '](img/B17040_13_8.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_13_8.jpg]'
- en: Figure 13.8 – A sample document in the ingest pipeline
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – 摄取管道中的样本文档
- en: Once you have configured your documents, click the **Run the pipeline** button
    and you should see a preview of your documents after they have passed through
    the inference pipeline.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您配置了您的文档，点击**运行管道**按钮，您应该会看到文档通过推理管道后的预览。
- en: Handling missing or corrupted data in ingest pipelines
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理导入管道中的缺失或损坏的数据
- en: 'Many, or maybe even most, real-world applications will not have neat datasets.
    Instead, data will be missing, mislabeled, and potentially even corrupted. It''s
    important to take a moment to look at what happens in such cases with inference
    processors so that you can recognize and mitigate these issues in your own pipelines:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 许多，或许甚至大多数，现实世界的应用将不会有整洁的数据集。相反，数据可能会缺失、标签错误，甚至可能被损坏。在这种情况下，花点时间看看推理处理器会发生什么，这样你就可以在自己的管道中识别并减轻这些问题：
- en: 'Let''s continue with the fictitious microblogging platform we used as our example
    previously and suppose that due to a misconfiguration error, we rename the `post`
    field, which contains the text string whose language we wish to detect, to `post_text`,
    as shown:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续使用我们之前作为示例使用的虚构微博平台，并假设由于配置错误，我们将包含我们希望检测其语言的文本字符串的`post`字段重命名为`post_text`，如下所示：
- en: '[PRE18]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: What happens once we send this text through the `language-identification-pipeline`?
    Let's perform the REST API call shown previously and then look at the ingested
    document in the **Discover** tab, as we did in the previous section.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们将这段文本通过`language-identification-pipeline`发送出去，会发生什么？让我们执行之前显示的REST API调用，然后查看在**发现**标签页中导入的文档，就像我们在上一节中所做的那样。
- en: 'As we can see from the document as shown in *Figure 13.9*, the model was not
    able to make a correct prediction about the language in which the text in the
    `post_text` field was written:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们从*图 13.9*中看到的文档所示，模型无法正确预测`post_text`字段中文本的语言：
- en: '![Figure 13.9 – A warning message in an ingested document'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.9 – 导入文档中的警告消息'
- en: '](img/B17040_13_9.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B17040_13_9.jpg](img/B17040_13_9.jpg)'
- en: Figure 13.9 – A warning message in an ingested document
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – 导入文档中的警告消息
- en: Real-world use cases and datasets are often messy and contain missing and corrupt
    data, so be on the lookout for this message to catch and rectify potential errors
    in your inference setup!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界的用例和数据集通常很混乱，包含缺失和损坏的数据，所以请注意这个消息，以便捕捉并纠正推理设置中的潜在错误！
- en: The first step to troubleshoot why this message appears is to compare the fields
    in the documents you are trying to ingest through the ingest pipeline with the
    analyzed_fields stored in the model’s metadata. Refer back to the section *Examining,
    Exporting and Importing your Trained Models with the Trained Models API* in this
    chapter for tips on how to view a model’s metadata.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此消息出现的原因的第一步是将您通过导入管道尝试导入的文档中的字段与模型元数据中存储的`analyzed_fields`进行比较。请参考本章中关于如何查看模型元数据的提示，即*使用Trained
    Models API检查、导出和导入您的训练模型*部分。
- en: Using inference processor configuration options to gain more insight into your
    predictions
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用推理处理器配置选项来深入了解您的预测
- en: 'In the previous sections, we configured our inference processors in such a
    way that the documents processed by this processor only contained four fields:
    the predicted class label of the document, the probability of the prediction,
    the score of the prediction, and the model ID. However, what happens in cases
    where you can see that the model has made an incorrect prediction, or you would
    like to have more information about the probabilities that the model was assigned
    to other potential classes? This can be useful for debugging.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们以这种方式配置了我们的推理处理器，使得该处理器处理的文档只包含四个字段：文档的预测类别标签、预测的概率、预测的分数和模型ID。然而，当您可以看到模型做出了错误的预测，或者您希望了解模型分配给其他潜在类别的概率时，会发生什么？这有助于调试。
- en: 'How do we configure the inference processor to provide us with more information?
    Let''s take a look:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何配置推理处理器以提供更多信息？让我们看看：
- en: 'Let''s begin by going back to the inference processor configuration that we
    saw in the previous section and taking a closer look at the `inference_config`
    configuration block, which we left empty. In this case, since we want to see a
    more detailed breakdown of different probabilities, we want to add a `num_top_classes`
    field to the configuration block. This configuration parameter controls the number
    of classes for which the probabilities are written out. For example, if we set
    it to 3, each document will contain the probabilities for the top 3 classes it
    was most likely to belong to:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从上一节中看到的推理处理器配置开始，并仔细看看我们留下的空的`inference_config`配置块。在这种情况下，因为我们想看到不同概率的更详细分解，我们想在配置块中添加一个`num_top_classes`字段。此配置参数控制要写入概率的类数量。例如，如果我们将其设置为3，每个文档将包含它最有可能属于的前三个类的概率：
- en: '[PRE19]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, let''s ingest a document through this new pipeline, `language-identification-pipeline-v2`,
    using the following REST API call:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用以下REST API调用通过这个新管道`language-identification-pipeline-v2`摄取一个文档：
- en: '[PRE20]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will see that as a result, the inference processor writes out a detailed
    breakdown of the possible languages (or classes if we are using classification
    terminology) to which the post belongs, as shown in *Figure 13.10*. Possible candidates
    are Finnish, denoted by the keyword **fi**, Swedish, denoted by the keyword **sv**,
    and Estonian, denoted by the keyword **eo**:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将看到，作为结果，推理处理器将详细列出可能的语言（或如果我们使用分类术语，则为类别）到哪个后缀属于，如图*图13.10*所示。可能的候选包括芬兰语，由关键字**fi**表示，瑞典语，由关键字**sv**表示，以及爱沙尼亚语，由关键字**eo**表示：
- en: '![Figure 13.10 – A detailed breakdown of the potential classes a given document
    belongs to along with the associated probabilities'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.10 – 给定文档可能属于的类别的详细分解以及相关的概率]'
- en: '](img/B17040_13_10.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17040_13_10.jpg]'
- en: Figure 13.10 – A detailed breakdown of the potential classes a given document
    belongs to along with the associated probabilities
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10 – 给定文档可能属于的类别的详细分解以及相关的概率
- en: We'll now move on to importing models using eland.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将转向使用eland导入模型。
- en: Importing external models into Elasticsearch using eland
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用eland将外部模型导入Elasticsearch
- en: Suppose you already have a model trained using one of the other frameworks.
    Is it possible to re-use the building blocks we discussed in the previous section
    to deploy your own externally trained models? The answer is yes, with a few limitations.
    In this section, we will take a look at how to use the **eland** library, along
    with **scikit-learn**, another machine learning library for creating and training
    external machine learning models and importing them into Elasticsearch for inference.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经使用其他框架之一训练了一个模型。是否可以使用我们在上一节中讨论的构建块来部署自己的外部训练模型？答案是肯定的，但有一些限制。在本节中，我们将探讨如何使用**eland**库，以及另一个机器学习库**scikit-learn**，用于创建和训练外部机器学习模型并将它们导入Elasticsearch进行推理。
- en: Learning about supported external models in eland
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解eland支持的外部模型
- en: 'Unfortunately, the inference functionality in the Elastic Stack does not yet
    have support for importing an externally trained machine learning model from any
    library (though it might at some point in the future!). Instead, the eland documentation
    ([https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model](https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model))
    contains a list of third-party libraries that produce supported models. As it
    stands currently, the list of supported model types is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Elastic Stack中的推理功能目前尚不支持从任何库导入外部训练的机器学习模型（尽管将来可能会有所改变！）相反，eland文档（[https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model](https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model)）包含了一组生成支持模型的第三方库。目前，支持模型类型的列表如下：
- en: '`sklearn.tree.DecisionTreeClassifier`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.tree.DecisionTreeClassifier`'
- en: '`sklearn.tree.DecisionTreeRegressor`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.tree.DecisionTreeRegressor`'
- en: '`sklearn.ensemble.RandomForestRegressor`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.ensemble.RandomForestRegressor`'
- en: '`sklearn.ensemble.RandomForestClassifier`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn.ensemble.RandomForestClassifier`'
- en: '`lightgbm.LGBMRegressor`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lightgbm.LGBMRegressor`'
- en: '`lightgbm.LGBMClassifier`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lightgbm.LGBMClassifier`'
- en: '`xgboost.XGBClassifier`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost.XGBClassifier`'
- en: '`xgboost.XGBRegressor`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost.XGBRegressor`'
- en: Note
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that there are some additional restrictions to models generated
    with the preceding libraries that pertain to the type of objective function that
    is selected or the type of encoding that must be enforced on the features, so
    please do make sure you check the eland documentation for the most up-to-date
    information of supported third-party models.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，使用前面库生成的模型有一些额外的限制，这些限制与所选目标函数的类型或必须对特征执行的编码类型有关，因此请务必检查eland文档以获取支持的第三方模型的最最新信息。
- en: Training a scikit-learn DecisionTreeClassifier and importing it into Elasticsearch
    using eland
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用eland将scikit-learn DecisionTreeClassifier训练并导入Elasticsearch
- en: 'Now that we have learned about the important preliminaries, let''s hit the
    ground running and take a look at how we can train an external machine learning
    model using the scikit-learn library. All of the code examples used in this walk-through
    will be available in the Jupyter notebook in the book''s GitHub repository here:
    [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了重要的预备知识，让我们立即开始，看看我们如何使用scikit-learn库训练外部机器学习模型。本指南中使用的所有代码示例都将可在本书GitHub仓库中的Jupyter笔记本中找到：[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models)：
- en: The first step in our project to import an external model into Elasticsearch
    will be to retrieve some training data and use it to train a decision tree model.
    The scikit-learn library has a great collection of built-in datasets that can
    be used for learning and quick prototyping. To continue with the same data theme
    that we have been developing in [*Chapter 11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209),
    *Classification Analysis* and in this chapter, we will be using the built-in Wisconsin
    Breast Cancer dataset (which is a variation of the dataset that we have been using).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们项目将外部模型导入Elasticsearch的第一步是检索一些训练数据并使用它来训练决策树模型。scikit-learn库拥有一个用于学习和快速原型设计的内置数据集集合。为了继续我们在[*第11章*](B17040_11_Epub_AM.xhtml#_idTextAnchor209)，*分类分析*和本章中开发的数据主题，我们将使用内置的威斯康星乳腺癌数据集（这是我们一直在使用的数据集的变体）。
- en: 'Before we begin, let''s make sure that we import all of the required functions
    and libraries into our Python script (or Jupyter notebook):'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们确保将所有必需的函数和库导入到我们的Python脚本（或Jupyter笔记本）中：
- en: '[PRE21]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now that we have our imports, let's load the dataset by calling the `load_breast_cancer`
    function.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了导入，让我们通过调用`load_breast_cancer`函数来加载数据集。
- en: '[PRE22]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Notice that this function returns two values, which we store in the variables
    `X` and `y`. The way that Elasticsearch organizes its training data is different
    from the conventions in scikit-learn. In Elasticsearch, our training data was
    stored in a single Elasticsearch index. Each document in the index represents
    one data point and is a combination of fields that represent the features and
    the field that represents the dependent variable (see [*Chapter 11*](B17040_11_Epub_AM.xhtml#_idTextAnchor209),
    *Classification Analysis* and [*Chapter 12*](B17040_12_Epub_AM.xhtml#_idTextAnchor230),
    *Regression*, for further information about dependent variables and why they are
    important in supervised learning).
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，此函数返回两个值，我们将它们存储在变量`X`和`y`中。Elasticsearch组织其训练数据的方式与scikit-learn中的约定不同。在Elasticsearch中，我们的训练数据存储在一个单独的Elasticsearch索引中。索引中的每个文档代表一个数据点，它是表示特征的字段和表示因变量的字段的组合（有关因变量的更多信息以及为什么它们在监督学习中很重要，请参阅[*第11章*](B17040_11_Epub_AM.xhtml#_idTextAnchor209)，*分类分析*和[*第12章*](B17040_12_Epub_AM.xhtml#_idTextAnchor230)，*回归*）。
- en: 'In contrast with Elasticsearch''s approach, scikit-learn represents each data
    point using a vector that contains all of the feature values. These vectors make
    up the matrix stored in the variable `X`. We can use Python''s slicing syntax
    to see what a sample data point would look like. An example is shown in *Figure
    13.11*:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与Elasticsearch的方法相比，scikit-learn使用包含所有特征值的向量来表示每个数据点。这些向量构成了存储在变量`X`中的矩阵。我们可以使用Python的切片语法来查看一个样本数据点的外观。一个示例在*图13.11*中显示：
- en: '![Figure 13.11 – A data point is represented as a vector of field values'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图13.11 – 数据点表示为字段值的向量'
- en: '](img/B17040_13_11.jpg)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_13_11.jpg)'
- en: Figure 13.11 – A data point is represented as a vector of field values
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.11 – 数据点表示为字段值的向量
- en: 'The dependent variables are stored in the separate variable `y`. In a similar
    fashion to our previous example, we can use Python''s slicing syntax to see which
    class the data point whose feature values (or **feature vector**) are depicted
    belongs to. This is shown in *Figure 13.12*:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因变量存储在单独的变量`y`中。与之前的例子类似，我们可以使用Python的切片语法来查看特征值（或**特征向量**）所描述的数据点属于哪个类别。这如图*13.12*所示：
- en: '![Figure 13.12 – The class label for the first data point is 0'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图13.12 – 第一个数据点的类别标签是0'
- en: '](img/B17040_13_12.jpg)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_13_12.jpg)'
- en: Figure 13.12 – The class label for the first data point is 0
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.12 – 第一个数据点的类别标签是0
- en: 'Now that we have imported our dataset and verified that it looks acceptable,
    we can move on to the next step, which is training our decision tree model. While
    Elasticsearch automatically splits our training data into a training and a testing
    dataset, in scikit-learn, we have to perform this step manually. Although it''s
    not strictly necessary to do this in this case, since we are not overly interested
    in systematically measuring the performance of our model, we will still show this
    as an example for interested readers who will need to do this in their own projects:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经导入了数据集并验证了它看起来是可接受的，我们可以继续下一步，即训练我们的决策树模型。虽然Elasticsearch会自动将我们的训练数据分成训练集和测试集，但在scikit-learn中，我们必须手动执行这一步骤。尽管在这种情况下这不是严格必要的，因为我们并不特别感兴趣于系统地衡量我们模型的性能，但我们仍将此作为示例提供给有兴趣的读者，他们需要在自己的项目中执行此操作：
- en: '[PRE23]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As you can see from the preceding code snippet, we pass in the variable containing
    the feature vectors for every data point and the variable containing the dependent
    variable values or class labels into the scikit-learn function, `train_test_split`,
    and the function returns the feature vectors and dependent variables that belong
    to the training set (`X_train` and `y_train`) and testing set (`X_test`, `y_test)`,
    respectively.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前述代码片段所示，我们将包含每个数据点特征向量的变量和包含因变量值或类别标签的变量传递给scikit-learn函数`train_test_split`，该函数返回属于训练集的特征向量（`X_train`）和因变量（`y_train`）以及测试集的特征向量（`X_test`）和因变量（`y_test`）。
- en: 'Now that we have our training and test dataset generated, we can move on to
    training the decision tree classifier. The first step is to create an instance
    of the `DecisionTreeClassifier` class and fit it with the feature vectors and
    class labels from the training dataset:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经生成了训练集和测试集，我们可以继续训练决策树分类器。第一步是创建`DecisionTreeClassifier`类的实例，并用训练数据集中的特征向量和类别标签来拟合它：
- en: '[PRE24]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The trained model is referenced by the `dec_tree` variable. This is the variable
    we will serialize and upload to Elasticsearch using eland slightly later in this
    tutorial. First, however, let''s run a quick check on our model by asking it to
    classify a data point from the testing dataset (remember, these are data points
    that the model has not seen previously during the training phase) as shown in
    *Figure 13.13*:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练好的模型通过`dec_tree`变量引用。这是我们将在本教程稍后使用eland序列化和上传到Elasticsearch的变量。首先，让我们通过要求它对测试数据集中的数据点进行分类来快速检查我们的模型（记住，这些是在训练阶段模型之前没有见过的数据点），如图*13.13*所示：
- en: '![Figure 13.13 – Predictions from the trained decision tree model'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图13.13 – 训练好的决策树模型的预测'
- en: '](img/B17040_13_13.jpg)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_13_13.jpg)'
- en: Figure 13.13 – Predictions from the trained decision tree model
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.13 – 训练好的决策树模型的预测
- en: 'The model predicts that the first data point in our testing dataset belongs
    to class 1\. We can double-check whether or not this is the actual label of the
    data point by checking the first element in the `y_test` variable, as shown in
    *Figure 13.14*:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型预测我们测试数据集的第一个数据点属于类别1。我们可以通过检查`y_test`变量中的第一个元素来双重验证这个数据点的实际标签，如图*13.14*所示：
- en: '![Figure 13.14 – The value of the dependent variable for the first data point
    in the testing set'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图13.14 – 测试集中第一个数据点的因变量值'
- en: '](img/B17040_13_14.jpg)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/B17040_13_14.jpg)'
- en: Figure 13.14 – The value of the dependent variable for the first data point
    in the testing set
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.14 – 测试集中第一个数据点的因变量值
- en: In this case, the model's prediction matches the actual label of the dataset.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，模型的预测与数据集的实际标签相匹配。
- en: 'Finally, let''s prepare to upload this model into our Elasticsearch cluster
    using eland. First, we have to import the required `MLModel` class, as shown in
    the following code sample:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们准备使用eland将此模型上传到我们的Elasticsearch集群。首先，我们必须导入所需的`MLModel`类，如下面的代码示例所示：
- en: '[PRE25]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once we have this class in our script or Jupyter notebook, we can proceed to
    the next step, which is retrieving the feature names from the original scikit-learn
    dataset. The steps required to do this are shown in the following code sample:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦我们在脚本或Jupyter笔记本中有了这个类，我们就可以进行下一步，即从原始scikit-learn数据集中检索特征名称。完成此操作所需的步骤如下所示：
- en: '[PRE26]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The interested reader can print out the `feature_names` variable (or take a
    look at the Jupyter notebook accompanying this walk-through) to see what kinds
    of features are included. In the interest of saving space, we will leave out the
    feature names list.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感兴趣的读者可以打印出`feature_names`变量（或查看此说明的配套Jupyter笔记本）以查看包含哪些类型的特征。为了节省空间，我们将省略特征名称列表。
- en: 'Finally, we will call the `import_model` method on the `MLModel` class, as
    shown in the following code snippet:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将在`MLModel`类上调用`import_model`方法，如下面的代码片段所示：
- en: '[PRE27]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, this is a method that requires quite a number of parameters.
    The first parameter, `es_client`, is an instance of the Elasticsearch client object,
    which specifies how to connect to our Elasticsearch cluster. This is discussed
    in further detail in the *Exporting and importing trained models with the Inference
    API and Python* section in this chapter.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您所见，这是一个需要相当多参数的方法。第一个参数`es_client`是Elasticsearch客户端对象的实例，它指定了如何连接到我们的Elasticsearch集群。这在本章的*使用推理API和Python导出和导入训练模型*部分有更详细的讨论。
- en: 'The second parameter is `model_id`, an identifier that will be used to identify
    the model once it has been uploaded into the Elasticsearch cluster. In this case,
    we have set the `model_id` variable, as shown in the following code snippet:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个参数是`model_id`，这是一个标识符，一旦模型被上传到Elasticsearch集群，它将被用来识别该模型。在这种情况下，我们已经设置了`model_id`变量，如下面的代码片段所示：
- en: '[PRE28]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Of course, however, it is possible to set this identifier to be one of your
    choosing. Finally, we pass in the variable name that contains a reference to our
    trained model, `dec_tree`, the list of `feature_names` that we retrieved from
    the original dataset, and set the `es_if_exists` flag to `'replace'`, which means
    that if we run the code snippet more than once, an existing model with the same
    `model_id` will be overwritten. There are cases where this might not be the desired
    behavior, but in this case, since we are prototyping, it is a useful flag to set.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当然，然而，您可以将此标识符设置为您的选择之一。最后，我们传递包含对训练模型引用的变量名，`dec_tree`，从原始数据集中检索到的`feature_names`列表，并将`es_if_exists`标志设置为`'replace'`，这意味着如果我们多次运行代码片段，具有相同`model_id`的现有模型将被覆盖。在某些情况下，这可能不是期望的行为，但在这个案例中，因为我们正在原型设计，这是一个有用的标志来设置。
- en: 'Once the command discussed in the preceding section has been run, we can use
    the Trained Models API to determine whether or not this model has been imported
    into our cluster successfully. To do this, we will run the following command:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦运行了前面章节中讨论的命令，我们可以使用训练模型API来确定此模型是否已成功导入到我们的集群中。为此，我们将运行以下命令：
- en: '[PRE29]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As we can see, based on the returned API response, our model has indeed been
    successfully imported into the cluster and is now ready to use in ingest pipelines.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所见，基于返回的API响应，我们的模型确实已成功导入到集群中，现在可以在摄取管道中使用。
- en: Reminder
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提醒
- en: 'All of the code samples used in the figures are available in the Jupyter notebook
    that is linked in the book''s GitHub repository here: [https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb).'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图中使用的所有代码示例都可在本书GitHub存储库中链接的Jupyter笔记本中找到：[https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb)。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have looked at the various options available for using supervised
    models trained in Elasticsearch and external libraries such as scikit-learn. We
    have learned about the Trained Models API, which is useful when managing and examining
    trained supervised learning models in an Elasticsearch cluster and how to make
    use of these models to make predictions on previously unseen examples with the
    help of inference processors and ingest pipelines. In the appendix following this
    chapter, we will provide some tips and tricks that make it easier to work with
    the Elastic Machine Learning stack.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在 Elasticsearch 和外部库（如 scikit-learn）中训练的监督模型的各种可用选项。我们学习了 Trained
    Models API，它在管理并检查 Elasticsearch 集群中的训练好的监督学习模型时非常有用，以及如何借助推理处理器和摄取管道，利用这些模型对先前未见过的示例进行预测。在本章附录中，我们将提供一些技巧和窍门，使与
    Elastic Machine Learning 堆栈协同工作变得更加容易。
