<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Bringing Your Apps to Life with OpenCV Machine Learning</h1></div></div></div><p>With so much data around us, we need better systems and applications to process it and extract relevant information out of it. A field of computer science that deals <a id="id348" class="indexterm"/>with this is <strong>machine learning</strong>. In this chapter, we will take a look at the different machine learning techniques that can be used to exploit all the data around us and build smart applications that can deal with unencountered situations or scenarios, without any form of human intervention.</p><p>In the recent years, computer vision and machine learning have formed a strong synergy between them, thus enabling technologies that have helped build some extremely efficient and useful applications. Humanoids, robotic arms, and assembly lines are some of the examples where computer vision and machine learning find applications. Developers and researchers are now trying to exploit mobile platforms and build light-weight applications that can be used by common people. In the following section, we will build an application for <a id="id349" class="indexterm"/>
<strong>Optical Character Recognition</strong> (<strong>OCR</strong>) using standard OpenCV and Android APIs. Toward the end, we will revisit the Sudoku solving application that we started developing in <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>, <em>Detecting Basic Features in Images</em>.</p><p>We will understand the machine learning techniques alongside building applications.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec40"/>Optical Character Recognition</h1></div></div></div><p>
<strong>Optical Character Recognition</strong> (<strong>OCR</strong>) is <a id="id350" class="indexterm"/>one of the favorite topics of research in computer vision and machine learning. There are a lot of efficient off-the-shelf implementations and algorithms readily available for OCR, but for better understanding of the concepts, we will build our own OCR Android application. Before we get down to writing the code for our application, let's take some time to take a look at the different character recognition techniques and how they work. In this chapter, we will use two standard machine learning techniques: <strong>k-nearest neighbors</strong> (<strong>KNN</strong>) <a id="id351" class="indexterm"/>and <strong>Support Vector Machines</strong> (<strong>SVM</strong>), while <a id="id352" class="indexterm"/>building our applications.</p><p>The aim of this<a id="id353" class="indexterm"/> chapter is to build a real-time digit recognition application. The application will have a live camera output being displayed on the mobile screen and as soon as the camera captures a digit, we will recognize the digit.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec56"/>OCR using k-nearest neighbors</h2></div></div></div><p>k-nearest neighbors<a id="id354" class="indexterm"/> is the<a id="id355" class="indexterm"/> one of the simplest algorithms used for <a id="id356" class="indexterm"/>supervised classification. In KNN, we give the training dataset and their corresponding labels as input. An n-dimensional space is created (where <em>n</em> is the length of each training data) and every training data is plotted as a point in it. While classification, we plot the data to be classified in the same n-dimensional space, and calculate the distance of that point from every other point in the space. The distance computed is used to find an appropriate class for the testing data.</p><p>Here is a step–by-step explanation of the working of the algorithm:</p><div><ol class="orderedlist arabic"><li class="listitem">Choose a user-defined value of <em>k</em>.</li><li class="listitem">Store the training data along with their classes in the form of a row vector.</li><li class="listitem">Take the input query (data to be classified) and calculate the distance from it to every other row vector in the training data (meaning of distance is explained in the following box).</li><li class="listitem">Sort all the row vectors in the ascending order of their distance (calculated in the previous step) from the query data.</li><li class="listitem">Finally, from the first <em>k</em> sorted row vectors, choose the class (training labels), which has the majority of row vectors as the predicted class.</li></ol></div><div><div><h3 class="title"><a id="note16"/>Note</h3><p>
<strong>Distance between vectors</strong>
</p><p>In Euclidean Space, we <a id="id357" class="indexterm"/>define the distance between two vectors as:</p><div><img src="img/B02052_07_03.jpg" alt="OCR using k-nearest neighbors"/></div><p>where, <em>xi</em> and <em>yi</em> are the <em>i<sup>th</sup></em> dimension values of the two vectors <em>x</em> and <em>y</em> respectively. <em>n</em> is the length of the training vectors (<em>x</em> and <em>y</em> in our case). The algorithm does not <a id="id358" class="indexterm"/>put any restriction on the type of distance we can use. Some other types of distances that we can use are <strong>Manhattan distance</strong>, Maximum distance and the likes. Refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Distance">http://en.wikipedia.org/wiki/Distance</a> for some other definitions of distance.</p></div></div><p>Simple enough! How do we use it with image data? For us to be able to use KNN on the image data, we need to convert the training images to some sort of a row vector.</p><p>Consider a 10x10 grayscale <a id="id359" class="indexterm"/>image of any digit from one to nine. The easiest and the fastest way to get a feature vector from the 10x10 image is to convert it into a 1x100 row vector. This can be done by appending the rows in the image one after another. This way, we can convert all the images in our training set to row vectors for later use in the KNN classifier.</p><p>To make it easier for us to build the digit recognition application, we will break it down into smaller parts listed as follows and finally use them together:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Making a camera application</li><li class="listitem" style="list-style-type: disc">Handling the training data</li><li class="listitem" style="list-style-type: disc">Recognizing the captured digit</li></ul></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec20"/>Making a camera application</h3></div></div></div><p>We will begin by <a id="id360" class="indexterm"/>building a simple camera application that displays the camera output on the screen, as we did in <a class="link" href="ch04.html" title="Chapter 4. Drilling Deeper into Object Detection – Using Cascade Classifiers">Chapter 4</a>, <em>Drilling Deeper into Object Detection – Using Cascade Classifiers</em>:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Create a new Android project in Eclipse (or Android Studio)</li><li class="listitem" style="list-style-type: disc">Initialize OpenCV in the project (refer to <a class="link" href="ch01.html" title="Chapter 1. Applying Effects to Images">Chapter 1</a>, <em>Applying Effects to Images</em>).</li><li class="listitem" style="list-style-type: disc">Add <code class="literal">JavaCameraView</code> to the main activity using the following code snippet:<div><pre class="programlisting">&lt;org.opencv.android.JavaCameraView
        android:layout_width="fill_parent"
        android:layout_height="fill_parent"
        android:visibility="gone"
        android:id="@+id/java_surface_view"
        opencv:camera_id="any" /&gt;</pre></div></li></ul></div><p>Once the camera is in place, draw<a id="id361" class="indexterm"/> a square on the screen that will help the user to localize the digit that he/she wants to recognize. The user will point to the digit and try to bring it within the square drawn on the screen (as shown in Figure 1). Copy the following piece of code in the Mat <code class="literal">onCameraFrame()</code> function:</p><div><pre class="programlisting">Mat temp = inputFrame.rgba();

Core.rectangle(temp, new Point(temp.cols()/2 - 200, temp.rows() / 2 - 200), new Point(temp.cols() / 2 + 200, temp.rows() / 2 + 200), new Scalar(255,255,255),1);</pre></div><p>In this code, we take the size of the frame captured by the mobile camera and draw a 400x400 (can vary according to the screen size) white rectangle in the center of the image (as shown in Figure 1). That's it. The camera application is ready. Next, is handling the training data in the application.</p><div><img src="img/B02052_07_01.jpg" alt="Making a camera application"/><div><p>Figure 1. Screenshot of the camera application</p></div></div></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec21"/>Handling the training data</h3></div></div></div><p>This is the<a id="id362" class="indexterm"/> trickiest part of the application. Training data plays a crucial role in any machine learning application. The amount of data that such applications deal with is usually in the order of few megabytes. This may not be a concern for a normal desktop application, but for a mobile application (because of resource constraints), even handling around 50 megabytes can lead to performance issues, if not done properly. The code needs to be concise, to the point, and should have minimum memory leaks.</p><p>For this application, we will make use of a publically available handwritten digits dataset —MNIST, to train the KNN classifier.</p><div><div><h3 class="title"><a id="note18"/>Note</h3><p>The MNIST database (<a class="ulink" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>) of handwritten<a id="id363" class="indexterm"/> digits available from this page, has a training set <a id="id364" class="indexterm"/>of 60,000 examples and a test set of 10,000 examples. It is a subset of a larger set available from MNIST. The digits have been size-normalized and centered in a fixed-size image. (Text taken from Prof. Yann LeCun's web page, which is available at <a class="ulink" href="http://yann.lecun.com">http://yann.lecun.com</a>.)</p></div></div><p>First, download the MNIST training data <a id="id365" class="indexterm"/>using the following links:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Training images at <a class="ulink" href="http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz">http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz</a></li><li class="listitem" style="list-style-type: disc">Training labels at <a class="ulink" href="http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz">http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz</a></li></ul></div><p>Extract the downloaded files and transfer them to an Android phone (make sure you have around 60 MB of free space available).</p><p>Getting back to our application, create a new <code class="literal">DigitRecognizer</code> class that will handle all the tasks related to digit recognition, including loading the dataset into the application, training the classifier, and finally, recognizing the digit. Add a new Java class to the project and name it <code class="literal">DigitRecognizer</code>.</p><p>So, we already have the training images and training labels stored in the phone. We need to load the data into the application. For this, all we have to do is read the data from these files and make them compatible with OpenCV's API.</p><p>Add a new function void <code class="literal">ReadMNISTData()</code> to the <code class="literal">DigitRecognizer</code> class created earlier. This function will read the MNIST dataset and store it in the form of a Mat (OpenCV's class to store images). Read the dataset in two parts: first, the training images and then the training labels.</p><p>In <code class="literal">ReadMNISTData()</code>, create a new <code class="literal">File</code> object that will store the path to the phone's SD card (as shown in the following code). In case the file is in the phone's internal memory, skip this step and provide an absolute path of the file that we wish to use later in the code:</p><div><pre class="programlisting">File external_storage = Environment.getExternalStorageDirectory();</pre></div><p>After doing this, create <a id="id366" class="indexterm"/>another <code class="literal">File</code> object that will point to the exact file that we want to read in our application, and an <code class="literal">InputStreamReader</code> object that will help us in reading the file:</p><div><pre class="programlisting">File mnist_images_file = new File(external_storage, images_path);

FileInputStream images_reader = new FileInputStream(mnist_images_file);</pre></div><p>Here, <code class="literal">images_path</code> is the absolute path of the <code class="literal">train-images-idx3-ubyte.idx3</code> training images file.</p><p>Before we continue with the code, we need to understand how images are stored in the file. Here is the description of the contents of the training images file:</p><div><pre class="programlisting">[offset] [type]          [value]          [description] 
0000     32 bit integer  0x00000803(2051) magic number 
0004     32 bit integer  60000            number of images 
0008     32 bit integer  28               number of rows 
0012     32 bit integer  28               number of columns 
0016     unsigned byte  ??                pixel 
0017     unsigned byte   ??               pixel 
........ 
xxxx     unsigned byte   ??               pixel</pre></div><p>Pixels are organized row-wise. Pixel values are 0 to 255, where 0 represents the background (white) and 255 represents the foreground (black).</p><p>With this information, we can continue writing the code:</p><div><pre class="programlisting">Mat training_images = null;

try{
            //Read the file headers which contain the total number of images and dimensions. First 16 bytes hold the header
            /*
            byte 0 -3 : Magic Number (Not to be used)
            byte 4 - 7: Total number of images in the dataset
            byte 8 - 11: width of each image in the dataset
            byte 12 - 15: height of each image in the dataset
            */

            byte [] header = new byte[16];
            images_reader.read(header, 0, 16);

            //Combining the bytes to form an integer
            ByteBuffer temp = ByteBuffer.wrap(header, 4, 12);
            total_images = temp.getInt();
            width = temp.getInt();
            height = temp.getInt();

            //Total number of pixels in each image
            int px_count = width * height;
            training_images = new Mat(total_images, px_count, CvType.CV_8U);

            //images_data = new byte[total_images][px_count];
            //Read each image and store it in an array.

            for (int i = 0 ; i &lt; total_images ; i++)
            {
                byte[] image = new byte[px_count];
                images_reader.read(image, 0, px_count);
                training_images.put(i,0,image);
            }
            training_images.convertTo(training_images, CvType.CV_32FC1);
            images_reader.close();
        }
        catch (IOException e)
        {
            Log.i("MNIST Read Error:", "" + e.getMessage());
        }</pre></div><p>In the preceding code, first, we<a id="id367" class="indexterm"/> read the first 16 bytes of the file, which stores the number of images, height, and width of the images (refer to the aforementioned table describing the contents of the file). Using the <code class="literal">ByteBuffer</code> class, we get four integers from the 16 bytes by combining four bytes, one each for an integer.</p><p>In OpenCV, KNN's implementation requires us to pass all the feature vectors using the Mat class. Every training image needs to be converted to a row vector that will form one row of the Mat object, which will be passed to the KNN classifier. For example, if we have 5,000 training images each with dimensions 20x20, we will need a Mat object with dimensions 5000x400 that can <a id="id368" class="indexterm"/>be passed to OpenCV's KNN training function. Confused? Continue reading!</p><p>Take a 20x20 image from the training dataset and convert it to a 1x400 vector by appending rows one after another. Do this for all the images. At the end, we will have 5,000 such 1x400 vectors. Now, create a new Mat object with dimensions 5000x400, and each row of this new Mat object will be the 1x400 vector that we got just now by resizing the original images in the dataset.</p><p>This is what the preceding piece of code intends to do. First, read all the pixels in an image using the following code:</p><div><pre class="programlisting">byte[] image = new byte[px_count];
images_reader.read(image, 0, px_count);</pre></div><p>Here, <code class="literal">px_count</code> is the <a id="id369" class="indexterm"/>total number of pixels in a training image and <code class="literal">image</code> is a row vector that stores the image. As explained earlier, we need to copy these row vectors to a Mat object (<code class="literal">training_images</code> refers to the Mat object that will be used to store these training images). Copy the <code class="literal">image</code> row vector to <code class="literal">training_images</code>, as follows:</p><div><pre class="programlisting">training_images.put(i,0,image);</pre></div><p>Training data is in place. We now need their corresponding labels. As we did for training images, their corresponding labels (label values are from 0 to 9) can be read in the same way. The contents of the <code class="literal">labels</code> file are arranged in the following way:</p><div><pre class="programlisting">[offset] [type]         [value]          [description] 
0000     32 bit integer  0x00000801(2049) magic 
  number (MSB first)
0004     32 bit integer  60000            number of items 
0008     unsigned byte   ??               label 
0009     unsigned byte   ??               label 
........ 
xxxx     unsigned byte   ??               label</pre></div><p>Here is the code to read the labels:</p><div><pre class="programlisting">//Read Labels
        Mat training_labels = null;

        labels_data = new byte[total_images];
        File mnist_labels_file = new File(external_storage, labels_path);
        FileInputStream labels_reader = new FileInputStream(mnist_labels_file);

        try{

            training_labels = new Mat(total_images, 1, CvType.CV_8U);
            Mat temp_labels = new Mat(1, total_images, CvType.CV_8U);
            byte[] header = new byte[8];
            //Read the header
            labels_reader.read(header, 0, 8);
            //Read all the labels at once
            labels_reader.read(labels_data,0,total_images);
            temp_labels.put(0,0, labels_data);

            //Take a transpose of the image
            Core.transpose(temp_labels, training_labels);
            training_labels.convertTo(training_labels, CvType.CV_32FC1);
            labels_reader.close();
        }
        catch (IOException e)
        {
            Log.i("MNIST Read Error:", "" + e.getMessage());
        }</pre></div><p>The basis of the preceding<a id="id370" class="indexterm"/> code is similar to the code used for reading images.</p><p>We have successfully loaded the training data in our application. At this point, you can use some diagnostic tools of Android to check the memory usage of the application. An important point that you need to take care of is to not duplicate the data. Doing this will increase the amount of memory consumed, which can affect the performance of your application as well as other applications running on your phone. Pass the <code class="literal">training_images</code> and <code class="literal">training_labels</code> Mat objects to OpenCV's KNN classifier object:</p><div><pre class="programlisting">knn = new CvKNearest();
knn.train(training_images, training_labels, new Mat(), false, 10, false);</pre></div><p>The KNN classifier is ready. We are now ready to classify the data.</p></div><div><div><div><div><h3 class="title"><a id="ch07lvl3sec22"/>Recognizing digits</h3></div></div></div><p>This is the final<a id="id371" class="indexterm"/> part of the application. Here, we use the frames that are captured from the camera as an input to the classifier and allow the classifier to predict the digit in the frame.</p><p>To begin with, add a new function void <code class="literal">FindMatch()</code> to the <code class="literal">DigitRecognizer</code> class created in the previous section as follows:</p><div><pre class="programlisting">void FindMatch(Mat test_image)
    {

        //Dilate the image
        Imgproc.dilate(test_image, test_image, Imgproc.getStructuringElement(Imgproc.CV_SHAPE_CROSS, new Size(3,3)));
        //Resize the image to match it with the sample image size
        Imgproc.resize(test_image, test_image, new Size(width, height));
        //Convert the image to grayscale
        Imgproc.cvtColor(test_image, test_image, Imgproc.COLOR_RGB2GRAY);
        //Adaptive Threshold
        Imgproc.adaptiveThreshold(test_image,test_image,255,Imgproc.ADAPTIVE_THRESH_MEAN_C, Imgproc.THRESH_BINARY_INV,15, 2);

        Mat test = new Mat(1, test_image.rows() * test_image.cols(), CvType.CV_32FC1);
        int count = 0;
        for(int i = 0 ; i &lt; test_image.rows(); i++)
        {
            for(int j = 0 ; j &lt; test_image.cols(); j++) {
                test.put(0, count, test_image.get(i, j)[0]);
                count++;
            }
        }
      
        Mat results = new Mat(1, 1, CvType.CV_8U);
        
        knn.find_nearest(test, 10, results, new Mat(), new Mat());
        Log.i("Result:", "" + results.get(0,0)[0]);

    }</pre></div><div><div><h3 class="title"><a id="note19"/>Note</h3><p>Note: Images in the training dataset are 28x28 binary images.</p></div></div><p>The camera output is not directly usable. We need to preprocess the images to bring them as close as possible to the images in the training dataset for our classifier to give accurate results.</p><p>Perform the following<a id="id372" class="indexterm"/> steps (preferably in the same order) to make the camera output usable by the KNN classifier:</p><div><ol class="orderedlist arabic"><li class="listitem">Dilate the image to make the digit more prominent in the image and reduce any background noise.</li><li class="listitem">Resize the image to 28x28. The training images are also of this dimension.</li><li class="listitem">Convert the image to a grayscale image.</li><li class="listitem">Perform adaptive threshold on the image to get a binary image.</li></ol></div><div><div><h3 class="title"><a id="note20"/>Note</h3><p>All the parameters used in the code here are subject to lighting conditions. You are requested to tweak these parameters to suit their environment for best results.</p></div></div><p>After following these steps, we will have a test that needs to go into the KNN classifier that we trained in the <a id="id373" class="indexterm"/>previous section. Before this can happen, there is one more thing that needs to be done to the test image—transforming the image to a row vector (remember the transformations we did to training images?). Convert the 28x28 test image to a 1x784 row vector. Use the following piece of code to transform the image:</p><div><pre class="programlisting">Mat test = new Mat(1, test_image.rows() * test_image.cols(), CvType.CV_32FC1);
int count = 0;
for(int i = 0 ; i &lt; test_image.rows(); i++)
{
    for(int j = 0 ; j &lt; test_image.cols(); j++) {
        test.put(0, count, test_image.get(i, j)[0]);
        count++;
    }
}</pre></div><p>Finally, pass the transformed <code class="literal">test</code> image to the KNN classifier and store the result in the 1x1 Mat object <code class="literal">results</code>. The last two parameters in the <code class="literal">find_nearest</code> function are optional:</p><div><pre class="programlisting">knn.find_nearest(test, 10, results, new Mat(), new Mat());</pre></div><p>One last thing, how <a id="id374" class="indexterm"/>and when do we call the <code class="literal">FindMatch</code> function? Since we are building a real-time digit recognition application, we need to perform the matching operation on every output frame of the camera. Because of this, we need to call this function in <code class="literal">onCameraFrame()</code> in the main activity class. The function should finally look like this:</p><div><pre class="programlisting">public Mat onCameraFrame(CvCameraViewFrame inputFrame) {

        //Get image size and draw a rectangle on the image for reference
        Mat temp = inputFrame.rgba();
        Core.rectangle(temp, new Point(temp.cols()/2 - 200, temp.rows() / 2 - 200), new Point(temp.cols() / 2 + 200, temp.rows() / 2 + 200), new Scalar(255,255,255),1);
        Mat digit = temp.submat(temp.rows()/2 - 180, temp.rows() / 2 + 180, temp.cols() / 2 - 180, temp.cols() / 2 + 180).clone();
        Core.transpose(digit,digit);
        mnist.FindMatch(digit);

        return temp;
    }</pre></div><p>We take the RGBA image of the camera output and extract the part of the image enclosed by the rectangle that we drew on the screen before. We want the user to bring the digit within the rectangle for it to be successfully recognized.</p><p>Since our <a id="id375" class="indexterm"/>application is written for landscape mode (set in the <code class="literal">AndroidManifest.xml</code> file) but we use it in the portrait mode, we need to transpose the test image before we can run the recognition algorithm. Hence, run this command:</p><div><pre class="programlisting">Core.transpose(digit,digit);</pre></div><p>We have successfully created a real-time digit recognition application. Let's take a look at another machine learning technique that can be used in recognizing digits.</p></div></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec57"/>OCR using Support Vector Machines</h2></div></div></div><p>
<strong>Support Vector Machines</strong> (<strong>SVMs</strong>) are <a id="id376" class="indexterm"/>supervised <a id="id377" class="indexterm"/>learning algorithms that are commonly used for classification and regression. In SVMs, the training data is divided into different regions using infinite hyperplanes, and each region represents a class. To test data, we plot the point in the same space as the training points and using the hyperplanes compute the region where the test point lies. SVMs are useful when dealing with high-dimensional data.</p><div><div><h3 class="title"><a id="note21"/>Note</h3><p>For details on SVMs, you can refer to <a class="ulink" href="http://www.support-vector-machines.org/">http://www.support-vector-machines.org/</a>.</p></div></div><p>In this section, we will learn how to use SVMs for digit recognition. As in KNN, to train an SVM, we will directly use the training images without any image manipulations or detecting any extra features.</p><div><div><h3 class="title"><a id="note22"/>Note</h3><p>Instead of directly using the training images, it is possible to extract some features from the images and use those features as training data for the SVM. One of the OpenCV tutorials implemented in Python follows a different path. Here, they first deskew the image using affine transformations, then compute Histogram of Orientation Gradients. These HoG features are used to train the SVM. The reason why we are not following the same path is because of the cost of computation involved in computing affine transformations and HoG.</p></div></div><p>Only slight <a id="id378" class="indexterm"/>modifications are involved in using SVM instead of KNN in the application that we built in the previous section. The basic camera application and handling training data remains as is. The only modification that has to happen is in the digit recognition part where we train the classifier.</p><p>In <code class="literal">ReadMNISTData()</code> function, instead of creating a KNN classifier object, we will create an SVM object. Remove the following lines where we declared and initialized a KNN object:</p><div><pre class="programlisting">knn = new CvKNearest();
knn.train(training_images, training_labels, new Mat(), false, 10, false);</pre></div><p>Now, replace them with the following lines (declaring and initializing an SVM object):</p><div><pre class="programlisting">svm = new CvSVM();
svm.train(training_images, training_labels);</pre></div><p>The SVM classifier is now ready. The next step for the KNN classifier is to pass a test image to the classifier and check the result. For this, we need to modify the <code class="literal">FindMatch()</code> function. Replace the line that uses KNN for classification with an appropriate line which uses SVM.</p><div><div><h3 class="title"><a id="note23"/>Note</h3><p>An optimization that the users can incorporate in the preceding application is that they can save the trained classifier in a file on the device. This will save time in training the classifier again and again.</p></div></div><p>Let's take a look at the following command:</p><div><pre class="programlisting">knn.find_nearest(test, 10, results, new Mat(), new Mat());</pre></div><p>We need to replace the preceding command with the following command:</p><div><pre class="programlisting">svm.predict(test);</pre></div><p>That's all. Our <a id="id379" class="indexterm"/>application is ready. We can run the application, check for the results, and probably compare which algorithm runs better under what condition.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Solving a Sudoku puzzle</h1></div></div></div><p>Remember<a id="id380" class="indexterm"/> the Sudoku puzzle project in <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>, <em>Detecting Basic Features in Images</em>? Now is the perfect time to revisit this project and see whether we can use anything that we learnt in this chapter to complete this application. So, in <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>, <em>Detecting Basic Features in Images</em>, we had successfully detected the Sudoku puzzle. Only two things were left in that application: recognizing digits and solving the Sudoku puzzle.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec58"/>Recognizing digits in the puzzle</h2></div></div></div><p>Let's<a id="id381" class="indexterm"/> pick up from where we left in <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>, <em>Detecting Basic Features in Images</em>. After detecting the grid successfully, we need to further break down the grid into 81 small squares. There are many possible ways of doing this, but here, we will look at only three techniques.</p><p>First, the easiest of all is to draw nine equally spaced vertical and horizontal lines each on the image, and assume the digits to be placed within the boxes made by these lines.</p><div><img src="img/B02052_07_02.jpg" alt="Recognizing digits in the puzzle"/><div><p>Figure 2. Vertical and horizontal lines drawn on a Sudoku grid</p></div></div><p>The second way<a id="id382" class="indexterm"/> is using Hough lines. Apply Hough lines on the Sudoku grid and store all the lines that are returned. Ideally, nine vertical and nine horizontal lines should be returned but chances of this happening are very bleak, unless you have a very good camera and perfect lighting conditions. There will be missing or incomplete lines that will reduce the application's performance or may lead to false results.</p><p>The third way is using corner detection. Run any corner detection algorithm and get all the corners in the image. These corners represent the vertices of the boxes enclosing the digits. Once you have all the corners, you can join four corners to form a box and extract that part of the image.</p><p>The previously mentioned techniques may not always guarantee perfect results. Different techniques may perform better, depending on the surroundings and the kind of camera being used.</p><p>Extract all 89 images using any of the previously mentioned technique, and pass them through a pretrained digit classifier—SVM or KNN (as seen in the previous sections). Done! Take the output of the classifier and make a 9x9 integer matrix in your code, and fill it up with the corresponding digits recognized from the grid. So now we have the grid with us. Use any brute force or <a id="id383" class="indexterm"/>Artificial Intelligence algorithm to get the correct solution of the Sudoku puzzle. Different algorithms that can be used are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Backtracking</li><li class="listitem" style="list-style-type: disc">Genetic algorithms</li><li class="listitem" style="list-style-type: disc">Sudoku as a constraint problem</li></ul></div><p>Refer to <a class="ulink" href="http://en.wikipedia.org/wiki/Sudoku_solving_algorithms">http://en.wikipedia.org/wiki/Sudoku_solving_algorithms</a> for a detailed explanation on these algorithms.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec42"/>Summary</h1></div></div></div><p>In this chapter, we looked at how to make an application intelligent by incorporating machine learning into them. We looked at Support Vector Machines and KNNs, and how we can use them to build applications that can learn patterns in user entered data. Till now we have covered many computer vision algorithms and their implementations in detail. In the next chapter, we will take a look at some commonly faced errors while building such applications, and some best practices that will help you make the applications more efficient.</p></div></body></html>