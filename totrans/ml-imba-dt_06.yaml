- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Imbalance in Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Class imbalanced data is a common issue for deep learning models. When one or
    more classes have significantly fewer samples, the performance of deep learning
    models can suffer as they tend to prioritize learning from the majority class,
    resulting in poor generalization for the minority class(es).
  prefs: []
  type: TYPE_NORMAL
- en: 'A lot of real-world data is imbalanced, which presents challenges to deep learning
    classification tasks. *Figure 6**.1* shows some common categories of imbalanced
    data problems in various deep learning applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Some common categories of imbalanced data problems
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data imbalance in deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of deep learning techniques to handle data imbalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-label classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, we’ll have a foundational understanding of deep
    learning and neural networks. We’ll have also grasped the impact of data imbalance
    on these models and gained a high-level overview of various strategies to address
    the challenges of imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `PyTorch`. `PyTorch` is an open source machine learning library that’s used
    for deep learning tasks and has grown in popularity recently because of its flexibility
    and ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: You can install `PyTorch` using `pip` or `conda`. Visit the official PyTorch
    website ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/))
    to get the appropriate command for your system configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: A brief introduction to deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a subfield of machine learning that focuses on artificial neural
    networks with multiple layers (deep models typically have three or more layers,
    including input, output, and hidden layers). These models have demonstrated remarkable
    capabilities in various applications, including image and speech recognition,
    natural language processing, and autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: The prevalence of “big data” (large volumes of structured or unstructured data,
    often challenging to manage with traditional data processing software) problems
    greatly benefited from the development of **Graphical Processing Units** (**GPUs**),
    which were initially designed for graphics processing.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will provide a concise introduction to the foundational
    elements of deep learning, discussing only what is necessary for the problems
    associated with data imbalance in deep learning. For a more in-depth introduction,
    we recommend referring to a more dedicated book on deep learning (please refer
    to the resources listed as [1] and [2] in the *References* section).
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are the foundation of deep learning. Inspired by the structure
    and function of the human brain, neural networks consist of interconnected nodes
    or artificial neurons organized in layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core data structure in `PyTorch` is tensors. Tensors are multi-dimensional
    arrays, similar to NumPy arrays, but with GPU acceleration support and capabilities
    for automatic differentiation. Tensors can be created, manipulated, and operated
    using `PyTorch` functions, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Mixing CUDA tensors with CPU-bound tensors will lead to errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`autograd` is a powerful feature in `PyTorch` that allows automatic differentiation
    for tensor operations. This is particularly useful for backpropagation (discussed
    later) in neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s review how this code works. It creates a PyTorch tensor, `x`, performs
    some operations to compute `y` and `z`, and then computes the gradient of `z`
    concerning `x` using the `backward()` method. The gradients are stored in `x.grad`.
    The operations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y = x + 2` increases each element in `x` by 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`z = y * y * 3` squares each element in `y` and then multiplies by 3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The gradient calculation, in this case, involves applying the chain rule to
    these operations to compute  dz _ dx:'
  prefs: []
  type: TYPE_NORMAL
- en: dz _ dx  =  dz _ dy  ⋅  dy _ dx
  prefs: []
  type: TYPE_NORMAL
- en: 'Ok, let’s calculate each piece in this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s compute  dz _ dy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dz _ dy  = 2 * y * 3 = 6 * y = 6 *(x + 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'dy _ dx = 1 because *y* is a linear function of *x*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, let’s compute  dz _ dx:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dz _ dx  =  dz _ dy  ⋅  dy _ dx  = 6 ⋅ (x + 2) ⋅ 1 = 6 ⋅ (x + 2)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Given `x = [[1., 2.], [3., 4.]]`, the output when we print `x.grad` should
    be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The output tensor corresponds to the evaluated gradients of `z` concerning `x`
    at the specific values of `x`.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron is the most basic unit of a neural network. It is a simple,
    linear classifier that takes a set of input values, multiplies them by their corresponding
    weights, adds a bias term, sums the results, and applies an activation function
    to produce an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – A simple perceptron
  prefs: []
  type: TYPE_NORMAL
- en: So, what’s an activation function, then?
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activation functions introduce non-linearity into neural networks and help determine
    the output of a neuron. Common activation functions include sigmoid, tanh, **Rectified
    Linear Unit** (**ReLU**), and softmax. These functions enable the network to learn
    complex, non-linear patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get into the various components of an artificial neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network typically consists of an input layer, one or more hidden (intermediate)
    layers, and an output layer. The input layer receives the raw data, while the
    hidden layers perform various transformations, and the output layer produces the
    final result.
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers in a neural network is called the **depth** of the network,
    while the number of neurons in each layer is called the **width** of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Counterintuitively, deeper and wider neural networks, though they have more
    capacity to learn complex patterns and representations in the training data, are
    not necessarily more robust to imbalanced datasets than shallower and narrower
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Deeper and wider networks are more prone to overfitting, especially in the context
    of imbalanced datasets, because large networks can memorize the patterns of the
    majority class(es), which can hamper the performance of minority class(es).
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feedforward neural network is a kind of neural network that has a unidirectional
    flow of information, starting from the input layer, progressing through any hidden
    layers, and ending at the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: There are no feedback loops or connections between layers that cycle back to
    previous layers, hence the name feedforward. These networks are widely used for
    tasks such as image classification, regression, and others.
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTorch` provides the `nn` module for creating and training neural networks
    – the `nn.Module` class is the base class for all neural network modules. The
    following code snippet defines a simple feedforward neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Loss functions** quantify the difference between the predicted output and
    target values. Common loss functions include **Mean Squared Error** (**MSE**),
    cross-entropy, and hinge loss.'
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used loss function, `CrossEntropyLoss`, which we used in the
    previous snippet, tends to favor the majority class examples in imbalanced datasets.
    This occurs because the majority class examples significantly outnumber those
    of the minority class. As a result, the loss function becomes biased toward the
    majority class and fails to account for the error in the minority classes adequately.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about several loss function modifications more suited to class
    imbalance problems in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level
    Deep* *Learning Techniques*.
  prefs: []
  type: TYPE_NORMAL
- en: MultiLayer Perceptron
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A MultiLayer Perceptron (MLP) is a feedforward neural network consisting of
    an input layer, one or more hidden layers, and an output layer. Each layer is
    fully connected, meaning every neuron in one layer is connected to all neurons
    in the next layer. MLPs can be used for various tasks, including classification,
    regression, and feature extraction. They are particularly suited for problems
    where the input data can be represented as a fixed-size vector, such as tabular
    data or flattened images. *Figure 6**.3* shows a multilayer perceptron with two
    input nodes, two hidden nodes, and one output node, using the specified weights
    w1 to w6:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – A multilayer perceptron
  prefs: []
  type: TYPE_NORMAL
- en: Training neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training neural networks involves finding the optimal weights and biases that
    minimize a loss function. Two essential algorithms are used in this process –
    gradient descent and backpropagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient descent**: Gradient descent is an optimization algorithm that minimizes
    the loss function by iteratively updating the weights and biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backpropagation**: Backpropagation is a critical algorithm for training neural
    networks. It computes the gradient of the loss function concerning each weight
    using the chain rule (a method for finding the derivative of composite functions),
    efficiently calculating the gradients from the output layer back to the input
    layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training a neural network involves iterating through a dataset, feeding the
    data into the network, computing the loss, and updating the weights using backpropagation,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the following output, which shows the loss for each epoch
    (note that the loss goes down as the training progresses):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code snippet, the line containing `loss.backward()` maps to
    the backpropagation process. The `optimizer.zero_grad()` and `optimizer.step()`
    lines both represent one step of the gradient descent process. `optimizer.zero_grad()`
    clears old gradients from the last step (otherwise, they will accumulate), while
    `optimizer.step()` performs the actual update of the parameters (weights and biases)
    in the direction that reduces the loss the most.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flowchart depicts the training logic in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – PyTorch training logic flowchart
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting** is a common problem in machine learning, where a model performs
    well on the training data but fails to generalize to unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting** happens when the model is too simple (think about when we
    used a linear regression model when we needed a decision tree regressor model)
    and does not capture the underlying patterns in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Both overfitting and underfitting lead to poor performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regularization techniques help mitigate overfitting by adding additional constraints
    the model must meet beyond merely optimizing the loss function. If L is the loss
    function, the most commonly used types of regularization are L1 and L2, which
    add a penalty term to the loss function, discouraging overly complex models:'
  prefs: []
  type: TYPE_NORMAL
- en: L1-loss = L + λ∑ i=1 n |w i|
  prefs: []
  type: TYPE_NORMAL
- en: L2-loss = L + λ∑ i=1 n w i 2
  prefs: []
  type: TYPE_NORMAL
- en: Here, λ is the regularization strength and w i are the model parameters (weights).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few other regularization techniques as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PyTorch`, the `torch.nn.Dropout` class implements dropout and takes the dropout
    rate (the probability of the neuron being zeroed) as a parameter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.nn.BatchNorm1d` and `torch.nn.BatchNorm2d`) is a technique that’s used
    to improve the training of deep neural networks. It normalizes the inputs to each
    layer by adjusting their mean and standard deviation, which helps stabilize and
    accelerate the training process, allowing the use of higher learning rates and
    reducing sensitivity to weight initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: Early stopping is a technique that’s used to prevent overfitting
    during the training of neural networks. It involves monitoring the model’s performance
    on a validation set and stopping the training process when the performance stops
    improving or starts to degrade. This helps with finding the point at which the
    model generalizes well to new data without the need to memorize the training set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Applying the early stopping technique when validation loss starts
    to increase with more epochs
  prefs: []
  type: TYPE_NORMAL
- en: The effect of the learning rate on data imbalance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen that gradient descent takes a step in the direction of the negative
    gradient – the size of that step is called the **learning rate**.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right learning rate is crucial for models operating on imbalanced
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate should be selected so that the model learns patterns from
    both majority and minority classes effectively. Monitoring training and validation
    loss and other evaluation metrics such as precision, recall, and F1-score can
    help fine-tune the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: A high learning rate means that the model’s weights are updated more drastically
    during each iteration of training. When applied to the minority class, these rapid,
    large updates can cause the model to skip over the optimal set of weights that
    minimize the loss for that class. It’s often beneficial to use techniques such
    as adaptive learning rates [3][4] or even class-specific learning rates [5] to
    ensure that the model learns effectively from both the majority and minority classes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s review some particular kinds of neural networks that have been quite
    useful for image and text domains.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing using Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**) is a deep learning model designed
    for image and video processing tasks. It consists of convolutional layers, pooling
    layers, and fully connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers apply filters to the input data, learning to detect local
    features such as edges or textures. These filters slide across the input data,
    performing element-wise multiplication and summing the results, which creates
    a feature map representing the presence of specific features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers, such as max-pooling or average pooling, reduce the spatial dimensions
    of the data, aggregating information and reducing computational complexity. These
    layers help build invariance to small translations and distortions in the input
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully connected layers process the high-level features extracted by the convolutional
    and pooling layers to make predictions. Fully connected layers are traditional
    neural network layers where each neuron is connected to every neuron in the previous
    layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs tend to overfit minority classes, which is in contrast with traditional
    machine learning algorithms, which usually underfit these minority classes [6].
    The intuition here is that CNNs, with their multiple layers and a large number
    of parameters, are designed to capture complex patterns. Additionally, being data-hungry,
    CNNs can learn intricate details from the data. When faced with imbalanced data,
    CNNs may focus excessively on the minority class, essentially “memorizing” the
    minority class instances.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss this in more detail in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*, in the *Class-Dependent Temperature
    (CDT)* *loss* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The imbalance problems in the computer vision domain can be categorized as
    shown in *Figure* *6**.6* [7]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Categorization of imbalance problems in computer vision (adapted
    from [7])
  prefs: []
  type: TYPE_NORMAL
- en: Text analysis using Natural Language Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) is another branch of AI that helps
    computers understand and analyze human language for extracting insights and organizing
    information. *Figure 6**.7* shows the categorization of imbalance problems in
    text based on data complexity levels, while *Figure 6**.8* shows the categorization
    based on some of the popular NLP application areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Categorization of imbalance problems in NLP based on the form of
    textual data
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Categorization of imbalance problems in NLP based on applications
  prefs: []
  type: TYPE_NORMAL
- en: With the basics out of the way, let’s see how data imbalance affects deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Data imbalance in deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While many classical machine learning problems that use tabular data are limited
    to binary classes and are interested in predicting the minority class, this is
    not the norm in domains where deep learning is often applied, especially computer
    vision or NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: Even benchmark datasets such as MNIST (a collection of handwritten digits containing
    grayscale images from 0 to 9) and CIFAR10 (color images with 10 different classes)
    have 10 classes to predict. So, we can say that **multi-class classification**
    is typical in problems that use deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'This data skew or imbalance can severely impact the model performance. We should
    review what we discussed about the typical kinds of imbalance in datasets in [*Chapter
    1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance in Machine
    Learning*. To simulate real-world data imbalance scenarios, two types of imbalance
    are usually investigated in the literature:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step imbalance**: All the minority classes have the same or almost the same
    number of examples, as do all the majority classes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Step imbalance
  prefs: []
  type: TYPE_NORMAL
- en: '**Long-tailed imbalance**: The number of examples across different classes
    follows an exponential decay. The plot usually has a long tail toward the left
    or the right:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Long-tailed imbalance
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, when comparing two datasets, the one with a higher imbalance ratio
    is likely to yield a lower ROC-AUC score [8]. There can be issues of class overlap,
    noisy labels, and concept complexity in imbalanced datasets [9][10]. These issues
    can significantly impact the performance of deep learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Class overlap**: Class overlap occurs when instances from different classes
    are close to each other in the feature space. Deep learning models often rely
    on complex decision boundaries to separate classes. When instances from different
    classes are close in the feature space, as is common in imbalanced datasets, the
    majority class can dominate these decision boundaries. This makes it especially
    challenging for deep learning models to accurately classify minority class instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy labels**: Noisy labels refers to instances in a dataset that have incorrect
    or ambiguous class labels. In imbalanced datasets, noisy labels can disproportionately
    affect the minority class as the model has fewer instances to learn from. Deep
    learning models are data-hungry and highly sensitive to the quality of the training
    data. This can lead to poor generalization in deep learning models, affecting
    their performance on new, unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept complexity**: Concept complexity is the inherent difficulty in distinguishing
    between classes based on the given features. Deep learning models excel at capturing
    intricate patterns in the data. However, the complexity of the relationships between
    features and class labels in imbalanced datasets can make it difficult for these
    models to effectively learn the minority class. The limited number of instances
    available for the minority class often compounds this issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of data imbalance on deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s also review how data imbalance affects deep learning models compared
    to classical machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model sensitivity**: The performance of deep learning models can be significantly
    impacted as the imbalance ratio of a dataset increases (*Figure 6**.11*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11– Model performance on an imbalanced version of CIFAR-10 with a fixed
    number of minority classes (adapted from [8])
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature engineering**: Classical machine learning models usually require
    manual feature engineering, which can help address data imbalance by creating
    new features or transforming existing ones to highlight the minority class. In
    deep learning models, feature engineering is typically performed automatically
    through the learning process, making it less reliant on human intervention to
    address the imbalance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Techniques to handle imbalance**: In classical machine learning, standard
    techniques for handling data imbalance include resampling (oversampling the minority
    class or undersampling the majority class), generating synthetic samples (for
    example, using the **Synthetic Minority Over-Sampling Technique** (**SMOTE**)),
    and using cost-sensitive learning (assigning different misclassification costs
    to different classes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some techniques from classical machine learning can also be applied in deep
    learning, but additional methods have been developed specifically for deep learning
    models. These include transfer learning (leveraging pre-trained models to learn
    from imbalanced data), using focal loss (a loss function that focuses on hard-to-classify
    examples), and employing data augmentation techniques to generate more varied
    and balanced training data. These data augmentation techniques will be discussed
    in detail in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep*
    *Learning Methods*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model interpretability**: Classical machine learning models are often more
    interpretable, which can help us understand the impact of data imbalance on the
    model’s decision-making process. Deep learning models, on the other hand, are
    often referred to as “black boxes” due to their lack of interpretability. This
    lack of interpretability can make it harder to understand how the model handles
    imbalanced data and whether it is learning meaningful patterns or simply memorizing
    the majority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training data size**: Deep learning models typically require large amounts
    of training data to achieve optimal performance. In cases of severe data imbalance,
    gathering sufficient data for the minority class may be more challenging, hindering
    the performance of deep learning models. Additionally, if a large dataset is available,
    it is more likely that instances of the minority class will be found within that
    vast amount of data. In contrast, in a smaller dataset, the minority class might
    never appear at all! On the other hand, classical machine learning algorithms
    can often achieve decent performance with smaller datasets, which is an advantage
    when dealing with imbalanced data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The impact of depth (number of layers) on deep learning models trained on
    imbalanced data problems**: The pros of more layers are the improved capacity
    to learn complex patterns and features in the data and improved generalization
    of the model. The cons of adding more layers can be the model overfitting and
    the problem of vanishing or exploding gradients worsening (as depth increases,
    the gradients during backpropagation can become very small (vanish) or very large
    (explode), making it challenging to train the model). This is summarized in *Figure
    6**.12*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Summarizing the impact of depth on deep learning models
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the influence of depth on a deep learning model varies with the data
    and model architecture requiring empirical evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling and undersampling**: The study titled *A systematic study of
    the* *class imbalance* *problem in convolutional neural networks*, by Mateusz
    Buda [8], thoroughly examined how class imbalance affects the performance of CNNs.
    The research utilized three well-known datasets – MNIST, CIFAR-10, and ImageNet
    – and employed models such as LeNet-5, All-CNN, and ResNet-10 for the experiments.
    Their key findings were as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In almost all analyzed cases, oversampling proved to be the most effective technique
    for mitigating class imbalance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling was more effective when the imbalance was eliminated while undersampling
    yielded better results when only reducing the imbalance partially
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrary to some traditional machine learning algorithms, CNNs did not overfit
    when oversampling was applied
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, undersampling performs on par with oversampling, even when using
    less data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Undersampling generally showed poor performance compared to the baseline and
    never showed a notable advantage over oversampling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the focus is on correctly classifying examples from minority classes, undersampling
    may be preferable to oversampling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If there’s too much data, undersampling might be the preferred or only option
    to save time, resources, and the cost of training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Threshold adjustment**: We discussed threshold adjustment in detail in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*. As a refresher,
    a decision threshold is a value that determines the boundary between different
    classes or outcomes in a classification problem. The paper by Johnson et al. [11]
    emphasized that the optimal threshold is linearly correlated with the minority
    class size (that is, the lower the minority class size, the lower the threshold).
    They trained and tested for fraud detection using deep learning models with two
    and four hidden layers on CMS Medicare data [12]. The default threshold often
    leads to poor classification, especially for the minority class, highlighting
    the need for threshold adjustment based on a validation set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A note about the datasets used in the second half of this book
  prefs: []
  type: TYPE_NORMAL
- en: 'In the remainder of this book, we will primarily use imbalanced versions of
    the MNIST and CIFAR10-LT (“LT” stands for “long-tailed”) datasets for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '• **MNIST**: A small grayscale image dataset with 10 classes containing digits
    from 0 to 9\. It consists of 60,000 training images and 10,000 test images, each
    28 pixels x 28 pixels. It’s faster to train/test compared to CIFAR-10.'
  prefs: []
  type: TYPE_NORMAL
- en: '• **CIFAR-10**: Used for object recognition, this color image dataset also
    has 10 classes. It includes 50,000 training images and 10,000 test images, each
    32 pixels x 32 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the training sets are imbalanced, the corresponding test sets have an
    equal number of examples in each class. This balanced test set approach provides
    several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Comparability**: Balanced test sets allow unbiased comparison across various
    classes and models'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Repeatability and reproducibility**: Using simple datasets such as MNIST
    and CIFAR-10 ensures ease of code execution and understanding'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Efficiency**: Smaller datasets enable quicker iterations, allowing us to
    try, test, and retry running the code in a reasonable timeframe'
  prefs: []
  type: TYPE_NORMAL
- en: '• **Alignment with research**: This approach is consistent with most research
    studies on long-tailed learning and imbalanced datasets, providing a common framework
    for comparison'
  prefs: []
  type: TYPE_NORMAL
- en: The next section will give us an overview of deep learning strategies for managing
    data imbalance. We will also see how various techniques for handling imbalanced
    datasets that were initially developed for classical machine learning techniques
    can be easily extended to deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of deep learning techniques to handle data imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Much like the first half of this book, where we focused on classical machine
    learning techniques, the major categories typically include sampling techniques,
    cost-sensitive techniques, threshold adjustment techniques, or a combination of
    these:'
  prefs: []
  type: TYPE_NORMAL
- en: The sampling techniques comprise either undersampling the majority class or
    oversampling the minority class data. Data augmentation is a fundamental technique
    in computer vision problems that’s used to increase the diversity of the training
    set. While not directly an oversampling method aimed at addressing class imbalance,
    data augmentation does have the effect of expanding the training data. We will
    discuss these techniques in more detail in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep* *Learning Methods*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive techniques usually involve changing the model loss function in
    some way to accommodate the higher cost of misclassifying the minority class examples.
    Some standard loss functions, such as `CrossEntropyLoss` in PyTorch, support the
    weight parameter to accommodate such costs. We will cover many of those, including
    several custom loss functions, in detail in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep* *Learning Techniques*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid deep learning techniques integrate the data-level and algorithm-level
    approaches. This fusion allows for more nuanced and effective solutions to tackle
    class imbalance. We’ll discuss this in [*Chapter 9*](B17259_09.xhtml#_idTextAnchor256),
    *Hybrid Deep* *Learning Methods*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Threshold adjustment techniques are applied to the scores produced from the
    model after the model has been trained. These techniques can help adjust the threshold
    so that the model metric, say, F1-score or geometric mean, gets optimized. This
    was discussed in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151)*,* *Cost-Sensitive
    Learning*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Categorization of deep learning techniques covered
  prefs: []
  type: TYPE_NORMAL
- en: We will not use deep learning methods for tabular data from this chapter onwards
    because classical models such as XGBoost, LightGBM, and CatBoost tend to perform
    well on such structured data. Several studies ([13] and [14]) have shown that
    traditional machine learning models often outperform deep learning models in supervised
    learning tasks involving tabular data. However, an ensemble of an XGBoost model
    and a deep learning model can outperform the XGBoost model alone [13]. Therefore,
    tasks using tabular datasets can still benefit from deep learning models. It’s
    likely only a matter of time before deep learning models catch up to the performance
    of classical models on tabular data. Nevertheless, we will focus our implementation
    and examples on vision and NLP problems when using deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our high-level discussion of various techniques for dealing with
    imbalanced datasets when using deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-label classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multi-label classification is a classification task where each instance can
    be assigned to multiple classes or labels simultaneously. In other words, an instance
    can belong to more than one category or have multiple attributes. For example,
    a movie can belong to multiple genres, such as action, comedy, and romance. Similarly,
    an image can have multiple objects in it (*Figure 6**.14*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Multi-label image classification with prediction probabilities
    shown
  prefs: []
  type: TYPE_NORMAL
- en: But how is it different from multi-class classification? Multi-class classification
    is a classification task where each instance can be assigned to only one class
    or label. In this case, the classes or categories are mutually exclusive, meaning
    an instance can belong to just one category. For example, a handwritten digit
    recognition task would be multi-class since each digit can belong to only one
    class (0-9).
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the main difference between multi-label and multi-class classification
    is that in multi-label classification, instances can have multiple labels. In
    contrast, in multi-class classification, instances can have only one label. This
    is summarized in *Figure 6**.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Distinction between multi-label and multi-class classification
  prefs: []
  type: TYPE_NORMAL
- en: Many real-world problems are inherently multi-labeled and experience class imbalance.
    Deep learning models are particularly useful here, especially when the data involved
    is unstructured, such as images, videos, or text.
  prefs: []
  type: TYPE_NORMAL
- en: In **Multi-Label Datasets** (**MLDs**), there can be tens or hundreds of labels,
    and each instance can be associated with a subset of those labels. The more different
    labels exist, the more possibilities there are that some have a very low presence,
    leading to a significant imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data imbalance in multi-label classification can occur at many levels [15]:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imbalance within labels**: A large disparity between negative and positive
    instances in each label can happen, causing classification models to struggle
    with minority classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalance between labels**: Unequal distribution of positive instances among
    labels, leading to poor performance for minority classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalance among label sets**: Sparse frequency of label sets (a combination
    of various labels) due to label sparseness, making it challenging for classification
    models to learn effectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6**.16* sums up these kinds of imbalances when classifying multi-label
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Kinds of imbalances in multi-label datasets
  prefs: []
  type: TYPE_NORMAL
- en: The imbalance handling approaches and metrics are similar to those used for
    imbalanced multi-class classification methods. Typical approaches include resampling
    methods, ensemble approaches, and cost-sensitive methods.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward approach is to convert the multi-label dataset into
    the multi-class dataset and then apply various re-sampling methods such as random
    oversampling, random undersampling, **Edited Nearest Neighbors** (**ENN**), and
    others. Similar strategies have been used in the literature to adapt the cost-sensitive
    approaches to fit multi-label classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has become essential in many fields, from computer vision and
    natural language processing to healthcare and finance. This chapter has provided
    a brief introduction to the core concepts and techniques in deep learning. We
    talked about PyTorch, the fundamentals of deep learning, activation functions,
    and data imbalance challenges. We also got a bird’s-eye view of the various techniques
    we will discuss in the following few chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these fundamentals will equip you with the knowledge necessary
    to explore more advanced topics and applications and ultimately contribute to
    the ever-evolving world of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at data-level deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are some challenges in porting data imbalance handling methods from classical
    machine learning models to deep learning models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could an imbalanced version of the MNIST dataset be created?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the MNIST dataset to train a CNN model with varying degrees of imbalance
    in the data. Record the model’s overall accuracy on a fixed test set. Plot how
    the overall accuracy changes as the imbalance in the training data increases.
    Observe whether the overall accuracy declines as the training data becomes more
    imbalanced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of using random oversampling with deep learning models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the data augmentation techniques that can be applied when dealing
    with limited or imbalanced data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does undersampling work in handling data imbalance, and what are its limitations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it important to ensure that the data augmentation techniques preserve
    the original labels of the dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A. W. Trask, Grokking Deep Learning (Manning, Shelter Island, NY, 2019).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. Chollet, *Deep Learning with Python*. Manning Publications, 2021.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “*Class-Balanced Loss Based
    on Effective Number of Samples*,” p. 10.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, *Learning Imbalanced Datasets
    with Label- Distribution-Aware Margin Loss* [Online]. Available at [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. Jantanasukon and A. Thammano, *Adaptive Learning Rate for Dealing with Imbalanced
    Data in Classification Problems*. In 2021 Joint International Conference on Digital
    Arts, Media and Technology with ECTI Northern Section Conference on Electrical,
    Electronics, Computer and Telecommunication Engineering, Cha-am, Thailand: IEEE,
    Mar. 2021, pp. 229–232, doi: 10.1109/ECTIDAMTNCON51128.2021.9425715.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, *Identifying and Compensating
    for Feature Deviation in Imbalanced Deep Learning*. arXiv, Jul. 10, 2022\. Accessed:
    Dec. 14, 2022\. [Online]. Available at [http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'V. Sampath, I. Maurtua, J. J. Aguilar Martín, and A. Gutierrez, *A survey on
    generative adversarial networks for imbalance problems in computer vision tasks*.
    J Big Data, vol. 8, no. 1, p. 27, Dec. 2021, doi: 10.1186/s40537-021-00414-0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Buda, A. Maki, and M. A. Mazurowski, *A systematic study of the class imbalance
    problem in convolutional neural networks*. Neural Networks, vol. 106, pp. 249–259,
    Oct. 2018, doi: 10.1016/j.neunet.2018.07.011.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, *On the
    combined effect of class imbalance and concept complexity in deep learning*. arXiv,
    Jul. 29, 2021\. Accessed: Mar. 28, 2023\. [Online]. Available at [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, *On the
    combined effect of class imbalance and concept complexity in deep learning*. arXiv,
    Jul. 29, 2021\. Accessed: Mar. 28, 2023\. [Online]. Available at [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Medicare fraud detection using neural
    networks*. J Big Data, vol. 6, no. 1, p. 63, Dec. 2019, doi: 10.1186/s40537-019-0225-0.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Medicare fraud & abuse: prevention, detection, and reporting*. Centers for
    Medicare & Medicaid Services. 2017\. [https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244](https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. Shwartz-Ziv and A. Armon, *Tabular Data: Deep Learning is Not All You Need*.
    arXiv, Nov. 23, 2021\. Accessed: Apr. 10, 2023\. [Online]. Available at [http://arxiv.org/abs/2106.03253](http://arxiv.org/abs/2106.03253).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, and G. Kasneci,
    *Deep Neural Networks and Tabular Data: A Survey*. IEEE Trans. Neural Netw. Learning
    Syst., pp. 1–21, 2022, doi: 10.1109/TNNLS.2022.3229161.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. N. Tarekegn, M. Giacobini, and K. Michalak, *A review of methods for imbalanced
    multi-label classification*. Pattern Recognition, vol. 118, p. 107965, Oct. 2021,
    doi: 10.1016/j. patcog.2021.107965.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
