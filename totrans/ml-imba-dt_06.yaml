- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Data Imbalance in Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的数据不平衡
- en: Class imbalanced data is a common issue for deep learning models. When one or
    more classes have significantly fewer samples, the performance of deep learning
    models can suffer as they tend to prioritize learning from the majority class,
    resulting in poor generalization for the minority class(es).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 类不平衡数据是深度学习模型的一个常见问题。当一个或多个类别样本显著较少时，深度学习模型的性能可能会受到影响，因为它们倾向于优先从多数类别中学习，从而导致少数类别（或类别组）的泛化能力较差。
- en: 'A lot of real-world data is imbalanced, which presents challenges to deep learning
    classification tasks. *Figure 6**.1* shows some common categories of imbalanced
    data problems in various deep learning applications:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的数据是不平衡的，这对深度学习分类任务提出了挑战。*图6.1*展示了在各种深度学习应用中常见的不平衡数据问题的一些类别：
- en: '![](img/B17259_06_01.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_01.jpg)'
- en: Figure 6.1 – Some common categories of imbalanced data problems
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 一些常见的不平衡数据问题类别
- en: 'We will cover the following topics in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: A brief introduction to deep learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Data imbalance in deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中的数据不平衡
- en: Overview of deep learning techniques to handle data imbalance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据不平衡的深度学习技术概述
- en: Multi-label classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签分类
- en: By the end of this chapter, we’ll have a foundational understanding of deep
    learning and neural networks. We’ll have also grasped the impact of data imbalance
    on these models and gained a high-level overview of various strategies to address
    the challenges of imbalanced data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将对深度学习和神经网络有一个基础的理解。我们还将掌握数据不平衡对这些模型的影响，并了解解决不平衡数据挑战的各种策略的高级概述。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `PyTorch`. `PyTorch` is an open source machine learning library that’s used
    for deep learning tasks and has grown in popularity recently because of its flexibility
    and ease of use.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用常见的库，如`numpy`、`scikit-learn`和`PyTorch`。`PyTorch`是一个开源的机器学习库，用于深度学习任务，因其灵活性和易用性而近年来越来越受欢迎。
- en: You can install `PyTorch` using `pip` or `conda`. Visit the official PyTorch
    website ([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/))
    to get the appropriate command for your system configuration.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`pip`或`conda`安装`PyTorch`。访问官方PyTorch网站([https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/))以获取适合您系统配置的相应命令。
- en: The code and notebooks for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter06)。
- en: A brief introduction to deep learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Deep learning is a subfield of machine learning that focuses on artificial neural
    networks with multiple layers (deep models typically have three or more layers,
    including input, output, and hidden layers). These models have demonstrated remarkable
    capabilities in various applications, including image and speech recognition,
    natural language processing, and autonomous driving.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子领域，它专注于具有多层（深度模型通常有三层或更多，包括输入、输出和隐藏层）的人工神经网络。这些模型在各种应用中展示了非凡的能力，包括图像和语音识别、自然语言处理和自动驾驶。
- en: The prevalence of “big data” (large volumes of structured or unstructured data,
    often challenging to manage with traditional data processing software) problems
    greatly benefited from the development of **Graphical Processing Units** (**GPUs**),
    which were initially designed for graphics processing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: “大数据”（大量结构化或非结构化数据，通常难以用传统的数据处理软件管理）问题的普遍存在，极大地得益于**图形处理单元**（**GPU**）的发展，这些GPU最初是为图形处理设计的。
- en: In this section, we will provide a concise introduction to the foundational
    elements of deep learning, discussing only what is necessary for the problems
    associated with data imbalance in deep learning. For a more in-depth introduction,
    we recommend referring to a more dedicated book on deep learning (please refer
    to the resources listed as [1] and [2] in the *References* section).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are the foundation of deep learning. Inspired by the structure
    and function of the human brain, neural networks consist of interconnected nodes
    or artificial neurons organized in layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'The core data structure in `PyTorch` is tensors. Tensors are multi-dimensional
    arrays, similar to NumPy arrays, but with GPU acceleration support and capabilities
    for automatic differentiation. Tensors can be created, manipulated, and operated
    using `PyTorch` functions, as shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Mixing CUDA tensors with CPU-bound tensors will lead to errors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here’s the output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`autograd` is a powerful feature in `PyTorch` that allows automatic differentiation
    for tensor operations. This is particularly useful for backpropagation (discussed
    later) in neural networks:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will see the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s review how this code works. It creates a PyTorch tensor, `x`, performs
    some operations to compute `y` and `z`, and then computes the gradient of `z`
    concerning `x` using the `backward()` method. The gradients are stored in `x.grad`.
    The operations are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '`y = x + 2` increases each element in `x` by 2'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`z = y * y * 3` squares each element in `y` and then multiplies by 3'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The gradient calculation, in this case, involves applying the chain rule to
    these operations to compute  dz _ dx:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: dz _ dx  =  dz _ dy  ⋅  dy _ dx
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Ok, let’s calculate each piece in this equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s compute  dz _ dy:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dz _ dy  = 2 * y * 3 = 6 * y = 6 *(x + 2)
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'dy _ dx = 1 because *y* is a linear function of *x*:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, let’s compute  dz _ dx:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: dz _ dx  =  dz _ dy  ⋅  dy _ dx  = 6 ⋅ (x + 2) ⋅ 1 = 6 ⋅ (x + 2)
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Given `x = [[1., 2.], [3., 4.]]`, the output when we print `x.grad` should
    be as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output tensor corresponds to the evaluated gradients of `z` concerning `x`
    at the specific values of `x`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The perceptron is the most basic unit of a neural network. It is a simple,
    linear classifier that takes a set of input values, multiplies them by their corresponding
    weights, adds a bias term, sums the results, and applies an activation function
    to produce an output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – A simple perceptron
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: So, what’s an activation function, then?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activation functions introduce non-linearity into neural networks and help determine
    the output of a neuron. Common activation functions include sigmoid, tanh, **Rectified
    Linear Unit** (**ReLU**), and softmax. These functions enable the network to learn
    complex, non-linear patterns in the data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get into the various components of an artificial neural network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Layers
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network typically consists of an input layer, one or more hidden (intermediate)
    layers, and an output layer. The input layer receives the raw data, while the
    hidden layers perform various transformations, and the output layer produces the
    final result.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络通常由输入层、一个或多个隐藏（中间）层和输出层组成。输入层接收原始数据，而隐藏层执行各种转换，输出层产生最终结果。
- en: The number of layers in a neural network is called the **depth** of the network,
    while the number of neurons in each layer is called the **width** of the network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的层数称为网络的**深度**，而每层的神经元数量称为网络的**宽度**。
- en: Counterintuitively, deeper and wider neural networks, though they have more
    capacity to learn complex patterns and representations in the training data, are
    not necessarily more robust to imbalanced datasets than shallower and narrower
    networks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与直觉相反，尽管深度和宽度更大的神经网络在训练数据中具有学习更复杂模式和表示的更多能力，但它们并不一定比浅层和窄网络对不平衡数据集更鲁棒。
- en: Deeper and wider networks are more prone to overfitting, especially in the context
    of imbalanced datasets, because large networks can memorize the patterns of the
    majority class(es), which can hamper the performance of minority class(es).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 深度和宽度更大的网络更容易过拟合，尤其是在不平衡数据集的背景下，因为大网络可以记住多数类别的模式，这可能会妨碍少数类别的性能。
- en: Feedforward neural networks
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: A feedforward neural network is a kind of neural network that has a unidirectional
    flow of information, starting from the input layer, progressing through any hidden
    layers, and ending at the output layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是一种具有单向信息流的神经网络，从输入层开始，通过任何隐藏层，最终到达输出层。
- en: There are no feedback loops or connections between layers that cycle back to
    previous layers, hence the name feedforward. These networks are widely used for
    tasks such as image classification, regression, and others.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有反馈回路或层与层之间的循环连接，因此得名前馈。这些网络广泛用于图像分类、回归和其他任务。
- en: '`PyTorch` provides the `nn` module for creating and training neural networks
    – the `nn.Module` class is the base class for all neural network modules. The
    following code snippet defines a simple feedforward neural network:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch`提供了`nn`模块用于创建和训练神经网络——`nn.Module`类是所有神经网络模块的基类。以下代码片段定义了一个简单的前馈神经网络：'
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Loss functions** quantify the difference between the predicted output and
    target values. Common loss functions include **Mean Squared Error** (**MSE**),
    cross-entropy, and hinge loss.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数**量化了预测输出和目标值之间的差异。常见的损失函数包括**均方误差**（**MSE**）、交叉熵和Hinge损失。'
- en: The most commonly used loss function, `CrossEntropyLoss`, which we used in the
    previous snippet, tends to favor the majority class examples in imbalanced datasets.
    This occurs because the majority class examples significantly outnumber those
    of the minority class. As a result, the loss function becomes biased toward the
    majority class and fails to account for the error in the minority classes adequately.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的损失函数`CrossEntropyLoss`，我们在前面的代码片段中使用过，在类别不平衡的数据集中往往倾向于多数类别的示例。这是因为多数类别的示例数量显著多于少数类别的示例。因此，损失函数偏向多数类别，并且未能充分考虑到少数类别的错误。
- en: We will learn about several loss function modifications more suited to class
    imbalance problems in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level
    Deep* *Learning Techniques*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第8章*](B17259_08.xhtml#_idTextAnchor235)“算法级深度学习技术”中学习更多关于针对类别不平衡问题更适合的损失函数修改。
- en: MultiLayer Perceptron
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A MultiLayer Perceptron (MLP) is a feedforward neural network consisting of
    an input layer, one or more hidden layers, and an output layer. Each layer is
    fully connected, meaning every neuron in one layer is connected to all neurons
    in the next layer. MLPs can be used for various tasks, including classification,
    regression, and feature extraction. They are particularly suited for problems
    where the input data can be represented as a fixed-size vector, such as tabular
    data or flattened images. *Figure 6**.3* shows a multilayer perceptron with two
    input nodes, two hidden nodes, and one output node, using the specified weights
    w1 to w6:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）是一个前馈神经网络，由输入层、一个或多个隐藏层和输出层组成。每一层都是全连接的，这意味着一个层中的每个神经元都与下一层中的所有神经元相连。MLPs可用于各种任务，包括分类、回归和特征提取。它们特别适合于输入数据可以表示为固定大小向量的问题，例如表格数据或展平的图像。*图6.3*展示了具有两个输入节点、两个隐藏节点和一个输出节点的多层感知器，使用指定的权重w1到w6：
- en: '![](img/B17259_06_03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_03.jpg)'
- en: Figure 6.3 – A multilayer perceptron
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 多层感知器
- en: Training neural networks
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Training neural networks involves finding the optimal weights and biases that
    minimize a loss function. Two essential algorithms are used in this process –
    gradient descent and backpropagation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及找到最小化损失函数的最优权重和偏置。在此过程中使用了两个基本算法 – 梯度下降和反向传播：
- en: '**Gradient descent**: Gradient descent is an optimization algorithm that minimizes
    the loss function by iteratively updating the weights and biases.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降**：梯度下降是一种优化算法，通过迭代更新权重和偏置来最小化损失函数。'
- en: '**Backpropagation**: Backpropagation is a critical algorithm for training neural
    networks. It computes the gradient of the loss function concerning each weight
    using the chain rule (a method for finding the derivative of composite functions),
    efficiently calculating the gradients from the output layer back to the input
    layer.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：反向传播是训练神经网络的关键算法。它使用链式法则（一种寻找复合函数导数的方法）计算损失函数相对于每个权重的梯度，有效地从输出层计算梯度到输入层。'
- en: 'Training a neural network involves iterating through a dataset, feeding the
    data into the network, computing the loss, and updating the weights using backpropagation,
    as shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络涉及遍历数据集，将数据输入网络，计算损失，并使用反向传播更新权重，如图所示：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code produces the following output, which shows the loss for each epoch
    (note that the loss goes down as the training progresses):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成以下输出，显示了每个epoch的损失（注意，随着训练的进行，损失会下降）：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the previous code snippet, the line containing `loss.backward()` maps to
    the backpropagation process. The `optimizer.zero_grad()` and `optimizer.step()`
    lines both represent one step of the gradient descent process. `optimizer.zero_grad()`
    clears old gradients from the last step (otherwise, they will accumulate), while
    `optimizer.step()` performs the actual update of the parameters (weights and biases)
    in the direction that reduces the loss the most.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，包含`loss.backward()`的行对应于反向传播过程。`optimizer.zero_grad()`和`optimizer.step()`这两行都代表梯度下降过程的一步。`optimizer.zero_grad()`清除上一步的旧梯度（否则它们会累积），而`optimizer.step()`执行参数（权重和偏置）的实际更新，以减少损失。
- en: 'The following flowchart depicts the training logic in PyTorch:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下流程图描述了PyTorch中的训练逻辑：
- en: '![](img/B17259_06_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_04.jpg)'
- en: Figure 6.4 – PyTorch training logic flowchart
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – PyTorch训练逻辑流程图
- en: '**Overfitting** is a common problem in machine learning, where a model performs
    well on the training data but fails to generalize to unseen data.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**是机器学习中常见的问题，模型在训练数据上表现良好，但无法泛化到未见数据。'
- en: '**Underfitting** happens when the model is too simple (think about when we
    used a linear regression model when we needed a decision tree regressor model)
    and does not capture the underlying patterns in the data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**欠拟合**发生在模型过于简单时（想想当我们需要决策树回归模型时却使用了线性回归模型的情况）并且无法捕捉数据中的潜在模式。'
- en: Both overfitting and underfitting lead to poor performance on unseen data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合都会导致在未见数据上的性能不佳。
- en: Regularization
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正则化
- en: 'Regularization techniques help mitigate overfitting by adding additional constraints
    the model must meet beyond merely optimizing the loss function. If L is the loss
    function, the most commonly used types of regularization are L1 and L2, which
    add a penalty term to the loss function, discouraging overly complex models:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化技术通过为模型必须满足的约束条件添加额外的限制来帮助减轻过拟合。如果L是损失函数，最常用的正则化类型是L1和L2，它们将惩罚项添加到损失函数中，从而阻止过于复杂的模型：
- en: L1-loss = L + λ∑ i=1 n |w i|
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: L1损失 = L + λ∑i=1n|w_i|
- en: L2-loss = L + λ∑ i=1 n w i 2
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: L2损失 = L + λ∑i=1nw_i^2
- en: Here, λ is the regularization strength and w i are the model parameters (weights).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，λ是正则化强度，w_i是模型参数（权重）。
- en: 'There are a few other regularization techniques as well:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种正则化技术：
- en: '`PyTorch`, the `torch.nn.Dropout` class implements dropout and takes the dropout
    rate (the probability of the neuron being zeroed) as a parameter.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PyTorch`中的`torch.nn.Dropout`类实现了dropout功能，并接受一个参数，即dropout率（神经元被置零的概率）。'
- en: '`torch.nn.BatchNorm1d` and `torch.nn.BatchNorm2d`) is a technique that’s used
    to improve the training of deep neural networks. It normalizes the inputs to each
    layer by adjusting their mean and standard deviation, which helps stabilize and
    accelerate the training process, allowing the use of higher learning rates and
    reducing sensitivity to weight initialization.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.BatchNorm1d`和`torch.nn.BatchNorm2d`是一种用于改进深度神经网络训练的技术。它通过调整每个层的均值和标准差来规范化每个层的输入，这有助于稳定和加速训练过程，允许使用更高的学习率并减少对权重初始化的敏感性。'
- en: '**Early stopping**: Early stopping is a technique that’s used to prevent overfitting
    during the training of neural networks. It involves monitoring the model’s performance
    on a validation set and stopping the training process when the performance stops
    improving or starts to degrade. This helps with finding the point at which the
    model generalizes well to new data without the need to memorize the training set:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早期停止**：早期停止是一种在神经网络训练过程中防止过拟合的技术。它涉及监控模型在验证集上的性能，并在性能停止提高或开始下降时停止训练过程。这有助于找到模型在没有需要记住训练集的情况下，对新数据泛化良好的点：'
- en: '![](img/B17259_06_05.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_05.jpg)'
- en: Figure 6.5 – Applying the early stopping technique when validation loss starts
    to increase with more epochs
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 当验证损失随着更多轮次增加时应用早期停止技术
- en: The effect of the learning rate on data imbalance
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率对数据不平衡的影响
- en: We have seen that gradient descent takes a step in the direction of the negative
    gradient – the size of that step is called the **learning rate**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，梯度下降在负梯度的方向上迈出一步——这一步的大小称为**学习率**。
- en: Choosing the right learning rate is crucial for models operating on imbalanced
    datasets.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的学习率对于在数据不平衡的数据集上运行的模型至关重要。
- en: The learning rate should be selected so that the model learns patterns from
    both majority and minority classes effectively. Monitoring training and validation
    loss and other evaluation metrics such as precision, recall, and F1-score can
    help fine-tune the learning rate.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率的选择应确保模型能够有效地从多数类和少数类中学习模式。监控训练和验证损失以及其他评估指标，如精确度、召回率和F1分数，可以帮助微调学习率。
- en: A high learning rate means that the model’s weights are updated more drastically
    during each iteration of training. When applied to the minority class, these rapid,
    large updates can cause the model to skip over the optimal set of weights that
    minimize the loss for that class. It’s often beneficial to use techniques such
    as adaptive learning rates [3][4] or even class-specific learning rates [5] to
    ensure that the model learns effectively from both the majority and minority classes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 高学习率意味着模型在每次训练迭代中更新权重更为剧烈。当应用于少数类时，这些快速、大的更新可能导致模型跳过最小化该类损失的优化权重集。通常，使用自适应学习率[3][4]或甚至类特定学习率[5]来确保模型从多数类和少数类中有效地学习是有益的。
- en: Now, let’s review some particular kinds of neural networks that have been quite
    useful for image and text domains.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一些对图像和文本领域非常有用的特定类型的神经网络。
- en: Image processing using Convolutional Neural Networks
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行图像处理
- en: '**Convolutional Neural Networks** (**CNNs**) is a deep learning model designed
    for image and video processing tasks. It consists of convolutional layers, pooling
    layers, and fully connected layers:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是一种为图像和视频处理任务设计的深度学习模型。它由卷积层、池化层和全连接层组成：'
- en: Convolutional layers apply filters to the input data, learning to detect local
    features such as edges or textures. These filters slide across the input data,
    performing element-wise multiplication and summing the results, which creates
    a feature map representing the presence of specific features.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层将滤波器应用于输入数据，学习检测局部特征，如边缘或纹理。这些滤波器在输入数据上滑动，执行逐元素乘法和求和结果，从而创建一个特征图，表示特定特征的存在。
- en: Pooling layers, such as max-pooling or average pooling, reduce the spatial dimensions
    of the data, aggregating information and reducing computational complexity. These
    layers help build invariance to small translations and distortions in the input
    data.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层，如最大池化或平均池化，减少了数据的空间维度，聚合信息并降低计算复杂度。这些层有助于构建对输入数据中小的平移和扭曲的不变性。
- en: Fully connected layers process the high-level features extracted by the convolutional
    and pooling layers to make predictions. Fully connected layers are traditional
    neural network layers where each neuron is connected to every neuron in the previous
    layer.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层处理卷积层和池化层提取的高级特征，以进行预测。完全连接层是传统的神经网络层，其中每个神经元都与前一层的每个神经元相连。
- en: CNNs tend to overfit minority classes, which is in contrast with traditional
    machine learning algorithms, which usually underfit these minority classes [6].
    The intuition here is that CNNs, with their multiple layers and a large number
    of parameters, are designed to capture complex patterns. Additionally, being data-hungry,
    CNNs can learn intricate details from the data. When faced with imbalanced data,
    CNNs may focus excessively on the minority class, essentially “memorizing” the
    minority class instances.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）倾向于过度拟合少数类别，这与通常欠拟合这些少数类别的传统机器学习算法形成对比[6]。这里的直觉是，CNNs具有多层和大量参数，旨在捕捉复杂模式。此外，由于对数据有很强的需求，CNNs可以从数据中学习到复杂的细节。当面对不平衡数据时，CNNs可能会过度关注少数类别，本质上“记忆”了少数类别的实例。
- en: We will discuss this in more detail in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep Learning Techniques*, in the *Class-Dependent Temperature
    (CDT)* *loss* section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第8章*（B17259_08.xhtml#_idTextAnchor235）的*算法级深度学习技术*部分，*类依赖温度（CDT）*损失部分中更详细地讨论这个问题。
- en: 'The imbalance problems in the computer vision domain can be categorized as
    shown in *Figure* *6**.6* [7]:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉领域中的不平衡问题可以按照*图6.6*所示进行分类[7]：
- en: '![](img/B17259_06_06.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_06.jpg)'
- en: Figure 6.6 – Categorization of imbalance problems in computer vision (adapted
    from [7])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 计算机视觉中不平衡问题的分类（改编自[7]）
- en: Text analysis using Natural Language Processing
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自然语言处理进行文本分析
- en: '**Natural Language Processing** (**NLP**) is another branch of AI that helps
    computers understand and analyze human language for extracting insights and organizing
    information. *Figure 6**.7* shows the categorization of imbalance problems in
    text based on data complexity levels, while *Figure 6**.8* shows the categorization
    based on some of the popular NLP application areas:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是人工智能的另一个分支，它帮助计算机理解和分析人类语言，以提取见解和组织信息。*图6.7*展示了基于数据复杂度级别的文本不平衡问题的分类，而*图6.8*展示了基于一些流行的NLP应用领域的分类：'
- en: '![](img/B17259_06_07.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_07.jpg)'
- en: Figure 6.7 – Categorization of imbalance problems in NLP based on the form of
    textual data
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 基于文本数据形式的NLP中不平衡问题的分类
- en: '![](img/B17259_06_08.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_06_08.jpg)'
- en: Figure 6.8 – Categorization of imbalance problems in NLP based on applications
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 基于应用的NLP中不平衡问题的分类
- en: With the basics out of the way, let’s see how data imbalance affects deep learning
    models.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本概念介绍完毕后，让我们看看数据不平衡如何影响深度学习模型。
- en: Data imbalance in deep learning
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的数据不平衡
- en: While many classical machine learning problems that use tabular data are limited
    to binary classes and are interested in predicting the minority class, this is
    not the norm in domains where deep learning is often applied, especially computer
    vision or NLP problems.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多使用表格数据的经典机器学习问题仅限于二进制类别，并且关注于预测少数类别，但在深度学习经常应用的领域中，这并不是常态，尤其是在计算机视觉或NLP问题中。
- en: Even benchmark datasets such as MNIST (a collection of handwritten digits containing
    grayscale images from 0 to 9) and CIFAR10 (color images with 10 different classes)
    have 10 classes to predict. So, we can say that **multi-class classification**
    is typical in problems that use deep learning models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'This data skew or imbalance can severely impact the model performance. We should
    review what we discussed about the typical kinds of imbalance in datasets in [*Chapter
    1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance in Machine
    Learning*. To simulate real-world data imbalance scenarios, two types of imbalance
    are usually investigated in the literature:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Step imbalance**: All the minority classes have the same or almost the same
    number of examples, as do all the majority classes:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_09.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Step imbalance
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Long-tailed imbalance**: The number of examples across different classes
    follows an exponential decay. The plot usually has a long tail toward the left
    or the right:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_10.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Long-tailed imbalance
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, when comparing two datasets, the one with a higher imbalance ratio
    is likely to yield a lower ROC-AUC score [8]. There can be issues of class overlap,
    noisy labels, and concept complexity in imbalanced datasets [9][10]. These issues
    can significantly impact the performance of deep learning models:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**Class overlap**: Class overlap occurs when instances from different classes
    are close to each other in the feature space. Deep learning models often rely
    on complex decision boundaries to separate classes. When instances from different
    classes are close in the feature space, as is common in imbalanced datasets, the
    majority class can dominate these decision boundaries. This makes it especially
    challenging for deep learning models to accurately classify minority class instances.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy labels**: Noisy labels refers to instances in a dataset that have incorrect
    or ambiguous class labels. In imbalanced datasets, noisy labels can disproportionately
    affect the minority class as the model has fewer instances to learn from. Deep
    learning models are data-hungry and highly sensitive to the quality of the training
    data. This can lead to poor generalization in deep learning models, affecting
    their performance on new, unseen data.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept complexity**: Concept complexity is the inherent difficulty in distinguishing
    between classes based on the given features. Deep learning models excel at capturing
    intricate patterns in the data. However, the complexity of the relationships between
    features and class labels in imbalanced datasets can make it difficult for these
    models to effectively learn the minority class. The limited number of instances
    available for the minority class often compounds this issue.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of data imbalance on deep learning models
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s also review how data imbalance affects deep learning models compared
    to classical machine learning models:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**Model sensitivity**: The performance of deep learning models can be significantly
    impacted as the imbalance ratio of a dataset increases (*Figure 6**.11*):'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型敏感性**：随着数据集不平衡率的增加，深度学习模型的表现可能会受到显著影响（*图6**.11*）：'
- en: '![](img/B17259_06_11.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_06_11.jpg)'
- en: Figure 6.11– Model performance on an imbalanced version of CIFAR-10 with a fixed
    number of minority classes (adapted from [8])
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11–在具有固定少数类数量的不平衡版本的CIFAR-10上的模型性能（改编自[8]）
- en: '**Feature engineering**: Classical machine learning models usually require
    manual feature engineering, which can help address data imbalance by creating
    new features or transforming existing ones to highlight the minority class. In
    deep learning models, feature engineering is typically performed automatically
    through the learning process, making it less reliant on human intervention to
    address the imbalance.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：经典机器学习模型通常需要手动特征工程，这有助于通过创建新特征或转换现有特征来突出显示少数类，从而解决数据不平衡问题。在深度学习模型中，特征工程通常通过学习过程自动执行，这使得它对人类干预来解决不平衡问题不那么依赖。'
- en: '**Techniques to handle imbalance**: In classical machine learning, standard
    techniques for handling data imbalance include resampling (oversampling the minority
    class or undersampling the majority class), generating synthetic samples (for
    example, using the **Synthetic Minority Over-Sampling Technique** (**SMOTE**)),
    and using cost-sensitive learning (assigning different misclassification costs
    to different classes).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理不平衡的技术**：在经典机器学习中，处理数据不平衡的标准技术包括重采样（对少数类进行过采样或对多数类进行欠采样）、生成合成样本（例如，使用**合成少数类过采样技术**（SMOTE））和使用代价敏感学习（为不同类别分配不同的误分类成本）。'
- en: Some techniques from classical machine learning can also be applied in deep
    learning, but additional methods have been developed specifically for deep learning
    models. These include transfer learning (leveraging pre-trained models to learn
    from imbalanced data), using focal loss (a loss function that focuses on hard-to-classify
    examples), and employing data augmentation techniques to generate more varied
    and balanced training data. These data augmentation techniques will be discussed
    in detail in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep*
    *Learning Methods*.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些来自经典机器学习的技术也可以应用于深度学习，但已经开发出专门针对深度学习模型的方法。这些包括迁移学习（利用预训练模型从不平衡数据中学习）、使用焦点损失（一个关注难以分类示例的损失函数）以及采用数据增强技术来生成更多样化和平衡的训练数据。这些数据增强技术将在[*第7章*](B17259_07.xhtml#_idTextAnchor205)，*数据级深度学习方法*中详细讨论。
- en: '**Model interpretability**: Classical machine learning models are often more
    interpretable, which can help us understand the impact of data imbalance on the
    model’s decision-making process. Deep learning models, on the other hand, are
    often referred to as “black boxes” due to their lack of interpretability. This
    lack of interpretability can make it harder to understand how the model handles
    imbalanced data and whether it is learning meaningful patterns or simply memorizing
    the majority class.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可解释性**：经典机器学习模型通常更可解释，这有助于我们了解数据不平衡对模型决策过程的影响。另一方面，由于缺乏可解释性，深度学习模型通常被称为“黑盒”。这种缺乏可解释性使得理解模型如何处理不平衡数据以及它是否在学习有意义的模式或仅仅是记忆多数类变得更加困难。'
- en: '**Training data size**: Deep learning models typically require large amounts
    of training data to achieve optimal performance. In cases of severe data imbalance,
    gathering sufficient data for the minority class may be more challenging, hindering
    the performance of deep learning models. Additionally, if a large dataset is available,
    it is more likely that instances of the minority class will be found within that
    vast amount of data. In contrast, in a smaller dataset, the minority class might
    never appear at all! On the other hand, classical machine learning algorithms
    can often achieve decent performance with smaller datasets, which is an advantage
    when dealing with imbalanced data.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据大小**：深度学习模型通常需要大量的训练数据以实现最佳性能。在数据严重不平衡的情况下，收集足够的数据以供少数类使用可能更具挑战性，这可能会阻碍深度学习模型的表现。此外，如果有一个大型数据集可用，那么在大量数据中找到少数类的实例的可能性更大。相比之下，在较小的数据集中，少数类可能根本不会出现！另一方面，经典机器学习算法通常可以在较小的数据集上实现相当不错的性能，这在处理不平衡数据时是一个优势。'
- en: '**The impact of depth (number of layers) on deep learning models trained on
    imbalanced data problems**: The pros of more layers are the improved capacity
    to learn complex patterns and features in the data and improved generalization
    of the model. The cons of adding more layers can be the model overfitting and
    the problem of vanishing or exploding gradients worsening (as depth increases,
    the gradients during backpropagation can become very small (vanish) or very large
    (explode), making it challenging to train the model). This is summarized in *Figure
    6**.12*:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_12.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Summarizing the impact of depth on deep learning models
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the influence of depth on a deep learning model varies with the data
    and model architecture requiring empirical evaluation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling and undersampling**: The study titled *A systematic study of
    the* *class imbalance* *problem in convolutional neural networks*, by Mateusz
    Buda [8], thoroughly examined how class imbalance affects the performance of CNNs.
    The research utilized three well-known datasets – MNIST, CIFAR-10, and ImageNet
    – and employed models such as LeNet-5, All-CNN, and ResNet-10 for the experiments.
    Their key findings were as follows:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In almost all analyzed cases, oversampling proved to be the most effective technique
    for mitigating class imbalance
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling was more effective when the imbalance was eliminated while undersampling
    yielded better results when only reducing the imbalance partially
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrary to some traditional machine learning algorithms, CNNs did not overfit
    when oversampling was applied
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In some cases, undersampling performs on par with oversampling, even when using
    less data
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Undersampling generally showed poor performance compared to the baseline and
    never showed a notable advantage over oversampling
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the focus is on correctly classifying examples from minority classes, undersampling
    may be preferable to oversampling
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If there’s too much data, undersampling might be the preferred or only option
    to save time, resources, and the cost of training.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Threshold adjustment**: We discussed threshold adjustment in detail in [*Chapter
    5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*. As a refresher,
    a decision threshold is a value that determines the boundary between different
    classes or outcomes in a classification problem. The paper by Johnson et al. [11]
    emphasized that the optimal threshold is linearly correlated with the minority
    class size (that is, the lower the minority class size, the lower the threshold).
    They trained and tested for fraud detection using deep learning models with two
    and four hidden layers on CMS Medicare data [12]. The default threshold often
    leads to poor classification, especially for the minority class, highlighting
    the need for threshold adjustment based on a validation set.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A note about the datasets used in the second half of this book
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'In the remainder of this book, we will primarily use imbalanced versions of
    the MNIST and CIFAR10-LT (“LT” stands for “long-tailed”) datasets for training:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '• **MNIST**: A small grayscale image dataset with 10 classes containing digits
    from 0 to 9\. It consists of 60,000 training images and 10,000 test images, each
    28 pixels x 28 pixels. It’s faster to train/test compared to CIFAR-10.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '• **CIFAR-10**: Used for object recognition, this color image dataset also
    has 10 classes. It includes 50,000 training images and 10,000 test images, each
    32 pixels x 32 pixels.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Though the training sets are imbalanced, the corresponding test sets have an
    equal number of examples in each class. This balanced test set approach provides
    several benefits:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '• **Comparability**: Balanced test sets allow unbiased comparison across various
    classes and models'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '• **Repeatability and reproducibility**: Using simple datasets such as MNIST
    and CIFAR-10 ensures ease of code execution and understanding'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '• **Efficiency**: Smaller datasets enable quicker iterations, allowing us to
    try, test, and retry running the code in a reasonable timeframe'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '• **Alignment with research**: This approach is consistent with most research
    studies on long-tailed learning and imbalanced datasets, providing a common framework
    for comparison'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The next section will give us an overview of deep learning strategies for managing
    data imbalance. We will also see how various techniques for handling imbalanced
    datasets that were initially developed for classical machine learning techniques
    can be easily extended to deep learning models.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Overview of deep learning techniques to handle data imbalance
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Much like the first half of this book, where we focused on classical machine
    learning techniques, the major categories typically include sampling techniques,
    cost-sensitive techniques, threshold adjustment techniques, or a combination of
    these:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The sampling techniques comprise either undersampling the majority class or
    oversampling the minority class data. Data augmentation is a fundamental technique
    in computer vision problems that’s used to increase the diversity of the training
    set. While not directly an oversampling method aimed at addressing class imbalance,
    data augmentation does have the effect of expanding the training data. We will
    discuss these techniques in more detail in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep* *Learning Methods*.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive techniques usually involve changing the model loss function in
    some way to accommodate the higher cost of misclassifying the minority class examples.
    Some standard loss functions, such as `CrossEntropyLoss` in PyTorch, support the
    weight parameter to accommodate such costs. We will cover many of those, including
    several custom loss functions, in detail in [*Chapter 8*](B17259_08.xhtml#_idTextAnchor235),
    *Algorithm-Level Deep* *Learning Techniques*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid deep learning techniques integrate the data-level and algorithm-level
    approaches. This fusion allows for more nuanced and effective solutions to tackle
    class imbalance. We’ll discuss this in [*Chapter 9*](B17259_09.xhtml#_idTextAnchor256),
    *Hybrid Deep* *Learning Methods*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Threshold adjustment techniques are applied to the scores produced from the
    model after the model has been trained. These techniques can help adjust the threshold
    so that the model metric, say, F1-score or geometric mean, gets optimized. This
    was discussed in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151)*,* *Cost-Sensitive
    Learning*:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_06_13.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Categorization of deep learning techniques covered
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: We will not use deep learning methods for tabular data from this chapter onwards
    because classical models such as XGBoost, LightGBM, and CatBoost tend to perform
    well on such structured data. Several studies ([13] and [14]) have shown that
    traditional machine learning models often outperform deep learning models in supervised
    learning tasks involving tabular data. However, an ensemble of an XGBoost model
    and a deep learning model can outperform the XGBoost model alone [13]. Therefore,
    tasks using tabular datasets can still benefit from deep learning models. It’s
    likely only a matter of time before deep learning models catch up to the performance
    of classical models on tabular data. Nevertheless, we will focus our implementation
    and examples on vision and NLP problems when using deep learning models.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our high-level discussion of various techniques for dealing with
    imbalanced datasets when using deep learning models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Multi-label classification
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multi-label classification is a classification task where each instance can
    be assigned to multiple classes or labels simultaneously. In other words, an instance
    can belong to more than one category or have multiple attributes. For example,
    a movie can belong to multiple genres, such as action, comedy, and romance. Similarly,
    an image can have multiple objects in it (*Figure 6**.14*):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_14.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Multi-label image classification with prediction probabilities
    shown
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: But how is it different from multi-class classification? Multi-class classification
    is a classification task where each instance can be assigned to only one class
    or label. In this case, the classes or categories are mutually exclusive, meaning
    an instance can belong to just one category. For example, a handwritten digit
    recognition task would be multi-class since each digit can belong to only one
    class (0-9).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the main difference between multi-label and multi-class classification
    is that in multi-label classification, instances can have multiple labels. In
    contrast, in multi-class classification, instances can have only one label. This
    is summarized in *Figure 6**.15*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_15.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Distinction between multi-label and multi-class classification
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Many real-world problems are inherently multi-labeled and experience class imbalance.
    Deep learning models are particularly useful here, especially when the data involved
    is unstructured, such as images, videos, or text.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In **Multi-Label Datasets** (**MLDs**), there can be tens or hundreds of labels,
    and each instance can be associated with a subset of those labels. The more different
    labels exist, the more possibilities there are that some have a very low presence,
    leading to a significant imbalance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The data imbalance in multi-label classification can occur at many levels [15]:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '**Imbalance within labels**: A large disparity between negative and positive
    instances in each label can happen, causing classification models to struggle
    with minority classes'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalance between labels**: Unequal distribution of positive instances among
    labels, leading to poor performance for minority classes'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imbalance among label sets**: Sparse frequency of label sets (a combination
    of various labels) due to label sparseness, making it challenging for classification
    models to learn effectively'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6**.16* sums up these kinds of imbalances when classifying multi-label
    datasets:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_06_16.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Kinds of imbalances in multi-label datasets
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The imbalance handling approaches and metrics are similar to those used for
    imbalanced multi-class classification methods. Typical approaches include resampling
    methods, ensemble approaches, and cost-sensitive methods.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward approach is to convert the multi-label dataset into
    the multi-class dataset and then apply various re-sampling methods such as random
    oversampling, random undersampling, **Edited Nearest Neighbors** (**ENN**), and
    others. Similar strategies have been used in the literature to adapt the cost-sensitive
    approaches to fit multi-label classification problems.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has become essential in many fields, from computer vision and
    natural language processing to healthcare and finance. This chapter has provided
    a brief introduction to the core concepts and techniques in deep learning. We
    talked about PyTorch, the fundamentals of deep learning, activation functions,
    and data imbalance challenges. We also got a bird’s-eye view of the various techniques
    we will discuss in the following few chapters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these fundamentals will equip you with the knowledge necessary
    to explore more advanced topics and applications and ultimately contribute to
    the ever-evolving world of deep learning.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at data-level deep learning methods.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are some challenges in porting data imbalance handling methods from classical
    machine learning models to deep learning models?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could an imbalanced version of the MNIST dataset be created?
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the MNIST dataset to train a CNN model with varying degrees of imbalance
    in the data. Record the model’s overall accuracy on a fixed test set. Plot how
    the overall accuracy changes as the imbalance in the training data increases.
    Observe whether the overall accuracy declines as the training data becomes more
    imbalanced.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of using random oversampling with deep learning models?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the data augmentation techniques that can be applied when dealing
    with limited or imbalanced data?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does undersampling work in handling data imbalance, and what are its limitations?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is it important to ensure that the data augmentation techniques preserve
    the original labels of the dataset?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A. W. Trask, Grokking Deep Learning (Manning, Shelter Island, NY, 2019).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. Chollet, *Deep Learning with Python*. Manning Publications, 2021.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, “*Class-Balanced Loss Based
    on Effective Number of Samples*,” p. 10.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, *Learning Imbalanced Datasets
    with Label- Distribution-Aware Margin Loss* [Online]. Available at [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. Jantanasukon and A. Thammano, *Adaptive Learning Rate for Dealing with Imbalanced
    Data in Classification Problems*. In 2021 Joint International Conference on Digital
    Arts, Media and Technology with ECTI Northern Section Conference on Electrical,
    Electronics, Computer and Telecommunication Engineering, Cha-am, Thailand: IEEE,
    Mar. 2021, pp. 229–232, doi: 10.1109/ECTIDAMTNCON51128.2021.9425715.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H.-J. Ye, H.-Y. Chen, D.-C. Zhan, and W.-L. Chao, *Identifying and Compensating
    for Feature Deviation in Imbalanced Deep Learning*. arXiv, Jul. 10, 2022\. Accessed:
    Dec. 14, 2022\. [Online]. Available at [http://arxiv.org/abs/2001.01385](http://arxiv.org/abs/2001.01385).'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'V. Sampath, I. Maurtua, J. J. Aguilar Martín, and A. Gutierrez, *A survey on
    generative adversarial networks for imbalance problems in computer vision tasks*.
    J Big Data, vol. 8, no. 1, p. 27, Dec. 2021, doi: 10.1186/s40537-021-00414-0.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Buda, A. Maki, and M. A. Mazurowski, *A systematic study of the class imbalance
    problem in convolutional neural networks*. Neural Networks, vol. 106, pp. 249–259,
    Oct. 2018, doi: 10.1016/j.neunet.2018.07.011.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, *On the
    combined effect of class imbalance and concept complexity in deep learning*. arXiv,
    Jul. 29, 2021\. Accessed: Mar. 28, 2023\. [Online]. Available at [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194).'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'K. Ghosh, C. Bellinger, R. Corizzo, B. Krawczyk, and N. Japkowicz, *On the
    combined effect of class imbalance and concept complexity in deep learning*. arXiv,
    Jul. 29, 2021\. Accessed: Mar. 28, 2023\. [Online]. Available at [http://arxiv.org/abs/2107.14194](http://arxiv.org/abs/2107.14194).'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Medicare fraud detection using neural
    networks*. J Big Data, vol. 6, no. 1, p. 63, Dec. 2019, doi: 10.1186/s40537-019-0225-0.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Medicare fraud & abuse: prevention, detection, and reporting*. Centers for
    Medicare & Medicaid Services. 2017\. [https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244](https://www.cms.gov/Outreach-and-Education/Medicare-Learning-Network-MLN/MLNProducts/MLN-Publications-Items/MLN4649244).'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'R. Shwartz-Ziv and A. Armon, *Tabular Data: Deep Learning is Not All You Need*.
    arXiv, Nov. 23, 2021\. Accessed: Apr. 10, 2023\. [Online]. Available at [http://arxiv.org/abs/2106.03253](http://arxiv.org/abs/2106.03253).'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'V. Borisov, T. Leemann, K. Sessler, J. Haug, M. Pawelczyk, and G. Kasneci,
    *Deep Neural Networks and Tabular Data: A Survey*. IEEE Trans. Neural Netw. Learning
    Syst., pp. 1–21, 2022, doi: 10.1109/TNNLS.2022.3229161.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. N. Tarekegn, M. Giacobini, and K. Michalak, *A review of methods for imbalanced
    multi-label classification*. Pattern Recognition, vol. 118, p. 107965, Oct. 2021,
    doi: 10.1016/j. patcog.2021.107965.'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
