<html><head></head><body>
<div id="_idContainer063">
<h1 class="chapter-number" id="_idParaDest-67"><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-68"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.2.1">Labeling Data for Regression</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will explore the process of labeling data for regression-based machine learning tasks, such as predicting housing prices, in situations where there is insufficient labeled data available for training. </span><span class="koboSpan" id="kobo.3.2">Regression tasks are tasks that involve predicting numerical values using a labeled training dataset, making them integral to fields such as finance and economics. </span><span class="koboSpan" id="kobo.3.3">However, real-world scenarios often present a challenge: labeled data is a precious commodity, often in </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">short supply.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">If there is a short supply of labeled data to train a machine learning model, you can still use summary statistics, semi-supervised learning, and clustering to predict the target labels for your unlabeled data. </span><span class="koboSpan" id="kobo.5.2">We have demonstrated this using house price data as an example and generated the predicted labels for house prices programmatically using Python. </span><span class="koboSpan" id="kobo.5.3">We will look at different approaches to labeling data for regression using Snorkel libraries, semi-supervised learning, data augmentation, and K-means clustering methods. </span><span class="koboSpan" id="kobo.5.4">In real-world projects, it is challenging to get the labeled data required for training machine learning regression models. </span><span class="koboSpan" id="kobo.5.5">For example, adequate data may not be available to train the model to predict house prices. </span><span class="koboSpan" id="kobo.5.6">In those cases, we prepare the training data by using various </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">Python libraries.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Using rules based on summary statistics to generate house price labels </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">for regression</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Using semi-supervised learning to label regression data for house price prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">with regression</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Generating house price data labels with data augmentation to generate synthetic data </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">for regression</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Using K-means clustering to label the house price data </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">for regression</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.17.1">By the end of this chapter, you will be able to generate labels for regression data using Python libraries programmatically. </span><span class="koboSpan" id="kobo.17.2">Furthermore, you’ll have the expertise required to overcome regression challenges adeptly, ensuring your data-driven endeavors steer </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">toward success.</span></span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.19.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.20.1">We will use the California house price dataset (</span><a href="https://www.kaggle.com/datasets/camnugent/california-housing-prices"><span class="koboSpan" id="kobo.21.1">https://www.kaggle.com/datasets/camnugent/california-housing-prices</span></a><span class="koboSpan" id="kobo.22.1">) for this chapter. </span><span class="koboSpan" id="kobo.22.2">You can download the </span><strong class="source-inline"><span class="koboSpan" id="kobo.23.1">housing.csv</span></strong><span class="koboSpan" id="kobo.24.1"> file from GitHub at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">path: </span></span><a href="https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/datasets"><span class="No-Break"><span class="koboSpan" id="kobo.26.1">https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/datasets</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.27.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">We also need to install Python 3.7+ and set up any of the following </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">Python editors:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.30.1">The VS </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">Code IDE</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.32.1">Anaconda</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.33.1">Jupyter Notebook</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.34.1">Replit</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.35.1">We recommend following the complete code on GitHub to follow along with </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">the chapter.</span></span></p>
<h1 id="_idParaDest-70"><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.37.1">Using summary statistics to generate housing price labels</span></h1>
<p><span class="koboSpan" id="kobo.38.1">In this section, we</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.39.1"> are going to generate house price labels using summary statistics of a small set of available</span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.40.1"> labeled housing price data. </span><span class="koboSpan" id="kobo.40.2">This is useful in real-world projects when there is insufficient labeled data for regression tasks. </span><span class="koboSpan" id="kobo.40.3">In such scenarios, we will generate labeled data by creating some rules based on </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">summary statistics.</span></span></p>
<p><span class="koboSpan" id="kobo.42.1">We</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.43.1"> decode the significance </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.44.1">of the data’s underlying trends. </span><span class="koboSpan" id="kobo.44.2">By computing the mean of each feature within the labeled training dataset, we embark on a journey to quantify the essence of the data. </span><span class="koboSpan" id="kobo.44.3">This approach ingeniously leverages distance metrics to unveil the closest match for a label, bestowing unlabeled data points with the wisdom of their </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">labeled counterparts.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">Let’s load the data from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.47.1">housing.csv</span></strong><span class="koboSpan" id="kobo.48.1"> file </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">using pandas:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.50.1">
import pandas as pd
# Load the labeled data
df_labeled = pd.read_csv('housing.csv')</span></pre> <p><span class="koboSpan" id="kobo.51.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<span class="koboSpan" id="kobo.53.1"><img alt="Figure 3.1 – Snippet of the DataFrame" src="image/B18944_03_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.54.1">Figure 3.1 – Snippet of the DataFrame</span></p>
<p><span class="koboSpan" id="kobo.55.1">After loading the labeled data using </span><strong class="source-inline"><span class="koboSpan" id="kobo.56.1">pd.read_csv</span></strong><span class="koboSpan" id="kobo.57.1">, we then compute the summary statistics for each feature by target label using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.58.1">groupby()</span></strong><span class="koboSpan" id="kobo.59.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.60.1">describe()</span></strong><span class="koboSpan" id="kobo.61.1"> methods. </span><span class="koboSpan" id="kobo.61.2">This gives us the mean, standard deviation, minimum, maximum, and quartile values for each feature by </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">target label:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.63.1">
# Compute the summary statistics for each feature by target label
summary_stats = df.groupby('median_house_value').describe()</span></pre> <p><span class="koboSpan" id="kobo.64.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<span class="koboSpan" id="kobo.66.1"><img alt="Figure 3.2 – Summary statistics of the house price dataset" src="image/B18944_03_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.67.1">Figure 3.2 – Summary statistics of the house price dataset</span></p>
<h2 id="_idParaDest-71"><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.68.1">Finding the closest labeled observation to match the label</span></h2>
<p><span class="koboSpan" id="kobo.69.1">We then</span><a id="_idIndexMarker156"/><span class="koboSpan" id="kobo.70.1"> loop through each row in the unlabeled data and compute the distances to each target label’s summary statistics using Euclidean distance. </span><span class="koboSpan" id="kobo.70.2">We select the target label with the minimum distance as the predicted target label and assign it to the </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">current row:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.72.1">
# Load the unlabeled data
df_unlabeled = pd.read_csv(''housing_unlabled.csv')</span></pre> <p><span class="koboSpan" id="kobo.73.1">The Euclidean distance is the distance between two points on a plane. </span><span class="koboSpan" id="kobo.73.2">Here, the distance between two points </span><strong class="source-inline"><span class="koboSpan" id="kobo.74.1">(x1, y1)</span></strong><span class="koboSpan" id="kobo.75.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.76.1">(x2, y2)</span></strong><span class="koboSpan" id="kobo.77.1"> is </span><strong class="source-inline"><span class="koboSpan" id="kobo.78.1">d = √[(x2 – x1)2 + (y2 – y1)2]</span></strong><span class="koboSpan" id="kobo.79.1">. </span><span class="koboSpan" id="kobo.79.2">This is used to find similar points, that is, the closest point to an unlabeled data point in the labeled data points, so that we can assign the corresponding label from the labeled dataset to the unlabeled data point. </span><span class="koboSpan" id="kobo.79.3">unlabeled dataset) is calculated by combining the distance between all the features in the row. </span><span class="koboSpan" id="kobo.79.4">We assign the target label of a row with the minimum distance from the predicted target label to the current row in the unlabeled dataset. </span><span class="koboSpan" id="kobo.79.5">This helps us to assign labels to the unlabeled dataset based on the closest match to the label in the training dataset using </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">distance metrics.</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">Here, the outermost </span><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">for</span></strong><span class="koboSpan" id="kobo.83.1"> loop reads one row at a time from unlabeled data and then performs the following steps on that row in the inner </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.84.1">for</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.85.1"> loop:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.86.1">
# Predict the target label for each data point in the unlabeled data
for i, row in df_unlabeled.iterrows():</span></pre> <p><span class="koboSpan" id="kobo.87.1">The outermost </span><strong class="source-inline"><span class="koboSpan" id="kobo.88.1">for</span></strong><span class="koboSpan" id="kobo.89.1"> loop reads one row at a time from unlabeled data and then performs the following steps on that row in the inner </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.90.1">for</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.91.1"> loop:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.92.1">
# Compute the distances to each target label's summary statistics
dists = {}</span></pre> <p><span class="koboSpan" id="kobo.93.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.94.1">for target</span></strong><span class="koboSpan" id="kobo.95.1"> loop iterates over each target label in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.96.1">summary_stats</span></strong><span class="koboSpan" id="kobo.97.1"> DataFrame. </span><span class="koboSpan" id="kobo.97.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.98.1">index</span></strong><span class="koboSpan" id="kobo.99.1"> attribute of a DataFrame returns the row labels, which in this case are the </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">target labels:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.101.1">
for target in summary_stats.index:</span></pre> <p><span class="koboSpan" id="kobo.102.1">The following line initializes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.103.1">dist</span></strong><span class="koboSpan" id="kobo.104.1"> variable to </span><strong class="source-inline"><span class="koboSpan" id="kobo.105.1">0</span></strong><span class="koboSpan" id="kobo.106.1">, which we will use to accumulate the distance between the current unlabeled data point and the current target label’s </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">summary statistics:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.108.1">
dist = 0
for col in df_unlabeled.columns:</span></pre> <p><span class="koboSpan" id="kobo.109.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.110.1">for col</span></strong><span class="koboSpan" id="kobo.111.1"> loop iterates over each column in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">df_unlabeled</span></strong><span class="koboSpan" id="kobo.113.1"> DataFrame. </span><span class="koboSpan" id="kobo.113.2">We want to compute the distance between the current unlabeled data point and each target label’s summary statistics for </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">each feature.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">The following line checks if the current column is not the target column. </span><span class="koboSpan" id="kobo.115.2">We don’t want to compute the distance between the current unlabeled data point and the summary statistics of the target column, as this would not </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">make sense:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.117.1">
if col != 'median_house_value':</span></pre> <p><span class="koboSpan" id="kobo.118.1">The </span><a id="_idIndexMarker157"/><span class="koboSpan" id="kobo.119.1">following line computes the squared distance between the current unlabeled data point’s feature value and the corresponding feature’s mean value in the current target label’s summary statistics. </span><span class="koboSpan" id="kobo.119.2">We square the distance to make it positive and exaggerate </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">the differences:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.121.1">
dist += (row[col] - summary_stats.loc[target, (col, 'mean')]) ** 2</span></pre> <p><span class="koboSpan" id="kobo.122.1">The following line saves the computed distance in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.123.1">dists</span></strong><span class="koboSpan" id="kobo.124.1"> dictionary for the current </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">target label:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.126.1">
dists[target] = dist
# Select the target label with the minimum distance
predicted_target = min(dists, key=dists.get)
# Assign the predicted target label to the current row
df_unlabeled.at[i, 'median_house_value'] = predicted_target</span></pre> <p><span class="koboSpan" id="kobo.127.1">By the end of this inner loop, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.128.1">dists</span></strong><span class="koboSpan" id="kobo.129.1"> dictionary will contain the squared distances between the current unlabeled data point and each target label’s summary statistics. </span><span class="koboSpan" id="kobo.129.2">We will then select the target label with the minimum distance as the predicted target label for the current </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">data point.</span></span></p>
<p><span class="koboSpan" id="kobo.131.1">The same process continues for each row of the unlabeled data to compute distances from each of the target label’s features mean values in the summary statistics to the corresponding column in the </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">unlabeled data.</span></span></p>
<p><span class="koboSpan" id="kobo.133.1">Finally, we save the labeled data to a new CSV file using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.134.1">to_csv()</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.135.1">method, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.136.1">df_unlabeled.to_csv('housing_result.csv')</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<span class="koboSpan" id="kobo.138.1"><img alt="Figure 3.3 – Labeled data with predicted median house value" src="image/B18944_03_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.139.1">Figure 3.3 – Labeled data with predicted median house value</span></p>
<p><span class="koboSpan" id="kobo.140.1">Now, we</span><a id="_idIndexMarker158"/><span class="koboSpan" id="kobo.141.1"> can see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">median_house_value</span></strong><span class="koboSpan" id="kobo.143.1"> is assigned to the row in the unlabeled dataset. </span><span class="koboSpan" id="kobo.143.2">Note that this approach assumes that the summary statistics of the labeled data can be used to predict the target labels of the unlabeled data accurately. </span><span class="koboSpan" id="kobo.143.3">Therefore, it is essential to validate the accuracy of the predictions before using them </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">in practice.</span></span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.145.1">Using semi-supervised learning to label regression data</span></h2>
<p><span class="koboSpan" id="kobo.146.1">In this section, we</span><a id="_idIndexMarker159"/><span class="koboSpan" id="kobo.147.1"> are going to use semi-supervised learning to label the regression data. </span><span class="koboSpan" id="kobo.147.2">Semi-supervised learning is a type of machine learning that combines both labeled and unlabeled data to improve the accuracy of a predictive model. </span><span class="koboSpan" id="kobo.147.3">In semi-supervised learning, a small amount of labeled data is used with a much larger amount of unlabeled data to train the model. </span><span class="koboSpan" id="kobo.147.4">The idea is that the unlabeled data can provide additional information about the underlying patterns in the data that can help the model to learn more effectively. </span><span class="koboSpan" id="kobo.147.5">By using </span><a id="_idIndexMarker160"/><span class="koboSpan" id="kobo.148.1">both labeled and unlabeled data, semi-supervised learning can improve the accuracy of machine learning models, especially when labeled data is scarce or expensive </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">to </span></span><span class="No-Break"><a id="_idIndexMarker161"/></span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">obtain.</span></span></p>
<p><span class="koboSpan" id="kobo.151.1">Now, let’s look in detail at the pseudo-labeling method and how it is used for </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">data labeling.</span></span></p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.153.1">Pseudo-labeling</span></h2>
<p><span class="koboSpan" id="kobo.154.1">Pseudo-labeling</span><a id="_idIndexMarker162"/><span class="koboSpan" id="kobo.155.1"> is a technique used in semi-supervised learning where a model trained on labeled data</span><a id="_idIndexMarker163"/><span class="koboSpan" id="kobo.156.1"> is used to predict the labels of the unlabeled data. </span><span class="koboSpan" id="kobo.156.2">These predicted labels are called pseudo-labels. </span><span class="koboSpan" id="kobo.156.3">The model then combines the labeled and pseudo-labeled data to retrain and improve the accuracy of the model. </span><span class="koboSpan" id="kobo.156.4">Pseudo-labeling is a way to leverage the unlabeled data to improve the performance of the model, especially when labeled data </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">is limited.</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">The pseudo-labeling process involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Train a model on labeled data</span></strong><span class="koboSpan" id="kobo.161.1">: Train</span><a id="_idIndexMarker164"/><span class="koboSpan" id="kobo.162.1"> a supervised learning model on the labeled data using a training algorithm. </span><span class="koboSpan" id="kobo.162.2">The model is fitted to the training set using the </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">provided labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.164.1">Predict labels for unlabeled data</span></strong><span class="koboSpan" id="kobo.165.1">: Use the trained model to predict the labels for the unlabeled data. </span><span class="koboSpan" id="kobo.165.2">These predicted labels are </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">called pseudo-labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.167.1">Combine labeled and pseudo-labeled data</span></strong><span class="koboSpan" id="kobo.168.1">: Combine the labeled data with the pseudo-labeled data to form a new, larger training set. </span><span class="koboSpan" id="kobo.168.2">The pseudo-labeled data is treated as if it were </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">labeled data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.170.1">Retrain the model</span></strong><span class="koboSpan" id="kobo.171.1">: Retrain the model using the combined dataset. </span><span class="koboSpan" id="kobo.171.2">The model is updated using both the labeled and pseudo-labeled data to improve the </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">model’s accuracy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.173.1">Repeat steps 2-4</span></strong><span class="koboSpan" id="kobo.174.1">: Iterate the process by reusing the updated model to predict labels for new, previously unlabeled data, and combining the newly labeled data with the existing labeled data for the next round of model retraining, and the process is repeated </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">until convergence.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.176.1">Pseudo-labeling</span><a id="_idIndexMarker165"/><span class="koboSpan" id="kobo.177.1"> can be an</span><a id="_idIndexMarker166"/><span class="koboSpan" id="kobo.178.1"> effective way to leverage the large amount of unlabeled data that is typically available in many applications. </span><span class="koboSpan" id="kobo.178.2">By using this unlabeled data to improve the accuracy of the model, pseudo-labeling can help to improve the performance of supervised machine learning models, especially when enough labeled training data is not </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">easily available.</span></span></p>
<p><span class="koboSpan" id="kobo.180.1">Let’s use the house price dataset to predict the labels </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">for regression:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.182.1">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import numpy as np</span></pre> <p><span class="koboSpan" id="kobo.183.1">Let’s load the house price dataset and then split the labeled data into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">labeled_data</span></strong><span class="koboSpan" id="kobo.185.1"> DataFrame and unlabeled data into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.186.1">unlabeled_data</span></strong><span class="koboSpan" id="kobo.187.1"> DataFrame, </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.189.1">
# Load the data
data = pd.read_csv("housing_data.csv")
# Split the labeled data into training and testing sets
train_data, test_data, train_labels, test_labels = \
    train_test_split(labeled_data.drop('price', axis=1), \
        labeled_data['price'], test_size=0.2)</span></pre> <p><span class="koboSpan" id="kobo.190.1">This code snippet is used to divide the labeled data into two parts: a training set and a testing set. </span><span class="koboSpan" id="kobo.190.2">The training set contains the features (input data) and the corresponding labels (output data) that we will use to train our machine learning model. </span><span class="koboSpan" id="kobo.190.3">The testing set is a small portion of the data that we will use to evaluate the model’s performance. </span><span class="koboSpan" id="kobo.190.4">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">train_test_split</span></strong><span class="koboSpan" id="kobo.192.1"> function from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">sklearn.model_selection</span></strong><span class="koboSpan" id="kobo.194.1"> library helps us achieve this division while specifying the size of the testing set (in this case, 20% of the data). </span><span class="koboSpan" id="kobo.194.2">Let’s train</span><a id="_idIndexMarker167"/><span class="koboSpan" id="kobo.195.1"> the model using the training dataset for regression, </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.197.1">
# Train a linear regression model on the labeled data
regressor = LinearRegression()
regressor.fit(train_data, train_labels)</span></pre> <p><span class="koboSpan" id="kobo.198.1">In this code snippet, we’re building and training a linear regression model using the labeled data. </span><span class="koboSpan" id="kobo.198.2">First, we import the </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">LinearRegression</span></strong><span class="koboSpan" id="kobo.200.1"> class from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">sklearn.linear_model</span></strong><span class="koboSpan" id="kobo.202.1"> library. </span><span class="koboSpan" id="kobo.202.2">Then, we create an instance of the linear regression model named </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">regressor</span></strong><span class="koboSpan" id="kobo.204.1">. </span><span class="koboSpan" id="kobo.204.2">Finally, we train the model using the training data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.205.1">train_data</span></strong><span class="koboSpan" id="kobo.206.1">) as the input features and the corresponding labels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">train_labels</span></strong><span class="koboSpan" id="kobo.208.1">) as the desired outputs. </span><span class="koboSpan" id="kobo.208.2">The model learns from this data to make predictions later. </span><span class="koboSpan" id="kobo.208.3">Now, let’s predict the labels using the regressor for the unlabeled dataset, </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.210.1">
# Use the trained model to predict the labels of the unlabeled data
predicted_labels = regressor.predict(
    unlabeled_data.drop('price', axis=1))</span></pre> <p><span class="koboSpan" id="kobo.211.1">In this code snippet, we’re employing the trained linear regression model to predict labels for unlabeled data points. </span><span class="koboSpan" id="kobo.211.2">We initialize an empty list, </span><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">predicted_labels</span></strong><span class="koboSpan" id="kobo.213.1">, to store the predictions. </span><span class="koboSpan" id="kobo.213.2">By applying the </span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">predict</span></strong><span class="koboSpan" id="kobo.215.1"> method of the trained </span><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">regressor</span></strong><span class="koboSpan" id="kobo.217.1"> model, we generate predictions based on the features (input data) in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.218.1">unlabeled_data</span></strong><span class="koboSpan" id="kobo.219.1">. </span><span class="koboSpan" id="kobo.219.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">price</span></strong><span class="koboSpan" id="kobo.221.1"> column is excluded since it’s the target variable we want to predict. </span><span class="koboSpan" id="kobo.221.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">predicted_labels</span></strong><span class="koboSpan" id="kobo.223.1"> list now holds the predicted outcomes of the regression model for the unlabeled data. </span><span class="koboSpan" id="kobo.223.2">Now we will </span><a id="_idIndexMarker168"/><span class="koboSpan" id="kobo.224.1">combine this predicted labeled data with the labeled data and train the model, </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.226.1">
# Combine the labeled and newly predicted data
new_data = pd.concat([labeled_data, unlabeled_data], ignore_index=True)
new_data['price'] = pd.concat([train_labels, \
    pd.Series(predicted_labels)], ignore_index=True)</span></pre> <p><span class="koboSpan" id="kobo.227.1">In this code snippet, we’re </span><a id="_idIndexMarker169"/><span class="koboSpan" id="kobo.228.1">creating a new dataset, </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">new_data</span></strong><span class="koboSpan" id="kobo.230.1">, by combining the labeled and the newly predicted data. </span><span class="koboSpan" id="kobo.230.2">First, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">pd.concat</span></strong><span class="koboSpan" id="kobo.232.1"> to concatenate the </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">labeled_data</span></strong><span class="koboSpan" id="kobo.234.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.235.1">unlabeled_data</span></strong><span class="koboSpan" id="kobo.236.1"> dataframes, creating a continuous dataset. </span><span class="koboSpan" id="kobo.236.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">ignore_index=True</span></strong><span class="koboSpan" id="kobo.238.1"> argument ensures that the index is reset for the </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">new dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.240.1">Next, we’re populating the </span><strong class="source-inline"><span class="koboSpan" id="kobo.241.1">'price'</span></strong><span class="koboSpan" id="kobo.242.1"> column in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">new_data</span></strong><span class="koboSpan" id="kobo.244.1"> DataFrame. </span><span class="koboSpan" id="kobo.244.2">We achieve this by concatenating the </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">train_labels</span></strong><span class="koboSpan" id="kobo.246.1"> (from the labeled data) with the predicted labels stored in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">predicted_labels</span></strong><span class="koboSpan" id="kobo.248.1"> list. </span><span class="koboSpan" id="kobo.248.2">This step ensures that our new dataset has complete labels for all data points, combining both known and </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">predicted values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.250.1">
# Train a new model on the combined data
new_train_data, new_test_data, new_train_labels, new_test_labels = \
    train_test_split(new_data.drop('price', axis=1), \
    new_data['price'], test_size=0.2)
new_regressor = LinearRegression()
new_regressor.fit(new_train_data, new_train_labels)</span></pre> <p><span class="koboSpan" id="kobo.251.1">In this code snippet, we’re training a new linear regression model on the combined dataset that includes both the labeled and predicted data. </span><span class="koboSpan" id="kobo.251.2">First, we split the combined data into new training and testing sets using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.252.1">train_test_split</span></strong><span class="koboSpan" id="kobo.253.1"> function, similar to what we did before. </span><span class="koboSpan" id="kobo.253.2">The new training data is stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">new_train_data</span></strong><span class="koboSpan" id="kobo.255.1">, and the corresponding labels are stored </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">new_train_labels</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.259.1">Next, we create a new instance of the linear regression model called </span><strong class="source-inline"><span class="koboSpan" id="kobo.260.1">new_regressor</span></strong><span class="koboSpan" id="kobo.261.1">. </span><span class="koboSpan" id="kobo.261.2">Finally, we train the new model using </span><strong class="source-inline"><span class="koboSpan" id="kobo.262.1">new_train_data</span></strong><span class="koboSpan" id="kobo.263.1"> as input features and </span><strong class="source-inline"><span class="koboSpan" id="kobo.264.1">new_train_labels</span></strong><span class="koboSpan" id="kobo.265.1"> as the desired outputs. </span><span class="koboSpan" id="kobo.265.2">This step ensures that our new model is fine-tuned to predict the combined data, leveraging both labeled and </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">predicted information:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.267.1">
# Evaluate the performance of the new model on the test data
score = new_regressor.score(new_test_data, new_test_labels)
print("R^2 Score: ", score)</span></pre> <p><span class="koboSpan" id="kobo.268.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.270.1"><img alt="Figure 3.4 – Performance of the model after adding pseudo-labeled data" src="image/B18944_03_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.271.1">Figure 3.4 – Performance of the model after adding pseudo-labeled data</span></p>
<p><span class="koboSpan" id="kobo.272.1">In this code snippet, we’re</span><a id="_idIndexMarker170"/><span class="koboSpan" id="kobo.273.1"> evaluating the performance of the new linear regression model on the test data that it hasn’t seen during training. </span><span class="koboSpan" id="kobo.273.2">The R-squared(coefficient of determination) score is calculated using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">score</span></strong><span class="koboSpan" id="kobo.275.1"> method of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.276.1">new_regressor</span></strong><span class="koboSpan" id="kobo.277.1"> model. </span><span class="koboSpan" id="kobo.277.2">The R^2 score</span><a id="_idIndexMarker171"/><span class="koboSpan" id="kobo.278.1"> is a measure of how well the model’s predictions match the actual data values. </span><span class="koboSpan" id="kobo.278.2">Higher R^2 scores indicate better </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">predictive accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">As we can see, the R^2 score is higher (</span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">0.6905783112767134</span></strong><span class="koboSpan" id="kobo.282.1">) with the combined dataset than with the original labeled trained dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">0.624186740765541</span></strong><span class="koboSpan" id="kobo.284.1">). </span><span class="koboSpan" id="kobo.284.2">Finally, we use this model to predict the labels. </span><span class="koboSpan" id="kobo.284.3">Now, let’s see another method, data augmentation, to generate synthetic data with labels </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">for regression.</span></span></p>
<h1 id="_idParaDest-74"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.286.1">Using data augmentation to label regression data</span></h1>
<p><span class="koboSpan" id="kobo.287.1">Data augmentation </span><a id="_idIndexMarker172"/><span class="koboSpan" id="kobo.288.1">can be used to </span><a id="_idIndexMarker173"/><span class="koboSpan" id="kobo.289.1">generate additional labeled data for regression tasks where labeled data is limited. </span><span class="koboSpan" id="kobo.289.2">Here is a way to use data augmentation to label </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">regression data:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.291.1">Collect labeled data</span></strong><span class="koboSpan" id="kobo.292.1">: Collect the limited labeled data available for the </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">regression task.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.294.1">Define data augmentation techniques</span></strong><span class="koboSpan" id="kobo.295.1">: Define a set of data augmentation techniques that can be used to generate new data points from the available labeled data. </span><span class="koboSpan" id="kobo.295.2">For regression tasks, common data augmentation techniques include adding noise, scaling, and rotating </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Generate augmented data</span></strong><span class="koboSpan" id="kobo.298.1">: Use data augmentation techniques to generate </span><a id="_idIndexMarker174"/><span class="koboSpan" id="kobo.299.1">new data points from the available labeled data. </span><span class="koboSpan" id="kobo.299.2">The new data points will have labels based on the labels of the original </span><span class="No-Break"><span class="koboSpan" id="kobo.300.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.301.1">Train the model</span></strong><span class="koboSpan" id="kobo.302.1">: Train a regression model using the augmented data and the original labeled data. </span><a id="_idIndexMarker175"/><span class="koboSpan" id="kobo.303.1">This step involves fitting a model to the combined dataset using a supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">learning algorithm.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.305.1">Evaluate the model</span></strong><span class="koboSpan" id="kobo.306.1">: Evaluate the performance of the trained model on a validation set. </span><span class="koboSpan" id="kobo.306.2">This step involves testing the accuracy of the model’s predictions on new, </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">unseen data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Fine-tune the model</span></strong><span class="koboSpan" id="kobo.309.1">: Fine-tune the model based on the performance on the validation set. </span><span class="koboSpan" id="kobo.309.2">This step involves adjusting the model’s hyperparameters to improve its performance on the </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">validation set.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.311.1">Test the model</span></strong><span class="koboSpan" id="kobo.312.1">: Finally, test the model’s performance on a test set to evaluate its </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">generalization performance.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.314.1">By using data augmentation to generate additional labeled data, it is possible to train a more accurate regression model even when limited labeled data is available. </span><span class="koboSpan" id="kobo.314.2">However, it is important to be careful when using data augmentation techniques to ensure that the generated data is meaningful and representative of the original </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">data distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">In the context of numerical data, we should focus on the following data augmentation techniques that are relevant and meaningful for the given dataset. </span><span class="koboSpan" id="kobo.316.2">For example, we can consider </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.318.1">Adding noise</span></strong><span class="koboSpan" id="kobo.319.1">: Adding random noise to numerical features and labels can simulate variations and uncertainties in </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">the data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.321.1">Scaling</span></strong><span class="koboSpan" id="kobo.322.1">: Scaling numerical features can simulate changes in units </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">or magnitudes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.324.1">Jittering</span></strong><span class="koboSpan" id="kobo.325.1">: Introducing small perturbations to numerical values can account for measurement errors </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">or fluctuations</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.327.1">Outlier injection</span></strong><span class="koboSpan" id="kobo.328.1">: Introducing outliers can help the model become more robust to </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">extreme values</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.330.1">Shuffling</span></strong><span class="koboSpan" id="kobo.331.1">: Randomly shuffling the order of data points can prevent the model from learning any </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">sequence-related bias</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.333.1">Remember that the choice of data augmentation techniques should be based on the characteristics of your dataset and the problem you’re trying to solve. </span><span class="koboSpan" id="kobo.333.2">The techniques should add meaningful variations that align with the nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">your data.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">Let’s see how we </span><a id="_idIndexMarker176"/><span class="koboSpan" id="kobo.336.1">generate augmented data for </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.337.1">the house price dataset to predict labels. </span><span class="koboSpan" id="kobo.337.2">Let’s import the necessary libraries, load the house price dataset, and define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.338.1">noise</span></strong><span class="koboSpan" id="kobo.339.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">scale</span></strong><span class="koboSpan" id="kobo.341.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.342.1">rotate</span></strong><span class="koboSpan" id="kobo.343.1"> data augmentation functions, </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.345.1">
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import random</span></pre> <p><span class="koboSpan" id="kobo.346.1">Then we load the labeled data stored in a CSV file named </span><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">labeled_data.csv</span></strong><span class="koboSpan" id="kobo.348.1"> with columns for the features and a column named </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">price</span></strong><span class="koboSpan" id="kobo.350.1"> for the </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">target variable:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.352.1">
# Load the available labeled data
df = pd.read_csv("labeled_data.csv")</span></pre> <p><span class="koboSpan" id="kobo.353.1">The following code defines two data augmentation techniques that add noise. </span><span class="koboSpan" id="kobo.353.2">It generates new data points by applying these augmentation techniques to the </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">labeled data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.355.1">
# Define the data augmentation techniques
def add_noise(x, std):
    noise = np.random.normal(0, std, len(x))
    return x + noise
# Define the range of data augmentation parameters
noise_range = [0.1, 0.2, 0.3]</span></pre> <p><span class="koboSpan" id="kobo.356.1">The range of </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.357.1">data augmentation</span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.358.1"> parameters for noise range is defined, and for each available data point, it generates multiple augmented data points with different </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">parameter values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.360.1">
# Generate augmented data
augmented_data = []
for _, row in df.iterrows():
    for noise in noise_range:
        new_row = row.copy()
        new_row["price"] = add_noise(row["price"], noise)
        augmented_data.append(new_row)</span></pre> <p><span class="koboSpan" id="kobo.361.1">By iterating </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.362.1">through each value in </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">noise_range</span></strong><span class="koboSpan" id="kobo.364.1"> and</span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.365.1"> adding noise to each data point’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">price</span></strong><span class="koboSpan" id="kobo.367.1"> feature, the code generates multiple data points with different levels of noise. </span><span class="koboSpan" id="kobo.367.2">This process results in more labeled data points for the machine learning model to learn from and improves the </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">model’s accuracy.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.369.1">noise_range</span></strong><span class="koboSpan" id="kobo.370.1"> is a list of standard deviation values for generating different levels of noise. </span><span class="koboSpan" id="kobo.370.2">It could be any list of values to add different levels of noise to the </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">data points:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">for noise in noise_range</span></strong><span class="koboSpan" id="kobo.373.1"> creates a loop that iterates through each value in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">noise_range</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.375.1"> list.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">new_row = row.copy()</span></strong><span class="koboSpan" id="kobo.377.1"> creates a copy of the original data point (</span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">i.e., row).</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">new_row["price"] = add_noise(row["price"], noise)</span></strong><span class="koboSpan" id="kobo.380.1"> adds noise to the copied data point’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">price</span></strong><span class="koboSpan" id="kobo.382.1"> feature using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.383.1">add_noise()</span></strong><span class="koboSpan" id="kobo.384.1"> function. </span><span class="koboSpan" id="kobo.384.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">add_noise()</span></strong><span class="koboSpan" id="kobo.386.1"> function adds random noise to each data point based on the standard deviation provided in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.387.1">noise</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.388.1"> variable.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.389.1">augmented_data.append(new_row)</span></strong><span class="koboSpan" id="kobo.390.1"> appends the newly generated data point to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">augmented_data</span></strong><span class="koboSpan" id="kobo.392.1"> list. </span><span class="koboSpan" id="kobo.392.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.393.1">augmented_data</span></strong><span class="koboSpan" id="kobo.394.1"> list contains all the newly generated data points for all levels of noise in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.395.1">noise_range</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.396.1"> list.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.397.1">Similarly, let’s define another data augmentation </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">scale function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.399.1">
def scale(x, factor):
    return x * factor
scale_range = [0.5, 0.75, 1.25, 1.5]</span></pre> <p><span class="koboSpan" id="kobo.400.1">The range of parameters for </span><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">scale_range</span></strong><span class="koboSpan" id="kobo.402.1"> is defined, and for each available data point, it generates multiple augmented data points with different </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">parameter values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.404.1">
for scale_factor in scale_range:
    new_row = row.copy()
    new_row["price"] = scale(row["price"], scale_factor)
    augmented_data.append(new_row)</span></pre> <p><span class="koboSpan" id="kobo.405.1">In this code</span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.406.1"> snippet, we’re utilizing data </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.407.1">augmentation to generate augmented data by applying scaling to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">price</span></strong><span class="koboSpan" id="kobo.409.1"> feature. </span><span class="koboSpan" id="kobo.409.2">For each scale factor within the specified </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">scale_range</span></strong><span class="koboSpan" id="kobo.411.1">, we duplicate the current data row by creating a copy of it using </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">row.copy()</span></strong><span class="koboSpan" id="kobo.413.1">. </span><span class="koboSpan" id="kobo.413.2">Then, we apply scaling to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">price</span></strong><span class="koboSpan" id="kobo.415.1"> feature using </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">scale_factor</span></strong><span class="koboSpan" id="kobo.417.1">, effectively modifying the price values while preserving the </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">data’s relationships.</span></span></p>
<p><span class="koboSpan" id="kobo.419.1">Finally, the augmented row is added to the list of augmented data stored in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">augmented_data</span></strong><span class="koboSpan" id="kobo.421.1"> list. </span><span class="koboSpan" id="kobo.421.2">This approach empowers us to explore how varying scales affect the </span><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">price</span></strong><span class="koboSpan" id="kobo.423.1"> feature and enrich our dataset with diverse instances for improved model training </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">and testing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.425.1">
# Combine the original data and the augmented data
combined_data = pd.concat([df, pd.DataFrame(augmented_data)])</span></pre> <p><span class="koboSpan" id="kobo.426.1">Here’s the </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">augmented data:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.428.1"><img alt="Figure 3.5 – Original data and augmented data" src="image/B18944_03_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.429.1">Figure 3.5 – Original data and augmented data</span></p>
<p><span class="koboSpan" id="kobo.430.1">The code</span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.431.1"> then combines the original labeled data with the augmented data, splits it into training and testing sets, trains a linear </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.432.1">regression model on the combined data, and evaluates the model’s performance on the test set using mean squared error as the </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">evaluation metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.434.1">
# Split the data into training and testing sets
X = combined_data.drop("price", axis=1)
y = combined_data["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, \
    test_size=0.2, random_state=42)
# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
# Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)</span></pre> <p><span class="koboSpan" id="kobo.435.1">By iterating through each value in </span><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">noise_range</span></strong><span class="koboSpan" id="kobo.437.1"> and adding noise to each available data point, it generates multiple augmented data points with different levels of noise. </span><span class="koboSpan" id="kobo.437.2">This process results in more labeled data points for the machine learning model to learn from and improves the model’s accuracy. </span><span class="koboSpan" id="kobo.437.3">Similarly, scale factor and rotation degree are used to generate labeled data using data augmentation to predict house prices </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">using regression.</span></span></p>
<p><span class="koboSpan" id="kobo.439.1">In this section, we have seen how to generate the augmented data using noise and scale techniques for regression. </span><span class="koboSpan" id="kobo.439.2">Now, let’s see how we can use the K-means clustering unsupervised learning method to label the house </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">price data.</span></span></p>
<h1 id="_idParaDest-75"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.441.1">Using k-means clustering to label regression data</span></h1>
<p><span class="koboSpan" id="kobo.442.1">In this section, we </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.443.1">are going to use the unsupervised </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.444.1">K-means clustering method to label the regression data. </span><span class="koboSpan" id="kobo.444.2">We use K-means to cluster data points into groups or clusters based on </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">their similarity.</span></span></p>
<p><span class="koboSpan" id="kobo.446.1">Once the clustering is done, we can compute the average label value for each cluster by taking the mean of the labeled data points that belong to that cluster. </span><span class="koboSpan" id="kobo.446.2">This is because the labeled data points in a cluster are likely to have similar label values since they are similar in terms of their </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">feature values.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.448.1"><img alt="Figure 3.6 – Basic k-means clustering with no. of clusters =3" src="image/B18944_03_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.449.1">Figure 3.6 – Basic k-means clustering with no. </span><span class="koboSpan" id="kobo.449.2">of clusters =3</span></p>
<p><span class="koboSpan" id="kobo.450.1">For example, let’s say we have a dataset of house prices with which we want to predict the price of a house based on features such as size, location, number of rooms, and so on. </span><span class="koboSpan" id="kobo.450.2">We have some labeled data points that consist of the features and their corresponding prices, but we also have some unlabeled data points with the </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">same features.</span></span></p>
<p><span class="koboSpan" id="kobo.452.1">We can use </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.453.1">K-means clustering to cluster the</span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.454.1"> labeled and unlabeled data points into groups based on their features. </span><span class="koboSpan" id="kobo.454.2">Then, we can compute the average price for each cluster by taking the mean of the labeled data points in </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">that cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.456.1">Finally, we can use these average prices to predict the prices of the unlabeled data points based on their </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">cluster assignment.</span></span></p>
<p><span class="koboSpan" id="kobo.458.1">We can use these predicted labels to create a new dataset by combining the labeled and unlabeled data. </span><span class="koboSpan" id="kobo.458.2">We then train a new model on the combined data and evaluate its performance on the </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">test data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.460.1">
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error
# Define labelled and unlabelled data
labelled_data = np.array([
[-122.23, 37.88, 41.0, 880.0, 129.0, 322.0, 126.0, 8.3252, 452600.0],
[-122.22, 37.86, 21.0, 7099.0, 1106.0, 2401.0, 1138.0, 8.3014, 358500.0]
])
unlabelled_data = np.array([
[-122.22, 47.86, 20.0, 7099.0, 1106.0, 2401.0, 1138.0, 8.3014, 0.0]
])
# Extract features and labels from labelled data
labelled_features = labelled_data[:, :-1]
labelled_labels = labelled_data[:, -1]</span></pre> <p><span class="koboSpan" id="kobo.461.1">Here, we import the necessary libraries and define the labeled and unlabeled </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">data arrays:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.463.1">
# Train K-means clustering model
n_clusters = 2 # Number of clusters (you can adjust this)
kmeans_model = KMeans(n_clusters=n_clusters)
kmeans_model.fit(labelled_features)</span></pre> <p><span class="koboSpan" id="kobo.464.1">We specify the number of clusters (</span><strong class="source-inline"><span class="koboSpan" id="kobo.465.1">n_clusters</span></strong><span class="koboSpan" id="kobo.466.1">) and use k-means clustering to fit the model to the </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">labeled features:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.468.1">
# Predict cluster labels for unlabelled data
unlabelled_clusters = kmeans_model.predict(unlabelled_data[:, :-1])</span></pre> <p><span class="koboSpan" id="kobo.469.1">We predict </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.470.1">cluster labels for the unlabeled </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.471.1">data using the trained </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">k-means model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.473.1">
# Calculate average prices for each cluster
cluster_avg_prices = []
for cluster_idx in range(n_clusters):
    cluster_mask = (kmeans_model.labels_ == cluster_idx)
    cluster_avg_price = np.mean(labelled_labels[cluster_mask])
    cluster_avg_prices.append(cluster_avg_price)</span></pre> <p><span class="koboSpan" id="kobo.474.1">We calculate the average prices for each cluster by iterating through cluster indices and calculating the mean of the labeled prices for </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">each cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">The line </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">cluster_mask = (kmeans_model.labels_ == cluster_idx)</span></strong><span class="koboSpan" id="kobo.478.1"> creates a boolean mask that identifies the data points in a </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">specific cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.480.1">Here’s a breakdown of what each part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">line does:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">kmeans_model.labels_</span></strong><span class="koboSpan" id="kobo.483.1">: This is an attribute of the K-means model that contains the cluster labels assigned to each data point during the clustering process. </span><span class="koboSpan" id="kobo.483.2">Each value in </span><strong class="source-inline"><span class="koboSpan" id="kobo.484.1">kmeans_model.labels_</span></strong><span class="koboSpan" id="kobo.485.1"> corresponds to the cluster label assigned to the corresponding data point in the order they appear in the </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">input data.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">cluster_idx</span></strong><span class="koboSpan" id="kobo.488.1">: This is the index of the cluster you’re interested in, ranging from 0 to the number of clusters minus one. </span><span class="koboSpan" id="kobo.488.2">It’s used to specify which cluster you want to create the </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">mask for.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.490.1">kmeans_model.labels_ == cluster_idx</span></strong><span class="koboSpan" id="kobo.491.1">: This part creates a boolean array where each element is </span><strong class="source-inline"><span class="koboSpan" id="kobo.492.1">True</span></strong><span class="koboSpan" id="kobo.493.1"> if the corresponding data point’s cluster label is equal to </span><strong class="source-inline"><span class="koboSpan" id="kobo.494.1">cluster_idx</span></strong><span class="koboSpan" id="kobo.495.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.496.1">False</span></strong><span class="koboSpan" id="kobo.497.1"> otherwise. </span><span class="koboSpan" id="kobo.497.2">Essentially, it’s checking which data points belong to the specific cluster </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">of interest.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">cluster_mask</span></strong><span class="koboSpan" id="kobo.500.1">: This is the resulting boolean mask that identifies the data points belonging to the cluster with the </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">index </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.502.1">cluster_idx</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.504.1">In summary, the line </span><strong class="source-inline"><span class="koboSpan" id="kobo.505.1">cluster_mask = (kmeans_model.labels_ == cluster_idx)</span></strong><span class="koboSpan" id="kobo.506.1"> creates a mask that helps you filter and select the data points in a specific cluster based on </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.507.1">their assigned cluster labels. </span><span class="koboSpan" id="kobo.507.2">This</span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.508.1"> mask can then be used to perform various operations on the data points belonging to that cluster. </span><span class="koboSpan" id="kobo.508.2">Predicted prices are assigned to the unlabeled data based on the calculated cluster </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">average prices:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.510.1">
# Assign predicted prices to unlabeled data
predicted_prices = np.array([cluster_avg_prices[cluster] \
    for cluster in unlabelled_clusters])</span></pre> <p><span class="koboSpan" id="kobo.511.1">Finally, we display the predicted prices for the unlabeled data using the K-means clustering technique, providing insights into the potential housing prices for the </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">unlabeled samples.:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.513.1">
print("Predicted Prices for Unlabelled Data:", predicted_prices)</span></pre> <p><span class="koboSpan" id="kobo.514.1">Here’s the output for the </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">predicted labels:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.516.1"><img alt="Figure 3.7 – Predicted price for unlabeled data based on the mean value of the labeled data cluster" src="image/B18944_03_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.517.1">Figure 3.7 – Predicted price for unlabeled data based on the mean value of the labeled data cluster</span></p>
<p><span class="koboSpan" id="kobo.518.1">As shown in</span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.519.1"> the output, we can predict the </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.520.1">house price for unlabeled data using K-means clustering when there is a scarce training dataset. </span><span class="koboSpan" id="kobo.520.2">Then, we can combine the predicted labeled dataset and the original training dataset to fit the model </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">using regression:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.522.1">
# Combine the labeled and newly predicted data
new_data = pd.concat([labeled_data, unlabeled_data], ignore_index=True)
new_data['price'] = pd.concat([train_labels, \
    pd.Series(predicted_labels)], ignore_index=True)
# Train a new model on the combined data
new_train_data, new_test_data, new_train_labels, new_test_labels = \
    train_test_split(new_data.drop('price', axis=1), \
    new_data['price'], test_size=0.2)
new_regressor = LinearRegression()
new_regressor.fit(new_train_data, new_train_labels)
# Evaluate the performance of the new model on the test data
score = new_regressor.score(new_test_data, new_test_labels)
print("R^2 Score: ", score)</span></pre> <p><span class="koboSpan" id="kobo.523.1">Overall, we have</span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.524.1"> seen how clustering can be used in </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.525.1">unsupervised learning to generate labels for unlabeled data. </span><span class="koboSpan" id="kobo.525.2">By computing the average label value for each cluster, we can effectively assign labels to the unlabeled data points based on their similarity to the </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">labeled data.</span></span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.527.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.528.1">In this chapter, we have explored a range of techniques to tackle the challenge of data labeling in regression tasks. </span><span class="koboSpan" id="kobo.528.2">We began by delving into the power of summary statistics, harnessing the mean of each feature in the labeled dataset to predict labels for unlabeled data. </span><span class="koboSpan" id="kobo.528.3">This technique not only simplifies the labeling process but also introduces a foundation for </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">accurate predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.530.1">Further enriching our labeling arsenal, we ventured into semi-supervised learning, leveraging a small set of labeled data to generate pseudo-labels. </span><span class="koboSpan" id="kobo.530.2">The amalgamation of genuine and pseudo-labels in model training not only extends our labeled data but also equips our models to make more informed predictions for </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">unlabeled data.</span></span></p>
<p><span class="koboSpan" id="kobo.532.1">Data augmentation has emerged as a vital tool in enhancing regression data. </span><span class="koboSpan" id="kobo.532.2">Techniques such as scaling and noise injection have breathed new life into our dataset, providing varied instances that empower models to discern patterns better and boost </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">prediction accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.534.1">The utilization of k-means clustering rounded off our exploration, as we ventured into grouping data into clusters and assigning labels based on cluster mean values. </span><span class="koboSpan" id="kobo.534.2">This approach not only saves time but also bolsters the prediction precision of </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">our models.</span></span></p>
<p><span class="koboSpan" id="kobo.536.1">The key takeaways from this chapter are that summary statistics simplify data labeling by leveraging means and distances. </span><span class="koboSpan" id="kobo.536.2">Semi-supervised learning merges genuine and pseudo-labels for comprehensive training. </span><span class="koboSpan" id="kobo.536.3">Data augmentation techniques such as scaling and noise addition enrich and diversify datasets. </span><span class="koboSpan" id="kobo.536.4">K-means clustering optimizes labeling by grouping data into clusters and assigning cluster-wide </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">mean labels.</span></span></p>
<p><span class="koboSpan" id="kobo.538.1">These acquired skills bestow resilience and versatility to our regression models, instilling them with the ability to handle real-world, unlabeled data effectively. </span><span class="koboSpan" id="kobo.538.2">In the next chapter, we’ll delve into the exploratory data analysis of image data in </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">machine learning.</span></span></p>
</div>


<div class="Content" id="_idContainer064">
<h1 id="_idParaDest-77" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.1.1">Part 2: Labeling Image Data</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part of the book, you will learn how to analyze image data, extract features from images, and label images using Python libraries such as Snorkel. </span><span class="koboSpan" id="kobo.2.2">The content also covers various methods of image data augmentation, along with the utilization of </span><strong class="bold"><span class="koboSpan" id="kobo.3.1">support vector machine</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.5.1">SVM</span></strong><span class="koboSpan" id="kobo.6.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">convolutional</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.8.1">neural network</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">CNN</span></strong><span class="koboSpan" id="kobo.11.1">), and pre-trained models such as YOLO for image classification </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">and labeling.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">This part comprises the </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">following chapters:</span></span></p>
<ul>
<li><a href="B18944_04.xhtml#_idTextAnchor081"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.16.1">, </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Exploring Image Data</span></em></li>
<li><a href="B18944_05.xhtml#_idTextAnchor104"><em class="italic"><span class="koboSpan" id="kobo.18.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.19.1">, </span><em class="italic"><span class="koboSpan" id="kobo.20.1">Labeling Image Data Using Rules</span></em></li>
<li><a href="B18944_06.xhtml#_idTextAnchor124"><em class="italic"><span class="koboSpan" id="kobo.21.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.22.1">, </span><em class="italic"><span class="koboSpan" id="kobo.23.1">Labeling Image Data Using Data Augmentation</span></em></li>
</ul>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer065">
</div>
</div>
</body></html>