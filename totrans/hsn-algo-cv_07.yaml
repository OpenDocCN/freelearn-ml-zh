- en: Object Detection – Features and Descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about processing videos and how to perform
    the operations and algorithms from all of the previous chapters on frames read
    from cameras or video files. We learned that each video frame can be treated as
    an individual image, so we can easily use algorithms, such as filtering, on videos
    in almost the same way as we did with images. After learning how to process videos
    using algorithms that work on single individual frames, we moved on to learn about
    video processing algorithms that require a set of consecutive video frames to
    perform object detection, tracking, and so on. We learned about how to use the
    magic of the Kalman filter to improve object-tracking results, and ended the chapter
    by learning about background and foreground extraction.
  prefs: []
  type: TYPE_NORMAL
- en: The object detection (and tracking) algorithms that we learned about in the
    previous chapter rely heavily on the color of an object, which has proven not
    to be too reliable, especially if the object and the environment we are working
    with are not controlled in terms of lighting. We all know that the brightness
    and color of an object can easily (and sometimes extremely) change under sunlight
    and moonlight, or if a light of a different color is near the object, such as
    a red traffic light. These difficulties are the reason why the detection of objects
    is more reliable when their physical shape and features are used as a basis for
    object detection algorithms. Obviously, the shape of an image is independent of
    its color. A circular object will remain circular during the day or night, so
    an algorithm that is capable of extracting the shape of such an object would be
    more reliable to be used for detecting that object.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're going to learn about computer vision algorithms, functions,
    and classes that can be used to detect and recognize objects using their features.
    We'll learn about a number of algorithms that can be used for shape extraction
    and analysis, and then we'll proceed to learning about key-point detection and
    descriptor-extraction algorithms. We'll also learn how to match descriptors from
    two images to detect objects of known shapes in an image. In addition to the topics
    that we just mentioned, this chapter will also include the required functions
    for proper visualization of key points and matching results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you''ll learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Template matching for object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting contours and using them for shape analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating and analyzing contours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting lines and circles using the Hough transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting, descripting, and matching features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An IDE to develop C++ or Python applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenCV library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to [Chapter 2](part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4),
    *Getting Started with OpenCV*, for more information about how to set up a personal
    computer and make it ready for developing computer vision applications using the
    OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following URL to download the source code and examples for
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter07](https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter07).'
  prefs: []
  type: TYPE_NORMAL
- en: Template matching for object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start with the shape-analysis and feature-analysis algorithms, we
    are going to learn about an easy-to-use, extremely powerful method of object detection
    called **template matching**. Strictly speaking, this algorithm does not fall
    into the category of algorithms that use any knowledge about the shape of an object,
    but it uses a previously acquired template image of an object that can be used
    to extract a template-matching result and consequently objects of known look,
    size, and orientation. You can use the `matchTemplate` function in OpenCV to perform
    a templating-matching operation. Here''s an example that demonstrates the complete
    usage of the `matchTemplate` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`method` must be an entry from the `TemplateMatchModes` enum, which can be
    any of the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TM_SQDIFF`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TM_SQDIFF_NORMED`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TM_CCORR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TM_CCORR_NORMED`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TM_CCOEFF`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TM_CCOEFF_NORMED`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For detailed information about each template-matching method, you can refer
    to the OpenCV documentation. For our practical examples, and to learn how the
    `matchTemplate` function is used in practice, it is important to note that each
    method will result in a different type of result, and consequently a different
    interpretation of the result is required, which we''ll learn about in this section.
    In the preceding example, we are trying to detect an object in a scene by using
    an object image and a scene image. Let''s assume the following images are the
    object (left-hand side) and the scene (right-hand side) that we''ll be using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00080.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The very simple idea in template matching is that we are searching for a point
    in the scene image on the right-hand side that has the highest possibility of
    containing the image on the left-hand side, or in other words, the template image.
    The `matchTemplate` function, depending on the method that is used, will provide
    a probability distribution. Let''s visualize the result of the `matchTemplate`
    function to better understand this concept. Another important thing to note is
    that we can only properly visualize the result of the `matchTemplate` function
    if we use any of the methods ending with `_NORMED`, which means they contain a
    normalized result, otherwise we have to use the normalize method to create a result
    that contains values in the displayable range of the OpenCV `imshow` function.
    Here is how it can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This function call will translate all the values in `result` to the range of
    `0.0` and `1.0`, which can then be properly displayed. Here is how the resulting
    image will look if it is displayed using the `imshow` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00081.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As mentioned previously, the result of the `matchTemplate` function and how
    it should be interpreted depends completely on the template matching method that
    is used. In the case that we use the `TM_SQDIFF` or `TM_SQDIFF_NORMED` methods
    for template matching, we need to look for the global minimum point in the result
    (it is shown using an arrow in the preceding image), which has the highest possibility
    of containing the template image. Here''s how we can find the global minimum point
    (along with global maximum, and so on) in the template matching result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the template-matching algorithm works only with objects of a fixed size
    and orientation, we can assume that a rectangle that has an upper-left point that
    is equal to the `minLoc` point and has a size that equals the template image is
    the best possible bounding rectangle for our object. We can draw the result on
    the scene image, for better comparison, using the following sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image depicts the result of the object detection operation that
    was performed using the `matchTemplate` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00082.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we use `TM_CCORR`, `TM_CCOEFF`, or their normalized versions, we must use
    the global maximum point as the point with the highest possibility of containing
    our template image. The following image depicts the result of the `TM_CCOEFF_NORMED`
    method used with the `matchTemplate` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00083.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the brightest point in the resultant image corresponds to the
    upper-left point of the template image in the scene image.
  prefs: []
  type: TYPE_NORMAL
- en: Before ending our template matching lesson, let's also note that the width and
    height of the template matching resultant image is smaller than the scene image.
    This is because the template matching resultant image can only contain the upper-left
    point of the template image, so the template image width and height are subtracted
    from the scene image's width and height to determine the resultant image's width
    and height in the template-matching algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting corners and edges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not always possible to just compare images pixel-wise and decide whether
    an object is present in an image or not, or whether an object has the expected
    shape or not, and many more similar scenarios that we can't even begin to list
    here. That is why the smarter way of interpreting the contents of an image is
    to look for meaningful features in it, and then base our interpretation on the
    properties of those features. In computer vision, a feature is synonymous with
    a keypoint, so don't be surprised if we use them interchangeably in this book.
    In fact, the word keypoint is better suited to describe the concept, since the
    most commonly used features in an image are usually *key points* in that image
    where there is a sudden change in color intensity, which can happen in corners
    and edges of shapes and objects in an image.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll learn about some of the most important and widely used
    keypoint-detection algorithms, namely the corner- and edge-detection algorithms
    that are the basis of almost all of the feature-based object detection algorithms
    we'll be learning about in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the Harris corner-detection algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most well-known corner- and edge-detection algorithms is the Harris
    corner-detection algorithm, which is implemented in the `cornerHarris` function
    in OpenCV. Here is how this function is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`blockSize` determines the width and height of the square block over which
    the Harris corner-detection algorithm will calculate a 2 x 2 gradient-covariance
    matrix. `ksize` is the kernel size of the Sobel operator internally used by the
    Harris algorithm. The preceding example demonstrates one of the most commonly
    used sets of Harris algorithm parameters, but for more detailed information about
    the Harris corner-detection algorithm and its internals mathematics, you can refer
    to the OpenCV documentation. It''s important to note that the `result` object
    from the preceding example code is not displayable unless it is normalized using
    the following example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result of the Harris corner-detection algorithm from the preceding
    example, when normalized and displayed using the OpenCV `imshow` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00084.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The OpenCV library includes another famous corner-detection algorithm, called
    **Good Features to Track** (**GFTT**). You can use the `goodFeaturesToTrack` function
    in OpenCV to use the GFTT algorithm to detect corners, as seen in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this function requires a single-channel image, so, before doing
    anything else, we have converted our BGR image to grayscale. Also, this function
    uses the `maxCorners` value to limit the number of detected corners based on how
    strong they are as candidates, and setting `maxCorners` to a negative value or
    to zero means all detected corners should be returned, which is not a good idea
    if you are looking for the best corners in an image, so make sure you set a reasonable
    value for this based on the environment in which you'll be using it. `qualityLevel`
    is the internal threshold value for accepting detected corners. `minDistance`
    is the minimum allowed distance between returned corners. This is another parameter
    that is completely dependent on the environment this algorithm will be used in.
    You have already seen  the remaining parameters in the previous algorithms from
    this chapter and the preceding one. It's important to note that this function
    also incorporates the Harris corner-detection algorithm, so, by setting `useHarrisDetector`
    to `true`, the resultant features will be calculated using the Harris corner-detection
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have already noticed that the `goodFeaturesToTrack` function returns
    a set of `Point` objects (`Point2f` to be precise) instead of a `Mat` object.
    The returned `corners` vector simply contains the best possible corners detected
    in the image using the GFTT algorithm, so we can use the `drawMarker` function
    to visualize the results properly, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result of the preceding example and detecting corners using the
    `goodFeaturesToTrack` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00085.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can also use the `GFTTDetector` class to detect corners in a similar way
    as you did with the `goodFeaturesToTrack` function. The difference here is that
    the returned type is a vector of `KeyPoint` objects. Many OpenCV functions and
    classes use the `KeyPoint` class to return various properties of detected keypoints,
    instead of just a `Point` object that corresponds to the location of the keypoint.
    Let''s see what this means with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters passed to the `GFTTDetector::create` function are no different
    from the parameters we used with the `goodFeaturesToTrack` function. You can also
    omit all of the given parameters and simply write the following to use the default
    and optimal values for all parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'But let''s get back to the `KeyPoint` class and the result of the `detect`
    function from the previous example. Recall that we used a loop to go through all
    of the detected points and draw them on the image. There is no need for this if
    we use the `GFTTDetector` class, since we can use an existing OpenCV function
    called `drawKeypoints` to properly visualize all of the detected keypoints. Here''s
    how this function is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `drawKeypoints` function goes through all `KeyPoint` objects in the `keypoints`
    vector and draws them using random colors on `image` and saves the result in the `outImg`
    object, which we can then display by calling the `imshow` function. The following
    image is the result of the `drawKeypoints` function when it is called using the
    preceding example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00086.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The `drawKeypoints` function can be provided with an additional (optional) color
    parameter in case we want to use a specific color instead of random colors. In
    addition, we can also provide a flag parameter that can be used to further enhance
    the visualized result of the detected keypoints. For instance, if the flag is
    set to `DRAW_RICH_KEYPOINTS`, the `drawKeypoints` function will also use the size
    and orientation values in each detected keypoint to visualize more properties
    of keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `KeyPoint` object may contain the following properties, depending on the
    algorithm used for calculating it:'
  prefs: []
  type: TYPE_NORMAL
- en: '- `pt`: A `Point2f` object containing the coordinates of the keypoint.'
  prefs: []
  type: TYPE_NORMAL
- en: '- `size`: The diameter of the meaningful keypoint neighborhood.'
  prefs: []
  type: TYPE_NORMAL
- en: '- `angle`: The orientation of the keypoint in degrees, or -1 if not applicable.'
  prefs: []
  type: TYPE_NORMAL
- en: '- `response`: The strength of the keypoint determined by the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '- `octave`: The octave or pyramid layer from which the keypoint was extracted.
    Using octaves allows us to deal with keypoints detected from the same image but
    in different scales. Algorithms that set this value usually require an input octave
    parameter, which is used to define the number of octaves (or scales) of an image
    that is used to extract keypoints.'
  prefs: []
  type: TYPE_NORMAL
- en: '- `class_id`: This integer parameter can be used to group keypoints, for instance,
    when keypoints belong to a single object, they can have the same optional `class_id`
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to Harris and GFTT algorithms, you can also use the FAST corner-detection
    algorithm using the `FastFeatureDetector` class, and the AGAST corner-detection
    algorithm (**Adaptive and Generic Corner Detection Based on the Accelerated Segment
    Test**) using the `AgastFeatureDetector` class, quite similar to how we used the `GFTTDetector`
    class. It's important to note that all of these classes belong to the `features2d`
    module in the OpenCV library and they are all subclasses of the `Feature2D` class,
    therefore all of them contain a static `create` function that creates an instance
    of their corresponding classes and a `detect` function that can be used to extract
    the keypoints from an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example code demonstrating the usage of `FastFeatureDetector` using
    all of its default parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Try increasing the `threshold` value if too many corners are detected. Also,
    make sure to check out the OpenCV documentation for more information about the
    `type` parameter used in the `FastFeatureDetector` class. As mentioned previously,
    you can simply omit all of the parameters in the preceding example code to use
    the default values for all parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `AgastFeatureDetector` class is extremely similar to using `FastFeatureDetector`.
    Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before moving on to edge-detection algorithms, it''s worth noting that OpenCV
    also contains the `AGAST` and `FAST` functions, which can be employed to directly
    use their corresponding algorithms and avoid dealing with creating an instance
    to use them; however, using the class implementation of these algorithms has the
    huge advantage of switching between algorithms using polymorphism. Here''s a simple
    example that demonstrates how we can use polymorphism to benefit from the class
    implementations of corner-detection algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`algorithm`, in the preceding example, is an integer value that can be set
    at run-time and will change the type of the corner-detection algorithm assigned
    to the `detector` object, which has the `Feature2D` type, or in other words, the
    base class of all corner-detection algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Edge-detection algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve gone through corner-detection algorithms, let''s take a look
    at edge-detection algorithms, which are crucial when it comes to shape analysis
    in computer vision. OpenCV contains a number of algorithms that can be used to
    extract edges from images. The first edge-detection algorithm that we''re going
    to learn about is called the **line-segment-detection algorithm**, and it can
    be performed by using the `LineSegmentDetector` class, as seen in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the `LineSegmentDetector` class requires a single-channel image
    as the input and produces a `vector` of lines. Each line in the result is `Vec4f`,
    or four floating-point values that represent *x1*, *y1*, *x2*, and *y2* values,
    or in other words, the coordinates of the two points that form each line. You
    can use the `drawSegments` function to visualize the result of the `detect` function
    of the `LineSegmentDetector` class, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To have more control over how the resultant lines are visualized, you might
    want to manually draw the lines vector, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image demonstrates the result of the line-segment-detection algorithm
    that was used in the preceding example codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00087.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For more details about how to customize the behavior of the `LineSegmentDetector`
    class, make sure to view the documentation of `createLineSegmentDetector` and
    its parameters. In our example, we simply omitted all of its input parameters
    and used the `LineSegmentDetector` class with the default values set for its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another function of the `LineSegmentDetector` class is comparing two sets of
    lines to find the number of non-overlapping pixels, and at the same time drawing
    the result of the comparison on an output image for visual comparison. Here''s
    an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `imageSize` is a `Size` object that contains the size
    of the input image where the lines were extracted from. The result is an integer
    value that contains the result of the comparison function, or the `compareSegments`
    function, which will be zero in the case of the complete overlapping of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next edge-detection algorithm is probably one of the most widely used and
    cited edge-detection algorithms in computer vision, called the **Canny algorithm**,
    which has a function of the same name in OpenCV. The biggest advantage of the
    `Canny` function is the simplicity of its input parameters. Let''s first see an
    example of how it''s used, and then walk through its details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The threshold values (`threshold1` and `threshold2`) are the lower and higher
    bound values for thresholding the input image. `apertureSize` is the internal
    Sobel operator aperture size, and `L2gradient` is used to enable or disable the
    more accurate L2 norm when calculating the gradient image. The result of the `Canny`
    function is a grayscale image that contains white pixels where edges are detected
    and black pixels for the rest of the pixels. This makes the result of the `Canny`
    function a suitable mask wherever such a mask is needed, or, as you'll see later
    on, a suitable set of points to extract contours from.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image depicts the result of the `Canny` function used in the
    previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00088.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we mentioned before, the result of the `Canny` function is suitable to use
    as the input to algorithms that require a binary image, or in other words, a grayscale
    image containing only absolute black and absolute white pixel values. The next
    algorithm that we'll learn about is one where the result of a previous `Canny`
    function must be used as the input, and it is called the **Hough transformation**.
    The Hough transformation can be used to extract lines from an image, and it is
    implemented in a function called `HoughLines` in the OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a complete example that demonstrates how the `HoughLines` function
    is used in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `Canny` function to detect edges in the input image, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Call the `HoughLines` function to extract lines from the detected edges:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to extract points in the standard coordinate system,
    and draw them over the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following images depict the result of the preceding example from left to
    right, starting with the original image, edges detected using the `Canny` function,
    lines detected using the `HoughLines` function, and finally the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To avoid having to deal with the coordinate-system change, you can use the `HoughLinesP`
    function to directly extract the points forming each detected line. Here''s an
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The Hough transformation is extremely powerful, and OpenCV contains more variations
    of the Hough transformation algorithm that we'll leave for you to discover using
    the OpenCV documentation and online resources. Note that using the Canny algorithm
    is a prerequisite of the Hough transformation, and, as you'll see in the next
    section, a prerequisite of many algorithms that deal with the shape of objects
    in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Contour calculation and analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contours of shapes and objects in an image are an important visual property
    that can be used to describe and analyze them. Computer vision is no exception,
    so there are quite a few algorithms in computer vision that can be used to calculate
    the contours of objects in an image or calculate their area and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image depicts two contours that are extracted from two 3D objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV includes a function, called `findContours`, that can be used to extract
    contours from an image. This function must be provided with a proper binary image
    that contains the best candidate pixels for contours; for instance, the result
    of the `Canny` function is a good choice. The following example demonstrates the
    steps required to calculate the contours of an image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the edges using the `Canny` function, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `findContours` function to calculate the contours using the detected
    edges. It''s worth noting that each contour is a `vector` of `Point` objects,
    making all contours a `vector` of `vector` of `Point` objects, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the contour-retrieval mode is set to `CV_RETR_TREE`
    and the contour-approximation method is set to `CV_CHAIN_APPROX_TC89_KCOS`. Make
    sure to go through the list of possible modes and methods by yourself, and compare
    the results to find the best parameters for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'A common method of visualizing detected contours is by using the `RNG` class,
    or the Random Number Generator class, to generate random colors for each detected
    contour. The following example demonstrates how you can use the `RNG` class in
    combination with the `drawContours` function to properly visualize the result
    of the `findContours` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image demonstrates the result of the `Canny` and `findContours`
    functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note the different colors in the image on the right-hand side, which correspond
    to one complete contour detected by using the `findContours` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'After calculating the contours, we can use contour-analysis functions to further
    modify them or analyze the shape of the object in an image. Let''s start with
    the `contourArea` function, which can be used to calculate the area of a given
    contour. Here is how this function is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use area as a threshold for ignoring the detected contours that do
    not pass certain criteria. For example, in the preceding example code where we
    used the `drawContours` function, we could get rid of contours with smaller areas
    than some predefined threshold value. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Setting the second parameter of the `contourArea` function (which is a Boolean
    parameter) to `true` would result in the orientation being considered in the contour
    area, which means you can get positive or negative values of the area depending
    on the orientation of the contour.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another contour-analysis function that can be quite handy is the `pointPolygonTest`
    function. As you can guess from its name, this function is used to perform a point-in-polygon
    test, or in other words, a point-in-contour test. Here is how this function is
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If the result is zero, it means the test point is right on the edge of the contour.
    A negative result would mean the test point is outside, and a positive result
    would mean the test point is inside the contour. The value itself is the distance
    between the test point and the nearest contour edge.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be able to check whether a contour is convex or not, you can use the `isContourConvex`
    function, as seen in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Being able to compare two contours with each other is probably the most essential
    algorithm that you''ll need when dealing with contour and shape analysis. You
    can use the `matchShapes` function in OpenCV to compare and try to match two contours.
    Here is how this function is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`method` can take any of the following values, while the last parameter must
    always be set to zero, unless specified by the method used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CONTOURS_MATCH_I1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTOURS_MATCH_I2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTOURS_MATCH_I3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For details about the mathematical difference between the preceding list of
    contour-matching methods, you can refer to the OpenCV documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Being able to find the boundaries of a contour is the same as being able to
    correctly localize it, for instance to find a region that can be used for tracking
    or performing any other computer vision algorithm. Let''s assume we have the following
    image and its single contour detected using the `findContours` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Having this contour, we can perform any of the contour- and shape-analysis
    algorithms that we''ve learned about. In addition, we can use a number of OpenCV
    functions to localize the extracted contour. Let''s start with the `boundingRect`
    function, which is used to find the minimal upright rectangle (`Rect` object)
    that contains a given point set or contour. Here''s how this function is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of drawing the upright rectangle acquired by using
    `boundingRect` in the preceding sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, you can use the `minAreaRect` function to find the minimal rotated
    rectangle that contains a given set of points or a contour. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the following code to visualize the resultant rotated rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You can draw an ellipse instead, using the `ellipse` function, or you can do
    both, which would result in something similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition to algorithms for finding the minimal upright and rotated bounding
    rectangles of contours, you can also use the `minEnclosingCircle` and `minEnclosingTriangle`
    functions to find the minimal bounding circle and rectangle of a given set of
    points or a contour. Here''s an example of how these functions can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: There is no end to the list of possible use cases of contours, but we will name
    just a few of them before moving on to the next section. You can try using contour-detection
    and shape-analysis algorithms in conjunction with thresholding algorithms or back-projection
    images, for instance, to make sure your tracking algorithm uses the shape information
    in addition to the color and intensity values of pixels. You can also use contours
    to count and analyze shapes of objects on a production line, where the background
    and the visual environment is more controlled.
  prefs: []
  type: TYPE_NORMAL
- en: The final section of this chapter will teach you how to use feature detection,
    descriptor extraction, and descriptor-matching algorithms to detect known objects,
    but with rotation, scale, and even perspective invariance.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting, descripting, and matching features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we learned earlier in this chapter, features or keypoints can be extracted
    from images using various feature-extraction (detection) algorithms, most of which
    rely on detecting points with a significant change in intensity, such as corners.
    Detecting the right keypoints is the same as being able to correctly determine
    which parts of an image are helpful in identifying it. But just a keypoint, or
    in other words the location of a significant point in an image, by itself is not
    useful. One might argue that the collection of keypoint locations in an image
    is enough, but even then, another object with a totally different look can have
    keypoints in the exact same locations in an image, say, by chance.
  prefs: []
  type: TYPE_NORMAL
- en: This is where feature descriptors, or simply descriptors, come into play. A
    descriptor, as you can guess from the name, is an algorithm-dependent method of
    describing a feature, for instance, by using its neighboring pixel values, gradients,
    and so on. There are many different descriptor-extraction algorithms, each one
    with its own advantages and disadvantages, and going through all of them would
    not be a fruitful endeavor, especially for a hands-on book, but it's worth noting
    that most of them simply take a list of keypoints and produce a vector of descriptors.
    After a set of descriptors are extracted from sets of keypoints, we can use descriptor-matching
    algorithms to find the matching features from two different images, for instance,
    an image of an object and a scene where that object exists.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV contains a large number of feature detectors, descriptor extractors,
    and descriptor matchers. All feature-detector and descriptor-extractor algorithms
    in OpenCV are subclasses of the `Feature2D` class, and they are located in either
    the `features2d` module, which is included by default in OpenCV packages, or the `xfeatures2d`
    (extra module) module. You should use these algorithms with care and always refer
    to the OpenCV documentation, since some of them are actually patented and require
    permission from their owners to be used in commercial projects. The following
    is a list of some of the main feature-detector and descriptor-extractor algorithms
    that are included in OpenCV by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BRISK** (**Binary Robust Invariant Scalable Keypoints**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KAZE**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AKAZE** (**Accelerated KAZE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORB**, or Oriented **BRIEF** (**Binary Robust Independent Elementary Features**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these algorithms are implemented in classes of exactly the same title
    in OpenCV, and to repeat once more, they are all subclasses of the `Feature2D`
    class. They are extremely simple to use, especially when no parameters are modified.
    In all of them, you can simply use the static `create` method to create an instance
    of them, call the `detect` method to detect the keypoints, and finally call `computer`
    to extract descriptors of the detected keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for descriptor-matcher algorithms, OpenCV contains the following matching
    algorithms by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FLANNBASED`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BRUTEFORCE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BRUTEFORCE_L1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BRUTEFORCE_HAMMING`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BRUTEFORCE_HAMMINGLUT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BRUTEFORCE_SL2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the `DescriptorMatcher` class, or its subclasses, namely `BFMatcher`
    and `FlannBasedMatcher`, to perform various matching algorithms. You simply need
    to use the static `create` method of these classes to create an instance of them,
    and then use the `match` method to match two sets of descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through all of what we''ve discussed in this section with a complete
    example, since breaking apart the feature detection, descriptor extraction, and
    matching is impossible, and they are all parts of a chain of processes that lead
    to the detection of an object in a scene using its features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the image of the object, and the scene that will be searched for the object,
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following picture, let''s assume the image on the left is the object
    we are looking for, and the image on the right is the scene that contains the
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Extract the keypoints from both of these images, which are now stored in `object`
    and `scene`. We can use any of the aforementioned algorithms for feature detection,
    but let''s assume we''re using KAZE for our example, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the keypoints of both the object image and the scene image. We can
    go ahead and view them using the `drawKeypoints` function, as we learned previously
    in this chapter. Try that on your own, and then use the same `KAZE` class to extract
    descriptors from the keypoints. Here''s how it''s done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`objDesc` and `scnDesc` correspond to the descriptors of the keypoints extracted
    from the object and scene images. As mentioned previously, descriptors are algorithm-dependent,
    and interpreting the exact values in them requires in-detail knowledge about the
    specific algorithm that was used to extract them. Make sure to refer to the OpenCV
    documentation to gain more knowledge about them, however, in this step, we''re
    going to simply use a brute-force matcher algorithm to match the descriptors extracted
    from both images. Here''s how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The `BFMatcher` class, which is a subclass of the `DescriptorMatcher` class,
    implements the brute-force matching algorithm. The result of descriptor matching
    is stored in a `vector` of `DMatch` objects. Each `DMatch` object contains all
    the necessary information for matched features, from the object descriptors to
    the scene descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now try to visualize the result of matching by using the `drawMatches`
    function, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, some of the matched features are obviously incorrect, some
    at the top of the scene image and a few at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The bad matches can be filtered out by using a threshold on the `distance`
    value of the `DMatch` objects. The threshold value depends on the algorithm and
    the type of image content, but in our example case, and with the KAZE algorithm,
    a value of `0.1` seems to be enough for us. Here''s how the thresholding is done
    to get good matches out of all the matches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image depicts the result of the `drawMatches` function on the `goodMatches`
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Obviously, the result of the filtered matches is much better now. We can use
    the `findHomography` function to find the transformation between good matched
    keypoints from the object image to the scene image. Here''s how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'As we''ve already seen in the preceding chapters, the result of the `findHomography`
    function can be used to transform a set of points. We can abuse this fact to create
    four points using the four corners of the object image, and then transform those
    points using the `perspectiveTransform` function to get the location of those
    points in the scene image. Here is an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformed points can be used to draw four lines that localize the detected
    object in the scene image, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s important to also change the *x* values of the resultant points to account
    for the width of the object image, if you are going to draw the four lines that
    localize the object, over the `drawMatches` image result. Here''s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image depicts the final result of our detection operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Make sure to try the rest of the feature-detection, descriptor-extraction,
    and matching algorithms by yourself and compare their results. Also, try to measure
    the time of the calculations for each one. For instance, you might notice that
    AKAZE is much faster than KAZE, or that BRISK is better suited to some images,
    while KAZE or ORB is better with others. As mentioned before, the feature-based
    methods of object detection are much more reliable with scale, rotation, and even
    perspective change. Try different views of the same objects to figure out the
    best parameters and algorithms for your own project and use case. For instance,
    here''s another example that demonstrates the rotation and scale invariance of
    the AKAZE algorithm and brute-force matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00099.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the source code used for producing the preceding output is created
    by using exactly the same set of instructions we went through in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by learning about a template-matching algorithm and
    an object detection algorithm that, despite its popularity, lacks some of the
    most essential aspects of a proper object detection algorithm, such as scale and
    rotation invariance; moreover, it's a pure pixel-based object detection algorithm.
    Building upon that, we learned how to use global maximum- and minimum-detection
    algorithms to interpret the template-matching algorithm result. Then, we learned
    about corner- and edge-detection algorithms, or in other words, algorithms that
    detect points and areas of significance in images. We learned how to visualize
    them, and then moved on to learn about contour-detection and shape-analysis algorithms.
    The final section of this chapter included a complete tutorial on how to detect
    keypoints in an image, extract descriptors from those keypoints, and use matcher
    algorithms to detect an object in a scene. We're now familiar with a huge set
    of algorithms that can be used to analyze images based not only on their pixel
    colors and intensity values, but also their content and existing keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: The final chapter of this book will take us through computer vision and machine
    learning algorithms in OpenCV and how they are employed to detect objects using
    a previously existing set of their images, among many other interesting artificial-intelligence-related
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The template-matching algorithm is not scale- and rotation-invariant by itself.
    How can we make it so for a) double the scale of the template image, and b) a
    90-degrees-rotated version of the template image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `GFTTDetector` class to detect keypoints with the Harris corner-detection
    algorithm. You can set any values for the corner-detection algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Hough transformation can also be used to detect circles in an image, using
    the `HoughCircles` function. Search for it in the OpenCV documentation and write
    a program to detect circles in an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect and draw the convex contours in an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `ORB` class to detect keypoints in two images, extract their descriptors,
    and match them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which feature-descriptor-matching algorithm is incompatible with the ORB algorithm,
    and why?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use the following OpenCV functions and the given sample to calculate
    the time required to run any number of lines of code. Use it to calculate the
    time it takes for the matching algorithms on your computer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
