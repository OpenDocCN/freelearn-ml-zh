- en: Feature Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our final chapter, where we will be exploring feature engineering techniques,
    we will be taking a look at what is likely the most powerful feature engineering
    tool at our disposal. Feature learning algorithms are able to take in cleaned
    data (yes, you still need to do some work) and create brand-new features by exploiting
    latent structures within data. If all of this sounds familiar, that is because
    this is the description that we used in the previous chapter for feature transformations.
    The differences between these two families of algorithms are in the *parametric*
    assumptions that they make when attempting to create new features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Parametric assumptions of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BernoulliRBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting RBM components from MNIST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using RBMs in a machine learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning text features—word vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parametric assumptions of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we say **parametric assumptions**, we are referring to base assumptions
    that algorithms make about the *shape* of the data. In the previous chapter, while
    exploring **principal component analysis** (**PCA**), we discovered that the end
    result of the algorithm produced components that we could use to transform data
    through a single matrix multiplication. The assumption that we were making was
    that the original data took on a shape that could be decomposed and represented
    by a single linear transformation (the matrix operation). But what if that is
    not true? What if PCA is unable to extract *useful* features from the original
    dataset? Algorithms such as PCA and **linear discriminate analysis** (**LDA**)
    will always be able to find features, but they may not be useful at all. Moreover,
    these algorithms rely on a predetermined equation and will always output the same
    features each and every time they are run. This is why we consider both LDA and
    PCA as being *linear transformations.*
  prefs: []
  type: TYPE_NORMAL
- en: Feature learning algorithms attempt to solve this issue by removing that parametric
    assumption. They do not make any assumptions about the shape of the incoming data
    and rely on *stochastic learning*. This means that, instead of throwing the same
    equation at the matrix of data every time, they will attempt to figure out the
    best features to extract by looking at the data points over and over again (in
    epochs) and converge onto a solution (potentially different ones at runtime).
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on how stochastic learning (and stochastic gradient descent)
    works, please refer to the *Principles of Data Science*, by Sinan Ozdemir at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science](https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science)'
  prefs: []
  type: TYPE_NORMAL
- en: This allows feature learning algorithms to bypass the parametric assumption
    made by algorithms such as PCA and LDA and opens us up to solve much more difficult
    questions than we could previously in this text. Such a complex idea (bypassing
    parametric assumptions) requires the use of complex algorithms. *Deep learning*
    algorithms are the choice of many data scientists and machine learning to learn
    new features from raw data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will assume that the reader has a basic familiarity with the neural network
    architecture in order to focus on applications of these architectures for feature
    learning. The following table summarizes the basic differences between feature
    learning and transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Parametric?** | **Simple to use?** | **Creates new feature set?** |
    **Deep learning?** |'
  prefs: []
  type: TYPE_TB
- en: '| Feature transformation algorithms | Yes | Yes | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| Feature learning algorithms | No | No (usually) | Yes | Yes (usually) |'
  prefs: []
  type: TYPE_TB
- en: 'The fact that both feature learning and feature transformation algorithms create
    new feature sets means that we regard both of them as being under the umbrella
    of *feature extraction. *The following figure shows this relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4587fa19-778b-42ac-b14d-65c784ba9a66.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature extraction as a superset of feature learning and feature transformation.
    Both families of algorithms work to exploit latent structure in order to transform
    raw data into a new feature set
  prefs: []
  type: TYPE_NORMAL
- en: Both **feature learning** and **feature transformation** fall under the category
    of feature extraction as they are both trying to create a new feature set from
    the latent structure of raw data. The methods in which they are allowed to work,
    though, are the main differentiators.
  prefs: []
  type: TYPE_NORMAL
- en: Non-parametric fallacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to mention that a model being non-parametric doesn't mean that
    there are no assumptions at all made by the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: While the algorithms that we will be introducing in this chapter forgo the assumption
    on the shape of the data, they still may make assumptions on other aspects of
    the data, for example, the values of the cells.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms of this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on two feature learning areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Restricted Boltzmann Machines** (**RBM**): A simple deep learning architecture
    that is set up to learn a set number of new dimensions based on a probabilistic
    model that data follows. These machines are in fact a family of algorithms with
    only one implemented in scikit-learn. The **BernoulliRBM **may be a non-parametric
    feature learner; however, as the name suggests, some expectations are set as to
    the values of the cells of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word embeddings**: Likely one of the biggest contributors to the recent deep
    learning-fueled advancements of natural language processing/understanding/generation
    is the ability to project strings (words and phrases) into an n-dimensional feature
    set in order to grasp context and minute detail in wording. We will use the `gensim` Python
    package to prepare our own word embeddings and then use pre-trained word embeddings
    to see some examples of how these word embeddings can be used to enhance the way
    we interact with text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these examples have something in common. They all involve learning brand
    new features from raw data. They then use these new features to enhance the way
    that they interact with data. For the latter two examples, we will have to move
    away from scikit-learn as these more advanced techniques are not (yet) implemented
    in the latest versions. Instead, we will see examples of deep learning neural
    architectures implemented in TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: For all of these techniques, we will be focusing less on the very low-level
    inner workings of the models, and more on how they work to interpret data. We
    will go in order and start with the only algorithm that has a scikit-learn implementation,
    the restricted Boltzmann machine family of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RBMs are a family of unsupervised feature learning algorithms that use probabilistic
    models to learn new features. Like PCA and LDA, we can use RBMs to extract a new
    feature set from raw data and use them to enhance machine learning pipelines.
    The features that are extracted by RBMs tend to work best when followed by linear
    models such as linear regression, logistic regression, perceptron's, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised nature of RBMs is important as they are more similar to PCA
    algorithms than they are to LDA. They do not require a ground-truth label for
    data points to extract new features. This makes them useful in a wider variety
    of machine learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, RBMs are shallow (two-layer) neural networks. They are thought
    to be the building blocks of a class of algorithms called **Deep Belief Networks**
    (**DBN**). Keeping with standard terminology, there is a visible layer (the first
    layer), followed by a hidden layer (the second layer). These are the only two
    layers of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31045c70-ef67-4429-9f70-4dc418d6373c.png)'
  prefs: []
  type: TYPE_IMG
- en: The setup for a restricted Boltzmann Machine. The circles represent nodes in
    the graph
  prefs: []
  type: TYPE_NORMAL
- en: Like any neural network, we have nodes in our two layers. The first visible
    layer of the network has as many layers as the input feature dimension. In our
    upcoming example, we will be working with 28 x 28 images necessitating 784 (28
    x 28) nodes in our input layer. The number of nodes in the hidden layer is a human-chosen
    number and represents the number of features that we wish to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Not necessarily dimension reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In PCA and LDA, we had severe limits to the number of components we were allowed
    to extract. For PCA, we were capped by the number of original features (we could
    only use less than or equal to the number of original columns), while LDA enforced
    the much stricter imposition that caps the number of extracted features to the
    number of categories in the ground truth minus one.
  prefs: []
  type: TYPE_NORMAL
- en: The only restriction on the number of features RBMs are allowed to learn is
    that they are limited by the computation power of the computer running the network
    and human interpretation. RBMs can learn fewer or *more* features than we originally
    began with. The exact number of features to learn is up to the problem and can
    be gridsearched.
  prefs: []
  type: TYPE_NORMAL
- en: The graph of a Restricted Boltzmann Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have seen the visible and hidden layers of RBMs, but we have not
    yet seen how they learn features. Each of the visible layer''s nodes take in a
    single feature from the dataset to be learned from. This data is then passed from
    the visible layer to the hidden layer through weights and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfe9aba3-88cd-4492-972b-e9df0618da42.png)'
  prefs: []
  type: TYPE_IMG
- en: This visualization of an RBM shows the movement of a single data point through
    the graph through a single hidden node
  prefs: []
  type: TYPE_NORMAL
- en: The preceding visualization of an RBM shows the movement of a single data point
    through the graph and through a single hidden node. The visible layer has four
    nodes, representing the four columns of the original data. Each arrow represents
    a single feature of the data point moving through the four visible nodes in the
    first layer of the RBM. Each of the feature values is multiplied by a weight associated
    to that feature and are added up together. This calculation can also be summed
    up by a dot product between an input vector of data and a weight vector. The resulting
    weighted sum of the data is added to a bias variable and sent through an activation
    function (sigmoidal is popular). The result is stored in a variable called `a`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example in Python, this code shows how a single data point (`inputs`)
    is multiplied by our `weights` vector and combined with the `bias` variable to
    create the activated variable, `a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In a real RBM, each of the visible nodes is connected to each of the hidden
    nodes, and it looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd0adfcc-12be-4d6c-91f0-8834abe52bc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Because inputs from each visible node are passed to every single hidden node,
    an RBM can be defined as a **symmetrical bipartite graph**. The symmetrical part
    comes from the fact that the visible nodes are all connected with each hidden
    node. Bipartite means it has two parts (layers).
  prefs: []
  type: TYPE_NORMAL
- en: The restriction of a Boltzmann Machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our two layers of visible and hidden nodes, we have seen the connection
    between the layers (inter-layer connections), but we haven't seen any connections
    between nodes in the same layer (intra-layer connections). That is because there
    aren't any. The restriction in the RBM is that we do not allow for any intra-layer
    communication. This lets nodes independently create weights and biases that end
    up being (hopefully) independent features for our data.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this forward pass of the network, we can see how data goes forward through
    the network (from the visible layer to the hidden layer), but that doesn't explain
    how the RBM is able to learn new features from our data without ground truths.
    This is done through multiple forward and backward passes through the network
    between our visible and hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the reconstruction phase, we switch the network around and let the hidden
    layer become the input layer and let it feed our activation variables (`a`) backwards
    into the visible layer using the same weights, but a new set of biases. The activated
    variables that we calculated during the forward pass are then used to reconstruct
    the original input vectors. The following visualization shows us how activations
    are fed backwards through our graph using the same weights and different biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20e88959-47cd-4c59-a7ea-f20841310bdf.png)'
  prefs: []
  type: TYPE_IMG
- en: This becomes the network's way of evaluating itself. By passing the activations
    backwards through the network and obtaining an approximation of the original input,
    the network can adjust the weights in order to make the approximations closer
    to the original input. Towards the beginning of training, because the weights
    are randomly initialized (this is standard practice), the approximations will
    likely be very far off. Backpropagation through the network, which occurs in the
    same direction as our forward pass (confusing, we know), then adjusts the weights
    to minimize the distance between original input and approximations. This process
    is then repeated until the approximations are as close to the original input as
    possible. The number of times this back and forth process occurs is called the
    number of **iterations**.
  prefs: []
  type: TYPE_NORMAL
- en: The end result of this process is a network that has an *alter-ego* for each
    data point. To transform data, we simply pass it through the network and retrieve
    the activation variables and call those the new features. This process is a type
    of *generative learning* that attempts to learn a probability distribution that
    generated the original data and exploit knowledge to give us a new feature set
    of our raw data.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we were given an image of a digit and asked to classify which
    digit (0-9) the image was of, the forward pass of the network asks the question,
    given these pixels, what digit should I expect? On the backwards pass, the network
    is asking given a digit, what pixels should I expect? This is called a **joint
    probability** and it is the simultaneous probability of *y *given *x *and *x *given *y,*
    and it is expressed as the shared weights between the two layers of our network.
  prefs: []
  type: TYPE_NORMAL
- en: Let's introduce our new dataset and let it elucidate the usefulness of RBMs
    in feature learning.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `MNIST` dataset consists of 6,000 images of handwritten digits between zero
    and nine and a ground-truth label to learn from. It is not unlike most of the
    other datasets that we have been working with in that we are attempting to fit
    a machine learning model to classify a response variable given a set of data points.
    The main difference here is that we are working with very low-level features as
    opposed to more interpretable features. Each data point will consist of 784 features
    (pixel values in a grey-scale image).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin with our imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The new import is the BernoulliRBM, which is the only RBM implementation in
    scikit-learn as of now. As the name suggests, we will have to do a small amount
    of preprocessing to ensure that our data complies with the assumptions required.
    Let''s import our dataset directly into a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify the number of rows and columns that we are working with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The 785 is comprised of 784 pixels and a single response column in the beginning
    (first column). Every column besides the response column holds a value between
    zero and 255 representing pixel intensity, where zero means a white background
    and 255 means a fully black pixel. We can extract the `X` and `y` variables from
    the data by separating the first column from the rest of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If we take a look at the first image, we will see what we are working with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/396ff5d9-160d-431d-88cd-0ea40f3db828.png)'
  prefs: []
  type: TYPE_IMG
- en: Looking good. Because the scikit-learn implementation of Restricted Boltzmann
    Machines will not allow for values outside of the range of 0-1, we will have to
    do a bit of preprocessing work.
  prefs: []
  type: TYPE_NORMAL
- en: The BernoulliRBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The only scikit-learn implemented version of a Restricted Boltzmann Machine is
    called **BernoulliRBM** because it imposes a constraint on the type of probability
    distribution it can learn. The Bernoulli distribution allows for data values to
    be between zero and one. The scikit-learn documentation states that the model
    *assumes the inputs are either binary values or values between zero and one*.
    This is done to represent the fact that the node values represent a probability
    that the node is activated or not. It allows for quicker learning of feature sets.
    To account for this, we will alter our dataset to account for only hardcoded white/black
    pixel intensities. By doing so, every cell value will either be zero or one (white
    or black) to make learning more robust. We will accomplish this in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We will scale the values of the pixels to be between zero and one
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will change the pixel values in place to be true if the value is over `0.5`,
    and false otherwise
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s start by scaling the pixel values to be between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the same number five digit, as we did previously, with
    our newly altered pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8c8f0b8-57a4-43e1-87ed-bb6374c77d48.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the fuzziness of the image has disappeared and we are left with
    a very crisp digit to classify with. Let's try now to extract features from our
    dataset of digits.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting PCA components from MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we move to our RBM, let's take a look at what happens when we apply a
    PCA to our dataset. Like we did in the last chapter, we will take our features
    (784 pixels that are either on or off) and apply an eigenvalue decomposition to
    the matrix to extract *eigendigits* from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take 100 components from the possible 784 and plot the components to
    see what the extracted features look like. We will do this by importing our PCA
    module, fitting it to our data with 100 components, and creating a matplotlib
    gallery to display the top 100 components available to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the plot of the preceding code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e10bb9a-dc1c-492f-90ba-2c523541c708.png)'
  prefs: []
  type: TYPE_IMG
- en: This gallery of images is showing us what the eigenvalues of the covariance
    matrix look like when reshaped to the same dimensions as the original images.
    This is an example of what extracted components look like when we focus our algorithms
    on a dataset of images. It is quite interesting to take a sneak peek into how
    the PCA components are attempting to grab linear transformations from the dataset.
    Each component is attempting to understand a certain "aspect" of the images that
    will translate into interpretable knowledge. For example, the first (and most
    important) eigen-image is likely capturing an images 0-quality that is, how like
    a 0 the digit looks.
  prefs: []
  type: TYPE_NORMAL
- en: It also is evident that the first ten components seem to retain some of the
    shape of the digits and after that, they appear to start devolving into what looks
    like nonsensical images. By the end of the gallery, we appear to be looking at
    random assortments of black and white pixels swirling around. This is probably
    because PCA (and also LDA) are parametric transformations and they are limited
    in the amount of information they can extract from complex datasets like images.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a look to see how much variance the first 30 components are explaining,
    we would see that they are able to capture the majority of the information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that the first few dozen components are doing a good job at capturing
    the essence of the data, but after that, the components are likely not adding
    too much.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be further seen in a scree plot showing us the cumulative explained
    variance for our PCA components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the plot of the scree plot where the number of PCA components
    are on the *x* axis and the amount of cumulative variance explained lives on the
    *y* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01e3257b-e9f3-430a-ad2a-39f16bcf8401.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we saw in the previous chapter, the transformations made by PCA are done
    through a single linear matrix operation by multiplying the components attribute
    of the PCA module with the data. We will show this again by taking the scikit-learn
    PCA object that we fit to 100 features and using it to transform a single MNIST
    image. We will take that transformed image and compare it the result of multiplying
    the original image with the `components_` attribute of the PCA module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Extracting RBM components from MNIST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now create our first RBM in scikit-learn. We will start by instantiating
    a module to extract 100 components from our `MNIST` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also set the verbose parameter to `True` to allow us visibility into
    the training process as well as the `random_state` parameter to `0`. The `random_state`
    parameter is an integer that allows for reproducibility in code. It fixes the
    random number generator and sets the weights and biases *randomly* at the same
    time, every time. We finally let `n_iter` be `20`. This is the number of iterations
    we wish to do, or back and forth passes of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once training is complete; we can explore the end result of the process. RBM
    also has a `components` module, like PCA does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot the RBM components that were learned by the module to see
    how they differ from our eigendigits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0691218-0d6d-463d-94a3-05cdd4ef3c01.png)'
  prefs: []
  type: TYPE_IMG
- en: These features look very interesting. Where the PCA components became visual
    distortions after a while, the RBM components seem to be extracting various shapes
    and pen strokes with each component. At first glance, it looks like we have repeat
    features (for example, feature 15, 63, 64, and 70). We can do a quick NumPy check
    to see if any of the components are actually repeating, or if they are just very
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code will check to see how many unique elements exist in `rbm.components_`.
    If the resulting shape has 100 elements in it, that means that every component
    of the RBM is in fact different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This validates that our components are all unique from one another. We can
    use the RBM to transform data like we can with PCA by utilizing the `transform`
    method within the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can also see that these components are **not** used in the same way
    as PCAs are, meaning that a simple matrix multiplication will not yield the same
    transformation as invoking the `transform` method embedded within the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know that we have 100 new features to work with and we've seen them,
    let's see how they interact with our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by grabbing the `20` most represented features for the first image
    in our dataset, the digit 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we actually have seven features in which the RBM has a full 100%.
    In our graph, this means that passing in these 784 pixels into our visible layers
    lights up nodes 56, 63, 62, 14, 69, 83, and 82 at full capacity. Let''s isolate
    these features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae5253eb-ec7b-4787-9a97-02b40badec81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking a look at some of these, they make quite a lot of sense. **Component
    45** seems to isolate the top-left corner of the digit **5**, while **Component
    21** seems to grab the bottom loop of the digit. **Component 82** and **Component
    34** seem to grab almost an entire 5 in one go. Let''s see what the bottom of
    the barrel looks like for the number 5 by isolating the bottom 20 features that
    lit up in the RBM graph when these pixels were passed through:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e54d08a5-bbb2-4253-934f-e637c13655ce.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Component 13**, **Component 4**, **Component 97**, and others seem to be
    trying to reveal different digits and not a 5, so it makes sense that these components
    are not being lit up by this combination of pixel strengths.'
  prefs: []
  type: TYPE_NORMAL
- en: Using RBMs in a machine learning pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Of course, we want to see how the RBM performs in our machine learning pipelines
    to not just visualize the workings of the model, but to see concrete results of
    the feature learning. To do this, we will create and run three pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: A logistic regression model by itself running on the raw pixel strengths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A logistic regression running on extracted PCA components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A logistic regression running on extracted RBM components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these pipelines will be grid-searched across a number of components
    (for PCA and RBM) and the `C` parameter for logistic regression. Let's start with
    our simplest pipeline. We will run the raw pixel values through a logistic regression
    to see if the linear model is enough to separate out the digits.
  prefs: []
  type: TYPE_NORMAL
- en: Using a linear model on raw pixel values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin, we will run the raw pixel values through a logistic regression model
    in order to obtain something of a baseline model. We want to see if utilizing
    PCA or RBM components will allow the same linear classifier to perform better
    or worse. If we can find that the extracted latent features are performing better
    (in terms of accuracy of our linear model) then we can be sure it is the feature
    engineering that we are employing that is enhancing our pipeline, and nothing
    else.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will create our instantiated modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we done this, we can fit our module to our raw image data. This will give
    us a rough idea of how the raw pixel data performs in a machine learning pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression by itself does a decent job at using the raw pixel values
    to identify digits by giving about an **88.75% cross-validated accuracy**.
  prefs: []
  type: TYPE_NORMAL
- en: Using a linear model on extracted PCA components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see if we can add in a PCA component to the pipeline to enhance this
    accuracy. We will begin again by setting up our variables. This time we will need
    to create a scikit-learn pipeline object to house the PCA module as well as our
    linear model. We will keep the same parameters that we used for the linear classifier
    and add new parameters for our PCA. We will attempt to find the optimal number
    of components between 10, 100, and 200 components. Try to take a moment and hypothesize
    which of three will end up being the best (hint, think back to the scree plot
    and explained variance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now fit the gridsearch object to our raw image data. Note that the pipeline
    will take care of automatically extracting features from and transforming our
    raw pixel data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We end up with a (slightly better) **88.95% cross-validated accuracy.** If we
    think about it, we should not be surprised that 100 was the best option out of
    10, 100, and 200\. From our brief analysis with the scree plot in a previous section,
    we found that 64% of the data was explained by a mere 30 components, so 10 components
    would definitely not be enough to explain the images well. The scree plot also
    started to level out at around 100 components, meaning that after the 100th component,
    the explained variance was truly not adding much, so 200 was too many components
    to use and would have started to lead to some overfitting. That leaves us with
    100 as being the optimal number of PCA components to use. It should be noted that
    we could go further and attempt some hyper-parameter tuning to find an even more
    optimal number of components, but for now we will leave our pipeline as is and
    move to using RBM components.
  prefs: []
  type: TYPE_NORMAL
- en: Using a linear model on extracted RBM components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even the optimal number of PCA components was unable to beat the logistic regression
    alone by much in terms of accuracy. Let's see how our RBM does. To make the following
    pipeline, we will keep the same parameters for the logistic regression model and
    find the optimal number of components between 10, 100, and 200 (like we did for
    the PCA pipeline). Note that we could try to expand the number of features past
    the number of raw pixels (784) but we will not attempt to.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin the same way by setting up our variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Fitting this grid search to our raw pixels will reveal the optimal number of
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our RBM module, with a **91.75% cross-validated accuracy**, was able to extract
    200 new features from our digits and give us a boost of three percent in accuracy
    (which is a lot!) by not doing anything other than adding the BernoulliRBM module
    into our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that 200 was the optimal number of components suggests that we may
    even obtain a higher performance by trying to extract more than 200 components.
    We will leave this as an exercise to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: This is evidence to the fact that feature learning algorithms work very well
    when dealing with very complex tasks such as image recognition, audio processing,
    and natural language processing. These large and interesting datasets have hidden
    components that are difficult for linear transformations like PCA or LDA to extract
    but non-parametric algorithms like RBM can.
  prefs: []
  type: TYPE_NORMAL
- en: Learning text features – word vectorizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our second example of feature learning will move away from images and towards
    text and natural language processing. When machines learn to read/write, they
    face a very large problem, context. In previous chapters, we have been able to
    vectorize documents by counting the number of words that appeared in each document
    and we fed those vectors into machine learning pipelines. By constructing new
    count-based features, we were able to use text in our supervised machine learning
    pipelines. This is very effective, up until a point. We are limited to only being
    to understand text as if they were only a **Bag of Words** (**BOW**). This means
    that we regard documents as being nothing more than a collection of words out
    of order.
  prefs: []
  type: TYPE_NORMAL
- en: What's more is that each word on its own has no meaning. It is only in a collection
    of other words that a document can have meaning when using modules such as `CountVectorizer`
    and `TfidfVectorizer`. It is for this reason that we will turn our attention away
    from scikit-learn and onto a module called `gensim` for computing word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until this point, we have used scikit-learn to embed documents (tweets, reviews,
    URLs, and so on) into a vectorized format by regarding tokens (words, n-grams)
    as features and documents as having a certain amount of these tokens. For example,
    if we had 1,583 documents and we told our `CountVectorizer` to learn the top 1,000
    tokens of `ngram_range` from one to five, we would end up with a matrix of shape
    (1583, 1000) where each row represented a single document and the 1,000 columns
    represented literal n-grams found in the corpus. But how do we achieve an even
    lower level of understanding? How do we start to teach the machine what words *mean*
    in context?
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we were to ask you the following questions, you may give the
    following answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q: What would you get if we took a king, removed the man aspect of it, and
    replaced it with a woman?*'
  prefs: []
  type: TYPE_NORMAL
- en: '***A: A queen***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Q: London is to England as Paris is to ____.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***A: France***'
  prefs: []
  type: TYPE_NORMAL
- en: You, a human, may find these questions simple, but how would a machine figure
    this out without knowing what the words by themselves mean in context? This is,
    in fact, one of the greatest challenges that we face in **natural language processing**
    (**NLP**) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word embeddings are one approach to helping a machine understand context. A **word
    embedding** is a vectorization of a single word in a feature space of n dimensions,
    where *n* represents the number of latent characteristics that a word can have.
    This means that every word in our vocabulary is not longer, just a string, but
    a vector in and of itself. For example, if we extracted n=5 characteristics about
    each word, then each word in our vocabulary would correspond to a 1 x 5 vector.
    For example, we might have the following vectorizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And with these vectorizations, we can tackle the question *What would you get
    if we took a king, removed the man aspect of it, and replaced it with a woman?*
    by performing the following operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king - man + woman*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, this would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This seems simple but it does some with a few caveats:'
  prefs: []
  type: TYPE_NORMAL
- en: Context (in the form of word embeddings) changes from corpus to corpus as does
    word meanings. This means that static word embeddings by themselves are not always
    the most useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embeddings are dependent on the corpus that they were learned from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embeddings allow us to perform very precise calculations on single words
    to achieve what we might consider context.
  prefs: []
  type: TYPE_NORMAL
- en: Two approaches to word embeddings - Word2vec and GloVe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two families of algorithms that dominate the space of word embeddings.
    They are called **Word2vec **and **GloVe**. Both methods are responsible for producing
    word embeddings by learning from very large corpus (collection of text documents).
    The main difference between these two algorithms is that the GloVe algorithm,
    out of Stanford, learns word embeddings through a series of matrix statistics
    while Word2vec, out of Google, learns them through a deep learning approach. Both
    algorithms have merits and our text will focus on using the Word2vec algorithm
    to learn word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec - another shallow neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to learn and extract word embeddings, Word2vec will implement another
    shallow neural network. This time, instead of generically throwing in new data
    into our visible layer, we will deliberately put in the correct data to give us
    the right word embeddings. Without going too into much detail, imagine a neural
    architecture with the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f071838-2990-44e4-bbdf-7953d705d259.png)'
  prefs: []
  type: TYPE_IMG
- en: Like the RBM, we have a visible input layer and a hidden layer. In this case,
    our input layer has as many nodes as the length of vocabulary that we wish to
    learn from. This comes in handy if we have a corpus of millions of words but we
    wish to only learn a handful of them. In the preceding graph, we would be learning
    the context of 5,000 words. The hidden layer in this graph represents the number
    of features we wish to learn about each word. In this case, we will be embedding
    words into a 300-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between this neural network and the one we used for RBM
    is the existence of an output layer. Note that in our graph, the output layer
    has as many nodes as the input layer. This is not a coincidence. Word embedding
    models work by *predicting* nearby words based on the existence of a reference
    word. For example, if we were to predict the word *calculus*, we would want the
    final `math` node to light up the most. This gives the semblance of a supervised
    machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We then train the graph on this structure and eventually learn the 300-dimension
    word representations by passing in one-hot vectors of words and extracting the
    hidden layer's output vector and using that as our latent structure. In production,
    the previous diagram is extremely inefficient due to the very large number of
    output nodes. In order to make this process extremely more computationally efficient,
    different loss functions are utilized to capitalize on the text's structure.
  prefs: []
  type: TYPE_NORMAL
- en: The gensim package for creating Word2vec embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will not be implementing a full working neural network that performs the
    word embedding procedure, however we will be using a Python package called `gensim`
    to do this work for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A `gensim` can take in a corpora of text and run the preceding neural network
    structure for us and obtain word embeddings with only a few lines of code. To
    see this in action, let''s import a standard corpus to get started. Let''s set
    a logger in our notebook so that we can see the training process in a bit more
    detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice the term `word2vec`. This is a specific algorithm used to calculate word
    embeddings and the main algorithm used by `gensim`. It is one of the standards
    for word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'For gensim to do its job, sentences needs to be any iterable (list, generator,
    tuple, and so on) that holds sentences that are already tokenized. Once we have
    such a variable, we can put `gensim` to work by learning word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This line of code will start the learning process. If you are passing in a
    large corpus, it may take a while. Now that the `gensim` module is done fitting,
    we can use it. We can grab individual embeddings by passing strings into the `word2vec`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `gensim` has built-in methods to get the most out of our word embeddings.
    For example, to answer the question about the `king`, we can use the the `most_similar`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmm, unfortunately this isn''t giving us the answer we''d expect: `queen`.
    Let''s try the `Paris` word association:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the word `Paris` was never even learned as it did not appear
    in our corpus. We can start to see the limitations to this procedure. Our embeddings
    will only be as good as the corpus we are selecting and the machines we use to
    calculate these embeddings. In our data directory, we have provided a pre-trained
    vocabulary of words that spans across 3,000,000 words found on websites indexed
    by Google with 300 dimensions learned for each word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and import these pre-trained embeddings. We can do this by
    using built-in importer tools in `gensim`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'These embeddings have been trained using vastly more powerful machines than
    anything we have at home and for a much longer period of time. Let''s try our
    word problems out now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Excellent! It seems as though these word embeddings were trained enough to
    allow us to answer these complex word puzzles. The `most_similar` method, as used
    previously, will return the token in the vocabulary that is most similar to the
    words provided. Words in the `positive` list are vectors added to one another,
    while words in the `negative` list are subtracted from the resulting vector. The
    following graph provides a visual representation of how we use word vectors to
    extract meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c327f18-9c2e-4bd7-9274-64ac71200591.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we are starting with the vector representation for **king** and adding
    to it the concept (vector) for **woman**. From there, we subtract the **man**
    vector (by adding the negative of the vector) to obtain the dotted vector. This
    vector is the most similar to the vector representation for **queen**. This is
    how we obtain the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king + woman - man = queen*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `gensim` has other methods that we may utilize such as `doesnt_match`.
    This method singles out words that do not belong to a list of words. It does so
    by isolating the word that is the most dissimilar on average to the rest of the
    words. For example, if we give the method four words, three of which are animals
    and the other is a plant, we hope it will figure out which of those doesn''t belong:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The package also includes methods to calculate a 0-1 score of similarity between
    single words that can be used to compare words on the fly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that `man` is more similar to `woman` than `man` is to `tree`.
    We can use these helpful methods in order to implement some useful applications
    of word embeddings that would not be possible otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Application of word embeddings - information retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are countless applications for word embeddings; one of these is in the
    field of information retrieval. When humans input keywords and key phrases into
    search engines, search engines are able to recall and surface specific articles/stories
    that match those keywords exactly. For example, if we search for articles about
    dogs, we will get articles that mention the word dog. But what if we search for
    the word canine? We should still expect to see articles about dogs based on the
    fact that canines are dogs. Let's implement a simple information retrieval system
    to showcase the power of word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a function that tries to grab embeddings of individual words
    from our gensim package and returns None if this lookup fails:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create three article titles, one about a `dog`, one about a `cat`,
    and one about absolutely `nothing` at all for a distractor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal is to input a reference word that is similar to `dog` or `cat` and
    be able to grab the more relevant title. To do this, we will first create a 3
    x 300 matrix of vectorizations for each sentence. We will do this by taking the
    mean of every word in the sentence and using the resulting mean vector as an estimation
    of the vectorization of the entire sentence. Once we have a vectorization of every
    sentence, we can compare that against the embedding of the reference word by taking
    a dot product between them. The closest vector is the one with the largest dot
    product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing to notice here is that we are creating a vectorization of documents
    (collection of words) and not considering the order of the words. How is this
    better than utilizing a `CountVectorizer` or a `TfidfVectorizer` to grab a count-based
    vectorization of text? The gensim method is attempting to project our text onto
    a latent structure learned by the context of individual words, while the scikit-learn
    vectorizers are only able to use the vocab at our disposal to create our vectorizations.
    In these three sentences, there are only seven unique words:'
  prefs: []
  type: TYPE_NORMAL
- en: '`this`, `is`, `about`, `a`, `dog`, `cat`, `nothing`'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the maximum shape our `CountVectorizer` or `TfidfVectorizer` can project
    is (3, 7). Let''s try to grab the most relevant sentence to the word `dog`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'That one was easy. Given the word `dog`, we should be able to retrieve the
    sentence about a `dog`. This should also hold true if we input the word `cat`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s try something harder. Let''s input the words `canine` and `tiger` and
    see if we get the `dog` and `cat` sentences respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try a slightly more interesting example. The following are chapter titles
    from Sinan''s first book,* Principles of Data Science*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This will give us a list of 12 different chapter titles to retrieve from. The
    goal then will be to use a reference word to sort and serve up the top three most
    relevant chapter titles to read, given the topic. For example, if we asked our
    algorithm to give us chapters relating to *math*, we might expect to be recommended
    the chapters about basic mathematics, statistics, and probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to see which chapters are the best to read, given human input. Before
    we do so, let''s calculate a matrix of vectorized documents like we did with our
    previous three sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s find the chapters that are most related to `math`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s say we are giving a talk about data and want to know which chapters
    are going to be the most helpful in that area:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, which chapters are about `AI`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We can see how we can use word embeddings to retrieve information in the form
    of text given context learned from the universe of text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter focused on two feature learning tools: RBM and word embedding
    processes.'
  prefs: []
  type: TYPE_NORMAL
- en: Both of these processes utilized deep learning architectures in order to learn
    new sets of features based on raw data. Both techniques took advantage of shallow
    networks in order to optimize for training times and used the weights and biases
    learned during the fitting phase to extract the latent structure of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Our next chapter will showcase four examples of feature engineering on real
    data taken from the open internet and how the tools that we have learned in this
    book will help us create the optimal machine learning pipelines.
  prefs: []
  type: TYPE_NORMAL
