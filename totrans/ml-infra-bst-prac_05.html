<html><head></head><body>
		<div id="_idContainer052">
			<h1 class="chapter-number"><a id="_idTextAnchor049"/>4</h1>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor050"/>Data Acquisition, Data Quality, and Noise</h1>
			<p>Data for<a id="_idIndexMarker154"/> machine learning systems can come directly from humans and software systems – usually called <em class="italic">source systems</em>. Where the data comes from has implications regarding what it looks like, what kind of quality it has, and how to <span class="No-Break">process it.</span></p>
			<p>The data that originates from humans is usually noisier than data that originates from software systems. We, as humans, are known for small inconsistencies and we can also understand things inconsistently. For example, the same defect reported by two different people could have a very different description; the same is true for requirements, designs, and <span class="No-Break">source code.</span></p>
			<p>The data that originates from software systems is often more consistent and contains less noise or the noise in the data is more regular than the noise in the human-generated data. This data is generated by source systems. Therefore, controlling and monitoring the quality of the data that’s generated automatically is different – for example, software systems do not “lie” in the data, so it makes no sense to check the believability of the data that’s <span class="No-Break">generated automatically.</span></p>
			<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>The different sources of data and what we can do <span class="No-Break">with them</span></li>
				<li>How to assess the quality of data that’s used for <span class="No-Break">machine learning</span></li>
				<li>How to identify, measure, and reduce noise <span class="No-Break">in data</span></li>
			</ul>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor051"/>Sources of data and what we can do with them</h1>
			<p>Machine<a id="_idIndexMarker155"/> learning software has become increasingly important in all fields today. Anything from telecommunication networks, self-driving vehicles, computer games, smart navigation systems, and facial recognition to websites, news production, cinematography, and experimental music creation can be done using machine learning. Some applications are very successful at, for example, using machine learning in search strings (BERT models). Some applications are not so successful, such as using machine learning in hiring processes. Often, this depends on the programmers, data scientists, or models that are used in these applications. However, in most cases, the success of a machine learning application is often in the data that is used to train it and use it. It depends on the quality of that data and the features that are extracted from it. For example, Amazon’s machine learning recommender was taken out of operation because it was biased against women. Since it was trained on historical recruitment data, which predominantly included male candidates, the system tended to recommend male candidates in <span class="No-Break">future hiring.</span></p>
			<p>Data that’s used in machine learning systems can originate from all kinds of sources. However, we can classify these sources into two main types – manual/human and automated software/hardware. These two types have different characteristics that determine how to organize these sources and how to extract features from the data from these sources. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.1</em> illustrates these types of data and provides examples of data of <span class="No-Break">each type.</span></p>
			<p>Manually <a id="_idIndexMarker156"/>generated data is the data that comes from human input or originates in humans. This kind of data is often much richer in information than the data generated by software, but it also has much larger variability. This variability can come from the natural variability of us, humans. The same question in a form, for example, can be understood and answered differently by two different people. This kind of data is often more susceptible to random errors than <span class="No-Break">systematic ones.</span></p>
			<p>Automatically generated data originates from hardware or software systems, usually measured by sensors or measurement scripts that collect data from other systems, products, processes, or organizations. This kind of data is often more consistent and repeatable but also more susceptible to <span class="No-Break">systematic errors:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer047">
					<img alt="Figure 4.1 – Sources of data and their classification. The green part of this figure is the scope of this book" src="image/B19548_04_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Sources of data and their classification. The green part of this figure is the scope of this book</p>
			<p>An example of<a id="_idIndexMarker157"/> data that originates from humans, and is often used in machine learning, is data about software defects. A human tester often inspects a problem and reports it using a form. This form contains fields about the testing phase, components affected, impact on the customer, and more, but one part of the form is often a description of the problem in natural language, which is an interpretation of what happens when it’s authored by a <span class="No-Break">human tester.</span></p>
			<p>Another type of data that’s generated by humans is source code. We, as programmers, write source code for the software in a programming language with a given syntax, and we use programming guidelines to keep a consistent style so that the product of our work – the source code – can be automatically interpreted or compiled by software. There are some structures in the code that we write, but it is far from being consistent. Even the same algorithm, when implemented by two different programmers, differs in the naming of variables, their types, or even how the problem is solved. A good example of this is the Rosetta code website, which provides the same solutions in different programming languages and sometimes even in the same <span class="No-Break">programming language.</span></p>
			<p>Requirement specifications or data input using forms have the same properties <span class="No-Break">and characteristics.</span></p>
			<p>However, there is one origin of data that is particularly interesting – medical data. This is the data from patients’ records and charts, input by the medical specialists as part of medical procedures. This data can be electronic, but it reflects the specialists’ understanding of the symptoms and their interpretation of the medical tests <span class="No-Break">and diagnoses.</span></p>
			<p>On the other hand, we have data that’s generated by software or hardware in one way or another. The data that’s generated automatically is more consistent, although not free from problems, and more repetitive. An example of such data is the data generated in telecommunication networks to transmit information from one telecommunication node to another. The radio signals are very stable compared to other types of data, and can be disturbed by external factors such as weather (precipitation) or obstructions such as construction cranes. The data is repeatable, with all variability originating from the <span class="No-Break">external factors.</span></p>
			<p>Another <a id="_idIndexMarker158"/>example is the data from vehicles, which register information around them and store it for further processing. This data can contain signaling between the vehicle’s components as well as communication with other vehicles or <span class="No-Break">the infrastructure.</span></p>
			<p>Medical data, such<a id="_idIndexMarker159"/> as <strong class="bold">electroencephalograph</strong> (<strong class="bold">EEG</strong> – that is, brain waves) or <strong class="bold">electrocardiogram</strong> (<strong class="bold">ECG</strong> – that is, heart rate), is collected from source systems, which<a id="_idIndexMarker160"/> we can see as measurement instruments. So, technically, it has been generated by computer systems, but it comes from human patients. This origin in patients means that the data has the same natural variability as any other data collected from humans. As every patient is a bit different, and the measurement systems can be attached to each patient a bit differently, the data generated by each patient differs slightly from other patients. For example, the ECG heartbeat data contains basic, consistent information – the number of beats per minute (among other parameters). However, the raw data differs in the amplitude of the ECG signal (depending on the placement of the measurement electrode) or the difference between spikes in the curve (R <span class="No-Break">and T-spikes).</span></p>
			<p>Therefore, my first best practice in this chapter has to do with the origin of the data that we use for <span class="No-Break">software systems.</span></p>
			<p class="callout-heading">Best practice #25</p>
			<p class="callout">Identify the origin of the data used in your software and create your data processing <span class="No-Break">pipeline accordingly.</span></p>
			<p>Since all types of <a id="_idIndexMarker161"/>data require different ways of working in terms of cleaning, formatting, and feature extraction, we should make sure that we know how the data is produced and what kind of problems we can expect (and handle). Therefore, first, we need to identify what kind of data we need, where it comes from, and what kind of problems it can carry <span class="No-Break">with it.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor052"/>Extracting data from software engineering tools – Gerrit and Jira</h1>
			<p>To illustrate how to work with data extraction, let’s extract data from a popular software engineering tool for code<a id="_idIndexMarker162"/> reviews – Gerrit. This tool is used for reviewing and discussing fragments of code developed by individual programmers, just before they are integrated into the main code base of <span class="No-Break">the product.</span></p>
			<p>The following<a id="_idIndexMarker163"/> program code shows how to access the database of Gerrit – that is, through the JSON API – and how to extract the list of all changes for a specific project. This program uses the Python <strong class="source-inline">pygerrit2</strong> package (<a href="https://pypi.org/project/pygerrit2/">https://pypi.org/project/pygerrit2/</a>). This module helps us use the JSON API as it provides <a id="_idIndexMarker164"/>Python functions instead of just <span class="No-Break">JSON strings:</span></p>
			<pre class="source-code">
# importing libraries
from pygerrit2 import GerritRestAPI
# A bit of config - repo
gerrit_url = "https://gerrit.onap.org/r"
# since we use a public OSS repository
auth = None
# this line gets sets the parameters for the HTML API
rest = GerritRestAPI(url=gerrit_url, auth = auth)
# the main query where we ask the endpoint to provide us the list and details of all changes
# each change is essentially a review that has been submitted to the repository
changes = rest.get("/changes/?q=status:merged&amp;o=ALL_FILES&amp;o=ALL_REVISIONS&amp;o=DETAILED_LABELS&amp;start=0",
headers={'Content-Type': 'application/json'})</pre>			<p>The key line in this code fragment is <strong class="source-inline">rest.get("/changes/?q=status:merged&amp;o=ALL_FILES&amp;o=ALL_REVISIONS&amp;o=DETAILED_LABELS&amp;start=0", headers={'Content-Type': 'application/json'})</strong>. This line specifies the endpoint for the changes to be retrieved, along <a id="_idIndexMarker165"/>with the parameters. It says that we want to access all files and all revisions, as well as all details of the changes. In these details, we can find the information about all revisions (particular patches/commits) and we can then parse each of these revisions. It is important to know that the JSON API returns a maximum of 500 changes in each query, and therefore, the last parameter – <strong class="source-inline">start=0</strong> – can be used to access changes from 500 upward. The output of this program is a very long list of changes in JSON, so I will not present it in detail in this book. Instead, I encourage you to execute this script and go through this file at your own pace. The script can be found in this book’s GitHub repository at <a href="https://github.com/miroslawstaron/machine_learning_best_practices">https://github.com/miroslawstaron/machine_learning_best_practices</a>, under <a href="B19548_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. The name of the script <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">gerrit_exporter.ipynb</strong></span><span class="No-Break">.</span></p>
			<p>Now, extracting just the list of changes is not very useful for analyses as it only provides the information that has been collected automatically – for example, which revisions exist, and who created these revisions. It does not contain information about which files and lines are commented on, or what the comments are – in other words, the particularly useful information. Therefore, we need to interact with Gerrit a bit more, as presented in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<p>The program flow presented in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.2</em> illustrates the complexity of the relationships in a code comment database such as Gerrit. Therefore, the program to access the database and export this data is a bit too long for this book. It can be accessed in the same repository as<a id="_idIndexMarker166"/> the previous one, under the <span class="No-Break">name </span><span class="No-Break"><strong class="source-inline">gerrit_exporter_loop.ipynb</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="Figure 4.2 – Interactions with Gerrit to extract commented files, commented lines, and the content of the comments" src="image/B19548_04_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Interactions with Gerrit to extract commented files, commented lines, and the content of the comments</p>
			<p>This kind of data can be used to train machine learning models to review the code or even identify which lines of code need to <span class="No-Break">be reviewed.</span></p>
			<p>When working with Gerrit, I have found the following best practice to <span class="No-Break">be useful.</span></p>
			<p class="callout-heading">Best practice # 26</p>
			<p class="callout">Extract as much data as you need and store it locally to reduce the disturbances for the software engineers who use the tool for <span class="No-Break">their work.</span></p>
			<p>Although it is possible to extract the changes one by one, it is better to extract the whole set of changes once and keep a local copy of it. In this way, we reduce the strain on the servers that others use for their daily work. We must remember that data extraction is a secondary task for these source systems, while their primary task is to support software engineers in <span class="No-Break">their work.</span></p>
			<p>Another good source of data for software systems that support software engineering tasks is JIRA, an issue <a id="_idIndexMarker167"/>and task management system. JIRA is used to document epics, user stories, software defects, and tasks and has become one of the most popular tools for this kind <span class="No-Break">of activity.</span></p>
			<p>Therefore, we can extract a lot of useful information about the processes from JIRA as a source system. Then, we can use this information to develop machine learning models to assess and improve tasks and requirements (in the form of user stories) and design tools that can help us identify overlapping user stories or group them into more coherent epics. Such software can be used to improve the quality of these tasks or provide <span class="No-Break">better estimations.</span></p>
			<p>The following <a id="_idIndexMarker168"/>code fragment illustrates how to make a connection to a JIRA instance and then how to extract all issues for a <span class="No-Break">particular project:</span></p>
			<pre class="source-code">
# import the atlassian module to be able to connect to JIRA
from atlassian import Jira
jira_instance = Jira(
    #Url of the JIRA server
    url="https://miroslawstaron.atlassian.net/",
    #  user name
    username='email@domain.com',
    # token
    password='your_token',
    cloud=True
)
# get all the tasks/ issues for the project
jql_request = 'project = MLBPB'
issues = jira_instance.jql(jql_request)</pre>			<p>In this fragment, for <a id="_idIndexMarker169"/>illustration purposes, I’m using my own JIRA database and my own project (<strong class="source-inline">MLBPB</strong>). This code requires the <strong class="source-inline">atlassian-python-api</strong> module to be imported. This module provides an API to connect to and interact with a JIRA database using Python, in a similar way as the API of Gerrit. Therefore, the same best practice as for Gerrit applies <span class="No-Break">to JIRA.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor053"/>Extracting data from product databases – GitHub and Git</h1>
			<p>JIRA and Gerrit <a id="_idIndexMarker170"/>are, to some extent, additional tools to the main product development tools. However, every software development organization uses a source code repository to store<a id="_idIndexMarker171"/> the main asset – the source code of the company’s software product. Today, the tools that are used the most are Git<a id="_idIndexMarker172"/> version control and its close relative, GitHub. Source code repositories can be a very useful source of data for machine learning systems – we can extract the source code of the product and <span class="No-Break">analyze it.</span></p>
			<p>GitHub<a id="_idIndexMarker173"/> is a great source of data for machine learning if we use it responsibly. Please remember that the source code provided as open source, by the community, is not for profiting off. We need to follow the licenses and we need to acknowledge the contributions that were made by the authors, contributors, and maintainers of the open source community. Regardless of the license, we are always able to analyze our own code or the code that belongs to <span class="No-Break">our company.</span></p>
			<p>Once we can<a id="_idIndexMarker174"/> access the source code of our product or the product that we want to analyze, the following code fragment helps us connect to the GitHub server and access <span class="No-Break">a repository:</span></p>
			<pre class="source-code">
# First create a Github instance:
# usi<a id="_idTextAnchor054"/>ng an access token
g = Github(token, per_page=100)
# get the <a id="_idTextAnchor055"/>repo for this book
repo = g.get_repo("miroslawstaron/machine_learning_best_practices")
# get all commits
commits = repo.get_commits()</pre>			<p>To support secure access to the code, GitHub uses access tokens rather than passwords when connecting to it. We can also use SSL and CLI interfaces, but for the sake of simplicity, we’ll use the HTTPS protocol with a token. The <strong class="source-inline">g = Github(token, per_page=100)</strong> line uses the token to instantiate the PyGitHub library’s main class. The token is individual and needs to be generated either per repository or <span class="No-Break">per user.</span></p>
			<p>The connection to the repository is done by the next line, <strong class="source-inline">repo = g.get_repo("miroslawstaron/machine_learning_best_practices")</strong>, which, in this example, connects to the repository associated with this book. Finally, the last line in the code fragment obtains the number of commits in the repository. Once obtained, we can<a id="_idIndexMarker175"/> print it and start analyzing it, as shown in the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# print the number of commits
print(f'Number of commits in this repo: {commits.totalCount}')
# print the last commit
print(f'The last commit message: {commits[0].commit.message}')</pre>			<p>The last line of this code fragment prints out the commit message of the latest commit. It is worth noting that the latest commit is always first in the list of commits. Once we know the commit, we can also access the list of files that are included in that commit. This is illustrated by the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# print the names of all files in the commit
# 0 means that we are looking at the latest commit
print(commits[0].file)</pre>			<p>Printing the list of the<a id="_idIndexMarker176"/> files in the commits is good, but it’s not very useful. Something more useful is to access these files and analyze them. The following code fragment shows how to access two files from the latest two commits. First, we access the files, after which we download their content and store it in two different variables – <strong class="source-inline">linesOne</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">linesTwo</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
# get one of the files from the commit
fileOne = commits[0].files[0]
# get the file from the second commit
fileTwo = commits[1].files[0]
# to get the content of the file, we need to get the sha of the commit
# otherwise we only get the content from the last commit
fl = repo.get_contents(fileOne.filename, ref=commits[0].sha)
fr = repo.get_contents(fileTwo.filename, ref=commits[1].sha)
# read the file content, but decoded into strings
# otherwise we would get the content in bytes
linesOne = fl.decoded_content
linesTwo = fr.decoded_content</pre>			<p>Finally, we can analyze the two files for one of the most important tasks – to get the diff between the two files. We can use the <strong class="source-inline">difflib</strong> Python library for this task, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
# calculate the diff using difflib
# for which we use a library difflib
import difflib
# print diff lines by iterating the list of lines
# returned by the difflib library
for line in difflib.unified_diff(str(linesOne),
                                 str(linesTwo),
                                 fromfile=fileOne.filename,
                                 tofile=fileTwo.filename):
  print(line)</pre>			<p>The preceding code fragment<a id="_idIndexMarker177"/> allows us to identify differences between files and print them in a way similar to how GitHub presents <span class="No-Break">the differences.</span></p>
			<p>My next best practice is related to the use of <span class="No-Break">public repositories.</span></p>
			<p class="callout-heading">Best practice # 27</p>
			<p class="callout">When accessing data from public repositories, please check the licenses and ensure you acknowledge the contribution of the community that created the <span class="No-Break">analyzed code.</span></p>
			<p>As I mentioned previously, open source programs are here for everyone to use, including to analyze and learn from them. However, the community behind this source code has spent countless hours creating and perfecting it. Therefore, we should use these repositories responsibly. If we use the repositories to create our own products, including machine learning software products, we need to acknowledge the community’s contribution and, if we use the software under a copyleft license, give back our work to <span class="No-Break">the community.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor056"/>Data quality</h1>
			<p>When designing and developing<a id="_idIndexMarker178"/> machine learning systems, we consider the data quality on a relatively low level. We look for missing values, outliers, or similar. They are important because they can cause problems when training machine learning models. Nevertheless, they are nearly enough from a software <span class="No-Break">engineering perspective.</span></p>
			<p>When engineering reliable software systems, we need to know more about the data we use than whether it contains (or not) missing values. We need to know whether  we can trust the data (whether it is believable), whether the data is representative, or whether it is up to date. So, we need a quality model for <span class="No-Break">our data.</span></p>
			<p>There are several quality models for data in software engineering, and the one I often use, and recommend, is the AIMQ model – a methodology for assessing <span class="No-Break">information quality.</span></p>
			<p>The quality dimensions <a id="_idIndexMarker179"/>of the AIMQ model are as follows (cited from Lee, Y.W., et al., <em class="italic">AIMQ: a methodology for information quality assessment</em>. Information &amp; management, 2002. 40(2): <span class="No-Break">p. 133-146):</span></p>
			<ul>
				<li><strong class="bold">Accessibility</strong>: The information is easily retrievable and accessible to <span class="No-Break">our system</span></li>
				<li><strong class="bold">Appropriate amount</strong>: The information is of sufficient volume for our needs and for <span class="No-Break">our applications</span></li>
				<li><strong class="bold">Believability</strong>: The information is trustworthy and can <span class="No-Break">be believed</span></li>
				<li><strong class="bold">Completeness</strong>: The information includes all the necessary values for <span class="No-Break">our system</span></li>
				<li><strong class="bold">Concise representation</strong>: The information is formatted compactly and appropriately for <span class="No-Break">our application</span></li>
				<li><strong class="bold">Consistent representation</strong>: The information is consistently presented in the same format, including its representation <span class="No-Break">over time</span></li>
				<li><strong class="bold">Ease of operation</strong>: The information is easy to manipulate to meet <span class="No-Break">our needs</span></li>
				<li><strong class="bold">Free of errors</strong>: The information is correct, accurate, and reliable for the application that <span class="No-Break">we’re creating</span></li>
				<li><strong class="bold">Interpretability</strong>: It is easy to interpret what the <span class="No-Break">information means</span></li>
				<li><strong class="bold">Objectivity</strong>: The information was objectively collected and is based <span class="No-Break">on facts</span></li>
				<li><strong class="bold">Relevance</strong>: The information is useful, relevant, appropriate, and applicable to <span class="No-Break">our system</span></li>
				<li><strong class="bold">Reputation</strong>: The information has a good reputation for quality and comes from <span class="No-Break">good sources</span></li>
				<li><strong class="bold">Security</strong>: The information is protected from unauthorized access and <span class="No-Break">sufficiently restricted</span></li>
				<li><strong class="bold">Timeliness</strong>: The information is sufficiently current for <span class="No-Break">our work</span></li>
				<li><strong class="bold">Understandability</strong>: The<a id="_idIndexMarker180"/> information is easy to understand <span class="No-Break">and comprehend</span></li>
			</ul>
			<p>Some of these dimensions are <a id="_idIndexMarker181"/>certainly universal for all kinds of applications. For example, the <em class="italic">free from error</em> dimension is relevant for all systems and all machine learning models. At the same time, <em class="italic">relevance</em> must be evaluated in the context of the application and software that we design. The dimensions that are closer to the raw data can be assessed automatically more easily than the dimensions related to applications. For application-related dimensions, we often need to make an expert assessment or conduct manual analyses <span class="No-Break">or investigations.</span></p>
			<p>Take believability, for example. To assess whether our source data is trustworthy and can be used for this application, we need to understand where the data comes from, who/what created that data, and under which premises. This cannot be automated as it requires human, <span class="No-Break">expert, judgment.</span></p>
			<p>Therefore, we can organize these dimensions at different abstraction levels – raw data or source systems, data for training and inference, machine learning models and algorithms, and the entire software product. It can be useful to show which of these dimensions are closer to the raw data, which ones are closer to the algorithms, and which ones are closer to the products. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> shows <span class="No-Break">this organization:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer049">
					<img alt="Figure 4.3 – Data quality attributes organized according to their logical relevance" src="image/B19548_04_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Data quality attributes organized according to their logical relevance</p>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> indicates that <a id="_idIndexMarker182"/>there is a level of abstraction in how we check information quality, which is entirely correct. The lowest abstraction levels, or the first checks, are intended to quantify the basic quality dimensions. These checks do not have to be very complex either. The entire quality measurement and monitoring system can be quite simplistic, yet very powerful. <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.4</em> presents a conceptual design of such a system. This system consists of three parts – the machine learning pipeline, the log files, and the information quality measurement <span class="No-Break">and monitoring:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer050">
					<img alt="Figure 4.4 – Information quality measurement system" src="image/B19548_04_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Information quality measurement system</p>
			<p>First, the machine<a id="_idIndexMarker183"/> learning pipeline contains probes, or measurement instruments, that collect information about problems related to information quality. For instance, these instruments can collect information if there are any problems with accessing the data, which can indicate problems with the <em class="italic">accessibility</em> <span class="No-Break">quality dimension.</span></p>
			<p>The following code fragment shows how this can be realized in practice. It configures a rudimentary log file that collects the error information from the machine <span class="No-Break">learning pipeline:</span></p>
			<pre class="source-code">
import logging
# create a logging file
# including the format of the log messages
logging.basicConfig(filename='./information_quality_gerrit.log',
                    filemode='w',
                    format='%(asctime)s;%(name)s;%(levelname)s;%(message)s',
                    level=logging.DEBUG)
# specifying the name of the logger,
# which will tell us that the message comes from this program
# and not from any other modules or components imported
logger = logging.getLogger('Gerrit data export pipeline')
# the first log message to indicate the start of the execution
# it is important to add this, since the same log-file can be re-used
# the re-use can be done by other components to provide one single point of logging
logger.info('Configuration started')</pre>			<p>This code fragment <a id="_idIndexMarker184"/>creates log files, provides them with a unique name that the entire machine learning pipeline uses, and specifies the format of the error messages. Then, in the machine learning pipeline itself, the log files are propagated with messages. The following code fragment shows an example of how the data export tool presented previously is instrumented to propagate <span class="No-Break">this information:</span></p>
			<pre class="source-code">
# A bit of config – repo
gerrit_url = "https://gerrit.onap.org/r"
fileName = "./gerrit_reviews.csv"
# since we use a public oss repository, we don't need to authenticate
auth = None
# this line gets sets the parameters for the HTML API
rest = GerritRestAPI(url=gerrit_url, auth = auth)
<strong class="bold">logger.info('REST API set-up complete')</strong>
# a set of parameters for the JSON API to get changes in batches of 500
start = 0                       # which batch we start from – usually 0
<strong class="bold">logger.info('Connecting to Gerrit server and accessing changes')</strong>
try:
    # the main query where we ask the endpoing to provide us the list and details of all changes
    # each change is essentially a review that has been submitted to the repository
    changes = rest.get("/changes/?q=status:merged&amp;o=ALL_FILES&amp;o=ALL_REVISIONS&amp;o=DETAILED_LABELS&amp;start={}".format(start),
                       headers={'Content-Type': 'application/json'})
except Exception as e:
    <strong class="bold">logger.error('ENTITY ACCESS – Error retrieving changes: {}'.format)</strong>
<strong class="bold">logger.info('Changes retrieved')</strong></pre>			<p>The boldface lines in the preceding code fragment log the information messages with the <strong class="source-inline">logger.info(…)</strong> statement as well as error messages with the <span class="No-Break"><strong class="source-inline">logger.error(…)</strong></span><span class="No-Break"> statement.</span></p>
			<p>The content of this log file can be quite substantial, so we need to filter the messages based on their importance. That’s why we distinguish between errors <span class="No-Break">and information.</span></p>
			<p>The following is a fragment of <a id="_idIndexMarker185"/>such a log file. The first line contains the information message (boldface <strong class="source-inline">INFO</strong>) to show that the machine learning pipeline has <span class="No-Break">been started:</span></p>
			<pre class="source-code">
2023-01-15 17:11:45,618;Gerrit data export pipeline;<strong class="bold">INFO</strong>;Configuration started
2023-01-15 17:11:45,951;Gerrit data export pipeline;INFO;Configuration ended
2023-01-15 17:11:46,052;Gerrit data export pipeline;INFO;Downloading fresh data to ./gerrit_reviews.csv
2023-01-15 17:11:46,055;pygerrit2;DEBUG;Error parsing netrc: netrc missing or no credentials found in netrc
2023-01-15 17:11:46,057;Gerrit data export pipeline;INFO;Geting the data about changes from 0 to 500
2023-01-15 17:11:46,060;urllib3.connectionpool;DEBUG;Starting new HTTPS connection (1): gerrit.onap.org:443</pre>			<p>We filter these messages in the last part of our measurement system – the information quality measurement and monitoring system. This last part reads through the log files and collects the error messages, categorizes them, and then <span class="No-Break">visualizes them:</span></p>
			<pre class="source-code">
try:
    logFile = open("./information_quality_gerrit.log", "r")
    for logMessage in logFile:
        # splitting the log information - again, this is linked to how we
        # structured the log message in the measurement system
        logItem = logMessage.split(';')
        logLevel = logItem[2]
        logSource = logItem[1]
        logTime = logItem[0]
        logProblem = logItem[3]
        # this part is about extracting the relevant information
        # if this is a problem at all:
        <strong class="bold">if (logLevel == 'ERROR'):</strong>
<strong class="bold">            # if this is a problem with the library</strong>
<strong class="bold">            if ('LIBRARIES' in logProblem):</strong>
<strong class="bold">                iq_problems_configuration_libraries += 1</strong>
<strong class="bold">            if ('ENTITY_ACCESS' in logProblem):</strong>
<strong class="bold">                iq_problems_entity_access += 1</strong>
except Exception as e:
    iq_general_error = 1</pre>			<p>The bold-faced lines <a id="_idIndexMarker186"/>categorize the error messages found – in other words, they quantify the quality dimensions. This quantification is important as we need to understand how many problems of each kind were found in the machine <span class="No-Break">learning pipeline.</span></p>
			<p>The next step is to visualize the information quality, and for that, we need a quality analysis model. Then, we can use this quality model to visualize the <span class="No-Break">quality dimensions:</span></p>
			<pre class="source-code">
def getIndicatorColor(ind_value):
    if ind_value &gt; 0:
        return 'red'
    else:
        return 'green'
columns = ('Information quality check', 'Value')
rows = ['Entity access check', 'Libraries']
cell_text = [[f'Entity access: {iq_problems_entity_access}'],
             [f'Libraries: {iq_problems_configuration_libraries}']]
colors = [[getIndicatorColor(iq_problems_entity_access)],
          [getIndicatorColor(iq_problems_configuration_libraries)]]</pre>			<p>The visualization can be <a id="_idIndexMarker187"/>done in several ways, but in the majority of cases, it is enough to visualize it in a tabular form, which is easy to overview and comprehend. The most important for this visualization is that it communicates whether there are (or not) any information <span class="No-Break">quality problems:</span></p>
			<pre class="source-code">
fig, ax = plt.subplots()
ax.axis('tight')
ax.axis('off')
the_table = ax.table(cellText=cell_text,
                     cellColours=colors,
                     colLabels=columns,
                     loc='left')
plt.show()</pre>			<p>The result of this code<a id="_idIndexMarker188"/> fragment is the visual representation shown in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.5</em>. This example can be found in this book’s <span class="No-Break">GitHub repository:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer051">
					<img alt="Figure 4.5 – Results from the quality checks, visualized in a tabular form" src="image/B19548_04_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Results from the quality checks, visualized in a tabular form</p>
			<p>This rudimentary way of checking the information’s quality illustrates my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice # 28</p>
			<p class="callout">Use simple logging to trace any problems in your machine learning pipeline to monitor <span class="No-Break">information quality.</span></p>
			<p>It’s generally a good practice to design and develop robust software systems. Logging is one of the<a id="_idIndexMarker189"/> mechanisms that’s used to detect potential problems. Logging is also a very good software engineering practice for systems that are not interactive, such as machine learning-based ones. Therefore, extracting the information from logs can help us understand the quality of the information that is used in a machine <span class="No-Break">learning-based system.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor057"/>Noise</h1>
			<p>Data quality in machine learning systems has one additional and crucial attribute – noise. Noise can be<a id="_idIndexMarker190"/> defined as data points that contribute negatively to the ability of machine learning systems to identify patterns in the data. These data points can be outliers that make the datasets skew toward one or several classes in classification problems. The outliers can also cause prediction systems to over- or under-predict because they emphasize patterns that do not exist in <span class="No-Break">the data.</span></p>
			<p>Another type of <a id="_idIndexMarker191"/>noise is contradictory entries, where two (or more) identical data points are labeled with different labels. We can illustrate this with the example of product reviews on Amazon, which we saw in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Let’s import them into a new Python script with <strong class="source-inline">dfData = pd.read_csv('./book_chapter_4_embedded_1k_reviews.csv')</strong>. In this case, this dataset contains a summary of the reviews and the score. We focus on these two columns and we define noise as different scores for the same summary review. For example, if one person provides a score of 5 for the review with the tag “Awesome!” and another person provides a score of 4 for another review with the tag “Awesome!,” the same data point becomes noisy as it is annotated with two different labels – two <span class="No-Break">different scores.</span></p>
			<p>So, first, we must check whether there are any <span class="No-Break">duplicate entries:</span></p>
			<pre class="source-code">
# now, let's check if there are any duplicate entries
# get the number of all data points
allDataPoints = len(dfData.Summary)
# and get the number of unique data points
uniqueDataPoints = len(dfData.Summary.unique())
# check if the number of unique and all data points is the same
if allDataPoints != uniqueDataPoints:
  print(f'There are {allDataPoints - uniqueDataPoints} duplicate entries, which can potentially be noisy')</pre>			<p>This code checks whether the number of data points is the same as the number of unique data points; if not, then we risk having noisy entries. We can check whether there are duplicate entries by using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# then, we find duplicate data points
# first we group the datapoints
dfGrouped = dfData.groupby(by=dfData.Summary).count()
# then we find the index of the ones that are not unique
lstDuplicated = dfGrouped[dfGrouped.Time &gt; 1].index.to_list()</pre>			<p>Now, we can remove all duplicate entries using the following command, though a simple solution would be to remove them (<strong class="source-inline">dfClean = dfData[~dfData.Summary.isin(lstDuplicated)]</strong>). A better solution is to check whether they are noisy <a id="_idIndexMarker192"/>entries or just duplicates. We can do this using the following <span class="No-Break">code fragment:</span></p>
			<pre class="source-code">
# for each of these data points, we check if these data points
# are classified to different labels adn remove only the ones that have different labels
for onePoint in lstDuplicated:
  # find all instances of this datapoint
  dfPoint = dfData[dfData.Summary == onePoint]
  # now check if these data points have a different score
  numLabels = len(dfPoint.Score.unique())
  # if the number of labels is more than 1, then
  # this means that we have noise in the dataset
  # and we should remove this point
  if numLabels &gt; 1:
    dfData.drop(dfData[dfData.Summary == onePoint].index, inplace=True)
    # let's also print the data point that we remove
    print(f'point: {onePoint}, number of labels: {len(dfPoint.Score.unique())}')</pre>			<p>After running this fragment of code, the dataset does not contain any contradictory entries and therefore no class noise. Although it is possible to adopt a different strategy – for example, instead of removing noisy data points, we could change them to one of the classes – such an approach changes the pattern in the data, and therefore is not fully representative of the data. We simply do not know which of the classes is more correct than the others, especially if there are duplicate data points with two <span class="No-Break">different labels.</span></p>
			<p class="callout-heading">Best practice # 29</p>
			<p class="callout">The best strategy to reduce the impact of noise on machine learning classifiers is to remove the noisy <span class="No-Break">data points.</span></p>
			<p>Although we can correct noisy data points by changing their label or reducing the impact of these attributes on the predictions, the best strategy is to remove these data points. Removing is better as it does not change the patterns in the data. Imagine that we relabel noisy entries – this creates a pattern in the data that does not exist, which causes the algorithms to mispredict future <span class="No-Break">data points.</span></p>
			<p>Removing noise <a id="_idIndexMarker193"/>from the data is the only one way to handle noise. Another method is to increase the number of features so that we can distinguish between data points. We can analyze data and identify whether there is a risk of noise, and then we can check whether  it is possible to add one more feature to the dataset to distinguish between entries labeled differently. However, this is outside the scope of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor058"/>Summary</h1>
			<p>Data for machine learning systems is crucial – without data, there can be no machine learning systems. In most machine learning literature, the process of training models usually starts with the data in tabular form. In software engineering, however, this is an intermediate step. The data is collected from source systems and needs to <span class="No-Break">be processed.</span></p>
			<p>In this chapter, we learned how to access data from modern software engineering systems such as Gerrit, GitHub, JIRA, and Git. The code included in this chapter illustrates how to collect data that can be used for further steps in the machine learning pipeline – feature extraction. We’ll focus on this in the <span class="No-Break">next chapter.</span></p>
			<p>Collecting data is not the only preprocessing step that is required to design and develop a reliable software system. Quantifying and monitoring information (and data) quality is equally important. We need to check that the data is fresh (timely) and that there are no problems in preprocessing <span class="No-Break">that data.</span></p>
			<p>One of the aspects that is specific to machine learning systems is the presence of noise in the data. In this chapter, we learned how to treat class noise in the data and how to reduce the impact of the noise on the final machine <span class="No-Break">learning algorithm.</span></p>
			<p>In the next chapter, we dive deeper into concepts related to data - clearning it from noise and quantifying <span class="No-Break">its properties.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor059"/>References</h1>
			<ul>
				<li><em class="italic">Vaswani, A. et al., Attention is all you need. Advances in neural information processing systems, </em><span class="No-Break"><em class="italic">2017. 30.</em></span></li>
				<li><em class="italic">Dastin, J., Amazon scraps secret AI recruiting tool that showed bias against women. In Ethics of Data and Analytics. 2018, Auerbach Publications. </em><span class="No-Break"><em class="italic">p. 296-299.</em></span></li>
				<li><em class="italic">Staron, M., D. Durisic, and R. Rana, </em><em class="italic">Improving measurement certainty by using calibration to find systematic measurement error—a case of lines-of-code measure. In Software Engineering: Challenges and Solutions. 2017, Springer. </em><span class="No-Break"><em class="italic">p. 119-132.</em></span></li>
				<li><em class="italic">Staron, M. and W. Meding, Software Development Measurement Programs. Springer. https://doi. org/10.1007/978-3-319-91836-5, 2018. 10: </em><span class="No-Break"><em class="italic">p. 3281333.</em></span></li>
				<li><em class="italic">Fenton, N. and J. Bieman, Software metrics: a rigorous and practical approach. 2014: </em><span class="No-Break"><em class="italic">CRC press.</em></span></li>
				<li><em class="italic">Li, N., M. Shepperd, and Y. Guo, A systematic review of unsupervised learning techniques for software defect prediction. Information and Software Technology, 2020. 122: </em><span class="No-Break"><em class="italic">p. 106287.</em></span></li>
				<li><em class="italic">Staron, M. et al. Robust Machine Learning in Critical Care—Software Engineering and Medical Perspectives. In </em><em class="italic">2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN). </em><span class="No-Break"><em class="italic">2021. IEEE.</em></span></li>
				<li><em class="italic">Zhang, J. et al., CoditT5: Pretraining for Source Code and Natural Language Editing. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:2208.05446, 2022.</em></span></li>
				<li><em class="italic">Staron, M. et al. Using machine learning to identify code fragments for manual review. In 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA). </em><span class="No-Break"><em class="italic">2020. IEEE.</em></span></li>
				<li><em class="italic">Ochodek, M., S. Kopczyńska, and M. Staron, Deep learning model for end-to-end approximation of COSMIC functional size based on use-case names. Information and Software Technology, 2020. 123: </em><span class="No-Break"><em class="italic">p. 106310.</em></span></li>
				<li><em class="italic">Cichy, C. and S. Rass, An overview of data quality frameworks. IEEE Access, 2019. 7: </em><span class="No-Break"><em class="italic">p. 24634-24648.</em></span></li>
				<li><em class="italic">Lee, Y.W. et al., AIMQ: a methodology for information quality assessment. Information &amp; management, 2002. 40(2): </em><span class="No-Break"><em class="italic">p. 133-146.</em></span></li>
				<li><em class="italic">Staron, M. and W. Meding. Ensuring reliability of information provided by measurement systems. In International Workshop on Software Measurement. </em><span class="No-Break"><em class="italic">2009. Springer.</em></span></li>
				<li><em class="italic">Pandazo, K. et al. Presenting software metrics indicators: a case study. In Proceedings of the 20th international conference on Software Product and Process Measurement (</em><span class="No-Break"><em class="italic">MENSURA). 2010.</em></span></li>
				<li><em class="italic">Staron, M. et al. Improving Quality of Code Review Datasets–Token-Based Feature Extraction Method. In International Conference on Software Quality. </em><span class="No-Break"><em class="italic">2021. Springer.</em></span></li>
			</ul>
		</div>
	</body></html>