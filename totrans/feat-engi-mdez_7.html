<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Feature Learning</h1>
                </header>
            
            <article>
                
<p>In our final chapter, where we will be exploring feature engineering techniques, we will be taking a look at what is likely the most powerful feature engineering tool at our disposal. Feature learning algorithms are able to take in cleaned data (yes, you still need to do some work) and create brand-new features by exploiting latent structures within data. If all of this sounds familiar, that is because this is the description that we used in the previous chapter for feature transformations. The differences between these two families of algorithms are in the <em>parametric</em> assumptions that they make when attempting to create new features.</p>
<p>We will be covering the following topics:</p>
<ul>
<li>Parametric assumptions of data</li>
<li>Restricted Boltzmann Machines</li>
<li>The BernoulliRBM</li>
<li>Extracting RBM components from MNIST</li>
<li>Using RBMs in a machine learning pipeline</li>
<li>Learning text features—word vectorization</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parametric assumptions of data</h1>
                </header>
            
            <article>
                
<p>When we say <strong>parametric assumptions</strong>, we are referring to base assumptions that algorithms make about the <em>shape</em> of the data. In the previous chapter, while exploring <strong>principal component analysis</strong> (<strong>PCA</strong>), we discovered that the end result of the algorithm produced components that we could use to transform data through a single matrix multiplication. The assumption that we were making was that the original data took on a shape that could be decomposed and represented by a single linear transformation (the matrix operation). But what if that is not true? What if PCA is unable to extract <em>useful</em> features from the original dataset? Algorithms such as PCA and <strong>linear discriminate analysis</strong> (<strong>LDA</strong>) will always be able to find features, but they may not be useful at all. Moreover, these algorithms rely on a predetermined equation and will always output the same features each and every time they are run. This is why we consider both LDA and PCA as being <em>linear transformations.</em></p>
<p>Feature learning algorithms attempt to solve this issue by removing that parametric assumption. They do not make any assumptions about the shape of the incoming data and rely on <em>stochastic learning</em>. This means that, instead of throwing the same equation at the matrix of data every time, they will attempt to figure out the best features to extract by looking at the data points over and over again (in epochs) and converge onto a solution (potentially different ones at runtime).</p>
<div class="packt_tip">For more information on how stochastic learning (and stochastic gradient descent) works, please refer to the <em>Principles of Data Science</em>, by Sinan Ozdemir at:<br/>
<a href="https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/principles-data-science</a></div>
<p>This allows feature learning algorithms to bypass the parametric assumption made by algorithms such as PCA and LDA and opens us up to solve much more difficult questions than we could previously in this text. Such a complex idea (bypassing parametric assumptions) requires the use of complex algorithms. <em>Deep learning</em> algorithms are the choice of many data scientists and machine learning to learn new features from raw data. </p>
<p>We will assume that the reader has a basic familiarity with the neural network architecture in order to focus on applications of these architectures for feature learning. The following table summarizes the basic differences between feature learning and transformation:</p>
<table>
<tbody>
<tr>
<td/>
<td>
<p><strong>Parametric?</strong></p>
</td>
<td>
<p><strong>Simple to use?</strong></p>
</td>
<td>
<p><strong>Creates new feature set?</strong></p>
</td>
<td>
<p><strong>Deep learning?</strong></p>
</td>
</tr>
<tr>
<td>
<p>Feature transformation algorithms</p>
</td>
<td>
<p>Yes</p>
</td>
<td>
<p>Yes</p>
</td>
<td>
<p>Yes</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>Feature learning algorithms</p>
</td>
<td>
<p>No</p>
</td>
<td>
<p>No (usually)</p>
</td>
<td>
<p>Yes</p>
</td>
<td>
<p>Yes (usually)</p>
</td>
</tr>
</tbody>
</table>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The fact that both feature learning and feature transformation algorithms create new feature sets means that we regard both of them as being under the umbrella of <em>feature extraction. </em>The following figure shows this relationship:</p>
<div class="CDPAlignCenter CDPAlign"><img height="283" src="assets/4587fa19-778b-42ac-b14d-65c784ba9a66.png" width="467"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Feature extraction as a superset of feature learning and feature transformation. Both families of algorithms work to exploit latent structure in order to transform raw data into a new feature set</div>
<p>Both <strong>feature learning</strong> and <strong>feature transformation</strong> fall under the category of feature extraction as they are both trying to create a new feature set from the latent structure of raw data. The methods in which they are allowed to work, though, are the main differentiators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Non-parametric fallacy</h1>
                </header>
            
            <article>
                
<p><span>It is important to mention that a model being non-parametric doesn't mean that there are no assumptions at all made by the model during training.</span></p>
<p><span>While the algorithms that we will be introducing in this chapter forgo the assumption on the shape of the data, they still may make assumptions on other aspects of the data, for example, the values of the cells.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The algorithms of this chapter</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will focus on two feature learning areas:</p>
<ul>
<li><strong>Restricted Boltzmann Machines</strong> (<strong>RBM</strong>): A simple deep learning architecture that is set up to learn a set number of new dimensions based on a probabilistic model that data follows. These machines are in fact a family of algorithms with only one implemented in scikit-learn. The <strong>BernoulliRBM </strong>may be a non-parametric feature learner; however, as the name suggests, some expectations are set as to the values of the cells of the dataset.</li>
<li><strong>Word embeddings</strong>: Likely one of the biggest contributors to the recent deep learning-fueled advancements of natural language processing/understanding/generation is the ability to project strings (words and phrases) into an n-dimensional feature set in order to grasp context and minute detail in wording. We will use the <kbd>gensim</kbd> Python package to prepare our own word embeddings and then use pre-trained word embeddings to see some examples of how these word embeddings can be used to enhance the way we interact with text.</li>
</ul>
<p>All of these examples have something in common. They all involve learning brand new features from raw data. They then use these new features to enhance the way that they interact with data. For the latter two examples, we will have to move away from scikit-learn as these more advanced techniques are not (yet) implemented in the latest versions. Instead, we will see examples of deep learning neural architectures implemented in TensorFlow and Keras.</p>
<p>For all of these techniques, we will be focusing less on the very low-level inner workings of the models, and more on how they work to interpret data. We will go in order and start with the only algorithm that has a scikit-learn implementation, the restricted Boltzmann machine family of algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restricted Boltzmann Machines</h1>
                </header>
            
            <article>
                
<p>RBMs are a family of unsupervised feature learning algorithms that use probabilistic models to learn new features. Like PCA and LDA, we can use RBMs to extract a new feature set from raw data and use them to enhance machine learning pipelines. The features that are extracted by RBMs tend to work best when followed by linear models such as linear regression, logistic regression, perceptron's, and so on.</p>
<p>The unsupervised nature of RBMs is important as they are more similar to PCA algorithms than they are to LDA. They do not require a ground-truth label for data points to extract new features. This makes them useful in a wider variety of machine learning problems.</p>
<p>Conceptually, <span>RBMs are shallow (two-layer) neural networks. They are thought to be the building blocks of a class of algorithms called</span> <strong>Deep Belief Networks</strong> (<strong>DBN</strong>). <span>Keeping with standard terminology, there is a visible layer (the first layer), followed by a hidden layer (the second layer). These are the only two layers of the network:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="239" src="assets/31045c70-ef67-4429-9f70-4dc418d6373c.png" width="151"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">The setup for a restricted Boltzmann Machine. The circles represent nodes in the graph</div>
<p>Like any neural network, we have nodes in our two layers. The first visible layer of the network has as many layers as the input feature dimension. In our upcoming example, we will be working with 28 x 28 images necessitating 784 (28 x 28) nodes in our input layer. The number of nodes in the hidden layer is a human-chosen number and represents the number of features that we wish to learn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Not necessarily dimension reduction</h1>
                </header>
            
            <article>
                
<p>In PCA and LDA, we had severe limits to the number of components we were allowed to extract. For PCA, we were capped by the number of original features (we could only use less than or equal to the number of original columns), while LDA enforced the much stricter imposition that caps the number of extracted features to the number of categories in the ground truth minus one.</p>
<p>The only restriction on the number of features RBMs are allowed to learn is that they are limited by the computation power of the computer running the network and human interpretation. RBMs can learn fewer or <em>more</em> features than we originally began with. The exact number of features to learn is up to the problem and can be gridsearched.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The graph of a Restricted Boltzmann Machine</h1>
                </header>
            
            <article>
                
<p>So far, we have seen the visible and hidden layers of RBMs, but we have not yet seen how they learn features. Each of the visible layer's nodes take in a single feature from the dataset to be learned from. This data is then passed from the visible layer to the hidden layer through weights and biases:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-medium wp-image-205 image-border" height="210" src="assets/bfe9aba3-88cd-4492-972b-e9df0618da42.png" width="313"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">This visualization of an RBM shows the movement of a single data point through the graph through a single hidden node</div>
<p><span>The preceding visualization of an RBM shows the movement of a single data point through the graph and through a single hidden node. The visible layer has four nodes, representing the four columns of the original data. Each arrow represents a single feature of the data point moving through the four visible nodes in the first layer of the RBM. Each of the feature values is multiplied by a weight associated to that feature and are added up together. This calculation can also be summed up by a dot product between an input vector of data and a weight vector. The resulting weighted sum of the data is added to a bias variable and sent through an activation function (sigmoidal is popular). The result is stored in a variable called <kbd>a</kbd>.</span></p>
<p>As an example in Python, this code shows how a single data point (<kbd>inputs</kbd>) is multiplied by our <kbd>weights</kbd> vector and combined with the <kbd>bias</kbd> variable to create the activated variable, <kbd>a</kbd>:</p>
<pre>import numpy as np<br/>import math<br/><br/># sigmoidal function<br/>def activation(x):<br/>    return 1 / (1 + math.exp(-x))<br/><br/>inputs = np.array([1, 2, 3, 4])<br/>weights = np.array([0.2, 0.324, 0.1, .001])<br/>bias = 1.5<br/><br/>a = activation(np.dot(inputs.T, weights) + bias)<br/><br/>print a<br/>0.9341341524806636</pre>
<p>In a real RBM, each of the visible nodes is connected to each of the hidden nodes, and it looks something like this:</p>
<div class="CDPAlignCenter CDPAlign"><img height="231" src="assets/dd0adfcc-12be-4d6c-91f0-8834abe52bc6.png" width="324"/></div>
<div>
<p>Because inputs from each visible node are passed to every single hidden node, an RBM can be defined as a<span> </span><strong>symmetrical bipartite graph</strong>. The symmetrical part comes from the fact that the visible nodes are all connected with each hidden node.<span> </span>Bipartite<span> </span>means it has two parts (layers).</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The restriction of a Boltzmann Machine</h1>
                </header>
            
            <article>
                
<p>With our two layers of visible and hidden nodes, we have seen the connection between the layers (inter-layer connections), but we haven't seen any connections between nodes in the same layer (intra-layer connections). That is because there aren't any. The restriction in the RBM is that we do not allow for any intra-layer communication. This lets nodes independently create weights and biases that end up being (hopefully) independent features for our data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reconstructing the data</h1>
                </header>
            
            <article>
                
<p>In this forward pass of the network, we can see how data goes forward through the network (from the visible layer to the hidden layer), but that doesn't explain how the RBM is able to learn new features from our data without ground truths. This is done through multiple forward and backward passes through the network between our visible and hidden layer.</p>
<p>In the reconstruction phase, we switch the network around and let the hidden layer become the input layer and let it feed our activation variables (<kbd>a</kbd>) backwards into the visible layer using the same weights, but a new set of biases. The activated variables that we calculated during the forward pass are then used to reconstruct the original input vectors. The following visualization shows us how activations are fed backwards through our graph using the same weights and different biases:</p>
<div class="CDPAlignCenter CDPAlign"><img height="281" src="assets/20e88959-47cd-4c59-a7ea-f20841310bdf.png" width="462"/></div>
<p>This becomes the network's way of evaluating itself. By passing the activations backwards through the network and obtaining an approximation of the original input, the network can adjust the weights in order to make the approximations closer to the original input. Towards the beginning of training, because the weights are randomly initialized (this is standard practice), the approximations will likely be very far off. Backpropagation through the network, which occurs in the same direction as our forward pass (confusing, we know), then adjusts the weights to minimize the distance between original input and approximations. This process is then repeated until the approximations are as close to the original input as possible. The number of times this back and forth process occurs is called the number of <strong>iterations</strong>.</p>
<p>The end result of this process is a network that has an <em>alter-ego</em> for each data point. To transform data, we simply pass it through the network and retrieve the activation variables and call those the new features. This process is a type of <em>generative learning</em> that attempts to learn a probability distribution that generated the original data and exploit knowledge to give us a new feature set of our raw data.</p>
<p>For example, if we were given an image of a digit and asked to classify which digit (0-9) the image was of, the forward pass of the network asks the question, given these pixels, what digit should I expect? On the backwards pass, the network is asking given a digit, what pixels should I expect? This is called a <strong>joint probability</strong> and it is the simultaneous probability of <em>y </em>given <em>x </em>and <em>x </em>given <em>y,</em> and it is expressed as the shared weights between the two layers of our network.</p>
<p>Let's introduce our new dataset and let it elucidate the usefulness of RBMs in feature learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MNIST dataset</h1>
                </header>
            
            <article>
                
<p>The <kbd>MNIST</kbd> dataset consists of 6,000 images of handwritten digits between zero and nine and a ground-truth label to learn from. It is not unlike most of the other datasets that we have been working with in that we are attempting to fit a machine learning model to classify a response variable given a set of data points. The main difference here is that we are working with very low-level features as opposed to more interpretable features. Each data point will consist of 784 features (pixel values in a grey-scale image).</p>
<ol>
<li>Let's begin with our imports:</li>
</ol>
<pre style="padding-left: 60px"># import numpy and matplotlib<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>from sklearn import linear_model, datasets, metrics<br/># scikit-learn implementation of RBM<br/>from sklearn.neural_network import BernoulliRBM<br/>from sklearn.pipeline import Pipeline</pre>
<ol start="2">
<li>The new import is the BernoulliRBM, which is the only RBM implementation in scikit-learn as of now. As the name suggests, we will have to do a small amount of preprocessing to ensure that our data complies with the assumptions required. Let's import our dataset directly into a NumPy array:</li>
</ol>
<pre style="padding-left: 60px"># create numpy array from csv<br/>images = np.genfromtxt('../data/mnist_train.csv', delimiter=',')</pre>
<ol start="3">
<li>We can verify the number of rows and columns that we are working with:</li>
</ol>
<pre style="padding-left: 60px"># 6000 images and 785 columns, 28X28 pixels + 1 response<br/>images.shape<br/><br/>(6000, 785)</pre>
<ol start="4">
<li>The 785 is comprised of 784 pixels and a single response column in the beginning (first column). Every column besides the response column holds a value between zero and 255 representing pixel intensity, where zero means a white background and 255 means a fully black pixel. We can extract the <kbd>X</kbd> and <kbd>y</kbd> variables from the data by separating the first column from the rest of the data:</li>
</ol>
<pre style="padding-left: 60px"># extract the X and y variable<br/>images_X, images_y = images[:,1:], images[:,0]<br/><br/># values are much larger than 0-1 but scikit-learn RBM version assumes 0-1 scaling<br/>np.min(images_X), np.max(images_X)<br/>(0.0, 255.0)</pre>
<ol start="5">
<li>If we take a look at the first image, we will see what we are working with:</li>
</ol>
<pre style="padding-left: 60px">plt.imshow(images_X[0].reshape(28, 28), cmap=plt.cm.gray_r)<br/><br/>images_y[0]</pre>
<p class="mce-root">The plot is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><br/>
<img height="205" src="assets/396ff5d9-160d-431d-88cd-0ea40f3db828.png" style="color: #333333;font-family: Merriweather, serif;font-size: 1em" width="208"/></div>
<p>Looking good. Because the scikit-learn implementation of Restricted Boltzmann Machines will not allow for values outside of the range of 0-1, we will have to do a bit of preprocessing work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The BernoulliRBM</h1>
                </header>
            
            <article>
                
<p>The only scikit-learn<span> implemented version of a Restricted Boltzmann Machine</span> is called <strong>BernoulliRBM</strong> because it imposes a constraint on the type of probability distribution it can learn. The Bernoulli distribution allows for data values to be between zero and one. The scikit-learn documentation states that the model <span><em>assumes the inputs are either binary values or values between zero and one</em>. This is done to represent the fact that the node values represent a probability that the node is activated or not. It allows for quicker learning of feature sets. To account for this, we will alter our dataset to account for only hardcoded white/black pixel intensities. By doing so, every cell value will either be zero or one (white or black) to make learning more robust. We will accomplish this in two steps:</span></p>
<ol>
<li>We will scale the values of the pixels to be between zero and one</li>
<li>We will change the pixel values in place to be true if the value is over <kbd>0.5</kbd>, and false otherwise</li>
</ol>
<p>Let's start by scaling the pixel values to be between 0 and 1:</p>
<pre class="mce-root" style="padding-left: 60px"># scale images_X to be between 0 and 1<br/> images_X = images_X / 255.<br/> <br/> # make pixels binary (either white or black)<br/> images_X = (images_X &gt; 0.5).astype(float)<br/> <br/> np.min(images_X), np.max(images_X)<br/> (0.0, 1.0)</pre>
<p class="mce-root">Let's take a look at the same number five digit, as we did previously, with our newly altered pixels:</p>
<pre class="mce-root">plt.imshow(images_X[0].reshape(28, 28), cmap=plt.cm.gray_r)<br/> <br/> images_y[0]</pre>
<p class="mce-root">The plot is as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="195" src="assets/c8c8f0b8-57a4-43e1-87ed-bb6374c77d48.png" width="198"/></div>
<p>We can see that the fuzziness of the image has disappeared and we are left with a very crisp digit to classify with. Let's try now to extract features from our dataset of digits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting PCA components from MNIST</h1>
                </header>
            
            <article>
                
<p>Before we move to our RBM, let's take a look at what happens when we apply a PCA to our dataset. Like we did in the last chapter, we will take our features (784 pixels that are either on or off) and apply an eigenvalue decomposition to the matrix to extract <em>eigendigits</em> from the dataset.</p>
<p>Let's take 100 components from the possible 784 and plot the components to see what the extracted features look like. We will do this by importing our PCA module, fitting it to our data with 100 components, and creating a matplotlib gallery to display the top 100 components available to us:</p>
<pre class="mce-root"># import Principal Components Analysis module<br/> from sklearn.decomposition import PCA<br/> <br/> # extract 100 "eigen-digits"<br/> pca = PCA(n_components=100)<br/> pca.fit(images_X)<br/> <br/> # graph the 100 components<br/> plt.figure(figsize=(10, 10))<br/> for i, comp in enumerate(pca.components_):<br/> plt.subplot(10, 10, i + 1)<br/> plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r)<br/> plt.xticks(())<br/> plt.yticks(())<br/> plt.suptitle('100 components extracted by PCA')<br/> <br/> plt.show()</pre>
<p>The following is the plot of the preceding code block:</p>
<div class="CDPAlignCenter CDPAlign"><img height="546" src="assets/4e10bb9a-dc1c-492f-90ba-2c523541c708.png" width="499"/></div>
<p>This gallery of images is showing us what the eigenvalues of the covariance matrix look like when reshaped to the same dimensions as the original images. This is an example of what extracted components look like when we focus our algorithms on a dataset of images. It is quite interesting to take a sneak peek into how the PCA components are attempting to grab linear transformations from the dataset. Each component is attempting to understand a certain "aspect" of the images that will translate into interpretable knowledge. For example, the first (and most important) eigen-image is likely capturing an images 0-quality that is, how like a 0 the digit looks.</p>
<p>It also is evident that the first ten components seem to retain some of the shape of the digits and after that, they appear to start devolving into what looks like nonsensical images. By the end of the gallery, we appear to be looking at random assortments of black and white pixels swirling around. This is probably because PCA (and also LDA) are parametric transformations and they are limited in the amount of information they can extract from complex datasets like images.</p>
<p>If we take a look to see how much variance the first 30 components are explaining, we would see that they are able to capture the majority of the information:</p>
<pre class="mce-root"># first 30 components explain 64% of the variance<br/> <br/> pca.explained_variance_ratio_[:30].sum()<br/> .637414</pre>
<p>This tells us that the first few dozen components are doing a good job at capturing the essence of the data, but after that, the components are likely not adding too much.</p>
<p>This can be further seen in a scree plot showing us the cumulative explained variance for our PCA components:</p>
<pre class="mce-root"># Scree Plot<br/> <br/> # extract all "eigen-digits"<br/> full_pca = PCA(n_components=784)<br/> full_pca.fit(images_X)<br/> <br/> plt.plot(np.cumsum(full_pca.explained_variance_ratio_))<br/> <br/> # 100 components captures about 90% of the variance</pre>
<p>The following is the plot of the scree plot where the number of PCA components are on the <em>x</em> axis and the amount of cumulative variance explained lives on the <em>y</em> axis:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-303 image-border" height="249" src="assets/01e3257b-e9f3-430a-ad2a-39f16bcf8401.png" width="366"/></div>
<p>As we saw in the previous chapter, the transformations made by PCA are done through a single linear matrix operation by multiplying the components attribute of the PCA module with the data. We will show this again by taking the scikit-learn PCA object that we fit to 100 features and using it to transform a single MNIST image. We will take that transformed image and compare it the result of multiplying the original image with the <kbd>components_</kbd> attribute of the PCA module:</p>
<pre class="mce-root"># Use the pca object, that we have already fitted, to transform the first image in order to pull out the 100 new features<br/>pca.transform(images_X[:1])<br/> <br/>array([[ 0.61090568, 1.36377972, 0.42170385, -2.19662828, -0.45181077, -1.320495 , 0.79434677, 0.30551126, 1.22978985, -0.72096767, ...<br/> <br/># reminder that transformation is a matrix multiplication away<br/>np.dot(images_X[:1]-images_X.mean(axis=0), pca.components_.T)<br/> <br/>array([[ 0.61090568, 1.36377972, 0.42170385, -2.19662828, -0.45181077, -1.320495 , 0.79434677, 0.30551126, 1.22978985, -0.72096767,</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting RBM components from MNIST</h1>
                </header>
            
            <article>
                
<p>Let's now create our first RBM in scikit-learn. We will start by instantiating a module to extract 100 components from our <kbd>MNIST</kbd> dataset.</p>
<p>We will also set the verbose parameter to <kbd>True</kbd> to allow us visibility into the training process as well as the <kbd>random_state</kbd> parameter to <kbd>0</kbd>. The <span><kbd>random_state</kbd> parameter is an integer that allows for reproducibility in code. It fixes the random number generator and sets the weights and biases <em>randomly</em> at the same time, every time. We finally let <kbd>n_iter</kbd> be <kbd>20</kbd>. This is the number of iterations we wish to do, or back and forth passes of the network:</span></p>
<pre class="mce-root"># instantiate our BernoulliRBM<br/> # we set a random_state to initialize our weights and biases to the same starting point<br/> # verbose is set to True to see the fitting period<br/> # n_iter is the number of back and forth passes<br/> # n_components (like PCA and LDA) represent the number of features to create<br/> # n_components can be any integer, less than , equal to, or greater than the original number of features<br/> <br/> rbm = BernoulliRBM(random_state=0, verbose=True, n_iter=20, n_components=100)<br/> <br/> rbm.fit(images_X)<br/> <br/> [BernoulliRBM] Iteration 1, pseudo-likelihood = -138.59, time = 0.80s<br/> [BernoulliRBM] Iteration 2, pseudo-likelihood = -120.25, time = 0.85s [BernoulliRBM] Iteration 3, pseudo-likelihood = -116.46, time = 0.85s ... [BernoulliRBM] Iteration 18, pseudo-likelihood = -101.90, time = 0.96s [BernoulliRBM] Iteration 19, pseudo-likelihood = -109.99, time = 0.89s [BernoulliRBM] Iteration 20, pseudo-likelihood = -103.00, time = 0.89s</pre>
<p>Once training is complete; we can explore the end result of the process. RBM also has a <kbd>components</kbd> module, like PCA does:</p>
<pre class="mce-root"># RBM also has components_ attribute<br/> len(rbm.components_)<br/> <br/> 100</pre>
<p class="mce-root">We can also plot the RBM components that were learned by the module to see how they differ from our eigendigits:</p>
<pre class="mce-root"># plot the RBM components (representations of the new feature sets)<br/> plt.figure(figsize=(10, 10))<br/> for i, comp in enumerate(rbm.components_):<br/> plt.subplot(10, 10, i + 1)<br/> plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r)<br/> plt.xticks(())<br/> plt.yticks(())<br/> plt.suptitle('100 components extracted by RBM', fontsize=16)<br/> <br/> plt.show()</pre>
<p>The following is the result of the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-210 image-border" height="477" src="assets/b0691218-0d6d-463d-94a3-05cdd4ef3c01.png" width="435"/></div>
<p>These features look very interesting. Where the PCA components became visual distortions after a while, the RBM components seem to be extracting various shapes and pen strokes with each component. At first glance, it looks like we have repeat features (for example, feature 15, 63, 64, and 70). We can do a quick NumPy check to see if any of the components are actually repeating, or if they are just very similar.</p>
<p>This code will check to see how many unique elements exist in <kbd>rbm.components_</kbd>. If the resulting shape has 100 elements in it, that means that every component of the RBM is in fact different:</p>
<pre class="mce-root"># It looks like many of these components are exactly the same but<br/> <br/> # this shows that all components are actually different (albiet some very slightly) from one another<br/> np.unique(rbm.components_.mean(axis=1)).shape<br/> <br/> (100,)</pre>
<p>This validates that our components are all unique from one another. We can use the RBM to transform data like we can with PCA by utilizing the <kbd>transform</kbd> method within the module:</p>
<pre class="mce-root"># Use our Boltzman Machine to transform a single image of a 5<br/> image_new_features = rbm.transform(images_X[:1]).reshape(100,)<br/> <br/> image_new_features<br/> <br/> array([ 2.50169424e-16, 7.19295737e-16, 2.45862898e-09, 4.48783657e-01, 1.64530318e-16, 5.96184335e-15, 4.60051698e-20, 1.78646959e-08, 2.78104276e-23, ...</pre>
<p>And we can also see that these components are <strong>not</strong> used in the same way as PCAs are, meaning that a simple matrix multiplication will not yield the same transformation as invoking the <kbd>transform</kbd> method embedded within the module:</p>
<pre class="mce-root"># not the same as a simple matrix multiplication anymore<br/> # uses neural architecture (several matrix operations) to transform features<br/> np.dot(images_X[:1]-images_X.mean(axis=0), rbm.components_.T)<br/> <br/> array([[ -3.60557365, -10.30403384, -6.94375031, 14.10772267, -6.68343281, -5.72754674, -7.26618457, -26.32300164, ...</pre>
<p>Now that we know that we have 100 new features to work with and we've seen them, let's see how they interact with our data.</p>
<p>Let's start by grabbing the <kbd>20</kbd> most represented features for the first image in our dataset, the digit 5:</p>
<pre class="mce-root"># get the most represented features<br/> top_features = image_new_features.argsort()[-20:][::-1]<br/> <br/> print top_features<br/> <br/> [56 63 62 14 69 83 82 49 92 29 21 45 15 34 28 94 18 3 79 58]<br/> <br/> <br/> print image_new_features[top_features]<br/> <br/> array([ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0.99999999, 0.99999996, 0.99999981, 0.99996997, 0.99994894, 0.99990515, 0.9996504 , 0.91615702, 0.86480507, 0.80646422, 0.44878366, 0.02906352, 0.01457827])</pre>
<p>In this case, we actually have seven features in which the RBM has a full 100%. In our graph, this means that passing in these 784 pixels into our visible layers lights up nodes 56, 63, 62, 14, 69, 83, and 82 at full capacity. Let's isolate these features:</p>
<pre class="mce-root"># plot the RBM components (representations of the new feature sets) for the most represented features<br/> plt.figure(figsize=(25, 25))<br/> for i, comp in enumerate(top_features):<br/> plt.subplot(5, 4, i + 1)<br/> plt.imshow(rbm.components_[comp].reshape((28, 28)), cmap=plt.cm.gray_r)<br/> plt.title("Component {}, feature value: {}".format(comp, round(image_new_features[comp], 2)), fontsize=20)<br/> plt.suptitle('Top 20 components extracted by RBM for first digit', fontsize=30)<br/> <br/> plt.show()</pre>
<p>We get the following result for the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-304 image-border" height="466" src="assets/ae5253eb-ec7b-4787-9a97-02b40badec81.png" width="407"/></div>
<p>Taking a look at some of these, they make quite a lot of sense. <strong>Component 45</strong> seems to isolate the top-left corner of the digit <strong>5</strong>, while <strong>Component 21</strong> seems to grab the bottom loop of the digit. <strong>Component 82</strong> and <strong>Component 34</strong> seem to grab almost an entire 5 in one go. Let's see what the bottom of the barrel looks like for the number 5 by isolating the bottom 20 features that lit up in the RBM graph when these pixels were passed through:</p>
<pre class="mce-root"># grab the least represented features<br/> bottom_features = image_new_features.argsort()[:20]<br/> <br/> plt.figure(figsize=(25, 25))<br/> for i, comp in enumerate(bottom_features):<br/> plt.subplot(5, 4, i + 1)<br/> plt.imshow(rbm.components_[comp].reshape((28, 28)), cmap=plt.cm.gray_r)<br/> plt.title("Component {}, feature value: {}".format(comp, round(image_new_features[comp], 2)), fontsize=20)<br/> plt.suptitle('Bottom 20 components extracted by RBM for first digit', fontsize=30)<br/> <br/> plt.show()</pre>
<p>We get the following plot for the preceding code:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-305 image-border" height="458" src="assets/e54d08a5-bbb2-4253-934f-e637c13655ce.png" width="398"/></div>
<p><strong>Component 13</strong>, <strong>Component 4</strong>, <strong>Component 97</strong>, and others seem to be trying to reveal different digits and not a 5, so it makes sense that these components are not being lit up by this combination of pixel strengths.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using RBMs in a machine learning pipeline</h1>
                </header>
            
            <article>
                
<p>Of course, we want to see how the RBM performs in our machine learning pipelines to not just visualize the workings of the model, but to see concrete results of the feature learning. To do this, we will create and run three pipelines:</p>
<ul>
<li>A logistic regression model by itself running on the raw pixel strengths</li>
<li>A logistic regression running on extracted PCA components</li>
<li>A logistic regression running on extracted RBM components</li>
</ul>
<p>Each of these pipelines will be grid-searched across a number of components (for PCA and RBM) and the <kbd>C</kbd> parameter for logistic regression. Let's start with our simplest pipeline. We will run the raw pixel values through a logistic regression to see if the linear model is enough to separate out the digits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a linear model on raw pixel values</h1>
                </header>
            
            <article>
                
<p>To begin, we will run the raw pixel values through a logistic regression model in order to obtain something of a baseline model. We want to see if utilizing PCA or RBM components will allow the same linear classifier to perform better or worse. If we can find that the extracted latent features are performing better (in terms of accuracy of our linear model) then we can be sure it is the feature engineering that we are employing that is enhancing our pipeline, and nothing else.</p>
<p>First we will create our instantiated modules:</p>
<pre class="mce-root"># import logistic regression and gridsearch module for some machine learning<br/> <br/> from sklearn.linear_model import LogisticRegression<br/> from sklearn.model_selection import GridSearchCV<br/> <br/> # create our logistic regression<br/> lr = LogisticRegression()<br/> params = {'C':[1e-2, 1e-1, 1e0, 1e1, 1e2]}<br/><br/> # instantiate a gridsearch class<br/> grid = GridSearchCV(lr, params)</pre>
<p>Once we done this, we can fit our module to our raw image data. This will give us a rough idea of how the raw pixel data performs in a machine learning pipeline:</p>
<pre class="mce-root"> # fit to our data<br/> grid.fit(images_X, images_y)<br/> <br/> # check the best params<br/> grid.best_params_, grid.best_score_<br/> <br/> ({'C': 0.1}, 0.88749999999999996)</pre>
<p>Logistic regression by itself does a decent job at using the raw pixel values to identify digits by giving about an <strong>88.75% cross-validated accuracy</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a linear model on extracted PCA components</h1>
                </header>
            
            <article>
                
<p>Let's see if we can add in a PCA component to the pipeline to enhance this accuracy. We will begin again by setting up our variables. This time we will need to create a scikit-learn pipeline object to house the PCA module as well as our linear model. We will keep the same parameters that we used for the linear classifier and add new parameters for our PCA. We will attempt to find the optimal number of components between 10, 100, and 200 components. Try to take a moment and hypothesize which of three will end up being the best (hint, think back to the scree plot and explained variance):</p>
<pre class="mce-root"># Use PCA to extract new features<br/> <br/>lr = LogisticRegression()<br/>pca = PCA()<br/> <br/># set the params for the pipeline<br/>params = {'clf__C':[1e-1, 1e0, 1e1],<br/>'pca__n_components': [10, 100, 200]}<br/> <br/># create our pipeline<br/>pipeline = Pipeline([('pca', pca), ('clf', lr)])<br/> <br/># instantiate a gridsearh class<br/>grid = GridSearchCV(pipeline, params)</pre>
<p>We can now fit the gridsearch object to our raw image data. Note that the pipeline will take care of automatically extracting features from and transforming our raw pixel data:</p>
<pre class="mce-root"> # fit to our data<br/>grid.fit(images_X, images_y)<br/> <br/># check the best params<br/>grid.best_params_, grid.best_score_<br/> <br/>({'clf__C': 1.0, 'pca__n_components': 100}, 0.88949999999999996)</pre>
<p>We end up with a (slightly better) <strong>88.95% cross-validated accuracy.</strong> If we think about it, we should not be surprised that 100 was the best option out of 10, 100, and 200. From our brief analysis with the scree plot in a previous section, we found that 64% of the data was explained by a mere 30 components, so 10 components would definitely not be enough to explain the images well. The scree plot also started to level out at around 100 components, meaning that after the 100th component, the explained variance was truly not adding much, so 200 was too many components to use and would have started to lead to some overfitting. That leaves us with 100 as being the optimal number of PCA components to use. It should be noted that we could go further and attempt some hyper-parameter tuning to find an even more optimal number of components, but for now we will leave our pipeline as is and move to using RBM components.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a linear model on extracted RBM components</h1>
                </header>
            
            <article>
                
<p>Even the optimal number of PCA components was unable to beat the logistic regression alone by much in terms of accuracy. Let's see how our RBM does. To make the following pipeline, we will keep the same parameters for the logistic regression model and find the optimal number of components between 10, 100, and 200 (like we did for the PCA pipeline). Note that we could try to expand the number of features past the number of raw pixels (784) but we will not attempt to.</p>
<p>We begin the same way by setting up our variables:</p>
<pre class="mce-root"># Use the RBM to learn new features<br/> <br/>rbm = BernoulliRBM(random_state=0)<br/> <br/># set up the params for our pipeline.<br/>params = {'clf__C':[1e-1, 1e0, 1e1],<br/>'rbm__n_components': [10, 100, 200]<br/>}<br/> <br/># create our pipeline<br/>pipeline = Pipeline([('rbm', rbm), ('clf', lr)])<br/> <br/># instantiate a gridsearch class<br/>grid = GridSearchCV(pipeline, params)</pre>
<p>Fitting this grid search to our raw pixels will reveal the optimal number of components:</p>
<pre class="mce-root"># fit to our data<br/>grid.fit(images_X, images_y)<br/> <br/># check the best params<br/>grid.best_params_, grid.best_score_<br/> <br/>({'clf__C': 1.0, 'rbm__n_components': 200}, 0.91766666666666663)</pre>
<div class="cell code_cell rendered running unselected">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<p class="CodeMirror cm-s-ipython">Our RBM module, with a <strong>91.75% cross-validated accuracy</strong>, was able to extract 200 new features from our digits and give us a boost of three percent in accuracy (which is a lot!) by not doing anything other than adding the BernoulliRBM module into our pipeline.</p>
</div>
</div>
</div>
</div>
<div class="packt_tip">The fact that 200 was the optimal number of components suggests that we may even obtain a higher performance by trying to extract more than 200 components. We will leave this as an exercise to the reader.</div>
<p>This is evidence to the fact that feature learning algorithms work very well when dealing with very complex tasks such as image recognition, audio processing, and natural language processing. These large and interesting datasets have hidden components that are difficult for linear transformations like PCA or LDA to extract but non-parametric algorithms like RBM can.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning text features – word vectorizations</h1>
                </header>
            
            <article>
                
<p>Our second example of feature learning will move away from images and towards text and natural language processing. When machines learn to read/write, they face a very large problem, context. In previous chapters, we have been able to vectorize documents by counting the number of words that appeared in each document and we fed those vectors into machine learning pipelines. By constructing new count-based features, we were able to use text in our supervised machine learning pipelines. This is very effective, up until a point. We are limited to only being to understand text as if they were only a <strong>Bag of Words</strong> (<strong>BOW</strong>). This means that we regard documents as being nothing more than a collection of words out of order.</p>
<p>What's more is that each word on its own has no meaning. It is only in a collection of other words that a document can have meaning when using modules such as <kbd>CountVectorizer</kbd> and <kbd>TfidfVectorizer</kbd>. It is for this reason that we will turn our attention away from scikit-learn and onto a module called <kbd>gensim</kbd> for computing word embeddings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word embeddings</h1>
                </header>
            
            <article>
                
<p>Up until this point, we have used scikit-learn to embed documents (tweets, reviews, URLs, and so on) into a vectorized format by regarding tokens (words, n-grams) as features and documents as having a certain amount of these tokens. For example, if we had 1,583 documents and we told our <kbd>CountVectorizer</kbd> to learn the top 1,000 tokens of <kbd>ngram_range</kbd> from one to five, we would end up with a matrix of shape (1583, 1000) where each row represented a single document and the 1,000 columns represented literal n-grams found in the corpus. But how do we achieve an even lower level of understanding? How do we start to teach the machine what words <em>mean</em> in context?</p>
<p><span>For example, if we were to ask you the following questions, you may give the following answers:</span></p>
<p><em>Q: What would you get if we took a king, removed the man aspect of it, and replaced it with a woman?</em></p>
<p><strong><em>A: A queen</em></strong></p>
<p><em>Q: London is to England as Paris is to ____.</em></p>
<p><strong><em>A: France</em></strong></p>
<p>You, a human, may find these questions simple, but how would a machine figure this out without knowing what the words by themselves mean in context? This is, in fact, one of the greatest challenges that we face in <strong>natural language processing</strong> (<strong>NLP</strong>) tasks.</p>
<p>Word embeddings are one approach to helping a machine understand context. A<span> </span><strong>word embedding</strong><span> </span>is a vectorization of a single word in a feature space of n dimensions, where<span> </span><em>n</em><span> </span>represents the number of latent characteristics that a word can have. This means that every word in our vocabulary is not longer, just a string, but a vector in and of itself. For example, if we extracted n=5 characteristics about each word, then each word in our vocabulary would correspond to a 1 x 5 vector. For example, we might have the following vectorizations:</p>
<pre class="CDPAlignCenter CDPAlign CDPAlignLeft"># set some fake word embeddings<br/>king = np.array([.2, -.5, .7, .2, -.9])<br/>man = np.array([-.5, .2, -.2, .3, 0.])<br/>woman = np.array([.7, -.3, .3, .6, .1])<br/><br/>queen = np.array([ 1.4, -1. , 1.2, 0.5, -0.8])</pre>
<p>And with these vectorizations, we can tackle the question <em>What would you get if we took a king, removed the man aspect of it, and replaced it with a woman?</em> by performing the following operation:</p>
<p style="padding-left: 240px"><em>king - man + woman</em></p>
<p>In code, this would look like:</p>
<pre>np.array_equal((king - man + woman), queen)<br/><br/>True</pre>
<p>This seems simple but it does some with a few caveats:</p>
<ul>
<li>Context (in the form of word embeddings) changes from corpus to corpus as does word meanings. This means that static word embeddings by themselves are not always the most useful</li>
<li>Word embeddings are dependent on the corpus that they were learned from</li>
</ul>
<p>Word embeddings allow us to perform very precise calculations on single words to achieve what we might consider context.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Two approaches to word embeddings - Word2vec and GloVe</h1>
                </header>
            
            <article>
                
<p>There are two families of algorithms that dominate the space of word embeddings. They are called <strong>Word2vec </strong>and <strong>GloVe</strong>. Both methods are responsible for producing word embeddings by learning from very large corpus (collection of text documents). The main difference between these two algorithms is that the GloVe algorithm, out of Stanford, learns word embeddings through a series of matrix statistics while Word2vec, out of Google, learns them through a deep learning approach. Both algorithms have merits and our text will focus on using the Word2vec algorithm to learn word embeddings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word2Vec - another shallow neural network</h1>
                </header>
            
            <article>
                
<p>In order to learn and extract word embeddings, Word2vec will implement another shallow neural network. This time, instead of generically throwing in new data into our visible layer, we will deliberately put in the correct data to give us the right word embeddings. Without going too into much detail, imagine a neural architecture with the following structure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="303" src="assets/8f071838-2990-44e4-bbdf-7953d705d259.png" width="404"/></div>
<p>Like the RBM, we have a visible input layer and a hidden layer. In this case, our input layer has as many nodes as the length of vocabulary that we wish to learn from. This comes in handy if we have a corpus of millions of words but we wish to only learn a handful of them. In the preceding graph, we would be learning the context of 5,000 words. The hidden layer in this graph represents the number of features we wish to learn about each word. In this case, we will be embedding words into a 300-dimensional space.</p>
<p>The main difference between this neural network and the one we used for RBM is the existence of an output layer. Note that in our graph, the output layer has as many nodes as the input layer. This is not a coincidence. Word embedding models work by <em>predicting</em> nearby words based on the existence of a reference word. For example, if we were to predict the word <em>calculus</em>, we would want the final <kbd>math</kbd> node to light up the most. This gives the semblance of a supervised machine learning algorithm.</p>
<p>We then train the graph on this structure and eventually learn the 300-dimension word representations by passing in one-hot vectors of words and extracting the hidden layer's output vector and using that as our latent structure. In production, the previous diagram is extremely inefficient due to the very large number of output nodes. In order to make this process extremely more computationally efficient, different loss functions are utilized to capitalize on the text's structure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The gensim package for creating Word2vec embeddings</h1>
                </header>
            
            <article>
                
<p>We will not be implementing a full working neural network that performs the word embedding procedure, however we will be using a Python package called <kbd>gensim</kbd> to do this work for us:</p>
<pre class="mce-root"># import the gensim package<br/> import gensim</pre>
<p>A <kbd>gensim</kbd> can take in a corpora of text and run the preceding neural network structure for us and obtain word embeddings with only a few lines of code. To see this in action, let's import a standard corpus to get started. Let's set a logger in our notebook so that we can see the training process in a bit more detail:</p>
<pre class="mce-root">import logging<br/> <br/> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)</pre>
<p class="mce-root">Now, let's create our corpus:</p>
<pre class="mce-root">from gensim.models import word2vec, Word2Vec<br/> <br/>sentences = word2vec.Text8Corpus('../data/text8')</pre>
<p>Notice the term <kbd>word2vec</kbd>. This is a specific algorithm used to calculate word embeddings and the main algorithm used by <kbd>gensim</kbd>. It is one of the standards for word embeddings.</p>
<p class="mce-root">For gensim to do its job, sentences needs to be any iterable (list, generator, tuple, and so on) that holds sentences that are already tokenized. Once we have such a variable, we can put <kbd>gensim</kbd> to work by learning word embeddings:</p>
<pre class="mce-root"># instantiate a gensim module on the sentences from above<br/> # min_count allows us to ignore words that occur strictly less than this value<br/> # size is the dimension of words we wish to learn<br/> model = gensim.models.Word2Vec(sentences, min_count=1, size=20)<br/> <br/> <br/> 2017-12-29 16:43:25,133 : INFO : collecting all words and their counts<br/> 2017-12-29 16:43:25,136 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types<br/> 2017-12-29 16:43:31,074 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences<br/> 2017-12-29 16:43:31,075 : INFO : Loading a fresh vocabulary<br/> 2017-12-29 16:43:31,990 : INFO : min_count=1 retains 253854 unique words (100% of original 253854, drops 0)<br/> 2017-12-29 16:43:31,991 : INFO : min_count=1 leaves 17005207 word corpus (100% of original 17005207, drops 0)<br/> 2017-12-29 16:43:32,668 : INFO : deleting the raw counts dictionary of 253854 items<br/> 2017-12-29 16:43:32,676 : INFO : sample=0.001 downsamples 36 most-common words<br/> 2017-12-29 16:43:32,678 : INFO : downsampling leaves estimated 12819131 word corpus (75.4% of prior 17005207)<br/> 2017-12-29 16:43:32,679 : INFO : estimated required memory for 253854 words and 20 dimensions: 167543640 bytes<br/> 2017-12-29 16:43:33,431 : INFO : resetting layer weights<br/> 2017-12-29 16:43:36,097 : INFO : training model with 3 workers on 253854 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5<br/> 2017-12-29 16:43:37,102 : INFO : PROGRESS: at 1.32% examples, 837067 words/s, in_qsize 5, out_qsize 0<br/> 2017-12-29 16:43:38,107 : INFO : PROGRESS: at 2.61% examples, 828701 words/s,<br/> ... 2017-12-29 16:44:53,508 : INFO : PROGRESS: at 98.21% examples, 813353 words/s, in_qsize 6, out_qsize 0 2017-12-29 16:44:54,513 : INFO : PROGRESS: at 99.58% examples, 813962 words/s, in_qsize 4, out_qsize 0<br/> ... 2017-12-29 16:44:54,829 : INFO : training on 85026035 raw words (64096185 effective words) took 78.7s, 814121 effective words/s</pre>
<div class="output_area">
<p class="output_subarea output_text output_stream output_stderr"><span>This line of code will start the learning process. If you are passing in a large corpus, it may take a while. Now that the <kbd>gensim</kbd> module is done fitting, we can use it. We can grab individual embeddings by passing strings into the <kbd>word2vec</kbd> object:</span></p>
</div>
<pre class="mce-root"># get the vectorization of a word<br/> model.wv['king']<br/> <br/> array([-0.48768288, 0.66667134, 2.33743191, 2.71835423, 4.17330408, 2.30985498, 1.92848825, 1.43448424, 3.91518641, -0.01281452, 3.82612252, 0.60087812, 6.15167284, 4.70150518, -1.65476751, 4.85853577, 3.45778084, 5.02583361, -2.98040175, 2.37563372], dtype=float32)</pre>
<p>The <kbd>gensim</kbd> has built-in methods to get the most out of our word embeddings. For example, to answer the question about the <kbd>king</kbd>, we can use the the <kbd>most_similar</kbd> method:</p>
<pre class="mce-root"># woman + king - man = queen<br/> model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)<br/> <br/> [(u'emperor', 0.8988120555877686), (u'prince', 0.87584388256073), (u'consul', 0.8575721979141235), (u'tsar', 0.8558996319770813), (u'constantine', 0.8515684604644775), (u'pope', 0.8496872782707214), (u'throne', 0.8495982885360718), (u'elector', 0.8379884362220764), (u'judah', 0.8376096487045288), (u'emperors', 0.8356839418411255)]</pre>
<p>Hmm, unfortunately this isn't giving us the answer we'd expect: <kbd>queen</kbd>. Let's try the <kbd>Paris</kbd> word association:</p>
<pre class="mce-root"># London is to England as Paris is to ____<br/> model.wv.most_similar(positive=['Paris', 'England'], negative=['London'], topn=1)<br/> <br/> <span class="ansi-red-fg">KeyError</span>: "word 'Paris' not in vocabulary"</pre>
<p>It appears that the word <kbd>Paris</kbd> was never even learned as it did not appear in our corpus. We can start to see the limitations to this procedure. Our embeddings will only be as good as the corpus we are selecting and the machines we use to calculate these embeddings. In our data directory, we have provided a pre-trained vocabulary of words that spans across 3,000,000 words found on websites indexed by Google with 300 dimensions learned for each word.</p>
<p>Let's go ahead and import these pre-trained embeddings. We can do this by using built-in importer tools in <kbd>gensim</kbd>:</p>
<pre class="mce-root"># use a pretrained vocabulary with 3,000,000 words<br/> import gensim<br/> <br/> model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)<br/> <br/> # 3,000,000 words in our vocab<br/> len(model.wv.vocab)<br/> <br/> 3000000</pre>
<p>These embeddings have been trained using vastly more powerful machines than anything we have at home and for a much longer period of time. Let's try our word problems out now:</p>
<pre class="mce-root"># woman + king - man = queen<br/> model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)<br/> <br/> [(u'queen', 0.7118192911148071)]<br/> <br/> # London is to England as Paris is to ____<br/> model.wv.most_similar(positive=['Paris', 'England'], negative=['London'], topn=1)<br/> <br/> [(u'France', 0.6676377654075623)]</pre>
<p>Excellent! It seems as though these word embeddings were trained enough to allow us to answer these complex word puzzles. The <kbd>most_similar</kbd> method, as used previously, will return the token in the vocabulary that is most similar to the words provided. Words in the <kbd>positive</kbd> list are vectors added to one another, while words in the <kbd>negative</kbd> list are subtracted from the resulting vector. The following graph provides a visual representation of how we use word vectors to extract meaning:</p>
<div class="CDPAlignCenter CDPAlign"><img height="326" src="assets/1c327f18-9c2e-4bd7-9274-64ac71200591.png" width="394"/></div>
<p class="mce-root">Here, we are starting with the vector representation for <strong>king</strong> and adding to it the concept (vector) for <strong>woman</strong>. From there, we subtract the <strong>man</strong> vector (by adding the negative of the vector) to obtain the dotted vector. This vector is the most similar to the vector representation for <strong>queen</strong>. This is how we obtain the formula:</p>
<p class="mce-root" style="padding-left: 150px"><em>king + woman - man = queen</em></p>
<p class="mce-root">The <kbd>gensim</kbd> has other methods that we may utilize such as <kbd>doesnt_match</kbd>. This method singles out words that do not belong to a list of words. It does so by isolating the word that is the most dissimilar on average to the rest of the words. For example, if we give the method four words, three of which are animals and the other is a plant, we hope it will figure out which of those doesn't belong:</p>
<pre class="mce-root"># Pick out the oddball word in a sentence<br/>model.wv.doesnt_match("duck bear cat tree".split())<br/> <br/>'tree'</pre>
<p>The package also includes methods to calculate a 0-1 score of similarity between single words that can be used to compare words on the fly:</p>
<pre class="mce-root"># grab a similarity score between 0 and 1<br/><br/># the similarity between the words woman and man, pretty similar<br/>model.wv.similarity('woman', 'man')<br/>0.766401223<br/> <br/># similarity between the words tree and man, not very similar<br/>model.wv.similarity('tree', 'man')<br/>0.229374587</pre>
<p>Here, we can see that <kbd>man</kbd> is more similar to <kbd>woman</kbd> than <kbd>man</kbd> is to <kbd>tree</kbd>. We can use these helpful methods in order to implement some useful applications of word embeddings that would not be possible otherwise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Application of word embeddings - information retrieval</h1>
                </header>
            
            <article>
                
<p>There are countless applications for word embeddings; one of these is in the field of information retrieval. When humans input keywords and key phrases into search engines, search engines are able to recall and surface specific articles/stories that match those keywords exactly. For example, if we search for articles about dogs, we will get articles that mention the word dog. But what if we search for the word canine? We should still expect to see articles about dogs based on the fact that canines are dogs. Let's implement a simple information retrieval system to showcase the power of word embeddings.</p>
<p><span>Let's create a function that tries to grab embeddings of individual words from our gensim package and returns None if this lookup fails:</span></p>
<pre class="mce-root"># helper function to try to grab embeddings for a word and returns None if that word is not found<br/>def get_embedding(string):<br/>    try:<br/>        return model.wv[string]<br/>    except:<br/>        return None</pre>
<p>Now, let's create three article titles, one about a <kbd>dog</kbd>, one about a <kbd>cat</kbd>, and one about absolutely <kbd>nothing</kbd> at all for a distractor:</p>
<pre class="mce-root"># very original article titles<br/> sentences = [<br/> "this is about a dog",<br/> "this is about a cat",<br/> "this is about nothing"<br/>]</pre>
<p>The goal is to input a reference word that is similar to <kbd>dog</kbd> or <kbd>cat</kbd> and be able to grab the more relevant title. To do this, we will first create a 3 x 300 matrix of vectorizations for each sentence. We will do this by taking the mean of every word in the sentence and using the resulting mean vector as an estimation of the vectorization of the entire sentence. Once we have a vectorization of every sentence, we can compare that against the embedding of the reference word by taking a dot product between them. The closest vector is the one with the largest dot product:</p>
<pre class="mce-root"># Zero matrix of shape (3,300)<br/>vectorized_sentences = np.zeros((len(sentences),300))<br/># for every sentence<br/>for i, sentence in enumerate(sentences):<br/>    # tokenize sentence into words<br/>    words = sentence.split(' ')<br/>    # embed whichever words that we can<br/>    embedded_words = [get_embedding(w) for w in words]<br/>    embedded_words = filter(lambda x:x is not None, embedded_words)<br/>    # Take a mean of the vectors to get an estimate vectorization of the sentence<br/>    vectorized_sentence = reduce(lambda x,y:x+y, embedded_words)/len(embedded_words)<br/>    # change the ith row (in place) to be the ith vectorization<br/>    vectorized_sentences[i:] = vectorized_sentence<br/> <br/>vectorized_sentences.shape<br/>(3, 300)</pre>
<p class="mce-root">One thing to notice here is that we are creating a vectorization of documents (collection of words) and not considering the order of the words. How is this better than utilizing a <kbd>CountVectorizer</kbd> or a <kbd>TfidfVectorizer</kbd> to grab a count-based vectorization of text? The gensim method is attempting to project our text onto a latent structure learned by the context of individual words, while the scikit-learn vectorizers are only able to use the vocab at our disposal to create our vectorizations. In these three sentences, there are only seven unique words:</p>
<p class="mce-root"><kbd>this</kbd>, <kbd>is</kbd>, <kbd>about</kbd>, <kbd>a</kbd>, <kbd>dog</kbd>, <kbd>cat</kbd>, <kbd>nothing</kbd></p>
<p>So, the maximum shape our <kbd>Count<span>Vectorizer</span></kbd> or <kbd>TfidfVectorizer</kbd> can project is (3, 7). Let's try to grab the most relevant sentence to the word <kbd>dog</kbd>:</p>
<pre class="mce-root"># we want articles most similar to the reference word "dog"<br/>reference_word = 'dog'<br/> <br/># take a dot product between the embedding of dof and our vectorized matrix<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]<br/> <br/># output the most relevant sentence<br/>sentences[best_sentence_idx]<br/> <br/>'this is about a dog'</pre>
<p>That one was easy. Given the word <kbd>dog</kbd>, we should be able to retrieve the sentence about a <kbd>dog</kbd>. This should also hold true if we input the word <kbd>cat</kbd>:</p>
<pre class="mce-root">reference_word = 'cat'<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]<br/> <br/>sentences[best_sentence_idx]<br/> <br/>'this is about a cat'</pre>
<p>Now, let's try something harder. Let's input the words <kbd>canine</kbd> and <kbd>tiger</kbd> and see if we get the <kbd>dog</kbd> and <kbd>cat</kbd> sentences respectively:</p>
<pre class="mce-root">reference_word = 'canine'<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]<br/> <br/>print sentences[best_sentence_idx]<br/> <br/>'this is about a dog'<br/> <br/>reference_word = 'tiger'<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]<br/> <br/>print sentences[best_sentence_idx]<br/> <br/>'this is about a cat'</pre>
<p>Let's try a slightly more interesting example. The following are chapter titles from Sinan's first book,<em> Principles of Data Science</em>:</p>
<pre class="mce-root"># Chapter titles from Sinan's first book, "The Principles of Data Science<br/> <br/> sentences = """How to Sound Like a Data Scientist<br/> Types of Data<br/> The Five Steps of Data Science<br/> Basic Mathematics<br/> A Gentle Introduction to Probability<br/> Advanced Probability<br/> Basic Statistics<br/> Advanced Statistics<br/> Communicating Data<br/> Machine Learning Essentials<br/> Beyond the Essentials<br/> Case Studies """.split('\n')</pre>
<p>This will give us a list of 12 different chapter titles to retrieve from. The goal then will be to use a reference word to sort and serve up the top three most relevant chapter titles to read, given the topic. For example, if we asked our algorithm to give us chapters relating to <em>math</em>, we might expect to be recommended the chapters about basic mathematics, statistics, and probability.</p>
<p>Let's try to see which chapters are the best to read, given human input. Before we do so, let's calculate a matrix of vectorized documents like we did with our previous three sentences:</p>
<pre class="mce-root"># Zero matrix of shape (3,300)<br/>vectorized_sentences = np.zeros((len(sentences),300))<br/># for every sentence<br/>for i, sentence in enumerate(sentences):<br/>    # tokenize sentence into words<br/>    words = sentence.split(' ')<br/>    # embed whichever words that we can<br/>    embedded_words = [get_embedding(w) for w in words]<br/>    embedded_words = filter(lambda x:x is not None, embedded_words)<br/>    # Take a mean of the vectors to get an estimate vectorization of the sentence<br/>    vectorized_sentence = reduce(lambda x,y:x+y, embedded_words)/len(embedded_words)<br/>    # change the ith row (in place) to be the ith vectorization<br/>    vectorized_sentences[i:] = vectorized_sentence<br/> <br/>vectorized_sentences.shape<br/>(12, 300)</pre>
<p>Now, let's find the chapters that are most related to <kbd>math</kbd>:</p>
<pre class="mce-root"># find chapters about math<br/>reference_word = 'math'<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-3:][::-1]<br/> <br/>[sentences[b] for b in best_sentence_idx]<br/> <br/>['Basic Mathematics', 'Basic Statistics', 'Advanced Probability ']</pre>
<div class="cell code_cell rendered unselected">
<div class="input">
<div class="inner_cell">
<p class="CodeMirror cm-s-ipython">Now, let's say we are giving a talk about data and want to know which chapters are going to be the most helpful in that area:</p>
<pre class="input_area"># which chapters are about giving talks about data<br/>reference_word = 'talk'<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-3:][::-1]<br/> <br/>[sentences[b] for b in best_sentence_idx]<br/> <br/>['Communicating Data ', 'How to Sound Like a Data Scientist', 'Case Studies ']</pre></div>
</div>
</div>
<p>And finally, which chapters are about <kbd>AI</kbd>:</p>
<pre class="mce-root"># which chapters are about AI<br/>reference_word = 'AI'<br/>best_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-3:][::-1]<br/> <br/>[sentences[b] for b in best_sentence_idx]<br/> <br/>['Advanced Probability ', 'Advanced Statistics', 'Machine Learning Essentials']</pre>
<p>We can see how we can use word embeddings to retrieve information in the form of text given context learned from the universe of text.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter focused on two feature learning tools: RBM and word embedding processes.</p>
<p>Both of these processes utilized deep learning architectures in order to learn new sets of features based on raw data. Both techniques took advantage of shallow networks in order to optimize for training times and used the weights and biases learned during the fitting phase to extract the latent structure of the data. </p>
<p>Our next chapter will showcase four examples of feature engineering on real data taken from the open internet and how the tools that we have learned in this book will help us create the optimal machine learning pipelines.</p>


            </article>

            
        </section>
    </body></html>