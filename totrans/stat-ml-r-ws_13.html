<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer199">&#13;
			<h1 id="_idParaDest-207" class="chapter-number"><a id="_idTextAnchor213"/>10</h1>&#13;
			<h1 id="_idParaDest-208"><a id="_idTextAnchor214"/>Probability Basics</h1>&#13;
			<p><strong class="bold">Probability distribution</strong> is <a id="_idIndexMarker849"/>an essential concept in statistics and machine learning. It describes the underlying distribution that governs the generation of potential outcomes or events in an experiment or random process. There are different types of probability distributions, depending on the specific domain and characteristics of the data. A proper probability distribution is a useful tool in understanding and modeling the behavior of random processes and events, providing convenient tools for decision-making and predictions when developing data-driven predictive and <span class="No-Break">optimization models.</span></p>&#13;
			<p>By the end of this chapter, you will understand the common probability distributions and their parameters. You will also be able to use these probability distributions to perform usual tasks such as sampling and probability calculations in R, as well as common sampling distribution and <span class="No-Break">order statistics.</span></p>&#13;
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>&#13;
			<ul>&#13;
				<li>Introducing <span class="No-Break">probability distribution</span></li>&#13;
				<li>Exploring common <span class="No-Break">discrete distributions</span></li>&#13;
				<li>Discovering common <span class="No-Break">continuous distributions</span></li>&#13;
				<li>Understanding common <span class="No-Break">sampling distributions</span></li>&#13;
				<li>Understanding <span class="No-Break">order statistics</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-209"><a id="_idTextAnchor215"/>Technical requirements</h1>&#13;
			<p>To run the code in this chapter, you will need to have the latest versions of the <span class="No-Break">following packages:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="source-inline">ggplot2</strong></span><span class="No-Break">, 3.4.0</span></li>&#13;
				<li><span class="No-Break"><strong class="source-inline">dplyr</strong></span><span class="No-Break">, 1.0.10</span></li>&#13;
			</ul>&#13;
			<p>Please note that the versions of the packages mentioned in the preceding list are the latest ones at the time of writing <span class="No-Break">this chapter.</span></p>&#13;
			<p>The code and data for this chapter is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_10/working.R"><span class="No-Break">https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_10/working.R</span></a><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-210"><a id="_idTextAnchor216"/>Introducing probability distribution</h1>&#13;
			<p>Probability distribution <a id="_idIndexMarker850"/>provides a framework for understanding and predicting the behavior of random variables. Once we know the underlying data-generating probability distribution, we can make more informed decisions about how things are likely to appear, either in a predictive or optimization context. In other words, if the selected probability distribution can model the observed data very well, we have a powerful tool to predict potential future values, as well as the uncertainty of <span class="No-Break">such occurrence.</span></p>&#13;
			<p>Here, a random variable<a id="_idIndexMarker851"/> is a variable whose value is not fixed and may assume multiple or infinitely many possible values, representing the outcomes (or realizations) of a random event. Probability distributions allow us to represent and analyze the probability of these outcomes, offering a comprehensive view of the underlying uncertainties in various scenarios. A probability distribution takes the random variable, denoted as <span class="_-----MathTools-_Math_Variable">x</span>, and converts it into a probability, <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span>, a floating number valued between 0 and 1. The <a id="_idIndexMarker852"/>probability distribution can be a <strong class="bold">probability density function</strong> (<strong class="bold">PDF</strong>) or <strong class="bold">probability mass function</strong> (<strong class="bold">PMF</strong>) that specifies the probability of observing an outcome<a id="_idIndexMarker853"/> for a continuous variable (or discrete variable), or a <strong class="bold">cumulative distribution function</strong> (<strong class="bold">CDF</strong>) that<a id="_idIndexMarker854"/> provides the total probability that a random variable is less than or equal to a given <span class="No-Break">fixed quantity.</span></p>&#13;
			<p>In the following example, we use <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> to represent the probability density function of <span class="_-----MathTools-_Math_Variable">x</span>, and <span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> to represent <span class="No-Break">the CDF.</span></p>&#13;
			<p>There are two main categories of probability distributions: discrete probability distribution<a id="_idIndexMarker855"/> and <a id="_idIndexMarker856"/>continuous probability distribution. A <a id="_idIndexMarker857"/>discrete probability <a id="_idIndexMarker858"/>distribution deals with discrete variables, which are random variables that can assume a limited or countable number of possible values. For example, if we have a probability distribution that specifies the probability of experiencing a rainy day in a week, the underlying random variable is the day of the week and can only take an integer value between 1 and 7. Let’s assume there is a total of <span class="_-----MathTools-_Math_Variable">C</span> possible values for the discrete random variable. For a given possible value, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span>, with <span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base">}</span>, the corresponding PMF is <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">]</span>, and all probabilities should sum to 1, giving <span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>. We can think of PMF <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> as a bar chart that specifies the probability output for each <span class="No-Break">discrete input.</span></p>&#13;
			<p>We will cover a few common discrete distributions. For example, the binomial distribution models the number of successes in a fixed number of <strong class="bold">Bernoulli</strong> trials<a id="_idIndexMarker859"/> with the same probability of success, the <strong class="bold">Poisson</strong> distribution<a id="_idIndexMarker860"/> models the number of events within a fixed interval of time or space, and the geometric distribution models the number of trials required for the first success in a sequence of Bernoulli trials. These will be covered later in <span class="No-Break">this chapter.</span></p>&#13;
			<p>Continuous probability <a id="_idIndexMarker861"/>distributions, on <a id="_idIndexMarker862"/>the other hand, involve continuous variables, which can take an infinite number of values within a specified range, denoted as <span class="_-----MathTools-_Math_Symbol_Extended">𝒳</span>. The random variable, <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝒳</span>, is now continuous, and the corresponding PDF <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">]</span> satisfies <span class="_-----MathTools-_Math_Operator">∫</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> for <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">𝒳</span>, where we have switched the summation sign to an integral to account for an infinite amount of possible values of the continuous variable, <span class="_-----MathTools-_Math_Variable">x</span>. We can think of PDF <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span> as a line plot that specifies the probability output for each <span class="No-Break">continuous input.</span></p>&#13;
			<p>We will cover a few widely used continuous distributions, starting with the normal (or Gaussian) distribution, which describes the distribution of many natural quantities. Other examples of continuous distributions include the exponential distribution, which models the time between independent events in a Poisson process, and the uniform distribution, which assigns equal probability to all outcomes within a <span class="No-Break">specified range.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> summarizes these two types of <span class="No-Break">probability distributions:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer181" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_001.jpg" alt="Figure 10.1 – Summarizing the two categories of probability distributions. Both distributions sum to 1" width="1416" height="587"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Summarizing the two categories of probability distributions. Both distributions sum to 1</p>&#13;
			<p>Note that each probability distribution has an associated closed-form expression with a corresponding set of parameters. We will highlight the expression and parameters for each distribution next, starting with the <span class="No-Break">discrete distributions.</span></p>&#13;
			<h1 id="_idParaDest-211"><a id="_idTextAnchor217"/>Exploring common discrete probability distributions</h1>&#13;
			<p>Discrete <a id="_idIndexMarker863"/>probability distributions<a id="_idIndexMarker864"/> are characterized by their corresponding PMFs, which assign a probability to each possible outcome of the input random variable. The sum of the probabilities for all possible outcomes in a discrete distribution equals 1, leading to <span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>. This also means that one of the outcomes <em class="italic">must</em> occur, giving <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>, <span class="_-----MathTools-_Math_Operator_Extended">∀</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">C</span></span><span class="No-Break">.</span></p>&#13;
			<p>Discrete probability distributions are vital in various fields, such as finance. They are commonly used for statistical analyses, including hypothesis testing, parameter estimation, and predictive modeling. We can use discrete probability distributions to quantify uncertainties, make predictions, and gain insights into the underlying data-generating process of the <span class="No-Break">observed outcomes.</span></p>&#13;
			<p>Let’s start with the most fundamental discrete distribution: the <span class="No-Break">Bernoulli distribution.</span></p>&#13;
			<h2 id="_idParaDest-212"><a id="_idTextAnchor218"/>The Bernoulli distribution</h2>&#13;
			<p>The <strong class="bold">Bernoulli distribution</strong> is a <a id="_idIndexMarker865"/>fundamental discrete probability <a id="_idIndexMarker866"/>distribution that specifies the behavior of a binary random variable in a single Bernoulli trial. Here, a <strong class="bold">Bernoulli trial</strong> is <a id="_idIndexMarker867"/>a single experiment with only two possible outcomes, which can also be labeled as “success” and “failure.” It is the simplest discrete probability distribution and serves as the basis for more complex distributions, such as binomial and <span class="No-Break">geometric distributions.</span></p>&#13;
			<p>The Bernoulli distribution is widely used in modeling scenarios with binary outcomes, such as coin tosses, yes/no survey questions, or the presence/absence of a specific feature in a dataset. For example, in statistical hypothesis testing, the Bernoulli distribution is often used in scenarios such as comparing the success rates of two treatments for a medical condition. In finance, the Bernoulli distribution can be used to model binary outcomes, such as a stock’s price going up <span class="No-Break">or down.</span></p>&#13;
			<p>As a convention, the two outcomes in a Bernoulli distribution are often encoded as <strong class="source-inline">1</strong> for success and <strong class="source-inline">0</strong> for failure. The Bernoulli distribution is characterized by a single parameter, <span class="_-----MathTools-_Math_Variable">p</span>, which represents the probability of success. In other words, we have <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span>. Similarly, since total probabilities sum to <strong class="source-inline">1</strong>, the probability of failure is given by <span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span>, giving <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break">.</span></p>&#13;
			<p>We can express the PMF of a Bernoulli distribution <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space">               </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Number"> </span></p>&#13;
			<p>Alternatively, we can express <span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span> in a more compact form, as follows. It is easy to verify that these two representations <span class="No-Break">are equivalent:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">f</span><span class="_-----MathTools-_Math_Variable_v-normal">o</span><span class="_-----MathTools-_Math_Variable_v-normal">r</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0,1</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>&#13;
			<p>Note that <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0,1</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">]</span></span><span class="No-Break">.</span></p>&#13;
			<p>As for the mean, <span class="_-----MathTools-_Math_Variable">μ</span> (first moment), and variance, <span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> (second moment), which characterize the Bernoulli distribution, we have <span class="No-Break">the following:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>&#13;
			<p>We’ll simulate and analyze Bernoulli-distributed random variables using R in the <span class="No-Break">following exercise.</span></p>&#13;
			<h3>Exercise 10.1 – simulating and analyzing Bernoulli-distributed random variables</h3>&#13;
			<p>In this exercise, we will<a id="_idIndexMarker868"/> simulate and analyze <a id="_idIndexMarker869"/>Bernoulli-distributed random variables using the <span class="No-Break"><strong class="source-inline">rbinom()</strong></span><span class="No-Break"> function:</span></p>&#13;
			<ol>&#13;
				<li>Simulate a single Bernoulli trial with a success probability <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.6</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# The probability of success&#13;
p = 0.6&#13;
# Produce a random Bernoulli outcome&#13;
outcome = rbinom(1, size = 1, prob = p)&#13;
&gt;&gt;&gt; print(outcome)&#13;
0</pre><p class="list-inset">Here, the outcome will be displayed as either <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. We can control the random seed to ensure the reproducibility of <span class="No-Break">the results:</span></p><pre class="source-code">set.seed(8)&#13;
&gt;&gt;&gt; rbinom(1, size = 1, prob = p)&#13;
1</pre></li>				<li>Generate five random Bernoulli outcomes with the same probability <span class="No-Break">of success:</span><pre class="source-code">&#13;
# Number of experiments&#13;
n = 5&#13;
# Generate corresponding outcomes&#13;
outcomes = rbinom(n, size = 1, prob = p)&#13;
&gt;&gt;&gt; print(outcomes)&#13;
1 0 0 1 0</pre></li>				<li>Calculate the mean and variance of the <span class="No-Break">Bernoulli distribution:</span><pre class="source-code">&#13;
# Get mean and variance&#13;
mean_bernoulli = p&#13;
var_bernoulli = p * (1 - p)&#13;
&gt;&gt;&gt; cat("Mean:", mean_bernoulli, "\nVariance:", var_bernoulli)&#13;
Mean: 0.6&#13;
Variance: 0.24</pre><p class="list-inset">Here, we use the <strong class="source-inline">cat()</strong> function to concatenate and print out <span class="No-Break">the results.</span></p></li>				<li>Analyze the<a id="_idIndexMarker870"/> results of multiple Bernoulli<a id="_idIndexMarker871"/> trials in terms of the observed/empirical probability <span class="No-Break">of success:</span><pre class="source-code">&#13;
# Number of successes&#13;
num_successes = sum(outcomes)&#13;
# Empirical probability of success&#13;
empirical_p = num_successes / n&#13;
&gt;&gt;&gt; cat("Number of successes:", num_successes, "\nEmpirical probability of success:", empirical_p)&#13;
Number of successes: 2&#13;
Empirical probability of success: 0.4</pre><p class="list-inset">Since we only sampled 5 times, the resulting empirical probability of success (<strong class="source-inline">0.4</strong>) is very far from the true probability (<strong class="source-inline">0.6</strong>). We can enlarge the size of random trials to get a more <span class="No-Break">reliable estimate:</span></p><pre class="source-code">n = 1000&#13;
num_successes = sum(rbinom(n, size = 1, prob = p))&#13;
empirical_p = num_successes / n&#13;
&gt;&gt;&gt; cat("Number of successes:", num_successes, "\nEmpirical probability of success:", empirical_p)&#13;
Number of successes: 600&#13;
Empirical probability of success: 0.6</pre><p class="list-inset">Using a <a id="_idIndexMarker872"/>total of <strong class="source-inline">1000</strong> trials, we can <a id="_idIndexMarker873"/>now reproduce the exact true probability <span class="No-Break">of success.</span></p></li>			</ol>&#13;
			<p>The next section reviews the <span class="No-Break">binomial distribution.</span></p>&#13;
			<h2 id="_idParaDest-213"><a id="_idTextAnchor219"/>The binomial distribution</h2>&#13;
			<p>The <strong class="bold">binomial distribution</strong> extends<a id="_idIndexMarker874"/> the Bernoulli distribution to <a id="_idIndexMarker875"/>multiple repeated trials, where each trial is a single Bernoulli experiment that assumes a value of <strong class="source-inline">1</strong> or <strong class="source-inline">0</strong>. Specifically, it is a discrete probability distribution that specifies the count of successes in a given number of Bernoulli trials. These trials are independent and share the same probability <span class="No-Break">of success.</span></p>&#13;
			<p>There are two parameters for the binomial <span class="No-Break">distribution PMF:</span></p>&#13;
			<ul>&#13;
				<li>The number of <span class="No-Break">trials, </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span></li>&#13;
				<li>The probability of success, <span class="_-----MathTools-_Math_Variable">p</span>, in <span class="No-Break">each trial</span></li>&#13;
			</ul>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">We still assume only two possible outcomes: success (<strong class="source-inline">1</strong>) or failure (<strong class="source-inline">0</strong>). The probability of failure can also be represented as <span class="_-----MathTools-_Math_Variable">q</span> , where <span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break">.</span></p>&#13;
			<p>The PMF of the binomial distribution with a total of <span class="_-----MathTools-_Math_Variable">k</span> successes is given in the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">−</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span> represents the number of combinations of choosing <span class="_-----MathTools-_Math_Variable">k</span> successes from <span class="_-----MathTools-_Math_Variable">n</span> trials, which can be calculated using the binomial <span class="No-Break">coefficient formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">!</span><span class="_-----MathTools-_Math_Operator"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">!</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">!</span><span class="_-----MathTools-_Math_Operator"> </span></p>&#13;
			<p>The first two moments are <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>&#13;
			<p>Using the PMF of the binomial distribution, we can compute the probability of observing a specific number of successes in a specific number <span class="No-Break">of trials.</span></p>&#13;
			<p>The binomial distribution has some important relationships with other probability distributions. For instance, as <span class="_-----MathTools-_Math_Variable">n</span> approaches infinity and <span class="_-----MathTools-_Math_Variable">p</span> remains constant, the binomial distribution converges to the normal distribution, a continuous probability distribution<a id="_idIndexMarker876"/> to<a id="_idIndexMarker877"/> be <span class="No-Break">introduced later.</span></p>&#13;
			<p>Let’s go through an exercise to get familiar with the functions related to <span class="No-Break">binomial distribution.</span></p>&#13;
			<h3>Exercise 10.2 – simulating and analyzing binomial random variables</h3>&#13;
			<p>In this exercise, we will simulate and analyze <a id="_idIndexMarker878"/>binomial-distributed<a id="_idIndexMarker879"/> random variables using the <strong class="source-inline">dbinom()</strong> and <span class="No-Break"><strong class="source-inline">pbinom()</strong></span><span class="No-Break"> functions:</span></p>&#13;
			<ol>&#13;
				<li>Use the <strong class="source-inline">dbinom()</strong> function to calculate the probability of observing 0 to 10 successes based on a binomial distribution with a success probability of <strong class="source-inline">0.5</strong> and a total of <span class="No-Break">10 trials:</span><pre class="source-code">&#13;
n = 10 # Number of trials&#13;
p = 0.5 # Probability of success&#13;
# Get binomial probabilities for different occurrences of successes&#13;
binom_probs = dbinom(0:n, n, p)&#13;
&gt;&gt;&gt; binom_probs&#13;
[1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000&#13;
 [5] 0.2050781250 0.2460937500 0.2050781250 0.1171875000&#13;
 [9] 0.0439453125 0.0097656250 0.0009765625</pre><p class="list-inset">Here, we use <strong class="source-inline">0:n</strong> to create a list of integers from 0 to 10, each of which will then get passed to the <strong class="source-inline">dbinom()</strong> function to evaluate the <span class="No-Break">corresponding probability.</span></p></li>				<li>Create a bar plot of the binomial probabilities using the <span class="No-Break"><strong class="source-inline">barplot()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
&gt;&gt;&gt; barplot(binom_probs, names.arg = 0:n, xlab = "Number of Successes", ylab = "Probability", main = "Binomial Distribution (n = 10, p = 0.5)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em>, which shows that the middle occurrence of 5<a id="_idIndexMarker880"/> has <a id="_idIndexMarker881"/>the <span class="No-Break">highest probability:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer182" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_002.jpg" alt="Figure 10.2 – Visualizing the binomial distribution with n=10 and p=0.5" width="1170" height="783"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Visualizing the binomial distribution with n=10 and p=0.5</p>&#13;
			<ol>&#13;
				<li value="3">Calculate the cumulative binomial probabilities using the <span class="No-Break"><strong class="source-inline">pbinom()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
cum_binom_probs &lt;- pbinom(0:n, n, p)&#13;
&gt;&gt;&gt; cum_binom_probs&#13;
[1] 0.0009765625 0.0107421875 0.0546875000 0.1718750000&#13;
 [5] 0.3769531250 0.6230468750 0.8281250000 0.9453125000&#13;
 [9] 0.9892578125 0.9990234375 1.0000000000</pre><p class="list-inset">The result shows that the cumulative binomial probabilities from the CDF are calculated as the cumulative sum of the previous element-wise probabilities in <span class="No-Break">the PMF.</span></p><p class="list-inset">We can also use the CDF to compute the probability of observing a specific value <span class="No-Break">or higher.</span></p></li>				<li>Calculate the <a id="_idIndexMarker882"/>probability of obtaining at least <a id="_idIndexMarker883"/><span class="No-Break">seven successes:</span><pre class="source-code">&#13;
prob_at_least_7_successes = 1 - pbinom(6, n, p)&#13;
&gt;&gt;&gt; prob_at_least_7_successes&#13;
0.171875</pre><p class="list-inset">Here, the probability of obtaining at least 7 successes is calculated by taking the complement of observing 6 or fewer successes – that is, we have <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≥</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">7</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">6</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break">.</span></p></li>			</ol>&#13;
			<p>Let’s go through another application-related exercise to put these calculations <span class="No-Break">into perspective.</span></p>&#13;
			<h3>Exercise 10.3 – calculating winning probabilities</h3>&#13;
			<p>In this exercise, we will <a id="_idIndexMarker884"/>calculate the winning probabilities of a <span class="No-Break">sports team:</span></p>&#13;
			<ol>&#13;
				<li>Suppose the sports team has a probability of 80% of winning a match. If there are a total of five matches, what is the probability of winning at least <span class="No-Break">four matches?</span><pre class="source-code">&#13;
n = 5&#13;
p = 0.8&#13;
prob_at_least_4_wins = 1 - pbinom(3, n, p)&#13;
&gt;&gt;&gt; prob_at_least_4_wins&#13;
0.73728</pre></li>				<li>Calculate the probability of winning, at most, <span class="No-Break">three matches:</span><pre class="source-code">&#13;
prob_at_most_3_wins = pbinom(3, n, p)&#13;
&gt;&gt;&gt; prob_at_most_3_wins&#13;
0.26272</pre><p class="list-inset">Note <a id="_idIndexMarker885"/>that this probability is a complement to the previous probability of winning at least four matches. We can verify this relationship <span class="No-Break">as follows:</span></p><pre class="source-code">&gt;&gt;&gt; prob_at_most_3_wins == 1 – prob_at_least_4_wins&#13;
TRUE</pre></li>			</ol>&#13;
			<p>In the following section, we’ll pause to discuss the normal approximation to the binomial distribution. This relationship is widely used in statistical analysis and has its roots in the central <span class="No-Break">limit theorem.</span></p>&#13;
			<h3>The normal approximation to the binomial distribution</h3>&#13;
			<p>The normal (or <strong class="bold">Gaussian</strong>) approximation<a id="_idIndexMarker886"/> to the binomial <a id="_idIndexMarker887"/>distribution says that the binomial distribution can be approximated by a normal distribution that shares the same mean value (<span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span>) and variance value (<span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span>). Such normal approximation to the binomial distribution becomes more accurate as the number of experiments (<span class="_-----MathTools-_Math_Variable">n</span>) increases, and the success probability (<span class="_-----MathTools-_Math_Variable">p</span>) does not approach 0 or 1. As a rule of thumb, we often use the normal approximation when both <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span> ≥ 10 and <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">q</span> ≥ <span class="No-Break">10.</span></p>&#13;
			<p>To use the normal approximation, we need to standardize the binomial random variable, <span class="_-----MathTools-_Math_Variable">x</span>, by converting it into the form of a standard normal variable, <span class="_-----MathTools-_Math_Variable">z</span>, based on the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable"> </span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span> and <span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span>. This is also called<a id="_idIndexMarker888"/> the <strong class="bold">z-score</strong>. Going through such standardization is a <a id="_idIndexMarker889"/>common practice when trying to compare different quantities on the same scale. We can make use of the standard normal distribution (to be introduced later) to work on the corresponding PDF or CDF, depending on the specific task. In this case, we can use the standard normal distribution (that is, <span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Number">)</span>) to approximate the probabilities associated with the <span class="No-Break">binomial distribution.</span></p>&#13;
			<p>Let’s look at a concrete example. Suppose we toss a coin 100 times (assuming a fair coin with an equal <a id="_idIndexMarker890"/>probability of landing with a head or a tail) and would like to compute the probability of obtaining between 40 and 60 heads (both inclusive). Let <span class="_-----MathTools-_Math_Variable">x</span> be the random variable that denotes the number of heads. We know that <span class="_-----MathTools-_Math_Variable">x</span> assumes a binomial distribution with <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">100</span> and <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">0.5</span></span><span class="No-Break">.</span></p>&#13;
			<p>To check whether the normal approximation is appropriate, we calculate <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">100</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">50</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10</span> and <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">100</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10</span>. We also verify the same using R, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
n = 100&#13;
p = 0.5&#13;
# check conditions for normal approximation&#13;
&gt;&gt;&gt; n*p &gt; 10&#13;
TRUE&#13;
&gt;&gt;&gt; n*p*(1-p) &gt; 10&#13;
TRUE</pre>			<p>Both conditions evaluate <strong class="source-inline">TRUE</strong>. Now, we can standardize the upper and lower limits (<strong class="source-inline">60</strong> and <strong class="source-inline">40</strong>, respectively) to convert them into a standardized score. To do this, we need to obtain the parameters of the binomial distribution, followed by applying the standardization formula, <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable"> </span>, to get the z-score. The following code snippet completes <span class="No-Break">the standardization:</span></p>&#13;
			<pre class="source-code">&#13;
# compute mean and std&#13;
mu = n*p&#13;
&gt;&gt;&gt; mu&#13;
50&#13;
std = sqrt(n*p*(1-p))&#13;
&gt;&gt;&gt; std&#13;
5&#13;
# compute P(lower_limit &lt;= X &lt;= upper_limit)&#13;
lower_limit = 40&#13;
upper_limit = 60&#13;
# Using z score&#13;
standard_lower_limit = (lower_limit – mu) / std&#13;
standard_upper_limit = (upper_limit – mu) / std&#13;
&gt;&gt;&gt; standard_lower_limit&#13;
-2&#13;
&gt;&gt;&gt; standard_upper_limit&#13;
2</pre>			<p>With the<a id="_idIndexMarker891"/> standardized z-score, we can now calculate the original probability based on the standard normal distribution alone. In other <a id="_idIndexMarker892"/>words, we have <span class="No-Break">the following:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">40</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">60</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">40</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">50</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">50</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">60</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">50</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>&#13;
			<p>As shown in the following code snippet, we can now call the <strong class="source-inline">pnorm()</strong> function to calculate the CDF at points <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> and <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, whose difference gives the <span class="No-Break">final probability:</span></p>&#13;
			<pre class="source-code">&#13;
# approximate using standard normal cdf&#13;
&gt;&gt;&gt; pnorm(standard_upper_limit) - pnorm(standard_lower_limit)&#13;
0.9544997</pre>			<p>Let’s also <a id="_idIndexMarker893"/>calculate the corresponding probability using the binomial distribution to see how close the normal approximation is. The following code snippet uses the <strong class="source-inline">pbino</strong><strong class="source-inline">m()</strong> function to obtain the CDF of the binomial at both limits and then takes the difference to give the total probability of observing an outcome in <span class="No-Break">this range:</span></p>&#13;
			<pre class="source-code">&#13;
# use binomial distribution&#13;
&gt;&gt;&gt; pbinom(upper_limit, n, p) - pbinom(lower_limit, n, p)&#13;
0.9539559</pre>			<p>The result<a id="_idIndexMarker894"/> shows that the approximation is accurate up to the second decimal place. Therefore, the normalization approximation provides an alternative approach to calculating the probabilities if directly using the binomial distribution is inconvenient. However, although the normal approximation is a powerful tool, we still need to check the required conditions to ensure a <span class="No-Break">good approximation.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.3</em> summarizes the two approaches to calculating the total probability of observing a specific range of values. We can calculate it by taking the difference in the CDF between the two boundaries of the range. Alternatively, we can rely on the normal distribution to approximate the binomial distribution and calculate the total probability after obtaining the standardized z-score based on these parameters, assuming the conditions <span class="No-Break">are satisfied:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer183" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_003.jpg" alt="Figure 10.3 – Summarizing the normal approximation to the binomial distribution when calculating the total probability of observing a specific range of values" width="1014" height="389"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Summarizing the normal approximation to the binomial distribution when calculating the total probability of observing a specific range of values</p>&#13;
			<p>We will review the Poisson distribution in the <span class="No-Break">next section.</span></p>&#13;
			<h2 id="_idParaDest-214"><a id="_idTextAnchor220"/>The Poisson distribution</h2>&#13;
			<p>Another popular discrete probability<a id="_idIndexMarker895"/> distribution is the Poisson distribution, which<a id="_idIndexMarker896"/> describes the number of events within a fixed interval of time or space. It has a single constant parameter that specifies the average rate of occurrence. Specifically, we must denote <span class="_-----MathTools-_Math_Variable">λ</span> as the average rate of occurrence of events in the given interval. We can express the PMF of the Poisson distribution <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">!</span><span class="_-----MathTools-_Math_Operator"> </span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span> denotes the probability of experiencing a total of <span class="_-----MathTools-_Math_Variable">k</span> occurrences in the fixed interval, <span class="_-----MathTools-_Math_Variable">e</span> is Euler’s number (approximately 2.71), and <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">!</span> is the factorial of <span class="_-----MathTools-_Math_Variable">k</span> (the product of all positive integers up <span class="No-Break">to </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">k</span></span><span class="No-Break">).</span></p>&#13;
			<p>With this equation, we can calculate the probability of observing any (integer) number of events within the given fixed interval. We can further calculate the mean and variance of the Poisson distribution <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span></p>&#13;
			<p>The Poisson distribution is often used to model rare events that occur independently and at a constant average rate. Some real-world applications of Poisson processes include the number of hotel bookings received at the front desk per hour and the number of emails arriving in an inbox within <span class="No-Break">a day.</span></p>&#13;
			<p>Let’s go through an exercise to get familiar with common probability calculations related to the <span class="No-Break">Poisson distribution.</span></p>&#13;
			<h3>Exercise 10.4 – simulating and analyzing Poisson-distributed random variables</h3>&#13;
			<p>In this exercise, we will use <a id="_idIndexMarker897"/>R to work with the Poisson <a id="_idIndexMarker898"/>distribution, including calculating the probabilities (using the PMF), plotting the distribution, and generating random samples. Specifically, we will calculate the Poisson probabilities for an average rate of occurrence of 5 events per interval (<span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span>), plot the PMF, and calculate the cumulative probabilities. We will also generate a random sample of 100 observations from this <span class="No-Break">Poisson distribution:</span></p>&#13;
			<ol>&#13;
				<li>Calculate the probabilities of observing 0 to 15 occurrences/events per interval based on a<a id="_idIndexMarker899"/> Poisson distribution<a id="_idIndexMarker900"/> parameterized by <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">5</span> using the <span class="No-Break"><strong class="source-inline">dpois()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
lambda = 5 # distribution parameter&#13;
# Calculate probabilities for each scenario&#13;
pois_probs = dpois(0:15, lambda)&#13;
&gt;&gt;&gt; pois_probs&#13;
[1] 0.0067379470 0.0336897350 0.0842243375 0.1403738958&#13;
 [5] 0.1754673698 0.1754673698 0.1462228081 0.1044448630&#13;
 [9] 0.0652780393 0.0362655774 0.0181327887 0.0082421767&#13;
[13] 0.0034342403 0.0013208616 0.0004717363 0.0001572454</pre></li>				<li>Create a bar plot of the <span class="No-Break">Poisson probabilities:</span><pre class="source-code">&#13;
&gt;&gt;&gt; barplot(pois_probs, names.arg = 0:15, xlab = "Number of Events", ylab = "Probability", main = "Poisson Distribution (lambda = 5)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.4</em>. As expected, the peak probabilities appear around <span class="No-Break">five times:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer184" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_004.jpg" alt="Figure 10.4 – Visualizing the PMF of the Poisson distribution as a bar plot" width="911" height="820"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Visualizing the PMF of the Poisson distribution as a bar plot</p>&#13;
			<ol>&#13;
				<li value="3">Calculate <a id="_idIndexMarker901"/>cumulative Poisson <a id="_idIndexMarker902"/>probabilities for each number of events (0 to 15) using the <span class="No-Break"><strong class="source-inline">ppois()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
cum_pois_probs = ppois(0:15, lambda)&#13;
&gt;&gt;&gt; cum_pois_probs&#13;
[1] 0.006737947 0.040427682 0.124652019 0.265025915&#13;
 [5] 0.440493285 0.615960655 0.762183463 0.866628326&#13;
 [9] 0.931906365 0.968171943 0.986304731 0.994546908&#13;
[13] 0.997981148 0.999302010 0.999773746 0.999930992</pre><p class="list-inset">Let’s also plot the CDF in a <span class="No-Break">bar chart:</span></p><pre class="source-code">&gt;&gt;&gt; barplot(cum_pois_probs, names.arg = 0:15, xlab = "Number of Events", ylab = "Cumulative Probability", main = "CDF of Poisson Distribution (lambda = 5)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.5</em>. Note that the CDF curve increases sharply around the mean occurrence of five times and gradually saturates as we move toward the right end of <span class="No-Break">the graph:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer185" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_005.jpg" alt="Figure 10.5 – Visualizing the CDF of the Poisson distribution as a bar plot" width="918" height="824"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Visualizing the CDF of the Poisson distribution as a bar plot</p>&#13;
			<ol>&#13;
				<li value="4">Generate <strong class="source-inline">100</strong> random <a id="_idIndexMarker903"/>samples from this <a id="_idIndexMarker904"/>Poisson distribution using the <span class="No-Break"><strong class="source-inline">rpois()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
pois_samples = rpois(100, lambda)&#13;
&gt;&gt;&gt; pois_samples&#13;
  [1]  8  5  8  4  3  4  6  2  5  6  3  3  7  8  8  7&#13;
 [17]  5  9  6  1  4  2  7  7  5  5  5  2  7  4  6  5&#13;
 [33]  4  4  3  0  8  5  4  4  7  5 11  6  5  4  8  8&#13;
 [49]  2  5  6  2  3  4  6  4  6  2  5  3  6  0  5  8&#13;
 [65]  7  1  8  4  4  4  4  5  4  4  4  5  5  6  3  4&#13;
 [81]  3  0  8  9  2  3  4 13  2  6  8  9  6  4  7  7&#13;
 [97]  8  6  3  5</pre><p class="list-inset">As expected, the majority of the occurrences are around <span class="No-Break">five times.</span></p></li>			</ol>&#13;
			<h2 id="_idParaDest-215"><a id="_idTextAnchor221"/>Poisson approximation to binomial distribution</h2>&#13;
			<p>As it turns out, we can <a id="_idIndexMarker905"/>also use the Poisson distribution to approximate the binomial distribution under specific conditions. For instance, when the number of trials (<span class="_-----MathTools-_Math_Variable">n</span>) in a binomial distribution is large and the success probability (<span class="_-----MathTools-_Math_Variable">p</span>) is small, the binomial distribution can be approximated by a Poisson distribution with <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break">.</span></p>&#13;
			<p>Let’s use an example to demonstrate how to apply the Poisson approximation to the binomial distribution. Suppose we have a binomial distribution with <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1000</span> and <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.01</span>. We want to find the probability of observing exactly <span class="No-Break">15 successes:</span></p>&#13;
			<ol>&#13;
				<li>We can start with the binomial probability and calculate the corresponding probability using the <strong class="source-inline">dbinom()</strong> function after specifying <span class="No-Break">the parameters:</span></li>&#13;
			</ol>&#13;
			<pre class="source-code">&#13;
# Binomial parameters&#13;
n = 1000&#13;
p = 0.01&#13;
# Probability of observing 15 successes&#13;
binom_prob = dbinom(15, n, p)&#13;
&gt;&gt;&gt; binom_prob&#13;
0.03454173</pre>			<p>Next, we must calculate the approximate Poisson parameter, <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
lambda_approx = n * p&#13;
&gt;&gt;&gt; lambda_approx&#13;
10</pre>			<p>Now, we can calculate the Poisson probability of observing <span class="No-Break">15 successes:</span></p>&#13;
			<pre class="source-code">&#13;
pois_approx_prob &lt;- dpois(15, lambda_approx)&#13;
&gt;&gt;&gt; pois_approx_prob&#13;
0.03471807</pre>			<p>The result suggests that the approximation is quite accurate to the third <span class="No-Break">decimal point.</span></p>&#13;
			<p>One more<a id="_idIndexMarker906"/> interesting property is that adding up several independent Poisson-distributed random variables also produces a Poisson distribution, which is parameterized by the sum of the corresponding individual <span class="_-----MathTools-_Math_Variable">λ</span> values. For example, if <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> and <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> are independent Poisson random variables with <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> and <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>, respectively, their sum (<span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>) also follows a Poisson distribution with <span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span>. This gives a convenient property when working with the sum of multiple Poisson-distributed <span class="No-Break">random variables.</span></p>&#13;
			<p>In the next section, we’ll cover another widely used discrete distribution: the <span class="No-Break">geometric distribution.</span></p>&#13;
			<h2 id="_idParaDest-216"><a id="_idTextAnchor222"/>The geometric distribution</h2>&#13;
			<p>The geometric distribution is a <a id="_idIndexMarker907"/>discrete probability distribution that describes the<a id="_idIndexMarker908"/> number of trials required for the first success in a sequence of independent Bernoulli trials, each with the same probability of success. Similar to the binomial distribution, the geometric distribution is a collection of multiple independent Bernoulli trials, although the subject of interest is the first occurrence of success in the sequence of trials. Here, the first occurrence of success means that all previous trials need to be non-success, and the current trial is the first success among multiple trials performed <span class="No-Break">so far.</span></p>&#13;
			<p>It is commonly used to model the waiting time until an event occurs or the number of attempts needed to achieve a desired outcome. Examples include the number of attempts to pass a driving test until success, the number of times we observe continuous sunny days, and the number of coin flips needed to obtain the first head. The geometric distribution is quite useful when modeling the waiting time or the number of attempts needed to achieve the first success in a sequence of independent <span class="No-Break">Bernoulli trials.</span></p>&#13;
			<p>The geometric distribution is defined by a single parameter, <span class="_-----MathTools-_Math_Variable">p</span>, which represents the probability of success on each Bernoulli trial. The PMF of the geometric distribution is given by the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">k</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">−</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">1</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span></p>&#13;
			<p>This formula specifies the probability of observing the first success on the <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">h</span> trial. Therefore, the probability is calculated as a joint probability of observing <span class="_-----MathTools-_Math_Variable">k</span> individual events, where the first <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> events are non-success with a joint probability of <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span>, and the last event is a success with a probability <span class="No-Break">of </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">p</span></span><span class="No-Break">.</span></p>&#13;
			<p>The mean and variance parameters of a geometric distribution can be expressed <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p>Note that the geometric distribution is <strong class="bold">memoryless</strong>, meaning that the probability of success in the<a id="_idIndexMarker909"/> next trial does not depend<a id="_idIndexMarker910"/> on the past trials. In other words, the waiting time until the first success does not change, regardless of the number of past trials that have already <span class="No-Break">been conducted.</span></p>&#13;
			<p>Let’s go through an exercise to simulate and analyze a random variable while following the <span class="No-Break">geometric distribution.</span></p>&#13;
			<h3>Exercise 10.5 – simulating and analyzing geometrically-distributed random variables</h3>&#13;
			<p>In this exercise, we will<a id="_idIndexMarker911"/> use R to work with the geometric <a id="_idIndexMarker912"/>distribution, including calculating the PMF and CDF probabilities, plotting the distribution, and generating <span class="No-Break">random samples:</span></p>&#13;
			<ol>&#13;
				<li>For a geometric distribution with a success probability of <span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.25</span>, calculate geometric probabilities for each number of trials (from 1 to 10) using the <span class="No-Break"><strong class="source-inline">dgeom()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Parameters&#13;
p = 0.25 # Probability of success&#13;
# Get geometric probabilities&#13;
geom_probs = dgeom(0:9, p)&#13;
&gt;&gt;&gt; geom_probs&#13;
[1] 0.25000000 0.18750000 0.14062500 0.10546875 0.07910156 0.05932617&#13;
 [7] 0.04449463 0.03337097 0.02502823 0.01877117</pre><p class="list-inset">Note that the <strong class="source-inline">0:9</strong> argument represents the number of failures in the <span class="No-Break"><strong class="source-inline">dgeom()</strong></span><span class="No-Break"> function.</span></p></li>				<li>Create a bar plot for these <span class="No-Break">geometric probabilities:</span><pre class="source-code">&#13;
&gt;&gt;&gt; barplot(geom_probs, names.arg = 1:10, xlab = "Number of Trials", ylab = "Probability", main = "Geometric Distribution (p = 0.25)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.6</em>. As expected, the probability of obtaining a longer sequence of continuous failures decreases as the number of <span class="No-Break">trials increases:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer186" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_006.jpg" alt="Figure 10.6 – Visualizing the PMF of the geometric distribution as a bar plot" width="997" height="827"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Visualizing the PMF of the geometric distribution as a bar plot</p>&#13;
			<ol>&#13;
				<li value="3">Calculate <a id="_idIndexMarker913"/>cumulative <a id="_idIndexMarker914"/>geometric probabilities for the previous trials using the <span class="No-Break"><strong class="source-inline">pgeom()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
cum_geom_probs = pgeom(0:9, p)&#13;
&gt;&gt;&gt; cum_geom_probs&#13;
[1] 0.2500000 0.4375000 0.5781250 0.6835938 0.7626953&#13;
 [6] 0.8220215 0.8665161 0.8998871 0.9249153 0.9436865</pre></li>				<li>Generate 100 random samples from this geometric distribution using the <span class="No-Break"><strong class="source-inline">rgeom()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
geom_samples = rgeom(100, p)&#13;
&gt;&gt;&gt; geom_samples&#13;
  [1]  0  0  0  2 10  1  1 10  0  0  1  1  5  3  1  0  2  0  0&#13;
 [20]  4  0  1  4  2  3  2  2  2  4  1  6 12  4  1  7  3  1  1&#13;
 [39]  0  2  1  2  3  0  8  0  0  2 10  3  2  8  0  3  1  2  3&#13;
 [58]  0  0  1  7  0  0  3  4 11  8  8  2  0  5  1  1  1  3  1&#13;
 [77]  3  1  3  3  6  0  0  7  1  0  0  1  0  1  0  0  0  3  0&#13;
 [96]  0  4 25  0  3</pre><p class="list-inset">The result<a id="_idIndexMarker915"/> shows a decreasing<a id="_idIndexMarker916"/> frequency as the numbers grow large, which matches the PMF of the <span class="No-Break">geometric distribution.</span></p></li>			</ol>&#13;
			<p>Let’s go through another application-related exercise on the probability of finding bugs in a <span class="No-Break">computer program.</span></p>&#13;
			<h3>Exercise 10.6 – simulating and analyzing geometrically-distributed random variables</h3>&#13;
			<p>In this exercise, we will visit a real-world example that involves a software tester trying to find bugs in a computer program. In this example, the software tester finds bugs in a program with a probability of <strong class="source-inline">0.1</strong> on each attempt, and the attempts are independent. The company wants to know the probability of finding the first bug within the first five attempts, as well as the expected number of attempts needed to find the <span class="No-Break">first bug:</span></p>&#13;
			<ol>&#13;
				<li>Calculate the probability of finding the first bug within the first five attempts using the <span class="No-Break"><strong class="source-inline">pgeom()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
p = 0.1 # Probability of finding a bug on each attempt&#13;
# Calculate the CDF for up to 5 attempts&#13;
prob_within_5_attempts = pgeom(4, p)&#13;
&gt;&gt;&gt; prob_within_5_attempts&#13;
0.40951</pre><p class="list-inset">Here, note that we use <strong class="source-inline">4</strong> since the <strong class="source-inline">dgeom()</strong> function uses <span class="No-Break">zero-based indexing.</span></p><p class="list-inset">As the <strong class="source-inline">pgeom()</strong> function returns the CDF of a specific input, we can equivalently calculate this probability by summing up all previous probabilities up to the current input, as <span class="No-Break">shown here:</span></p><pre class="source-code">&gt;&gt;&gt; sum(dgeom(0:4, p))&#13;
0.40951</pre></li>				<li>Calculate the <a id="_idIndexMarker917"/>expected number of<a id="_idIndexMarker918"/> attempts needed to find the first bug using the mean (expected value) of the <span class="No-Break">geometric distribution:</span><pre class="source-code">&#13;
mean_attempts &lt;- 1 / p&#13;
&gt;&gt;&gt; mean_attempts&#13;
10</pre></li>				<li>Visualize the probabilities of finding the first bug within different numbers of attempts (from <strong class="source-inline">1</strong> to <strong class="source-inline">20</strong>) in a <span class="No-Break">bar chart:</span><pre class="source-code">&#13;
geom_probs &lt;- dgeom(0:19, p)&#13;
# Create a bar plot of probabilities&#13;
barplot(geom_probs, names.arg = 1:20, xlab = "Number of Attempts", ylab = "Probability", main = "Geometric Distribution (p = 0.1)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.7</em>. This figure suggests that rare events (those requiring a continuous stream of failures) assume lower probabilities as we move toward <span class="No-Break">the right:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer187" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_007.jpg" alt="Figure 10.7 – Visualizing the probabilities of finding the first bug within different numbers of attempts" width="902" height="823"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Visualizing the probabilities of finding the first bug within different numbers of attempts</p>&#13;
			<p>As this <a id="_idIndexMarker919"/>figure suggests, the probability of<a id="_idIndexMarker920"/> observing the first bug decreases as the number of <span class="No-Break">attempts increases.</span></p>&#13;
			<h2 id="_idParaDest-217"><a id="_idTextAnchor223"/>Comparing different discrete probability distributions</h2>&#13;
			<p>The discrete probability <a id="_idIndexMarker921"/>distributions introduced so far in this chapter are essential tools to model scenarios where the outcome variable takes on discrete values. Each discrete distribution has specific assumptions, properties, and applications. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.8</em> provides a summary takeaway and analysis by comparing the main characteristics of the binomial, Poisson, and <span class="No-Break">geometric distributions:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer188" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_008.jpg" alt="Figure 10.8 – Summarizing and comparing different discrete distributions" width="1644" height="980"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Summarizing and comparing different discrete distributions</p>&#13;
			<p>With different discrete distributions at hand, it is important to understand the specific assumptions and requirements of each distribution to select the appropriate one for a given problem. For example, the binomial distribution is suitable for modeling the number of successes in a given number of experiments, the Poisson distribution is suitable for modeling the number of events occurring in a fixed period, and the geometric distribution is often used to model the number of trials required to achieve the <span class="No-Break">first success.</span></p>&#13;
			<p>In practice, these <a id="_idIndexMarker922"/>discrete probability distributions can be used to analyze various real-world scenarios, make predictions, and optimize processes. Getting a good understanding of the characteristics and applications of each distribution allows you to choose the right distribution for the specific problem and perform relevant analyses <span class="No-Break">using R.</span></p>&#13;
			<p>The next section introduces continuous distributions, including normal distribution, exponential distribution, and <span class="No-Break">uniform distribution.</span></p>&#13;
			<h1 id="_idParaDest-218"><a id="_idTextAnchor224"/>Discovering common continuous probability distributions</h1>&#13;
			<p>Continuous probability distributions <a id="_idIndexMarker923"/>model the probability of random variables that assume any value within a specific continuous range. In other words, the underlying random variable is continuous instead of discrete. These distributions describe the probabilities of observing values that fall within a continuous interval, rather than equal to individual discrete outcomes in a discrete probability distribution. Specifically, in a continuous probability distribution, the probability of the random variable equal to any specific value is typically zero, since the possible outcomes are uncountable. Instead, probabilities for continuous distributions are calculated for intervals or ranges <span class="No-Break">of values.</span></p>&#13;
			<p>We can use a PDF to describe a continuous distribution. This corresponds to the PMF of a discrete probability distribution. The PDF defines the probability of observing a value within an infinitesimally small interval around a given point. The area under the PDF curve over a specific range represents the probability of the random variable falling within that range – that is, the probabilities are calculated for intervals or ranges of values by integrating the PDF over the desired range. In contrast, probabilities are assigned to individual points in a PMF, and the probabilities are calculated for a set of discrete values by summing up their <span class="No-Break">individual probabilities.</span></p>&#13;
			<p>In addition, the visualization for continuous probability distributions is also different. Compared to the bar chart used for the PMF of discrete probabilities distributions, the PDF of continuous distributions is plotted as smooth curves called density plots. The area under the curve over a specific range represents the probability of the random variable falling within <span class="No-Break">that range.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.9</em> summarizes<a id="_idIndexMarker924"/> the main <a id="_idIndexMarker925"/>differences between discrete and continuous <span class="No-Break">probability distributions:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer189" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_009.jpg" alt="Figure 10.9 – Summarizing the differences between discrete and continuous probability distributions" width="1251" height="663"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Summarizing the differences between discrete and continuous probability distributions</p>&#13;
			<p>In summary, discrete <a id="_idIndexMarker926"/>and continuous probability distributions model different types of random <a id="_idIndexMarker927"/>variables. Discrete probability distributions represent countable outcomes, while continuous distributions represent uncountable possibilities within a continuous range. These differences between these two types of distributions determine how we select the appropriate distribution for a given problem and perform <span class="No-Break">relevant analyses.</span></p>&#13;
			<p>In the next section, we’ll introduce the most widely used continuous probability distribution: the normal <span class="No-Break">probability distribution.</span></p>&#13;
			<h2 id="_idParaDest-219"><a id="_idTextAnchor225"/>The normal distribution</h2>&#13;
			<p>The <strong class="bold">normal probability distribution</strong>, also called the <strong class="bold">Gaussian distribution</strong>, is a continuous probability<a id="_idIndexMarker928"/> distribution that models scenarios where the <a id="_idIndexMarker929"/>continuous outcomes are <a id="_idIndexMarker930"/>symmetrically distributed around the mean. It is the most widely used probability distribution in practice as many natural and social phenomena tend to follow a normal distribution as a result of the central <span class="No-Break">limit theorem.</span></p>&#13;
			<p>Two parameters are used to characterize the normal distribution: the mean (<span class="_-----MathTools-_Math_Variable">μ</span>) and the standard deviation (<span class="_-----MathTools-_Math_Variable">σ</span>). The mean represents the central tendency or the average value of the distribution. The outcomes around the center of the distribution get the highest probability. The standard deviation describes the dispersion or spread of the data from the mean, serving as a measure of variability in <span class="No-Break">the distribution.</span></p>&#13;
			<p>The PDF of the normal distribution is specified <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">π</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p>We can also write <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span>, which reads as the random variable, <span class="_-----MathTools-_Math_Variable">x</span>, follows a normal distribution parameterized by <span class="_-----MathTools-_Math_Variable">μ</span> and <span class="No-Break"><span class="_-----MathTools-_Math_Variable">σ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break">.</span></p>&#13;
			<p>Graphically, the normal distribution looks like a bell-shaped curve, with most values concentrated near the mean and fewer values toward the extremes at both ends. An empirical rule, called the 68-95-99.7 rule, says that approximately 68% of the values lie within 1 standard deviation of the mean, 95% within 2 standard deviations, and 99.7% within 3 <span class="No-Break">standard deviations.</span></p>&#13;
			<p>A particular normal distribution that is commonly used is the standard normal distribution, which is written as <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span> – that is, a standard normal distribution has <span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span> and <span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span>. A special property is that we can transform any normally distributed random variable into a standard normal variable using the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable"> </span></p>&#13;
			<p>We can then<a id="_idIndexMarker931"/> use the standard normal <a id="_idIndexMarker932"/>table (called a Z-table) to find probabilities and percentiles <span class="No-Break">of interest.</span></p>&#13;
			<p>Let’s go through an exercise to practice the calculations related to the <span class="No-Break">normal distribution.</span></p>&#13;
			<h3>Exercise 10.7 – simulating and analyzing normal random variables</h3>&#13;
			<p>In this<a id="_idIndexMarker933"/> exercise, we will simulate and analyze normal-distributed <a id="_idIndexMarker934"/><span class="No-Break">random variables:</span></p>&#13;
			<ol>&#13;
				<li>Calculate the<a id="_idIndexMarker935"/> probability density of the standard <a id="_idIndexMarker936"/>normal distribution using the <strong class="source-inline">dnorm()</strong> function from <strong class="source-inline">-4</strong> to <strong class="source-inline">4</strong> with a step size <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.1</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# Parameters&#13;
mu = 0      # Mean&#13;
sigma = 1   # Standard deviation&#13;
# Get the probability density for different x&#13;
x = seq(-4, 4, by = 0.1)&#13;
normal_density = dnorm(x, mu, sigma)</pre><p class="list-inset">Here, we use the <strong class="source-inline">seq()</strong> function to create a vector of equally spaced values and extract the corresponding probability for each of the input values using the <span class="No-Break"><strong class="source-inline">dnorm()</strong></span><span class="No-Break"> function.</span></p></li>				<li>Plot the normal distribution as a continuous curve using the <span class="No-Break"><strong class="source-inline">plot()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Plot the normal distribution&#13;
&gt;&gt;&gt; plot(x, normal_density, type = "l", xlab = "x", ylab = "Probability Density", main = "Normal Distribution (μ = 0, σ = 1)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.10</em>, which shows that the PDF is centered around the mean, <strong class="source-inline">0</strong>, and has a standard deviation of <strong class="source-inline">1</strong> as <span class="No-Break">the spread:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer190" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_010.jpg" alt="Figure 10.10 – Visualizing the density plot of the standard normal distribution" width="954" height="818"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Visualizing the density plot of the standard normal distribution</p>&#13;
			<ol>&#13;
				<li value="3">Calculate the cumulative probabilities of the normal distribution using the <span class="No-Break"><strong class="source-inline">pnorm()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Get cumulative probabilities for different x&#13;
normal_cum_prob &lt;- pnorm(x, mu, sigma)</pre><p class="list-inset">Similarly, we<a id="_idIndexMarker937"/> can<a id="_idIndexMarker938"/> plot the CDF <span class="No-Break">as follows:</span></p><pre class="source-code">&gt;&gt;&gt; plot(x, normal_cum_prob, type = "l", xlab = "x", ylab = "Cumulative Probability Density", main = "Cumulative Normal Distribution (μ = 0, σ = 1)")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer191" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_011.jpg" alt="Figure 10.11 – Visualizing the cumulative density function of the standard normal distribution" width="961" height="819"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Visualizing the cumulative density function of the standard normal distribution</p>&#13;
			<ol>&#13;
				<li value="4">Generate <a id="_idIndexMarker939"/>random samples from a normal<a id="_idIndexMarker940"/> distribution using the <span class="No-Break"><strong class="source-inline">rnorm()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Generate 100 random samples from a normal distribution with μ = 0 and σ = 1&#13;
normal_samples &lt;- rnorm(100, mu, sigma)</pre></li>				<li>Find the 90<span class="superscript">th</span> quantile (inverse cumulative probability) for a given probability using the <span class="No-Break"><strong class="source-inline">qnorm()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
# Find the quantile corresponding to the 90th percentile&#13;
quantile_90 &lt;- qnorm(0.9, mu, sigma)&#13;
&gt;&gt;&gt; quantile_90&#13;
1.281552</pre></li>			</ol>&#13;
			<p>Let’s look at another exercise for solving practical problems using the <span class="No-Break">normal distribution.</span></p>&#13;
			<h3>Exercise 10.8 – calculating probabilities with the normal distribution</h3>&#13;
			<p>Let’s assume a <a id="_idIndexMarker941"/>company manufactures batteries<a id="_idIndexMarker942"/> with an average life span of 100 hours and a standard deviation of 10 hours. Let’s also assume that the lifespan of the batteries follows a <span class="No-Break">normal distribution:</span></p>&#13;
			<ol>&#13;
				<li>Simulate a dataset of <span class="No-Break">1,000 batteries:</span><pre class="source-code">&#13;
set.seed(8)&#13;
mean_lifespan = 100&#13;
sd_lifespan = 10&#13;
n = 1000&#13;
lifespans = rnorm(n, mean_lifespan, sd_lifespan)</pre><p class="list-inset">Here, we use the <strong class="source-inline">rnomr()</strong> function to sample from the given normal distribution randomly. We also specify the random seed <span class="No-Break">for reproducibility.</span></p></li>				<li>Calculate the probability that a randomly chosen battery will last more than <span class="No-Break">120 hours:</span><pre class="source-code">&#13;
threshold = 120&#13;
probability = 1 - pnorm(threshold, mean_lifespan, sd_lifespan)&#13;
&gt;&gt;&gt; probability&#13;
0.02275013</pre><p class="list-inset">Here, we use the <strong class="source-inline">pnorm()</strong> function to calculate the total probability of being smaller than <strong class="source-inline">120</strong>, then take the complement to get the probability of being larger <span class="No-Break">than </span><span class="No-Break"><strong class="source-inline">120</strong></span><span class="No-Break">.</span></p><p class="list-inset">As expected, the probability of deviating from the mean by two standard deviations is <span class="No-Break">quite small.</span></p></li>				<li>Plot the <a id="_idIndexMarker943"/>PDF of the lifespans with <a id="_idIndexMarker944"/>the area under the curve above <strong class="source-inline">120</strong> <span class="No-Break">hours shaded:</span><pre class="source-code">&#13;
df &lt;- data.frame(lifespan = lifespans)&#13;
df_density &lt;- density(lifespans)&#13;
df_shaded &lt;- data.frame(x = df_density$x, y = df_density$y)&#13;
df_shaded &lt;- df_shaded[df_shaded$x &gt; threshold,]&#13;
ggplot(df, aes(x=lifespan)) +&#13;
  geom_density(fill="lightblue") +&#13;
  geom_vline(xintercept = threshold, linetype="dashed", color="red") +&#13;
  geom_area(data = df_shaded, aes(x=x, y=y), fill="orange", alpha=0.5) +&#13;
  theme_minimal() +&#13;
  labs(title="Lifespan of batteries", x="Lifespan (hours)", y="Probability Density")</pre><p class="list-inset">Here, we build a DataFrame, <strong class="source-inline">df</strong>, to store the sample value and the corresponding density that was obtained using the <strong class="source-inline">density()</strong> function. We then subset to get the corresponding DataFrame, <strong class="source-inline">df_shaded</strong>, for the area to be shaded. In <strong class="source-inline">ggplot</strong>, we use the <strong class="source-inline">geom_density()</strong> function to plot the density curve, <strong class="source-inline">geom_vline()</strong> to add a vertical line indicating the threshold, and <strong class="source-inline">geom_area()</strong> to shade the area toward <span class="No-Break">the right.</span></p><p class="list-inset">Running <a id="_idIndexMarker945"/>this <a id="_idIndexMarker946"/>code generates <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer192" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_012.jpg" alt="Figure 10.12 – Visualizing the PDF of the empirical normal distribution with the area above the threshold shaded" width="1154" height="902"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Visualizing the PDF of the empirical normal distribution with the area above the threshold shaded</p>&#13;
			<p>The next section covers another continuous probability distribution: the <span class="No-Break">exponential distribution.</span></p>&#13;
			<h2 id="_idParaDest-220"><a id="_idTextAnchor226"/>The exponential distribution</h2>&#13;
			<p>The <strong class="bold">exponential distribution</strong> is a <a id="_idIndexMarker947"/>continuous probability distribution<a id="_idIndexMarker948"/> that’s commonly used to model the time or space between events in a Poisson process. As covered in the previous section, a Poisson process models events that occur independently and at a constant average rate. The exponential distribution is often employed to describe the waiting time between rare events, such as the time between phone calls at a <span class="No-Break">call center.</span></p>&#13;
			<p>The PDF of the exponential distribution is given by the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≥</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span></p>&#13;
			<p>Here, <span class="_-----MathTools-_Math_Variable">λ</span> is the rate parameter, which represents the average number of events that occur per unit of time or space, and <span class="_-----MathTools-_Math_Variable">e</span> is the base of the <span class="No-Break">natural logarithm.</span></p>&#13;
			<p>A defining characteristic of an exponential distribution is the memoryless property, which says that the probability of an event occurring in the future is independent of the time that has already elapsed since the last event. This makes the exponential distribution a suitable choice for modeling waiting times between independent and <span class="No-Break">rare events.</span></p>&#13;
			<p>The mean and standard deviation of the exponential distribution are <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Variable"> </span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">λ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span></p>&#13;
			<p>The exponential distribution contains a single parameter, <span class="_-----MathTools-_Math_Variable">λ</span>, and models the average number of events occurring per unit of time or space for the waiting time between events. The same parameter, <span class="_-----MathTools-_Math_Variable">λ</span>, in the context of modeling a Poisson process, refers to the average<a id="_idIndexMarker949"/> number of events occurring in a fixed interval, in terms of <a id="_idIndexMarker950"/>either time or space. Both distributions are used to model different aspects of the same <span class="No-Break">Poisson process.</span></p>&#13;
			<p>Now, let’s go through an exercise to review the probability calculations related to the <span class="No-Break">exponential distribution.</span></p>&#13;
			<h3>Exercise 10.9 – calculating probabilities with the exponential distribution</h3>&#13;
			<p>In this exercise, we will look<a id="_idIndexMarker951"/> at generating <a id="_idIndexMarker952"/>random samples while following an exponential distribution and calculate and visualize the total probability above a <span class="No-Break">certain threshold:</span></p>&#13;
			<ol>&#13;
				<li>Generate<a id="_idIndexMarker953"/> a random<a id="_idIndexMarker954"/> sample of 1,000 data points from an exponential distribution with a rate parameter of <strong class="source-inline">0.01</strong> using the <span class="No-Break"><strong class="source-inline">rexp()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
set.seed(8) # Set seed for reproducibility&#13;
lambda = 0.01&#13;
sample_size = 1000&#13;
exponential_sample = rexp(sample_size, rate = lambda)</pre></li>				<li>Calculate the probability that the waiting time between events is more than 150 units using the <span class="No-Break"><strong class="source-inline">pexp()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
threshold = 150&#13;
probability_above_threshold = 1 - pexp(threshold, rate = lambda)&#13;
&gt;&gt;&gt; probability_above_threshold&#13;
0.2231302</pre></li>				<li>Plot the PDF and shade the area under the curve for waiting times greater than <span class="No-Break">the threshold:</span><pre class="source-code">&#13;
# Create a data frame for the waiting times&#13;
waiting_times = seq(0, max(exponential_sample), length.out = 1000)&#13;
density_values = dexp(waiting_times, rate = lambda)&#13;
df = data.frame(waiting_times, density_values)&#13;
# Filter data for the shaded region&#13;
df_shaded = df[df$waiting_times &gt; threshold,]&#13;
# Plot the PDF of the exponential distribution&#13;
ggplot(df, aes(x = waiting_times, y = density_values)) +&#13;
  geom_line() +&#13;
  geom_area(data = df_shaded, aes(x = waiting_times, y = density_values), fill = "orange", alpha = 0.5) +&#13;
  geom_vline(xintercept = threshold, linetype = "dashed", color = "red") +&#13;
  theme_minimal() +&#13;
  labs(title = "Exponential Distribution (<img src="Images/011.png" alt="" role="presentation" width="24" height="26"/> = 0.01)", x = "Waiting Time", y = "Probability Density")</pre><p class="list-inset">Here, we<a id="_idIndexMarker955"/> create a <a id="_idIndexMarker956"/>DataFrame for different waiting times generated in the random samples, obtain the corresponding densities using the <strong class="source-inline">dexp()</strong> function, and build the <strong class="source-inline">df</strong> and <strong class="source-inline">df_shaded</strong> DataFrames to be used <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">ggplot</strong></span><span class="No-Break">.</span></p><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<p class="IMG---Figure"> </p>&#13;
			<div>&#13;
				<div id="_idContainer194" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_013.jpg" alt="Figure 10.13 – Visualizing the empirical PDF of the exponential distribution with the area above the threshold shaded" width="942" height="760"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Visualizing the empirical PDF of the exponential distribution with the area above the threshold shaded</p>&#13;
			<p>The next section introduces <span class="No-Break">uniform distribution.</span></p>&#13;
			<h2 id="_idParaDest-221"><a id="_idTextAnchor227"/>Uniform distribution</h2>&#13;
			<p>As the name suggests, uniform <a id="_idIndexMarker957"/>distribution<a id="_idIndexMarker958"/> is a continuous probability distribution where all outcomes within a given range are equally likely. The PDF, which appears as a straight line (illustrated next), is characterized by two parameters, the lower bound (<span class="_-----MathTools-_Math_Variable">a</span>) and the upper bound (<span class="_-----MathTools-_Math_Variable">b</span>), which define the range of possible values. All values within this range have the same probability of occurrence, and values outside the range have a probability <span class="No-Break">of zero.</span></p>&#13;
			<p>The PDF of the uniform distribution is given by the <span class="No-Break">following formula:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base">]</span><span class="_-----MathTools-_Math_Base">  </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">h</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable"> </span></p>&#13;
			<p>The parameters of a uniform distribution are <span class="No-Break">as follows:</span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span></p>&#13;
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">b</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">12</span><span class="_-----MathTools-_Math_Number"> </span></p>&#13;
			<p>Let’s go through a similar exercise to analyze uniformly distributed <span class="No-Break">random variables.</span></p>&#13;
			<h3>Exercise 10.10 – calculating probabilities with uniform distribution</h3>&#13;
			<p>In this exercise, we will<a id="_idIndexMarker959"/> look at generating random samples, calculating<a id="_idIndexMarker960"/> probabilities, and plotting the PDF of a <span class="No-Break">uniform distribution:</span></p>&#13;
			<ol>&#13;
				<li>Generate a random sample of 10,000 data points from a uniform distribution with a lower bound (<strong class="source-inline">a</strong>) of <strong class="source-inline">2</strong> and an upper bound (<strong class="source-inline">b</strong>) of <strong class="source-inline">10</strong> using the <span class="No-Break"><strong class="source-inline">runif()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
set.seed(8) # Set seed for reproducibility&#13;
a = 2&#13;
b = 10&#13;
sample_size = 10000&#13;
uniform_sample = runif(sample_size, min = a, max = b)</pre></li>				<li>Calculate the probability that a value selected from the uniform distribution is greater than <strong class="source-inline">7</strong> using the <span class="No-Break"><strong class="source-inline">punif()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
threshold = 7&#13;
probability = 1 - punif(threshold, min = a, max = b)&#13;
&gt;&gt;&gt; probability&#13;
0.375</pre><p class="list-inset">Here, we use the <strong class="source-inline">punif()</strong> function to calculate the CDF between <strong class="source-inline">a</strong> and <strong class="source-inline">threshold</strong>. We can also approximate this probability using the <span class="No-Break">empirical samples:</span></p><pre class="source-code">probability2 = sum(uniform_sample &gt; t&#13;
hreshold) / length(uniform_sample)&#13;
&gt;&gt;&gt; probability2&#13;
0.3771</pre><p class="list-inset">We can <a id="_idIndexMarker961"/>see that the approximation<a id="_idIndexMarker962"/> is quite close, and it will be even closer when the sample size <span class="No-Break">gets larger.</span></p></li>				<li>Plot its PDF <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">ggplot()</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
library(ggplot2)&#13;
# Create a data frame for the distribution&#13;
x_values = seq(a, b, length.out = 1000)&#13;
density_values = dunif(x_values, min = a, max = b)&#13;
df = data.frame(x_values, density_values)&#13;
# Plot the PDF of the uniform distribution&#13;
ggplot(df, aes(x = x_values, y = density_values)) +&#13;
  geom_line() +&#13;
  theme_minimal() +&#13;
  labs(title = "Uniform Distribution (a = 2, b = 10)", x = "Value", y = "Probability Density")</pre><p class="list-inset">Here, we <a id="_idIndexMarker963"/>build a sequence of <a id="_idIndexMarker964"/>placeholders on the <em class="italic">x</em> axis. Then, we obtain the corresponding densities and combine them into a DataFrame for plotting. Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer195" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_014.jpg" alt="Figure 10.14 – Visualizing the PDF of a uniform distribution" width="1022" height="790"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Visualizing the PDF of a uniform distribution</p>&#13;
			<p>Uniform distribution can be used to generate normally distributed random samples. Let’s look at how this can <span class="No-Break">be done.</span></p>&#13;
			<h2 id="_idParaDest-222"><a id="_idTextAnchor228"/>Generating normally distributed random samples</h2>&#13;
			<p>So far, we <a id="_idIndexMarker965"/>have learned how to sample from a Gaussian distribution using the <strong class="source-inline">rnorm()</strong> function. It helps to look at how these random samples are generated under the hood. This specific technique is known as the inverse transform method, which relies on the inverse CDF or quantile function of the target distribution (in this case, the <span class="No-Break">normal distribution).</span></p>&#13;
			<p>The process involves three steps. First, we will generate random samples from a uniform distribution, usually <span class="_-----MathTools-_Math_Variable">U</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">)</span>, where all values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> are equally likely. Next, for each uniform random sample, we will locate the corresponding value in the target distribution using the inverse CDF (quantile function) of the standard normal distribution. Lastly, we will apply the scale-location transformation to convert the standard normal random sample into a random sample from the target <span class="No-Break">normal distribution.</span></p>&#13;
			<p>This method relies on the property that the CDF of a continuous random variable is a function that maps the sample space (the domain of the PDF) to the range of [<strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>]. The inverse CDF is the inverse of the CDF and maps the interval, [0, 1], back to the sample space. By passing the uniform random samples into the inverse CDF, we reverse the mapping and obtain random samples that follow the target distribution, subject to <span class="No-Break">additional transformation.</span></p>&#13;
			<p>Let’s look at this process in greater detail. Suppose we want to sample from a normal distribution, <span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">)</span>. How do we generate random samples from this particular distribution? One approach is to generate a random sample, <span class="_-----MathTools-_Math_Variable">x</span>, from a standard normal distribution <span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">)</span>, and then apply the scale-location transformation to obtain the final sample, <span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">μ</span>. By scaling the sample based on <span class="_-----MathTools-_Math_Variable">σ</span>, followed by adding <span class="_-----MathTools-_Math_Variable">μ</span>, the resulting sample will follow a normal distribution with mean, <span class="_-----MathTools-_Math_Variable">μ</span>, and variance, <span class="No-Break"><span class="_-----MathTools-_Math_Variable">σ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break">.</span></p>&#13;
			<p>The first step of sampling from a standard normal distribution is the key, whereas the second step is a simple and deterministic transformation. The approach we introduced earlier is to transform a uniformly distributed variable using the inverse CDF of a standard normal distribution. For example, if <span class="_-----MathTools-_Math_Variable">U</span> is uniformly distributed on <span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">]</span>, then <span class="_-----MathTools-_Math_Variable_v-normal">Φ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">U</span><span class="_-----MathTools-_Math_Base">)</span> follows <span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">)</span>, where <span class="_-----MathTools-_Math_Variable_v-normal">Φ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span> is the inverse of the cumulative function of a standard <span class="No-Break">normal distribution.</span></p>&#13;
			<p>An illustrative example is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.15</em>. First, we sample a random point from the uniform distribution on <span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">]</span>. Next, we use the inverse CDF of the standard normal distribution to obtain the corresponding sample in the CDF space, given the fact that the CDF monotonically maps an arbitrary input value to an output on <span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Base">]</span>. Mathematically, the random sample of the standard normal is given by <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">Φ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">U</span><span class="_-----MathTools-_Math_Base">)</span>. Considering the one-to-one mapping relationship between the PDF and the CDF of the standard normal, we could obtain the same input, <span class="_-----MathTools-_Math_Variable">x</span>, in the PDF space as well. We would also expect most of the samples to be centered around the mean. Finally, we apply the scale-location transformation to convert to the random sample of the normal distribution with the desired mean <span class="No-Break">and variance:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer196" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_015.jpg" alt="Figure 10.15 – Obtaining a random sample from the desired univariate Gaussian distribution" width="1413" height="818"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – Obtaining a random sample from the desired univariate Gaussian distribution</p>&#13;
			<p>Let’s look at <a id="_idIndexMarker966"/>a concrete example of generating normally distributed random samples using the inverse transform method. In the following code snippet, we first set the seed for reproducibility and define the parameters of the target normal distribution. We then generate 5 samples while following the uniform distribution between 0 and 1. Finally, we calculate the corresponding quantiles for the uniform sample using the inverse CDF (the quantile function, <strong class="source-inline">qnorm()</strong>) of the <span class="No-Break">normal distribution:</span></p>&#13;
			<pre class="source-code">&#13;
set.seed(8) # Set seed for reproducibility&#13;
# Define the target normal distribution parameters&#13;
mu = 5&#13;
sigma = 2&#13;
# Generate uniform random variables&#13;
n = 5&#13;
uniform_sample = runif(n)&#13;
# Calculate the corresponding quantiles for the uniform sample using the inverse CDF (quantile function) of the normal distribution&#13;
normal_sample = qnorm(uniform_sample, mean = mu, sd = sigma)&#13;
&gt;&gt;&gt; normal_sample&#13;
[1] 4.830828 3.372006 6.680800 5.780755 4.073034</pre>			<p>We can also apply the scale-location transformation to obtain the same random samples, as shown in the following <span class="No-Break">code snippet:</span></p>&#13;
			<pre class="source-code">&#13;
normal_sample2 = qnorm(uniform_sample, mean = 0, sd = 1)&#13;
&gt;&gt;&gt; normal_sample2 * sigma + mu&#13;
[1] 4.830828 3.372006 6.680800 5.780755 4.073034</pre>			<p>The next section covers <span class="No-Break">sampling distributions.</span></p>&#13;
			<h1 id="_idParaDest-223"><a id="_idTextAnchor229"/>Understanding common sampling distributions</h1>&#13;
			<p>A sampling distribution<a id="_idIndexMarker967"/> is a probability distribution of a sample statistic based on many samples drawn from a population. In other words, it is the distribution of a particular statistic (such as the mean, median, or proportion) calculated from many sets of samples from the same population, where each set has the same size. There are two things to take note of here. First, the sampling distribution is not about the random samples drawn from the PDF. Instead, it is a distribution that’s made from an aggregate statistic, which comes from another distribution drawn from the PDF. Second, we would need to sample from the PDF in multiple rounds to create the sampling distribution, where each round consists of multiple samples from <span class="No-Break">the PDF.</span></p>&#13;
			<p>Let’s look at an exercise in R to illustrate the concept of the sampling distribution using the sample mean as the statistic of interest. We will generate samples from a population whose distribution is given and calculate the sample means. Then, we will create a histogram of the sample means to visualize the sampling distribution of the <span class="No-Break">sample mean.</span></p>&#13;
			<h3>Exercise 10.11 – generating a sampling distribution</h3>&#13;
			<p>In this exercise, we will first <a id="_idIndexMarker968"/>generate a population of samples from a normal distribution. We will then sample from this population in multiple rounds, with each round consisting of multiple samples. Finally, we will extract the mean of each round of samples and plot them together in <span class="No-Break">a histogram:</span></p>&#13;
			<ol>&#13;
				<li>Generate 100,000 samples from <span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">50,10</span><span class="_-----MathTools-_Math_Base">)</span>. Check the summary of the samples <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">summary()</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
set.seed(8) # Set seed for reproducibility&#13;
# Define the population parameters&#13;
population_mean = 50&#13;
population_sd = 10&#13;
population_size = 100000&#13;
# Generate the population using a normal distribution&#13;
population &lt;- rnorm(population_size, mean = population_mean, sd = population_sd)&#13;
&gt;&gt;&gt; summary(population)&#13;
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.&#13;
  7.597  43.261  50.051  50.027  56.781  89.365</pre></li>				<li>Define a function to sample 50 numbers from the previous population and return the mean of <span class="No-Break">the samples:</span><pre class="source-code">&#13;
# Define the sample size in each round&#13;
sample_size_per_round = 50&#13;
# Function to draw a sample and calculate its mean&#13;
get_sample_mean &lt;- function(population, sample_size_per_round) {&#13;
  sample &lt;- sample(population, size = sample_size_per_round, replace = FALSE)&#13;
  return(mean(sample))&#13;
}</pre><p class="list-inset">Here, we<a id="_idIndexMarker969"/> use the <strong class="source-inline">sample()</strong> function to sample 50 numbers from the population of samples created earlier without replacement. We can test out this function and observe a different return in each run, which is also close to the population mean <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">50</strong></span><span class="No-Break">:</span></p><pre class="source-code">&gt;&gt;&gt; get_sample_mean(population, sample_size_per_round)&#13;
50.30953&#13;
&gt;&gt;&gt; get_sample_mean(population, sample_size_per_round)&#13;
48.9098</pre></li>				<li>Repeat this function for a total of 1,000 times to obtain a corresponding set of 1,000 <span class="No-Break">sample means:</span><pre class="source-code">&#13;
# Generate multiple rounds of sample means&#13;
num_rounds = 1000 # the number of rounds to sample&#13;
sample_means = replicate(num_rounds, get_sample_mean(population, sample_size))&#13;
&gt;&gt;&gt; summary(sample_means)&#13;
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.&#13;
  49.76   49.96   50.02   50.03   50.09   50.34</pre><p class="list-inset">Here, we use the <strong class="source-inline">replicate()</strong> function to apply the <strong class="source-inline">get_sample_mean()</strong> function repeatedly for a specified number of times. It is often used in simulations, resampling methods, or any situation where you need to perform the same operation multiple times and collect <span class="No-Break">the results.</span></p><p class="list-inset">The result shows a sample mean from the sampling distribution that is very close to the <span class="No-Break">population mean.</span></p></li>				<li>Visualize the<a id="_idIndexMarker970"/> sampling distribution of the sample mean using <span class="No-Break">a histogram:</span><pre class="source-code">&#13;
library(ggplot2)&#13;
sampling_distribution_df = data.frame(sample_means)&#13;
ggplot(sampling_distribution_df, aes(x = sample_means)) +&#13;
  geom_histogram(aes(y = after_stat(density)), bins = 30, color = "black", fill = "lightblue") +&#13;
  geom_density(color = "red", lwd = 1.2) +&#13;
  theme_minimal() +&#13;
  labs(title = "Sampling Distribution of the Sample Mean",&#13;
       x = "Sample Mean",&#13;
       y = "Density")</pre><p class="list-inset">Running this code generates <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<p class="IMG---Figure"> </p>&#13;
			<div>&#13;
				<div id="_idContainer197" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_016.jpg" alt="Figure 10.16 – Visualizing the sampling distribution of the sample mean in a histogram" width="930" height="758"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Visualizing the sampling distribution of the sample mean in a histogram</p>&#13;
			<p>In the next section, we’ll introduce common sampling distributions that are used in statistical estimation and <span class="No-Break">hypothesis testing.</span></p>&#13;
			<h2 id="_idParaDest-224"><a id="_idTextAnchor230"/>Common sampling distributions</h2>&#13;
			<p>Several common sampling distributions are widely used in statistical inference. These distributions arise from the properties of different statistics calculated from random samples drawn from a population. Some of the most important sampling distributions include <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Sampling distribution of the sample mean</strong>: When the population follows a normal <a id="_idIndexMarker971"/>distribution, the sampling distribution of the sample also assumes a normal distribution, where the mean is the same and the standard deviation is a scaled version of the population standard deviation. According to the <strong class="bold">central limit theorem</strong> (to be introduced<a id="_idIndexMarker972"/> in the next chapter), the sampling distribution of the sample mean approaches a normal distribution as more samples are collected, regardless of what the raw population distribution is. In other words, when the sample size is large (usually <strong class="source-inline">n &gt; 30</strong>), the sampling distribution of the sample mean approaches a normal distribution, even if the original population distribution is not necessarily normal. This property allows us to use the normal distribution to make inferences for the sample mean, such as hypothesis<a id="_idIndexMarker973"/> testing and confidence <span class="No-Break">interval construction.</span></li>&#13;
				<li><strong class="bold">Sampling distribution of the sample proportion</strong>: The distribution of the sample proportion (the<a id="_idIndexMarker974"/> proportion of successes in a group of samples) in a large number of independent Bernoulli trials (binary outcomes) follows a normal distribution, given a sufficiently large sample size and a success probability not too close to 0 or 1. In this case, the mean stays the same and the standard deviation is a transformed population proportion, adjusted by the sample size. In particular, under certain conditions (<span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10</span> and <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&gt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10</span> based on our previous introduction on approximating the binomial distribution via normal distribution), the sampling distribution of the sample proportion can be approximated by a normal distribution with the same mean and a standard deviation given by <span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">√</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">n</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"> </span></span><span class="No-Break">.</span></li>&#13;
				<li><strong class="bold">t-distribution</strong>: The <strong class="bold">t-distribution</strong> is also called the <strong class="bold">Student’s t-distribution</strong>. It is used when <a id="_idIndexMarker975"/>estimating the population mean from a small <a id="_idIndexMarker976"/>sample (typically <span class="_-----MathTools-_Math_Variable">n</span>&lt;30) drawn from a<a id="_idIndexMarker977"/> normal population, whose standard deviation is not provided. The t-distribution is similar in shape to the normal distribution but has a thicker tail, with the exact shape determined by the degree of freedom (<em class="italic">df</em>), which is typically equal to <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>. The t-distribution can be used to calculate the t-scores, which are used in hypothesis testing and confidence interval construction when the population standard deviation is <span class="No-Break">not provided.</span></li>&#13;
				<li><strong class="bold">Chi-square distribution</strong>: The <strong class="bold">chi-square distribution</strong> is used in hypothesis testing and the<a id="_idIndexMarker978"/> construction of<a id="_idIndexMarker979"/> confidence intervals for the population variance. It can also be used to test independence in contingency tables. It is a family of right-skewed distributions defined by a single parameter, the degree of freedom. In the context of hypothesis testing for the population variance, the test statistic assumes a chi-square distribution with <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> degrees of freedom. In the context of contingency tables, the test statistic follows a chi-square distribution with <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span> degrees of freedom, where <span class="_-----MathTools-_Math_Variable">r</span> denotes the number of rows and <span class="_-----MathTools-_Math_Variable">c</span> denotes the number of columns in <span class="No-Break">the table.</span></li>&#13;
				<li><strong class="bold">F-distribution</strong>: The <strong class="bold">F-distribution</strong> is used in the <strong class="bold">analysis of variance</strong> (<strong class="bold">ANOVA</strong>) to test the <a id="_idIndexMarker980"/>null hypothesis <a id="_idIndexMarker981"/>that the means of multiple groups are equal. It is<a id="_idIndexMarker982"/> a family of right-skewed distributions that are defined by two parameters, the <em class="italic">df</em> for the numerator (<em class="italic">df1</em>) and the denominator (<em class="italic">df2</em>). For example, in the context of one-way ANOVA, the test statistic follows an F-distribution with <em class="italic">df1=k - 1</em> and <em class="italic">df2=N - </em><span class="No-Break"><em class="italic">k</em></span><span class="No-Break">.</span></li>&#13;
			</ul>&#13;
			<p>Overall, these sampling distributions are important tools in various inferential procedures, such as making statistical inferences about the parameters of the population distribution based on sample data via hypothesis testing or constructing confidence intervals. The four types of sampling distributions are also a subset of a bigger list in the playbook of statistical inferences, each looking at different assumptions, sample statistics, and <span class="No-Break">inference procedures.</span></p>&#13;
			<p>Let’s look at an exercise on constructing the confidence interval for the population mean using <span class="No-Break">the t-distribution.</span></p>&#13;
			<h3>Exercise 10.12 – estimating the population mean using the t-distribution</h3>&#13;
			<p>In this exercise, we <a id="_idIndexMarker983"/>will use a small sample to estimate the <a id="_idIndexMarker984"/>population mean and construct the confidence interval of the estimate using the t-distribution <span class="No-Break">in R:</span></p>&#13;
			<ol>&#13;
				<li>Generate <a id="_idIndexMarker985"/>10 samples from a normal<a id="_idIndexMarker986"/> distribution, <span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">50,10</span><span class="_-----MathTools-_Math_Number">)</span>, using the <span class="No-Break"><strong class="source-inline">rnorm()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
sample_size = 10&#13;
mu = 50&#13;
sigma = 10&#13;
samples = rnorm(sample_size, mean = mu, sd = sigma)&#13;
&gt;&gt;&gt; samples&#13;
[1] 41.57424 39.61629 59.86689 58.94655 43.43934 28.41854 67.05759 50.36661 51.61680 37.71842</pre></li>				<li>Calculate the sample mean and <span class="No-Break">standard deviation:</span><pre class="source-code">&#13;
sample_mean = mean(samples)&#13;
sample_sd = sd(samples)&#13;
&gt;&gt;&gt; sample_mean&#13;
47.86213&#13;
&gt;&gt;&gt; sample_sd&#13;
11.85024</pre><p class="list-inset">We will now use the sample mean and standard deviation to estimate the <span class="No-Break">population mean.</span></p></li>				<li>Calculate the 95% confidence interval using <span class="No-Break">the t-distribution:</span><pre class="source-code">&#13;
alpha = 0.05&#13;
t_critical = qt(1 - alpha/2, df = sample_size - 1)  # t-value for a two-tailed test with alpha = 0.05 and df = n - 1&#13;
margin_of_error_t = t_critical * (sample_sd / sqrt(sample_size))&#13;
ci_t = c(sample_mean - margin_of_error_t, sample_mean + margin_of_error_t)&#13;
&gt;&gt;&gt; ci_t&#13;
39.38497 56.33928</pre><p class="list-inset">Here, we <a id="_idIndexMarker987"/>use the <strong class="source-inline">qt()</strong> function<a id="_idIndexMarker988"/> to find the critical t-value corresponding to a two-tailed test with a significance level of <strong class="source-inline">0.05</strong> and degrees of freedom, <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>. Then, we calculate the margin of error by multiplying the critical t-value by the standard error of the sample mean (note that we need to divide by the square root of sample size) and construct the confidence interval by adding and subtracting the margin of error from the <span class="No-Break">sample mean.</span></p></li>			</ol>&#13;
			<p>We will introduce constructing the confidence interval around a sample estimate in more detail in the next chapter. For now, it suffices to understand how a sampling distribution could be derived and used to produce estimates about the profile of <span class="No-Break">the population.</span></p>&#13;
			<p>The next section covers another interesting and important topic: <span class="No-Break">order statistics.</span></p>&#13;
			<h1 id="_idParaDest-225"><a id="_idTextAnchor231"/>Understanding order statistics</h1>&#13;
			<p><strong class="bold">Order statistics</strong> are the values of a collection<a id="_idIndexMarker989"/> of samples when arranged in ascending or descending order. These ordered samples provide useful information about the distribution and characteristics of the sampled data. Usually, the <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">h</span> order statistic is the <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">h</span> smallest value in the <span class="No-Break">sorted sample.</span></p>&#13;
			<p>For example, for a collection of samples of size <span class="_-----MathTools-_Math_Variable">n</span>, the order statistics are denoted as <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span>, <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">…</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span>, where <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span> is the smallest value (the minimum), <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span> is the largest value (the maximum), and <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">k</span> represents the <span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">h</span> smallest value in the <span class="No-Break">sorted sample.</span></p>&#13;
			<p>Let’s look at how to extract order statistics <span class="No-Break">in R.</span></p>&#13;
			<h2 id="_idParaDest-226"><a id="_idTextAnchor232"/>Extracting order statistics</h2>&#13;
			<p>Extracting the <a id="_idIndexMarker990"/>order statistics of a collection of samples could involve two types of tasks. We may be interested in collecting samples in an ordered fashion, which can be achieved using the <strong class="source-inline">sort()</strong> function. Alternatively, we may be interested in extracting a specific order statistic from the ordered collection of samples, such as finding the third-largest sample in the collection or calculating a particular quantile of <span class="No-Break">the collection.</span></p>&#13;
			<p>Let’s look at an example of such extraction <span class="No-Break">in R.</span></p>&#13;
			<h3>Exercise 10.13 – extracting order statistics</h3>&#13;
			<p>In this exercise, we will <a id="_idIndexMarker991"/>generate a collection of normally distributed random samples and look at sorting the samples and extracting particular order statistics <span class="No-Break">from them:</span></p>&#13;
			<ol>&#13;
				<li>Generate 10 random samples from a normal distribution with a mean of <strong class="source-inline">50</strong> and a standard deviation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
set.seed(8)&#13;
samples = rnorm(10, mean = 50, sd = 10)&#13;
&gt;&gt;&gt; samples&#13;
[1] 49.15414 58.40400 45.36517 44.49165 57.36040 48.92119 48.29711 39.11668 19.88948 44.06826</pre></li>				<li>Sort them in ascending order using the <span class="No-Break"><strong class="source-inline">sort()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
sorted_samples = sort(samples)&#13;
&gt;&gt;&gt; sorted_samples&#13;
[1] 19.88948 39.11668 44.06826 44.49165 45.36517 48.29711 48.92119 49.15414 57.36040 58.40400</pre><p class="list-inset">We can see that these samples are now ordered in ascending order. We can switch to descending order by setting <strong class="source-inline">decreasing = T</strong> in the <span class="No-Break"><strong class="source-inline">sort()</strong></span><span class="No-Break"> function:</span></p><pre class="source-code">&gt;&gt;&gt; sort(samples, decreasing = T)&#13;
[1] 58.40400 57.36040 49.15414 48.92119 48.29711 45.36517 44.49165 44.06826 39.11668 19.88948</pre></li>				<li>Find the minimum value (first <span class="No-Break">order statistic):</span><pre class="source-code">&#13;
min_value = sorted_samples[1]&#13;
&gt;&gt;&gt; min_value&#13;
19.88948</pre></li>				<li>Find the maximum value (last <span class="No-Break">order statistic):</span><pre class="source-code">&#13;
max_value = sorted_samples[length(sorted_samples)]&#13;
&gt;&gt;&gt; max_value&#13;
58.404</pre><p class="list-inset">Here, we <a id="_idIndexMarker992"/>obtain the index of the last entry using the length of the collection <span class="No-Break">of samples.</span></p></li>				<li>Find the third order statistic (that is, <strong class="source-inline">k = </strong><span class="No-Break"><strong class="source-inline">3</strong></span><span class="No-Break">):</span><pre class="source-code">&#13;
k = 3&#13;
kth_order_stat = sorted_samples[k]&#13;
&gt;&gt;&gt; kth_order_stat&#13;
44.06826</pre></li>				<li>Calculate the median (<span class="No-Break">50th percentile):</span><pre class="source-code">&#13;
median_value = median(samples)&#13;
&gt;&gt;&gt; median_value&#13;
46.83114</pre><p class="list-inset">Note that the median (or any other order statistics) stays the same in the raw samples or the <span class="No-Break">sorted samples:</span></p><pre class="source-code">&gt;&gt;&gt; median(sorted_samples)&#13;
46.83114</pre></li>				<li>Calculate the 25th and 75th percentiles (first and third quartiles) using the <span class="No-Break"><strong class="source-inline">quantile()</strong></span><span class="No-Break"> function:</span><pre class="source-code">&#13;
quartiles = quantile(samples, probs = c(0.25, 0.75))&#13;
&gt;&gt;&gt; quartiles&#13;
    25%     75%&#13;
44.1741 49.0959</pre><p class="list-inset">Again, applying<a id="_idIndexMarker993"/> the same function to the ordered samples gives the <span class="No-Break">same results:</span></p><pre class="source-code">&gt;&gt;&gt; quantile(sorted_samples, probs = c(0.25, 0.75))&#13;
    25%     75%&#13;
44.1741 49.0959</pre></li>			</ol>&#13;
			<p>In the next section, we’ll cover a very important use of order statistics in finance: the value <span class="No-Break">at risk.</span></p>&#13;
			<h2 id="_idParaDest-227"><a id="_idTextAnchor233"/>Calculating the value at risk</h2>&#13;
			<p>The <strong class="bold">value at risk</strong> (<strong class="bold">VaR</strong>) is a<a id="_idIndexMarker994"/> widely used risk management metric in finance that estimates the potential loss in a portfolio or investment over a specified period for a given confidence level, such as 95% or 99%. It is used to quantify the downside risk and allocate capital accordingly. Here, the confidence level represents the probability that the potential loss will not exceed the calculated VaR. A higher confidence level indicates a more conservative estimate of <span class="No-Break">the risk.</span></p>&#13;
			<p>There are different approaches to calculating<a id="_idIndexMarker995"/> the VaR. We are going to focus on the simplest approach – that is, using <strong class="bold">historical simulation</strong>. This method uses historical data to simulate potential losses. First, we must sort the historical returns in ascending order. The VaR is then calculated as the percentile of return corresponding to the specified confidence level. This is a simple and intuitive approach, although it assumes that the historical behavior of the returns is representative of <span class="No-Break">the future.</span></p>&#13;
			<p>Let’s go through an exercise to illustrate how to calculate <span class="No-Break">the VaR.</span></p>&#13;
			<h3>Exercise 10.14 – calculating the VaR</h3>&#13;
			<p>In this exercise, we will <a id="_idIndexMarker996"/>discuss how to calculate the VaR, a measure of the potential loss in the value of a portfolio over a specified period for a given <span class="No-Break">confidence level:</span></p>&#13;
			<ol>&#13;
				<li>Generate daily returns in a year (252 trading days) from a normal distribution with a mean of <strong class="source-inline">0.08</strong> and a standard deviation <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.05</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
# Set a seed for reproducibility&#13;
set.seed(8)&#13;
# Generate a random sample of daily returns from a normal distribution&#13;
sample_size = 252  # Number of trading days in a year&#13;
mu = 0.08          # Mean daily return&#13;
sigma = 0.05       # Standard deviation of daily returns&#13;
daily_returns = rnorm(sample_size, mean = mu, sd = sigma)</pre><p class="list-inset">We can check the summary of the daily returns as follows, which shows that the daily return could be as high as 20% and as low as -7%. This means that although the underlying asset is profitable on average (with an expected return of 8%), there is still a significant risk in terms of daily fluctuations. The VaR gives us a measure of how<a id="_idIndexMarker997"/> large the risk is in <span class="No-Break">extreme situations:</span></p><pre class="source-code">&gt;&gt;&gt; summary(daily_returns)&#13;
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.&#13;
-0.07073  0.04480  0.07926  0.07799  0.11424  0.20195</pre></li>				<li>Calculate the VaR at the 95% confidence level for a total portfolio value of 1 <span class="No-Break">million USD:</span><pre class="source-code">&#13;
confidence_level = 0.95&#13;
portfolio_value = 1000000  # Portfolio value in USD&#13;
sorted_returns = sort(daily_returns)&#13;
VaR_index = ceiling(sample_size * (1 - confidence_level))&#13;
VaR = sorted_returns[VaR_index]&#13;
VaR_amount = portfolio_value * (1 - (1 + VaR))&#13;
&gt;&gt;&gt; VaR&#13;
-0.006301223&#13;
&gt;&gt;&gt; VaR_amount&#13;
6301.223</pre><p class="list-inset">Here, we sort the daily returns in ascending order and store them in <strong class="source-inline">sorted_returns</strong>. Then, we obtain the index of the bottom 5% quantile in <strong class="source-inline">VaR_index</strong>, which is then used to retrieve the corresponding VaR of daily returns. Finally, we convert the percentage return into the loss in portfolio value in <strong class="source-inline">VaR_amount</strong>. Note that although the result is a negative number, we often report it as a positive number to indicate the potential loss (or even more) that could occur in an <span class="No-Break">extreme situation.</span></p></li>				<li>Visualize the daily<a id="_idIndexMarker998"/> returns as a density plot and shade the area of <span class="No-Break">the VaR:</span><pre class="source-code">&#13;
library(dplyr)&#13;
daily_returns_df &lt;- data.frame(DailyReturns = daily_returns)&#13;
# Create the density plot&#13;
density_plot &lt;- ggplot(daily_returns_df, aes(x = DailyReturns)) +&#13;
  geom_density(fill = "blue", alpha = 0.5) +&#13;
  geom_vline(aes(xintercept = VaR), linetype = "dashed", color = "red") +&#13;
  labs(x = "Daily Returns", y = "Density", title = "Density Plot of Daily Returns with VaR") +&#13;
  theme_minimal()&#13;
# Add shaded area below the VaR to the density plot&#13;
density_data &lt;- ggplot_build(density_plot)$data[[1]] %&gt;%&#13;
  as.data.frame() %&gt;%&#13;
  filter(x &lt; VaR)&#13;
density_plot +&#13;
  geom_ribbon(data = density_data, aes(x = x, ymin = 0, ymax = y), fill = "red", alpha = 0.5)</pre><p class="list-inset">Here, we first convert the daily returns into a DataFrame, which is used by <strong class="source-inline">ggplot()</strong> to plot a density curve using <strong class="source-inline">geom_density()</strong>. We also add a vertical line representing the VaR using <strong class="source-inline">geom_vline()</strong>. To shade the area below the VaR, we use <strong class="source-inline">ggplot_build()</strong> to filter the data and use <strong class="source-inline">geom_ribbon()</strong> to color the region that satisfies the <span class="No-Break">filtering condition.</span></p><p class="list-inset">Running this code<a id="_idIndexMarker999"/> generates <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.17</em></span><span class="No-Break">:</span></p></li>			</ol>&#13;
			<div>&#13;
				<div id="_idContainer198" class="IMG---Figure">&#13;
					<img src="Images/B18680_10_017.jpg" alt="Figure 10.17 – Visualizing the daily returns and the VaR area" width="1166" height="788"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – Visualizing the daily returns and the VaR area</p>&#13;
			<p>Thus, we can quantify the<a id="_idIndexMarker1000"/> VaR based on the empirical distribution of the observed <span class="No-Break">daily returns.</span></p>&#13;
			<h1 id="_idParaDest-228"><a id="_idTextAnchor234"/>Summary</h1>&#13;
			<p>In this chapter, we covered common probability distributions. We started by introducing discrete probability distributions, including the Bernoulli distribution, the binomial distribution, the Poisson distribution, and the geometric distribution. We followed by covering common continuous probability distributions, including the normal distribution, the exponential distribution, and the uniform distribution. Next, we introduced common sampling distributions and their use in statistical inferences for population statistics. Finally, we covered order statistics and their use in calculating the VaR in the context of daily <span class="No-Break">stock returns.</span></p>&#13;
			<p>In the next chapter, we will cover statistical estimation procedures, including point estimation, the central limit theorem, and the <span class="No-Break">confidence interval.</span></p>&#13;
		</div>&#13;
	</div></body></html>