- en: Building a Streaming Data Analysis Pipeline
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建流式数据分析管道
- en: In this final chapter of the book, we will build an end-to-end streaming data
    pipeline that integrates Amazon ML within the Kinesis Firehose, AWS Lambda, and
    Redshift pipeline. We extend the Amazon ML capabilities by integrating it with
    these other AWS data services to implement real-time Tweet classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章，我们将构建一个端到端的流式数据处理管道，该管道将 Amazon ML 集成到 Kinesis Firehose、AWS Lambda
    和 Redshift 管道中。我们通过将其与其他 AWS 数据服务集成来扩展 Amazon ML 的功能，以实现实时推文分类。
- en: In a second part of the chapter, we show how to address problems beyond a simple
    regression and classification and use Amazon ML for **Named Entity Recognition**
    and content-based recommender systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将展示如何解决超出简单回归和分类的问题，并使用 Amazon ML 进行**命名实体识别**和基于内容的推荐系统。
- en: 'The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Training a twitter classification model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练推特分类模型
- en: Streaming data with Kinesis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kinesis 进行流式数据处理
- en: Storing with Redshift
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Redshift 进行存储
- en: Using AWS Lambda for processing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 进行处理
- en: Named entity recognition and recommender systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别和推荐系统
- en: In the chapter's conclusion, we will summarize Amazon ML's strengths and weaknesses.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的结论中，我们将总结 Amazon ML 的优势和劣势。
- en: Streaming Twitter sentiment analysis
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式推特情感分析
- en: In this chapter, our main project consists of real-time sentiment classification
    of Tweets. This will allow us to demonstrate how to use an Amazon ML model that
    we've trained to process real-time data streams, by leveraging the AWS data ecosystem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的主要项目是实时推文情感分类。这将使我们能够展示如何使用我们训练的 Amazon ML 模型来处理实时数据流，通过利用 AWS 数据生态系统。
- en: 'We will build an infrastructure of AWS services that includes the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个包括以下内容的 AWS 服务基础设施：
- en: '**Amazon ML**: to provide a real-time classification endpoint'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon ML**：提供实时分类端点'
- en: '**Kinesis firehose**: To collect the Tweets'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kinesis firehose**：收集推文'
- en: '**AWS Lambda**: To call an Amazon ML streaming endpoint'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lambda**：调用 Amazon ML 的流式端点'
- en: '**Redshift**: To store the Tweets and their sentiment'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Redshift**：存储推文及其情感'
- en: '**S3**: To act as a temporary store for the Tweets collected by Kinesis Firehose'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3**：作为 Kinesis Firehose 收集的推文的临时存储'
- en: '**AWS Cloudwatch**: To debug and monitor'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Cloudwatch**：调试和监控'
- en: We will also write the necessary Python scripts that feed the Tweets to Kinesis
    Firehose.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将编写必要的 Python 脚本，将推文输入到 Kinesis Firehose。
- en: Popularity contest on twitter
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推特上的流行竞赛
- en: All good data science projects start with a question. We wanted a social network
    question not tied to a current political or societal context. We will be looking
    into the popularity of vegetables on twitter. We want to know what the most popular
    vegetables on twitter are. It's a question that could stand the test of time and
    be adapted to other lists of things, such as fruits, beverages, weather conditions,
    animals, and even brands and politicians. The results will surprise you... or
    not.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有优秀的数据科学项目都始于一个问题。我们想要一个与社会网络相关的问题，而不是与当前的政治或社会背景相关。我们将研究推特上蔬菜的流行度。我们想知道推特上最受欢迎的蔬菜是什么。这是一个经得起时间考验的问题，可以适应其他事物列表，如水果、饮料、天气条件、动物，甚至品牌和政治人物。结果可能会让你感到惊讶……或者不会。
- en: We will start by training a binary classification Amazon ML model using a large
    public twitter sentiment analysis dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用一个大型公开的推特情感分析数据集来训练一个二元分类的 Amazon ML 模型。
- en: 'There are many available sentiment analysis libraries that can, with a few
    lines of code, and given a piece of text, return a sentiment score or a polarity
    (positive, neutral, negative). `TextBlob` is such a library in Python. It is available
    at [https://textblob.readthedocs.io](https://textblob.readthedocs.io). Built on
    top of **NLTK**, `TextBlob` is a powerful library to extract information from
    documents. Besides sentiment analysis, it can carry out some aspects of speech
    tagging, classification, tokenization, named entity recognition, as well as many
    other features. We compare `TextBlob` results with our own Amazon ML classification
    model. Our goal, first and foremost, is to learn how to make these AWS services
    work together seamlessly. Our vegetable popularity contest on social media will
    follow these steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的情感分析库，只需几行代码，就可以根据一段文本返回情感评分或极性（正面、中性、负面）。`TextBlob`是Python中的一个这样的库。它可在[https://textblob.readthedocs.io](https://textblob.readthedocs.io)找到。建立在**NLTK**之上，`TextBlob`是一个强大的库，可以从文档中提取信息。除了情感分析外，它还可以执行一些词性标注、分类、分词、命名实体识别等功能。我们将`TextBlob`的结果与我们的Amazon
    ML分类模型进行比较。我们的首要目标是学习如何使这些AWS服务无缝协作。我们的社交媒体蔬菜流行比赛将遵循以下步骤：
- en: We start by training a twitter sentiment classification model on Amazon ML and
    creating a real-time prediction endpoint.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先在Amazon ML上训练一个Twitter情感分类模型，并创建一个实时预测端点。
- en: We set up a Kinesis Firehose that stores content on S3.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了一个Kinesis Firehose，它将内容存储在S3上。
- en: We write a simple Python script called a `producer` that collects the Tweets
    from the twitter API and sends them to Kinesis. At this point, Kinesis Firehose
    stores the Tweets in S3.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们编写了一个简单的Python脚本，称为`producer`，它从Twitter API收集推文并将它们发送到Kinesis。在此阶段，Kinesis
    Firehose将推文存储在S3中。
- en: We move on from storing streaming data in S3 to storing streaming data in Redshift.
    To do so, we must launch a Redshift cluster and create the required tables.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从将流数据存储在S3转移到将流数据存储在Redshift。为此，我们必须启动一个Redshift集群并创建所需的表。
- en: Finally, we add an AWS Lambda function to our pipeline in order to interrogate
    our Amazon ML classification endpoint.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将一个AWS Lambda函数添加到我们的管道中，以便查询我们的Amazon ML分类端点。
- en: Throughout the project, we use AWS CloudWatch to check the status and debug
    our data pipeline.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个项目中，我们使用AWS CloudWatch来检查状态和调试我们的数据管道。
- en: We end up with a collection of Tweets, with two types of sentiment scoring that
    we can compare. We will look at the ranking of vegetables according to the two
    methods, their variability, and the inter-rate agreement between the two methods,
    and we will try to assess which one is better. It's worth noting at this point
    that we refrain from any involved text processing of Tweets. Tweets are a very
    specific type of textual content that contains URLs, emoticons, abbreviations,
    slang, and hashtags. There is a large number of publications on sentiment analysis
    for twitter that explore different preprocessing and information extraction techniques.
    We will not use any particular feature extraction technique and restrict ourselves
    to a simple bag-of-words approach. Our comparison between sentiment analysis via
    Amazon ML and TextBlog is a proof of concept, not a benchmark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到一个包含推文的集合，其中包含两种类型的情感评分，我们可以进行比较。我们将查看根据两种方法对蔬菜的排名、它们的可变性以及两种方法之间的评分一致性，并尝试评估哪一种更好。值得注意的是，我们避免对推文进行任何复杂的文本处理。推文是一种非常具体的文本内容，其中包含URL、表情符号、缩写、俚语和标签。关于Twitter情感分析的出版物有很多，它们探讨了不同的预处理和信息提取技术。我们不会使用任何特定的特征提取技术，而将自身限制在简单的词袋方法上。我们通过Amazon
    ML和TextBlog进行的情感分析比较是一个概念验证，而不是基准。
- en: The training dataset and the model
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据集和模型
- en: 'The first step in our project is to train a sentiment analysis and classification
    Amazon ML model for Tweets. Fortunately, we have access to a rather large twitter
    sentiment dataset composed of over 1.5M Tweets tagged 0/1 for negative/positive
    sentiments. The dataset is available at [http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/).
    This dataset is an aggregation of two twitter sentiment analysis datasets:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们项目的第一步是为推文训练一个情感分析和分类的Amazon ML模型。幸运的是，我们可以访问一个相当大的Twitter情感数据集，该数据集由超过150万条标记为0/1的推文组成，分别代表负面/正面情感。该数据集可在[http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/)找到。该数据集是两个Twitter情感分析数据集的汇总：
- en: '**University of Michigan Sentiment Analysis** competition on Kaggle: [https://inclass.kaggle.com/c/si650winter11](https://inclass.kaggle.com/c/si650winter11)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle 上的 **University of Michigan Sentiment Analysis** 竞赛：[https://inclass.kaggle.com/c/si650winter11](https://inclass.kaggle.com/c/si650winter11)
- en: '**Twitter Sentiment Corpus** by *Niek Sanders*: [http://www.sananalytics.com/lab/twitter-sentiment/](http://www.sananalytics.com/lab/twitter-sentiment/)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 *Niek Sanders* 编制的 **Twitter Sentiment Corpus**：[http://www.sananalytics.com/lab/twitter-sentiment/](http://www.sananalytics.com/lab/twitter-sentiment/)
- en: 'This Twitter **Sentiment Analysis** Dataset contains 1,578,627 classified Tweets.
    Each row is tagged with 0/1 for negative/positive sentiment. We use a sample of
    that dataset (approximately 10%, 158K Tweets) to train a classification model
    in Amazon ML. We load the dataset on S3, and train and evaluate a binary classification
    model using the Amazon ML service. We apply no specific text transformation on
    the texts. The recipe is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Twitter **情感分析** 数据集包含 1,578,627 个分类推文。每一行都标记为 0/1，表示负面/正面情感。我们使用该数据集的样本（大约
    10%，158K 条推文）在 Amazon ML 中训练一个分类模型。我们在 S3 上加载数据集，并使用 Amazon ML 服务训练和评估一个二元分类模型。我们对文本没有进行特定的文本转换。配方如下：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We set the model to have mild L2 regularization and 100 passes. Training the
    model takes a bit of time (over 10 minutes) to complete, probably due to a large
    number of samples in play. The model evaluation shows pretty good overall performances
    with an AUC of 0.83\. Roughly two-thirds of the Tweets are properly classified
    by the Amazon ML model. The predictions are smoothly distributed across the range
    of class probabilities, as shown by the model''s evaluation graph:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型设置为具有轻微的 L2 正则化和 100 次迭代。训练模型需要一些时间（超过 10 分钟）才能完成，这可能是由于样本数量较多。模型评估显示整体性能相当不错，AUC
    为 0.83。大约三分之二的推文被 Amazon ML 模型正确分类。预测在类别概率范围内均匀分布，如模型评估图所示：
- en: '![](img/B05028_10_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_10_01.png)'
- en: 'We now create a real-time prediction endpoint from the model''s page by clicking
    on the Create endpoint button at the bottom of the page:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过点击模型页面底部的“创建端点”按钮从模型页面创建一个实时预测端点：
- en: '![](img/B05028_10_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_10_02.png)'
- en: 'The endpoint URL is of the form `https://realtime.machinelearning.us-east-1.amazonaws.com`. We
    will use that endpoint to classify new Tweets through requests from the Lambda
    service. Let''s test whether our Amazon ML classifier works as expected with the following
    Python script:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 端点 URL 的形式为 `https://realtime.machinelearning.us-east-1.amazonaws.com`。我们将使用该端点通过
    Lambda 服务的请求来对新推文进行分类。让我们通过以下 Python 脚本来测试我们的 Amazon ML 分类器是否按预期工作：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The sentence `Hello world, this is a great day` returns a score of *0.91* for
    the probability that the sentence is positive, and classifies the sentence as
    *1*, while a `Hello world, this is a sad day` sentence returns a probability of
    *0.08*, and classifies the sentence as 0\. The words `great` and `sad` are the
    words driving the sentiment classification. The classifier is working as expected.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 句子 `Hello world, this is a great day` 返回句子为积极的概率评分为 *0.91*，并将其分类为 *1*，而句子 `Hello
    world, this is a sad day` 返回的概率为 *0.08*，并分类为 0。单词 `great` 和 `sad` 是驱动情感分类的单词。分类器按预期工作。
- en: Let's now switch our attention to the Kinesis service.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向 Kinesis 服务。
- en: Kinesis
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kinesis
- en: 'Kinesis is a multiform service organized around three subservices: **Streams**,
    **Firehose**, and **Analytics**. Kinesis acts as a highly available pipeline to
    stream messages between data producers and data consumers.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 是一个围绕三个子服务组织的多形式服务：**Streams**、**Firehose** 和 **Analytics**。Kinesis
    作为高度可用的管道，在数据生产者和数据消费者之间传输消息。
- en: Data producers are sources of data coming from streaming APIs, IoT devices, system
    logs, and other high volume data streams
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据生产者是来自流式 API、物联网设备、系统日志和其他高容量数据流的来源
- en: Data consumers will most commonly be used to store, process data, or trigger
    alerts
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据消费者最常用于存储、处理数据或触发警报
- en: Kinesis is able to handle up to 500 Terabytes of data per hour. We use Kinesis
    at its minimal level and with its most simple configuration. Readers interested
    in the more complex usage of Kinesis should read the AWS documentation at [https://aws.amazon.com/documentation/kinesis/](https://aws.amazon.com/documentation/kinesis/).
    A good overview of the different concepts and elements behind AWS Kinesis on the
    blog post at [https://www.sumologic.com/blog/devops/kinesis-streams-vs-firehose/](https://www.sumologic.com/blog/devops/kinesis-streams-vs-firehose/).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis 能够每小时处理高达 500 太字节的数据。我们使用 Kinesis 的最低级别和最简单的配置。对 Kinesis 的更复杂用法感兴趣的用户应阅读
    AWS 文档，网址为 [https://aws.amazon.com/documentation/kinesis/](https://aws.amazon.com/documentation/kinesis/)。在博客文章
    [https://www.sumologic.com/blog/devops/kinesis-streams-vs-firehose/](https://www.sumologic.com/blog/devops/kinesis-streams-vs-firehose/)
    中可以找到 AWS Kinesis 的不同概念和元素的良好概述。
- en: Kinesis Stream
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kinesis Stream
- en: Kinesis Stream is used to collect streaming data given by a producer and processed
    by a consumer. It's the simplest of the three Kinesis services as it does not
    store the data in any way. Kinesis Stream mainly acts like a buffer, and the data
    is kept available between 24 hours to 168 hours. An IoT device or a log service
    would typically act as the producer and send data to the Kinesis stream. At the
    same time, a consumer is running to process that data. Consumers services that
    trigger alerts (SMS, e-mails) based on event detection algorithms, real-time dashboards
    updated in real time, data aggregators, or anything that retrieves the data synchronously.
    Both producer and consumer must be running simultaneously for the Kinesis stream
    to function. If your producer and consumer are scripts running on your local machine,
    they must both be running in parallel.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Stream 用于收集由生产者提供的流数据并由消费者处理。它是三个 Kinesis 服务中最简单的一个，因为它以任何方式都不存储数据。Kinesis
    Stream 主要充当缓冲区，数据在 24 小时到 168 小时内保持可用。物联网设备或日志服务通常会作为生产者并将数据发送到 Kinesis 流。同时，一个消费者正在运行以处理这些数据。消费者服务基于事件检测算法触发警报（短信、电子邮件）、实时仪表板实时更新、数据聚合器或任何同步检索数据的任何东西。生产者和消费者必须同时运行，Kinesis
    流才能正常工作。如果您的生产者和消费者是在您的本地机器上运行的脚本，它们必须并行运行。
- en: 'It is possible to add processing to the incoming data by adding an AWS Lambda
    function to the stream. The whole data pipeline will now follow these steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过向流中添加 AWS Lambda 函数来对传入的数据进行处理。整个数据处理管道现在将遵循以下步骤：
- en: A custom application or script sends records to the stream (the producer).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义应用程序或脚本将记录发送到流中（生产者）。
- en: AWS Lambda polls the stream and invokes your Lambda function when new records
    are detected.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS Lambda 会轮询流，并在检测到新记录时调用您的 Lambda 函数。
- en: AWS Lambda executes the Lambda function and sends back the record, original
    or modified, to the Kinesis Stream.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS Lambda 执行 Lambda 函数并将记录（原始或修改后的）发送回 Kinesis 流。
- en: We will not be using Kinesis Streams since we want to store the data we are
    collecting. Kinesis stream is a good way to start with the Kinesis service.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不会使用 Kinesis Streams，因为我们想存储我们收集的数据。Kinesis 流是开始使用 Kinesis 服务的良好方式。
- en: Kinesis Analytics
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kinesis Analytics
- en: '**Kinesis Analytics** is AWS''s most recent add-on to the Kinesis mix. Kinesis
    Analytics allows you analyzing real-time streaming data with standard SQL queries.
    The idea is to go from querying a static representation of the data to a dynamic
    one that evolves as the data streams in. Instead of using AWS Lambda to process
    the data via a scripting language, the goal is to process the data in SQL and
    feed the results to dashboards or alerting systems. We will not be using Kinesis
    Analytics, but instead, focus on Kinesis Firehose.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kinesis Analytics** 是 AWS 对 Kinesis 混合的最新补充。Kinesis Analytics 允许您使用标准 SQL
    查询分析实时流数据。想法是从查询数据的静态表示形式转变为动态表示形式，该表示形式随着数据流的到来而演变。而不是使用 AWS Lambda 通过脚本语言处理数据，目标是使用
    SQL 处理数据并将结果馈送到仪表板或警报系统。我们将不会使用 Kinesis Analytics，而是专注于 Kinesis Firehose。'
- en: Setting up Kinesis Firehose
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Kinesis Firehose
- en: 'We will focus on the Kinesis Firehose service, which offers two important features:
    data received can be fed to an AWS Lambda function for extra processing, and the
    resulting data can be stored on S3 or on a Redshift database.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于 Kinesis Firehose 服务，该服务提供两个重要功能：接收到的数据可以馈送到 AWS Lambda 函数进行额外处理，并且结果数据可以存储在
    S3 或 Redshift 数据库上。
- en: 'We will start by setting up a Kinesis Firehose delivery stream that stores
    data on S3 without any Lambda functionality:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先设置一个 Kinesis Firehose 交付流，该流将数据存储在 S3 而不使用任何 Lambda 功能：
- en: Go to the Kinesis Firehose dashboard at [https://console.aws.amazon.com/firehose/](https://console.aws.amazon.com/firehose/) and
    click on Create Delivery Stream.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往Kinesis Firehose仪表板[https://console.aws.amazon.com/firehose/](https://console.aws.amazon.com/firehose/)并点击“创建传输流”。
- en: 'Fill in the following fields:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写以下字段：
- en: 'Destination: `Amazon S3`'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '目标: `Amazon S3`'
- en: 'Delivery stream name: `veggieTweets` (or choose your own name)'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '传输流名称: `veggieTweets`（或选择你自己的名称）'
- en: 'S3 bucket: `aml.packt`'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'S3存储桶: `aml.packt`'
- en: 'S3 prefix: `veggies`'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'S3前缀: `veggies`'
- en: 'Click on Next:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“下一步”：
- en: '![](img/B05028_10_05.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_10_05.png)'
- en: 'Note that the S3 prefix field corresponds to a folder in your S3 bucket. You
    should now go to S3 and create a `veggies` folder in your bucket. Mimic the setup
    from the next screenshot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，S3前缀字段对应于你的S3存储桶中的一个文件夹。你现在应该前往S3并在你的存储桶中创建一个`veggies`文件夹。模仿下一个截图的设置：
- en: '![](img/B05028_10_06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_10_06.png)'
- en: For the moment, we will keep the Data transformation with AWS Lambda disabled.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，我们将保持使用AWS Lambda进行数据转换的禁用状态。
- en: Enable Error Logging, since we will need to debug our data delivery errors with CloudWatch
    Logs.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用错误日志记录，因为我们需要使用CloudWatch Logs调试我们的数据交付错误。
- en: For the IAM role, select Firehose Delivery IAM Role. You will be taken to the
    IAM service and guided through the creation of the required role. You can choose
    an existing policy and create a new one. The Role/Policy wizard will handle the
    details. Click on Allow to be redirected to the initial Firehose configuration
    screen.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于IAM角色，选择Firehose Delivery IAM角色。你将被带到IAM服务，并指导创建所需的角色。你可以选择现有的策略并创建一个新的策略。角色/策略向导将处理详细信息。点击“允许”以跳转到初始Firehose配置屏幕。
- en: Click on Next, review the details of the Firehose delivery stream, and click
    on Create Delivery Stream.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“下一步”，查看Firehose传输流的详细信息，然后点击“创建传输流”。
- en: We now have a Kinesis Firehose delivery stream that will, once we send data
    to it, store data in the S3 packt.aml/veggies location. We now need to create
    a producer script that will send the data to the Kinesis Firehose service.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个Kinesis Firehose传输流，一旦我们向其发送数据，数据将存储在S3 packt.aml/veggies位置。我们现在需要创建一个生产脚本，将数据发送到Kinesis
    Firehose服务。
- en: Producing tweets
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发布推文
- en: A Kinesis producer can take many shapes as long as it sends data to the Kinesis
    Stream. We will use the Python Firehose SDK and write a simple script that collects
    Tweets from the twitter API, filters some of them, and sends them to the Kinesis
    Firehose. We use the Python-Twitter library.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis生产者可以有多种形式，只要它向Kinesis流发送数据。我们将使用Python Firehose SDK并编写一个简单的脚本，从Twitter
    API收集推文，过滤其中的一些，并将它们发送到Kinesis Firehose。我们使用Python-Twitter库。
- en: There are several Twitter API Python packages available on GitHub. Two of the
    more popular ones, `Twitter` and `Python-Twitter` ([https://pypi.python.org/pypi/python-twitter/](https://pypi.python.org/pypi/python-twitter/))
    share the same import calls `import twitter`, but not the same method calls, which
    can lead to confusion and time being wasted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上有几个Twitter API Python包可用。其中两个更受欢迎的，`Twitter`和`Python-Twitter`([https://pypi.python.org/pypi/python-twitter/](https://pypi.python.org/pypi/python-twitter/))有相同的导入调用`import
    twitter`，但方法调用不同，这可能导致混淆并浪费时间。
- en: 'The Python-Twitter package offers a `GetSearch` method that takes either a
    query string `term` or a `raw_query` as a parameter for the Twitter search:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Python-Twitter包提供了一个`GetSearch`方法，该方法接受查询字符串`term`或`raw_query`作为Twitter搜索的参数：
- en: The query string (`term` ) corresponds to the keywords you would write down
    on the Twitter website search bar; for instance, `term = brocolli OR potato OR
    tomato`.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询字符串（`term`）对应于你会在Twitter网站搜索栏中写下的关键词；例如，`term = brocolli OR potato OR tomato`。
- en: 'The `raw_query` parameter takes for input the parameter part of the URL once
    you''ve hit the search button: the string after the `?` in the URL. You can build
    advanced queries from the twitter advanced search page [https://twitter.com/search-advanced](https://twitter.com/search-advanced).
    For instance, our search for "*brocolli OR potato OR tomato*" translates to a
    `raw_query = q=brocolli%20OR%20potato%20OR%20tomato&src=typd`. We use the `raw_query`
    parameter in our call to the search API.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你点击搜索按钮后，`raw_query`参数会接收URL的参数部分：URL中`?`后面的字符串。你可以从Twitter高级搜索页面[https://twitter.com/search-advanced](https://twitter.com/search-advanced)构建高级查询。例如，我们搜索"*西兰花
    OR 土豆 OR 番茄*"转换为`raw_query = q=brocolli%20OR%20potato%20OR%20tomato&src=typd`。我们在调用搜索API时使用`raw_query`参数。
- en: To obtain your own Twitter development API keys, go to [https://apps.twitter.com/](https://apps.twitter.com/) and
    create an application.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要获取自己的Twitter开发API密钥，请访问[https://apps.twitter.com/](https://apps.twitter.com/)并创建一个应用程序。
- en: 'We first define a class that initializes the access to the twitter API. This
    class has a `capture` method, which runs a search, given a raw query. Save the
    following code in a `tweets.py` file:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个类，该类初始化对Twitter API的访问。这个类有一个`capture`方法，它根据一个原始查询执行搜索。将以下代码保存到`tweets.py`文件中：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Given this class and a `raw_query` string, gathering Tweets consists in initializing
    the `ATweets` class with the `raw_query` and applying the capture method on the
    instantiated object as such:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个类和一个`raw_query`字符串，收集Tweets的过程包括用`raw_query`初始化`ATweets`类，并在实例化的对象上应用捕获方法，如下所示：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, `statuses` is a list of Tweets, each containing many elements. We only
    use a few of these elements. Now that we can gather Tweets from the `Twitter`
    API, we need a producer script that will send the Tweets to Kinesis. The producer
    Python script is as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`statuses`是一个包含许多元素的Tweets列表。我们只使用其中的一些元素。现在我们能够从`Twitter` API收集Tweets，我们需要一个生产脚本，将Tweets发送到Kinesis。以下是一个生产Python脚本：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Save that code to a `producer.py` file in the same folder as the `tweets.py`
    file.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将此代码保存到与`tweets.py`文件相同的文件夹中的`producer.py`文件。
- en: 'As you may have noticed, we restricted the Twitter search to English Tweets
    by specifying `lang = ''en''` in the call to `GetSearch`. However, that did not
    produce the expected results, and many non-English Tweets were returned. In a
    later version of the producer script, we add the following conditions to the Tweets
    themselves before sending them to the Firehose, actually filtering out non-English
    tweets shorter than 10 characters or those send as retweets:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所注意到的，我们通过在`GetSearch`调用中指定`lang = 'en'`将Twitter搜索限制为英文Tweets。然而，这并没有产生预期的结果，并且返回了许多非英文Tweets。在后续版本的生产脚本中，我们在将Tweets发送到Firehose之前，添加了以下条件到Tweets本身，实际上过滤掉了长度小于10个字符或作为转发发送的非英文Tweets：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are now ready to run our producer script. One last important detail to pay
    attention to is that calls to the Twitter API are capped. If you call the API
    too often, you will have to wait longer and longer between your requests until
    they are allowed to go through. There are reliable ways to deal with these restrictions,
    and it''s easy to find code online that show you how to delay your API calls.
    We will simply use the `watch` command line with a delay of 10 minutes (600 s)
    between calls. The `watch` command simply executes whatever command you write
    afterwards, every nth second. To run your producer code, launch a terminal, and
    run the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行我们的生产脚本了。最后，需要注意的一个重要细节是，对Twitter API的调用是有限制的。如果你调用API过于频繁，你将不得不在请求之间等待越来越长时间，直到它们被允许通过。有可靠的方法来处理这些限制，并且很容易在网上找到显示如何延迟API调用的代码。我们将简单地使用带有10分钟（600秒）延迟的`watch`命令行。`watch`命令简单地执行你之后写的任何命令，每n秒执行一次。要运行你的生产代码，打开一个终端，并运行以下命令：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Every 10 minutes, tweets will be captured and sent to Kinesis Firehose. To
    verify that both your script and delivery stream work, go to your S3 bucket and
    the `aml.packt/veggies` folder. You should see files piling up. The files are
    saved by Kinesis in subfolders structured by date/year/month/day and hour. The
    filenames in the last subfolder follow a format similar to `veggieTweets-2-2017-04-02-19-21-51-1b78a44f-12b2-40ce-b324-dbf4bb950458`.
    In these files, you will find the records as they have been defined in the producer
    code. Our producer code sends comma-separated data formatted as tweet ID/twitter
    username/vegetable/tweet content. An example of such as record is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每10分钟，将捕获Tweets并发送到Kinesis Firehose。为了验证你的脚本和交付流是否正常工作，请访问你的S3存储桶和`aml.packt/veggies`文件夹。你应该会看到文件堆积。这些文件是由Kinesis按日期/年/月/日和小时结构化的子文件夹保存的。在最后一个子文件夹中的文件名格式类似于`veggieTweets-2-2017-04-02-19-21-51-1b78a44f-12b2-40ce-b324-dbf4bb950458`。在这些文件中，你会找到在生产代码中定义的记录。我们的生产代码发送以逗号分隔的数据，格式为tweet
    ID/twitter用户名/蔬菜/tweet内容。以下是一个这样的记录示例：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will now set up Redshift so that these tweets and related elements end up stored
    in an SQL database.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将设置Redshift，以便这些Tweets和相关元素最终存储在SQL数据库中。
- en: The Redshift database
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Redshift数据库
- en: 'We saw how to create a Redshift cluster in [Chapter 8](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=688&action=edit),
    *Creating Datasources from Redshift*, we won''t go through the steps again. For
    our vegetable contest project, we create a vegetable cluster and a `vegetablesdb`
    database. Wait till the endpoint for the cluster is ready and the endpoint is
    defined, and then connect to your Redshift database via this `Psql` command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第8章](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=688&action=edit)“从Redshift创建数据源”中看到了如何创建Redshift集群，我们不会再次介绍这些步骤。对于我们的蔬菜竞赛项目，我们创建了一个蔬菜集群和一个`vegetablesdb`数据库。等待集群端点就绪并定义后，通过以下`Psql`命令连接到您的Redshift数据库：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once connected to the database, create the following table with this SQL query:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到数据库后，使用以下SQL查询创建以下表：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that there are no `blob` or `text` data types in Redshift SQL. We defined
    the tweets as `varchar(65535)`, which is probably far too large, but since we
    used `varchar` and not `char`, the volume occupied by the data shrank to the actual
    length of the text and not the whole 65KB. In that table, we only capture the
    ID of the tweet, the tweet itself, what vegetable the tweet was associated with,
    and the screen name of the person writing the tweet. We disregard any other tweet
    elements.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在Redshift SQL中没有`blob`或`text`数据类型。我们将推文定义为`varchar(65535)`，这可能是过于大了，但由于我们使用了`varchar`而不是`char`，数据占用的体积缩小到文本的实际长度，而不是整个65KB。在该表中，我们只捕获推文的ID、推文本身、与推文关联的蔬菜以及撰写推文的人的屏幕名。我们忽略任何其他推文元素。
- en: Adding Redshift to the Kinesis Firehose
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Redshift添加到Kinesis Firehose
- en: 'This part is delicate as several pieces from different services must fit together:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分比较复杂，因为来自不同服务的多个部分必须相互配合：
- en: The data structure, table declaration, and Kinesis Redshift configuration
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构、表声明和Kinesis Redshift配置
- en: The data fields aggregation and subsequent parsing
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据字段聚合和后续解析
- en: The role and associated policies
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角色及其相关策略
- en: 'The fields of the Redshift table that stores the data need to be synchronized in
    three different places:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据的Redshift表字段需要在三个不同的地方进行同步：
- en: The Redshift table with a proper definition of fields.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有适当字段定义的Redshift表。
- en: 'The script that sends the data to Kinesis. Depending on how the record sent
    to Kinesis is aggregated together and later parsed by Redshift, the script must
    concatenate the same number of fields in the same order as the ones defined in
    the Redshift table. For instance, when we write `record = '',''.join([str(st.id)`,
    `st.user.screen_name,veggie, clean_tweet]) + ''n''` in the script, it implies
    that the table has four fields with the right types: `int` , `varchar`, `varchar`,
    and `varchar`.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据发送到Kinesis的脚本。根据发送到Kinesis的记录是如何聚合在一起以及随后由Redshift解析的方式，脚本必须以与Redshift表中定义的相同顺序连接相同数量的字段。例如，当我们在脚本中写入`record
    = ','.join([str(st.id), st.user.screen_name,veggie, clean_tweet]) + 'n'`时，这意味着表有四个字段，且类型正确：`int`，`varchar`，`varchar`，和`varchar`。
- en: The columns as defined in the Kinesis Firehose definition.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如Kinesis Firehose定义中定义的列。
- en: 'For that last point, we need to go back to the Firehose dashboard, create a
    new stream, and define it as a Redshift-based delivery stream. Click on Create
    Delivery Stream, and select Redshift as the destination. Follow the different
    screens and fill in the following values:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最后一个问题，我们需要回到Firehose仪表板，创建一个新的流，并将其定义为基于Redshift的交付流。点击创建交付流，选择Redshift作为目标。按照不同的屏幕提示，填写以下值：
- en: 'S3 bucket: Your own bucket'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3存储桶：您的自己的存储桶
- en: 'S3 prefix: We keep the prefix veggies'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: S3前缀：我们保留前缀veggies
- en: 'Data transformation: Disabled for now'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换：目前禁用
- en: 'Redshift cluster: `Vegetables`'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift集群：`Vegetables`
- en: 'Redshift database: `Vegetablesdb`'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift数据库：`Vegetablesdb`
- en: 'Redshift table columns: ID, `screen_name`, veggie, text (**this one is important**)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift表列：ID，`screen_name`，veggie，text（这个非常重要）
- en: 'Redshift username: The username you access Redshift with'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift用户名：您使用该用户名访问Redshift
- en: 'Redshift COPY options: Delimiter '','' (**very important too**)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift COPY选项：分隔符`,`（同样非常重要）
- en: 'Once created, your Kinesis Firehose stream should resemble the following screenshot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，您的Kinesis Firehose流应类似于以下截图：
- en: '![](img/B05028_10_03.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_10_03.png)'
- en: 'Notice the `COPY` command in the bottom right corner of the screen, which is
    reproduced here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意屏幕右下角的`COPY`命令，此处重现：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This command indicates how Redshift will ingest the data that Kinesis sends
    to S3, what fields it expects, and how it will parse the different fields (for
    instance, separated by a comma). There are other potential `COPY` formats including
    JSON or CSV. We found this one to be simple and working. It's important that the
    way the record is defined and formatted in the producer script (four variables separated
    by commas) corresponds to the COPY part of the `table name (name of the four fields)` command
    with the right definition of the delimiter `','`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令指示Redshift将如何摄取Kinesis发送到S3的数据，它期望哪些字段，以及它将如何解析不同的字段（例如，由逗号分隔）。还有其他潜在的`COPY`格式，包括JSON或CSV。我们发现这个格式简单且有效。重要的是，记录在生成脚本中的定义和格式（由逗号分隔的四个变量）与`table
    name (name of the four fields)`命令的COPY部分相对应，并具有正确的分隔符定义`','`。
- en: This COPY command is also a good way to debug the pipeline when the data is
    not getting recorded into the database. Psql into the database, and run the same
    query in order to get useful error messages on why the queries are failing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据没有记录到数据库中时，这个COPY命令也是调试管道的好方法。进入数据库的Psql，然后运行相同的查询，以便获取有关查询失败原因的有用错误信息。
- en: It's now time for a word on role-based access control.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候谈谈基于角色的访问控制了。
- en: Setting up the roles and policies
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置角色和策略
- en: 'There are two types of access control in AWS: key based and role based. Key
    based is much easier to set up but cannot be used to make Kinesis, Redshift, and
    S3 talk to each other, as AWS indicates at [http://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html](http://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: AWS中有两种类型的访问控制：基于密钥和基于角色。基于密钥的设置更容易，但不能用来使Kinesis、Redshift和S3相互通信，如AWS在[http://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html](http://docs.aws.amazon.com/redshift/latest/dg/copy-usage_notes-access-permissions.html)中指出的：
- en: With role-based access control, your cluster temporarily assumes an IAM role
    on your behalf. Then, based on the authorizations granted to the role, your cluster
    can access the required AWS resources. An IAM role is similar to an IAM user,
    in that it is an AWS identity with permission policies that determine what the
    identity can and cannot do in AWS. However, instead of being uniquely associated
    with one user, a role can be assumed by any entity that needs it. Also, a role
    doesn’t have any credentials (a password or access keys) associated with it. Instead,
    if a role is associated with a cluster, access keys are created dynamically and
    provided to the cluster. We recommend using role-based access control because
    it provides more secure, fine-grained control of access to AWS resources and sensitive
    user data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基于角色的访问控制允许您的集群代表您临时假定一个IAM角色。然后，根据授予该角色的授权，您的集群可以访问所需的AWS资源。IAM角色与IAM用户类似，因为它是一个AWS身份，具有权限策略，该策略确定身份可以在AWS中做什么和不能做什么。然而，与唯一关联于一个用户不同，任何需要它的实体都可以假定一个角色。此外，角色没有与之关联的任何凭证（密码或访问密钥）。相反，如果角色与集群关联，则会动态创建访问密钥并将其提供给集群。我们建议使用基于角色的访问控制，因为它提供了对AWS资源和敏感用户数据的更安全、更细粒度的访问控制。
- en: We must create the right role for your user to be able to access Redshift, and
    then we must give it the necessary policies.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为您的用户创建正确的角色，以便他们能够访问Redshift，然后我们必须给它必要的策略。
- en: 'There are three steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个步骤：
- en: First, authorize Amazon Redshift to access other AWS services on your behalf.
    Follow the instructions at: [http://docs.aws.amazon.com/redshift/latest/mgmt/authorizing-redshift-service.html](http://docs.aws.amazon.com/redshift/latest/mgmt/authorizing-redshift-service.html)
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步，授权Amazon Redshift代表你访问其他AWS服务。请按照以下说明操作：[http://docs.aws.amazon.com/redshift/latest/mgmt/authorizing-redshift-service.html](http://docs.aws.amazon.com/redshift/latest/mgmt/authorizing-redshift-service.html)
- en: Second, attach the role in clustering. Take a look at [http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html](http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html).
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步，将角色附加到聚类中。查看[http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html](http://docs.aws.amazon.com/redshift/latest/mgmt/copy-unload-iam-role.html)。
- en: 'Finally, using the console to manage IAM role associations, perform the following
    steps:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用控制台管理IAM角色关联，执行以下步骤：
- en: Go to Redshift, and click on Manage IAM Roles.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往Redshift，然后点击管理IAM角色。
- en: Select from the available roles.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从可用角色中选择。
- en: Wait for the status to go from Modifying to Available.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待状态从修改中变为可用。
- en: Roles and policies in AWS, when trying to have different services connect with
    one another, can be challenging and time consuming. There is an obvious need for
    strict access control for production-level applications and services, but the
    lack of a more relaxed or loose generalized access level to allow for proof of
    concepts and pet projects is definitely missing from the AWS platform. The general
    hacking idea when facing role-related access problems is to go to the IAM Role
    page and attach the policies you think are necessary to the role giving you trouble.
    Trial and error will get you there in the end.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS中，当尝试让不同的服务相互连接时，角色和策略可能会具有挑战性和耗时。对于生产级应用程序和服务，显然需要严格的访问控制，但AWS平台缺乏一个更宽松或松散的通用访问级别，以允许进行概念验证和宠物项目。面对与角色相关的访问问题时的一般黑客思路是，前往IAM角色页面，并将你认为必要的策略附加到给你带来麻烦的角色上。通过试错，你最终会找到解决方案。
- en: 'If all goes well, when you run the producer script, you should see the following
    happening:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，当你运行生产者脚本时，你应该看到以下情况发生：
- en: Files and date-based subfolders are created in the `{bucket}/veggies` folder
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件和基于日期的子文件夹将创建在`{bucket}/veggies`文件夹中
- en: Graphs and queries should show up or be updated in the Redshift cluster page
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图表和查询应在Redshift集群页面上显示或更新
- en: On the Firehose delivery stream page, check the S3 Logs and Redshift Logs tabs
    for error messages
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在“Firehose 传输流页面”上，检查S3日志和Redshift日志标签以查找错误信息
- en: Your `vegetablesdb.tweets` should start filling up with rows of content.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的`vegetablesdb.tweets`应该开始填充内容行。
- en: If that's not the case and you are not seeing tweets in your database, it's
    time to start debugging.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果情况并非如此，并且你未在数据库中看到推文，那么是时候开始调试了。
- en: Dependencies and debugging
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖关系和调试
- en: Connecting up the different services -- Firehose, Redshift, S3 is not a straightforward
    task if you're not a seasoned AWS user. Many details need to be ironed out, and
    the documentation is not always clear, and sometimes, too complex. Many errors
    can also happen in a hidden fashion, and it's not always obvious where the error
    is happening and how to detect it, let alone make sense of it. Of all the bugs
    and problems we had to solve, these were the most time-consuming ones.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不是经验丰富的AWS用户，连接不同的服务——Firehose、Redshift、S3并不是一项简单任务。许多细节需要解决，而且文档并不总是清晰，有时甚至过于复杂。许多错误也可能以隐蔽的方式发生，而且错误发生的位置和如何检测它并不总是明显，更不用说理解它了。在我们必须解决的所有的错误和问题中，这些是最耗时的。
- en: Data format synchronization
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据格式同步
- en: 'If you send some data to Redshift, it needs to be parsed by Redshift. You are
    sending a string formatted as `id, username`, `sentiment, tweet` or a JSON string
    `{id: ''id''`, `username:''twetterin_chief''`, `sentiment: ''0.12''`, `tweet:''Hello
    world it''s a beautiful day''}`. You need to make sure that the Redshift configuration
    in Kinesis follows the format of your data. You do so in the Kinesis-Redshift
    configuration screen with the two following fields:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你向Redshift发送一些数据，它需要被Redshift解析。你发送的字符串格式为`id, username`，`sentiment, tweet`或JSON字符串`{id:
    ''id''`, `username:''twetterin_chief''`, `sentiment: ''0.12''`, `tweet:''Hello
    world it''s a beautiful day''}`。你需要确保Kinesis中的Redshift配置遵循你的数据格式。你可以在Kinesis-Redshift配置屏幕上的以下两个字段中这样做：'
- en: The Redshift table columns
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift表列
- en: The Redshift COPY options
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift的COPY选项
- en: Debugging
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试
- en: 'When you run your producer, and the data does not end up in the redshift table,
    you should remember that there is a delay. That delay is set when you create the
    Kinesis delivery stream, and is set, by default, to 3,600 seconds. Set it to a
    minimum of 60 seconds if you want to avoid long waits. These are the places to
    check when your data is not streaming in your database:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行你的生产者，数据最终没有出现在Redshift表中时，你应该记住有一个延迟。这个延迟是在你创建Kinesis传输流时设置的，默认设置为3,600秒。如果你想避免长时间的等待，请将其设置为至少60秒。以下是在你的数据没有在数据库中流式传输时需要检查的地方：
- en: '**Check S3**: The S3 prefix corresponds to a folder in the bucket you have
    defined. If there are errors, you will see a new subfolder called `errors` or
    `processing errors`. Click through the subfolders until you reach the actual error
    file, make it public (there''s a button), download the file, and examine it. It
    will sometimes contain useful information. The error subfolder also contains a
    manifest file. The manifest file is useful to reprocess failed files.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查S3**：S3前缀对应于您定义的存储桶中的一个文件夹。如果有错误，您将看到一个名为`errors`或`processing errors`的新子文件夹。点击子文件夹直到到达实际的错误文件，将其公开（有一个按钮），下载文件，并检查它。它有时会包含有用的信息。错误子文件夹还包含一个清单文件。清单文件对于重新处理失败的文件很有用。'
- en: Connect to your Redshift database, and check the `STL_LOAD_ERRORS` table with
    `select * from STL_LOAD_ERRORS`. If the problem is caused by an SQL-based error
    (probably parsing related), useful information will show up there. The error messages
    are not always explanatory though. In our case, that table was showing the first
    tweet Redshift failed to ingest, which helped a lot in figuring out what was wrong.
    In the end, the problem we were facing was that some characters were taken as
    column delimiters by Redshift. We removed these characters from the tweets directly
    in the producer.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到您的Redshift数据库，并使用`select * from STL_LOAD_ERRORS`检查`STL_LOAD_ERRORS`表。如果问题是由基于SQL的错误（可能是解析相关）引起的，那么有用的信息将显示在那里。不过，错误消息并不总是解释性的。在我们的情况下，那个表显示了Redshift未能摄取的第一条推文，这极大地帮助了我们找出问题所在。最后，我们面临的问题是某些字符被Redshift当作列分隔符。我们在生产者直接从推文中移除了这些字符。
- en: Check the **Redshift queries** page, where you will see the latest queries.
    If you see that the queries have been terminated instead of completed, you have
    an SQL-query-related problem.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查**Redshift查询**页面，您将看到最新的查询。如果您看到查询被终止而不是完成，那么您有一个与SQL查询相关的问题。
- en: In the end, a good debugging method is to connect to your database and run the
    COPY query shown in the Kinesis delivery stream recap page without forgetting
    to replace the account ID and the role name with the right values. This will mimic
    how Redshift is actually trying to ingest the data from the S3 buckets. If it
    fails, the related errors will bring you more information.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，一个好的调试方法是连接到您的数据库，并运行在Kinesis传输流摘要页面中显示的COPY查询，同时别忘了用正确的值替换账户ID和角色名称。这将模拟Redshift实际从S3存储桶中摄取数据的方式。如果失败，相关的错误会为您提供更多信息。
- en: Preprocessing with Lambda
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Lambda进行预处理
- en: 'We would now like to send the tweets for sentiment classification to our Amazon
    ML model. In order to do that, we will enable the data transformation available
    in the Kinesis Firehose delivery stream page and use a Lambda function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在希望将推文用于情感分类发送到我们的Amazon ML模型。为了做到这一点，我们将启用Kinesis Firehose传输流页面中可用的数据转换，并使用一个Lambda函数：
- en: '![](img/B05028_10_04.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_10_04.png)'
- en: AWS Lambda is a data-processing service that allows you to run scripts (in a
    variety of languages, including Python 2.7, but not Python 3). It is used in conjunction
    with other services, such as Kinesis, as a data processing add-on. You can divert
    your data stream, send it to Lambda for processing, and, if you wish, have the
    result sent back to the initial stream. You can also use Lambda to call other
    services, such as sending alerts or using other storage services.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda是一种数据处理服务，允许您运行脚本（包括Python 2.7等多种语言，但不包括Python 3）。它与其他服务（如Kinesis）一起用作数据处理附加组件。您可以将数据流重定向到Lambda进行处理，如果需要，可以将结果发送回初始流。您还可以使用Lambda调用其他服务，例如发送警报或使用其他存储服务。
- en: 'The main default of AWS Lambda is that the choice of packages you can import
    into your Python script is limited. Trying to import packages, such as `scikit-learn`,
    NLTK, or for that matter, any package not already available, is rather complex.
    For a guide for how to use `scikit-learn` on AWS Lambda, go to  [https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/](https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/)
    or [https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/](https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/).
    This significantly restricts what can be done out of the box with Lambda. Our
    use of AWS Lambda is much simpler. We will use AWS Lambda to do the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda的主要默认设置是，你可以导入Python脚本中的包的选择是有限的。尝试导入包，如`scikit-learn`、NLTK，或者任何尚未提供的包，相当复杂。有关如何在AWS
    Lambda上使用`scikit-learn`的指南，请访问[https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/](https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/)或[https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/](https://serverlesscode.com/post/deploy-scikitlearn-on-lamba/)。这显著限制了Lambda的即插即用功能。我们使用AWS
    Lambda的方式要简单得多。我们将使用AWS Lambda来完成以下工作：
- en: Catch the data from Kinesis Firehose.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Kinesis Firehose捕获数据。
- en: Parse the data and extract the tweet.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析数据并提取推文。
- en: Send the data to Amazon ML real time end point.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据发送到Amazon ML实时端点。
- en: Extract the classification score from the response.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从响应中提取分类分数。
- en: Send back the data along with the classification score to the Kinesis Firehose
    delivery stream.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据连同分类分数一起发送回Kinesis Firehose传输流。
- en: 'Go to AWS Lambda, and click on Create Lambda Function.  Then perform the following
    steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前往AWS Lambda，点击创建Lambda函数。然后执行以下步骤：
- en: Select the Blank Function blueprint and the Python 2.7 runtime.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Blank Function蓝图和Python 2.7运行时。
- en: Do not configure a trigger.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要配置触发器。
- en: Fill in the Name as `vegetablesLambda`, and select Python 2.7 Runtime.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写名称为`vegetablesLambda`，并选择Python 2.7运行时。
- en: 'Finally, paste the following code in the inline editor:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将以下代码粘贴到内联编辑器中：
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `lambda_handler` function is triggered automatically by the Kinesis Firehose.
    It catches and parses the message (aka the payload) `event[''records'']`, extracts
    the tweets, and calls a `get_sentiment()` function that returns a sentiment score
    and a sentiment label. Finally, it adds the sentiment numbers back to the record, rebuilds
    the payload, and sends it back to Kinesis. The `get_sentiment()` function sends
    the tweet to our Amazon classification endpoint and returns the  `predicted_label`,
    and `predicted_score`.  It is defined in the following script:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda_handler`函数由Kinesis Firehose自动触发。它捕获并解析消息（即负载）`event[''records'']`，提取推文，并调用返回情感分数和情感标签的`get_sentiment()`函数。最后，它将情感数字添加回记录，重建负载，并将其发送回Kinesis。`get_sentiment()`函数将推文发送到我们的Amazon分类端点，并返回`predicted_label`和`predicted_score`。该函数定义在以下脚本中：'
- en: '[PRE12]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since we added two new elements to the payload, we also need to add them to
    the Redshift table and to the Kinesis-Redshift configuration. To recreate the
    `tweets` table in Redshift, run the following query:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在负载中添加了两个新元素，我们还需要将它们添加到Redshift表和Kinesis-Redshift配置中。为了在Redshift中重新创建`tweets`表，请运行以下查询：
- en: '[PRE13]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: At the Kinesis level, change the Redshift table columns field to `id,screen_name`,
    `veggie,ml_label`, `ml_score, text`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kinesis级别，将Redshift表列字段改为`id,screen_name`, `veggie,ml_label`, `ml_score, text`。
- en: Analyzing the results
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析结果
- en: 'We now have a complete pipeline of streaming data that is caught, transformed,
    and stored. Once you have collected a few thousand tweets, you are ready to analyze
    your data. There are a couple of things that remain to be done before we can get
    the answers we set out to find at the beginning of this chapter:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个完整的流数据管道，它能够捕获、转换和存储数据。一旦收集了几千条推文，你就可以开始分析你的数据了。在我们能够找到本章开头设定的答案之前，还有一些事情需要完成：
- en: What are the most popular vegetables on twitter?
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推特上最受欢迎的蔬菜是什么？
- en: How does `TextBlob` compare to our Amazon ML classification model?
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TextBlob`与我们的Amazon ML分类模型相比如何？'
- en: Our simple producer does not attempt to handle duplicate tweets. However, in
    the end, our dataset has many duplicate tweets. Broccoli and carrots are less
    frequent Tweet subjects than one could expect them to be. So, as we collect about
    a hundred tweets every 10 minutes, many tweets end up being collected several
    times. We also still need to obtain a sentiment score and related classes from
    `TextBlob`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单的生产者不尝试处理重复的推文。然而，最终，我们的数据集有许多重复的推文。西兰花和胡萝卜作为推文主题的频率比预期的要低。因此，当我们每 10 分钟收集大约一百条推文时，许多推文最终被收集多次。我们还需要从
    `TextBlob` 获取情感得分和相关类别。
- en: We will now download our collected dataset of tweets, remove duplicates, and
    use the `TextBlob` classification.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将下载我们收集的推文数据集，删除重复项，并使用 `TextBlob` 分类。
- en: Download the dataset from RedShift
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 RedShift 下载数据集
- en: 'The right way to download data from Redshift is to connect to the database
    using Psql and use the `Unload` command to dump the results of an SQL query in
    S3\. The following command exports all the tweets to the `s3://aml.packt/data/veggies/results/`
    location using an appropriate role:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Redshift 下载数据的正确方式是使用 Psql 连接到数据库，并使用 `Unload` 命令将 SQL 查询的结果导出到 S3。以下命令使用适当的角色将所有推文导出到
    `s3://aml.packt/data/veggies/results/` 位置：
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then download the files and aggregate them:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以下载文件并将它们汇总：
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `veggie_tweets.csv` file is not comma separated. The values are separated
    by the `|` character. We can replace all the pipes in the file with commas with the
    following command line:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`veggie_tweets.csv` 文件不是以逗号分隔的。值是通过 `|` 字符分隔的。我们可以使用以下命令行将文件中的所有管道符号替换为逗号：'
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We are now ready to load the data into a pandas dataframe:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好将数据加载到 pandas 数据框中：
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we could have also used `|` as a delimiter when loading the pandas
    dataframe with `df = pd.read_csv('data/veggie_tweets.tmp'`, `delimiter = '|'`,
    `header=None`, `names = ['id'`, `'username'`, `'vegetable'`, `'ml_label'`, `'ml_score'`,
    `'text']`).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们也可以在加载 pandas 数据框时使用 `|` 作为分隔符，即 `df = pd.read_csv('data/veggie_tweets.tmp'`,
    `delimiter = '|'`, `header=None`, `names = ['id'`, `'username'`, `'vegetable'`,
    `'ml_label'`, `'ml_score'`, `'text']`）。
- en: Sentiment analysis with TextBlob
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TextBlob 进行情感分析
- en: '`TextBlob` gives you sentiment analysis, scoring, and classification in a couple
    of Python lines. For the given a text, initialize a `TextBlob` instance, and retrieve
    its polarity with these two lines of code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextBlob` 在几行 Python 代码中提供情感分析、评分和分类。对于给定的文本，初始化一个 `TextBlob` 实例，并使用以下两行代码检索其极性：'
- en: '[PRE18]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `TextBlob` sentiment object has a polarity and a subjectivity score. The
    polarity of a text ranges from -1 to +1, negative to positive, and the subjectivity
    from 0 to 1, very objective to very subjective. For instance, the sentence `I
    love brocoli` returns `Sentiment(polarity=0.5`, `subjectivity=0.6)`, while the
    sentence `I hate brocoli` returns a sentiment of `Sentiment(polarity=-0.8`, `subjectivity=0.9)`.
     We can add the `TextBlob` sentiment whenever we process the tweet, either within
    the producer or once we''ve downloaded the dataset with these simple lines:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextBlob` 情感对象有一个极性和主观性得分。文本的极性范围从 -1 到 +1，从负到正，主观性从 0 到 1，从非常客观到非常主观。例如，句子
    `I love brocoli` 返回 `Sentiment(polarity=0.5`, `subjectivity=0.6)`，而句子 `I hate
    brocoli` 返回情感 `Sentiment(polarity=-0.8`, `subjectivity=0.9)`。我们可以在处理推文时添加 `TextBlob`
    情感，无论是在生产者内部还是在下载了这些数据集后，都可以用以下简单行代码实现：'
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Each row of our dataframe now also has a sentiment score.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据框的每一行现在也都有一个情感得分。
- en: Removing duplicate tweets
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除重复推文
- en: In all Twitter-based NLP analysis, you end up dealing with bots, even when collecting
    tweets about vegetables! In our dataset, we had many versions of promotion tweets
    where the text was the same across tweets, but the links and users were different. We
    remove duplicate tweets by first removing the URL from the tweets and then using
    the `drop_duplicates` Pandas method.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有基于 Twitter 的 NLP 分析中，你最终都会处理机器人，即使是在收集关于蔬菜的推文时也是如此！在我们的数据集中，我们有许多版本的宣传推文，其中推文中的文本相同，但链接和用户不同。我们通过首先从推文中删除
    URL，然后使用 `drop_duplicates` Pandas 方法来删除重复推文。
- en: 'Noting that all URLs in Tweets start with `https://t.co/`, it''s easy to remove
    all URLs from the Tweets. We will create a new tweet column without URLs in our
    dataframe. We enter the following line, which, given a tweet, returns the tweet
    without URLs:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到推文中所有的 URL 都以 `https://t.co/` 开头，因此很容易从推文中移除所有 URL。我们将在我们的数据框中创建一个不带 URL
    的新推文列。我们输入以下行，它将返回不带 URL 的推文：
- en: '[PRE20]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When working with pandas dataframes, a very practical way to create new columns
    based on some operation on other columns of the dataframe is to combine the apply
    method with a Lambda function. The overall pattern to create a `new_column` from
    `existing_column` is:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 pandas 数据框时，创建基于数据框其他列操作的新列的一个非常实用的方法是将 apply 方法与 Lambda 函数结合使用。创建从 `existing_column`
    到 `new_column` 的 `new_column` 的整体模式是：
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We apply this pattern to create the `no_urls` column containing the tweets
    with no urls:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用这个模式来创建包含没有网址的推文的 `no_urls` 列：
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `no_urls` columns no longer contain any URLs. We can now remove duplicates
    based on this column with the following line:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`no_urls` 列不再包含任何网址。现在我们可以使用以下行基于此列删除重复项：'
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This removed about 30% of our tweets.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约移除了我们推文的 30%。
- en: And what is the most popular vegetable?
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么最受欢迎的蔬菜是什么呢？
- en: 'It''s interesting to compare the sentiment score distributions between our
    Amazon ML model and `TextBlob`. We can see in the following plot that our Amazon
    ML model is good at separating positive and negative tweets, while `TextBlob`
    has a more centered distribution. In fact, a significant portion of tweets were
    scored as 0 (neutral) by `TextBlob`. We removed them from the histogram:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 比较我们的 Amazon ML 模型和 `TextBlob` 的情感分数分布很有趣。我们可以在下面的图中看到，我们的 Amazon ML 模型擅长区分正面和负面的推文，而
    `TextBlob` 的分布则更加集中。事实上，相当一部分推文被 `TextBlob` 评分为零（中性）。我们从直方图中移除了它们：
- en: '![](img/B05028_10_07.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_10_07.png)'
- en: 'According to our Amazon ML model, the most popular vegetables on Twitter are
    green beans, followed by asparagus and garlic. According to `TextBlob`, cauliflower
    is ranked fourth favorite, followed by leeks and cucumber. The following plot
    shows the 10 vegetables with the larger amount of tweets and their respective
    sentiment scores obtained with `TextBlob` and our own Amazon ML binary classifier:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的 Amazon ML 模型，Twitter 上最受欢迎的蔬菜是绿豆，其次是芦笋和大蒜。根据 `TextBlob`，花椰菜排名第四受欢迎，其次是韭菜和黄瓜。以下图显示了推文量较多的前
    10 种蔬菜以及使用 `TextBlob` 和我们自己的 Amazon ML 二元分类器获得的相应情感分数：
- en: '![](img/B05028_10_08.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_10_08.png)'
- en: The striking result is that green beans are the least popular vegetables according
    to `TextBlob`. It so happens that `TextBlob` gives a negative *-0.2* sentiment
    score to the word `green`. So the phrase `green beans` directly scores *-0.2*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的结果是，根据 `TextBlob`，绿豆是最不受欢迎的蔬菜。碰巧的是，`TextBlob` 给单词 `green` 分配了一个负的 *-0.2*
    情感分数。因此，短语 `green beans` 直接得分 *-0.2*。
- en: Our Amazon ML model seems to be more reliable than `TextBlob`. After all, green
    beans are bound to be more popular than cauliflower!
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 Amazon ML 模型似乎比 `TextBlob` 更可靠。毕竟，绿豆肯定比花椰菜更受欢迎！
- en: Going beyond classification and regression
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越分类和回归
- en: 'Although Amazon ML is set to solve classification and regression problems,
    the service can also be used for other supervised data science problems. In this
    last section, we looked at two classic problems: Recommender systems and named
    entity recognition.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Amazon ML 被设置为解决分类和回归问题，但该服务也可以用于其他监督数据科学问题。在本节的最后，我们探讨了两个经典问题：推荐系统和命名实体识别。
- en: '**Making recommendations**: A recommender system seeks to predict the rating
    or preference that a user would give to an item. There are several strategies
    to build recommender systems:'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制作推荐**：推荐系统试图预测用户会对一个项目给出多少评分或偏好。构建推荐系统有几种策略：'
- en: '**Collaborative filtering**: This involves using the behavioral patterns of
    similar users to predict a given user''s preferences. It''s the other people also
    bought this approach.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤**：这涉及到使用类似用户的的行为模式来预测给定用户的偏好。这是“其他人也买了这个”的方法。'
- en: '**Content-based filtering**: This is the strategy where the features of a certain
    content are used to group similar products or content.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于内容的过滤**：这是一种策略，其中使用特定内容的特征来分组相似的产品或内容。'
- en: 'To use Amazon ML for recommendations, you can frame your solution as a content-based
    recommendation problem. One way to do this is to extract features for your products
    and users and build a training dataset where the outcome is binary: the user either
    liked the product or did not. The recommender system is transformed into a binary
    recommendation problem.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Amazon ML 进行推荐，你可以将你的解决方案构造成一个基于内容的推荐问题。一种方法是提取产品和用户的特征，并构建一个训练数据集，其中结果为二元：用户要么喜欢该产品，要么不喜欢。推荐系统被转换为一个二元推荐问题。
- en: '**Named entity recognition: **Named entity recognition seeks to locate and
    classify entities in the text into predefined categories, such as the names of
    persons, organizations, locations, and so forth. Amazon ML can also be used for
    named entity recognition problems. The idea is to use single words, and extract
    features as training data. Potential features could include the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '**命名实体识别：**命名实体识别旨在在文本中定位和分类实体到预定义的类别中，例如人名、组织、地点等。亚马逊机器学习也可以用于命名实体识别问题。想法是使用单个单词，并提取特征作为训练数据。潜在的特征可能包括以下内容：'
- en: The word itself
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词本身
- en: '`ngram()` or `osb()` of the context around the word, such as the previous and
    subsequent three words.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram()`或`osb()`的上下文，例如前后的三个单词。'
- en: Prefixes and suffixes
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀和后缀
- en: The predicted class of the previous three words
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前三个单词的预测类别
- en: The length of the word
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词的长度
- en: Whether the word is capitalized?
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词是否首字母大写？
- en: Whether the word has a hyphen?
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词是否有连字符？
- en: The first word in the sentence
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子中的第一个单词
- en: The frequency of the word in your dataset
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词在数据集中的频率
- en: Numeric features -- is the word a number?
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字特征--单词是否是数字？
- en: Part of speech of the word or surrounding words
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词或周围单词的词性
- en: Some of these feature extraction methods are available in Amazon ML; others
    will need external processing.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些特征提取方法在亚马逊机器学习中可用；其他则需要外部处理。
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we leveraged the power of the AWS ecosystem to build a real-time
    streaming data classification pipeline. Our pipeline was able to classify streaming
    tweets using an Amazon ML classification endpoint.  The AWS data ecosystem is
    diverse and complex, and for a given problem, there are often several possible
    solutions and architectures. The **Kinesis-Lambda-Redshift-Machine** **Learning**
    architecture we built is simple, yet very powerful.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们利用了AWS生态系统的力量来构建一个实时流数据分类管道。我们的管道能够使用亚马逊机器学习分类端点对实时推文进行分类。AWS数据生态系统多样且复杂，对于给定的问题，通常有几种可能的解决方案和架构。我们构建的**Kinesis-Lambda-Redshift-机器学习**架构简单，但非常强大。
- en: The true strength of the Amazon ML service lies in its ease of use and simplicity.
    Training and evaluating a model from scratch can be done in a few minutes with a
    few clicks, and it can result in very good performances. Using the **AWS CLI**
    and the SDK, more complex data flows and model explorations can easily be implemented.
    The service is agile enough to become a part of a wider data flow by providing
    real-time classification and regression.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习服务的真正优势在于其易用性和简单性。从头开始训练和评估一个模型只需几分钟和几个点击，并且可以产生非常好的性能。使用**AWS CLI**和SDK，可以轻松实现更复杂的数据流和模型探索。该服务足够敏捷，可以通过提供实时分类和回归成为更广泛数据流的一部分。
- en: Underneath the simple interface, the machine learning expertise of Amazon shines
    at many levels. From the automated data transformations, to the tuning of the
    stochastic gradient algorithm, there are many elements that drive the overall
    performance of the models. A good balance between user control over the models
    and automatic optimization is achieved. The user can try several types of regularization
    and data transformations to optimize the models and the feature set, but the overall
    feeling is that using the default parameters would often work as well.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的界面之下，亚马逊的机器学习专业知识在许多层面上都闪耀着光芒。从自动数据转换到随机梯度算法的调整，有许多元素推动了模型的总体性能。在用户对模型的控制与自动优化之间达到了良好的平衡。用户可以尝试几种类型的正则化和数据转换来优化模型和特征集，但整体感觉是使用默认参数通常也能很好地工作。
- en: The service simplicity has some drawbacks, mostly in terms of limited data transformation
    and the absence of any embedded cross-validation mechanisms, which are central
    to a data science workflow.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的简单性有一些缺点，主要是在数据转换有限以及缺乏任何嵌入的交叉验证机制，这对于数据科学工作流程至关重要。
- en: In the end, Amazon ML is a useful regression and classification service that
    brings machine learning automatization closer. What matters in a data science
    project, as in any other software project, is the true costs of ownership. Compared
    to a home-grown solution, Amazon ML wins in terms of ease of use, maintainability,
    resources costs, and often performance.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，亚马逊机器学习是一个有用的回归和分类服务，将机器学习自动化更近一步。在数据科学项目中，就像在其他任何软件项目中一样，重要的是真正的拥有成本。与自建解决方案相比，亚马逊机器学习在易用性、可维护性、资源成本和通常性能方面占优势。
