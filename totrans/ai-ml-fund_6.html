<html><head></head><body>
<div id="_idContainer086" class="Content">
<p class="hidden" data-amznremoved-m8="true" data-amznremoved="mobi7">6</p>
</div>
<div id="_idContainer087" class="Content">
<h1 id="_idParaDest-140"><a id="_idTextAnchor148"></a>
 Clustering</h1>
</div>
<div id="_idContainer088" class="Content">
<h2>Learning Objectives</h2>
<p>By the end of this chapter, you will be able to:</p>
<ul>
<li class="bullets">Summarize the basics of clustering</li>
<li class="bullets">Perform flat clustering with the k-means algorithm</li>
<li class="bullets">Perform hierarchical clustering with the mean shift algorithm</li>
</ul>
<p>In this chapter, you will learn about the fundamentals of clustering, which will be illustrated with two unsupervised learning algorithms.</p>
</div>
<div id="_idContainer096" class="Content">
<h2 id="_idParaDest-141"><a id="_idTextAnchor149"></a>
 Introduction to Clustering</h2>
<p>In the previous chapters, we dealt with supervised learning algorithms to perform classification and regression. We used training data to train our classification or regression model, and then we validated our model using testing data.</p>
<p>In this chapter, we will perform unsupervised learning by using clustering algorithms.</p>
<p>We may use clustering to analyze data to find certain patterns and create groups. Apart from that, clustering can be used for many purposes:</p>
<ul>
<li>Market segmentation detects the best stocks in the market you should be focusing on fundamentally. We can detect trends, segment customers, or recommend certain products to certain customer types using clustering.</li>
<li>In computer vision, image segmentation is performed using clustering, where we find different objects in an image that a computer processes.</li>
<li>Clustering can be combined with classification, where clustering may generate a compact representation of multiple features, which can then be fed to a classifier.</li>
<li>Clustering may also filter data points by detecting outliers.</li>
</ul>
<p>Regardless of whether we are applying clustering to genetics, videos, images, or social networks, if we analyze data using clustering, we may find similarities between data points that are worth treating uniformly.</p>
<p>We perform clustering without specified labels. Clustering defines clusters based on the distance between their data points. While; in classification, we define exact label classes to group classified data points, in clustering, there are no labels. We just give the machine learning model the features, and the model has to figure out the clusters in which those feature sets are grouped.</p>
<h3 id="_idParaDest-142"><a id="_idTextAnchor150"></a>
 Defining the Clustering Problem</h3>
<p class="_idGenParaOverride-1">Suppose you are a store manager who's responsible for ensuring the profitability of your store. Your products are divided into categories. Different customers of the store prefer different items.</p>
<div></div>
<p>For instance, a customer interested in bio products tends to select products that are bio in nature. If you check out Amazon, you will also find suggestions for different groups of products. This is based on what users are likely to be interested in.</p>
<p>We will define the clustering problem in such a way that we will be able to find these similarities between our data points. Suppose we have a dataset that consists of points. Clustering helps us understand this structure by describing how these points are distributed.</p>
<p>Let's look at an example of data points in a two-dimensional space:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer089" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00055.jpg" alt="" />
</div>
</div>
<h6 class="_idGenParaOverride-1">Figure 6.1: Data points in a two-dimensional space</h6>
<div></div>
<p>In this example, it is evident that there are three clusters:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer090" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00056.jpg" alt="" />
</div>
</div>
<h6>Figure 6.2: Three clusters formed using the data points in a two-dimensional space</h6>
<p>The three clusters were easy to detect because the points are close to one another. Clustering determines data points that are close to each other. There are also some outlier points that do not belong to any cluster. The clustering algorithm should be prepared to treat these outlier points properly, without moving them into a cluster.</p>
<p class="_idGenParaOverride-1">While it is easy to recognize clusters in a two-dimensional space, we normally have multidimensional data points. Therefore, it is important to know which data points are close to one other. Also, it is important to define distance metrics that detect whether data points are close to each other. One well-known distance metric is the Euclidean distance. In mathematics, we often use Euclidean distance to measure the distance between two points. Therefore, Euclidean distance is an intuitive choice when it comes to clustering algorithms so that we can determine the proximity of data points when locating clusters.</p>
<div></div>
<p>There is one drawback to most distance metrics, including Euclidean distance: the more we increase the dimensions, the more uniform these distances will become compared to each other. Therefore, getting rid of features that act as noise rather than useful information may greatly increase the accuracy of the clustering model.</p>
<h3 id="_idParaDest-143"><a id="_idTextAnchor151"></a>
 Clustering Approaches</h3>
<p>There are two types of clustering: <strong class="bold _idGenCharOverride-1">flat</strong>
 and <strong class="bold _idGenCharOverride-1">hierarchical</strong>
 .</p>
<p>In flat clustering, we specify the number of clusters we would like the machine to find. One example of flat clustering is the k-means algorithm, where K specifies the number of clusters, we would like the algorithm to use.</p>
<p>In hierarchical clustering, the machine learning algorithm finds out the number of clusters that are needed.</p>
<p>Hierarchical clustering also has two approaches:</p>
<ul>
<li>
<strong class="keyword _idGenCharOverride-1">Bottom-up hierarchical clustering</strong>
 treats each point as a cluster. This approach unites clusters that are close to each other.</li>
<li>
<strong class="keyword _idGenCharOverride-1">Top-down hierarchical clustering</strong>
 treats data points as if they were in one cluster spanning the whole state space. Then, the clustering algorithm splits our clusters into smaller ones.</li>
<li>
<strong class="keyword _idGenCharOverride-1">Point assignment clustering</strong>
 assigns new data points to existing clusters based on how close the new data point is to these clusters.</li>
</ul>
<h3 id="_idParaDest-144"><a id="_idTextAnchor152"></a>
 Clustering Algorithms Supported by scikit-learn</h3>
<p>In this chapter, we will learn about two clustering algorithms supported by scikit-learn: the <strong class="keyword _idGenCharOverride-1">k-means</strong>
 algorithm and the <strong class="keyword _idGenCharOverride-1">mean shift</strong>
 algorithm.</p>
<p>
<strong class="bold _idGenCharOverride-1">k-means</strong>
 is an example of flat clustering, where we have to specify the number of clusters in advance. k-means is a generic purpose clustering algorithm that performs well if the number of clusters is not too high and the size of the clusters is even.</p>
<p class="_idGenParaOverride-1">
<strong class="bold _idGenCharOverride-1">Mean-shift</strong>
 is an example of hierarchical clustering, where the clustering algorithm determines the number of clusters. Mean shift is used when we don't know the number of clusters in advance. In contrast with k-means, mean shift supports use cases where many clusters are present, even if the size of the clusters greatly differs.</p>
<div></div>
<p>scikit-learn provides other clustering algorithms. These are as follows:</p>
<ul>
<li>
<strong class="keyword _idGenCharOverride-1">Affinity Propagation</strong>
 : Performs similarly to Mean Shift</li>
<li>
<strong class="keyword _idGenCharOverride-1">Spectral clustering</strong>
 : Performs better if only a few clusters are present, with even cluster sizes</li>
<li>
<strong class="keyword _idGenCharOverride-1">Ward hierarchical clustering</strong>
 : Used when many clusters are expected</li>
<li>
<strong class="keyword _idGenCharOverride-1">Agglomerative clustering</strong>
 : Used when many clusters are expected</li>
<li>
<strong class="keyword _idGenCharOverride-1">DBSCAN clustering</strong>
 : Supports uneven cluster sizes and non-flat geometry of point distributions</li>
<li>
<strong class="keyword _idGenCharOverride-1">Gaussian mixtures</strong>
 : Uses flat geometry, which is good for density estimations</li>
<li>
<strong class="keyword _idGenCharOverride-1">Birch clustering</strong>
 : Supports large datasets, removes outliers, and supports data reduction</li>
</ul>
<p>For a complete description of clustering algorithms, including performance comparisons, visit the clustering page of scikit-learn at <a href="http://scikit-learn.org/stable/modules/clustering.html">http://scikit-learn.org/stable/modules/clustering.html</a>
 .</p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor153"></a>
 The k-means Algorithm</h2>
<p>The k-means algorithm is a flat clustering algorithm. It works as follows:</p>
<ul>
<li>Set the value of K.</li>
<li>Choose K data points from the dataset that are initial centers of the individual clusters.</li>
<li>Calculate the distance of each data point to the chosen center points, and group each point in the cluster whose initial center is the closest to the data point.</li>
<li>Once all of the points are in one of the K clusters, calculate the center point of each cluster. This center point does not have to be an existing data point in the dataset; it is just an average.</li>
<li class="_idGenParaOverride-1">Repeat this process of assigning each data point into the cluster that has a center closest to the data point. Repetition continues until the center points no longer move.</li>
<li style="list-style: none; display: inline"><div></div>
</li>
</ul>
<p>To make sure that the k-means algorithm terminates, we need the following:</p>
<ul>
<li>A maximum level of tolerance when we exit in case the centroids move less than the tolerance value</li>
<li>A maximum number of repetitions of shifting the moving points</li>
</ul>
<p>Due to the nature of the k-means algorithm, it will have a hard time dealing with clusters that greatly differ in size.</p>
<p>The k-means algorithm has many use cases that are part of our everyday lives:</p>
<p>
<strong class="bold _idGenCharOverride-1">Market segmentation:</strong>
 Companies gather all sorts of data from their customer base. Performing k-means clustering analysis on the customer base of a company reveals market segments that have defined characteristics. Customers belonging to the same segment can be treated similarly. Different segments receive different treatment.</p>
<p>
<strong class="bold _idGenCharOverride-1">Classification of books, movies, or other documents:</strong>
 When influencers build their personal brand, authors write books and create books, or a company manages its social media accounts, content is king. Content is often described by hashtags and other data. This data can be used as a basis for clustering to locate groups of documents that are similar in nature.</p>
<p>
<strong class="bold _idGenCharOverride-1">Detection of fraud and criminal activities:</strong>
 Fraudsters often leaves clues in the form of unusual customer or visitor behavior. For instance, car insurance protects drivers from theft and damage arising from accidents. Real theft and fake theft are characterized by different feature values. Similarly, wrecking a car on purpose leaves different traces than wrecking a car by accident. Clustering can often detect fraud, helping industry professionals understand the behavior of their worst customers better.</p>
<h3 id="_idParaDest-146"><a id="_idTextAnchor154"></a>
 Exercise 19: k-means in scikit-learn</h3>
<p>To plot data points in a two-dimensional plane and execute the k-means algorithm on them to perform clustering, execute the following steps:</p>
<ol>
<li value="1">We will create an artificial dataset as a NumPy Array to demonstrate the k-means algorithm:<p class="snippet _idGenParaOverride-2">import numpy as np</p>
<p class="snippet _idGenParaOverride-2">data_points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    [1, 1],</p>
<p class="snippet _idGenParaOverride-2">    [1, 1.5],</p>
<p class="snippet _idGenParaOverride-2">    [2, 2],</p>
<p class="snippet _idGenParaOverride-2">    [8, 1],</p>
<p class="snippet _idGenParaOverride-2">    [8, 0],</p>
<p class="snippet _idGenParaOverride-2">    [8.5, 1],</p>
<p class="snippet _idGenParaOverride-2">    [6, 1],</p>
<p class="snippet _idGenParaOverride-2">    [1, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 9.5],</p>
<p class="snippet _idGenParaOverride-2">    [10, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 8.5]</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="_idGenParaOverride-3">We can plot these data points in the two-dimensional plane using <strong class="inline _idGenCharOverride-2">matplotlib.pyplot</strong>
 :</p>
<p class="snippet _idGenParaOverride-2">import matplotlib.pyplot as plot</p>
<p class="snippet _idGenParaOverride-2">plot.scatter(data_points.transpose()[0], data_points.transpose()[1])</p>
<p class="_idGenParaOverride-3">The output is as follows:</p>
<div id="_idContainer091" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00057.jpg" alt="" />
</div>
<h6 class="_idGenParaOverride-5">Figure 6.3: Graph showing the data points on a two-dimensional plane using matplotlib.pyplot</h6>
<h4 class="_idGenParaOverride-6">Note</h4>
<p class="callout _idGenParaOverride-7">We used the <strong class="inline _idGenCharOverride-3">transpose array</strong>
 method to get the values of the first feature and the second feature. This is in alignment with the previous chapters. We could also use proper array indexing to access these columns: <strong class="inline _idGenCharOverride-3">dataPoints[:,0]</strong>
 is equivalent to <strong class="inline _idGenCharOverride-3">dataPoints.transpose()[0]</strong>
 .</p>
<div></div>
</li>
<li value="2">Now that we have the data points, it's time to execute the k-means algorithm on them. If we define K as <strong class="inline _idGenCharOverride-2">3</strong>
 in the k-means algorithm, we expect a cluster on the bottom-left, top-left, and bottom-right corner of the graph:<p class="snippet _idGenParaOverride-2">from sklearn.cluster import KMeans</p>
<p class="snippet _idGenParaOverride-2">k_means_model = KMeans(n_clusters=3)</p>
<p class="snippet _idGenParaOverride-2">k_means_model.fit(data_points)</p>
</li>
<li value="3">Once the clustering is done, we can access the center point of each cluster:<p class="snippet _idGenParaOverride-2">k_means_model.cluster_centers_</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[1.33333333, 1.5     ],</p>
<p class="snippet _idGenParaOverride-2">        [3.1     , 9.6     ],</p>
<p class="snippet _idGenParaOverride-2">        [7.625     , 0.75     ]])</p>
<p class="_idGenParaOverride-3">Indeed, the center points of the clusters appear to be in the bottom-left, top-left, and bottom-right corners of the graph. The X-coordinate of the top-left cluster is 3.1, most likely because it contains our outlier data point [10, 10].</p>
</li>
<li value="4">Let's plot the clusters with different colors and their center points. To know which data point belongs to which cluster, we have to query the <strong class="inline _idGenCharOverride-2">labels_</strong>
 property of the k-means classifier:<p class="snippet _idGenParaOverride-2">k_means_model.labels_</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2">array([0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1])</p>
</li>
<li value="5">The output array shows which data point belongs to which cluster. This is all we need to plot the data:<p class="snippet _idGenParaOverride-2">plot.scatter(</p>
<p class="snippet _idGenParaOverride-2">    k_means_model.cluster_centers_[:,0],</p>
<p class="snippet _idGenParaOverride-2">    k_means_model.cluster_centers_[:,1]</p>
<p class="snippet _idGenParaOverride-2">)</p>
<p class="snippet _idGenParaOverride-2">for i in range(len(data_points)):</p>
<p class="snippet _idGenParaOverride-2">    plot.plot(</p>
<p class="snippet _idGenParaOverride-2">        data_points[i][0],</p>
<p class="snippet _idGenParaOverride-2">        data_points[i][1],</p>
<p class="snippet _idGenParaOverride-2">        ['ro','go','yo'][k_means_model.labels_[i]]</p>
<p class="snippet _idGenParaOverride-2">    )</p>
<p class="snippet _idGenParaOverride-8">plot.show()</p>
<div></div>
<p class="_idGenParaOverride-3">The output is as follows:</p>
<div id="_idContainer092" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00058.jpg" alt="" />
</div>
<h6 class="_idGenParaOverride-5">Figure 6.4: Graph showing the data points in red, green, and blue while selecting three clusters</h6>
<p class="_idGenParaOverride-3">The blue center points are indeed inside their clusters, which are represented by the red points, the green points, and the yellow points.</p>
</li>
<li value="6">Let's see what happens if we choose only two clusters instead of three:<p class="snippet _idGenParaOverride-2">k_means_model = KMeans(n_clusters=2)</p>
<p class="snippet _idGenParaOverride-2">k_means_model.fit(data_points)</p>
<p class="snippet _idGenParaOverride-2">plot.scatter(</p>
<p class="snippet _idGenParaOverride-2">    k_means_model.cluster_centers_[:,0],</p>
<p class="snippet _idGenParaOverride-2">    k_means_model.cluster_centers_[:,1]</p>
<p class="snippet _idGenParaOverride-2">)</p>
<p class="snippet _idGenParaOverride-2">for i in range(len(data_points)):</p>
<p class="snippet _idGenParaOverride-2">    plot.plot(</p>
<p class="snippet _idGenParaOverride-2">        data_points[i][0],</p>
<p class="snippet _idGenParaOverride-2">        data_points[i][1],</p>
<p class="snippet _idGenParaOverride-2">        ['ro','go'][k_means_model.labels_[i]]</p>
<p class="snippet _idGenParaOverride-2">    )</p>
<p class="snippet _idGenParaOverride-8">plot.show()</p>
<div></div>
<p class="_idGenParaOverride-3">The output is as follows:</p>
<div id="_idContainer093" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00059.jpg" alt="" />
</div>
<h6 class="_idGenParaOverride-5">Figure 6.5: Graph showing the datapoints in red, blue, and green while selecting two clusters</h6>
<p class="_idGenParaOverride-3">This time, we only have red and green points, and we have a bottom cluster and a top cluster. Interestingly, the top red cluster in the second example contains the same points as the top cluster in the first example. The bottom cluster of the second example consists of the data points joining the bottom-left and the bottom-right clusters of the first example.</p>
</li>
<li value="7">We can also use the k-means model for prediction. The output is an array containing the cluster numbers belonging to each data point:<p class="snippet _idGenParaOverride-2">k_means_model.predict([[5,5],[0,10]])</p>
<p class="_idGenParaOverride-3">The output is as follows:</p>
<p class="snippet _idGenParaOverride-8"> array([1, 0])</p>
<div></div>
</li>
</ol>
<h3 id="_idParaDest-147"><a id="_idTextAnchor155"></a>
 Parameterization of the k-means Algorithm in scikit-learn</h3>
<p>Like the classification and regression models in Chapters 3, 4, and 5, the k-means algorithm can also be parameterized. The complete list of parameters can be found at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a>
 .</p>
<p>Some examples are as follows:</p>
<ul>
<li>
<strong class="inline _idGenCharOverride-2">n_clusters</strong>
 : The number of clusters in which the data points are separated. The default value is <strong class="bold _idGenCharOverride-1">8</strong>
 .</li>
<li>
<strong class="inline _idGenCharOverride-2">max_iter</strong>
 : The maximum number of iterations.</li>
<li>
<strong class="inline _idGenCharOverride-2">tol</strong>
 : The tolerance for checking whether we can exit the k-means algorithm.</li>
</ul>
<p>In the previous section, we used two attributes to retrieve the cluster center points and the clusters themselves:</p>
<p>
<strong class="inline _idGenCharOverride-2">cluster_centers_</strong>
 : This returns the coordinates of the cluster center points.</p>
<p>
<strong class="inline _idGenCharOverride-2">labels_</strong>
 : This returns an array of integers symbolizing the number of clusters the data point belongs to. Numbering starts from zero.</p>
<h3 id="_idParaDest-148"><a id="_idTextAnchor156"></a>
 Exercise 20: Retrieving the Center Points and the Labels</h3>
<p>To understand the usage of <strong class="inline _idGenCharOverride-2">cluster_centers_</strong>
 and <strong class="inline _idGenCharOverride-2">labels_</strong>
 , perform the following steps:</p>
<ol>
<li class="ParaOverride-1" value="1">Recall the example that we had from executing the k-means algorithm in scikit-learn. We had 12 data points and three clusters:<p class="snippet _idGenParaOverride-2">data_points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    [1, 1],</p>
<p class="snippet _idGenParaOverride-2">    [1, 1.5],</p>
<p class="snippet _idGenParaOverride-2">    [2, 2],</p>
<p class="snippet _idGenParaOverride-2">    [8, 1],</p>
<p class="snippet _idGenParaOverride-2">    [8, 0],</p>
<p class="snippet _idGenParaOverride-2">    [8.5, 1],</p>
<p class="snippet _idGenParaOverride-2">    [6, 1],</p>
<p class="snippet _idGenParaOverride-2">    [1, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 9.5],</p>
<p class="snippet _idGenParaOverride-2">    [10, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 8.5]</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-8">k_means_model.cluster_centers_</p>
<div></div>
<p class="_idGenParaOverride-3">The output is as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[1.33333333, 1.5     ],</p>
<p class="snippet _idGenParaOverride-2">      [3.1     , 9.6     ],</p>
<p class="snippet _idGenParaOverride-2">       [7.625     , 0.75     ]])</p>
</li>
<li value="2">Apply the <strong class="bold _idGenCharOverride-1">labels_</strong>
 property on the cluster:<p class="snippet _idGenParaOverride-2">k_means_model.labels_</p>
<p class="_idGenParaOverride-3">The output is as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1])</p>
<p class="_idGenParaOverride-3">The output of the <strong class="bold _idGenCharOverride-1">cluster_centers_</strong>
 property is obvious: it shows the X and Y coordinates of the center points. The <strong class="bold _idGenCharOverride-1">labels_</strong>
 property is an array of length 12, showing the cluster of each of the 12 data points it belongs to. The first cluster is associated with the number 0, the second is associated with 1, the third is associated with 2, and so on.</p>
</li>
</ol>
<h3 id="_idParaDest-149"><a id="_idTextAnchor157"></a>
 k-means Clustering of Sales Data</h3>
<p>In the upcoming activity, we will be considering sales data and we will perform k-means clustering on that sales data.</p>
<h3 id="_idParaDest-150"><a id="_idTextAnchor158"></a>
 Activity 12: k-means Clustering of Sales Data</h3>
<p>In th<a id="_idTextAnchor159"></a>
 is section, we will detect product sales that perform similarly to recognize trends in product sales.</p>
<p>We will be using the Sales Transactions Weekly Dataset, found at the following URL:</p>
<p class="_idGenParaOverride-1">
<a href="https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly">https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly</a>
 Perform clustering on the dataset using the k-means algorithm. Make sure that you prepare your data for clustering based on what you have learned in the previous chapters.</p>
<div></div>
<p>Use the default settings for the k-means algorithm:</p>
<ol>
<li class="ParaOverride-1" value="1">Load the dataset using pandas. If you examine the data in the CSV file, you will realize that the first column contains product ID strings. These values just add noise to the clustering process. Also, notice that for weeks 0 to 51, there is a W-prefixed label and a normalized label. Using the normalized label makes more sense so that we can drop the regular weekly labels from the dataset.Create a k-means clustering model and fit the data points into 8 clusters.Retrieve the center points and the labels from the clustering algorithm.</li>
<li value="2">The labels belonging to each data point can be retrieved using the <strong class="bold _idGenCharOverride-1">labels_</strong>
 property. These labels determine the clustering of the rows of the original data frame. How are these labels beneficial?</li>
</ol>
<p>Suppose that, in the original data frame, the product names are given. You can easily recognize the fact that similar types of products sell similarly. There are also products that fluctuate a lot, and products that are seasonal in nature. For instance, if some products promote fat loss and getting into shape, they tend to sell during the first half of the year, before the beach season.</p>
<h4>Note</h4>
<p class="callout">The solution to this activity is available at page 291.</p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor160"></a>
 Mean Shift Algorithm</h2>
<p>Mean shift is a hierarchical clustering algorithm. Unlike the k-means algorithm, in mean shift, the clustering algorithm determines how many clusters are needed, and also performs the clustering. This is advantageous because we rarely know how many clusters we are looking for.</p>
<p>This algorithm also has many use cases in our everyday lives. For instance, the Xbox Kinect device detects human body parts using the mean shift algorithm. Some mobile phones also use the Mean Shift algorithm to detect faces. With the growth of social media platforms, image segmentation is a feature that many users have gotten used to. As image segmentation is also a basis of computer vision, some applications can be found there. The mean shift algorithm may also save lives, as it is built into the car detection software of many modern cars. Imagine that someone emergency brakes in front of you. The image segmentation software of your car detects that the car in front of you is getting alarmingly close to you and applies the emergency brake before you even realize the emergency situation. These driver aids are widespread in modern cars. Self-driving cars are just one step away.</p>
<h3 id="_idParaDest-152"><a id="_idTextAnchor161"></a>
 Exercise 21: Illustrating Mean Shift in 2D</h3>
<p>To learn clustering by using the mean shift algorithm, execute the following steps:</p>
<ol>
<li class="ParaOverride-1" value="1">Let's recall the data points from the previous topic:<p class="snippet _idGenParaOverride-2">data_points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    [1, 1],</p>
<p class="snippet _idGenParaOverride-2">    [1, 1.5],</p>
<p class="snippet _idGenParaOverride-2">    [2, 2],</p>
<p class="snippet _idGenParaOverride-2">    [8, 1],</p>
<p class="snippet _idGenParaOverride-2">    [8, 0],</p>
<p class="snippet _idGenParaOverride-2">    [8.5, 1],</p>
<p class="snippet _idGenParaOverride-2">    [6, 1],</p>
<p class="snippet _idGenParaOverride-2">    [1, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 9.5],</p>
<p class="snippet _idGenParaOverride-2">    [10, 10],</p>
<p class="snippet _idGenParaOverride-2">    [1.5, 8.5]</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-2">r = 2;</p>
</li>
<li value="2">Our task now is to find a point P (x, y), for which the number of data points within a radius R from point P is maximized. The points are distributed as follows:<div id="_idContainer094" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00060.jpg" alt="" />
</div>
<h6 class="_idGenParaOverride-9">Figure 6.6: Graph showing the data points from the data_points array</h6>
<div></div>
</li>
<li value="3">Suppose we initially equate point P to the first data point, [1, 1]:<p class="snippet _idGenParaOverride-2">P = [1, 1]</p>
</li>
<li value="4">Let's find the points that are within a distance of R from this point:<p class="snippet _idGenParaOverride-2">from scipy.spatial import distance</p>
<p class="snippet _idGenParaOverride-2">points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    p0 for p0 in data_points if distance.euclidean(p0, P) &lt;= r</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-2">points</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[1. , 1. ],</p>
<p class="snippet _idGenParaOverride-2">   [1. , 1.5],</p>
<p class="snippet _idGenParaOverride-2">   [2. , 2. ]])</p>
</li>
<li value="5">Let's calculate the mean of the data points:<p class="snippet _idGenParaOverride-2">import numpy as np</p>
<p class="snippet _idGenParaOverride-2">P = [</p>
<p class="snippet _idGenParaOverride-2">    np.mean( points.transpose()[0] ),</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[1] )</p>
<p class="snippet _idGenParaOverride-2">]</p>
<p class="snippet _idGenParaOverride-2">P</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> [1.3333333333333333, 1.5]</p>
</li>
<li value="6">Now that the new mean has been calculated, we can retrieve the points within the given radius again:<p class="snippet _idGenParaOverride-2">points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    p0 for p0 in data_points if distance.euclidean( p0, P) &lt;= r</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-2">points</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[1. , 1. ],</p>
<p class="snippet _idGenParaOverride-2">   [1. , 1.5],</p>
<p class="snippet _idGenParaOverride-8">   [2. , 2. ]])</p>
<div></div>
</li>
<li value="7">These are the same three points, so we can stop here. Three points have been found around the mean of <strong class="inline _idGenCharOverride-2">[1.3333333333333333, 1.5]</strong>
 . The points around this center within a radius of 2 form a cluster.</li>
<li value="8">If we examined the data points [1, 1.5] and [2, 2], we would get the same result. Let's continue with the fourth point in our list, [8, 1]:<p class="snippet _idGenParaOverride-2">P = [8, 1]</p>
<p class="snippet _idGenParaOverride-2">points = np.array( [</p>
<p class="snippet _idGenParaOverride-2">    p0 for p0 in data_points if distance.euclidean(p0, P) &lt;= r</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-2">points</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[8. , 1. ],</p>
<p class="snippet _idGenParaOverride-2">        [8. , 0. ],</p>
<p class="snippet _idGenParaOverride-2">        [8.5, 1. ],</p>
<p class="snippet _idGenParaOverride-2">        [6. , 1. ]])</p>
</li>
<li value="9">This time, all four points in the area were found. Therefore, we can simply calculate their mean:<p class="snippet _idGenParaOverride-2">P = [</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[0]),</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[1])</p>
<p class="snippet _idGenParaOverride-2">]</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> [7.625, 0.75]</p>
<p class="_idGenParaOverride-3">This mean will not change, as in the next iteration, we will find the same data points.</p>
</li>
<li value="10">Notice that we got lucky with the selection of the point [8, 1]. If we started with <strong class="inline _idGenCharOverride-2">P = [8, 0]</strong>
 or <strong class="inline _idGenCharOverride-2">P = [8.5, 1]</strong>
 , we would only find three points instead of four:<p class="snippet _idGenParaOverride-2">P = [8, 0]</p>
<p class="snippet _idGenParaOverride-2">points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    p0 for p0 in data_points if distance.euclidean(p0, P) &lt;= r</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-8">points</p>
<div></div>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[8. , 1. ],</p>
<p class="snippet _idGenParaOverride-2">   [8. , 0. ],</p>
<p class="snippet _idGenParaOverride-2">   [8.5, 1. ]])</p>
</li>
<li value="11">After calculating the mean of these three points, we would have to rerun the distance calculation with the shifted mean:<p class="snippet _idGenParaOverride-2">P = [</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[0]),</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[1])</p>
<p class="snippet _idGenParaOverride-2">]</p>
<p class="snippet _idGenParaOverride-2">P</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> [8.166666666666666, 0.6666666666666666]</p>
</li>
<li value="12">The output for point P = [8.5, 1] is the following array:<p class="snippet _idGenParaOverride-2"> array([[8. , 1. ],</p>
<p class="snippet _idGenParaOverride-2">   [8. , 0. ],</p>
<p class="snippet _idGenParaOverride-2">   [8.5, 1. ]])</p>
<p class="_idGenParaOverride-3">We only found the same three points again. This means that starting from [8,1], we got a larger cluster than starting from [8, 0] or [8.5, 1]. Therefore, we have to take the center point that contains the maximum number of data points.</p>
</li>
<li value="13">Now, let's examine what would happen if we started the discovery from the fourth data point, <strong class="bold _idGenCharOverride-1">[6, 1]</strong>
 :<p class="snippet _idGenParaOverride-2">P = [6, 1]</p>
<p class="snippet _idGenParaOverride-2">points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    p0 for p0 in data_points if distance.euclidean(p0, P) &lt;= r</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-2">points</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[8., 1.],</p>
<p class="snippet _idGenParaOverride-8">  [6., 1.]])</p>
<div></div>
</li>
<li value="14">We successfully found the data point [8, 1]. Therefore, we have to shift the mean from [6, 1] to the calculated new mean, [7, 1]:<p class="snippet _idGenParaOverride-2">P = [</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[0]),</p>
<p class="snippet _idGenParaOverride-2">    np.mean(points.transpose()[1])</p>
<p class="snippet _idGenParaOverride-2">]</p>
<p class="snippet _idGenParaOverride-2">P</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> [7.0, 1.0]</p>
</li>
<li value="15">Let's check if we found more points:<p class="snippet _idGenParaOverride-2">points = np.array([</p>
<p class="snippet _idGenParaOverride-2">    p0 for p0 in data_points if distance.euclidean(p0, P) &lt;= r</p>
<p class="snippet _idGenParaOverride-2">])</p>
<p class="snippet _idGenParaOverride-2">points</p>
<p class="_idGenParaOverride-3">The output will be as follows:</p>
<p class="snippet _idGenParaOverride-2"> array([[8. , 1. ],</p>
<p class="snippet _idGenParaOverride-2">   [8. , 0. ],</p>
<p class="snippet _idGenParaOverride-2">   [8.5, 1. ],</p>
<p class="snippet _idGenParaOverride-2">   [6. , 1. ]])</p>
<p class="_idGenParaOverride-3">Yes â€“ we successfully found all four points! Therefore, we have successfully defined a cluster of size 4. The mean will be the same as before: <strong class="inline _idGenCharOverride-2">[7.625, 0.75]</strong>
 .</p>
<p class="_idGenParaOverride-3">This was a simple clustering example that applied the mean shift algorithm. We only provided an illustration of what the algorithm considers to find the clusters. There is one remaining question, though: the value of the radius.</p>
<p class="_idGenParaOverride-10">Note that if the radius of 2 was not set, we could simply start either with a huge radius including all data points and then reduce the radius or start with a very small radius, making sure that each data point is in its own cluster, and then increase the radius until we get the desired result.</p>
<div></div>
</li>
</ol>
<h3 id="_idParaDest-153"><a id="_idTextAnchor162"></a>
 Mean Shift Algorithm in scikit-learn</h3>
<p>Let's use the same data points as in the k-means algorithm:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">data_points = np.array([</p>
<p class="snippet">    [1, 1],</p>
<p class="snippet">    [1, 1.5],</p>
<p class="snippet">    [2, 2],</p>
<p class="snippet">    [8, 1],</p>
<p class="snippet">    [8, 0],</p>
<p class="snippet">    [8.5, 1],</p>
<p class="snippet">    [6, 1],</p>
<p class="snippet">    [1, 10],</p>
<p class="snippet">    [1.5, 10],</p>
<p class="snippet">    [1.5, 9.5],</p>
<p class="snippet">    [10, 10],</p>
<p class="snippet">    [1.5, 8.5]</p>
<p class="snippet">])</p>
<p>The syntax of the mean shift clustering algorithm is similar to the k-means clustering algorithm.</p>
<p class="snippet">from sklearn.cluster import MeanShift</p>
<p class="snippet">mean_shift_model = MeanShift()</p>
<p class="snippet">mean_shift_model.fit(data_points)</p>
<p>Once clustering is done, we can access the center point of each cluster:</p>
<p class="snippet _idGenParaOverride-1">mean_shift_model.cluster_centers_</p>
<div></div>
<p>The output will be as follows:</p>
<p class="snippet">array([[ 1.375     , 9.5     ],</p>
<p class="snippet">       [ 1.33333333, 1.5     ],</p>
<p class="snippet">       [ 8.16666667, 0.66666667],</p>
<p class="snippet">       [ 6.        , 1.        ],</p>
<p class="snippet">       [10.        , 10.        ]])</p>
<p>The Mean Shift model found 5 clusters with the centers shown in the preceding code.</p>
<p>Similar to k-means,, we can also get the labels:</p>
<p class="snippet">mean_shift_model.labels_</p>
<p>The output will be as follows:</p>
<p class="snippet"> array([1, 1, 1, 2, 2, 2, 3, 0, 0, 0, 4, 0], dtype=int64)</p>
<p>The output array shows which data point belongs to which cluster. This is all we need to plot the data:</p>
<p class="snippet">plot.scatter(</p>
<p class="snippet">    mean_shift_model.cluster_centers_[:,0],</p>
<p class="snippet">    mean_shift_model.cluster_centers_[:,1]</p>
<p class="snippet">)</p>
<p class="snippet">for i in range(len(data_points)):</p>
<p class="snippet">    plot.plot(</p>
<p class="snippet">        data_points[i][0],</p>
<p class="snippet">        data_points[i][1],</p>
<p class="snippet">        ['ro','go','yo', 'ko', 'mo'][mean_shift_model.labels_[i]]</p>
<p class="snippet">    )</p>
<p class="snippet _idGenParaOverride-1">plot.show()</p>
<div></div>
<p>The output will be as follows:</p>
<div class="_idGenObjectLayout-1">
<div id="_idContainer095" class="IMG---Figure"><img class="_idGenObjectAttribute-1" src="Image00061.jpg" alt="" />
</div>
</div>
<h6>Figure 6.7: Graph based on k-means,</h6>
<p>The three blue points are the center points of the red, green, and yellow clusters. There are two more single dot clusters in the coordinate system, belonging to the points (6,1) and (10,10).</p>
<h3 id="_idParaDest-154"><a id="_idTextAnchor163"></a>
 Image Processing in Python</h3>
<p>To solve the upcoming activity, you need to know how to process images in Python. We will use the SciPy library for this.</p>
<p>There are multiple ways that you can read an image file from a path.</p>
<p>The easiest one is the <strong class="bold _idGenCharOverride-1">Image</strong>
 interface from the Python Imaging Library (PIL):</p>
<p class="snippet">from PIL import Image</p>
<p class="snippet">image = Image.open('file.jpg')</p>
<p class="_idGenParaOverride-1">The preceding code assumes that the file path specified in the string argument of the <strong class="bold _idGenCharOverride-1">open</strong>
 method points to a valid image file.</p>
<div></div>
<p>We can get the size of the image by querying the size property:</p>
<p class="snippet">image.size</p>
<p>The output will be as follows:</p>
<p class="snippet"> (750, 422)</p>
<p>We can create a two-dimensional NumPy array from the image containing the RGB values of each pixel:</p>
<p class="snippet">import numpy as np</p>
<p class="snippet">pixel_array = np.array(image)</p>
<p>Once the pixel array has been constructed, we can easily retrieve and manipulate each pixel:</p>
<p class="snippet">pixel_array[411][740]</p>
<p>The output will be as follows:</p>
<p class="snippet"> array([29, 33, 32], dtype=uint8)</p>
<p>The pixels of the image can also be made accessible using the <strong class="inline _idGenCharOverride-2">load()</strong>
 method of the image. Once we get access to these pixels, we can get the RGB or RGBA values of each pixel, depending on the file format:</p>
<p class="snippet">pixels = image.load()</p>
<p class="snippet">pixels[740, 411]</p>
<p>The output will be as follows:</p>
<p class="snippet"> (29, 33, 32)</p>
<p>Notice that the order of pixel coordinates is the opposite, that is, <strong class="inline _idGenCharOverride-2">pixel_array[411][740]</strong>
 when reading from left to right. We are reading the exact same pixel, but we have to supply the coordinates differently.</p>
<p>We can also set pixels to a new value:</p>
<p class="snippet">pixels[740, 411] = (255, 0, 0)</p>
<p>If you want to save changes, use the <strong class="inline _idGenCharOverride-2">save()</strong>
 method of the image:</p>
<p class="snippet _idGenParaOverride-1">image.save('test.jpg')</p>
<div></div>
<p>To perform clustering analysis on the pixels of the image, we need to convert the image to a data frame. This implies that we have to convert the pixels of the image to a tuple or array of <strong class="inline _idGenCharOverride-2">['x', 'y', 'red', 'green', 'blue']</strong>
 values. Once we have a one-dimensional array of these values, we can convert them to a pandas DataFrame:</p>
<p class="snippet">import pandas</p>
<p class="snippet">data_frame = pandas.DataFrame(</p>
<p class="snippet">    [[x,y,pixels[x,y][0], pixels[x,y][1], pixels[x,y][2]]</p>
<p class="snippet">        for x in range(image.size[0])</p>
<p class="snippet">        for y in range(image.size[1])</p>
<p class="snippet">    ],</p>
<p class="snippet">    columns=['x', 'y', 'r', 'g', 'b' ]</p>
<p class="snippet">)</p>
<p class="snippet">data_frame.head()</p>
<p>The output will be as follows:</p>
<p class="snippet">   x y r g b</p>
<p class="snippet">0 0 0 6 29 71</p>
<p class="snippet">1 0 1 7 32 73</p>
<p class="snippet">2 0 2 8 37 77</p>
<p class="snippet">3 0 3 8 41 82</p>
<p class="snippet">4 0 4 7 45 84</p>
<p>This is all you need to know to complete the activity on processing images using the Mean Shift algorithm.</p>
<h3 id="_idParaDest-155"><a id="_idTextAnchor164"></a>
 Activity 13: Shape Recognition with the Mean Shift Algorithm</h3>
<p>In this section, we will learn how images can be clustered. Imagine you are working for a company that detects human emotions from photos. Your task is to extract pixels making up a face in an avatar photo.</p>
<p class="_idGenParaOverride-1">Create a clustering algorithm with Mean Shift to cluster pixels of images. Examine the results of the Mean Shift algorithm and check whether any of the clusters contain a face when used on avatar images.</p>
<div></div>
<p>Then, apply the k-means, algorithm with a fixed default number of clusters (8, in this case). Compare your results with the Mean Shift clustering algorithm:</p>
<ol>
<li class="ParaOverride-1" value="1">Select an image you would like to cluster and load the image.</li>
<li value="2">Transform the pixels into a data frame to perform clustering. Perform Mean Shift clustering on the image using scikit-learn. Note that, this time, we will skip normalizing the features, because the proximity of the pixels and the proximity of the color components are represented in a close to equal weight. The algorithm will find two clusters.</li>
<li value="3">Depending on the image you use, notice how the Mean Shift algorithm treats human skin color, and what other parts of the image are placed in the same cluster. The cluster containing most of the skin in the avatar often includes data points that are very near and/or have a similar color as the color of the skin.</li>
<li value="4">Let's use the k-means algorithm to formulate eight clusters on the same data.</li>
</ol>
<p>You will see that the clustering algorithm indeed located data points that are close and contain similar colors.</p>
<h4>Note</h4>
<p class="callout">The solution to this activity is available at page 293.</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor165"></a>
 Summary</h2>
<p>In this chapter, we learned how clustering works. Clustering is a form of unsupervised learning, where the features are given, and the clustering algorithm finds the labels.</p>
<p>There are two types of clustering: flat and hierarchical.</p>
<p>The k-means algorithm is a flat clustering algorithm, where we determine K center points for our K clusters, and the algorithm finds the data points.</p>
<p>Mean Shift is an example of a hierarchical clustering algorithm, where the number of distinct label values is to be determined by the algorithm.</p>
<p>The final chapter will introduce a field that has become popular this decade due to the explosion of computation power and cheap, scalable online server capacity. This field is the science of neural networks and deep learning.</p>
</div>
</body></html>