- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Identify Common Types of Computer Vision Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B22207_05.xhtml#_idTextAnchor077), *Describe Azure Machine
    Learning Capabilities*, you learned to describe the capabilities of **automated
    machine learning** (**AutoML**), data and compute services for data science and
    **machine learning** (**ML**), as well as model management and deployment capabilities
    in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the Azure solutions available to solve
    problems for the **artificial intelligence** (**AI**) area of **computer vision**
    (**CV**), which encompasses manipulating, analyzing, interpreting, and deriving
    information and understanding from the pixel values of digital sources such as
    images and videos.
  prefs: []
  type: TYPE_NORMAL
- en: This content requires some understanding of ML principles; if you have skipped
    straight to this chapter and are new to ML or just want to refresh on some existing
    knowledge, then you can refer to *Part 2 – Describe the Fundamental Principles
    of Machine Learning on Azure* as an ML primer before you continue with this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: CV solutions can provide *pre-built* and *customizable* **ML models** that can
    be used by developers to enhance their applications, software, and services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objectives and skills we’ll cover in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CV solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify features of image classification solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify features of object detection solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify features of **optical character recognition** (**OCR**) solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify features of facial detection and facial analysis solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you should be aware of the features of solutions
    for detecting and analyzing images, objects, optical characters, and faces.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CV solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of **CV** solutions is to gain insights, meaning, and understanding
    from an image; that is, what information is the image telling me?
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CV’s capabilities are built upon the concept of *interpretation*, *understanding*,
    and *manipulation* of an image through its **pixel values**; computers see an
    array of *numeric pixel values* when they look at an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This computer processing of images is illustrated in *Figure 6**.1*, which
    shows how a slice of pepperoni pizza (*my personal favorite; no pineapple allowed*)
    would appear to a computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – How an image appears to a computer](img/B22207_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – How an image appears to a computer
  prefs: []
  type: TYPE_NORMAL
- en: Images are seen as **pixel arrays**, the array in *Figure 6**.1* is made up
    of **7 rows** and **5 columns**; the *image resolution* is therefore expressed
    as **7 x 5**. Each pixel in the array for a color image is given a **numeric pixel
    value** that represents the **RGB channels** (*layers*); for grayscale images,
    the pixel values would be represented by differing shades of gray, each with a
    value of between *255* for *white* pixels and *0* for *black* pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Pixel values are altered through the use of **filters** to carry out image processing
    operations for visual effect creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many filters, some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Blur
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Color inversion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharpen
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This image filter operation is carried out by **convolving** (*moving or passing*)
    the filter over an image, which provides a new array of values for the image that
    will manipulate (*change/alter*) the RGB layer pixel values. This provides a transformed
    image that is different from the original; this is a common use case seen in digital
    photo editing on your smartphone or image editing software.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see this illustrated in *Figure 6**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Image filter operation example](img/B22207_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Image filter operation example
  prefs: []
  type: TYPE_NORMAL
- en: In this image, a simple filter has been applied to a portion of the image, which
    changes the affected pixels where the filter has been passed over.
  prefs: []
  type: TYPE_NORMAL
- en: CV ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An ML model that has been trained on a vast volume of images allows us to extract
    information that can be actionable or provide meaning. This can be better understood
    from *Figure 6**.3*, which shows a simple example that can provide *descriptive
    information* by returning associated **tags** from the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Image analysis information provided using CV](img/B22207_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Image analysis information provided using CV
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.3*, the detected attributes can be seen in the JSON output, such
    as identifying the image as `dog` and the breed as `french bulldog`, and with
    confidence ratings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the corresponding API’s JSON response for the object detection
    from *Figure 6**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Both *pre-trained* and *custom models* can be used for image analysis; the custom
    vision model would be used where you wish to train the model with your own image
    collection and not images from the dataset that was used with the pre-trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **foundation model** used for the tasks of CV is the **Microsoft Florence
    model**. It is pre-trained on vast amounts of captioned internet images in order
    to build models that can be used for image analysis tasks such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification**: This is the capability of identifying an image category;
    can detect objects and people in a photo, and how many appear'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object detection**: This is the capability of identifying object location
    in an image, photo, or video; could detect location'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Captioning**: This is the capability of generating human-readable descriptions
    for everything that appears in images such as a photo'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tagging**: This is the capability of generating a list of tags that can be
    associated with an image for its detected attributes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OCR**: This is the capability of extracting printed or handwritten text from
    **documents**, such as forms, invoices, expense receipts, and **images**, which
    could include vehicle number plates, street signs, product brand labels, names
    on movie posters, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6**.4* represents how the CV tasks listed previously use **adaptive
    models** built from the **Microsoft Florence** **foundation model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Microsoft Florence multi-modal model for image data](img/B22207_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Microsoft Florence multi-modal model for image data
  prefs: []
  type: TYPE_NORMAL
- en: This `pixel values`, extracts features of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about **Project Florence** at this URL: https://www.microsoft.com/en-us/research/project/projectflorence/'
  prefs: []
  type: TYPE_NORMAL
- en: In the first section of this chapter, you were introduced to the concept of
    CV. You then learned about the Microsoft Florence model and how it can be used
    as a foundation for CV tasks. In the following sections, you will learn to identify
    the features of CV solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Identify features of image classification solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CV capabilities of Azure Machine Learning can be used as a solution for
    image classification.
  prefs: []
  type: TYPE_NORMAL
- en: '**CV** includes APIs that can interpret information and provide understanding
    from still and streaming images and allow further processing and analysisto take
    place.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification** refers to category prediction or classing of an image
    or sub-regions into groups based on predefined *classes* or *categories*; it addresses
    the question of “What is in this picture?”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of image classification can be seen in *Figure 6**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Image classified as a car](img/B22207_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Image classified as a car
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an extract of the corresponding API’s JSON response for the
    image classification from *Figure 6**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: From this JSON output, you can see the “caption” result returned for this image,
    along with a confidence rating. Note that there are no “location” coordinates;
    this will be covered in the *Identifying features of object detection* *solutions*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: You could train an ML model to identify which *type* of *transportation* category
    is shown, such as **car**, **bus**, and **bicycle**. Another example could be
    to identify a *type* of *animal*, such as a **bird**, **cat**, **dog**, or **horse**,
    or identify a species of animal.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that a `categories` can be predicted using a **supervised
    ML** (**SML**) model.
  prefs: []
  type: TYPE_NORMAL
- en: An **unsupervised ML** (**UML**) **clustering** model based on grouping features
    would not be appropriate for this use case; neither would a **regression** model,
    as this model would be used for any type of *number* *value* estimation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Use case**: This may be useful in the scenario where you need to know if
    an image contains an animal, a building, people, and so on; identifying species
    of plants or birds could also be a use case for image analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: This section looked at identifying features of image classification solutions
    using CV. In the next section, you will learn about identifying features of object
    detection solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Identify features of object detection solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CV capabilities of Azure Machine Learning can be used as a solution for
    object detection.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, you learned that `tag` information with a confidence
    rating.
  prefs: []
  type: TYPE_NORMAL
- en: While both **classification** and **detection** of images share a common objective
    of analyzing and interpreting image content, the difference is in the amount of
    detail returned; in addition to the categorization and caption labels that describe
    the image provided by the classification, the tag information has a percentage
    value on the confidence of what is the object detected.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will look at an example of object detection API operation information returned
    in more detail using the **Azure AI Vision service** using **Azure Vision Studio**
    in [*Chapter 7*](B22207_07.xhtml#_idTextAnchor129), *Identify Azure Tools and
    Services for Computer* *Vision Tasks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see in *Figure 6**.6* a simple scenario for object detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Objects located within an image](img/B22207_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Objects located within an image
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 6**.6*, you can see that two objects have been detected and that
    each has a `person` object and the position in the background are partially covered
    by the second detected object tagged **bulldog** in the foreground.
  prefs: []
  type: TYPE_NORMAL
- en: Some of you may have already noticed a flaw in the **bulldog** detected object,
    which will highlight the caution needed when working with AI models and why the
    responsible AI principle of “reliability and safety” is so critical. The issue
    to be flagged here is that the API has returned that the model has predicted that
    the detected object is a bulldog, but is only **75.50%** certain that is the breed,
    when the breed is, in fact, a boxer. This would require the model to be trained
    further with more images to have a more accurate score and return the correct
    breed attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the corresponding API’s JSON response for the object detection
    from *Figure 6**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this JSON output, you can see additional information that is returned than
    when using **image classification**; you can see the “location coordinates” in
    pixels for each object detected, a tag for each object, and a confidence rating
    for that applied tag for the object. As mentioned, the model should be trained
    further to increase the confidence score so that the correct tag can be applied
    to the object; in this case, the tag should have been *boxer* and not *bulldog*.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that object detection does not use a **regression** or **clustering**
    model but an **image** **analysis** model.
  prefs: []
  type: TYPE_NORMAL
- en: As well as a **pre-trained** CV model for object detection, a **custom** model
    can be trained using our own images; it could be trained to identify products
    or multiple instances of products, or certain aspects of products such as orientation,
    flaws, damage, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Use case**: This may be useful in the scenario where you need to determine
    the location, placement, positioning, orientation, or spatial relationships of
    the categorized object(s) in the image.'
  prefs: []
  type: TYPE_NORMAL
- en: This section looked at identifying the features of object detection solutions
    using CV. In the next section, you will learn about identifying features of OCR
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Identify features of OCR solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Computer Vision (CV) capabilities of Azure Machine Learning can be used
    as a solution for OCR.
  prefs: []
  type: TYPE_NORMAL
- en: The **OCR** solution can be used to extract “text” from an image. Letters and
    numbers are identified from shapes and then converted into *machine-encoded text*
    that can then be further utilized for processing by applications or users.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of OCR for an image can be seen in *Figure 6**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Extracting text from an image with OCR capability](img/B22207_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Extracting text from an image with OCR capability
  prefs: []
  type: TYPE_NORMAL
- en: 'The OCR model is trained to recognize elements of text, including punctuation,
    as well as numerals from individual shapes, and then produce an output as text.
    An example of a text output produced by an OCR model is shown in *Figure 6**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – OCR model text extraction output](img/B22207_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – OCR model text extraction output
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an extract of the corresponding API’s JSON response for the
    object detection from *Figure 6**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: From this JSON output, as you saw in the object detection solution, there is
    still some inaccuracy in the text output that could be improved, such as `"`*roroft*`"`
    instead of `"`*Microsoft*`"`, which showed a confidence rating of *0.68* in recognizing
    the shapes into a recognized word. This can be achieved with further training
    of a model, which highlights again the caution needed with AI models and why the
    responsible AI principle of “reliability and safety” is so critical.
  prefs: []
  type: TYPE_NORMAL
- en: We use the generic word “images,” but the capability can be used to extract
    *printed* and *handwritten* text from photos, documents, note paper, TIFF files,
    and scanned PDF files.
  prefs: []
  type: TYPE_NORMAL
- en: '**Use case**: This capability of “text extraction” may be useful in the scenario
    where you need to provide an inventory for a book collection, read a receipt for
    expense submission, extract invoice information, digitize historical documents
    for archival purposes, digitize handwritten report notes, extract names from road
    signs or movie posters, extract numbers from sporting event competitors, extract
    vehicle number plate numbers from CCTV, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: This section looked at identifying the features of OCR using CV. The next section
    will explore facial detection and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Identify features of facial detection and facial analysis solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CV aspects using Azure Machine Learning algorithms can be utilized as a solution
    for creating image and video facial detection, analysis, and recognition capabilities
    in applications.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection and analysis capabilities in ML can be used in the areas of identity
    verification and security. The capabilities can provide security access controls
    by determining the level of access by identifying and verifying the person’s face.
    You could think of this like **role-based access control** (**RBAC**), but instead
    of controlling access based on a user’s role, the level of access is determined
    by their face.
  prefs: []
  type: TYPE_NORMAL
- en: '**Use case**: This may be useful in the scenario where you wish to tag recognized
    friends in social media photographs; for identifying and targeting demographic
    groups for use in advertising; identifying celebrities; locating missing or wanted
    persons using CCTV footage; for intelligent monitoring of face pose; attribute
    detection such as mask wearing.'
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The capability of **facial detection** in **Computer Vision** (**CV)** can take
    an image and identify human faces within it. Faces that are detected have a square
    box around them, and the location of faces can be provided with “bounding box”
    coordinates returned, which can help inform a relationship between the images.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to know that this operation of *detection* is the first-stage
    capability for performing all other face service capabilities such as *analysis*
    and *recognition*; that is, we must first detect a face or faces to be able to
    derive any further information or understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.9* shows an example of facial detection by CV to extract the human
    faces located in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Detecting faces capability of CV](img/B22207_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Detecting faces capability of CV
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.9* shows the returned face objects and also the value of `no` for
    the `Face` `mask` attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an extract (not a full response) of the corresponding API’s
    JSON response for the object detection from *Figure 6**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From this JSON output, you can see the “bounding box” coordinates (in pixels)
    for the face object and the “face attributes” returned values; the “face landmark”
    value returned is part of facial analysis, which you will explore in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: This section looked at identifying the features of facial detection solutions
    using CV. Next, you will learn about facial analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Facial analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With ML models, the **facial analysis** capability can take an image and provide
    information on facial features, also referred to as **landmarks**, including categories
    of **mouth**, **lips**, **nose**, **eyes**, and **eyebrows**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Detecting facial landmarks capability using CV](img/B22207_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Detecting facial landmarks capability using CV
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.10* shows the facial features that can be used for ML model training
    of facial analysis using CV.'
  prefs: []
  type: TYPE_NORMAL
- en: This section looked at identifying the features of facial analysis solutions
    using CV. Next, you will learn about facial recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Facial recognition** involves known individuals being identified from their
    facial features in an image. This works by training an ML model with that individual’s
    images; then, for all new images on which the model has not been trained, they
    can be identified by the trained model in those images. This capability is illustrated
    in *Figure 6**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Identifying individuals using CV](img/B22207_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Identifying individuals using CV
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.11* shows an example of facial recognition to identify the **Microsoft
    Certified Trainer** (**MCT**) using a trained ML model using CV to identify known
    individuals located in the image; this can be customized to identify the student(s)
    from the trainer(s).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides comprehensive coverage on the identification of common
    types of computer vision solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you were introduced to CV solutions, including image classification,
    object detection, and OCR. We then concluded your learning for this chapter with
    information about CV solutions for facial detection and facial analysis solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, you learn to describe the capabilities of the following
    Azure AI services: Azure AI Vision, Azure AI Face, and Azure AI Video Indexer.'
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Before you proceed
  prefs: []
  type: TYPE_NORMAL
- en: If you don't have a Packt Library subscription or you haven't purchased this
    book from the Packt store, you will need to unlock the online resources to access
    the exam readiness drills. Unlocking is free and needs to be done only once. To
    learn how to do that, head over to the chapter titled [*Chapter 12*](B22207_12.xhtml#_idTextAnchor228)*,
    Accessing the* *Online Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/AI-900_CH06](https://packt.link/AI-900_CH06).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following QR code (*Figure 6**.12*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12– QR code that opens Chapter Review Questions for logged-in users](img/B22207_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12– QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 6**.13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Chapter Review Questions for Chapter 6](img/B22207_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Chapter Review Questions for Chapter 6
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Your aim is to keep the score the same while trying to answer these questions
    as quickly as possible. Here’s an example of how your next attempts should look
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
