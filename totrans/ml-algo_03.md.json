["```py\nfrom sklearn.datasets import load_boston\n\n>>> boston = load_boston()\n>>> X = boston.data\n>>> Y = boston.target\n\n>>> X.shape\n(506, 13)\n>>> Y.shape\n(506,)\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1000)\n```", "```py\nfrom sklearn.utils import check_random_state\n\n>>> rs = check_random_state(1000)\n<mtrand.RandomState at 0x12214708>\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=rs)\n```", "```py\nimport numpy as np\n\n>>> X = np.random.uniform(0.0, 1.0, size=(10, 2))\n>>> Y = np.random.choice(('Male','Female'), size=(10))\n>>> X[0]\narray([ 0.8236887 ,  0.11975305])\n>>> Y[0]\n'Male'\n```", "```py\nfrom sklearn.preprocessing import LabelEncoder\n\n>>> le = LabelEncoder()\n>>> yt = le.fit_transform(Y)\n>>> print(yt)\n[0 0 0 1 0 1 1 0 0 1]\n\n>>> le.classes_array(['Female', 'Male'], dtype='|S6')\n```", "```py\n>>> output = [1, 0, 1, 1, 0, 0]\n>>> decoded_output = [le.classes_[i] for i in output]\n['Male', 'Female', 'Male', 'Male', 'Female', 'Female']\n```", "```py\nfrom sklearn.preprocessing import LabelBinarizer\n\n>>> lb = LabelBinarizer()\n>>> Yb = lb.fit_transform(Y)\narray([[1],\n       [0],\n       [1],\n       [1],\n       [1],\n       [1],\n       [0],\n       [1],\n       [1],\n       [1]])\n\n>>> lb.inverse_transform(Yb)\narray(['Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male',\n       'Male', 'Male'], dtype='|S6')\n```", "```py\nimport numpy as np\n\n>>> Y = lb.fit_transform(Y)\narray([[0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0],\n       [1, 0, 0, 0, 0]])\n\n>>> Yp = model.predict(X[0])\narray([[0.002, 0.991, 0.001, 0.005, 0.001]])\n\n>>> Ypr = np.round(Yp)\narray([[ 0.,  1.,  0.,  0.,  0.]])\n\n>>> lb.inverse_transform(Ypr)\narray(['Female'], dtype='|S6')\n```", "```py\ndata = [\n   { 'feature_1': 10.0, 'feature_2': 15.0 },\n   { 'feature_1': -5.0, 'feature_3': 22.0 },\n   { 'feature_3': -2.0, 'feature_4': 10.0 }\n]\n```", "```py\nfrom sklearn.feature_extraction import DictVectorizer, FeatureHasher\n\n>>> dv = DictVectorizer()\n>>> Y_dict = dv.fit_transform(data)\n\n>>> Y_dict.todense()\nmatrix([[ 10.,  15.,   0.,   0.],\n        [ -5.,   0.,  22.,   0.],\n        [  0.,   0.,  -2.,  10.]])\n\n>>> dv.vocabulary_\n{'feature_1': 0, 'feature_2': 1, 'feature_3': 2, 'feature_4': 3}\n\n>>> fh = FeatureHasher()\n>>> Y_hashed = fh.fit_transform(data)\n\n>>> Y_hashed.todense()\nmatrix([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n        [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder\n\n>>> data = [\n   [0, 10],\n   [1, 11],\n   [1, 8],\n   [0, 12],\n   [0, 15]\n]\n\n>>> oh = OneHotEncoder(categorical_features=[0])\n>>> Y_oh = oh.fit_transform(data1)\n\n>>> Y_oh.todense()\nmatrix([[  1.,   0.,  10.],\n        [  0.,   1.,  11.],\n        [  0.,   1.,   8.],\n        [  1.,   0.,  12.],\n        [  1.,   0.,  15.]])\n```", "```py\nfrom sklearn.preprocessing import Imputer\n\n>>> data = np.array([[1, np.nan, 2], [2, 3, np.nan], [-1, 4, 2]])\n\n>>> imp = Imputer(strategy='mean')\n>>> imp.fit_transform(data)\narray([[ 1\\. ,  3.5,  2\\. ],\n       [ 2\\. ,  3\\. ,  2\\. ],\n       [-1\\. ,  4\\. ,  2\\. ]])\n\n>>> imp = Imputer(strategy='median')\n>>> imp.fit_transform(data)\narray([[ 1\\. ,  3.5,  2\\. ],\n       [ 2\\. ,  3\\. ,  2\\. ],\n       [-1\\. ,  4\\. ,  2\\. ]])\n\n>>> imp = Imputer(strategy='most_frequent')\n>>> imp.fit_transform(data)\narray([[ 1.,  3.,  2.],\n       [ 2.,  3.,  2.],\n       [-1.,  4.,  2.]])\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\n>>> ss = StandardScaler()\n>>> scaled_data = ss.fit_transform(data)\n```", "```py\nfrom sklearn.preprocessing import RubustScaler\n\n>>> rb1 = RobustScaler(quantile_range=(15, 85))\n>>> scaled_data1 = rb1.fit_transform(data)\n\n>>> rb1 = RobustScaler(quantile_range=(25, 75))\n>>> scaled_data1 = rb1.fit_transform(data)\n\n>>> rb2 = RobustScaler(quantile_range=(30, 60))\n>>> scaled_data2 = rb2.fit_transform(data)\n```", "```py\nfrom sklearn.preprocessing import Normalizer\n\n>>> data = np.array([1.0, 2.0])\n\n>>> n_max = Normalizer(norm='max')\n>>> n_max.fit_transform(data.reshape(1, -1))\n[[ 0.5, 1\\. ]]\n\n>>> n_l1 = Normalizer(norm='l1')\n>>> n_l1.fit_transform(data.reshape(1, -1))\n[[ 0.33333333,  0.66666667]]\n\n>>> n_l2 = Normalizer(norm='l2')\n>>> n_l2.fit_transform(data.reshape(1, -1))\n[[ 0.4472136 ,  0.89442719]]\n```", "```py\nfrom sklearn.feature_selection import VarianceThreshold\n\n>>> X[0:3, :]\narray([[-3.5077778 , -3.45267063,  0.9681903 ],\n       [-3.82581314,  5.77984656,  1.78926338],\n       [-2.62090281, -4.90597966,  0.27943565]])\n\n>>> vt = VarianceThreshold(threshold=1.5)\n>>> X_t = vt.fit_transform(X)\n\n>>> X_t[0:3, :]\narray([[-0.53478521, -2.69189452],\n       [-5.33054034, -1.91730367],\n       [-1.17004376,  6.32836981]])\n```", "```py\nfrom sklearn.datasets import load_boston, load_iris\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_regression\n\n>>> regr_data = load_boston()\n>>> regr_data.data.shape\n(506L, 13L)\n\n>>> kb_regr = SelectKBest(f_regression)\n>>> X_b = kb_regr.fit_transform(regr_data.data, regr_data.target)\n\n>>> X_b.shape\n(506L, 10L)\n\n>>> kb_regr.scores_\narray([  88.15124178,   75.2576423 ,  153.95488314,   15.97151242,\n        112.59148028,  471.84673988,   83.47745922,   33.57957033,\n         85.91427767,  141.76135658,  175.10554288,   63.05422911,\n        601.61787111])\n\n>>> class_data = load_iris()\n>>> class_data.data.shape\n(150L, 4L)\n\n>>> perc_class = SelectPercentile(chi2, percentile=15)\n>>> X_p = perc_class.fit_transform(class_data.data, class_data.target)\n\n>>> X_p.shape\n(150L, 1L)\n\n>>> perc_class.scores_\narray([  10.81782088,    3.59449902,  116.16984746,   67.24482759]) \n```", "```py\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\n\n>>> digits = load_digits()\n```", "```py\n>>> pca = PCA(n_components=36, whiten=True)\n>>> X_pca = pca.fit_transform(digits.data / 255)\n```", "```py\n>>> pca.explained_variance_ratio_\narray([ 0.14890594,  0.13618771,  0.11794594,  0.08409979,  0.05782415,\n        0.0491691 ,  0.04315987,  0.03661373,  0.03353248,  0.03078806,\n        0.02372341,  0.02272697,  0.01821863,  0.01773855,  0.01467101,\n        0.01409716,  0.01318589,  0.01248138,  0.01017718,  0.00905617,\n        0.00889538,  0.00797123,  0.00767493,  0.00722904,  0.00695889,\n        0.00596081,  0.00575615,  0.00515158,  0.00489539,  0.00428887,\n        0.00373606,  0.00353274,  0.00336684,  0.00328029,  0.0030832 ,\n        0.00293778])\n```", "```py\n>>> X_rebuilt = pca.inverse_transform(X_pca)\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import NMF\n\n>>> iris = load_iris()\n>>> iris.data.shape\n(150L, 4L)\n\n>>> nmf = NMF(n_components=3, init='random', l1_ratio=0.1)\n>>> Xt = nmf.fit_transform(iris.data)\n\n>>> nmf.reconstruction_err_\n1.8819327624141866\n\n>>> iris.data[0]\narray([ 5.1,  3.5,  1.4,  0.2])\n>>> Xt[0]\narray([ 0.20668461,  1.09973772,  0.0098996 ])\n>>> nmf.inverse_transform(Xt[0])\narray([ 5.10401653,  3.49666967,  1.3965409 ,  0.20610779])\n```", "```py\nfrom sklearn.decomposition import SparsePCA\n\n>>> spca = SparsePCA(n_components=60, alpha=0.1)\n>>> X_spca = spca.fit_transform(digits.data / 255)\n\n>>> spca.components_.shape\n(60L, 64L)\n```", "```py\nfrom sklearn.datasets import make_circles\n\n>>> Xb, Yb = make_circles(n_samples=500, factor=0.1, noise=0.05)\n```", "```py\nfrom sklearn.decomposition import KernelPCA\n\n>>> kpca = KernelPCA(n_components=2, kernel='rbf', fit_inverse_transform=True, gamma=1.0)\n>>> X_kpca = kpca.fit_transform(Xb)\n```", "```py\nfrom sklearn.decomposition import DictionaryLearning\n\n>>> dl = DictionaryLearning(n_components=36, fit_algorithm='lars', transform_algorithm='lasso_lars')\n>>> X_dict = dl.fit_transform(digits.data)\n```"]