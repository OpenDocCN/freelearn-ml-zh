<html><head></head><body>
		<div id="_idContainer109">
			<h1 id="_idParaDest-109"><em class="italic"><a id="_idTextAnchor108"/>Chapter 8</em>: Jumpstarting ML with SageMaker JumpStart and Autopilot</h1>
			<p><strong class="bold">SageMaker JumpStart</strong> offers complete solutions for select use cases as a starter kit for the world of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) with Amazon SageMaker without any code development. SageMaker JumpStart also catalogs popular pretrained <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>) and <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) models for you to easily deploy or fine-tune for your dataset. <strong class="bold">SageMaker Autopilot</strong> is an AutoML solution that explores your data, engineers features on your behalf, and trains an optimal model from various algorithms and hyperparameters. You don't have to write any code: Autopilot does it for you and returns notebooks to show you how it does it.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Launching a SageMaker JumpStart solution</li>
				<li>Deploying and fine-tuning a model from the SageMaker JumpStart model zoo</li>
				<li>Creating a high-quality model with SageMaker Autopilot</li>
			</ul>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor109"/>Technical requirements</h1>
			<p>For this chapter, you need to have permission to use JumpStart templates. You can confirm it from your domain and user profile. The code used in this chapter can be found at <a href="https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter08">https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter08</a>. </p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor110"/>Launching a SageMaker JumpStart solution</h1>
			<p>SageMaker <a id="_idIndexMarker503"/>JumpStart is particularly useful if you would like to learn a set of best practices for how AWS services should be used together to create an ML solution. You can do the same, too. Let's open up the JumpStart browser. There are multiple ways to open it, as shown in <em class="italic">Figure 8.1</em>. You can open it from the SageMaker Studio Launcher on the right or from the JumpStart asset browser in the left sidebar. </p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B17447_08_001.jpg" alt="Figure 8.1 – Opening the JumpStart browser from the Launcher or the left sidebar&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Opening the JumpStart browser from the Launcher or the left sidebar</p>
			<p>A new <a id="_idIndexMarker504"/>tab named <strong class="bold">SageMaker JumpStart</strong> will pop up in the main working area. Go to the <strong class="bold">Solutions</strong> section and click <strong class="bold">View all</strong>, as shown in <em class="italic">Figure 8.2</em>.</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B17447_08_02.jpg" alt="Figure 8.2 – Viewing all solutions in JumpStart&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Viewing all solutions in JumpStart</p>
			<p>Let's next move on to the solutions catalog for industries.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor111"/>Solution catalog for industries</h2>
			<p>There are <a id="_idIndexMarker505"/>more than a dozen solutions available in JumpStart as shown in <em class="italic">Figure 8.3</em>. These solutions are based on use cases spanning multiple industries, including manufacturing, retail, and finance. </p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B17447_08_003.jpg" alt="Figure 8.3 – JumpStart solution catalog – Click each card to see more information&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – JumpStart solution catalog – Click each card to see more information</p>
			<p>They are <a id="_idIndexMarker506"/>created by AWS developers and architects who know the given industry and use case. You can read more about each use case by clicking on the card. You will be greeted with a welcome page describing the use case, methodology, dataset, solution architecture, and any other external resources. On each solution page, you should also see a <strong class="bold">Launch</strong> button, which will deploy the solution and all cloud resources into your AWS account from a CloudFormation template.</p>
			<p>Let's use the <strong class="bold">Product Defect Detection</strong> solution from the catalog as our example, and we will walk through the deployment and the notebooks together.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor112"/>Deploying the Product Defect Detection solution</h2>
			<p>Visual inspection is widely adopted as a quality control measure in manufacturing processes. Quality <a id="_idIndexMarker507"/>control used to be a manual process where staff members would visually inspect the product either on the line or via imagery captured with cameras. However, manual inspection does not scale for the large quantities of products created in factories today. ML is a powerful tool that can identify product defects at an error rate that may, if trained properly, be even better than a human inspector. The <strong class="bold">Product Defect Detection</strong> SageMaker JumpStart solution is a great starting point to jump-start your CV project to detect defects in images using a state-of-the-art deep learning model. You will see how SageMaker manages training with a PyTorch script, and how model hosting is used. You will also learn how to make inferences against a hosted endpoint. The dataset is a balanced dataset across six types of surface defects and contains ground truths for both classification and drawing bounding boxes. Please follow these steps and read through the content of the notebooks:</p>
			<ol>
				<li>From the <strong class="bold">Solution</strong> catalog, please select <strong class="bold">Product Defect Detection in Images</strong>. As shown in <em class="italic">Figure 8.4</em>, you can read about the solution on the main page. You can learn about the sample data, the algorithm, and the cloud solution architecture.</li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B17447_08_04.jpg" alt="Figure 8.4 – Main page of the Product Defect Detection in Images solution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Main page of the Product Defect Detection in Images solution</p>
			<ol>
				<li value="2">Hit the <strong class="bold">Launch</strong> button, as shown in <em class="italic">Figure 8.4</em>, to start the deployment. You should see <a id="_idIndexMarker508"/>the deployment in progress on the screen. What is happening is that we just initiated a resource deployment <a id="_idIndexMarker509"/>using <strong class="bold">AWS CloudFormation</strong> in the background. AWS CloudFormation is a service that helps create, provision, and manage AWS resources in an orderly fashion through a template in JSON or YAML declarative code. This deployment takes a couple of minutes. </li>
				<li>Once the solution becomes <strong class="bold">Ready</strong>, click on <strong class="bold">Open Notebook</strong> in the tab to open the first notebook, <strong class="source-inline">0_demo.ipynb</strong>, from the solution. This notebook is the first of four notebooks that are deployed as part of the CloudFormation setup into your home directory at <strong class="source-inline">S3Downloads/jumpstart-prod-dfd_xxxxxxx/notebooks/</strong>. The notebook requires the <strong class="bold">SageMaker JumpStart PyTorch 1.0</strong> kernel as we are going to build a PyTorch-based solution. The kernel startup might take a minute or two if this is the first time using the kernel.</li>
				<li>Run all the cells in the <strong class="source-inline">0_demo.ipynb</strong> notebook. This notebook downloads the <strong class="source-inline">NEU-DET</strong> detection dataset to the filesystem and creates a SageMaker hosted endpoint using the SageMaker SDK's <strong class="source-inline">sagemaker.pytorch.PyTorchModel</strong> class for a <a id="_idIndexMarker510"/>pretrained PyTorch model. At the end of the notebook, you should see a figure showing the patches detected by the pretrained model compared to the ground truth, as in <em class="italic">Figure 8.5</em>.</li>
			</ol>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B17447_08_05.jpg" alt="Figure 8.5 – Final output of the 0_demo.ipynb notebook, showing a steel surface example, the ground truth, and the model prediction by a pretrained model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Final output of the 0_demo.ipynb notebook, showing a steel surface example, the ground truth, and the model prediction by a pretrained model</p>
			<p>This notebook demonstrates a key flexibility SageMaker offers, that is, you can bring a model trained from outside of SageMaker and host it in SageMaker. To create a SageMaker model from a PyTorch model, you need the model file <strong class="source-inline">.pt</strong>/<strong class="source-inline">.pth</strong> archived in a <strong class="source-inline">model.tar.gz</strong> archive and an entry point, <strong class="source-inline">detector.py</strong> script in this case, that instructs how the inference should be made. We can take a look at the <strong class="source-inline">detector.py</strong> script to learn more.</p>
			<ol>
				<li value="5">(Optional) Add a new cell and fill in the following commands:<p class="source-code">!aws s3 cp {sources}source_dir.tar.gz .</p><p class="source-code">!tar zxvf source_dir.tar.gz</p></li>
			</ol>
			<p>This will <a id="_idIndexMarker511"/>get the entire code base locally. Please open the <strong class="source-inline">detector.py</strong> file and locate the part that SageMaker uses to make inferences:</p>
			<p class="source-code">def model_fn(model_dir):</p>
			<p class="source-code">    backbone = "resnet34"</p>
			<p class="source-code">    num_classes = 7  # including the background</p>
			<p class="source-code">    mfn = load_checkpoint(Classification(backbone, num_classes - 1).mfn, model_dir, "mfn")</p>
			<p class="source-code">    rpn = load_checkpoint(RPN(), model_dir, "rpn")</p>
			<p class="source-code">    roi = load_checkpoint(RoI(num_classes), model_dir, "roi")</p>
			<p class="source-code">    model = Detection(mfn, rpn, roi)</p>
			<p class="source-code">    model = model.eval()</p>
			<p class="source-code">    freeze(model)</p>
			<p class="source-code">    return model</p>
			<p>SageMaker requires at least a <strong class="source-inline">model_fn(model_dir)</strong> function when importing a PyTorch model to instruct how the model is defined. In this example, <strong class="source-inline">Detection()</strong>class is a <strong class="source-inline">GeneralizedRCNN</strong> model defined in <strong class="source-inline">S3Downloads/jumpstart-prod-dfd_xxxxxx/notebooks/sagemaker_defect_detection/models/ddn.py</strong> with weights loaded from the provided model. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Other inference related functions you can implement include the following:</p>
			<p class="callout">Deserializing the invoke request body into an object we can perform prediction on:</p>
			<p class="callout"><strong class="source-inline">input_object = input_fn(request_body, request_content_type)</strong></p>
			<p class="callout">Performing prediction on the deserialized object with the loaded model:</p>
			<p class="callout"><strong class="source-inline">prediction = predict_fn(input_object, model)</strong></p>
			<p class="callout">Serializing the prediction result into the desired response content type:</p>
			<p class="callout"><strong class="source-inline">output = output_fn(prediction, response_content_type)</strong></p>
			<p class="callout">SageMaker has default implementations for these three functions if you don't override them. If you have a custom approach for making inferences, you can override these functions.</p>
			<ol>
				<li value="6">Proceed <a id="_idIndexMarker512"/>to the end of the notebook and click on <strong class="bold">Click here to continue</strong> to advance to the next notebook, <strong class="source-inline">1_retrain_from_checkpoint.ipynb</strong>. </li>
				<li>Run all the cells in <strong class="source-inline">1_retrain_from_checkpoint.ipynb</strong>. This notebook fine-tunes the pretrained model from a checkpoint with the downloaded dataset for a few more epochs. The solution includes training code in <strong class="source-inline">detector.py</strong> from <strong class="source-inline">osp.join(sources, "source_dir.tar.gz")</strong>. The solution uses the SageMaker SDK's PyTorch estimator to create a training job that launches an on-demand compute resource of one <strong class="source-inline">ml.g4dn.2xlarge</strong> instance and trains it from a provided pretrained checkpoint. The training takes about 10 minutes. The following lines of code show how you can feed the training data and a pretrained checkpoint to SageMaker PyTorch estimator to perform a model fine-tuning job:<p class="source-code">finetuned_model.fit(</p><p class="source-code">    {</p><p class="source-code">        "training": neu_det_prepared_s3,</p><p class="source-code">        "pretrained_checkpoint": osp.join(s3_pretrained, "epoch=294-loss=0.654-main_score=0.349.ckpt"),</p><p class="source-code">    }</p><p class="source-code">)</p><p class="callout-heading">Note</p><p class="callout">The naming of the dictionary keys to <strong class="source-inline">.fit()</strong> call is done by design. These keys are registered as environment variables with a SM_CHANNEL_ prefix inside the training container and can be accessed in the training script. The keys need to match what is written in the <strong class="source-inline">detector.py</strong> file in order to make this <strong class="source-inline">.fit()</strong> training call work. For example, see line 310 and 349 in <strong class="source-inline">detector.py</strong>:</p><p class="callout"><strong class="source-inline">aa("--data-path", metavar="DIR", type=str, default=os.environ["SM_CHANNEL_TRAINING"])</strong></p><p class="callout"><strong class="source-inline">aa("--resume-sagemaker-from-checkpoint", type=str, default=os.getenv("SM_CHANNEL_PRETRAINED_CHECKPOINT", None))</strong></p></li>
			</ol>
			<p>After the <a id="_idIndexMarker513"/>training, the model is deployed as a SageMaker hosted endpoint, as in the <strong class="source-inline">0_demo.ipynb</strong> notebook. In the end, a comparison between the ground truth, the inference from the pretrained model from <strong class="source-inline">0_demo.ipynb</strong>, and the inference from the fine-tuned model is visualized. We can see that the inference from the fine-tuned model has one fewer false positive, yet still isn't able to pick up a patch on the right side of the sample image. This should be considered a false negative.</p>
			<ol>
				<li value="8">Proceed to click on <strong class="bold">Click here to continue</strong> to advance to the next notebook, <strong class="source-inline">2_detection_from_scratch.ipynb</strong>. </li>
				<li>Run all the cells in the <strong class="source-inline">2_detection_from_scratch.ipynb</strong> notebook. Instead of training from a checkpoint, we train a model from scratch with 10 epochs using the same dataset and compare the inference to that from the pretrained model. The model is significantly undertrained, as expected with the small epoch size used. You are encouraged to increase the epoch size (the <strong class="source-inline">EPOCHS</strong> variable) to 300 to achieve better performance. However, this will take significantly more than 10 minutes.<p class="callout-heading">Note</p><p class="callout">We control whether we train from a checkpoint or from scratch by whether we include a <strong class="source-inline">pretrained_checkpoint</strong> key in a dictionary to <strong class="source-inline">.fit()</strong> or not.</p></li>
				<li>Proceed to click on <strong class="bold">Click here to continue</strong> to advance to the next notebook, <strong class="source-inline">3_classification_from_scratch.ipynb</strong>. </li>
			</ol>
			<p>In this <a id="_idIndexMarker514"/>notebook, we train a classification model using <strong class="source-inline">classifier.py</strong> for 50 epochs, instead of an object detection model from scratch, using the NEU-CLS classification dataset. A classification model is different from the previous object detection models. Image classification recognizes the types of defect in an entire image, whereas an object detection model can also localize where the defect is. Image classification is useful if you do not need to know the location of the defect, and can be used as a triage model for product defects. </p>
			<p>Training a classification model is faster, as you can see from the job. The classification accuracy on the validation set reaches <strong class="source-inline">0.99</strong>, as shown in the cell output from the training job, which is very accurate:</p>
			<p class="source-code"><strong class="bold">Epoch 00016: val_acc reached 0.99219 (best 0.99219), saving model to /opt/ml/model/epoch=16-val_loss=0.028-val_acc=0.992.ckpt as top 1</strong></p>
			<ol>
				<li value="11">This is the end of the solution. Please make sure to execute the last cell in each notebook to delete the models and endpoints, especially the last cell in the <strong class="source-inline">0_demo.ipynb</strong> notebook, where the deletion is commented out. Please uncomment this and execute it to delete the pretrained model and endpoint.</li>
			</ol>
			<p>With this SageMaker JumpStart solution, you built and trained four deep learning models based on a PyTorch implementation of Faster RCNN to detect and classify six types of <a id="_idIndexMarker515"/>defects in steel imagery with minimal coding effort. You also hosted them as SageMaker endpoints for real-time prediction. You can expect a similar experience with other solutions in SageMaker JumpStart to learn different aspects of SageMaker features used in the context of solving common use cases.</p>
			<p>Now, let's switch gears to the SageMaker JumpStart model zoo.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>SageMaker JumpStart model zoo</h1>
			<p>There are more than 200 popular prebuilt and pretrained models in SageMaker JumpStart <a id="_idIndexMarker516"/>for you to use out of the box or continue to train for your use case. What are they good for? Training an accurate deep learning model is time consuming and complex, even with the most powerful GPU machine. It also requires large amounts of training and labeled data. Now, with these models that have been developed by the community, pretrained on large datasets, you do not have to reinvent the wheel.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>Model collection</h2>
			<p>There are two groups of models: <strong class="bold">text models</strong> and <strong class="bold">vision models</strong>  in SageMaker JumpStart model zoo. These models are the most <a id="_idIndexMarker517"/>popular ones among the ML community. You can quickly browse the models in SageMaker JumpStart and select the one that meets your needs. On each model page, you will see an introduction to the model, its usage, and how to prepare a dataset for fine-tuning purposes. You can deploy models into AWS as a hosted endpoint for your use case or fine-tune the model further with your own dataset. </p>
			<p>Text models are sourced from the following three hubs: TensorFlow Hub, PyTorch Hub, and Hugging Face. Each model is specifically trained for a particular type of NLP task using <a id="_idIndexMarker518"/>a dataset such as text classification, question answering, or text generation. Notably, there are many flavors of <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), <strong class="bold">Cross-lingual Language Model </strong>(<strong class="bold">XLM</strong>), <strong class="bold">ELECTRA</strong>, and <strong class="bold">Generative Pretrained Transformer</strong> (<strong class="bold">GPT)</strong> up for grabs. </p>
			<p>Vision <a id="_idIndexMarker519"/>models are sourced from TensorFlow Hub, PyTorch <a id="_idIndexMarker520"/>Hub, and Gluon CV. There are models that perform image classification, image <a id="_idIndexMarker521"/>feature vector extraction, and <a id="_idIndexMarker522"/>object <a id="_idIndexMarker523"/>detection. <strong class="bold">Inception</strong>, <strong class="bold">SSD</strong>, <strong class="bold">ResNet</strong>, and <strong class="bold">Faster R-CNN</strong> models <a id="_idIndexMarker524"/>are some of the most notable <a id="_idIndexMarker525"/>and widely used models in the field of CV.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Deploying a model</h2>
			<p>Let's find <a id="_idIndexMarker526"/>a question-answering model and see how we can deploy it to our AWS account. In the search bar, type in <strong class="source-inline">question</strong> <strong class="bold">answering</strong> hit <strong class="bold">Return</strong>, and you should see a list of models that perform such tasks returned to you, as shown in <em class="italic">Figure 8.6</em>. </p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B17447_08_06.jpg" alt="Figure 8.6 – Searching for question-answering models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Searching for question-answering models</p>
			<p>Let's find <a id="_idIndexMarker527"/>and double-click <strong class="bold">DistilRoBERTa Base </strong>for <strong class="bold">Question Answering</strong> in the search results. This model is trained on <strong class="source-inline">OpenWebTextCorpus</strong> and is distilled from the RoBERTa model checkpoint. It has 6 layers, 768 hidden, 12 heads, and 82 million parameters. 82 million! It is not easy to train such a large model, for sure. Luckily with SageMaker JumpStart, we have a model that we can deploy out of the box. As shown in <em class="italic">Figure 8.7</em>, please expand the <strong class="bold">Deployment Configuration</strong> section, choose <strong class="bold">Ml.M5.Xlarge</strong> as the machine type, leave the endpoint name as <a id="_idIndexMarker528"/>default, and hit <strong class="bold">Deploy</strong>. Ml.M5.Xlarge is a general-purpose instance type that has 4 vCPU and 16 GB of memory, which is sufficient for this example. The deployment will take a couple of minutes. </p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B17447_08_07.jpg" alt="Figure 8.7 – Deploying a JumpStart DistilRoBERTa Base model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Deploying a JumpStart DistilRoBERTa Base model</p>
			<p>Once the model is deployed, a notebook will be provided to you to show how you can make an API call to the hosted endpoint (<em class="italic">Figure 8.8</em>). You can find a list of models in the JumpStart left sidebar.</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B17447_08_08.jpg" alt="Figure 8.8 – Opening a sample inference notebook after the model is deployed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Opening a sample inference notebook after the model is deployed</p>
			<p>In the <a id="_idIndexMarker529"/>sample notebook, two questions from the <strong class="bold">SQuAD v2</strong> dataset, one of the most widely used question-answering <a id="_idIndexMarker530"/>datasets for evaluation, are provided to show how inferencing can be done. Let's also ask our model other questions based on the following passage (Can you guess where you've read it before? Yes, it's the opening statement of this chapter!):</p>
			<p>Context:</p>
			<p class="author-quote">SageMaker JumpStart offers complete solutions for select use cases as a starter kit to the world of machine learning (ML) with Amazon SageMaker without any code development. SageMaker JumpStart also catalogs popular pretrained computer vision (CV) and natural language processing (NLP) models for you to easily deploy or fine-tune to your dataset. SageMaker Autopilot is an AutoML solution that explores your data, engineers features on your behalf, and trains an optimal model from various algorithms and hyperparameters. You don't have to write any code: Autopilot does it for you and returns notebooks to show how it does it.</p>
			<p>Questions:</p>
			<ul>
				<li><em class="italic">What does SageMaker JumpStart do?</em></li>
				<li><em class="italic">What is NLP?</em></li>
			</ul>
			<p>In the notebook, we should add the following to the second cell:</p>
			<p class="source-code">question_context3 = ["What does SageMaker JumpStart do?", "SageMaker JumpStart offers complete solutions for select use cases as a starter kit to the world of machine learning (ML) with Amazon SageMaker without any code development. SageMaker JumpStart also catalogs popular pretrained computer vision (CV) and natural language processing (NLP) models for you to easily deploy or fine-tune to your dataset. SageMaker Autopilot is an AutoML solution that explores your data, engineers features on your behalf and trains an optimal model from various algorithms and hyperparameters. You don't have to write any code: Autopilot does it for you and returns notebooks to show how it does it."]</p>
			<p class="source-code">question_context4 = ["What is NLP?", question_context3[-1]]</p>
			<p>In the <a id="_idIndexMarker531"/>third cell, append the two new question context pairs to the list in the <strong class="source-inline">for</strong> loop and execute all cells in the notebook:</p>
			<p class="source-code">for question_context in [question_context1, question_context2, question_context3, question_context4]:</p>
			<p>And voila! We get responses from our model that answer our questions about SageMaker JumpStart's capabilities and the full form of NLP as natural language processing.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor116"/>Fine-tuning a model</h2>
			<p>It is typical to perform model fine-tuning when you take a pretrained model off the shelf to expose <a id="_idIndexMarker532"/>the model to your dataset so that it can perform better on your dataset compared to the performance without such exposure. Furthermore, model fine-tuning takes less training time and requires a smaller amount of labeled data compared to training a model from scratch. To fine-tune a pretrained model from SageMaker JumpStart, first we need to make sure that the model you would like to use supports fine-tuning. You can find this attribute in the overview cards. Secondly, you need to point a dataset to the model. Taking the DistilRoBERTa Base model as an example, SageMaker JumpStart provides the default dataset of <strong class="bold">SQuAD-v2</strong>, which allows you to quickly start a training job. You can also create a dataset of your own by following the instructions on the JumpStart model page. We are going to do just that.</p>
			<p>Let's <a id="_idIndexMarker533"/>fine-tune the base DistilRoBERTa Base model with some questions and answers about Buddhism, which is one of the topics in the <strong class="source-inline">SquAD-v2</strong> dataset. Please follow these steps:</p>
			<ol>
				<li value="1">Open the <strong class="source-inline">chapter08/1-prep_data_for_finetune.ipynb</strong> notebook in the repository and execute all cells to download the dataset, extract the paragraphs that are related to Buddhism, and organize them as the fine-tune trainer expects. This is detailed on the description page in the <strong class="bold">Fine-tune the Model on a New Dataset</strong> section:<ul><li><strong class="bold">Input</strong>: A directory containing a <strong class="source-inline">data.csv</strong> file:<ul><li>The first column of the <strong class="source-inline">data.csv</strong> should have a question.</li><li>The second column should have the corresponding context.</li><li>The third column should have the integer character starting position for the answer in the context.</li><li>The fourth column should have the integer character ending position for the answer in the context.</li></ul></li><li><strong class="bold">Output</strong>: A trained model that can be deployed for inference.</li></ul></li>
				<li>At the end of the notebook, the <strong class="source-inline">data.csv</strong> file will be uploaded to your SageMaker default bucket: <strong class="source-inline">s3://sagemaker-&lt;region&gt;-&lt;accountID&gt;/chapter08/buddhism/data.csv</strong>. </li>
				<li>Once this is done, let's switch back to the model page and configure the fine-tuning job. As in <em class="italic">Figure 8.9</em>, select <strong class="bold">Enter S3 bucket location</strong>, paste in your CSV file URI into the box below, optionally append <strong class="source-inline">-buddhism</strong> onto the model name, leave the machine type and hyperparameters as their defaults, and hit <strong class="bold">Train</strong>. The default <strong class="bold">Ml.P3.2xlarge</strong> instance type, with one NVIDIA Tesla V100 GPU, is a <a id="_idIndexMarker534"/>great choice for fast model fine-tuning. The default hyperparameter setting performs fine-tuning with a <strong class="bold">batch size of 4</strong>, <strong class="bold">learning rate of 2e-5</strong>, and <strong class="bold">3 epochs</strong>. This is sufficient for us to demonstrate how the fine-tuning works. Feel free to change the values here to reflect your actual use case.</li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B17447_08_09.jpg" alt="Figure 8.9 – Configuring a fine-tuning job for a custom dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Configuring a fine-tuning job for a custom dataset</p>
			<p>The training job should take about 6 minutes with the <strong class="bold">Ml.P3.2xlarge</strong> instance. </p>
			<ol>
				<li value="4">Once <a id="_idIndexMarker535"/>the job completes, you can deploy the model to an endpoint with an <strong class="bold">Ml.M5.Xlarge</strong> instance, as shown in <em class="italic">Figure 8.10</em>. Ml.M5.Xlarge is a general-purpose CPU instance, which is a good starting point for model hosting.</li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B17447_08_010.jpg" alt="Figure 8.10 – Deploying the fine-tuned model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.10 – Deploying the fine-tuned model</p>
			<p>Of <a id="_idIndexMarker536"/>course, we now need to test how well the fine-tuned model performs on questions related to Buddha and Buddhism. Once the deployment finishes, you will be prompted with an option to open a prebuilt notebook to use the endpoint, similar to what is shown in <em class="italic">Figure 8.8</em>. </p>
			<ol>
				<li value="5">We can replace the question-context pair in the second cell with the following snippet from <a href="https://www.history.com/topics/religion/buddhism">https://www.history.com/topics/religion/buddhism</a>:<p class="source-code">question_context1 = ["When was Buddhism founded?", "Buddhism is a faith that was founded by Siddhartha Gautama ("the Buddha") more than 2,500 years ago in India. With about 470 million followers, scholars consider Buddhism one of the major world religions. Its practice has historically been most prominent in East and Southeast Asia, but its influence is growing in the West. Many Buddhist ideas and philosophies overlap with those of other faiths."]</p><p class="source-code"><strong class="bold">question_context2 = ["Where is Buddhism popular among?", question_context1[-1]]</strong></p></li>
			</ol>
			<p>Then, execute the cells in the notebook and you will see how well our new model performs. </p>
			<p>It's not <a id="_idIndexMarker537"/>quite what we would like the model to be. This is due to the very small epochs used and perhaps the unoptimized batch size and learning rate. As we are providing new data points for the model, the weights in the network are once again being updated and need to perform training for a sufficient number of epochs to converge on a lower loss and thus create a more accurate model. These hyperparameters often need to be tuned in order to obtain a good model even with fine-tuning. You are encouraged to further experiment with different hyperparameters to see if the model provides better responses to the questions.</p>
			<p>We have just created three ML models, which are supposed to be complex and difficult to train, without much coding at all. Now we are going to learn how to use SageMaker Autopilot to automatically create a high-quality model without any code.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor117"/>Creating a high-quality model with SageMaker Autopilot</h1>
			<p>Have you ever wanted to build an ML model without the hassle of data preprocessing, feature <a id="_idIndexMarker538"/>engineering, exploring <a id="_idIndexMarker539"/>algorithms, and optimizing the hyperparameters? Have you ever thought about how, for some use cases, you just wanted something quick to see if ML is even a possible approach for a certain business use case? Amazon SageMaker Autopilot makes it easy for you to build an ML model for tabular datasets without any code.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/>Wine quality prediction</h2>
			<p>To demonstrate SageMaker Autopilot, let's use a wine quality prediction use case. The wine <a id="_idIndexMarker540"/>industry has been <a id="_idIndexMarker541"/>searching for a technology that can help winemakers and the market to assess the quality of wine faster and with a better standard. Wine quality assessment and certification is a key part of the wine market in terms of production and sales and prevents the illegal adulteration of wines. Wine assessment is performed by expert oenologists based on physicochemical and sensory tests that produce features such as density, alcohol level, and pH level. However, when a human is involved, the standard can vary between oenologists or between testing trials. Having an ML approach to support oenologists in providing analytical information therefore becomes an important task in the wine industry. </p>
			<p>We are going to train an ML model to predict wine quality based on the physicochemical sensory values for 4,898 white wines produced between 2004 and 2007 in Portugal. The dataset is available from UCI at https://archive.ics.uci.edu/ml/datasets/Wine+Quality.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor119"/>Setting up an Autopilot job</h2>
			<p>Let's <a id="_idIndexMarker542"/>begin:</p>
			<ol>
				<li value="1">Please <a id="_idIndexMarker543"/>open the <a href="http://chapter08/2-prep_data_for_sm_autopilot.ipynb">chapter08/2-prep_data_for_sm_autopilot.ipynb</a> notebook from the repository, and execute all of the cells to download the data from the source, hold out a test set, and upload the training data to an S3 bucket. Please note the paths to the training data. </li>
				<li>Next, open <a id="_idIndexMarker544"/>the Launcher <a id="_idIndexMarker545"/>and select <strong class="bold">New Autopilot Experiment</strong>, as in <em class="italic">Figure 8.11</em>.</li>
			</ol>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B17447_08_011.jpg" alt="Figure 8.11 – Creating a new Autopilot experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.11 – Creating a new Autopilot experiment</p>
			<p>A new window will pop up for us to configure an Autopilot job. </p>
			<ol>
				<li value="3">As shown in <em class="italic">Figure 8.12</em>, provide an <strong class="bold">Experiment name</strong> of your choice, such as <strong class="source-inline">white-wine-predict-quality</strong>. </li>
			</ol>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B17447_08_012.jpg" alt="Figure 8.12 – Configuring an Autopilot job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Configuring an Autopilot job</p>
			<ol>
				<li value="4">As <a id="_idIndexMarker546"/>shown in <em class="italic">Figure 8.12</em>, provide <a id="_idIndexMarker547"/>the training data in the <strong class="bold">CONNECT YOUR DATA</strong> section, check the <strong class="bold">Find S3 bucket</strong> radio button, select your <strong class="source-inline">sagemaker-&lt;region&gt;-&lt;accountID&gt;</strong> from the <strong class="bold">S3 bucket name</strong> drop-down menu, and select the <strong class="source-inline">sagemaker-studio-book/chapter08/winequality/winequality-white-train.csv</strong> file from the <strong class="bold">Dataset file name</strong> drop-down menu. Set <strong class="bold">Target</strong> to <strong class="bold">quality</strong> to predict the quality of wine with the rest of attributes in the CSV file.</li>
				<li>In the <a id="_idIndexMarker548"/>lower half of the configuration page, as shown in <em class="italic">Figure 8.13</em>, provide a path to save <a id="_idIndexMarker549"/>the output data to, check the <strong class="bold">Find S3 bucket</strong> radio button, select your <strong class="source-inline">sagemaker-&lt;region&gt;-&lt;accountID&gt;</strong> from the <strong class="bold">S3 bucket name</strong> drop-down menu, and paste the <strong class="source-inline">sagemaker-studio-book/chapter08/winequality/</strong> path into the <strong class="bold">Dataset directory name</strong> field as the output location. This path is where we have the training CSV file.</li>
			</ol>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B17447_08_013.jpg" alt="Figure 8.13 – Configuring an Autopilot job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Configuring an Autopilot job</p>
			<ol>
				<li value="6">As shown in <em class="italic">Figure 8.13</em>, choose <strong class="bold">Multiclass classification</strong> from the <strong class="bold">Select the machine learning problem type</strong> drop-down menu. Then choose <strong class="bold">F1macro</strong> from the <strong class="bold">Objective metric</strong> drop-down menu so that we can expect a more balanced model should the data be biased toward a certain quality rank.</li>
				<li>As shown in <em class="italic">Figure 8.13</em>, choose <strong class="bold">Yes</strong> for <strong class="bold">Do you want to run a complete experiment?</strong>. Then toggle the <strong class="bold">Auto deploy</strong> option to <strong class="bold">off</strong> as we would like to walk through the evaluation process in SageMaker Studio before deploying our best model.</li>
				<li>As shown in <em class="italic">Figure 8.13</em>, expand the <strong class="bold">ADVANCED SETTINGS – Optional</strong> section <a id="_idIndexMarker550"/>and input <strong class="source-inline">100</strong> in the <strong class="bold">Max candidates</strong> field. By default, Autopilot runs 250 training jobs <a id="_idIndexMarker551"/>with different preprocessing steps, training algorithms, and hyperparameters. By using a limited number of candidates, we should expect the full experiment to complete faster than with the default setting.</li>
				<li>Hit <strong class="bold">Create Experiment</strong> to start the Autopilot job.</li>
			</ol>
			<p>You will see a new window that shows the progress of the Autopilot job. Please let it crunch the numbers a bit and come back in a couple of minutes. You will see more progress and output in the progress tab, as shown in <em class="italic">Figure 8.14</em>. </p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B17447_08_014.jpg" alt="Figure 8.14 – Viewing the progress of an Autopilot experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Viewing the progress of an Autopilot experiment</p>
			<p>A lot <a id="_idIndexMarker552"/>is going on <a id="_idIndexMarker553"/>here. Let's dive in.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor120"/>Understanding an Autopilot job</h2>
			<p>Amazon <a id="_idIndexMarker554"/>SageMaker Autopilot <a id="_idIndexMarker555"/>executes an end-to-end ML model-building exercise automatically. It <a id="_idIndexMarker556"/>performs <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>), does data preprocessing, and creates feature engineering and a model-training recipe. It then executes the recipe in order to find the best model given the conditions. You can see the progress in the middle portion of <em class="italic">Figure 8.14</em>. </p>
			<p>What makes Autopilot unique is the full visibility that it provides. Autopilot unboxes the typical AutoML black box by giving you the EDA results and the code that Autopilot runs to perform the feature engineering and ML modeling in the form of Jupyter notebooks. You can access the two notebooks by clicking the <strong class="bold">Open data exploration notebook</strong> button for the EDA results and the <strong class="bold">Open candidate generation notebook</strong> button for the recipe. </p>
			<p>The data exploration notebook is helpful for understanding the data, the distribution, and how Autopilot builds the recipe based on the characteristics of the data. For example, Autopilot looks for missing values in the dataset, the distribution of numerical features, and the cardinality of the categorical features. This information gives data scientists <a id="_idIndexMarker557"/>a baseline understanding of the data, along with actionable insights on whether the input data contains <a id="_idIndexMarker558"/>reasonable entries or not. Should you see many features with high percentages of missing values (the <strong class="bold">Percent of Missing Values</strong> section), you could take the suggested actions to investigate the issue from the data creation perspective and apply some level of pre-processing to either remove the feature or apply domain-specific imputation. You may ask, "<em class="italic">Doesn't Autopilot apply data pre-processing and feature engineering to the data?</em>" Yes, it does. However, Autopilot does not have domain-specific knowledge of your data. You should expect a more generic, data science-oriented approach to the issues surfaced by Autopilot, which may not be as effective. </p>
			<p>The candidate generation notebook prescribes a recipe for how the model should be built and trained based on the EDA of the data. The amount of code might look daunting, but if you read through it carefully, you can see, for example, what data preprocessing steps and modeling approaches Autopilot is attempting, as shown in the <strong class="bold">Candidate Pipelines</strong> section. The following is one example of this:</p>
			<p class="source-code"><strong class="bold">The SageMaker Autopilot Job has analyzed the dataset and has generated 9 machine learning pipeline(s) that use 3 algorithm(s).</strong></p>
			<p>Autopilot <a id="_idIndexMarker559"/>bases the <a id="_idIndexMarker560"/>pipelines on <a id="_idIndexMarker561"/>three algorithms: <strong class="bold">XGBoost</strong>, <strong class="bold">linear learner</strong>, and <strong class="bold">multi-layer perceptron</strong> (<strong class="bold">MLP)</strong>. XGBoost is a popular gradient-boosted tree algorithm that combines an ensemble of weak predictors to form the final predictor in an efficient and flexible manner. XGBoost is one of SageMaker's built-in algorithms. Linear learner, also a SageMaker built-in algorithm, trains multiple linear models with different hyperparameters, and finds the best model with a distributed stochastic gradient descent optimization. MLP is a neural network-based supervised learning algorithm that can have multiple hidden layers of neurons to create a non-linear model.</p>
			<p>You can <a id="_idIndexMarker562"/>also see the list of hyperparameters <a id="_idIndexMarker563"/>and ranges Autopilot is exploring (the <strong class="bold">MultiAlgorithm Hyperparameter Tuning</strong> section). Not only does Autopilot provide you visibility, but it also gives you full control of the experimentation. You can click on the <strong class="bold">Import notebook</strong> button the top right to get a copy of the notebook that you can actually customize and execute to obtain your next best model.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Evaluating Autopilot models</h2>
			<p>If you see that the job status in the tab, as shown in <em class="italic">Figure 8.14</em>, has changed to <strong class="bold">Completed</strong>, then it is <a id="_idIndexMarker564"/>time to evaluate the models Autopilot <a id="_idIndexMarker565"/>has generated. Autopilot has trained 100 models using various mixtures of feature engineering, algorithms, and hyperparameters as you can see in the list of trials. This leaderboard also shows the performance metric, the F1 score on a random validation split, used to evaluate the models. You can click on <strong class="bold">Objective: F1</strong> to sort the models by score. </p>
			<p>Let's take a closer look at the best model, the one that has the highest F1 score and a star next to the trial name. Right-click on the trial and select <strong class="bold">Open in model details</strong> to view more information.</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B17447_08_015.jpg" alt="Figure 8.15 – Viewing Autopilot model details in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Viewing Autopilot model details in SageMaker Studio</p>
			<p>Autopilot reports a lot of detail on this page, as shown in <em class="italic">Figure 8.15</em>. First of all, we can see that this model is built based on the <strong class="bold">XGBoost</strong> algorithm. We also see a chart of feature importance that Autopilot generates for our convenience. This chart tells us how the <a id="_idIndexMarker566"/>model considers the importance, or contribution, of the input features. Autopilot computes the <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) values using <strong class="bold">SageMaker Clarify</strong> for this XGBoost model and dataset. SHAP values explain how features contribute to the model forming the decision based on game theory. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can hover over the bars to see the actual values. SageMaker provides more detail so that you can learn more about how these SHAP values are calculated in the white papers in the <strong class="bold">Want to learn more?</strong> section. </p>
			<p>Back <a id="_idIndexMarker567"/>to the chart, you can also download an automatically generated PDF report that contains this <a id="_idIndexMarker568"/>chart for review and distribution (<strong class="bold">Export PDF report</strong>). If you want to work with the raw data in JSON format in order to integrate the SHAP values in other applications, you can download the data (<strong class="bold">Download raw data</strong>). By clicking the two buttons, you will be redirected to the S3 console as shown in <em class="italic">Figure 8.16</em>. You can download the file from the S3 bucket on the console by clicking the <strong class="bold">Download</strong> button.</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B17447_08_016.jpg" alt="Figure 8.16 – Downloading the feature importance PDF report in the S3 console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – Downloading the feature importance PDF report in the S3 console</p>
			<p>Besides the feature importance, the model performance on the training and validation sets is also very important in understanding how the model would perform in real life. You can see the metrics captured during the training run in the <strong class="bold">Metrics</strong> part. In addition to the <strong class="source-inline">ObjectiveMetric</strong> used to rank the models on the leaderboard, we see the following metrics:</p>
			<ul>
				<li><strong class="source-inline">train:f1</strong></li>
				<li><strong class="source-inline">train:merror</strong></li>
				<li><strong class="source-inline">validation:f1</strong></li>
				<li><strong class="source-inline">validation:merror</strong></li>
			</ul>
			<p>They are the multi-class F1macro and the multi-class error for the train and validation split of the data. As you can tell by the identical values, <strong class="source-inline">ObjectiveMetric</strong> is essentially <strong class="source-inline">validation:f1</strong>. With <strong class="source-inline">train:f1</strong> well above <strong class="source-inline">validation:f1</strong>, we may come to the conclusion that the model is overfitted to the training dataset. But why is this?</p>
			<p>We <a id="_idIndexMarker569"/>can further verify the model performance in more detail with the test data that we held out at the <a id="_idIndexMarker570"/>beginning. Please open the <strong class="source-inline">chapter08/3-evaluate_autopilot_models.ipynb</strong> notebook from the repository and execute all cells. In this notebook, you will retrieve the top models based on the <strong class="source-inline">ObjectiveMetric</strong> from the Autopilot job, perform inference <a id="_idIndexMarker571"/>in the cloud using the <strong class="bold">SageMaker Batch Transform</strong> feature, and run some evaluations for each model. Feel free to change the value in <strong class="source-inline">TOP_N_CANDIDATES</strong> to a different number. You should see the F1 score computed with the macro, an unweighted mean, weighted approaches, a classification report (from a sklearn function), and a confusion matrix on the test data as the output of the last cell.</p>
			<p>With the top model, a couple of things jump out at me here. The data is imbalanced in nature. There is a higher concentration of scores <strong class="source-inline">5</strong>, <strong class="source-inline">6</strong>, and <strong class="source-inline">7</strong>. Few wines got a score of <strong class="source-inline">3</strong>, <strong class="source-inline">4</strong>, or <strong class="source-inline">8</strong>. The confusion matrix also shows that wines that got a score of <strong class="source-inline">3</strong> were all incorrectly classified. Under this situation, the <strong class="source-inline">f1</strong> macro measure will be drastically lowered by incorrect classification of a minority class out of proportion. If we look at the weighted version of the <strong class="source-inline">f1</strong> score, we get a significantly higher score as the scoring weights the dominant classes more heavily:</p>
			<p class="source-code">Candidate name:  white-wine-predict-qualitysZ1CBE-003-1a47413b</p>
			<p class="source-code">Objective metric name:  validation:f1</p>
			<p class="source-code">Objective metric value:  0.4073199927806854</p>
			<p class="source-code">f1 = 0.51, Precision = 0.59 (macro)</p>
			<p class="source-code">f1 = 0.67, Precision = 0.68 (weighted)</p>
			<p class="source-code">              precision    recall  f1-score   support</p>
			<p class="source-code">           3       0.00      0.00      0.00         3</p>
			<p class="source-code">           4       0.70      0.39      0.50        18</p>
			<p class="source-code">           5       0.63      0.67      0.65       144</p>
			<p class="source-code">           6       0.67      0.77      0.72       215</p>
			<p class="source-code">           7       0.76      0.57      0.65        94</p>
			<p class="source-code">           8       0.78      0.44      0.56        16</p>
			<p class="source-code">    accuracy                           0.67       490</p>
			<p class="source-code">   macro avg       0.59      0.47      0.51       490</p>
			<p class="source-code">weighted avg       0.68      0.67      0.67       490</p>
			<p class="source-code">[[  0   0   3   0   0   0]</p>
			<p class="source-code"> [  0   7   8   3   0   0]</p>
			<p class="source-code"> [  0   2  96  45   1   0]</p>
			<p class="source-code"> [  0   1  37 166  10   1]</p>
			<p class="source-code"> [  0   0   8  31  54   1]</p>
			<p class="source-code"> [  0   0   0   3   6   7]]</p>
			<p>It is <a id="_idIndexMarker572"/>also important <a id="_idIndexMarker573"/>to measure the model's performance using the metrics that matter the most to the use case. As the author of the cited study stated about the importance of precision measure:</p>
			<p class="author-quote">"This statistic is important in practice, since in a real deployment setting the actual values are unknown and all predictions within a given column would be treated the same." </p>
			<p>We should compare the precision measure used in the original research study (in <em class="italic">Table 3</em> in the study, linked in the <em class="italic">Further reading</em> section) where the individual precisions are the following: </p>
			<ul>
				<li>4: 63.3%</li>
				<li>5: 72.6% </li>
				<li>6: 60.3%</li>
				<li>7: 67.8% </li>
				<li>8: 85.5% </li>
			</ul>
			<p>when tolerance <strong class="source-inline">T = 0.5</strong> for white wines. Our first Autopilot model overperforms in precision in some categories and underperforms in others. </p>
			<p>Another strategy to find a model that serves the business problem better is to evaluate more models in addition to the best model suggested by Autopilot. We can see the evaluation for two others (or more, depending on your setting for <strong class="source-inline">TOP_N_CANDIDATES</strong>). We find that even though the second and third models have lower <strong class="source-inline">validation:f1</strong> (macro) scores than the first model, they actually have higher F1 scores on the held-out test set. The individual precision scores for the third model are all better <a id="_idIndexMarker574"/>than the model in the original research, except for class 5, by 2.6%. What a charm! The third model <a id="_idIndexMarker575"/>in the leaderboard actually has better performance on the test data as measured by the precision metric, which makes the most sense to the use case.</p>
			<p>After evaluation, we can deploy the optimal model into an endpoint for real-time inference. Autopilot makes it easy to deploy a model. In the leaderboard, select the line item that you would like to deploy, and click on the <strong class="bold">Deploy model</strong> button. A new page will pop up for you to configure the endpoint. Most options are straightforward and self-explanatory for an experienced SageMaker Studio user. Two things to note are that you can enable the data capture, which is useful if you want to set up SageMaker Model Monitor later. If you want the model to return more than just the <strong class="bold">predicted_label</strong>, such as the hard label of the winning class in a multiclass use case, you can choose to return the <strong class="bold">probability</strong> of the winning label, the <strong class="bold">labels</strong> of all classes, and the <strong class="bold">probabilities</strong> of all classes. The order of the selection will also determine the order of the output.</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor122"/>Summary</h1>
			<p>In this chapter, we introduced two features integrated into SageMaker Studio—JumpStart and Autopilot—with three ML use cases to demonstrate low-to-no code ML options for ML developers. We learned how to browse JumpStart solutions in the catalog and how to deploy an end-to-end CV solution from JumpStart to detect defects in products. We also deployed and fine-tuned a question-answering model using the DistilRoBERTa Base model from the JumpStart model zoo without any ML coding. With Autopilot, we built a white wine quality prediction model simply by pointing Autopilot to a dataset stored in S3 and starting an Autopilot job – no code necessary. It turned out that Autopilot even outperforms the model created by the original researchers, which may have taken months of research.</p>
			<p>With the next chapter, we begin the next part of the book: <em class="italic">Production and Operation of Machine Learning with SageMaker Studio</em>. We will learn how we can move from prototyping to production ML training at scale with distributed training in SageMaker, how to monitor model training easily with SageMaker Debugger, how to save training cost with managed spot training..</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Further reading</h1>
			<p>For more information take a look at the following resources:</p>
			<ul>
				<li>P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. <em class="italic">Modeling wine preferences by data mining from physicochemical properties</em>. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.<a href=" https://bit.ly/3enCZUz"> https://bit.ly/3enCZUz</a></li>
			</ul>
		</div>
	</body></html>