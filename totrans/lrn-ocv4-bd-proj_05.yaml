- en: Automated Optical Inspection, Object Segmentation, and Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](ceaab6b4-2f4a-45e4-9f5d-2544c75bd405.xhtml), *Delving into Histogram
    and Filters*, we learned about histograms and filters, which allow us to understand
    image manipulation and create a photo application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce the basic concepts of object segmentation
    and detection. This means isolating the objects that appear in an image for future
    processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Noise removal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light/background removal basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connected components for object segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding contours for object segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many industries use complex computer vision systems and hardware. Computer vision
    tries to detect problems and minimize errors produced in the production process,
    improving the quality of final products.
  prefs: []
  type: TYPE_NORMAL
- en: In this sector, the name for this computer vision task is **Automated Optical
    Inspection** (**AOI**). This name appears in the inspection of printed circuit
    board manufacturers, where one or more cameras scan each circuit to detect critical
    failures and quality defects. This nomenclature was used in other manufacturing
    industries so that they could use optical camera systems and computer vision algorithms
    to increase product quality. Nowadays, optical inspection using different camera
    types (infrared or 3D cameras), depending on the requirements, and complex algorithms
    are used in thousands of industries for different purposes such as defect detection,
    classification, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires familiarity with the basic C++ programming language.
    All of the code that''s used in this chapter can be downloaded from the following
    GitHub link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_05](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_05).
    The code can be executed on any operating system, though it is only tested on
    Ubuntu.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2DRbMbz](http://bit.ly/2DRbMbz)'
  prefs: []
  type: TYPE_NORMAL
- en: Isolating objects in a scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce the first step in an AOI algorithm
    and try to isolate different parts or objects in a scene. We are going to take
    the example of the object detection and classification of three object types (screw,
    packing ring, and nut) and develop them in this chapter and [Chapter 6](83822325-00be-4874-813c-b90097030d85.xhtml),
    *Learning Object Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we are in a company that produces these three objects. All of
    them are in the same carrier tape. Our objective is to detect each object in the
    carrier tape and classify each one to allow a robot to put each object on the
    correct shelf:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91eceec6-11d9-4b16-820d-cf0197e6f02c.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we are going to learn how to isolate each object and detect
    its position in the image in pixels. In the next chapter, we are going to learn
    how to classify each isolated object to recognize if it is a nut, screw, or a
    packing ring.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we show our desired result, where there are a
    few objects in the left image. In the right image, we have drawn each one in a
    different color, showing different features such as area, height, width, and contour
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/530371fc-0a24-4292-84d5-06ddbc6015e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To reach this result, we are going to follow different steps that allow us
    to understand and organize our algorithms better. We can see these steps in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b95b6820-afcb-47af-8838-0c76b82a8b8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Our application will be divided into two chapters. In this chapter, we are going
    to develop and understand the preprocessing and segmentation steps. In [Chapter
    6](83822325-00be-4874-813c-b90097030d85.xhtml), *Learning Object Classification*,
    we are going to extract the characteristics of each segmented object and train
    our machine learning system/algorithm on how to recognize each object class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our preprocessing steps will be divided into three more subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noise Removal**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Light Removal**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binarization**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the segmentation step, we are going to use two different algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Contour detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connected components** extraction (labeling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see these steps and the application flow in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/620e2715-7061-4e32-92a5-19d5b9a3f671.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, it's time to start the preprocessing step so that we can get the best **Binarization**
    image by removing the noise and lighting effects. This minimizes any possible
    detection errors.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an application for AOI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To create our new application, we require a few input parameters. When a user
    executes the application, all of them are optional, excluding the input image
    to process. The input parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Input image to process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light image pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Light operation, where a user can choose between difference or divide operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the user sets `0` as a value, the difference operation is applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the user set `1` as a value, the division operation is applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation, where the user can choose between connected components with or
    without statistics and find contour methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the user sets `1` as the input value, the connected components method for
    segment is applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the user sets `2` as the input value, the connected components method with
    the statistics area is applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the user sets `3` as the input value, the find contours method is applied
    for Segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To enable this user selection, we are going to use the `command line parser`
    class with the following keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use the `command line parser` class in the `main` function
    by checking the parameters. The `CommandLineParser` is explained in [Chapter 2](37cf2702-b8c6-41ff-a935-fd4030f8ce64.xhtml),
    *An Introduction to the Basics of OpenCV*, in the *Reading videos and cameras*
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After parsing our command-line user data, we need to check the input image
    has been loaded correctly. We then load the image and check it has data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to create our AOI process of segmentation. We are going to
    start with the preprocessing task.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the input image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces some of the most common techniques that we can apply
    for preprocessing images in the context of object segmentation/detection. The
    preprocessing is the first change we make to a new image before we start working
    and extracting the information we require from it. Normally, in the preprocessing
    step, we try to minimize the image noise, light conditions, or image deformation
    due to a camera lens. These steps minimize errors while detecting objects or segments
    in our image.
  prefs: []
  type: TYPE_NORMAL
- en: Noise removal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we don't remove the noise, we can detect more objects than we expect because
    noise is normally represented as small points in the image and can be segmented
    as an object. The sensor and scanner circuit normally produces this noise. This
    variation of brightness or color can be represented in different types, such as
    Gaussian noise, spike noise, and shot noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different techniques that can be used to remove the noise. Here,
    we are going to use a smooth operation, but depending on the type of noise, some
    are better than others. A median filter is normally used for removing salt-and-pepper
    noise; for example, consider the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/578318f1-5c92-42a9-a75a-fb6c2d6cee45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is the original input with salt-and-pepper noise. If we
    apply a median blur, we get an awesome result in which we lose small details.
    For example, we lose the borders of the screw, but we maintain perfect edges.
    See the result in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee868769-9c8d-4a81-b863-80c4c3455735.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we apply a box filter or Gaussian filter, the noise is not removed but made
    smooth, and the details of the objects are lost and smoothened too. See the following
    image for the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddc55313-3189-4ceb-b113-ec491c19868e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV brings us the `medianBlur` function, which requires three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image with the `1`, `3`, or `4` channel's image. When the kernel size
    is bigger than `5`, the image depth can only be `CV_8U`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An output image, which is the resulting image on applying median blur with the
    same type and depth as the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel size, which is an aperture size greater than `1` and odd, for example,
    3, 5, 7, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code is used to remove noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Removing the background using the light pattern for segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to develop a basic algorithm that will enable
    us to remove the background using a light pattern. This preprocessing gives us
    better segmentation. The input image without noise is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c86fa656-74fa-4826-abe4-7fc224178280.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we apply a basic threshold, we will obtain an image result like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3f80307-88d1-41d1-a11b-87e56bf39eef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the top image artifact has a lot of white noise. If we apply
    a light pattern and background removal technique, we can obtain an awesome result
    in which we can see that there are no artifacts in the top of image, like the
    previous threshold operation, and we will obtain better results when we have to
    segment. We can see the result of background removal and thresholding in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e37e0ca-7254-44ac-ac6f-b57b85ad9de5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, how can we remove the light from our image? This is very simple: we only
    need a picture of our scenario without any objects, taken from exactly the same
    position and under the same lighting conditions that the other images were taken
    under; this is a very common technique in AOI because the external conditions
    are supervised and well-known. The image result for our case is similar to the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3ed20a6-9d23-4926-aba8-d79d7a9a950c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, using a simple mathematical operation, we can remove this light pattern.
    There are two options for removing it:'
  prefs: []
  type: TYPE_NORMAL
- en: Difference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Division
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The difference option is the simplest approach. If we have the light pattern
    `L` and the image picture `I`, the resulting removal `R` is the difference between
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This division is a bit more complex, but simple at the same time. If we have
    the light pattern matrix `L` and the image picture matrix `I`, the result removal
    `R` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we divide the image by the light pattern, and we have the assumption
    that if our light pattern is white and the objects are darker than the background
    carrier tape, then the image pixel values are always the same or lower than the
    light pixel values. The result we obtain from `I/L` is between `0` and `1`. Finally,
    we invert the result of this division to get the same color direction range and
    multiply it by `255` to get values within the range of `0-255`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our code, we are going to create a new function called `removeLight` with
    the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: An input image to remove the light/background
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A light pattern, `Mat`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A method, with a `0` value for difference and `1` for division
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result is a new image matrix without light/background. The following code
    implements the removal of the background through the use of the light pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's explore this. After creating the `aux` variable to save the result, we
    select the method chosen by the user and pass the parameter to the function. If
    the method that was selected is `1`, we apply the division method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The division method requires a 32-bit float of images to allow us to divide
    the images and not truncate the numbers into integers. The first step is to convert
    the image and light pattern mat to floats of 32 bits. To convert images of this
    format, we can use the `convertTo` function of the `Mat` class. This function
    accepts four parameters; the output converted image and the format you wish to
    convert to the required parameters, but you can define alpha and beta parameters,
    which allow you to scale and shift the values following the next function, where
    *O* is the output image and *I* the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '*O*(*x*,*y*)=*cast*<*Type*>(*α* * *I*(*x*,*y*)+*β*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code changes the image to 32-bit float:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can carry out the mathematical operations on our matrix as we described,
    by dividing the image by the pattern and inverting the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have the result but it is required to return it to an 8-bit depth image,
    and then use the convert function as we did previously to convert the image''s
    `mat` and scale from `0` to `255` using the alpha parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can return the `aux` variable with the result. For the difference method,
    the development is very easy because we don''t have to convert our images; we
    only need to apply the difference between the pattern and image and return it.
    If we don''t assume that the pattern is equal to or greater than an image, then
    we will require a few checks and truncate values that can be less than `0` or
    greater than `255`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following images are the results of applying the image light pattern to
    our input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/462bbbc5-adf9-4030-bc5c-58d0e3883a95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the results that we obtain, we can check how the light gradient and the
    possible artifacts are removed. But what happens when we don''t have a light/background
    pattern? There are a few different techniques to obtain this; we are going to
    present the most basic one here. Using a filter, we can create one that can be
    used, but there are better algorithms to learn about the background of images
    where the pieces appear in different areas. This technique sometimes requires
    a background estimation image initialization, where our basic approach can play
    very well. These advanced techniques will be explored in [Chapter 8](58a72603-be5a-465f-aa7b-fc8ab1aae596.xhtml),
    *Video Surveillance, Background Modeling, and Morphological Operations*. To estimate
    the background image, we are going to use a blur with a large kernel size applied
    to our input image. This is a common technique used in **optical character recognition**
    *(***OCR**), where the letters are thin and small relative to the whole document,
    allowing us to do an approximation of the light patterns in the image. We can
    see the light/background pattern reconstruction in the left-hand image and the
    ground truth in the right-hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49a8a85d-84fb-4670-8638-9fe58368fb37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that there are minor differences in the light patterns, but this
    result is enough to remove the background. We can also see the result in the following
    image when using different images. In the following image, the result of applying
    the image difference between the original input image and the estimated background
    image computed with the previous approach is depicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be080517-2a8e-4778-8751-3a22adc85bc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The `calculateLightPattern` function creates this light pattern or background
    approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This basic function applies a blur to an input image by using a big kernel size
    relative to the image size. From the code, it is **one-third** of the original
    width and height.
  prefs: []
  type: TYPE_NORMAL
- en: Thresholding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After removing the background, we only have to binarize the image for future
    segmentation. We are going to do this with threshold. `Threshold` is a simple
    function that sets each pixel''s values to a maximum value (255, for example).
    If the pixel''s value is greater than the **threshold** value or if the pixel''s
    value is lower than the **threshold** value, it will be set to a minimum (0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1576ef1-198b-4e9f-ab66-1f109349699f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we are going to apply the `threshold` function using two different `threshold`
    values: we will use a 30 `threshold` value when we remove the light/background
    because all non-interesting regions are black. This is because we apply background
    removal. We will also a medium value `threshold` (140) when we do not use a light
    removal method, because we have a white background. This last option is used to
    allow us to check the results with and without background removal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are going to continue with the most important part of our application:
    the segmentation. We are going to use two different approaches or algorithms here:
    connected components and find contours.'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting our input image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we are going to introduce two techniques to segment our threshold image:'
  prefs: []
  type: TYPE_NORMAL
- en: Connected components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find contours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these two techniques, we are allowed to extract each **region of interest**
    (**ROI**) of our image where our targets objects appear. In our case, these are
    the nut, screw, and ring.
  prefs: []
  type: TYPE_NORMAL
- en: The connected components algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The connected component algorithm is a very common algorithm that''s used to
    segment and identify parts in binary images. The connected component is an iterative
    algorithm with the purpose of labeling an image using eight or four connectivity
    pixels. Two pixels are connected if they have the same value and are neighbors.
    In an image, each pixel has eight neighbor pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b60dde12-2347-4af3-b0a1-063963874bee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Four-connectivity means that only the **2**, **4**, **5**, and **7** neighbors
    can be connected to the center if they have the same value as the center pixel.
    With eight-connectivity, the **1**, **2**, **3**, **4**, **5**, **6**, **7**,
    and **8** neighbors can be connected if they have the same value as the center
    pixel. We can see the differences in the following example from a four- and eight-connectivity
    algorithm. We are going to apply each algorithm to the next binarized image. We
    have used a small **9 x 9** image and zoomed in to show how connected components
    work and the differences between four- and eight-connectivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26a8c0f5-f584-4d8b-b660-12eb8b7261f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The four-connectivity algorithm detects two objects; we can see this in the
    left image. The eight-connectivity algorithm detects only one object (the right
    image) because two diagonal pixels are connected. Eight-connectivity takes care
    of diagonal connectivity, which is the main difference compared with four-connectivity,
    since this where only vertical and horizontal pixels are considered. We can see
    the result in the following image, where each object has a different gray color
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b40375cf-b26e-4e47-bb95-189154b06334.png)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV brings us the connected components algorithm with two different functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`connectedComponents` (image, labels, connectivity= `8`, type= `CV_32S`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`connectedComponentsWithStats` (image, labels, stats, centroids, connectivity=
    `8`, type= `CV_32S`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both functions return an integer with the number of detected labels, where
    label `0` represents the background. The difference between these two functions
    is basically the information that is returned. Let''s check the parameters of
    each one. The `connectedComponents` function gives us the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: The input image to be labeled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labels**: An output mat that''s the same size as the input image, where each
    pixel has the value of its label, where all OS represents the background, pixels
    with `1` value represent the first connected component object, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connectivity**: Two possible values, `8` or `4`, that represent the connectivity
    we want to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type**: The type of label image we want to use. Only two types are allowed:
    `CV32_S` and `CV16_U`. By default, this is `CV32_S`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `connectedComponentsWithStats` function has two more parameters defined.
    These are stats and centroids:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stats**: This is an output parameter that gives us the following statistical
    values for each label (background inclusive):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_LEFT`: The leftmost `x` coordinate of the connected component object'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_TOP`: The topmost `y` coordinate of the connected component object'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_WIDTH`: The width of the connected component object defined by its
    bounding box'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_HEIGHT`: The height of the connected component object defined by its
    bounding box'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CC_STAT_AREA`: The number of pixels (area) of the connected component object'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Centroids**: The centroid points to the float type for each label, inclusive
    of the background that''s considered for another connected component.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example application, we are going to create two functions so that we
    can apply these two OpenCV algorithms. We will then show the user the obtained
    result in a new image with colored objects in the basic connected component algorithm.
    If we select the connected component with the stats method, we are going to draw
    the respective calculated area that returns this function over each object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define the basic drawing for the connected component function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'First of all, we call the OpenCV `connectedComponents` function, which returns
    the number of objects detected. If the number of objects is less than two, this
    means that only the background object is detected, and then we don''t need to
    draw anything and we can finish. If the algorithm detects more than one object,
    we show the number of objects that have been detected on the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are going to draw all detected objects in a new image with different
    colors. After this, we need to create a new black image with the same input size
    and three channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will loop over each label, except for the `0` value, because this is the
    background:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract each object from the label image, we can create a mask for each
    `i` label using a comparison and save this in a new image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set a pseudo-random color to the output image using the `mask`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After looping all of the images, we have all of the detected objects with different
    colors in our output and we only have to show the output image in a window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result in which each object is painted with different colors or
    a gray value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1ba4e5a-e7ba-4e3e-b50f-1feb158a6dab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we are going to explain how to use the connected components with the `stats`
    OpenCV algorithm and show some more information in the resultant image. The following
    function implements this functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s understand this code. As we did in the non-stats function, we call the
    connected components algorithm, but here, we do this using the `stats` function,
    checking whether we detected more than one object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have two more output results: the stats and centroid variables. Then,
    for each detected label, we are going to show the centroid and area through the
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can check the call to the stats variable to extract the area using the
    column constant `stats.at<int>(I, CC_STAT_AREA)`. Now, like before, we paint the
    object labeled with `i` over the output image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, in the centroid position of each segmented object, we want to draw
    some information (such as the area) on the resultant image. To do this, we use
    the stats and centroid variables using the `putText` function. First, we have
    to create a `stringstream` so that we can add the stats area information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to use `putText`, using the centroid as the text position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The result for this function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10e79360-47ac-4ae7-83c2-67547df40016.png)'
  prefs: []
  type: TYPE_IMG
- en: The findContours algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `findContours` algorithm is one of the most used OpenCV algorithms in regards
    to segment objects. This is because this algorithm was included in OpenCV from
    version 1.0 and gives developers more information and descriptors, including shapes,
    topological organizations, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain each parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: Input binary image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contours**: A contour''s output where each detected contour is a vector of
    points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchy**: This is the optional output vector where the hierarchy of contours
    is saved. This is the topology of the image where we can get the relations between
    each contour. The hierarchy is represented as a vector of four indices, which
    are (next contour, previous contour, first child, parent contour). Negative indices
    are given where the given contour has no relationship with other contours. A more
    detailed explanation can be found at [https://docs.opencv.org/3.4/d9/d8b/tutorial_py_contours_hierarchy.html](https://docs.opencv.org/3.4/d9/d8b/tutorial_py_contours_hierarchy.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode**: This method is used to retrieve the contours:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_EXTERNAL` retrieves only the external contours.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_LIST` retrieves all contours without establishing the hierarchy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_CCOMP` retrieves all contours with two levels of hierarchy, external
    and holes. If another object is inside one hole, this is put at the top of the
    hierarchy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RETR_TREE` retrieves all contours, creating a full hierarchy between contours.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method**: This allows us to use the approximation method for retrieving the
    contour''s shapes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If `CV_CHAIN_APPROX_NONE` is set, then this does not apply any approximation
    to the contours and stores the contour's points.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_CHAIN_APPROX_SIMPLE` compresses all horizontal, vertical, and diagonal
    segments, storing only the start and end points.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CV_CHAIN_APPROX_TC89_L1` and `CV_CHAIN_APPROX_TC89_KCOS` apply the **Telchin**
    **chain** **approximation** algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offset**: This is an optional point value to shift all contours. This is
    very useful when we are working in an ROI and need to retrieve global positions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: The input image is modified by the `findContours` function. Create a
    copy of your image before sending it to this function if you need it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know the parameters of the `findContours` function, let''s apply
    this to our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Let's explain our implementation, line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we don''t need any hierarchy, so we are only going to retrieve
    the external contours of all possible objects. To do this, we can use the `RETR_EXTERNAL`
    mode and basic contour encoding by using the `CHAIN_APPROX_SIMPLE` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the connected component examples we looked at before, first we check how
    many contours we have retrieved. If there are none, then we exit our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we draw the contour for each detected object. We draw this in our
    output image with different colors. To do this, OpenCV gives us a function to
    draw the result of the find contours image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `drawContours` function allows the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: The output image to draw the contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contours**: The vector of contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contour index**: A number indicating the contour to draw. If this is negative,
    all contours are drawn.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Color**: The color to draw the contour.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Thickness**: If it is negative, the contour is filled with the chosen color.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line type**: This specifies whether we want to draw with anti-aliasing or
    another drawing method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchy**: This is an optional parameter that is only needed if you want
    to draw some of the contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max Level**: This is an optional parameter that is only taken into account
    when the hierarchy parameter is available. If it is set to `0`, only the specified
    contour is drawn. If it is `1`, the function draws the current contour and the
    nested contours too. If it is set to `2`, then the algorithm draws all of the
    specified contour hierarchy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Offset**: This is an optional parameter for shifting the contours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of our example can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eea4536f-0250-4228-8f2f-27ddc44bf863.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the basics of object segmentation in a controlled
    situation where a camera takes pictures of different objects. Here, we learned
    how to remove background and light to allow us to binarize our image better, thus
    minimizing the noise. After binarizing the image, we learned about three different
    algorithms that we can use to divide and separate each object of one image, allowing
    us to isolate each object to manipulate or extract features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see this whole process in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/617ededa-c684-4f4c-b460-2d1fe8d99849.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we extracted all of the objects on an image. You will need to do this
    to continue with the next chapter, where we are going to extract characteristics
    of each of these objects to train a machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to predict the class of any objects in an
    image and then call a robot or any other system to pick any of them, or detect
    an object that is not in the correct carrier tape. We will then look at notifying
    a person to pick it up.
  prefs: []
  type: TYPE_NORMAL
