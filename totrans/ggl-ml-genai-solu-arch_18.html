<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer202" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-309"><a id="_idTextAnchor371" class="calibre6 pcalibre pcalibre1"/>15</h1>
<h1 id="_idParaDest-310" class="calibre5"><a id="_idTextAnchor372" class="calibre6 pcalibre pcalibre1"/>Introduction to Generative AI</h1>
<p class="calibre3"><strong class="bold">Generative artificial intelligence</strong> (<strong class="bold">GenAI</strong>) is <a id="_idIndexMarker1748" class="calibre6 pcalibre pcalibre1"/>certainly the term on everybody’s lips at the moment. If you haven’t already had an opportunity to “get your hands dirty” with GenAI, then you will soon learn why it is currently taking the world by storm as we dive into the kinds of amazing things we can do with this relatively new set of technologies. In this chapter, we will explore some of the concepts underpinning what GenAI is and its distinctions from other <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>)/<strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) approaches. We’ll also cover some of the major <a id="_idIndexMarker1749" class="calibre6 pcalibre pcalibre1"/>historical <a id="_idIndexMarker1750" class="calibre6 pcalibre pcalibre1"/>developments that have led to its meteoric rise, and examples of how it is being <span>used today.</span></p>
<p class="calibre3">We’ll begin the chapter by introducing some fundamental concepts before moving on to more complex topics and the evolution of various GenAI approaches, such as <strong class="bold">autoencoders</strong> (<strong class="bold">AEs</strong>), <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>), diffusion, and <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>). Considering <a id="_idIndexMarker1751" class="calibre6 pcalibre pcalibre1"/>that this is an <a id="_idIndexMarker1752" class="calibre6 pcalibre pcalibre1"/>introductory chapter, we will mainly <a id="_idIndexMarker1753" class="calibre6 pcalibre pcalibre1"/>lay the groundwork for the deeper dives that will follow in later chapters. Specifically, this chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Fundamentals <span>of GenAI</span></li>
<li class="calibre8">GenAI techniques <span>and evolution</span></li>
<li class="calibre8"><span>LLMs</span></li>
</ul>
<p class="calibre3">As makes logical sense, let’s begin with <span>the fundamentals!</span></p>
<h1 id="_idParaDest-311" class="calibre5"><a id="_idTextAnchor373" class="calibre6 pcalibre pcalibre1"/>Fundamentals of GenAI</h1>
<p class="calibre3">This section<a id="_idIndexMarker1754" class="calibre6 pcalibre pcalibre1"/> introduces the basic concepts we need to understand when discussing GenAI, starting with an overview of what <span>GenAI is!</span></p>
<h2 id="_idParaDest-312" class="calibre9"><a id="_idTextAnchor374" class="calibre6 pcalibre pcalibre1"/>What is GenAI?</h2>
<p class="calibre3">I’ll begin explaining<a id="_idIndexMarker1755" class="calibre6 pcalibre pcalibre1"/> this topic by focusing on what distinguishes GenAI from the other AI/ML approaches we’ve covered. Think about all of the various algorithms and approaches I’ve described so far throughout this book, and, more specifically, think about the primary goal that was being pursued in each approach. Whether we were using linear regression in <strong class="source-inline">scikit-learn</strong> to predict a numeric value for some target variable based on its features, logistic regression in XGBoost to implement a binary classifier model or using time-series data to predict the future in TensorFlow, there is one common theme, which is that we were trying to <strong class="bold">predict</strong> or <span><strong class="bold">estimate</strong></span><span> something.</span></p>
<p class="calibre3">The key concept to understand here is that the output from our various sophisticated models was generally a distinct data point that was either right or wrong, or at least as close to right as possible. Our models typically produced a single, simple answer based on relationships the model had learned (or estimated) in <span>the data.</span></p>
<p class="calibre3">Even in the<a id="_idIndexMarker1756" class="calibre6 pcalibre pcalibre1"/> case of <strong class="bold">unsupervised learning</strong> (<strong class="bold">UL</strong>) approaches such as K-means clustering, the model simply finds mathematical relationships in the data and categorizes the data points into groups based on <span>those relationships.</span></p>
<p class="calibre3">The huge leap forward that has been achieved with the introduction of GenAI is that the models can now go beyond simple “yes”/“no” answers or numeric estimates based on sheer mathematical number crunching and pattern recognition. With GenAI, the models can now <a id="_idIndexMarker1757" class="calibre6 pcalibre pcalibre1"/>create (or <strong class="bold">generate</strong>) new data. That’s the big difference, and it turns out that the implications of this are <span>pretty huge!</span></p>
<p class="calibre3">Taking a step back for a moment: when I was young, I thought that AI research could design machines that think like humans, and that fascinated me, so I started learning about how ML algorithms work. I was quite disappointed to learn that, although some models can do an amazing job at “understanding” consumer behavior and accurately recommending products that a given person might like to purchase, or even diagnose potential illnesses based on input data related to a patient, it was all just based on feeding large amounts of data into a mathematical algorithm that “learns” to detect a pattern in the data. There was no actual “intelligence” there, although the science is <span>still fascinating.</span></p>
<p class="calibre3">The fact that GenAI models can go beyond specific mathematical answers and create new content is a significant leap forward in the pursuit of AI. Have a chat with any of the latest and greatest GenAI models out there, and I don’t doubt for a moment that you will be very impressed by the kinds of mind-boggling things they can do, such as composing music, painting an imaginative scene, writing a catchy poem, or creating a web application. You’ll learn how to harness GenAI for implementing many different kinds of use cases later in this book, and I’m sure you’ll agree that it’s a dramatic <span>technological advancement.</span></p>
<p class="calibre3">It’s important to understand, however, that the amazing feats performed by GenAI models are still powered by many of the AI/ML concepts we have already covered in this book. At its core, GenAI works by understanding and replicating the underlying patterns and structures in the data it has been trained on, and it still uses algorithms<a id="_idIndexMarker1758" class="calibre6 pcalibre pcalibre1"/> and <strong class="bold">neural networks</strong> (<strong class="bold">NNs</strong>) to perform these kinds of activities, although the network architectures used, and the novel ways in which they are used, are considerably more advanced. In the case of GenAI, the goal is not just to interpret the data but to form an understanding<a id="_idIndexMarker1759" class="calibre6 pcalibre pcalibre1"/> that can be used to create something new. Let’s dive into these topics in more detail to better understand what sets GenAI apart from other <span>AI approaches.</span></p>
<h2 id="_idParaDest-313" class="calibre9"><a id="_idTextAnchor375" class="calibre6 pcalibre pcalibre1"/>What is non-GenAI?</h2>
<p class="calibre3">Now that GenAI has <a id="_idIndexMarker1760" class="calibre6 pcalibre pcalibre1"/>taken the world by storm, how can we refer to all of the other AI/ML approaches that existed before it (that is, much of the stuff we already covered in this book)? One of the most popular emerging terms for this is “traditional AI/ML.” You may also hear it being referred to as “predictive AI/ML” because the goal is generally to predict something, or “discriminative AI/ML” in the case of classification <span>use cases.</span></p>
<h2 id="_idParaDest-314" class="calibre9"><a id="_idTextAnchor376" class="calibre6 pcalibre pcalibre1"/>Diving deeper into GenAI versus non-GenAI</h2>
<p class="calibre3">To understand the<a id="_idIndexMarker1761" class="calibre6 pcalibre pcalibre1"/> basic differences between GenAI and non-GenAI, we need<a id="_idIndexMarker1762" class="calibre6 pcalibre pcalibre1"/> to revisit some of the mathematical concepts that underpin AI/ML. Don’t worry – we’ll just cover the concepts at a level required to define the distinctions between GenAI <span>and non-GenAI.</span></p>
<h3 class="calibre11">The role of probability</h3>
<p class="calibre3">Remember that <a id="_idIndexMarker1763" class="calibre6 pcalibre pcalibre1"/>mathematical probability is a fundamental concept in many ML use cases. For example, when we ask a model to classify a data point into a specific category, it rarely will do so with 100% certainty. Instead, it will calculate the probability of the data point belonging to each category. Then we, or the model itself, depending on the implementation, can select the category with the <span>highest probability.</span></p>
<p class="calibre3">One of the key factors that distinguishes GenAI from traditional AI is how probability is used in the learning <a id="_idIndexMarker1764" class="calibre6 pcalibre pcalibre1"/>process. Let’s explore this in more detail, starting with <span>traditional AI.</span></p>
<h4 class="calibre20">Traditional AI and conditional probability</h4>
<p class="calibre3">In the case of <a id="_idIndexMarker1765" class="calibre6 pcalibre pcalibre1"/>traditional AI, the models try to estimate the probability that the target variable <strong class="source-inline">Y</strong> contains a specific value based on the values of the predictor variables (or features) in the dataset. This is referred to<a id="_idIndexMarker1766" class="calibre6 pcalibre pcalibre1"/> as the <strong class="bold">conditional probability</strong> of the target variable’s values based on the values of the input features. Conditional probability is the probability of an event occurring given that another event has already happened, and it is represented by the <span>following formula:</span></p>
<p class="calibre3">P(B|A) = P(A∩B) / <span>P(A)</span></p>
<p class="calibre3">Here, the <span>following applies:</span></p>
<ul class="calibre16">
<li class="calibre8">P(B|A) is the conditional probability of B <span>given A.</span></li>
<li class="calibre8">P(A ∩ B) is the probability of both A and B occurring. This is also referred to as the “joint probability,” which we will explore in more <span>detail later.</span></li>
<li class="calibre8">P(A) is the probability of <span>A occurring.</span></li>
</ul>
<p class="calibre3">To<a id="_idIndexMarker1767" class="calibre6 pcalibre pcalibre1"/> understand <a id="_idIndexMarker1768" class="calibre6 pcalibre pcalibre1"/>conditional probability, imagine a game scenario in which there are three straws hidden from our view. Of the three straws, two are long straws, and one is a short straw. We will take turns picking straws, and whoever picks the short straw loses. Each time it’s our turn to pick a straw, there’s a specific probability that we will pick the <span>short straw.</span></p>
<p class="calibre3">Initially, no straws were picked, so in the first turn, the probability of picking the short straw is 1/3 (or approximately 33.3%). Now, let’s imagine that we pick a straw, and it turns out to be a long straw. This means that two straws remain, and since we picked a long straw, it means that one long straw remains, as well as the short straw. In the next turn, the probability of picking the short straw is, therefore, 1/2 (<span>or 50%).</span></p>
<p class="calibre3">The important thing to note here is that since we already picked a long straw, it influences the probability distribution of the overall scenario. This is a very simple example of the concept. In ML, there can be many factors (that is, features) involved, leading to very large numbers of potential combinations of those factors that could influence the outcome. This is why ML models usually need to crunch through large amounts of data to learn patterns in those various combinations <span>of features.</span></p>
<p class="calibre3">Mapping this back to ML use cases, consider the dataset represented in <span><em class="italic">Figure 15</em></span><em class="italic">.1</em>, which shows a simplified version of the Titanic dataset from OpenML (<a href="https://www.openml.org/search?type=data&amp;id=40945" class="calibre6 pcalibre pcalibre1">https://www.openml.org/search?type=data&amp;id=40945</a>). The target variable is represented by the <strong class="source-inline">survived</strong> column, which is highlighted by the green box. This represents <em class="italic">B</em> in our preceding equation. All of the other columns combined constitute the input features, and they represent <em class="italic">A</em> in our previous equation, as highlighted by the red box. Essentially, this depicts that when an ML model learns to predict the value of the target variable based on the values of the input variables, it is asking the question: “What is the probability of <em class="italic">B</em> (that is, the target variable), given the values of <em class="italic">A</em> (that is, the input variables)?” In <a id="_idIndexMarker1769" class="calibre6 pcalibre pcalibre1"/>other<a id="_idIndexMarker1770" class="calibre6 pcalibre pcalibre1"/> words: “What is the probability of <em class="italic">B</em>, <span>given </span><span><em class="italic">A</em></span><span>?”:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer194">
<img alt="Figure 15.1: Separation of target variable from input feature" src="image/B18143_15_1.jpg" class="calibre183"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.1: Separation of target variable from input feature</p>
<p class="calibre3">Next, let’s look at how probability can be used in a slightly different way in GenAI <span>use cases.</span></p>
<h4 class="calibre20">GenAI and joint probability</h4>
<p class="calibre3">In the case of GenAI, the models are designed to learn the <strong class="bold">joint probability distribution</strong> of the <a id="_idIndexMarker1771" class="calibre6 pcalibre pcalibre1"/>dataset. As we briefly saw in the preceding equation for conditional probability, joint probability refers to the probability of two or more events happening at the same time. We can use this to understand how different variables or events are connected and how they influence each other. It is represented as <span>the following:</span></p>
<p class="calibre3">P(A∩B) = P(A) × <span>P(B)</span></p>
<p class="calibre3">Here, the <span>following applies:</span></p>
<ul class="calibre16">
<li class="calibre8">P(A) is the probability of <span>A occurring</span></li>
<li class="calibre8">P(B) is the probability of <span>B occurring</span></li>
</ul>
<p class="calibre3">Let’s again use an example to describe this concept in more detail. A common analogy is to imagine rolling two fair six-sided dice and calculating the probability of both dice showing a certain number; <span>for example:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Event A</strong>: The first die shows <span>a 3</span></li>
<li class="calibre8"><strong class="bold">Event B</strong>: The second die shows <span>a 5</span></li>
</ul>
<p class="calibre3">Here, the joint probability P(A ∩ B) is the probability of both the first die showing a 3 and the second die showing a 5 when they come to rest. Since each die roll is independent and each side has a 1/6 chance of appearing, the joint probability is <span>the following:</span></p>
<p class="calibre3">P(A∩B) = P(A) × P(B) = 1/6 x 1/6 = <span>1/36</span></p>
<p class="calibre3">This means there’s a 1/36 chance that both dice will show the exact numbers you predicted. The key thing to understand here is the events are independent, but there’s a shared<a id="_idIndexMarker1772" class="calibre6 pcalibre pcalibre1"/> probability <a id="_idIndexMarker1773" class="calibre6 pcalibre pcalibre1"/>distribution that governs the <span>overall outcome.</span></p>
<p class="calibre3">Mapping this back to ML use cases again, the joint probability distribution in a dataset includes the target variable as well as the input variables, as represented by the red box in <span><em class="italic">Figure 15</em></span><span><em class="italic">.2</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer195">
<img alt="Figure 15.2: Joining target variable with input features" src="image/B18143_15_2.jpg" class="calibre183"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.2: Joining target variable with input features</p>
<p class="calibre3">A major difference here is that, while discriminative models use conditional probability to predict the value of the target variable given the values of the input variables, generative models try to learn the overall joint probability distribution of the dataset, including the <span>target variable.</span></p>
<p class="calibre3">By learning the overall joint probability distribution of the dataset, the model can develop an understanding of how the dataset is constructed and how all of the features relate to each other, including the target variable. In this manner, by accurately approximating how the training dataset is constructed, it can estimate how to create similar data points that do not already exist in the dataset but are of similar structure and composition – that is, it can generate new, <span>similar content.</span></p>
<p class="calibre3">In addition to the joint probability distribution, today’s generative models learn hidden relationships and structure in the dataset by using the attention mechanism outlined in the famous <em class="italic">Attention Is All You Need</em> (Vaswani et al., 2017) paper that we’ve referenced numerous times in this book. Before the attention mechanism was developed, models mostly treated all inputs equally. And, considering that all input features (and related hidden features) carry information, treating all inputs equally results in a low signal-to-noise ratio that limits the effectiveness of the learning and prediction processes. For example, when a model tries to predict the next word in a sentence, not all prior words in the sentence contribute equally to predicting the next word. Instead, the most <a id="_idIndexMarker1774" class="calibre6 pcalibre pcalibre1"/>probable <a id="_idIndexMarker1775" class="calibre6 pcalibre pcalibre1"/>next word may be more dependent on (or more highly correlated with) the existence of specific other words in the sentence. For example, consider the sentence: “I added a pinch of salt and a dash of [BLANK].” When a modern generative model attempts to predict the next word in that sentence, it will likely predict the word “pepper.” While the word “pepper” must make sense in the overall context of the sentence (that is, it will have varying degrees of contextual relationships to the other words in the sentence), the word “salt” likely has a higher impact on the prediction than other words, such as “pinch,” and the attention mechanism helps the models to learn these kinds of <span>important relationships.</span></p>
<p class="calibre3">While this section focuses mainly on the differences between GenAI and “traditional AI,” I also want to mention that the demarcation is not always so clear, and sometimes the lines may <span>be blurred.</span></p>
<h3 class="calibre11">Blurring the lines</h3>
<p class="calibre3">It’s important to understand that the demarcations between non-GenAI and GenAI often become blurred because many applications combine both approaches. For example, a model or application that analyzes text and generates a summary may use both discriminative and <span>generative processes.</span></p>
<p class="calibre3">Consider chatbots as an example. Chatbots commonly generate responses by constructing sentences “on the fly.” A popular approach in constructing sentences is to predict the most appropriate next word based on previous words in the sentence. This is an instance of using conditional probability. However, if the application or model only did this, then its abilities would be quite limited. In order to generate complex and coherent responses that<a id="_idIndexMarker1776" class="calibre6 pcalibre pcalibre1"/> adhere to <strong class="bold">natural language</strong> (<strong class="bold">NL</strong>) constructs, the model needs to have learned an accurate representation of the structure (that is, the joint probability distribution) of the language in which it <span>generates responses.</span></p>
<p class="calibre3">An additional interesting scenario is the use of GenAI to produce new data that can then be used as features for traditional ML models. An example of this use case, as suggested by my colleague, Jeremy Wortz, would be to ask a generative model to rate songs with a score (let’s say between 1 and 5) based on a customer persona, and then make playlists with those generated features from the ratings and outputs from a process called “chain-of-thought reasoning,” which essentially asks the LLM to elaborate on its thought process. I will describe the chain-of-thought concept in more <span>detail later.</span></p>
<p class="calibre3">Now that we’ve discussed<a id="_idIndexMarker1777" class="calibre6 pcalibre pcalibre1"/> the<a id="_idIndexMarker1778" class="calibre6 pcalibre pcalibre1"/> differences between GenAI and “traditional AI,” we will explore the development of techniques that have led to the evolution <span>of GenAI.</span></p>
<h1 id="_idParaDest-315" class="calibre5"><a id="_idTextAnchor377" class="calibre6 pcalibre pcalibre1"/>GenAI techniques and evolution</h1>
<p class="calibre3">As we’ve already<a id="_idIndexMarker1779" class="calibre6 pcalibre pcalibre1"/> discussed, while there are some distinctions between GenAI and “traditional AI,” they share much of the same history. For example, in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, we discussed a brief history of AI/ML, and in <em class="italic">Chapters 9</em> and <em class="italic">14</em>, we learned about the evolution of various types of NNs, such<a id="_idIndexMarker1780" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">recurrent NNs</strong> (<strong class="bold">RNNs</strong>), <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) networks, and Transformers. All of those concepts and <a id="_idIndexMarker1781" class="calibre6 pcalibre pcalibre1"/>milestones also apply to the history and evolution of GenAI. Considering that GenAI is a relatively newer subset of AI, we can view its evolutionary timeline as an extension of the evolution of AI in general. Therefore, the topics in this section build upon what we’ve already covered in <span>that regard.</span></p>
<p class="calibre3">The evolution of GenAI could comprise an entire book by itself, and for the purpose of this current book, it would be an unnecessary level of information to cover all of the major milestones and developments that contributed to the evolution of GenAI in great detail. Instead, I will summarize some of the most prominent milestones and contributory <a id="_idIndexMarker1782" class="calibre6 pcalibre pcalibre1"/>developments <a id="_idIndexMarker1783" class="calibre6 pcalibre pcalibre1"/>at a high<a id="_idIndexMarker1784" class="calibre6 pcalibre pcalibre1"/> level, such as Markov chains and <strong class="bold">hidden Markov models</strong> (<strong class="bold">HMMs</strong>), <strong class="bold">restricted Boltzmann machines</strong> (<strong class="bold">RBMs</strong>), <strong class="bold">deep belief networks</strong> (<strong class="bold">DBNs</strong>), AEs, <span>and GANs.</span></p>
<p class="calibre3">It’s important also to note that some mechanisms that were originally developed primarily for discriminative use cases can be reappropriated for generative use cases. For example, while simple Naïve Bayes classifiers are commonly used to predict a given class based on the features in the dataset (that is, an application of conditional probability for discriminative use cases), the algorithm is also capable of learning an approximation of the joint probability distribution of the dataset during training due to how it applies Bayes’ Theorem with the (naïve) assumption that each feature is independent. (To learn more about how Naïve Bayes classifiers work, I recommend reviewing the paper at the following URL: <a href="https://doi.org/10.48550/arXiv.1404.0933" class="calibre6 pcalibre pcalibre1">https://doi.org/10.48550/arXiv.1404.0933</a>). A similar reappropriation can be done with more complex applications of Bayes’ Theorem, such as <span>Bayesian networks.</span></p>
<p class="calibre3">Before diving into <a id="_idIndexMarker1785" class="calibre6 pcalibre pcalibre1"/>specific GenAI approaches, I want to introduce two important concepts that will form the foundation for many of the topics that we will discuss throughout the remainder of this book, referred to as <strong class="bold">embeddings</strong> and <span><strong class="bold">latent space</strong></span><span>.</span></p>
<h2 id="_idParaDest-316" class="calibre9"><a id="_idTextAnchor378" class="calibre6 pcalibre pcalibre1"/>Embeddings and latent space</h2>
<p class="calibre3">In <a href="B18143_07.xhtml#_idTextAnchor215" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 7</em></span></a>, we discussed the topic of dimensionality reduction, and we used mechanisms such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) to project features from our datasets into lower-dimensional feature spaces. These lower-dimensional feature spaces can be referred to as “latent spaces,” and the representations of our data in the latent space can be referred to as “embeddings.” Allow me to explain these important concepts in more detail, starting <span>with embeddings.</span></p>
<h3 class="calibre11">Embeddings</h3>
<p class="calibre3">Embeddings are<a id="_idIndexMarker1786" class="calibre6 pcalibre pcalibre1"/> numerical representations<a id="_idIndexMarker1787" class="calibre6 pcalibre pcalibre1"/> of data in a lower-dimensional space (that is, the latent space). They can be seen as numerical “fingerprints” that capture the meaning or characteristics of the original data. For example, word embeddings capture the meaning of words, and if words such as “king” and “queen” have similar embeddings, a language model can infer a relationship between the two. Models can also embed other types of data, such as images, audio, <span>and graphs.</span></p>
<p class="calibre3">Next, let’s dive into the concept of latent space in <span>more detail.</span></p>
<h3 class="calibre11">Latent space</h3>
<p class="calibre3">We use the term “latent space” to define the abstract feature space where the intrinsic properties or<a id="_idIndexMarker1788" class="calibre6 pcalibre pcalibre1"/> features <a id="_idIndexMarker1789" class="calibre6 pcalibre pcalibre1"/>of a dataset are represented. This space captures underlying structure and patterns within the data that might not be apparent in its original form (hence the <span>term “latent”).</span></p>
<p class="calibre3">It’s important to understand that the latent space and its dimensions map in some way to the original features. This means that relationships among the original features are captured as relationships among the projected features in the latent space (that is, semantic context is represented in some way). These representations and mappings are generally learned by models during the training process and are therefore often not easily interpretable by humans, but there are also methods by which we can explicitly create embeddings for the contents of our datasets, which I will describe in more detail in <a href="B18143_16.xhtml#_idTextAnchor383" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 16</em></span></a>. For now, I’ll briefly explain how embeddings and latent spaces <span>are used.</span></p>
<h3 class="calibre11">Using embeddings and latent space</h3>
<p class="calibre3">When models have<a id="_idIndexMarker1790" class="calibre6 pcalibre pcalibre1"/> created<a id="_idIndexMarker1791" class="calibre6 pcalibre pcalibre1"/> embeddings<a id="_idIndexMarker1792" class="calibre6 pcalibre pcalibre1"/> for our data, these representations in the latent space can be used in interesting ways. One of the most useful ways to use this data is to measure the distances between embeddings in the latent space, using familiar distance metrics such as Euclidean distance or cosine similarity. Considering that the embeddings in the latent space capture the essence of the concepts they represent, we can identify concepts that may be similar or related to the original space. An example would be products in a retail website’s product catalog. By embedding the product details and identifying which ones are near each other in the latent space, our models can get an understanding of which products may be related to each other. A recommender system could then use this<a id="_idIndexMarker1793" class="calibre6 pcalibre pcalibre1"/> information to display insights such as “customers who purchased this item also purchased these <span>other items.”</span></p>
<p class="calibre3">You may ask, “Why not just perform the same kinds of actions in the original space?” The process of encoding embeddings converts the objects to vectors, providing a much more efficient way to represent the concepts. Also, rather than dealing with words and images, ML algorithms and models love to work with vectors, so these vectorized representations are more well suited to ML <span>use cases.</span></p>
<p class="calibre3">Another example that highlights the efficiency of embeddings is when we use ML models for image-processing use cases. Consider high-definition images that contain millions of pixels, in which each pixel is considered a feature. If we have a dataset consisting of hundreds of millions of images, and each image has millions of features, this could lead to extremely compute-intensive training and inference processing. Instead, mapping the features to a lower-dimensional feature space will significantly optimize processing efficiency. Also, individual pixels often don’t convey much meaning; rather, it’s often the relationships between pixels and patterns that define what an <span>image represents.</span></p>
<p class="calibre3">We’ll revisit the concepts of embeddings and the latent space many more times throughout the rest of this book because they are such foundational concepts in GenAI. Now that I’ve<a id="_idIndexMarker1794" class="calibre6 pcalibre pcalibre1"/> introduced<a id="_idIndexMarker1795" class="calibre6 pcalibre pcalibre1"/> these<a id="_idIndexMarker1796" class="calibre6 pcalibre pcalibre1"/> important concepts, we’ll begin our journey through various important milestones and approaches that led to the development of GenAI, as outlined at a high level in <span><em class="italic">Figure 15</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer196">
<img alt="Figure 15.3: Milestones in GenAI evolution" src="image/B18143_15_3.jpg" class="calibre184"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.3: Milestones in GenAI evolution</p>
<p class="calibre3">Of course, many additional GenAI models and approaches have been developed and released in the <a id="_idIndexMarker1797" class="calibre6 pcalibre pcalibre1"/>past<a id="_idIndexMarker1798" class="calibre6 pcalibre pcalibre1"/> few <a id="_idIndexMarker1799" class="calibre6 pcalibre pcalibre1"/>decades beyond those shown in <span><em class="italic">Figure 15</em></span><em class="italic">.3</em>, but here I’m focusing on high-level milestones and developments that have most notably influenced the evolution of GenAI. Let’s begin diving into each one, starting with Markov chains <span>and HMMs.</span></p>
<h3 class="calibre11">Markov chains and HMMs</h3>
<p class="calibre3">The concept of <a id="_idIndexMarker1800" class="calibre6 pcalibre pcalibre1"/>Markov chains was introduced by<a id="_idIndexMarker1801" class="calibre6 pcalibre pcalibre1"/> Andrey Markov in 1906, and <a id="_idIndexMarker1802" class="calibre6 pcalibre pcalibre1"/>they<a id="_idIndexMarker1803" class="calibre6 pcalibre pcalibre1"/> are based on what’s referred to as the Markov property, which posits that the next or future state in a sequence or scenario depends only on the current state. They can be used to predict (and therefore generate) the next item in a sequence. HMMs extend this concept to include hidden states that cannot be directly observed in a given scenario. A very simple example would be the potential correlation between current weather conditions and umbrella sales. If we can directly observe the weather, and we see that it is currently raining, then that’s an observable state we can use in a Markov chain to predict an increase in umbrella sales. On the other hand, if we cannot observe the weather for some reason (perhaps we’re working in a store that’s below ground level in a mall) but we notice an increase in umbrella sales, then we could surmise that it’s currently raining outside. In this case, the state of the weather is hidden, but we can speculate the probability of rain based on the secondary observable state of increased umbrella sales. This would be a very simple representation of a hidden state in <span>an HMM.</span></p>
<p class="calibre3">The next concept I’d like to introduce briefly <span>is RBMs.</span></p>
<h3 class="calibre11">RBMs</h3>
<p class="calibre3"><strong class="bold">Boltzmann machines</strong> (<strong class="bold">BMs</strong>) are a type of <strong class="bold">energy-based model</strong> (<strong class="bold">EBM</strong>) that borrows<a id="_idIndexMarker1804" class="calibre6 pcalibre pcalibre1"/> some concepts from physics. “Boltzmann” refers to<a id="_idIndexMarker1805" class="calibre6 pcalibre pcalibre1"/> Ludwig Boltzmann, a physicist associated<a id="_idIndexMarker1806" class="calibre6 pcalibre pcalibre1"/> with<a id="_idIndexMarker1807" class="calibre6 pcalibre pcalibre1"/> statistical mechanics, which uses statistics and probability theory to model the behavior of microscopic particles. The Boltzmann probability distribution (also called the Gibbs distribution) provides the probability of a system being in a particular state based on its energy and temperature. A key concept here is that states with lower energy are more likely to occur than states with <span>higher energy.</span></p>
<p class="calibre3">Mapping this to data science: Unlike the conditional probability used in traditional AI as described earlier in this chapter, EBMs use an “energy function” to assign a value to every possible configuration of variables, where lower “energy” values represent a more likely or <span>desirable configuration.</span></p>
<p class="calibre3">Due to the work by Geoffrey Hinton and Ruslan Salakhutdinov, BMs were refined to form RBMs, which are a<a id="_idIndexMarker1808" class="calibre6 pcalibre pcalibre1"/> type of <strong class="bold">artificial NN</strong> (<strong class="bold">ANN</strong>) that consists of <span>two layers:</span></p>
<ul class="calibre16">
<li class="calibre8">The visible layer, which represents the input data (for example, the pixels in an image or the words in <span>a sentence).</span></li>
<li class="calibre8">The hidden layer, which learns to capture higher-level features and dependencies in the data. This relates to the concept of “latent space” that we <span>introduced earlier.</span></li>
</ul>
<p class="calibre3">With RBMs, the learning process involves adjusting the weights between the visible and hidden layers to minimize what’s referred to as<a id="_idIndexMarker1809" class="calibre6 pcalibre pcalibre1"/> the <strong class="bold">reconstruction error</strong>, which measures how well the RBM can reproduce the original input data. By doing this, the RBM learns to model the probability distribution of the <span>input data.</span></p>
<p class="calibre3">In the original BM, all of the layers were connected, and the models were quite complex and compute-intensive to train. However, in RBMs, while the visible layers and hidden layers are fully connected to each other, there are no connections within a layer (hence the term “restricted”), which makes them less computationally intensive <span>to train.</span></p>
<p class="calibre3">RBMs are commonly used for UL use cases, especially for feature extraction and dimensionality reduction, and they can also be seen as a kind of building block for <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>), which <a id="_idIndexMarker1810" class="calibre6 pcalibre pcalibre1"/>can be used to build many other kinds of models, including<a id="_idIndexMarker1811" class="calibre6 pcalibre pcalibre1"/> classification and<a id="_idIndexMarker1812" class="calibre6 pcalibre pcalibre1"/> regression. The idea of using RBMs as building blocks brings me to the next <span>topic: DBNs.</span></p>
<h3 class="calibre11">DBNs</h3>
<p class="calibre3">DBNs can be viewed<a id="_idIndexMarker1813" class="calibre6 pcalibre pcalibre1"/> as a composition of simple, unsupervised<a id="_idIndexMarker1814" class="calibre6 pcalibre pcalibre1"/> networks such as RBMs, where each sub-network’s hidden layer serves as the visible layer for the next sub-network. By stacking multiple RBMs, each layer can learn increasingly abstract and complex representations of <span>the data.</span></p>
<p class="calibre3">Training DBNs consists of two main phases: pre-training <a id="_idIndexMarker1815" class="calibre6 pcalibre pcalibre1"/>and <strong class="bold">fine-tuning</strong> (<strong class="bold">FT</strong>). During pre-training, the DBN is trained one layer at a time in an unsupervised manner. Each layer is trained as an RBM, which learns to represent the features passed from the layer below. This layer-wise pre-training helps initialize the weights of the network in a way that makes the subsequent FT phase <span>more effective.</span></p>
<p class="calibre3">After pre-training, a DBN can sample from the representations it has learned to generate new data similar to the original dataset. We can also use <strong class="bold">supervised learning</strong> (<strong class="bold">SL</strong>) approaches to<a id="_idIndexMarker1816" class="calibre6 pcalibre pcalibre1"/> fine-tune the network for more <span>specific applications.</span></p>
<p class="calibre3">Each of the techniques and algorithms I’ve outlined so far in this chapter are what I would consider to be fundamental milestones and conceptual building blocks that contribute to the development of the more advanced GenAI applications we see today. The next two approaches I will outline are significant steps forward in the evolution of GenAI. The <a id="_idIndexMarker1817" class="calibre6 pcalibre pcalibre1"/>first I <a id="_idIndexMarker1818" class="calibre6 pcalibre pcalibre1"/>will introduce is the concept <span>of AEs.</span></p>
<h3 class="calibre11">AEs</h3>
<p class="calibre3">Most – if not all – of the<a id="_idIndexMarker1819" class="calibre6 pcalibre pcalibre1"/> datasets we use for ML use cases<a id="_idIndexMarker1820" class="calibre6 pcalibre pcalibre1"/> represent specific concepts with intrinsic properties (or features); for example, people, cars, medical images, or even more specific concepts, such as people who boarded the Titanic or items purchased on a <span>company’s website.</span></p>
<p class="calibre3">In the case of discriminative ML use cases, the models generally try to predict some kind of output based on the values of the features associated with each instance of the concept being represented. For example, which customers will likely purchase a particular product based on their prior <span>purchasing history?</span></p>
<p class="calibre3">In the case of GenAI, on the other hand, the models are often expected to generate new data points that validly represent the concept originally represented in the training dataset. For example, having been trained on images of many different concepts, such as cats and motorcycles, the model may be asked to draw a cartoon image of a cat on a motorcycle. To do this, the model needs to “understand” what those <span>concepts are.</span></p>
<p class="calibre3">Earlier in this chapter, we introduced the topics of embeddings and the latent space, in which latent representations of the data points in our dataset are defined. The process of creating an embedding in the latent space can be referred to <a id="_idIndexMarker1821" class="calibre6 pcalibre pcalibre1"/>as <strong class="bold">encoding</strong>, and AEs are a type of ANN used for UL of efficient representations (encodings) for our datasets. At a high level, an AE takes input data, converts it into a smaller, dense representation that captures the essence of the data, and then reconstructs the input data as closely as possible from this <a id="_idIndexMarker1822" class="calibre6 pcalibre pcalibre1"/>representation. Let’s dive into how <span>they work.</span></p>
<h4 class="calibre20">How AEs work</h4>
<p class="calibre3">AEs are generally made<a id="_idIndexMarker1823" class="calibre6 pcalibre pcalibre1"/> up of three main components: the <strong class="bold">encoder</strong>, the latent space (also referred to as<a id="_idIndexMarker1824" class="calibre6 pcalibre pcalibre1"/> the <strong class="bold">bottleneck</strong>), and the <strong class="bold">decoder</strong>, as depicted in <span><em class="italic">Figure 15</em></span><span><em class="italic">.4</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer197">
<img alt="Figure 15.4: AE architecture" src="image/B18143_15_4.jpg" class="calibre185"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.4: AE architecture</p>
<p class="calibre3">During the forward pass in an AE, each input data point is passed through the encoder part of the network. This section of the network compresses the input into a smaller, dense representation in the latent space (hidden layer). The encoded data is then passed through the decoder part of the network, and the decoder tries to reconstruct the original input data from the compressed representation. This is the interesting difference between AEs and traditional, discriminative models – that is, while traditional models generally try to predict an output (<em class="italic">Y</em>) based on an input (<em class="italic">X</em>), AEs try to predict (or generate) the original <span>input (</span><span><em class="italic">X</em></span><span>).</span></p>
<p class="calibre3">Just as we did in traditional NNs, we can calculate the difference between the output value and the expected value (<em class="italic">X</em>), which is referred to as <a id="_idIndexMarker1825" class="calibre6 pcalibre pcalibre1"/>the <strong class="bold">reconstruction error</strong>, and then we can use backpropagation to update the network and continue the training cycle as usual. The quality of the reconstruction depends on how well the AE has learned to represent the data in the <span>latent space.</span></p>
<p class="calibre3">During this training process, the encoder learns to retain only the most relevant features of the data, effectively learning a new way to represent the input data in a smaller-dimensional space. The latent space then contains the compressed knowledge that the network has learned about the data – that is, the essence of the data. To again paraphrase a description by my colleague, Jeremy Wortz, the concept of bottleneck features relates to the quality of the embeddings; since we are typically “squeezing” our most critical information between the encoder and decoder, feature importance is <span>implicitly optimized.</span></p>
<p class="calibre3">Note, however, that while AEs can learn to reconstruct the original dataset and thereby learn how to perform dimensionality reduction to accurately represent the data in a lower-dimensional<a id="_idIndexMarker1826" class="calibre6 pcalibre pcalibre1"/> space, they are not designed to generate new data. To enable that functionality, we need to bring probability distributions into the image, and this is what has given rise to the development of <strong class="bold">variational AEs</strong> (<strong class="bold">VAEs</strong>), which I’ll <span>describe next.</span></p>
<h4 class="calibre20">VAEs</h4>
<p class="calibre3">VAEs extend <a id="_idIndexMarker1827" class="calibre6 pcalibre pcalibre1"/>the AE <a id="_idIndexMarker1828" class="calibre6 pcalibre pcalibre1"/>concept by introducing probabilistic approaches to enable the generation of new data points that are similar in structure to the training data. Building on what we’ve discussed about how AEs work, the easiest way to describe VAEs is to highlight their subtle differences from <span>regular AEs.</span></p>
<p class="calibre3">As with traditional AEs, VAEs consist of an encoder and a decoder. However, rather than mapping input data to a fixed, deterministic vector as in regular AEs, the encoder in a VAE maps the input data to a probability distribution over the latent space. That’s a lot to take in, so let’s clarify <span>that further.</span></p>
<p class="calibre3">As we learned in the previous section, regular AEs encode the input data into a vector that contains latent features representing the original inputs. However, in the case of VAEs, instead of learning a single point in the latent space for each data sample, the encoder learns probability distributions. To be more precise, instead of outputting specific feature values in the latent space, the encoder outputs parameters (such as mean and standard deviation) that describe the probability distribution for features in the latent space. This distribution represents where the encoder believes the input data should be encoded. This approach introduces randomness or variability into the encoding process, which is important for potentially generating new data points that are similar to the input data, and not just reconstructions of the <span>original data.</span></p>
<p class="calibre3">The following are the high-level steps that are performed when an input is passed through <span>the VAE:</span></p>
<ol class="calibre7">
<li class="calibre8">The encoder provides the parameters of a probability distribution within the <span>latent space.</span></li>
<li class="calibre8">A point is sampled from this <span>learned distribution.</span></li>
<li class="calibre8">This sampled point then gets passed through the decoder, which tries to reconstruct the input data from this <span>probabilistic encoding.</span></li>
</ol>
<p class="calibre3">The goal, again, is to minimize the difference between the original input and its reconstruction, similar to <span>regular AEs.</span></p>
<p class="calibre3">However, in addition to the reconstruction error, VAEs have an extra term in their loss function referred to<a id="_idIndexMarker1829" class="calibre6 pcalibre pcalibre1"/> as the <strong class="bold">Kullback-Leibler</strong> (<strong class="bold">KL</strong>) divergence, which measures how much the learned distributions for each input deviate from a standard normal distribution. Without getting into too much detail regarding the mathematics involved, the important thing to understand is that the KL divergence regularization enforces smoothness and continuity in the latent space, which makes the models more robust and can help <span>reduce overfitting.</span></p>
<p class="calibre3">We covered some complex details in this section, but the key takeaway is that VAEs introduce<a id="_idIndexMarker1830" class="calibre6 pcalibre pcalibre1"/> probabilistic<a id="_idIndexMarker1831" class="calibre6 pcalibre pcalibre1"/> mechanisms that enable them to go beyond simply reconstructing the input data and begin to generate new, similar <span>data points.</span></p>
<p class="calibre3">We are now firmly getting into the realm of generative models, and the next approach I will outline is that <span>of GANs.</span></p>
<h3 class="calibre11">GANs</h3>
<p class="calibre3">We briefly introduced<a id="_idIndexMarker1832" class="calibre6 pcalibre pcalibre1"/> GANs in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, and in this<a id="_idIndexMarker1833" class="calibre6 pcalibre pcalibre1"/> section, we will explore them in more detail. You may be familiar with the concept of “deepfakes,” in which AI is used to create photorealistic images or movies or audiorealistic tracks. By “photorealistic,” we mean synthetic data that looks like real photographs or video, and by “audiorealistic,” we mean synthetic data that sounds like real audio recordings. GANs are a popular mechanism for generating synthetic data that mimics real-world data. As with any technology, GANs could be used for malicious purposes, such as generating deepfakes of real people without their consent, but they have many useful applications that we will cover shortly. However, let’s first dive into what GANs are and how <span>they work.</span></p>
<h4 class="calibre20">High-level concepts – the generator and the discriminator</h4>
<p class="calibre3">The name “generative <a id="_idIndexMarker1834" class="calibre6 pcalibre pcalibre1"/>adversarial networks” (Goodfellow et al., 2014) may sound somewhat abstract, but it perfectly describes the concepts used in this approach to GenAI. In short, GAN implementations consist of two NNs that work in an adversarial manner (that is, they work against each other), in which one of the networks generates synthetic data, and the other network tries to ascertain whether the data is real or not. The network that generates the data is called, not surprisingly, the <strong class="bold">generator</strong>, and the network that tries to ascertain the realness of the data is called <a id="_idIndexMarker1835" class="calibre6 pcalibre pcalibre1"/>the <strong class="bold">discriminator</strong>. The main premise is that the generator tries to fool the discriminator into believing that the data <span>is real.</span></p>
<p class="calibre3">One could imagine that having an effective generator is the most important requirement for a GAN, but it is also essential to have an effective discriminator because the discriminator can be seen <a id="_idIndexMarker1836" class="calibre6 pcalibre pcalibre1"/>as the <strong class="bold">quality control</strong> (<strong class="bold">QC</strong>) mechanism for generated data. If you have an ineffective discriminator, it could easily be fooled by data that does not accurately mimic real data. The more effective your discriminator is at identifying fake data, the harder your generator will need to work to create realistic data. Therefore, both sides of the adversarial partnership need to be trained effectively in order for the GAN to create <span>high-quality data.</span></p>
<p class="calibre3">The analogy that is often used to describe this process is to imagine a person who wants to create forgeries of expensive art pieces (that is, a forger) and another person who is dedicated to identifying whether a piece of art is real or a forgery (that is, an art expert). In the beginning, the forgeries might be amateur attempts that are easy to identify as fakes. However, as the forger refines their work and learns to create more convincing forgeries, the art expert must become more skilled at identifying minute nuances that distinguish real art from fake art pieces. This cycle then repeats until, ideally, the generator creates<a id="_idIndexMarker1837" class="calibre6 pcalibre pcalibre1"/> data that is indistinguishable from the <span>real data.</span></p>
<h4 class="calibre20">Diving deeper – GAN training process</h4>
<p class="calibre3">In a GAN, both <a id="_idIndexMarker1838" class="calibre6 pcalibre pcalibre1"/>networks are<a id="_idIndexMarker1839" class="calibre6 pcalibre pcalibre1"/> usually <strong class="bold">convolutional NNs</strong> (<strong class="bold">CNNs</strong>). The discriminator is usually a binary classifier that classifies the data as either real or fake. This means that its training process involves many of the same steps that are already familiar to us from earlier chapters in this book, in which we trained <span>classifier models.</span></p>
<p class="calibre3">As an example, let’s imagine that we want to generate images of cats. The discriminator could learn to identify images of cats by being trained on a labeled dataset. In this dataset, some of the inputs are real images of cats, and they are <span>labeled accordingly.</span></p>
<p class="calibre3">As we briefly mentioned in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, the generator and the discriminator engage in a kind of adversarial game. In this game, the objective of the discriminator is to <em class="italic">minimize</em> the error rate for classifying images of cats (that is, it wants to accurately identify images of cats as much as possible). On the other hand, the generator is trying to <em class="italic">maximize</em> the discriminator’s error rate. Due to the opposing objectives of each of the game’s participants, where one participant wants to minimize a certain metric and the other participant wants to maximize that same metric, this is referred to <a id="_idIndexMarker1840" class="calibre6 pcalibre pcalibre1"/>as a <span><strong class="bold">minimax</strong></span><span> game.</span></p>
<p class="calibre3">In addition to training the discriminator on the labeled training dataset, the discriminator also receives outputs from the generator and is asked to classify those outputs, as depicted in <span><em class="italic">Figure 15</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer198">
<img alt="Figure 15.5: GAN (source: https://developers.google.com/machine-learning/gan/gan_structure)" src="image/B18143_15_5.jpg" class="calibre186"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.5: GAN (source: https://developers.google.com/machine-learning/gan/gan_structure)</p>
<p class="calibre3">If the discriminator thinks a data point is real when, in fact, it was created by the generator, that counts as an error. This, therefore, increases the error rate for the discriminator, which<a id="_idIndexMarker1841" class="calibre6 pcalibre pcalibre1"/> is a win for the generator. Conversely, if the discriminator accurately classifies the data point as fake, then it reduces the error rate for the discriminator (which is, of course, a win for <span>the discriminator).</span></p>
<p class="calibre3">While the discriminator uses a typical mechanism such as gradient descent to learn from its mistakes and minimize the loss, the generator can also learn from the discriminator’s mistakes, and adjust its weights in a reverse process that aims to increase the discriminator’s error rate. This is the novel approach used by GANs, in which the generator doesn’t receive direct labels of  “good” or “bad” during training, but instead receives feedback on the discriminator’s performance. If the discriminator accurately identifies a fake, then the gradients are sent back to the generator so that it can update its weights to create an output that better fools the discriminator in the <span>next round.</span></p>
<p class="calibre3">During this process, the generator learns the underlying probability distribution of the real data and becomes more adept at generating new data that aligns with that probability distribution (that is, new data with similar properties as the real data). It’s also important to understand that each network takes turns in the training process. When the generator is being trained, the discriminator’s weights are frozen, and <span>vice versa.</span></p>
<p class="calibre3">By pitting the models against each other in this fashion, they must continually get better in order to outperform each other. As the discriminator gets better at identifying fakes, the generator learns to generate more realistic data to fool the discriminator, and <span>so on.</span></p>
<p class="calibre3">GANs gained a lot of popularity since they were first created by Ian Goodfellow and colleagues in 2014, and multiple different types of GAN implementations have emerged, such <a id="_idIndexMarker1842" class="calibre6 pcalibre pcalibre1"/>as <strong class="bold">conditional GANs</strong> (<strong class="bold">cGANs</strong>), CycleGANs, StyleGANs, <strong class="bold">deep convolutional GANs</strong> (<strong class="bold">DCGANs</strong>), and progressive GANs. A comprehensive <a id="_idIndexMarker1843" class="calibre6 pcalibre pcalibre1"/>discussion of all of these variants would constitute more detail than is necessary in this book. I encourage you to research these <a id="_idIndexMarker1844" class="calibre6 pcalibre pcalibre1"/>variants further if you have a specific interest <span>in them.</span></p>
<p class="callout-heading">Important note – look out for “mode collapse”</p>
<p class="callout">In addition to the typical challenges faced when training many types of NN architectures, such as exploding and vanishing gradients, GANs also often suffer from a specific challenge that is related to their adversarial training process. This problem is called “mode collapse,” and it refers to a situation in which the generator may start producing a limited variety of outputs, especially if those outputs successfully trick the discriminator. Since the generator’s primary goal is to fool the discriminator, it may learn to generate patterns that are effective in doing so but do not accurately represent the target data distribution. This is an ongoing area of research, in which new mechanisms are being introduced to combat <span>this phenomenon.</span></p>
<p class="calibre3">Next, let’s discuss<a id="_idIndexMarker1845" class="calibre6 pcalibre pcalibre1"/> some common applications <span>of GANs.</span></p>
<h4 class="calibre20">Applications of GANs</h4>
<p class="calibre3">In addition to <a id="_idIndexMarker1846" class="calibre6 pcalibre pcalibre1"/>generating photorealistic images and audiorealistic data, GANs can be used for other interesting use cases, such as style transfer and image-to-image translation. For example, some online services enable you to upload a photo and convert it into a cartoon or make it look like an oil painting in the style of a famous artist such as Van Gogh. While these are fun applications, GANs are also utilized in various use cases such as text generation, new drug discovery, and other types of synthetic <span>data generation.</span></p>
<p class="calibre3">We’ll discuss the importance of high-quality synthetic data later, but next, I’d like to introduce another <a id="_idIndexMarker1847" class="calibre6 pcalibre pcalibre1"/>popular GenAI approach, referred to <span>as </span><span><strong class="bold">diffusion</strong></span><span>.</span></p>
<h3 class="calibre11">Diffusion</h3>
<p class="calibre3">Diffusion is utilized for<a id="_idIndexMarker1848" class="calibre6 pcalibre pcalibre1"/> many of the same kinds of use <a id="_idIndexMarker1849" class="calibre6 pcalibre pcalibre1"/>cases as GANs, such as image and audio generation, image-to-image translation, style transfer, and even new drug discovery. However, diffusion uses a different approach and often provides more favorable results for some use cases. We’ll dive into some of the differences between diffusion models and GANs in this section, but first, as always, let’s learn about what diffusion is and how <span>it works.</span></p>
<h4 class="calibre20">High-level concepts – noising and denoising</h4>
<p class="calibre3">At a high level, diffusion<a id="_idIndexMarker1850" class="calibre6 pcalibre pcalibre1"/> consists of two <span>main steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Adding noise <span>to images</span></li>
<li class="calibre8">Reversing the process – that is, removing noise to work back toward the <span>original image</span></li>
</ol>
<p class="calibre3">I’ll describe these concepts in much more detail, starting with clarifying what it means to add noise to an image. <span><em class="italic">Figure 15</em></span><em class="italic">.6</em> shows an image consisting <span>of noise:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer199">
<img alt="Figure 15.6: Image of noise (source: https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)" src="image/B18143_15_6.jpg" class="calibre187"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.6: Image of noise (source: https://commons.wikimedia.org/wiki/File:256x256_Dissolve_Noise_Texture.png)</p>
<p class="calibre3">Some of you may be familiar with the kind of image shown in <span><em class="italic">Figure 15</em></span><em class="italic">.6</em> if you’ve ever seen noise displayed on a TV screen when it is not tuned to a channel that provides a image signal. This image consists purely of noise, in which we cannot make out any discernable shapes. However, if we were to take a high-quality image and add some noise to it, then it would become somewhat blurry, but we could still make out what’s in the image, as long as not too much noise is added. The more noise we add, the more difficult it becomes to identify the contents of <span>the image.</span></p>
<p class="calibre3">This concept is used in the <strong class="bold">noising</strong> process when training diffusion models. We take images and add small amounts of noise to them to make it progressively more difficult to identify what’s in the images. This is also referred to <a id="_idIndexMarker1851" class="calibre6 pcalibre pcalibre1"/>as the <strong class="bold">forward </strong><span><strong class="bold">diffusion</strong></span><span> process.</span></p>
<p class="calibre3">The <strong class="bold">reverse diffusion</strong> or <strong class="bold">denoising</strong> process then tries to take a noisy image and work back <a id="_idIndexMarker1852" class="calibre6 pcalibre pcalibre1"/>toward the <a id="_idIndexMarker1853" class="calibre6 pcalibre pcalibre1"/>original image. This might sound a bit pointless – that is, why bother adding noise to images and then trying to train a model to learn how to remove noise to generate the original images? Well, just as in the case of GANs, we are training our model to understand the probability distribution of the original dataset. Diving into a bit more detail, our model actually learns to predict noise in the input. The next step of the process, then, is to simply remove that noise from the input in order to estimate or generate the desired <span>denoised output.</span></p>
<p class="calibre3">The reason this is called <a id="_idIndexMarker1854" class="calibre6 pcalibre pcalibre1"/>diffusion may be somewhat self-explanatory, but more specifically, the name relates to a concept in the field of non-equilibrium thermodynamics. That’s a bit of a mouthful, and, don’t worry – we’re not going to dive into the physical concept except to briefly introduce why it is named <span>as such.</span></p>
<p class="calibre3">The analogy often used to explain this concept is to think of a drop of ink being added to a bucket of water. When the ink is added to the water, at first, it occupies a small space in a specific location in the bucket. However, over time, the ink dissipates throughout the bucket of water. Soon, it has spread throughout the bucket, and the bucket then contains a mixture of water and ink. The color of the contents of the bucket may be different from the color that existed before you added the ink, but it’s no longer possible to pinpoint the specific location of the ink in the bucket. This is the forward diffusion process, in which the ink diffuses throughout the water in <span>the bucket.</span></p>
<p class="calibre3">In the physical world, this process of diffusion is usually impossible to reverse – that is, no matter what you try to do, you will not be able to separate the ink from the water and condense the drop of ink back into the original position it occupied when it was first added to <span>the bucket.</span></p>
<p class="calibre3">In the case of <a id="_idIndexMarker1855" class="calibre6 pcalibre pcalibre1"/>diffusion in ML, however, we try to reverse the process and get back to the original <span>input state.</span></p>
<h4 class="calibre20">Diving deeper</h4>
<p class="calibre3">It’s important to <a id="_idIndexMarker1856" class="calibre6 pcalibre pcalibre1"/>understand that noise is added to the data in a controlled manner. We don’t simply add completely random noise, but instead, we add what is referred<a id="_idIndexMarker1857" class="calibre6 pcalibre pcalibre1"/> to as <strong class="bold">Gaussian noise</strong>,  meaning the noise is characterized by the “normal” (or “Gaussian”) probability distribution. As a refresher, a Gaussian distribution is represented by the familiar “bell curve” as depicted in <span><em class="italic">Figure 15</em></span><span><em class="italic">.7</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer200">
<img alt="Figure 15.7: Gaussian distribution" src="image/B18143_15_7.jpg" class="calibre188"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.7: Gaussian distribution</p>
<p class="calibre3">As we can see in <span><em class="italic">Figure 15</em></span><em class="italic">.7</em>, the Gaussian distribution is symmetrical, with its mean at the center of the bell curve. The width of the curve represents the variance (or its square root, the standard deviation) of the distribution – that is, how far from the mean we are likely to see data points that fit the distribution. In a Gaussian distribution, data points that are most likely to occur appear closer to the mean, while rarer data points are further from the mean. In <span><em class="italic">Figure 15</em></span><em class="italic">.7</em>, the mean is 0, and the standard deviation is 1, which is a special type of Gaussian distribution referred to as the <strong class="bold">standard normal distribution</strong>. By <a id="_idIndexMarker1858" class="calibre6 pcalibre pcalibre1"/>using normally distributed noise, we can easily change the intensity of noise by tweaking the variance of <span>the distribution.</span></p>
<p class="calibre3">In addition to controlling noise by ensuring it fits the normal distribution, we also introduce noise in a controlled manner by incrementally adding noise to our source images during training, rather than adding too much noise all at once. The addition of noise in this manner is performed according to a <strong class="bold">schedule</strong>, in which more noise is added at different time intervals in the process. Remember that, as we discussed in earlier sections of this chapter, randomization is important in generative processes because we don’t just want to produce exact images from the original dataset but instead want to produce <span>similar images.</span></p>
<p class="calibre3">To introduce some controllable randomization into the process, the mean and the variance of noise added in each step are different, and we also add different amounts of noise at various parts of the schedule. Each time we add noise, we form a step in a Markov chain. As you may remember from earlier in this chapter, each step in a Markov chain is dependent only on the step that directly precedes it, and this is important to understand in the context of the reverse diffusion process. As we add noise in each step (during the forward diffusion process), we are training our model to identify (or predict) noise that has been<a id="_idIndexMarker1859" class="calibre6 pcalibre pcalibre1"/> added. Then, during the reverse diffusion process, we start with a noisy image, and we want to try to generate the image in the previous step in the Markov chain and progressively work our way through the chain in that manner until we get back to an approximation of the <span>original image.</span></p>
<p class="calibre3">To describe this process in more detail, imagine that we start with a image of a cat, and we progressively add noise in each step until we finally end up with a image similar to the image depicted in <span><em class="italic">Figure 15</em></span><em class="italic">.6</em>, which consists almost purely of noise. It would be very difficult to try to jump from such an extremely noisy image back to a image of a cat, so we instead train our model on how to progressively work backward through each step in the Markov chain, predicting the small amount of noise that was added in each step. As our model gets better and better, it can more clearly distinguish between noise and the underlying probability distribution of the original dataset. This understanding of probability distributions allows us to sample new images by starting with pure noise and iteratively denoising them according to what the model <span>has learned.</span></p>
<p class="calibre3">At the beginning of this section, I promised that I would outline some important differences between<a id="_idIndexMarker1860" class="calibre6 pcalibre pcalibre1"/> diffusion and GANs. Let’s take a look at those <span>differences next.</span></p>
<h4 class="calibre20">Differences between diffusion and GANs</h4>
<p class="calibre3">I mentioned the <a id="_idIndexMarker1861" class="calibre6 pcalibre pcalibre1"/>concept of mode collapse in the context of training GANs<a id="_idIndexMarker1862" class="calibre6 pcalibre pcalibre1"/> and how that can introduce instability into the training process. Diffusion uses a more stable training process, but diffusion models require many steps to generate data, which can be computationally intensive. Some recent advances aim to reduce the number of required steps and computational costs, but this is still a notable consideration for <span>diffusion models.</span></p>
<p class="calibre3">Similarly, at inference time, generating samples from diffusion models can be computationally expensive, whereas GANs can generate samples more quickly because they require only a single forward pass through the <span>generator network.</span></p>
<p class="calibre3">Which approach is best, then? Well, it’s a matter of selecting the right tool for the job based on the business requirements for a specific <span>use case.</span></p>
<p class="calibre3">Next, it’s time to move on to discussing perhaps the crowning glory of the GenAI world in recent <span>times: LLMs.</span></p>
<h1 id="_idParaDest-317" class="calibre5"><a id="_idTextAnchor379" class="calibre6 pcalibre pcalibre1"/>LLMs</h1>
<p class="calibre3">This is another case <a id="_idIndexMarker1863" class="calibre6 pcalibre pcalibre1"/>in which the name of the technology very accurately describes what the technology is; LLMs are large models that are particularly useful for language-based use cases, such as the summarization of large amounts of textual data, or chatbots that can have a conversation with <span>a human.</span></p>
<p class="calibre3">Behind the scenes, they use statistical methods to process, predict, and generate language based on the context provided to them. They are trained on diverse datasets that include large amounts of textual information, from books and articles to websites and human interactions, enabling them to learn language patterns, grammar, <span>and semantics.</span></p>
<p class="calibre3">How large are they? Well, some of the latest models at the time of writing this in February 2024 consist of billions or even trillions of parameters. That’s pretty huge! Somebody might read this book 20 years from now and laugh at the fact that we considered these sizes to be huge, but these are the biggest models on the planet at the moment, and they constitute an enormous advancement from any models that have existed <span>before now.</span></p>
<p class="calibre3">Before diving into <a id="_idIndexMarker1864" class="calibre6 pcalibre pcalibre1"/>details on how LLMs are created, let’s take a look at some historical milestones that have led to <span>their evolution.</span></p>
<h2 id="_idParaDest-318" class="calibre9"><a id="_idTextAnchor380" class="calibre6 pcalibre pcalibre1"/>Evolution of LLMs</h2>
<p class="calibre3">The history of language <a id="_idIndexMarker1865" class="calibre6 pcalibre pcalibre1"/>models began with simple rule-based systems that used hand-coded rules to interpret and generate language. It turns out that trying to hand-code all of the complex rules of human language is quite impractical, but the research in this field had to <span>start somewhere.</span></p>
<p class="calibre3">Next, statistical methods such as N-gram models used probabilities to predict the likelihood of a sequence of words. They were trained on large bodies of text and could capture more language nuances than rule-based systems, but they still struggled with long-term dependencies <span>in text.</span></p>
<p class="calibre3">The next step forward was to use models such as HMMs and simple NNs for language tasks, and while these models offered better performance by learning more advanced patterns in data, they were still limited <span>in complexity.</span></p>
<p class="calibre3">Major breakthroughs began when scientists started<a id="_idIndexMarker1866" class="calibre6 pcalibre pcalibre1"/> applying <strong class="bold">deep NNs</strong> (<strong class="bold">DNNs</strong>) to language use cases. In <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a> and <a href="B18143_14.xhtml#_idTextAnchor348" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 14</em></span></a>, we discussed the evolution of RNNs, LSTM networks, and Transformers, and how they each enabled progressively more complex language processing. The pivotal event that occurred in the industry was when Google invented the Transformer architecture (Vaswani et al., 2017), which has become the primary technology used in all of the biggest and most advanced LLMs in the <span>industry today.</span></p>
<p class="calibre3">When we discussed the overall evolution of AI/ML in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, I mentioned that it was not only the development of more complex ML algorithms that led to the kinds of breakthroughs we’ve seen in recent decades but also advancements in computing power and the proliferation of available data for training models. These factors all work together to form a kind of ecosystem in which we continue to advance all of these technologies <span>in concert.</span></p>
<p class="calibre3">Next, we’ll dive into how LLMs <span>are created.</span></p>
<h2 id="_idParaDest-319" class="calibre9"><a id="_idTextAnchor381" class="calibre6 pcalibre pcalibre1"/>Building LLMs</h2>
<p class="calibre3">Let’s get one thing <a id="_idIndexMarker1867" class="calibre6 pcalibre pcalibre1"/>out of the way, right from the beginning; it’s unlikely that you or I could create our own LLMs from scratch, for two <span>main reasons:</span></p>
<ul class="calibre16">
<li class="calibre8">Training LLMs requires obscenely large amounts of data. Some of the LLMs you or I would interact with are trained on the entire internet, for example. This is one of the things that makes them so powerful and knowledgeable; they are trained on all publicly available data that humans have ever created, in addition to some private and proprietary datasets owned or sourced by the companies that <span>trained them.</span></li>
<li class="calibre8">It is generally extremely expensive to train LLMs due to the sheer amount of computing power needed. I’m referring to using thousands of high-performance accelerators, such as the latest and greatest GPUs and TPUs, for months on end to train these models, and those things don’t <span>come cheap!</span></li>
</ul>
<p class="calibre3">As a result, most people and companies will use LLMs that have already been <strong class="bold">pre-trained</strong>, which I will <span>describe next.</span></p>
<h3 class="calibre11">The LLM training process</h3>
<p class="calibre3">While the companies<a id="_idIndexMarker1868" class="calibre6 pcalibre pcalibre1"/> that build commercially available LLMs almost certainly use a lot of secret magic behind the scenes that they are unlikely to share externally, the following high-level steps are generally involved in the LLM <span>training process:</span></p>
<ul class="calibre16">
<li class="calibre8">Unsupervised or <span>semi-supervised pre-training</span></li>
<li class="calibre8"><span>Supervised tuning</span></li>
</ul>
<p class="calibre3">Let’s take a look at each of these in <span>more detail.</span></p>
<h4 class="calibre20">Unsupervised or semi-supervised pre-training</h4>
<p class="calibre3">Remember from<a id="_idIndexMarker1869" class="calibre6 pcalibre pcalibre1"/> our activities<a id="_idIndexMarker1870" class="calibre6 pcalibre pcalibre1"/> earlier<a id="_idIndexMarker1871" class="calibre6 pcalibre pcalibre1"/> in this book that supervised training requires labeled datasets, which<a id="_idIndexMarker1872" class="calibre6 pcalibre pcalibre1"/> can be cumbersome and expensive to source or create. Bearing in mind that LLMs are often trained on enormous bodies of data such as the entire internet, it would be impossible to label all of that data. Instead, a very clever trick is used to train LLMs in such a way that they can learn from these large bodies of text without the need for explicit labeling, and that trick is <a id="_idIndexMarker1873" class="calibre6 pcalibre pcalibre1"/>called <strong class="bold">masked language </strong><span><strong class="bold">modeling</strong></span><span> (</span><span><strong class="bold">MLM</strong></span><span>).</span></p>
<p class="calibre3">MLM is actually a very simple yet effective concept; it’s essentially a game of “fill in the blanks.” You may remember playing this game as a child or encountering it in exam scenarios. Quite simply, we take a sentence and blank out (or mask) one or more of the words, and the task is then to guess what the missing words should be. For example, consider the <span>following sentence:</span></p>
<p class="author-quote">“You really hit the nail on the [BLANK] with that last comment.”</p>
<p class="calibre3">What do you think would be the most appropriate word to use for filling in the <em class="italic">[BLANK]</em> portion of that sentence? If you’re familiar with the term, “hit the nail on the head,” then you may have guessed that “head” would be the best word to use, and you’d be correct in <span>guessing that.</span></p>
<p class="calibre3">This approach can be extremely effective in training LLMs because we can feed millions of sentences to the LLM, randomly masking out various words and asking the LLM to predict what the correct words should be. The beauty of this approach is that the masked-out words become the labels, so we don’t need to explicitly label the dataset. This is why we refer to this process as UL <a id="_idIndexMarker1874" class="calibre6 pcalibre pcalibre1"/>or <strong class="bold">semi-supervised learning</strong> (<strong class="bold">SSL</strong>). For example, after the LLM predicts the word to use, we can reveal the word that the sentence actually contained in the masked-out position, and the model can then learn from that. If the model predicted a different word, it would count as an error, and the loss function would reflect this accordingly <span>during training.</span></p>
<p class="calibre3">The important thing to note is that the model gets to see billions of different sentences during training, and by doing so, it sees words being used in various contexts. Over time, it builds up an understanding of what each word means and which other words it commonly appears alongside. Conceptually, it can build up a graph of how various words relate to each other, as depicted in <span><em class="italic">Figure 15</em></span><span><em class="italic">.8</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer201">
<img alt="Figure 15.8: Graph of word associations" src="image/B18143_15_8.jpg" class="calibre189"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 15.8: Graph of word associations</p>
<p class="calibre3">In <span><em class="italic">Figure 15</em></span><em class="italic">.8</em>, we can see how various words in the English language relate to each other. Notice how some words may be ambiguous, such as the word “Turkey,” which could relate to the country, to the animal, or to food. Consider the <span>following sentences:</span></p>
<ul class="calibre16">
<li class="calibre8">“I really like <span>eating turkey.”</span></li>
<li class="calibre8">“Last year, we went on vacation <span>in Turkey.”</span></li>
<li class="calibre8">“I really liked the food <span>in Turkey.”</span></li>
</ul>
<p class="calibre3">Assume that the LLM<a id="_idIndexMarker1875" class="calibre6 pcalibre pcalibre1"/> has<a id="_idIndexMarker1876" class="calibre6 pcalibre pcalibre1"/> seen all of those sentences during <a id="_idIndexMarker1877" class="calibre6 pcalibre pcalibre1"/>training. By<a id="_idIndexMarker1878" class="calibre6 pcalibre pcalibre1"/> seeing these sentences (and billions more) and learning how words are used in various contexts, the LLM forms a probabilistic understanding of the <a id="_idIndexMarker1879" class="calibre6 pcalibre pcalibre1"/>meaning (or <strong class="bold">semantic context</strong>) of the words. For example, how does it know that the word “Turkey” might refer to a country based on a sentence about people going on vacation there? Well, from seeing the word “vacation” in millions or billions of sentences, it comes to understand that people go on vacation in places, so in this context, “Turkey” must be a place. It also would have seen other sentences containing the words “Turkey” and “immigration,” and it would have developed an understanding that immigration refers not only to places but, more specifically, to countries. It would also have learned what the word “country” means and that its plural form is “countries,” based on seeing those words in many other contexts. Similarly, it learns that the word “eat” refers to food, so in the sentence “I really like eating turkey,” it understands that “turkey” must be a type <span>of food.</span></p>
<p class="calibre3">I’ve presented just a handful of simple sentences here, and we can already see a proliferation of different potential combinations of words and how they relate to each other. Imagine the complexity of the combinations that exist when we bring every sentence on the internet into scope. This is what LLMs learn: extremely complex webs of associations and the underlying meaning of the concepts those words represent. This is an enormous step forward in terms of intelligence that LLMs bring to the AI industry, and this is actually how our own minds learn to understand words, also, which is fascinating. As children, we don’t just learn individual words, but we learn words as representations of concepts, usually by forming graphs of associations. For example, imagine you were a young child who was trying to climb up a tree, and your parent said, “Don’t climb that tree, or you might fall and injure yourself.” You may not have heard all of those words before, but from hearing that sentence, you might learn about the concept of a tree, or of climbing, or falling, or injuring, and you also may intuit that falling and injuring are undesirable things <span>to happen.</span></p>
<p class="calibre3">In addition to masking words within sentences, another method called <strong class="bold">next sentence prediction</strong> (<strong class="bold">NSP</strong>) may <a id="_idIndexMarker1880" class="calibre6 pcalibre pcalibre1"/>also be used to train our LLMs to not only predict words but, as the name suggests, also to predict entire sentences based on the provided context. In this case, the LLM is presented with pairs of sentences, and sometimes these sentences have a natural sequential relationship (for example, two consecutive lines from an article), or <a id="_idIndexMarker1881" class="calibre6 pcalibre pcalibre1"/>in<a id="_idIndexMarker1882" class="calibre6 pcalibre pcalibre1"/> other<a id="_idIndexMarker1883" class="calibre6 pcalibre pcalibre1"/> cases, they <a id="_idIndexMarker1884" class="calibre6 pcalibre pcalibre1"/>are unrelated sentences. The LLM’s task is to determine if the second sentence logically follows from the first, and by doing this, it learns to discriminate between logical and incoherent sequences of sentences. This pushes the LLM to reason about context across multiple sentences, which helps it generate coherent text in longer responses <span>to requests.</span></p>
<p class="calibre3">It’s important to note that there’s an additional layer of abstraction in how the LLMs learn. Earlier in this chapter, we discussed embeddings and latent space. Generally, the kinds of associations I’ve just outlined occur in the latent space. The latent space is the LLM’s representation of the world and all of the various concepts and associations it learns. Concepts with similar meaning or semantic context may be located nearer to each other in the <span>latent space.</span></p>
<p class="calibre3">After the LLM has learned everything it can learn from the available data via the process I’ve just described, the pre-training phase is complete. At this point, the LLM has built its latent space representation of the world, and the next step is to teach it how to be useful in responding to requests from humans or other machines. Many different processes can be used<a id="_idIndexMarker1885" class="calibre6 pcalibre pcalibre1"/> for<a id="_idIndexMarker1886" class="calibre6 pcalibre pcalibre1"/> this purpose, but I’ll start<a id="_idIndexMarker1887" class="calibre6 pcalibre pcalibre1"/> with <a id="_idIndexMarker1888" class="calibre6 pcalibre pcalibre1"/>the common practice of <span>supervised tuning.</span></p>
<h4 class="calibre20">Supervised tuning of LLMs</h4>
<p class="calibre3">As we know by now, supervised<a id="_idIndexMarker1889" class="calibre6 pcalibre pcalibre1"/> training means that we have a dataset that contains labels that can be used to teach a model. For example, if we want our model to perform sentiment analysis, we can label phrases in the dataset with the sentiments expressed by those phrases, such as positive, negative, or neutral. Similarly, for summarization, we could show the model examples of good summarizations, and the model could learn from <span>those examples.</span></p>
<p class="calibre3">While enormous datasets are usually needed for the pre-training phase, supervised tuning of LLMs requires smaller datasets that are carefully curated and labeled for the particular task at hand. Considering that LLMs usually contain a lot of knowledge from the pre-training phase, surprisingly good tuning results can come from just a few (or perhaps a few hundred) examples in the tuning dataset. Again, this is similar to how humans incrementally learn new skills. For example, imagine there are two people, one of whom has never learned to drive, and another who has been driving cars for 20 years. Now, we want each of those people to learn how to drive a large truck by next week. Who is most likely to succeed? The person who has been driving cars for 20 years already has a lot of knowledge of the rules of the road and how to operate vehicles, so learning how to drive a large truck will just require a relatively small amount of incremental learning in comparison to the person who has never driven any <span>vehicles before.</span></p>
<p class="calibre3">It’s also important to understand that there are various approaches or levels of tuning that we can apply. For <a id="_idIndexMarker1890" class="calibre6 pcalibre pcalibre1"/>example, <strong class="bold">few-shot learning</strong> (<strong class="bold">FSL</strong>) refers to a practice in which we provide just a few examples to the LLM. We could even provide these examples along with our prompt (that is, our request to the LLM) rather than needing to provide a dedicated training dataset. This can be effective for some tasks, but, of course, it is quite limited since we are only providing a few examples. When examples are provided along with a prompt, this is an example of prompt engineering, which is an emerging science in itself that focuses on how to craft requests to LLMs in ways that produce the best results. In these cases, the underlying LLM generally does not get retrained on the examples we provide. Instead, the LLM simply uses the examples provided to tailor its responses in alignment with <span>those examples.</span></p>
<p class="calibre3">On the other side of the spectrum<a id="_idIndexMarker1891" class="calibre6 pcalibre pcalibre1"/> is <strong class="bold">full FT</strong> (<strong class="bold">FFT</strong>). This level of tuning could require thousands of examples, and the model’s parameters get updated based on those examples (that is, the model learns and incorporates the new data into its <span>core structure).</span></p>
<p class="calibre3">Then, there are levels in between, such<a id="_idIndexMarker1892" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">adapter tuning</strong>, in which <strong class="bold">adapter layers</strong> are<a id="_idIndexMarker1893" class="calibre6 pcalibre pcalibre1"/> inserted into the original model, and only those layers get tuned and updated while the original model weights are frozen. Another type of tuning may focus only on updating the output layer of the model while leaving the rest of the <span>model untouched.</span></p>
<p class="calibre3">On a slightly separate but related note, I want to briefly introduce the topic of <strong class="bold">reinforcement learning from human feedback</strong> (<strong class="bold">RLHF</strong>). Rather than updating the LLM with more<a id="_idIndexMarker1894" class="calibre6 pcalibre pcalibre1"/> data, this method mainly focuses on teaching the LLM to respond in ways that are more favorable to humans, such as aligning with human values. In this approach, the LLM may provide multiple responses to a prompt, and a human can select (and therefore label) which response is preferred, based on nuances of human communication and culture, such as tone, sentiment, ethical considerations, and many other factors. We introduced the concept of <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>) in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>. In the case of RLHF for tuning LLMs, the human<a id="_idIndexMarker1895" class="calibre6 pcalibre pcalibre1"/> feedback is used as the reward signal, teaching the LLM to generate outputs that better align with human preferences, even if there might be <a id="_idIndexMarker1896" class="calibre6 pcalibre pcalibre1"/>multiple technically “correct” ways <span>to respond.</span></p>
<p class="calibre3">We will explore tuning in more detail in the next chapter, but before we move on to that chapter, let’s reflect on what we’ve learned <span>so far.</span></p>
<h1 id="_idParaDest-320" class="calibre5"><a id="_idTextAnchor382" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">We started this chapter by introducing the fundamental topics that underpin GenAI, including concepts such as embeddings and latent space. We then described what GenAI is and how it contrasts against “traditional AI,” whereby traditional AI typically tries to predict a specific answer, such as a revenue forecast based on historical data or identifying whether a image contains a cat, but GenAI goes beyond those kinds of tasks and creates <span>new content.</span></p>
<p class="calibre3">We dived into the role of probability in GenAI versus traditional AI, and we discussed how traditional AI often uses conditional probability to predict the values of a target variable based on the values of features in the dataset. On the other hand, GenAI approaches typically try to learn the joint probability distribution of both the features and the <span>target variable.</span></p>
<p class="calibre3">Next, we explored the evolution of GenAI and various model development milestones that led to the kinds of models we use today. We began this exploration by covering early developments such as Markov chains, HMMs, RBMs, and DBNs. We then covered more recent developments such as AEs, GANs, <span>and diffusion.</span></p>
<p class="calibre3">Finally, we dived into the world of LLMs. We highlighted major milestones in their evolution by refreshing what we had learned in previous chapters about RNNs, LSTM networks, and Transformers. We then discussed how LLMs are trained, including the kinds of mechanisms used in the pre-training phase, such as MLM and NSP, and how FFT can be used to further train the models for specific <span>use cases.</span></p>
<p class="calibre3">Although this is an introduction chapter for the GenAI section of our book, we’ve covered a lot of important concepts in a considerable amount of depth, and this will give us a strong foundation for the <span>remaining chapters.</span></p>
<p class="calibre3">With that in mind, let’s continue our journey into the world <span>of GenAI.</span></p>
</div>
</div></body></html>