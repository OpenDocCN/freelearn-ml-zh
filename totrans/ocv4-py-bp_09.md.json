["```py\nimport numpy as np\nimport cv2\nimport tensorflow.keras as K\n```", "```py\ndef draw_box(frame: np.ndarray, box: np.ndarray) -> np.ndarray:\n    h, w = frame.shape[0:2]\n    pts = (box.reshape((2, 2)) * np.array([w, h])).astype(np.int)\n    cv2.rectangle(frame, tuple(pts[0]), tuple(pts[1]), (0, 255, 0), 2)\n    return frame\n```", "```py\nmodel = K.models.load_model(\"localization.h5\")\ncap = cv2.VideoCapture(0)\n```", "```py\nfor _, frame in iter(cap.read, (False, None)):\n    input = cv2.resize(frame, (224, 224))\n    input = cv2.cvtColor(input, cv2.COLOR_BGR2RGB)\n```", "```py\n    box, = model.predict(input[None] / 255)\n```", "```py\n    cv2.imshow(\"res\", frame)\n    if(cv2.waitKey(1) == 27):\n        break\n```", "```py\nimport glob\nimport os\n\nfrom itertools import count\nfrom collections import defaultdict, namedtuple\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport xml.etree.ElementTree as ET\n```", "```py\nDATASET_DIR = \"dataset\"\nfor type in (\"annotations\", \"images\"):\n    tf.keras.utils.get_file(\n        type,\n        f\"https://www.robots.ox.ac.uk/~vgg/data/pets/data/{type}.tar.gz\",\n        untar=True,\n        cache_dir=\".\",\n        cache_subdir=DATASET_DIR)\n```", "```py\nIMAGE_SIZE = 224\nIMAGE_ROOT = os.path.join(DATASET_DIR,\"images\")\nXML_ROOT = os.path.join(DATASET_DIR,\"annotations\")\n```", "```py\nData = namedtuple(\"Data\",\"image,box,size,type,breed\")\n```", "```py\ntypes = defaultdict(count().__next__ )\nbreeds = defaultdict(count().__next__ )\n```", "```py\ndef parse_xml(path: str) -> Data:\n```", "```py\nwith open(path) as f:\n    xml_string = f.read()\nroot = ET.fromstring(xml_string)\n```", "```py\nimg_name = root.find(\"./filename\").text\nbreed_name = img_name[:img_name.rindex(\"_\")]\n```", "```py\nbreed_id = breeds[breed_name]\n```", "```py\ntype_id = types[root.find(\"./object/name\").text]\n```", "```py\nbox = np.array([int(root.find(f\"./object/bndbox/{tag}\").text)\n                for tag in \"xmin,ymin,xmax,ymax\".split(\",\")])\nsize = np.array([int(root.find(f\"./size/{tag}\").text)\n                 for tag in \"width,height\".split(\",\")])\nnormed_box = (box.reshape((2, 2)) / size).reshape((4))\n```", "```py\nreturn Data(img_name,normed_box,size,type_id,breed_id)\n```", "```py\nxml_paths = glob.glob(os.path.join(XML_ROOT,\"xmls\",\"*.xml\"))\nxml_paths.sort()\nparsed = np.array([parse_xml(path) for path in xml_paths])\n```", "```py\nprint(f\"{len(types)} TYPES:\", *types.keys(), sep=\", \")\nprint(f\"{len(breeds)} BREEDS:\", *breeds.keys(), sep=\", \")\n```", "```py\n2 TYPES:, cat, dog\n37 BREEDS:, Abyssinian, Bengal, Birman, Bombay, British_Shorthair, Egyptian_Mau, Maine_Coon, Persian, Ragdoll, Russian_Blue, Siamese, Sphynx, american_bulldog, american_pit_bull_terrier, basset_hound, beagle, boxer, chihuahua, english_cocker_spaniel, english_setter, german_shorthaired, great_pyrenees, havanese, japanese_chin, keeshond, leonberger, miniature_pinscher, newfoundland, pomeranian, pug, saint_bernard, samoyed, scottish_terrier, shiba_inu, staffordshire_bull_terrier, wheaten_terrier, yorkshire_terrier\n```", "```py\nnp.random.seed(1)\nnp.random.shuffle(parsed)\n```", "```py\nds = tuple(np.array(list(i)) for i in np.transpose(parsed))\nds_slices = tf.data.Dataset.from_tensor_slices(ds)\n```", "```py\nfor el in ds_slices.take(1):\n    print(el)\n```", "```py\n(<tf.Tensor: id=14, shape=(), dtype=string, numpy=b'american_pit_bull_terrier_157.jpg'>, <tf.Tensor: id=15, shape=(4,), dtype=float64, numpy=array([0.07490637, 0.07 , 0.58426966, 0.44333333])>, <tf.Tensor: id=16, shape=(2,), dtype=int64, numpy=array([267, 300])>, <tf.Tensor: id=17, shape=(), dtype=int64, numpy=1>, <tf.Tensor: id=18, shape=(), dtype=int64, numpy=13>)\n```", "```py\nfor el in ds_slices:\n    b = el[1].numpy()\n    if(np.any((b>1) |(b<0)) or np.any(b[2:]-b[:2] < 0)):\n        print(f\"Invalid box found {b} image: {el[0].numpy()}\")\n```", "```py\ndef prepare(image,box,size,type,breed):\n    image = tf.io.read_file(IMAGE_ROOT+\"/\"+image)\n    image = tf.image.decode_png(image,channels=3)\n    image = tf.image.resize(image,(IMAGE_SIZE,IMAGE_SIZE))\n    image /= 255\n    return Data(image,box,size,tf.one_hot(type,len(types)),tf.one_hot(breed,len(breeds)))\n```", "```py\nds = ds_slices.map(prepare).prefetch(32)\n```", "```py\nif __name__ == \"__main__\":\n    def illustrate(sample):\n        breed_num = np.argmax(sample.breed)\n        for breed, num in breeds.items():\n            if num == breed_num:\n                break\n        image = sample.image.numpy()\n        pt1, pt2 = (sample.box.numpy().reshape(\n            (2, 2)) * IMAGE_SIZE).astype(np.int32)\n        cv2.rectangle(image, tuple(pt1), tuple(pt2), (0, 1, 0))\n        cv2.putText(image, breed, (10, 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 1, 0))\n        return image\n```", "```py\nsamples_image = np.concatenate([illustrate(sample)\n                                for sample in ds.take(3)], axis=1)\ncv2.imshow(\"samples\", samples_image)\ncv2.waitKey(0)\n```", "```py\nimport tensorflow.keras as K\nfrom data import ds\n```", "```py\nbase_model = K.applications.MobileNetV2(input_shape=(224,224, 3), include_top=False)\n```", "```py\nprint(base_model.output.shape)\n```", "```py\n(None, 7, 7, 1280)\n```", "```py\nfor layer in base_model.layers:\n    layer.trainable = False\n```", "```py\nx = K.layers.GlobalAveragePooling2D()(base_model.output)\n```", "```py\nprint(base_model.output.shape, x.shape)\n```", "```py\n(None, 7, 7, 1280) (None, 1280)\n```", "```py\nis_breeds = True\nif is_breeds:\n    out = K.layers.Dense(37,activation=\"softmax\")(x)\n    inp_ds = ds.map(lambda d: (d.image,d.breed))\nelse:\n    out = K.layers.Dense(2,activation=\"softmax\")(x)\n    inp_ds = ds.map(lambda d: (d.image,d.type))\n```", "```py\nmodel = K.Model(inputs=base_model.input, outputs=out)\n```", "```py\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\",\"top_k_categorical_accuracy\"])\n```", "```py\nevaluate = inp_ds.take(1000)\ntrain = inp_ds.skip(1000).shuffle(10**4)\n```", "```py\nmodel.fit(train.batch(32), epochs=4)\nmodel.evaluate(valid.batch(1))\n```", "```py\nEpoch 4/4\n 84/84 [==============================] - 13s 156ms/step - loss: 0.0834 - categorical_accuracy: 0.9717 - top_k_categorical_accuracy: 1.0000\n```", "```py\nmodel.evaluate(valid.batch(1))\n```", "```py\n1000/1000 [==============================] - 9s 9ms/step - loss: 0.0954 - categorical_accuracy: 0.9730 - top_k_categorical_accuracy: 1.0000\n```", "```py\nEpoch 4/4\n 84/84 [==============================] - 13s 155ms/step - loss: 0.3272 - categorical_accuracy: 0.9233 - top_k_categorical_accuracy:\n 0.9963\n```", "```py\n1000/1000 [==============================] - 11s 11ms/step - loss: 0.5646 - categorical_accuracy: 0.8080 - top_k_categorical_accuracy: 0.9890\n```", "```py\nimport tensorflow.keras as K\n\nfrom data import ds\n\nbase_model = K.applications.MobileNetV2(\n    input_shape=(224, 224, 3), include_top=False)\n```", "```py\nconv_opts = dict(\n    activation='relu',\n    padding='same',\n    kernel_regularizer=\"l2\")\n```", "```py\nx = K.layers.Conv2D(256, (1, 1), **conv_opts)(base_model.output)\n```", "```py\nx = K.layers.Conv2D(256, (3, 3), strides=2, **conv_opts)(x)\n```", "```py\nout = K.layers.Flatten()(x)\nout = K.layers.Dense(4, activation=\"sigmoid\")(out)\n```", "```py\nmodel = K.Model(inputs=base_model.input, outputs=out)\nmodel.compile(\n    loss=\"mean_squared_error\",\n    optimizer=\"adam\",\n    metrics=[\n        K.metrics.RootMeanSquaredError(),\n        \"mae\"])\n```", "```py\ninp_ds = ds.map(lambda d: (d.image,d.box))\nvalid = inp_ds.take(1000)\ntrain = inp_ds.skip(1000).shuffle(10000)\n```", "```py\ncheckpoint = K.callbacks.ModelCheckpoint(\"localization.h5\",\n    monitor='val_root_mean_squared_error',\n    save_best_only=True, verbose=1)\n```", "```py\nmodel.fit(\n    train.batch(32),\n    epochs=12,\n    validation_data=valid.batch(1),\n    callbacks=[checkpoint])\n```", "```py\nEpoch 8/12\n 83/84 [============================>.] - ETA: 0s - loss: 0.0012 - root_mean_squared_error: 0.0275 - mae: 0.0212\n Epoch 00008: val_root_mean_squared_error improved from 0.06661 to 0.06268, saving model to best_model.hdf5\n 84/84 [==============================] - 39s 465ms/step - loss: 0.0012 - root_mean_squared_error: 0.0275 - mae: 0.0212 - val_loss: 0.0044 - val_root_mean_squared_error: 0.0627 - val_mae: 0.0454 \n```"]