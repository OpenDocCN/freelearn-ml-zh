- en: Appendix B
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Basic Linear Algebra
  prefs: []
  type: TYPE_NORMAL
- en: '*Algebra is generous. She often gives you more than is asked of her.*'
  prefs: []
  type: TYPE_NORMAL
- en: — Jean le Rond d’Alembert
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will present a very broad overview of linear algebra. More
    than anything, this is meant to be a refresher. If you would like to learn linear
    algebra from the basics, we suggest reading Sheldon Axler’s wonderful book [[116](ch030.xhtml#Xaxler-lalg)].
    If you are all-in with abstract algebra, we can also recommend the great book
    by Dummit and Foote [[115](ch030.xhtml#Xdummit-foote)]. With this out of the way,
    let’s do some algebra!
  prefs: []
  type: TYPE_NORMAL
- en: When most people think of vectors, they think of fancy arrows pointing in a
    direction. But, where others see arrows, we mathematicians — in our tireless pursuit
    of abstraction — see elements of vector spaces. And what is a vector space? Simple!
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Vector spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let ![F](img/file1320.png "F") be the real or the complex numbers. An ![F](img/file1320.png
    "F")-vector space is a set ![V](img/file379.png "V") together with an ”addition”
    function (usually represented by ![+](img/file1509.png "+"), for obvious reasons)
    and a ”multiplication by scalars” function (denoted like usual multiplication).
    Addition needs to take any two vectors and return another vector, that is, ![+](img/file1509.png
    "+") needs to be a function ![\left. V \times V\rightarrow V \right.](img/file1510.png
    "\left. V \times V\rightarrow V \right."). Multiplication by scalars, as the name
    suggests, must take a scalar (an element of ![F](img/file1320.png "F")) and a
    vector, and return a vector, that is, it needs to be a function ![\left. F \times
    V\rightarrow V \right.](img/file1511.png "\left. F \times V\rightarrow V \right.").
    Moreover, vector spaces must satisfy, for any arbitrary ![\alpha_{1},\alpha_{2}
    \in F](img/file1512.png "\alpha_{1},\alpha_{2} \in F") and ![v_{1},v_{2},v_{3}
    \in V](img/file1513.png "v_{1},v_{2},v_{3} \in V"), the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Associativity for addition : ![(v_{1} + v_{2}) + v_{3} = v_{1} + (v_{2} + v_{3})](img/file1514.png
    "(v_{1} + v_{2}) + v_{3} = v_{1} + (v_{2} + v_{3})")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Commutativity for addition: ![v_{1} + v_{2} = v_{2} + v_{1}](img/file1515.png
    "v_{1} + v_{2} = v_{2} + v_{1}")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identity element for addition: there must exist a ![0 \in V](img/file1516.png
    "0 \in V") such that, for every vector ![v \in V](img/file1517.png "v \in V"),
    ![v + 0 = v](img/file1518.png "v + 0 = v")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Opposites for addition: there must exist a ![- v_{1} \in V](img/file1519.png
    "- v_{1} \in V") such that ![v_{1} + ( - v_{1}) = 0](img/file1520.png "v_{1} +
    ( - v_{1}) = 0")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compatibility of multiplication by scalars with multiplication in ![F](img/file1320.png
    "F"): ![(\alpha_{1} \cdot \alpha_{2}) \cdot v_{1} = \alpha_{1} \cdot (\alpha_{2}
    \cdot v_{1})](img/file1521.png "(\alpha_{1} \cdot \alpha_{2}) \cdot v_{1} = \alpha_{1}
    \cdot (\alpha_{2} \cdot v_{1})")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributivity with respect to vector addition: ![\alpha_{1}(v_{1} + v_{2})
    = \alpha_{1}v_{1} + \alpha_{1}v_{2}](img/file1522.png "\alpha_{1}(v_{1} + v_{2})
    = \alpha_{1}v_{1} + \alpha_{1}v_{2}")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributivity with respect to scalar addition: ![(\alpha_{1} + \alpha_{2})v_{1}
    = \alpha_{1}v_{1} + \alpha_{2}v_{1}](img/file1523.png "(\alpha_{1} + \alpha_{2})v_{1}
    = \alpha_{1}v_{1} + \alpha_{2}v_{1}")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identity for multiplication by scalars: ![1 \cdot v_{1} = v_{1}](img/file1524.png
    "1 \cdot v_{1} = v_{1}")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: If you, like us, love abstraction, you should know that vector spaces are usually
    defined over an arbitrary **field** — not just over the real or complex numbers!
    If you want to learn more, we suggest reading the book by Dummit and Foote [[115](ch030.xhtml#Xdummit-foote)].
  prefs: []
  type: TYPE_NORMAL
- en: 'These are some examples of vector spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: The set of real numbers with the usual addition and multiplication is a real
    vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of complex numbers with complex number addition and multiplication is
    a complex vector space. Moreover, it can be trivially transformed into a real
    vector space by restricting multiplication by scalars to multiplication of complex
    numbers by real numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The set ![R^{n}](img/file1212.png "R^{n}") with the usual component-wise addition
    and multiplication by scalars (real numbers) is a vector space. If we fix ![n
    = 2,3](img/file1525.png "n = 2,3"), that’s where we can find those fancy arrows
    everyone is talking about!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most importantly for us, the set ![C^{n}](img/file1526.png "C^{n}") with component-wise
    addition and scalar multiplication by complex numbers is a vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just to give a cute example, the set of all smooth functions on a closed finite
    interval of the real numbers is a vector space. You can try to define addition
    and multiplication by scalars of functions yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we refer to a vector space on a set ![V](img/file379.png "V") with addition
    ![+](img/file1509.png "+") and multiplication by scalars ![\cdot](img/file1202.png
    "\cdot"), we should denote it as ![(V, + , \cdot )](img/file1527.png "(V, + ,
    \cdot )") in order to indicate what function we are considering as the addition
    function and what function we are taking to be the multiplication by scalars.
    Nevertheless, in all honesty, ![(V, + , \cdot )](img/file1527.png "(V, + , \cdot
    )") is a pain to write, and we mathematicians — like all human beings — have a
    natural tendency towards laziness. So we usually just write ![V](img/file379.png
    "V") and let ![+](img/file1509.png "+") and ![\cdot](img/file1202.png "\cdot")
    be inferred from context whenever that is reasonable to do.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Bases and coordinates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some ![F](img/file1320.png "F")-vector spaces ![V](img/file379.png "V") are
    **finite-dimensional**: this means that there is a finite family of vectors ![\{
    v_{1},\ldots,v_{n}\} \subseteq V](img/file1528.png "\{ v_{1},\ldots,v_{n}\} \subseteq
    V") such that, for any vector ![v \in V](img/file1517.png "v \in V"), there exist
    some unique scalars ![\alpha_{1},\ldots,\alpha_{n} \in F](img/file1529.png "\alpha_{1},\ldots,\alpha_{n}
    \in F") for which'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![v = \alpha_{1}v_{1} + \cdots + \alpha_{n}v_{n}.](img/file1530.png "v =
    \alpha_{1}v_{1} + \cdots + \alpha_{n}v_{n}.") |'
  prefs: []
  type: TYPE_TB
- en: The scalars ![\alpha_{1},\ldots,\alpha_{n}](img/file1531.png "\alpha_{1},\ldots,\alpha_{n}")
    are said to be the **coordinates** of ![v](img/file1532.png "v") with respect
    to the basis ![\{ v_{1},\ldots,v_{n}\}](img/file1313.png "\{ v_{1},\ldots,v_{n}\}").
    The natural number ![n](img/file244.png "n") is said to be the dimension of the
    vector space, and it is a fact of life that any two bases of a vector space need
    to have the same number of elements, so the dimension is well-defined. If you
    want proof (which you should want!), check your favorite linear algebra textbook;
    either of the two that we have suggested should do the job.
  prefs: []
  type: TYPE_NORMAL
- en: Two examples of finite dimensional vector spaces are ![R^{n}](img/file1212.png
    "R^{n}") and ![C^{n}](img/file1526.png "C^{n}") (with the natural addition and
    multiplication operations). For example, a basis of ![C^{3}](img/file1533.png
    "C^{3}") or ![R^{3}](img/file1214.png "R^{3}") would be
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\{(1,0,0),(0,1,0),(0,0,1)\}.](img/file1534.png "\{(1,0,0),(0,1,0),(0,0,1)\}.")
    |'
  prefs: []
  type: TYPE_TB
- en: To further illustrate this, if we considered the vector ![(i,3 + 2i, - 2)](img/file1535.png
    "(i,3 + 2i, - 2)") in ![C^{3}](img/file1533.png "C^{3}"), we would have
  prefs: []
  type: TYPE_NORMAL
- en: '| ![(i,3 + 2i, - 2) = i \cdot (1,0,0) + (3 + 2i) \cdot (0,1,0) + ( - 2) \cdot
    (0,0,1),](img/file1536.png "(i,3 + 2i, - 2) = i \cdot (1,0,0) + (3 + 2i) \cdot
    (0,1,0) + ( - 2) \cdot (0,0,1),") |'
  prefs: []
  type: TYPE_TB
- en: and this representation in terms of these basis vectors is, clearly, unique.
    What is more, this basis is so natural and common that it has a name, the **canonical
    basis**, and its vectors are usually denoted as ![\{ e_{1},e_{2},e_{3}\}](img/file1537.png
    "\{ e_{1},e_{2},e_{3}\}"). An analogous basis can be defined on ![R^{n}](img/file1212.png
    "R^{n}") and ![C^{n}](img/file1526.png "C^{n}") for any ![n](img/file244.png "n").
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: We use the canonical basis extensively in this book, but with a different notation.
    We refer to it as the **computational basis**.
  prefs: []
  type: TYPE_NORMAL
- en: When you have a vector in a finite-dimensional vector space, sometimes it is
    handy to work with its coordinates with respect to some basis of your choice rather
    than working with its ”raw” expression. In order to do this, we sometimes represent
    a vector ![v](img/file1532.png "v") with coordinates ![\alpha_{1},\ldots,\alpha_{n}](img/file1531.png
    "\alpha_{1},\ldots,\alpha_{n}") by a column matrix having the coordinates as entries.
    For example, in the previous example, the vector ![(1,3 + 2i, - 2)](img/file1538.png
    "(1,3 + 2i, - 2)") would be represented by the column matrix of coordinates
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} 1 \\ {3 + 2i} \\ {- 2} \\ \end{pmatrix}](img/file1539.png
    "\begin{pmatrix} 1 \\ {3 + 2i} \\ {- 2} \\ \end{pmatrix}") |'
  prefs: []
  type: TYPE_TB
- en: with respect to the canonical basis ![\{ e_{1},e_{2},e_{3}\}](img/file1537.png
    "\{ e_{1},e_{2},e_{3}\}").
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to remember that the column matrix of coordinates of a
    vector is always defined with respect to a certain basis.
  prefs: []
  type: TYPE_NORMAL
- en: If we considered, for instance, the basis ![\{ e_{1},e_{3},e_{2}\}](img/file1540.png
    "\{ e_{1},e_{3},e_{2}\}"), then the coordinates of the aforementioned vector would
    be
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} 1 \\ {- 2} \\ {3 + 2i} \\ \end{pmatrix}.](img/file1541.png
    "\begin{pmatrix} 1 \\ {- 2} \\ {3 + 2i} \\ \end{pmatrix}.") |'
  prefs: []
  type: TYPE_TB
- en: And, yes, order matters.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Linear maps and eigenstuff
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what vector spaces are, it is natural to wonder how we can
    define transformations ![\left. L:V\rightarrow W \right.](img/file1542.png "\left.
    L:V\rightarrow W \right.") between some ![F](img/file1320.png "F")-vector spaces
    ![V](img/file379.png "V") and ![W](img/file483.png "W"). In fairness, you could
    define any such transformation ![L](img/file1012.png "L") however you wanted —
    we are not here to set boundaries on your mathematical freedom. But, if you want
    ![L](img/file1012.png "L") to play nicely with the vector space structure of ![V](img/file379.png
    "V") and ![W](img/file483.png "W"), you will want it to be linear. That is, you
    will want to have, for any vectors ![v_{1},v_{2} \in V](img/file1543.png "v_{1},v_{2}
    \in V") and any scalar ![\alpha \in F](img/file1544.png "\alpha \in F"),
  prefs: []
  type: TYPE_NORMAL
- en: '| ![L(v_{1} + v_{2}) = L(v_{1}) + L(v_{2}),\qquad L(\alpha \cdot v_{1}) = \alpha
    L(v_{1}).](img/file1545.png "L(v_{1} + v_{2}) = L(v_{1}) + L(v_{2}),\qquad L(\alpha
    \cdot v_{1}) = \alpha L(v_{1}).") |'
  prefs: []
  type: TYPE_TB
- en: Keep in mind that the addition and multiplication by scalars on the left-hand
    side of these expressions is that of ![V](img/file379.png "V"), while the operations
    on the right-hand side of the expressions are those of ![W](img/file483.png "W").
  prefs: []
  type: TYPE_NORMAL
- en: Linear maps are wonderful. Not only do they have very nice properties, but they
    are also very easy to define. If ![v_{1},\ldots,v_{n}](img/file1546.png "v_{1},\ldots,v_{n}")
    is a basis of ![V](img/file379.png "V") and you want to define a linear map ![\left.
    L:V\rightarrow W \right.](img/file1542.png "\left. L:V\rightarrow W \right."),
    all you have to do is give a value — any value — to ![L(v_{k})](img/file1547.png
    "L(v_{k})") for every ![k = 1,\ldots,n](img/file1548.png "k = 1,\ldots,n"). Then,
    by linearity, the function can be extended to all of ![V](img/file379.png "V")
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| ![L(\alpha_{1}v_{1} + \cdots + \alpha_{n}v_{n}) = \alpha_{1}L(v_{1}) + \cdots
    + \alpha_{n}L(v_{n})](img/file1549.png "L(\alpha_{1}v_{1} + \cdots + \alpha_{n}v_{n})
    = \alpha_{1}L(v_{1}) + \cdots + \alpha_{n}L(v_{n})") |'
  prefs: []
  type: TYPE_TB
- en: for any scalars ![\alpha_{1},\ldots,\alpha_{n} \in F](img/file1529.png "\alpha_{1},\ldots,\alpha_{n}
    \in F"). Furthermore, if we let ![\{ w_{1},\ldots,w_{m}\}](img/file1550.png "\{
    w_{1},\ldots,w_{m}\}") be a basis of ![W](img/file483.png "W") and we let ![a_{k,l}
    \in F](img/file1551.png "a_{k,l} \in F") be the unique scalars such that
  prefs: []
  type: TYPE_NORMAL
- en: '| ![L(v_{k}) = a_{1k}w_{1} + \cdots + a_{nk}w_{n},](img/file1552.png "L(v_{k})
    = a_{1k}w_{1} + \cdots + a_{nk}w_{n},") |'
  prefs: []
  type: TYPE_TB
- en: then the coordinates of ![L(v)](img/file1553.png "L(v)") for any ![v = \alpha_{1}v_{1}
    + \cdots + \alpha_{n}v_{n} \in V](img/file1554.png "v = \alpha_{1}v_{1} + \cdots
    + \alpha_{n}v_{n} \in V") with respect to ![\{ w_{1},\ldots,w_{m}\}](img/file1550.png
    "\{ w_{1},\ldots,w_{m}\}") will be
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} a_{11} & \cdots & a_{1n} \\ {\vdots} & \ddots & {\vdots}
    \\ a_{n1} & \cdots & a_{nn} \\ \end{pmatrix}\begin{pmatrix} \alpha_{1} \\ {\vdots}
    \\ \alpha_{n} \\ \end{pmatrix}.](img/file1555.png "\begin{pmatrix} a_{11} & \cdots
    & a_{1n} \\ {\vdots} & \ddots & {\vdots} \\ a_{n1} & \cdots & a_{nn} \\ \end{pmatrix}\begin{pmatrix}
    \alpha_{1} \\ {\vdots} \\ \alpha_{n} \\ \end{pmatrix}.") |'
  prefs: []
  type: TYPE_TB
- en: To put it in perhaps more schematic terms,
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} &#124; \\ {L(v)} \\ &#124; \\ \end{pmatrix} = \begin{pmatrix}
    a_{11} & \cdots & a_{1n} \\ {\vdots} & \ddots & {\vdots} \\ a_{n1} & \cdots &
    a_{nn} \\ \end{pmatrix}\begin{pmatrix} &#124; \\ v \\ &#124; \\ \end{pmatrix},](img/file1556.png
    "\begin{pmatrix} &#124; \\ {L(v)} \\ &#124; \\ \end{pmatrix} = \begin{pmatrix}
    a_{11} & \cdots & a_{1n} \\ {\vdots} & \ddots & {\vdots} \\ a_{n1} & \cdots &
    a_{nn} \\ \end{pmatrix}\begin{pmatrix} &#124; \\ v \\ &#124; \\ \end{pmatrix},")
    |'
  prefs: []
  type: TYPE_TB
- en: where the column matrices represent the coordinates of the vectors with respect
    to the bases ![\{ v_{1},\ldots,v_{n}\}](img/file1313.png "\{ v_{1},\ldots,v_{n}\}")
    and ![\{ w_{1},\ldots,w_{m}\}](img/file1550.png "\{ w_{1},\ldots,w_{m}\}"). We
    say that the matrix ![{(a_{kl})}_{kl}](img/file1557.png "{(a_{kl})}_{kl}") is
    the **coordinate** **matrix** of ![L](img/file1012.png "L") with respect to these
    bases. If ![V = W](img/file1558.png "V = W") and we have a map ![\left. L:V\rightarrow
    V \right.](img/file1559.png "\left. L:V\rightarrow V \right."), we say that ![L](img/file1012.png
    "L") is an **endomorphism** and, usually, we consider the same basis everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a very special kind of endomorphism that can be defined on any vector
    space: the **identity**. This is just a function ![\text{id}](img/file1560.png
    "\text{id}") that takes any vector ![v](img/file1532.png "v") to ![\text{id}(v)
    = v](img/file1561.png "\text{id}(v) = v"). If ![\left. L:V\rightarrow V \right.](img/file1559.png
    "\left. L:V\rightarrow V \right.") is an endomorphism, we say that a function
    ![L^{- 1}](img/file1562.png "L^{- 1}") is the **inverse** of ![L](img/file1012.png
    "L") if both ![L \circ L^{- 1}](img/file1563.png "L \circ L^{- 1}") and ![L^{-
    1} \circ L](img/file1564.png "L^{- 1} \circ L") are equal to the identity — actually,
    checking either of the two conditions is already sufficient when working with
    endomorphisms on finite-dimensional vector spaces. The coordinate matrix of the
    inverse of a map with coordinate matrix ![A](img/file183.png "A") is just the
    usual inverse matrix ![A^{- 1}](img/file1565.png "A^{- 1}"). What is more, a linear
    map is invertible if and only if so is its coordinated matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: When you have an endomorphism ![\left. L:V\rightarrow V \right.](img/file1559.png
    "\left. L:V\rightarrow V \right."), there may be some vectors ![0 \neq v \in V](img/file1566.png
    "0 \neq v \in V") for which there exists a scalar ![\lambda](img/file1567.png
    "\lambda") such that ![L(v) = \lambda v](img/file1568.png "L(v) = \lambda v").
    These vectors are said to be **eigenvectors** and the corresponding value ![\lambda](img/file1567.png
    "\lambda") is said to be their **eigenvalue**. In some cases, you will be able
    to find a basis of eigenvectors ![v_{1},\ldots,v_{n}](img/file1546.png "v_{1},\ldots,v_{n}")
    with some associated eigenvectors ![\lambda_{1},\ldots,\lambda_{n}](img/file1569.png
    "\lambda_{1},\ldots,\lambda_{n}"). With respect to this basis, the coordinate
    matrix of ![L](img/file1012.png "L") would be a diagonal matrix
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} \lambda_{1} & & \\ & \ddots & \\ & & \lambda_{n} \\ \end{pmatrix}.](img/file1570.png
    "\begin{pmatrix} \lambda_{1} & & \\  & \ddots & \\  & & \lambda_{n} \\ \end{pmatrix}.")
    |'
  prefs: []
  type: TYPE_TB
- en: B.4 Inner products and adjoint operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On an ![F](img/file1320.png "F")-vector space ![V](img/file379.png "V"), we
    may wish to define an **inner product** ![\left\langle - \middle| - \right\rangle](img/file1571.png
    "\left\langle - \middle| - \right\rangle"). This will be an operation taking any
    pair of vectors and returning a scalar, that is, a function ![\left. V \times
    V\rightarrow F \right.](img/file1572.png "\left. V \times V\rightarrow F \right."),
    satisfying the following properties for any ![u,v_{1},v_{2} \in V](img/file1573.png
    "u,v_{1},v_{2} \in V"), and ![\alpha_{1},\alpha_{2} \in F](img/file1512.png "\alpha_{1},\alpha_{2}
    \in F"):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conjugate symmetry**: ![\left\langle v_{1} \middle| v_{2} \right\rangle =
    \left\langle v_{2} \middle| v_{1} \right\rangle^{\ast}](img/file1574.png "\left\langle
    v_{1} \middle| v_{2} \right\rangle = \left\langle v_{2} \middle| v_{1} \right\rangle^{\ast}").
    Of course, if the vector space is defined over ![R](img/file1575.png "R"), then
    ![\left\langle v_{2} \middle| v_{1} \right\rangle^{\ast} = \left\langle v_{2}
    \middle| v_{1} \right\rangle](img/file1576.png "\left\langle v_{2} \middle| v_{1}
    \right\rangle^{\ast} = \left\langle v_{2} \middle| v_{1} \right\rangle"), so ![\left\langle
    v_{1} \middle| v_{2} \right\rangle = \left\langle v_{2} \middle| v_{1} \right\rangle](img/file1577.png
    "\left\langle v_{1} \middle| v_{2} \right\rangle = \left\langle v_{2} \middle|
    v_{1} \right\rangle").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity**: ![\left\langle u \middle| \alpha_{1}v_{1} + \alpha_{2}v_{2}
    \right\rangle = \alpha_{1}\left\langle u \middle| v_{1} \right\rangle + \alpha_{2}\left\langle
    u \middle| v_{2} \right\rangle](img/file1578.png "\left\langle u \middle| \alpha_{1}v_{1}
    + \alpha_{2}v_{2} \right\rangle = \alpha_{1}\left\langle u \middle| v_{1} \right\rangle
    + \alpha_{2}\left\langle u \middle| v_{2} \right\rangle").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positive-definiteness**: If ![u \neq 0](img/file1579.png "u \neq 0"), ![\left\langle
    u \middle| u \right\rangle](img/file1580.png "\left\langle u \middle| u \right\rangle")
    is real and greater than ![0](img/file12.png "0").'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is easy to check that the following is an inner product on ![C^{n}](img/file1526.png
    "C^{n}"):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left\langle (\alpha_{1},\ldots,\alpha_{n}) \middle&#124; (\beta_{1},\ldots,\beta_{n})
    \right\rangle = \alpha_{1}^{\ast}\beta_{1} + \cdots + \alpha_{n}^{\ast}\beta_{n}.](img/file1581.png
    "\left\langle (\alpha_{1},\ldots,\alpha_{n}) \middle&#124; (\beta_{1},\ldots,\beta_{n})
    \right\rangle = \alpha_{1}^{\ast}\beta_{1} + \cdots + \alpha_{n}^{\ast}\beta_{n}.")
    |'
  prefs: []
  type: TYPE_TB
- en: When we have a vector space with an inner product — which is commonly said to
    be an **inner product space** — two vectors ![v](img/file1532.png "v") and ![w](img/file1267.png
    "w") are said to be orthogonal if ![\left\langle v \middle| w \right\rangle =
    0](img/file1582.png "\left\langle v \middle| w \right\rangle = 0"). Moreover,
    a basis is said to be orthogonal if all its vectors are pairwise orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: With an inner product, we can define a **norm** on a vector space. We won’t
    get into the details of what norms are but, very vaguely, we can think of them
    as a way of measuring the length of a vector (don’t think about arrows, please,
    don’t think about arrows…). The norm induced by a scalar product ![\left\langle
    \cdot \middle| \cdot \right\rangle](img/file1583.png "\left\langle \cdot \middle|
    \cdot \right\rangle") is
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left\&#124; v \right\&#124; = \sqrt{\left\langle v \middle&#124; v \right\rangle}.](img/file1584.png
    "\left\&#124; v \right\&#124; = \sqrt{\left\langle v \middle&#124; v \right\rangle}.")
    |'
  prefs: []
  type: TYPE_TB
- en: We say that a basis is **orthonormal** if, in addition to being orthogonal,
    the norm of all its vectors is equal to ![1](img/file13.png "1").
  prefs: []
  type: TYPE_NORMAL
- en: When we are given a matrix ![A = (a_{kl})](img/file1585.png "A = (a_{kl})"),
    we define its **conjugate transpose** to be ![A^{\dagger} = (a_{kl}^{\ast})](img/file1586.png
    "A^{\dagger} = (a_{kl}^{\ast})"), that is
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\begin{pmatrix} a_{11} & \cdots & a_{1n} \\ {\vdots} & \ddots & {\vdots}
    \\ a_{n1} & \cdots & a_{nn} \\ \end{pmatrix}^{\dagger} = \begin{pmatrix} a_{11}^{\ast}
    & \cdots & a_{n1}^{\ast} \\ {\vdots} & \ddots & {\vdots} \\ a_{1n}^{\ast} & \cdots
    & a_{nn}^{\ast} \\ \end{pmatrix}.](img/file1587.png "\begin{pmatrix} a_{11} &
    \cdots & a_{1n} \\ {\vdots} & \ddots & {\vdots} \\ a_{n1} & \cdots & a_{nn} \\
    \end{pmatrix}^{\dagger} = \begin{pmatrix} a_{11}^{\ast} & \cdots & a_{n1}^{\ast}
    \\ {\vdots} & \ddots & {\vdots} \\ a_{1n}^{\ast} & \cdots & a_{nn}^{\ast} \\ \end{pmatrix}.")
    |'
  prefs: []
  type: TYPE_TB
- en: 'The following identities can be easily checked for square matrices and, therefore,
    for linear maps:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![{(A + B)}^{\dagger} = A^{\dagger} + B^{\dagger},\qquad{(AB)}^{\dagger}
    = B^{\dagger}A^{\dagger}.](img/file1588.png "{(A + B)}^{\dagger} = A^{\dagger}
    + B^{\dagger},\qquad{(AB)}^{\dagger} = B^{\dagger}A^{\dagger}.") |'
  prefs: []
  type: TYPE_TB
- en: Here, ![AB](img/file1589.png "AB") denotes the usual matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: If ![\left. L:V\rightarrow V \right.](img/file1559.png "\left. L:V\rightarrow
    V \right.") is an endomorphism on a finite-dimensional vector space ![V](img/file379.png
    "V"), we can define its **Hermitian adjoint** as the only linear map ![\left.
    L^{\dagger}:V\rightarrow V \right.](img/file1590.png "\left. L^{\dagger}:V\rightarrow
    V \right.") that has as coordinate basis with respect to some basis the conjugate
    transpose of the coordinated matrix of ![L](img/file1012.png "L") with respect
    to that same basis. It can be shown that this notion is well-defined, that is,
    that you always get the same linear map regardless of your choice of basis.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: The definition that we have given is, well, not the most rigorous one. Usually,
    when you have a pair of inner product spaces ![V](img/file379.png "V") and ![W](img/file483.png
    "W") with inner products ![\left\langle \cdot \middle| \cdot \right\rangle_{V}](img/file1591.png
    "\left\langle \cdot \middle| \cdot \right\rangle_{V}") and ![\left\langle \cdot
    \middle| \cdot \right\rangle_{W}](img/file1592.png "\left\langle \cdot \middle|
    \cdot \right\rangle_{W}"), the adjoint of a linear map ![\left. L:V\rightarrow
    W \right.](img/file1542.png "\left. L:V\rightarrow W \right.") is defined to be
    the only linear map ![\left. L^{\dagger}:W\rightarrow V \right.](img/file1593.png
    "\left. L^{\dagger}:W\rightarrow V \right.") such that, for every ![v \in V](img/file1517.png
    "v \in V") and ![w \in W](img/file1594.png "w \in W"),
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\left\langle w \middle&#124; L(v) \right\rangle_{W} = \left\langle L^{\dagger}(w)
    \middle&#124; v \right\rangle_{V}.](img/file1595.png "\left\langle w \middle&#124;
    L(v) \right\rangle_{W} = \left\langle L^{\dagger}(w) \middle&#124; v \right\rangle_{V}.")
    |'
  prefs: []
  type: TYPE_TB
- en: We invite you to check that, for the particular case that we have considered
    (![V = W](img/file1558.png "V = W") finite dimensional), both definitions agree.
  prefs: []
  type: TYPE_NORMAL
- en: We say that an endomorphism ![L](img/file1012.png "L") is **self-adjoint** or
    **Hermitian** if ![L = L^{\dagger}](img/file1596.png "L = L^{\dagger}"). And it
    is a fact of life (again, we encourage you to check your favorite linear algebra
    textbook) that every Hermitian operator has an orthonormal basis of real eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we say that an endomorphism ![U](img/file51.png "U") is **unitary** if
    ![U^{\dagger}U = UU^{\dagger} = I](img/file1597.png "U^{\dagger}U = UU^{\dagger}
    = I"), where ![I](img/file53.png "I") denotes the identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Matrix exponentiation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every calculus student is familiar with the exponential function, which is taken
    to be ![{\exp}(x) = e^{x}](img/file1598.png "{\exp}(x) = e^{x}"). If you dive
    deeper into the wonders of mathematical analysis, you’ll learn that the exponential
    function is actually defined as the sum of a series, namely
  prefs: []
  type: TYPE_NORMAL
- en: '![{\exp}(x) = \sum\limits_{k = 1}^{\infty}\frac{x^{k}}{k!}.](img/file1599.png
    "{\exp}(x) = \sum\limits_{k = 1}^{\infty}\frac{x^{k}}{k!}.")'
  prefs: []
  type: TYPE_IMG
- en: As it turns out, this definition can be extended far beyond the real numbers.
    For instance, Euler’s formula — which we introduced in *Appendix* *[*A*](ch024.xhtml#x1-225000A)*,
    Complex* *Numbers* — is the result of extending the definition of the exponential
    function to every ![x \in C](img/file1600.png "x \in C").*
  prefs: []
  type: TYPE_NORMAL
- en: '*Most importantly for our purposes, the exponential function can be extended
    to…matrices! In this way, the exponential of a square matrix is defined, rather
    unsurprisingly, as'
  prefs: []
  type: TYPE_NORMAL
- en: '![{\exp}(A) = \sum\limits_{k = 1}^{\infty}\frac{A^{k}}{k!}.](img/file1601.png
    "{\exp}(A) = \sum\limits_{k = 1}^{\infty}\frac{A^{k}}{k!}.")'
  prefs: []
  type: TYPE_IMG
- en: 'What is more, this definition also works for endomorphisms. If the coordinate
    matrix of an endomorphism ![L](img/file1012.png "L") is ![A](img/file183.png "A")
    (with respect to a particular basis), we can define the exponential of ![L](img/file1012.png
    "L") to be the endomorphism that has coordinate matrix ![{\exp}(A)](img/file1602.png
    "{\exp}(A)") with respect to the basis under consideration. It can be checked
    that this notion is well-defined: we always get the same endomorphism regardless
    of the basis we consider.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, setting out to compute the exponential of a matrix just by summing
    up an infinite series might not be the best of ideas. Thankfully, there is an
    easier way. If a matrix is diagonal, it can be shown that
  prefs: []
  type: TYPE_NORMAL
- en: '| ![\exp\begin{pmatrix} \lambda_{1} & & \\ & \ddots & \\ & & \lambda_{n} \\
    \end{pmatrix} = \begin{pmatrix} e^{\lambda_{1}} & & \\ & \ddots & \\ & & e^{\lambda_{n}}
    \\ \end{pmatrix}.](img/file1603.png "\exp\begin{pmatrix} \lambda_{1} & & \\  &
    \ddots & \\  & & \lambda_{n} \\ \end{pmatrix} = \begin{pmatrix} e^{\lambda_{1}}
    & & \\  & \ddots & \\  & & e^{\lambda_{n}} \\ \end{pmatrix}.") |'
  prefs: []
  type: TYPE_TB
- en: As we mentioned in the previous section, when an endomorphism is Hermitian,
    one can always find a basis with respect to which the coordinate matrix of the
    endomorphism is diagonal (a basis of eigenvectors), so this enables us to compute
    the exponential of Hermitian operators. In general, it is always possible to compute
    the exponential of a matrix [[115](ch030.xhtml#Xdummit-foote), Section 12.3],
    but we won’t discuss how to do that here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to bring this appendix to an end, we will briefly touch upon a fairly
    unrelated topic that we will nevertheless use in some parts of the book: modular
    arithmetic.'
  prefs: []
  type: TYPE_NORMAL
- en: B.6 A crash course in modular arithmetic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your watch says it’s 15:00 and we ask you the time, you will say that it
    is 03:00\. But you would be lying, wouldn’t you? Your watch says it’s 15:00 but
    you’ve just said that it is 3:00\. What is wrong with you? Well, probably nothing.
    It turns out that, when you were telling us the time, you were subconsciously
    working in arithmetic modulo ![12](img/file601.png "12").
  prefs: []
  type: TYPE_NORMAL
- en: Vaguely speaking, when you work with numbers modulo ![n](img/file244.png "n")
    all you are doing is assuming that ![n](img/file244.png "n") and ![0](img/file12.png
    "0") represent the same number. In this way, when you work in arithmetic modulo
    ![4](img/file143.png "4"), for example,
  prefs: []
  type: TYPE_NORMAL
- en: '| ![0 \equiv 4 \equiv 8 \equiv 12 \equiv 16\;({mod}\; 4),](img/file1604.png
    "0 \equiv 4 \equiv 8 \equiv 12 \equiv 16\;({mod}\; 4),") |'
  prefs: []
  type: TYPE_TB
- en: '| ![1 \equiv 5 \equiv 9 \equiv 13 \equiv 17\;({mod}\; 4),](img/file1605.png
    "1 \equiv 5 \equiv 9 \equiv 13 \equiv 17\;({mod}\; 4),") |'
  prefs: []
  type: TYPE_TB
- en: '| ![2 \equiv 6 \equiv 10 \equiv 14 \equiv 18\;({mod}\; 4),](img/file1606.png
    "2 \equiv 6 \equiv 10 \equiv 14 \equiv 18\;({mod}\; 4),") |'
  prefs: []
  type: TYPE_TB
- en: and so on, and so forth. Notice how we have written ![\equiv](img/file112.png
    "\equiv") rather than ![=](img/file447.png "=") to denote that those numbers are
    not, well, equal on their own, but just that they are equal modulo ![4](img/file143.png
    "4") — that’s also why we have that cute ![(\text{mod~}4)](img/file1607.png "(\text{mod~}4)")
    on the right.
  prefs: []
  type: TYPE_NORMAL
- en: In this modular arithmetic setting, you can compute additions and multiplications
    as usual. For example, when working modulo ![4](img/file143.png "4"),
  prefs: []
  type: TYPE_NORMAL
- en: '| ![2 \times 3 = 6 \equiv 2\;({mod}\; 4).](img/file1608.png "2 \times 3 = 6
    \equiv 2\;({mod}\; 4).") |'
  prefs: []
  type: TYPE_TB
- en: 'Ha! Look at what we have done! Now you can tell all your friends that ![2](img/file302.png
    "2") times ![3](img/file472.png "3") is ![2](img/file302.png "2") (you can then
    silently whisper ”modulo ![4](img/file143.png "4")” and still be technically correct).
    But, wait, here comes our favorite one:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![1 + 1 \equiv 0\;({mod}\; 2).](img/file1609.png "1 + 1 \equiv 0\;({mod}\;
    2).") |'
  prefs: []
  type: TYPE_TB
- en: In the end, all those people who claimed that ”one plus one doesn’t necessarily
    equal two” had a point, huh? They surely were talking about modular arithmetic.
    We have no doubt.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more…
  prefs: []
  type: TYPE_NORMAL
- en: Can’t get enough of modular arithmetic? Dummit and Foote have you covered! Have
    fun. [[115](ch030.xhtml#Xdummit-foote)]*
  prefs: []
  type: TYPE_NORMAL
